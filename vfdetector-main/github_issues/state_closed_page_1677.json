[{"number": 2593, "title": "Cherry-pick internal change: Reduce computation load in metric_ops_test to prevent timeout", "body": "The test methods named \"testWithMultipleUpdates\" in test classes \"StreamingAUCTest and \"StreamingPrecisionRecallThresholdsTest\" previously used a large number of samples (5000), which lead to repeated test timeouts in non-copt builds in TensorFlow's OSS Jenkins. For example, see:\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=NO_OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/111/console\n\nOur Jenkins slaves are known to be substantially slower than the internal build machines.\n\nThis CL reduces the compute load in these two test methods. The average non-copt test time of metric_ops_test\nWithout this CL: ~175 s\nWith this CL: ~54 s\nChange: 123628402\n", "comments": []}, {"number": 2592, "title": "dynamic split", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(Reopen if you can sign the CLA)\n", "Am I correct in assuming that this PR provided a variant of `tf.split` which supported dynamic arguments --- i.e. for which arguments `num_or_size_splits` and/or `num` can be integer-typed tensors? \r\n\r\nIf so, did something along these lines ever end up being incorporated into TensorFlow (esp. a differentiable version thereof)?  I'm aware of `tf.dynamic_partition`; but, am under the impression that a `dynamic_split` method would be decidedly more efficient (owing to implicit contiguity assumptions)."]}, {"number": 2591, "title": "TensorFlow \"Development\" Build Issue", "body": "I just built Tensorflow from the most recent source (81db154), and attempted to set it up for a \"development\" install - using `python setup.py develop` after following the instructions in the [documentation](https://www.tensorflow.org/versions/master/get_started/os_setup.html#setting-up-tensorflow-for-development). After doing this, running `import tensorflow` in Python results in a \"package not found\" error. \n\nI have found that changing the line in the documentation \n\n```\nln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow* .\n```\n\nto\n\n```\nln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .\n```\n\nsolves the problem (the difference is an extra backtick near the end of the line). Perhaps this was a typo?\n", "comments": ["Yes, it should be a typo.\n", "Just also pointing out that this documentation is also incorrect for bazel version 2.1 and earlier. The `org_tensorflow/` subdirectory doesn't exist, and these instructions will fail even if the typo is fixed (because of these two changesets: [one](https://github.com/tensorflow/tensorflow/pull/2336/files#diff-1cf444a338533a8469b274d1cd1b7229), [two](https://github.com/tensorflow/tensorflow/commit/14ac2235699509f512b44b71160239c153ab413d)).\n\nThe page mentions that users should \"download the latest stable bazel version\", but no supported (or minimum) version is mentioned. It looks like the project believes that [bazel 2.0+](https://github.com/tensorflow/tensorflow/blob/14ac2235699509f512b44b71160239c153ab413d/WORKSPACE) is sufficient. \n", "From: https://github.com/tensorflow/tensorflow/issues/2497#issuecomment-221631692\n\n```\nBazel 0.2.1 and lower, any TF: the path will be build_pip_package.runfiles/*\nBazel 0.2.2 with TensorFlow before 32fa42a: build_pip_package.runfiles/*\nBazel 0.2.2, TF after 32fa42a: build_pip_package.runfiles/org_tensorflow/*.\nBazel 0.2.3, TF before 32fa42a: build_pip_package.runfiles/__main__/*\nBazel 0.2.3, TF after 32fa42a: build_pip_package.runfiles/org_tensorflow/*\n```\n", "Yep, I was just re-discovering this because of ongoing problems over in #1704 (but thank you for the pointer&mdash;it's good to confirm this is real behavior). These changes broke the workarounds that I was using to get around the broken protobuf behavior. I went your route, too: just blow everything away and pull new copies of everything. I'll update #1704 when the dust settles, since [14ac223](https://github.com/tensorflow/tensorflow/commit/14ac2235699509f512b44b71160239c153ab413d) is supposed to fix this.\n"]}, {"number": 2590, "title": "Added Raspberry Pi cross-compilation support to makefile", "body": "", "comments": []}, {"number": 2589, "title": "Request for Schedule learning rate at arbitrary step(echo)", "body": "There is only tf.train.exponential_decay for decaying the learning rate at constant step.\n\nHowever in **ResNet-1001**: \"Identity Mappings in Deep Residual Networks\", arXiv:1603.05027, 2016,\n**The learning rate starts from 0.1, and is divided by 10 at 32k and 48k iterations.**\nFollowing [1], for all CIFAR experiments we warm up then training by using a smaller learning rate of 0.01 at the beginning 400 iterations and go back to 0.1 after that.\n\nIt's hard to implement in tensor flow since tensor doesn\u2019t support python comparison.\nThe learning rate can\u2019t be set in if/else according to global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n", "comments": ["The `piecewise_constant()` function added in #2442 enables you to specify learning rate schedules like this one&mdash;the trick is to use `tf.case()` (or `tf.cond()` for simpler conditions).\n"]}, {"number": 2588, "title": "Rename atrous_conv2d to dilated_conv2d?", "body": "The name comes from French \"\u00e0 trous\" so it seems there should be an extra underscore in there at least... but this is still unintelligible to non-french speakers. Better would be to use the English word 'dilated'.\n", "comments": ["Seems reasonable to add another alias so that dialated_conv2d == atrous_conv2d.  You can add the alias into nn_ops.py like we do for other kinds of renames here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py#L1212\n", "May I make dilated_conv2d the default name (particularly in the documentation) and leave atrous_conv2d as the alias?\n", "@gpapan what do you think?\n", "Surprisingly, the correct spelling seems to be \"dilated\".\n", "@tobin oops, thanks\n", "@ry I prefer atrous_conv2d.\n\nI will reserve the name dilation for the morphological operation https://en.wikipedia.org/wiki/Dilation_(morphology) which will be added very soon to TF.\n\nWe used the term \"atrous convolution\" in the CVPR 2015 paper:\nhttp://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Papandreou_Modeling_Local_and_2015_CVPR_paper.html\nbefore the term \"dilated convolution\" was proposed in https://arxiv.org/abs/1511.07122.\n", "ok\n"]}, {"number": 2587, "title": "session_ops should not place Placeholder(<string>) onto GPU", "body": "TLDR; session ops should be changed to always place string handle Placeholder on CPU. Currently it places it on the same device as corresponding `GetSessionTensor` op, which fails when data lives on GPU.\n\nSession ops like `get_session_tensor` consist of string `Placeholder` and `GetSessionTensor` ops. They are created with tf.device(None), and then moved to the same device as the input handle. If input_handle represents tensor on GPU, it will try to move both `Placeholder` and `GetSessionTensor` on the GPU.\nHowever, since there's no implementation of string Placeholder on GPU, this fails with `Could not satisfy explicit device specification`. Since `Placeholder` is meant for reading string through `feed_dict`, the data comes from CPU originally, and there's no performance benefit in using GPU implementation of string placeholder, so it should be pinned to CPU.\n\nTest below fails with `InvalidArgumentError: Cannot assign a device to node 'Placeholder_1': Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:GPU:0' because no supported kernel for GPU devices is available.`\n\n```\nimport tensorflow as tf\n\nclass HandleTest(tf.test.TestCase):\n\n  def testHandle(self):\n    config = tf.ConfigProto(log_device_placement=True)\n    with self.test_session(config=config) as sess:\n      dtype=tf.float32\n      with tf.device(\"/gpu:0\"):\n        a = tf.constant(1, dtype)\n\n      a_handle = sess.run(tf.get_session_handle(a))\n      b_holder, b_tensor = tf.get_session_tensor(dtype)\n      print(sess.run(b_tensor, feed_dict={b_holder:\n                                          a_handle.handle}))\n\n\nif __name__ == \"__main__\":\n  tf.test.main()\n\n```\n\n@yuanbyu \n", "comments": ["Fixed by registering GPU kernels for the placeholder op. \n", "btw, you can add \"fixes #2587\" somewhere in public section of internal CL, and this issue will be closed automatically when the CL is merged into github\n"]}, {"number": 2586, "title": "Persistent tensors get placed onto GPU unexpectedly", "body": "Running `get_session_handle` ops seems to affect where subsequent `get_session_tensor` is placed.\nThe following fails when built with `config=cuda` with\n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1239] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE; host dst: 0x208a00000; GPU src: 0x1b1a4a0; size: 4=0x4\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:296] GPU->CPU Memcpy failed\n\n```\n\nLooking at device placement, it seems that `GetSessionTensor` ops are placed on GPU. Changing `failure_case` to `False` makes the test below pass as everything is placed on CPU as expected.\n\n```\nimport tensorflow as tf                                                         \n\nclass HandleTest(tf.test.TestCase):                                             \n\n  def testHandle(self):                                                         \n    with self.test_session() as sess:                                           \n      a = tf.constant(1.0)                                                      \n      a_handle_op = tf.get_session_handle(a)                                    \n      b = tf.constant(2.0)                                                      \n      b_handle_op = tf.get_session_handle(b)                                    \n\n      failure_case = True                                                       \n      if failure_case:                                                          \n        a_p, a_t = tf.get_session_tensor(tf.float32)                            \n        b_p, b_t = tf.get_session_tensor(tf.float32)                            \n        a_handle = sess.run(a_handle_op)                                        \n        b_handle = sess.run(b_handle_op)                                        \n      else:                                                                     \n        a_handle = sess.run(a_handle_op)                                        \n        b_handle = sess.run(b_handle_op)                                        \n        a_p, a_t = tf.get_session_tensor(tf.float32)                            \n        b_p, b_t = tf.get_session_tensor(tf.float32)                            \n\n      c = tf.add(a_t, b_t)                                                      \n      c_handle = sess.run(                                                      \n        tf.get_session_handle(c),                                               \n        feed_dict={a_p: a_handle.handle,                                        \n                   b_p: b_handle.handle})                                       \n      self.assertEqual(3.0, c_handle.eval())                                    \n\n\nif __name__ == \"__main__\":                                                      \n  tf.test.main()                                                                \n\n```\n", "comments": ["@yuanbyu \n", "I found one work-around for this case -- explicitly specify device `/cpu:0` when creating `get_session_tensor` op and modify `session_ops.py` to propagate that info as follows:\n\nreplace\n\n```\n  with ops.device(None):\n    # Commit the device when it is used the first time.\n    holder = array_ops.placeholder(dtypes.string)\n    _register_handle_feeder(holder.graph, holder, dtype)\n    tensor = gen_data_flow_ops._get_session_tensor(holder, dtype, name=name)\n  return (holder, tensor)\n```\n\nwith\n\n```\n  with ops.device(None):\n    # Commit the device when it is used the first time.\n    holder = array_ops.placeholder(dtypes.string)\n    _register_handle_feeder(holder.graph, holder, dtype)\n  tensor = gen_data_flow_ops._get_session_tensor(holder, dtype, name=name)\n  return (holder, tensor)\n```\n", "@yuanbyu has a CL fixing in this in the works.\n"]}, {"number": 2585, "title": "Add links to nightly Python3.5 builds and artifacts", "body": "Including links to build history and whl artifacts\n", "comments": []}, {"number": 2584, "title": "Why tf.image.decode_jpeg is slower than cv2.imread ?", "body": "", "comments": ["Could you quantify how much slower tensor flow's read is vs opencv's?  They are both using libjpeg, so it seems like they should be close.\n", "@aselle Thanks for your reply. I just use the following code to test the speed of tf.image.decode_jpeg. It takes about 10ms + for one image.  However, cv2.imread only takes 1ms.  \n\n```\ndef test_tf_image(path):\n  with tf.device('/cpu:0'): \n    img = tf.read_file(path)\n    image = tf.image.decode_jpeg(img, channels=3)\n\n  with tf.Session() as sess:\n    start = time.time()\n    out = sess.run(image)\n    print time.time() - start\n```\n", "@aselle  Because of this, I can't find the solution to extract AlexNet fc7 features 1000 images per second. It's easy to make it in Caffe. Can you give me some suggestions?\n", "If you built tensorflow yourself make sure you used \"-c opt\" on the bazel command line to compile in optimized mode.\n", "@FangxiangFeng: Can you check whether the slowness is the `tf.read_file` bit or the `tf.image.decode_jpeg` bit?  As Andy said, I don't know why our call into libjpeg would be slower than opencv's. \n", "@aselle I have tried both the released  v0.8 package and tensorflow built by myself. They produced the same result.  \n@girving tf.read_file takes about 5ms. \n", "You may be timing initialization cost, not just the actual operations.  When you call `sess.run` the first time it does some setup work.  What do you get for\n\n```\ndef test_tf_image(path):\n  with tf.device('/cpu:0'): \n    img = tf.read_file(path)\n    image = tf.image.decode_jpeg(img, channels=3)\n\n  with tf.Session() as sess:\n    start = time.time()\n    for i in xrange(1000):\n      out = sess.run(image)\n    print time.time() - start\n```\n", "@girving Thanks.  I will close this issue.\n", "@FangxiangFeng: Does that mean the timings with 1000 iterations looked different?  If so, what were they? \n", "@girving. Yes, your code takes about 100ms to process 1000 images. \n", " PIL.Image.open ,tf.image.decode_jpeg  and opencv2.imread to parse image files to arrays. But found that the pixel values is different", "@lin65505578 that could be from the way tensorflow decodes the images. try the decode_jpeg method with dct_method='INTEGER_ACCURATE' argument"]}, {"number": 2583, "title": "On the return values of bidirectional_rnn", "body": "Due to the overhead of `tf.concat` operations, it is recommended to use `state_is_tuple` flag in recurrent neural network layers.\nHowever, I found that in `bidirectional_rnn` implementation, `outputs` (first return value) is returned as a concatenation of `outputs_fw` and `outputs_bw`.\nIs there any reason that it should return the concatenated one?\nI think it is trace of the days where concatenated states are used, and it would be nice to return forward and backward outputs separately.\nIt seems that it can be implemented simply without breaking backward compatibility, by adding a flag indicating what should be returned.\n", "comments": ["We don't currently support the output being a tuple, only the state.  This should wait until we support the output_shape being a (possibly nested) tuple.  Then we can add this change.\n", "I don't understand exactly; can't we just return two variables `outputs_fw` and `outputs_bw` separately?\nis there a reason that `outputs` should be a single variable?\nI think the typical usage of `bidirectional_rnn` is to multiply `outputs` with a weight matrix of shape `(2 * hidden_size, vocab_size)`, which is identical with adding two results of multiplying weight matrices of `(hidden_size, vocab_size)` to forward and backward outputs.\nIn my opinion, separating two would give a performance gain and a bit more flexibility.\n\n## \n\nI didn't think of the multi-layer cases; is the issue related to multi-layer RNNs?\n", "For backwards compatibility reasons, we can't change bidirectional_rnn.  However we have an open PR implementing dynamic bidirectional rnn, and we can change the signature there to emit the final states.\n", "Indeed - your dynamic bidirectional rnn PR is the one i'm talking about ;)\n"]}, {"number": 2582, "title": "Revert changed brackets", "body": "I was a bit overzealous in 62f76be7b8e1e65d3fdcbe3993638956929710bb and changed `[]` to `()` where it wasn't appropriate.\nThis PR reverts that change.\n", "comments": ["Can one of the admins verify this patch?\n", "Test this please \n", "@tensorflow-jenkins test this please\n"]}, {"number": 2581, "title": "Implement bidirectional_dynamic_rnn (#1779)", "body": "In #1779, it seems that the problem of implementing bidirectional version of `dynamic_rnn` is that `tf.reverse_sequence` cannot reverse a sequence whose shape is unknown.\nHowever, now it can reverse an unknown-shaped sequence according to #1816, so I implemented the dynamic version of `dynamic_rnn` and its tests.\n", "comments": ["Can one of the admins verify this patch?\n", "ping for @ebrevdo \n", "Will look tomorrow\nOn Jun 7, 2016 5:55 PM, \"Vijay Vasudevan\" notifications@github.com wrote:\n\n> ping for @ebrevdo https://github.com/ebrevdo\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2581#issuecomment-224457640,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtimwGtHxN1ISsuG_2iujnK03rBfYErks5qJhMGgaJpZM4Ip5gv\n> .\n", "ping :)\n", "Jenkins, test this please.\n", "@ebrevdo Modified! It now returns `(outputs, states)`, where `outputs` is a tuple of the forward and the backward outputs, and `states` is a tuple of the forward and the backward final state.\nI also modified the documentation a bit.\n", "@jihunchoi sorry for the late response.  some more comments.\n", "@ebrevdo Now it uses keyword arguments, and I refined some of the docstring.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "@ebrevdo is this ready to merge?\n", "Yes, I think so.  Thanks for your patience @jihunchoi.\n"]}, {"number": 2580, "title": "Update RNN PTB example to use state_is_tuple", "body": "Currently, the PTB example emits warnings that using without `state_is_tuple` would soon be deprecated.\nIt can confuse newbies (like me), so it would be nice to use the one that makes no warning.\n", "comments": ["Can one of the admins verify this patch?\n", "ping for @ebrevdo \n", "ping for this too!  thanks\n", "Jenkins, test this please.\n", "Good to merge?\n", "Still some issues. Will review today.\n", "Hi, any updates on this? wondering if the proposed changes are alright to use since the latest version of TensorFlow emits warning as mentioned.\n", "Hello, thank you for the comments! I will look through them in a few days.\n", "Thanks a lot, this works fine, just used the same for modeling on sequences\n", "Hi , there are a `tutorial_ptb_lstm_state_is_tuple` in TensorLayer repo, hope it help. \nhttps://github.com/zsdonghao/tensorlayer\n", "ping @jihunchoi \n", "Hello, sorry for the late response.\nI have been apart from the library for weeks...\nAccording to the comments, I modified codes to be more clear.\nI removed messy index-magics by using LSTMStateTuple; look line 260 and 263-270.\n\n**Found some awkward behavior; modifying it.**\n", "Fixed a stupid typo.\n", "Jenkins, test this please.\n", "Thanks for the pull request!\n"]}, {"number": 2579, "title": "fixed atrous_conv2d to work with unknown shape of the input tensor", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "This is great. Could you add a test to make sure it keeps working?\n", "The tests should still work. I ran them and fixed some imports. Still they are failing, because the calculated padding is somehow wrong. Currently I have no time to investigate. Maybe someone sees the cause immediately.\n", "The current atrous_conv2d seems having some shape issue, I got the following error:.\n\n```\nInvalidArgumentError: Image height 232 and width 248should be divisible by block_size: 16\n     [[Node: atrous_conv2d_7/SpaceToBatch = SpaceToBatch[T=DT_FLOAT, block_size=16, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](transpose_21, atrous_conv2d_7/concat)]]\nCaused by op u'atrous_conv2d_7/SpaceToBatch', defined at:\n```\n\n-----------Update-------------------\n\nI fond changing the space_to_batch_pad from\n\n```\nv = array_ops.expand_dims(array_ops.pack([pad_top, pad_bottom]),1)\nh = array_ops.expand_dims(array_ops.pack([pad_left, pad_right]),1)   \nspace_to_batch_pad = array_ops.concat(1, [v,h])\n```\n\nback to \n\n```\nspace_to_batch_pad = [[pad_top, pad_bottom],\n                          [pad_left, pad_right]]\n```\n\nhelps\n", "Thanks @shampool! Works for me.\n", "sorry, some conflicts have sprung up, can you rebase?\n", "Seems like somebody else applied a fix in the meanwhile: https://github.com/tensorflow/tensorflow/commit/3a744a417178646712e9cda0a85655d5d8968e19\n"]}, {"number": 2578, "title": "Support GIF image decode ops by FreeImage library", "body": "Use FreeImage library to decode GIF images. This is the initial commit with simple implementation first, will add more features and optimize later. Thanks.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "We're not comfortable with the FreeImage dependency. Is there a gif reader library we can use instead?\n", "@martinwicke  \nFor pure gif decoder C++ library, there are:\n\ngiflib  http://giflib.sourceforge.net/\nlibnsgif  http://www.netsurf-browser.org/projects/libnsgif/\n\nto be selected from.\n", "giflib it is. Can you use that instead? Thanks!\n", "OK, I'll use giflib to refactor this code, and submit another PR after future several days. Decoding gif image files is really important in my tensorflow use case.\n\nAt the same time, thank you for your advise.\n", "Closing this in favor of #3020.\n"]}, {"number": 2577, "title": "Support complex numbers in Tensor Transformation Operations", "body": "A number of changes have been made to support complex128, but the the tensor transformation op suite seems to be missing this support (At least in tf.pack and tf.concat). See the example script below which works for float32 and float64, but not complex64 and complex128.\n### Environment info\n\nOperating System: Ubuntu 14.04.4\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n``` bash\n$ ls -l /usr/local/cuda-7.5/lib64/libcud*\n-rw-r--r-- 1 root root    322936 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root        19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root    383336 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root    720192 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 3319 users       13 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 3319 users       17 Feb  9 19:48 /usr/local/cuda-7.5/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 3319 users 61453024 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 3319 users 62025862 Feb  9 00:12 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. **Python 2 GPU nightly**\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\". **0.8.0**\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Vary **dtype** in the following test script between `float32, float64, complex64, complex128`\n   \n   ``` python\n   import numpy as np\n   import tensorflow as tf\n   \n   # Data type for this test case\n   dtype=np.float64\n   \n   # Array shapes\n   shape=(10,20,30,1)\n   # Expected concatenated array shape\n   cshape=(10,20,30,4)\n   \n   # Helpful lambda\n   rn = lambda s, d: tf.random_normal(shape=s, dtype=d)\n   \n   # Produce random arrays of shape depending on the type\n   def r(shape, dtype):\n       if dtype in (np.float32, np.float64):\n           return rn(shape, dtype)\n       elif dtype == np.complex64:\n           return tf.complex(rn(shape, np.float32), rn(shape, np.float32))\n       elif dtype == np.complex128:\n           return tf.complex(rn(shape, np.float64), rn(shape, np.float64))\n       else:\n           raise ValueError(\"Unhandled dtype '{dt}'\".format(dt=dtype))\n   \n   XX = r(shape, dtype)\n   XY = r(shape, dtype)\n   YX = r(shape, dtype)\n   YY = r(shape, dtype)\n   \n   with tf.device('/gpu:0'):\n       concat = tf.concat(3, [XX, XY, YX, YY],)\n   \n   with tf.Session() as S:\n       result = S.run(concat)\n       assert result.shape == cshape\n   ```\n2. If dtype is `np.complex64` of `np.complex128`, the following is thrown:\n   \n   ``` python\n   tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'concat': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n        [[Node: concat = Concat[N=4, T=DT_COMPLEX64, _device=\"/device:GPU:0\"](concat/concat_dim, Complex, Complex_1, Complex_2, Complex_3)]]\n   Caused by op u'concat', defined at:\n     File \"tmp/concat_fail.py\", line 32, in <module>\n       concat = tf.concat(3, [XX, XY, YX, YY],)\n   ```\n### What have you tried?\n\nN/A\n", "comments": ["While `complex64` and `complex128` support is almost complete in terms of CPU ops, the majority of kernels hasn't been enabled for the GPU yet, unfortunately.\nIf the tensorflow devs are interested in bringing more `complex64` and `complex128` ops to the GPU, we should probably start by polishing the Eigen tensor library a bit (e.g. by writing efficient reduction code for these dtypes).\n", "related but distinct: #3624 \n", "Looks like `pack` and `concat` support `complex128` now."]}, {"number": 2576, "title": "compile error", "body": "### What have you tried?\n1. bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nI've got these errors.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n[2,065 / 2,589] Linking tensorflow/core/kernels/libcast_op.pic.lo\nINFO: From Compiling tensorflow/core/kernels/batch_matmul_op.cc:\nIn file included from external/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/CXX11Meta.h:14:0,\n                 from external/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/Tensor:18,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from tensorflow/core/kernels/batch_matmul_op.cc:21:\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Run(In, In, bool, bool, Out, int, int) [with In = Eigen::TensorMap<Eigen::Tensor<const std::complex<double>, 3, 1, long int>, 16>; Out = Eigen::TensorMapEigen::Tensor<std::complex<double, 3, 1, long int>, 16>; Scalar = std::complex<double>]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<std::complex<float >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 3, 1, long int>, 16> > >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 3, 1, long int>, 16> > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 3, 1, long int>, 16> >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<std::complex<float >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<std::complex<float >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 3, 1, long int>, 16> > >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<std::complex<float >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Run(In, In, bool, bool, Out, int, int) [with In = Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 3, 1, long int>, 16>; Out = Eigen::TensorMapEigen::Tensor<std::complex<float, 3, 1, long int>, 16>; Scalar = std::complex<float>]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<int, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<int, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const int, 3, 1, long int>, 16> > >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const int, 3, 1, long int>, 16> > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<int, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const int, 3, 1, long int>, 16> >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<int, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const int, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<int, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<int, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const int, 3, 1, long int>, 16> > >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<int, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const int, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Run(In, In, bool, bool, Out, int, int) [with In = Eigen::TensorMap<Eigen::Tensor<const int, 3, 1, long int>, 16>; Out = Eigen::TensorMap<Eigen::Tensor<int, 3, 1, long int>, 16>; Scalar = int]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<double, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<double, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const double, 3, 1, long int>, 16> > >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const double, 3, 1, long int>, 16> > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<double, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const double, 3, 1, long int>, 16> >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<double, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const double, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<double, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<double, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const double, 3, 1, long int>, 16> > >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<double, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const double, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Run(In, In, bool, bool, Out, int, int) [with In = Eigen::TensorMap<Eigen::Tensor<const double, 3, 1, long int>, 16>; Out = Eigen::TensorMap<Eigen::Tensor<double, 3, 1, long int>, 16>; Scalar = double]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<float, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const float, 3, 1, long int>, 16> > >, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const float, 3, 1, long int>, 16> > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const float, 3, 1, long int>, 16> >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<float, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const float, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorChippingOp<0l, Eigen::TensorMap<Eigen::Tensor<float, 3, 1, long int>, 16> >, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 1ul>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<float, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const float, 3, 1, long int>, 16> > >, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_conjugate_op<float, const Eigen::TensorChippingOp<0l, const Eigen::TensorMap<Eigen::Tensor<const float, 3, 1, long int>, 16> > > > >]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h: In static member function 'static void tensorflow::LaunchBatchMatMul<Eigen::ThreadPoolDevice, Scalar>::Run(In, In, bool, bool, Out, int, int) [with In = Eigen::TensorMap<Eigen::Tensor<const float, 3, 1, long int>, 16>; Out = Eigen::TensorMap<Eigen::Tensor<float, 3, 1, long int>, 16>; Scalar = float]':\nexternal/eigen_archive/eigen-eigen-f3a13643ac1f/unsupported/Eigen/CXX11/src/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nERROR: /home/suzuki/tensorflow/tensorflow/core/kernels/BUILD:296:1: C++ compilation of rule '//tensorflow/core/kernels:pad_op' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 ... (remaining 111 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee file:///usr/share/doc/gcc-4.8/README.Bugs for instructions.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n", "comments": ["This issue is caused by OOM, I added a larger swap file and tried again. The issue was fixed.\n"]}, {"number": 2575, "title": "while loop endless loop", "body": "### Environment info\n\nOperating System: CentOS 7\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCPU version\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n0.8.0\n### Steps to reproduce\n\nI am using TensorFlow to implement a network that needs to use `tf.while_loop()`\n\n```\nimport tensorflow as tf\nimport numpy as np\nclass model(object):\n    def __init__(self):\n        self.argmax_ep_gate_array = [ tf.placeholder(tf.int32, [None]) for _ in range(10)]\n        argmax_ep_gate_array_concat = tf.concat(0, self.argmax_ep_gate_array)\n        story_len = tf.constant(7)\n        starter = tf.constant(0)\n        z = []\n        def body(hops):\n            hops = tf.add(hops,1)\n            z.append(hops)\n            return hops\n        def condition(hops):\n            return tf.logical_and(tf.less(tf.gather(argmax_ep_gate_array_concat, hops),story_len),tf.less(hops,tf.constant(20)))\n\n        self.gate_index = tf.while_loop(condition,body,[starter])\n        self.z=tf.concat(0,z)\n\n    def step(self, sess):\n        feed={}\n        for i in range(10):\n            feed[self.argmax_ep_gate_array[i].name]=[i]\n        print (sess.run([self.gate_index,self.z],feed))\nwith tf.Session() as sess:\n    while_loop = model()\n    sess.run(tf.initialize_all_variables())\n    while_loop.step(sess)\n\n```\n### What have you tried?\n\nI find that If I want to sess.run() any variable in the body() that is not returned, tensorflow would stuck into endless loop. \nThe above example is trivial, but it reveals something. In the real case, I am using `tf.while_loop()` running a RNN which includes y= wx+b something like that, but the `w` and `b` are not returned after while loop. In the forward network, it works fine. However, if I run the back propagation, the program would stuck into endless loop. I suppose the code above reproducing my issue, because back propagation do need to modify `w` and `b`. Or is there any way to handle this issue? \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nNo logs, Just stuck into endless loop\n", "comments": ["Same issue here. the while_loop stuck in infinite loop. I found some people asked in stack overflow, but nobody answer it.  I am not sure whether this is the bug from Tensorflow or we didn't use it in a correct way to use it. any developer in tensorflow can answer this problem?\n", "I [answered this on Stack Overflow](http://stackoverflow.com/a/37573208/3574081). Feel free to continue the discussion there if it does not fix your problem.\n", "Trigger update_date\n"]}, {"number": 2574, "title": "Remove unnecessary check from docker_run_gpu.sh", "body": "docker_run_gpu.sh was checking for /usr/local/cuda, which is not the default install location on ubuntu 16.04. The check was not being used by anything else anyways, so removing.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2573, "title": "avg/max_pool3d description has a bug.", "body": "In file:\n[tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.nn.avg_pool3d.md\n](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.nn.avg_pool3d.md)\n[tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.max_pool3d.md\n](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.max_pool3d.md)\nOriginal:\n<b>`ksize`</b>: A list of `ints` that has length `>= 5`.\n    1-D tensor of length 5. The size of the window for each dimension of\n    the input tensor. Must have `ksize[0] = ksize[1] = 1`.\n\nI think\n`ksize[0] = ksize[1] = 1`. should change to `ksize[0] = ksize[4] = 1`, according to the test file https://github.com/tensorflow/tensorflow/blob/712e41cf8b316ef2c33c6dd7fd6ade2b4e93ddc0/tensorflow/python/kernel_tests/pooling_ops_3d_test.py#L48\n", "comments": ["This looks to be the case, and it seems like MaxPool3D, MaxPoolGrad3D, AvgPoolGrad3D all have the same issue. Thanks.\n"]}, {"number": 2572, "title": "freeze_graph gives Unexpected EOF from Bazel server (Scikit Flow) or gcc internal compiler error", "body": "I can't get the hang of exporting a scikit flow model to a model.pb file to use it with android.\n### Environment info\n\nOperating System:16.04 LTS\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nNo such file or directory\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. -?\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.8.0\n### Steps to reproduce\n1. classifier.save('model/')\n2. copy contents of model folder to tensorflow root\n3. cd tensorflow && bazel build tensorflow/python/tools:freeze_graph && bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=graph.pbtxt --input_checkpoint=checkpoint --output_graph=/tmp/frozen_graph.pb --output_node_names=softmax --jobs 4 --verbose_failures > output.txt  2>&1\n\n-> should compile a *.pb file from the *.pbtxt file. bazel build tensorflow/python/tools:freeze_graph fails already.\n\nResult after a while of compiling:\nUnexpected EOF from Bazel server or gcc internal compiler error\n### What have you tried?\n1. bazel clean -> only takes longer\n2. adding RAM and increasing RAM of VM (11GB of 16GB)\n3. --jobs 4\n### Logs or other output that would be helpful\n\n![bazel](https://cloud.githubusercontent.com/assets/10122382/15635531/ce333328-25e0-11e6-8b4d-4e753f0d47a5.PNG)\nTraceback (most recent call last):\n  File \"/home/administrator/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/tensorflow/python/tools/freeze_graph.py\", line 40, in <module>\n    import tensorflow as tf\n  File \"/home/administrator/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/administrator/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/tensorflow/python/**init**.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/administrator/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/administrator/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/tensorflow/python/pywrap_tensorflow.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow\nImportError: No module named _pywrap_tensorflow\n", "comments": ["Ok figured it out. The problem was that the --jobs 4 should also be at another of those statements. The right statement is:\n`cd tensorflow && bazel build tensorflow/python/tools:freeze_graph --jobs 4 && bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=graph.pbtxt --input_checkpoint=checkpoint --output_graph=/tmp/frozen_graph.pb --output_node_names=softmax --jobs 4 --verbose_failures` notice  bazel build tensorflow/python/tools:freeze_graph **--jobs 4**. I now have another problem but this one is closed.\n"]}, {"number": 2571, "title": "_pywrap_tensorflow.so: Undefined symbol \"_ZN9perftools8gputools3rng10RngSupport13kMinSeedBytesE\"", "body": "Hello,\n\nI am trying to bring TensorFlow support to FreeBSD. Considering there is Mac support, I figured FreeBSD support couldn't be too bad to work on. I have hit a few snags along the way but nothing show stopping yet. My latest problem is I'm getting an undefined symbol error. I do not want gpu support for the FreeBSD port, but I can see ::perftools::gputools are scattered all throughout the source code. If you could give me any feedback as to what to look into to fix this, that'd be great.\n### Environment info\n\nOperating System: FreeBSD 10.3\nInstalled version of CUDA and cuDNN: None\n\nIf installed from sources, provide the commit hash: d8eb4bb6470d4cb3d0f67f2111a39fa50f1c28e5\n### Steps to reproduce\n1. Build TensorFlow on FreeBSD 10.3 with Clang 3.8\n2. Try to import tensorflow in Python3 REPL\n### Things I've tried\n\nI also tried this with Python 2. Same results.\n### Logs or other output that would be helpful\n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.4/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python3.4/site-packages/tensorflow/python/__init__.py\", line 48, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/usr/local/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/usr/local/lib/python3.4/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/usr/local/lib/python3.4/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\nImportError: /usr/local/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow.so: Undefined symbol \"_ZN9perftools8gputools3rng10RngSupport13kMinSeedBytesE\"\n```\n", "comments": ["Not sure if this is related, but this symbol is special-cased for `#if defined(__APPLE__)`, so it might be relevant:\n\nhttps://github.com/tensorflow/tensorflow/blob/e39d8feebb9666a331345cd8d960f5ade4652bba/tensorflow/stream_executor/rng.cc#L44\n", "That is probably exactly it. I will add an `|| defined(_BSD_SOURCE)` to my local copy.\n", "Awesome. When you get it working on FreeBSD, please feel free to submit a pull request\n", "After removing that define, TensorFlow runs on FreeBSD 10.3. To anyone reading this, there is another place where `defined(__APPLE__)` that caused problems (ENOENT undefined error). You may or may not run into it, but if you do just go into the source and edit the causing file. Very small change.\n\nI would also like to mention that even with 5GB of RAM and 1GB of swap, TensorFlow compilation eats everything and eventually your computer will grind to a halt. Don't worry though - you can continue the compilation process after the OS has killed the process.\n\n```\n[lee@ ~/3rdparty/]$ python\nPython 2.7.11 (default, May  7 2016, 01:22:47) \n[GCC 4.2.1 Compatible FreeBSD Clang 3.4.1 (tags/RELEASE_34/dot1-final 208032)] on freebsd10\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\n>>> import tensorflow as tf \n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> sess = tf.Session()\ncan't determine number of CPU cores: assuming 4\ncan't determine number of CPU cores: assuming 4\n>>> print(sess.run(hello))\nHello, TensorFlow!\n>>> a = tf.constant(10)\n>>> b = tf.constant(32)\n>>> print(sess.run(a + b))\n42\n>>> :)\n```\n\nThis is so great!\n\nI will try to formalize a pull request or the required changes to get it working for others.\n\nEDIT: I have summarized the changes in this write up here: http://ecc-comp.blogspot.com/2016/06/tensorflow-on-freebsd.html\n"]}, {"number": 2570, "title": "Adding DecodeJpeg Op to Android results in build errors (jpeg-9a is missing)", "body": "I tried to add the jpeg libraries to the Android build so that I could easily use the model I trained using the transfer learning example, after adding the decode_jpeg_op.cc and removing the jpeg exclude from the android core libraries I ran into issues with the jpeg-9a includes from this file not being anywhere in the repo:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/jpeg.h#L26\n\nI'm not sure if this library being missing is just an oversight or whether this is unsupported at the moment. I can probably deal with it not being there by stripping that node out of my graph, it just seems to be a random sharp edge\n", "comments": ["I guess I can't really deal with this since Graphs are considered append-only? As per http://stackoverflow.com/questions/34235225/is-it-possible-to-modify-an-existing-tensorflow-computation-graph\n\nI did find some information on how to copy graphs: https://codesachin.wordpress.com/2015/11/20/recursively-copying-elements-from-one-graph-to-another-in-tensorflow/ but that seems like it's liable to break since it's digging into the graph internals...\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/strip_unused.py might be useful for you, potentially?\n", "@vrv Thank you very much! That seems to get me one step closer, but when I specify DecodeJpeg as my input node, and then try to import the graph I get the following error:\n\nValueError: graph_def is invalid at node u'Cast': Input tensor 'DecodeJpeg:0' Cannot convert a tensor of type float32 to an input of type uint8.\n\nCast is the node that DecodeJpeg feeds into. When I set Cast as my input node it works fine though, so \n\n[EDIT]: Nevermind, I see I forgot to supply the type variable, passing in uint8 as the type fixed that error (duh...)\n", "As you seem to have been able to resolve your problem, I am closing the issue for now.\n", "@aselle I managed to resolve my issue with a workaround, but the jpeg.h file I linked to still references header files not in the repo. It's not enabled on Android by default, but still seems broken...\n", "@kuza55 kuza55, \n\nMate, I too have the same issue \"No OpKernel was registered to support Op 'DecodeJpeg'\" I changed the input in Jni file to DecodeJpeg:0 and also tried to strip the graph of unused nodes, still same issue.\nhelp.\n", "@Shaikjalal Are you sure it's getting stripped out? You should be passing bypassing the DecodeJpeg node completely on Android.\n\n@kuza55 Agreed that the code seems to imply we support jpeg on Android, which is inconsistent with the fact that we don't. I'll take a look at providing a better error message in the invalid configurations.\n", "@kuza55 jpeg.h and png.h have been updated to explicitly exclude mobile builds so that it's clear we don't support them there.\n", "@andrewharp the jpeg decoding libraries seem to be indeed excluded from build, but they are used in the ios example code, is there something to do to add them?\n", "@andrewharp sorry the strip_unused wasn't stripping it.\nI recently updated to 0.9 version and now I get a protobuf error when running the strip_unused script any ideas. \n\nraise self._ParseError('Expected identifier.')\ngoogle.protobuf.text_format.ParseError: 2:1 : Expected identifier.\n", "@Shaikjalal What's the command line you're giving to strip_unused? It sounds like it's not reading from a valid text-format protobuf file.\n", "@maelp Sorry, I'm not sure what you mean. AFAICT libjpeg and libpng are not included in any mobile build. The makefile-based iOS build uses https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/gen_file_lists.sh to create the source file list, and this specifically excludes png and jpeg files.\n\nYou should be able to get them build if you start by adding them to the dependency tree for //tensorflow/core:android_tensorflow_lib_lite. We just aren't supporting that right now as there are built-in image manipulation options available on mobile platforms.\n", "@andrewharp this is the command line i am running \nbazel-bin/tensorflow/python/tools/strip_unused --input_graph=/BBGraph/output_graph.pb --output_graph=/tmp/stripped_graph.pb --input_node_names=cast --output_node_names=final_result\n", "@andrewharp: sorry it was related to the bug #2754 that I closed -- it was linked to the model I downloaded from the web which included some operations that are not in the mobile app, but the link to the model has been updated\n", "@Shaikjalal Can you try running strip_unused with --input_binary=true? It may be trying to read a binary proto as if it were text format.\n", "@andrewharp Thanks that worked, I will try load the new graph in the demo and see if it infers, fingers crossed. :)\nCheers!\n", "@kuza55 Did you use bazel or makefile to add `DecodeJpeg` to android?"]}, {"number": 2569, "title": "Correct resnet.py and hdf5_classification.py", "body": "**1. Update tensorflow/examples/skflow/resnet.py**\nmove \"upscale to the next block size\" to the outer loops\n**2. Update tensorflow/examples/skflow/hdf5_classification.py**\nclassifier.fit and metrics.accuracy_score takes ndarray as input.\nDirect input of HDF5 dataset will cause TypeError: PointSelection **getitem** only works with bool use HDF5 dataset's value or convert to numpy array\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I have signed it\uff01\nGoogle account: sunyu.buaa@gmail.com\nGitHub account: sun9700@foxmail.com\n\nBest wishes\uff01\n\n> \u5728 2016\u5e745\u670829\u65e5\uff0c23:48\uff0cgooglebot notifications@github.com \u5199\u9053\uff1a\n> \n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n> \n> If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check your existing CLA data and verify that your email is set on your git commits.\n> If you signed the CLA as a corporation, please let us know the company's name.\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@sun9700 Could you provide a failing example that's reproducible? Thanks.\n", "@terrytangyuan \n**The resnet.py 's error is obvious**\nTraceback (most recent call last):\n  File \"/home/sun/tensorflow/tensorflow/examples/skflow/resnet.py\", line 158, in <module>\n    mnist.train.images, mnist.train.labels, logdir='models/resnet/')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 241, in fit\n    self._setup_training()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 156, in _setup_training\n    self._out)\n  File \"/home/sun/tensorflow/tensorflow/examples/skflow/resnet.py\", line 119, in res_net\n    net = conv + net\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 520, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 44, in add\n    return _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 694, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2155, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1611, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1367, in _BroadcastShape\n    % (shape_x, shape_y))\n**ValueError: Incompatible shapes for broadcasting: (?, 14, 14, 128) and (?, 14, 14, 256)**\n", "Sorry I meant hdf5 example. BTW, you need to use the primary email of your Github to sign CLA. \n", "@terrytangyuan The error of hdf5_classification.py only occurs in the latest (20160530) nightly binaries. It works in my previous tensorflow.\n\nTraceback (most recent call last):\n  File \"/home/sun/tensorflow/tensorflow/examples/skflow/hdf5_classification.py\", line 46, in <module>\n    classifier.fit(X_train, y_train)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 172, in fit\n    monitors=monitors)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 433, in _train_model\n    monitors=monitors)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 346, in train\n    reraise(*excinfo)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 272, in train\n    feed_dict = feed_fn() if feed_fn is not None else None\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py\", line 333, in _feed_dict_fn\n    if len(self.X.shape) == 1 else self.X[batch_indices])\n  File \"/usr/lib/python2.7/dist-packages/h5py/_hl/dataset.py\", line 419, in __getitem__\n    selection = sel.select(self.shape, args, dsid=self.id)\n  File \"/usr/lib/python2.7/dist-packages/h5py/_hl/selections.py\", line 71, in select\n    sel[arg]\n  File \"/usr/lib/python2.7/dist-packages/h5py/_hl/selections.py\", line 209, in __getitem__\n    raise TypeError(\"PointSelection **getitem** only works with bool arrays\")\n**TypeError: PointSelection **getitem** only works with bool arrays**\n", "@sun9700 Thanks for reporting. However, we should fix it in data_feeder instead. Some recent changes broke the example and [this test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/tests/test_data_feeder.py#L137) is not checked on Jenkins since we don't have it installed there. @ilblackdragon @martinwicke Moving forward, are we planning to test those on Jenkins? What's the status on data_feeder refactoring?\n", "@terrytangyuan Yes, thanks for your efforts.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Yes, we should be running those tests. You can add the h5py package to install_pip_packages.sh (and possibly the 3.5 version of that), then the tests should start running. @caisq FYI in case I forgot something important that would make that a bad idea. \n", "@tensorflow-jenkins test this please!\n", "@tensorflow-jenkins test this please\n", "Needs to be rebased.\n", "(Sorry for the delay).\n", "Hi @sun9700. It's been a month since the last ping, so I'm going to close this PR as stale for now. Please feel free to re-open it when you have time to update it, we would be glad to have it.\n"]}, {"number": 2568, "title": "question-words.txt link broken for word2vec", "body": "The link for the word2vec analogy-question text file is borken:\n[https://word2vec.googlecode.com/svn/trunk/questions-words.txt](https://word2vec.googlecode.com/svn/trunk/questions-words.txt)\n", "comments": ["You can download the archived source code of the original code and extract a single file by:\n\n``` terminal\nwget https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip\nunzip -p source-archive.zip  word2vec/trunk/questions-words.txt > questions-words.txt\nrm source-archive.zip\n```\n\nWell somebody might update  `./tensorflow/models/embedding/README.md`\n", "This link is still present in master at https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/embedding/word2vec_optimized.py#L54\n", "Fixed internally, will be sync'd soon.\n", "Thanks!\nOn Sat, Aug 20, 2016 at 2:00 AM Andrew Selle notifications@github.com\nwrote:\n\n> Fixed internally, will be sync'd soon.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2568#issuecomment-241089509,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAHwCgVoLpe0mc24auLK-KmdokK5aovyks5qhe9IgaJpZM4IpTVf\n> .\n", "This is fixed as of 1c131f29d7877e3155f1e58ef3aa99b51faf399b\n"]}, {"number": 2567, "title": "Enable segment reduction ops for complex numbers", "body": "I've enabled segment reduction ops for complex 64 and complex128 on the CPU,\nin order to bring their behavior in line with the regular reduction ops.\nOnly sum and prod reductions have been enabled, as min and max are not\nappropriate and mean would require changes to the code and is probably\nnot worth it.\n\nThis fixes #2255\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2566, "title": "Android Example utils fail on models > 64MB", "body": "I've been trying to get started with tensorflow and I wanted to use an image recognition model I trained in the android example, but when I tried to replace the models in the example with my model, it would crash without a lot of information.\n\nI finally figure out that ReadFileToProto in jni_utils.cc does not support reading files larger than 64MB: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/jni_utils.cc#L107\n\nAdding these lines seemed to fix the issue of not being able to load my graph at all:\n\n```\n::google::protobuf::io::CodedInputStream coded_stream(&adaptor);\ncoded_stream.SetTotalBytesLimit(1024LL << 20, 512LL << 20);\n\ncoded_stream.Skip(start);\nCHECK(message->ParseFromCodedStream(&coded_stream));\n```\n\nI'm probably not the first to stumble into this problem, but I feel like I just wasted hours of my life on a trivially avoidable problem and I hope no-one else has to waste time on that little bit of protobuf trivia.\n", "comments": ["There are some known issues with protobuf and large files that we need to work on. Thank you for tracking it down and the report.\n", "Thanks for the report @kuza55! I'll work on getting a patch into the Android example.\n", "I've got your suggested fix checked in @kuza55, thanks for that. I'm closing this as fixed.\n", "Apologies, the code isn't in Github yet, so reopening!\n", "This should actually be fixed now.\n"]}, {"number": 2565, "title": "examples/skflow/out_of_core_data_classification.py dose not work", "body": "/usr/bin/python2.7 /home/sun/tensorflow/tensorflow/examples/skflow/out_of_core_data_classification.py\nTraceback (most recent call last):\n  File \"/home/sun/tensorflow/tensorflow/examples/skflow/out_of_core_data_classification.py\", line 47, in <module>\n    classifier.fit(X_train, y_train)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 168, in fit\n    x, y, n_classes=self.n_classes, batch_size=self.batch_size)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/data_feeder.py\", line 116, in setup_train_data_feeder\n    X, y, n_classes, batch_size, shuffle=shuffle, epochs=epochs)\nTypeError: **init**() got an unexpected keyword argument 'epochs'\n\nProcess finished with exit code 1\n", "comments": ["Examples was deleted as untested/unsupported one :(\n"]}, {"number": 2564, "title": "add sparse gradients support for RMSPropOptimizier", "body": "#2476\n", "comments": ["Can one of the admins verify this patch?\n", "Looks good at first glance, assiging to @lukaszkaiser - do you know someone who can double check the math portions?\n", "George Dahl agreed to take a look, he's just joining the right groups on github.\n", "I took a very quick glance and I believe this PR does the \"wrong\" thing efficiently, however it is a pretty reasonable \"wrong\" thing to do. What I mean is that for sparse updates, I don't think this will behave the same was as treating them as dense would. So doing the same optimization, but treating the updates as sparse will produce different results than treating them as dense, even if all the gradients are the same. This property needs to be documented, because one might somehow assume that using the sparse version would do the same thing as the dense one, just more efficiently when you have sparse updates.\n\nEssentially, RMSProp and similar algorithms generate an update for each weight at each step and the approach here discards the updates for weights that have a zero gradient on that step. This gives the rarely updated indices a lower learning rate than they would otherwise have in the dense version of the algorithm since \"correct\" RMSprop would divide by gradient statistics and amplify the updates for weights that had many zero gradients. However, that said, for sparse updates this might work ok sometimes and it isn't clear what the best thing to do is, so doing the easy and efficient thing could be worth implementing.\n\nI am pretty sure Adam in TF for sparse updates has the same problem as exhibited in this PR, and since this PR was for a feature request spawned by the inconsistency between ADAM supporting sparse updates and RMSProp not supporting them, it makes sense to have an analogous behavior. Of course it would be good to add something to the ADAM docs explaining that for sparse updates it only updates weights with non-zero gradient that step and makes no attempt at correcting for this.\n\nSo to summarize, the basic approach of the PR seems OK, but I would like to see the exact algorithm documented better in the sparse case so users can be warned not to think it works the same way as the dense update algorithm. ADAM might also need a doc improvement.\n", "@georgedahl thank you very much! I took a look at `RMSProp algorithm with Nesterov momentum` presented in Bengio's `deep learning`, you can record a variable like timestamp to calculate how many times you have not multiplied `decay rate`, but the simple implement is good enough maybe.\n", "Sorry, you should comment when you've pushed new changes, otherwise we don't know when the code is ready.  You'll have to resolve conflicts and then update us when you've done so.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@vrv modifications of .md have been removed and conflicts have been solved.\nbut merges too many commits and someone does not signed CLA\n", "I think you rebased incorrectly, can you try again?\n\nThe only commits that should show up here are your own, even after a rebase.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@vrv seems ok now\n", "@tensorflow-jenkins test this please\n", "@vrv MacOS CPU Tests failed, but Linux CPU Tests succeeded\n\n```\n//tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test FAILED in 0.1s\n  /private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/testlogs/tensorflow/core/common_runtime_direct_session_with_tracking_alloc_test/test.log\n\nExecuted 402 out of 517 tests: 516 tests pass and 1 fails locally.\n```\n", "I think it's a flaky test, i will try again  @tensorflow-jenkins test this please\n"]}]