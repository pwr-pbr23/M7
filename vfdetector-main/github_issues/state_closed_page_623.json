[{"number": 34951, "title": "Add multi-algorithm deterministic cuDNN convolutions", "body": "This current pull request intends to address a bug in the functionality of the environment variable `TF_CUDNN_DETERMINISTIC` (see [PR 24747](https://github.com/tensorflow/tensorflow/pull/24747)) and also the environment variable `TF_DETERMINISTIC_OPS` (see [PR 31465](https://github.com/tensorflow/tensorflow/pull/31465)).\r\n\r\nThe current implementation (before application of this current pull request) of deterministic cuDNN convolution in TensorFlow chooses, for any layer configuration, one fixed deterministic algorithm for each of the forward and two backward propagation paths.\r\n\r\nI have since come to appreciate that each algorithm is not guaranteed to work on all layer configurations. The solution represented by this current PR addresses that problem. It uses the existing auto-tuning mechanism to attempt to use all of the available deterministic algorithms, and then, instead of choosing the fastest one (as regular auto-tuning does), this solution chooses the first deterministic algorithm that works. It does this in a deterministic way, based on its index in a intentionally ordered array. ", "comments": ["@duncanriach Can you please check reviewer comments and keep us posted. Thanks!", "@gbaned. I have responded to the reviewer. Please remove `stat:awaiting response`.", "Changes pushed, and discussed [here](https://github.com/tensorflow/tensorflow/pull/34951#discussion_r361750483).", "@gbaned, please will you add the `awaiting review` tag again?\r\n\r\nHistory:\r\n* 7 days ago: You added the `awaiting review` tag.\r\n* 3 days ago: I pushed some changes with an [explanation](https://github.com/tensorflow/tensorflow/pull/34951#discussion_r361750483) with the intention of moving this PR forward.\r\n* 2 days ago: The bot removed the `awaiting review tag`.\r\n* Today: I added another [response](https://github.com/tensorflow/tensorflow/pull/34951#discussion_r362102193).\r\n", "This change had to be rolled back. It seems one of our test targets became flaky with this CL.", "@akuegel, oh no! Could you tell me which test target became flaky?\r\n\r\n(adding link to [roll-back commit](https://github.com/tensorflow/tensorflow/commit/2938772a08ed02ced4663ca38168ab3f82e8f81b) here)", "> @akuegel, oh no! Could you tell me which test target became flaky?\r\n\r\nIt is an internal target, but AFAICT the target was always flaky.  I'm now following up with the team internally.", "Awesome. Thank you, @sanjoy!", "I will try to roll forward again.", "Thank you, @akuegel.", "Hi @duncanriach , I think your pull introduced non-deterministic behavior of tensorflow 2.5.0. For example, I launched the tensorflow twice to do inference for my convolution graph. using following environment variables:\r\n\r\n`setenv CUDA_VISIBLE_DEVICES 0 ; setenv TF_CPP_MAX_VLOG_LEVEL 4 ; setenv TF_DUMP_GRAPH_PREFIX ; setenv TF_DETERMINISTIC_OPS 1 ; setenv TF_USE_DEFAULT_CONV_ALGO 0 ; setenv TF_CUDNN_USE_AUTOTUNE 1`\r\n\r\nThe 1st run cudnn autotune giving the following convolution algorithms:\r\n2021-11-21 04:07:25.562823: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 6\r\n2021-11-21 04:07:25.966770: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 6\r\n2021-11-21 04:07:26.366184: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 6\r\n2021-11-21 04:07:26.407513: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 0\r\nBut second run, cudnn autotune gives:\r\n2021-11-21 04:07:48.726334: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 6\r\n2021-11-21 04:07:49.133102: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 6\r\n2021-11-21 04:07:49.533022: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 6\r\n2021-11-21 04:07:49.574372: I tensorflow/core/kernels/conv_ops.cc:1306] Convolution Algorithm: 1\r\nThen, these 2 run will give generate different inference result due to different conv algos are used.\r\n\r\nDo you happen to have a work around to avoid this non-deterministic behavior? Turn off autotune by TF_CUDNN_USE_AUTOTUNE, and reply on cudnnGetConvolutionForwardAlgorithm_v7 to select convolution algorithm is a feasible solution? Thanks in advance.", "Hi @Leiwu-Zheng, I believe that the code to deterministically select cuDNN convolution algorithms has evolved significantly since this pull request. @kaixih, please could you comment on this?", "Is this for the TF2 code? For the cudnn frontend API, we will use the *first* and working deterministic engine when `TF_CUDNN_DETERMINISTIC=1`. So, it should be able to guarantee an in-run and run-to-run determinism. For the legacy cudnn API (as the algos shown in https://github.com/tensorflow/tensorflow/pull/34951#issuecomment-974714116), I feel `TF_CUDNN_DETERMINISTIC=1` can only ensure the in-run determinism since it will use a list of deterministic algos but not be certain which one is used. (I might miss something, since I am not very clear about the context of the thread.)", "@Leiwu-Zheng, @kaixih and I are working on this together. We'll get back to you, but it will probably be in the new year (2022).\r\n\r\nCCing @reedwm as well. Reed, my understanding is that @Leiwu-Zheng is setting `TF_DETERMINISTIC_OPS` to `\"1\"` or `\"true\"` in TF2 and getting nondeterministic selection of deterministic algorithms.\r\n\r\nWait, what version of TensorFlow are you using, @Leiwu-Zheng? According to [my notes](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#auto-tuning-of-cudnn-convolution-algorithms), in version 2.0, you'll need to use `TF_CUDNN_DETERMINISTIC`, but in version 2.1, and later, you'll need to use `TF_DETERMINISTIC_OPS`.\r\n\r\nActually, scratch all of that. @Leiwu-Zheng, please will you open an issue with a clear and well-contained reproducer of the problem you're witnessing.", "@duncanriach, thanks for the investigation. I have updated my previous comment to make it more clear. I will open an issue. Currently, I am looking for a work around that version 2.5.0 can generate deterministic result for both training and inference, but TF_USE_DEFAULT_CONV_ALGO=1 only works for inference.\r\n@kaixih , I noticed in the version 2.5.0 codes, TF_CUDNN_USE_FRONTEND is turned off by default. It means TF_CUDNN_USE_FRONTEND = 1 could be a workaround for version 2.5.0 to get in-run and run-to-run determinism?\r\n\r\nUpdate:\r\nI just launched  tensorflow 2.5.0 10 times to do the inference w/ following settings, it always generate the same result. \r\n`setenv CUDA_VISIBLE_DEVICES 0 ; setenv TF_CPP_MAX_VLOG_LEVEL 0 ; setenv TF_DUMP_GRAPH_PREFIX ; setenv TF_DETERMINISTIC_OPS 1 ; setenv TF_USE_DEFAULT_CONV_ALGO 0 ; setenv TF_CUDNN_USE_AUTOTUNE 1 ; setenv TF_CUDNN_USE_FRONTEND 1`\r\n@duncanriach , do you still we still need to open a bug issue?", "@Leiwu-Zheng, thank you for looking into this more deeply and for discovering this work-around: `TF_CUDNN_USE_FRONTEND = \"1\"`.\r\n\r\ncuDNN convolution algorithm selection should be deterministic when deterministic ops are enabled (via `TF_DETERMINISTIC_OPS` or `tf.config.experimental.enable_op_determinism`, in version 2.8 onwards) without having to enable the cuDNN front-end. In the legacy cuDNN API, cuDNN convolution algorithm selection should be deterministic when deterministic ops are enabled.\r\n\r\nI think this is a bug.\r\n\r\nPlease will you confirm that this issue exists in the latest release (version 2.7; if possible) and open an issue (after attempting to confirm that one does not already exist for this). Provide a simple-as-possible, well-contained reproducer that demonstrates this issue on a specific version of TensorFlow. Please tag me, @reedwm, and @kaixih in that new issue, and reference this discussion.\r\n\r\nThank you for doing all this work, Leiwu."]}, {"number": 34950, "title": "load_weights h5file failed (cause by BN layer) in tensorflow2.0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc1-51-g2646d23074 2.0.0-rc2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NO\r\n- GPU model and memory: NO\r\n\r\n\r\n**Describe the current behavior**\r\nI save my model using .h5 format(model.save_weights(\"./save_weights/resNet.h5\")). And I load this weights(model.load_weights('./save_weights/resNet.h5', by_name=True)) then failed. this error is shape mismatch.\r\ni did some tests:\r\n**(1) when i save model using ckpt format, i can successfully load weights.\r\n(2) when i save model that no use bn layer using h5 format, i can successfully load weights.**\r\n\r\n**Describe the expected behavior**\r\ni think i should load model weights(include bn layer) using .h5 format.\r\n\r\n**Code to reproduce the issue**\r\n(1) **i tracked the problem. the problem is in \"\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\" file(754 line, \"if K.int_shape(symbolic_weights[i]) != weight_values[i].shape:\")**.\r\n(2) when i load model weights, i find the **variables order(layers order) is different between \"symbolic_weights\" and \"weight_values\"** . so you can't simply use list index to load weights. \r\n(3) the main reason is caused by bn layer. i check the varables \"symbolic_weights\" and \"weight_values\",  the bn parameters order in \"weight_values\" is like [gamma, bata, mean, variance, gamma, bata, mean, variance] but in \"symbolic_weights\" is like [gamma, bata, gamma, bata, mean, variance, mean, variance].\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1175, in load_weights\r\n    saving.load_weights_from_hdf5_group_by_name(f, self.layers)\r\n  File \"E:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\", line 760, in load_weights_from_hdf5_group_by_name\r\n    str(weight_values[i].shape) + '.')\r\nValueError: Layer #4 (named \"block1\"), weight <tf.Variable 'block1/unit_1/conv2/kernel:0' shape=(3, 3, 64, 64) dtype=float32, numpy=\r\narray([[[[ 0.00050041, -0.06431282, -0.00577746, ..., -0.05891485,\r\n          -0.02970275, -0.06618612],\r\n.......\r\n-0.04930668, -0.02927044]]]], dtype=float32)> has shape (3, 3, 64, 64), but the saved weight has shape (64,).\r\n", "comments": ["Will it be possible to share minimal standalone code or Colab link to reproduce the issue in our environment.It will be helpful for localizing the issue faster.Thanks!", "@ravikyram  Hello,\r\n**first, I train my resnet101 network, then save the weights using .h5 format.** \r\n```\r\nmodel.save_weights(\"./save_weights/resNet_{}.h5\".format(epoch))\r\n```\r\n\r\n**second, I just simply load model weights( .h5 format).** \r\n```\r\nfeature = resnet101(num_classes=5, include_top=False)\r\nmodel = tf.keras.Sequential([feature,\r\n                             tf.keras.layers.GlobalAvgPool2D(),\r\n                             tf.keras.layers.Dropout(rate=0.2),\r\n                             tf.keras.layers.Dense(1024),\r\n                             tf.keras.layers.Dropout(rate=0.2),\r\n                             tf.keras.layers.Dense(5)])\r\nmodel.load_weights('./save_weights/resNet_5.h5', by_name=True)\r\n```\r\n**if you try to load model weights( .h5 format) that have bn layers, maybe meet this bug.**\r\nthis problem from  \"\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\"\r\n```\r\n# Set values.\r\n      for i in range(len(weight_values)):\r\n        if K.int_shape(symbolic_weights[i]) != weight_values[i].shape:\r\n          raise ValueError('Layer #' + str(k) +' (named \"' + layer.name +\r\n                           '\"), weight ' + str(symbolic_weights[i]) +\r\n                           ' has shape {}'.format(K.int_shape(\r\n                               symbolic_weights[i])) +\r\n                           ', but the saved weight has shape ' +\r\n                           str(weight_values[i].shape) + '.')\r\n```", "@WZMIAOMIAO \r\n\r\nWill it be possible to provide reproducible code and .h5 file . It helps us in localizing the issue faster. Thanks!", "@ravikyram hello\uff0cI just tested it again. I find that when i load weights if i comment out \u201cfeature.trainable=False\u201d, i can get error from hdf5_format.py . the test code is following:\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef my_model(im_height=64, im_width=64):\r\n    in_im = tf.keras.Input(shape=(im_height, im_width, 3))\r\n    # 64x64x3\r\n    x = tf.keras.layers.Conv2D(\r\n        filters=32, kernel_size=3, strides=2, padding=\"same\", use_bias=False)(in_im)\r\n    x = tf.keras.layers.BatchNormalization()(x)\r\n    x = tf.keras.layers.ReLU()(x)\r\n    # 32x32x32\r\n    x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2)(x)\r\n    # 16x16x32\r\n    x = tf.keras.layers.Conv2D(\r\n        filters=64, kernel_size=3, strides=2, padding=\"same\", use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization()(x)\r\n    x = tf.keras.layers.ReLU()(x)\r\n    # 8x8x64\r\n\r\n    model = tf.keras.Model(inputs=in_im, outputs=x)\r\n\r\n    return model\r\n\r\n\r\n# save weights\r\nfeature = my_model()\r\nfeature.trainable = False\r\nmodel = tf.keras.Sequential([feature,\r\n                             tf.keras.layers.GlobalAvgPool2D(),\r\n                             tf.keras.layers.Dropout(rate=0.2),\r\n                             tf.keras.layers.Dense(1024),\r\n                             tf.keras.layers.Dropout(rate=0.2),\r\n                             tf.keras.layers.Dense(5)])\r\nmodel.save_weights('my_net.h5')\r\n\r\n# load weights\r\nfeature = my_model()\r\n# feature.trainable = False\r\nmodel = tf.keras.Sequential([feature,\r\n                             tf.keras.layers.GlobalAvgPool2D(),\r\n                             tf.keras.layers.Dropout(rate=0.2),\r\n                             tf.keras.layers.Dense(1024),\r\n                             tf.keras.layers.Dropout(rate=0.2),\r\n                             tf.keras.layers.Dense(5)])\r\nmodel.load_weights('my_net.h5')\r\n```  ", "I have tried on colab with TF version 2.0 ,2.1.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/addfb60c246fc9b1b625e1c448dce0da/untitled477.ipynb). Thanks!", "Currently hdf5 weights are saved with a specific ordering that depends on whether layers/variables are trainable. It's difficult to change this behavior as it would break all currently existing checkpoints. \r\n\r\nCan you set trainable to `False` after the weights have been loaded? Alternatively, stick to using the ckpt format. Unfortunately there's not much we can do to fix this for hdf5.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34950\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34950\">No</a>\n"]}, {"number": 34949, "title": "How to use TF_VARIANT in c_api?", "body": "Hi :\r\n      How to use TF_VARIANT in c_api?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 34948, "title": "Added a Usage Example to flip_left_right()", "body": "", "comments": ["@jgulian thank you, it is failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/b658a5bb-872d-4e4b-9a6d-b09a904abff6/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests).\r\n\r\nPlease run the doctest locally as mentioned here in the [contributor guidelines](https://www.tensorflow.org/community/contribute/docs_ref).", "Sorry for making mistakes; this is my first time contributing to tensorflow.", "@jgulian Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 34947, "title": "polyval gives TypeError when run inside tf.function with Tensor coeffs, but not when run eagerly", "body": "This is my first issue, so let me know if it is reported in the wrong place. Thanks! \r\n\r\n**System information**\r\n- Have I written custom code: Example Python code to reproduce provided below\r\n- OS Platform and Distribution: Windows 10 Pro 1903\r\n- TensorFlow installed from binary with pip\r\n- TensorFlow version: 'v2.0.0-rc2-26-g64c3d382ca'; '2.0.0'\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\ntf.math.polyval works correctly when executing eagerly, but when it is called from within a function that has the @tf.function decorator then a TypeError is raised:\r\n\r\n> TypeError: len is not well defined for symbolic Tensors. (eye/diag:0) Please call `x.shape` rather than `len(x)` for shape information.\r\n\r\n**Describe the expected behavior**\r\nThe output of this example should be the same regardless of eager execution or tf.function decoration:\r\n```\r\n[1 1 5]\r\n[[-2 1 10]]\r\n[[8 2 20]]\r\n[[-18 6 30]]\r\n[[24 24 24]]\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable([[-1.0], [0.0], [1.0]])\r\n\r\n@tf.function\r\ndef func():\r\n    with tf.GradientTape(persistent=True) as t:\r\n        t.watch(x)\r\n        coeffs = tf.eye(5)\r\n        pv = tf.math.polyval(coeffs, x)\r\n        y = tf.reduce_sum(pv, axis=1)\r\n        dy_dx = t.gradient(y, x)\r\n        d2y_dx2 = t.gradient(dy_dx, x)\r\n        d3y_dx3 = t.gradient(d2y_dx2, x)\r\n        d4y_dx4 = t.gradient(d3y_dx3, x)\r\n    del t\r\n\r\n    tf.print(y)\r\n    tf.print(tf.transpose(dy_dx))\r\n    tf.print(tf.transpose(d2y_dx2))\r\n    tf.print(tf.transpose(d3y_dx3))\r\n    tf.print(tf.transpose(d4y_dx4))\r\n\r\nfunc()\r\n```\r\n\r\n", "comments": ["For whatever it's worth, if you use polyval inside a custom Keras layer then eager execution of polyval doesn't work either due to an OperatorNotAllowedInGraphError. I can also provide a simple example of this, but I'm not sure whether it merits a new issue.", "I have tried on colab with TF version 2.0,2.1.0-dev20191013,2.1.0-rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/4a1b6e066c3e50b9011e86cc0213475d/untitled455.ipynb). Thanks!", "I've been getting this too using custom_gradients. Would love to know a solution", "`tf.math.polyval` only works with lists of tensors, but it doesn't verify its arguments before starting the work and it errors out internally. The fact that it works in eager mode is incidental.\r\n\r\nSo you'll need to split `coeffs`:\r\n\r\n```\r\n        coeffs = tf.eye(5)\r\n        coeffs = tf.split(coeffs, 5)  # Convert coeffs to a list of tensors.\r\n        pv = tf.math.polyval(coeffs, x)\r\n```\r\n\r\nThe op implementation could be improved in a couple of ways:\r\n * it should ensure coeffs is a list and raise an appropriate error message\r\n * it may be made to work with tensor coeffs, which should be fairly straightforward", "Hello! I would love to work on this. Could you guide me?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34947\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34947\">No</a>\n", "@Joey155 it might be interesting to see if polyval can be made to work with tensor inputs by taking its source code and putting it in a `tf.function`. That said, there are a couple of bugs and `GradientTape` doesn't give high-order gradients for `tf.while_loop`, which would need to be fixed first.", "@mdanatg the issue is closed now. But i will check out your directions all the same."]}, {"number": 34945, "title": "Fix saved_model_cli tensorrt conversion", "body": "The existing saved_model_cli convert tensorrt script fails in 2.X with module\r\nnot found \"tensorflow.contrib\". Updated the script to use the V2 API for\r\nTensorRT to convert a saved_model.\r\n\r\nThe max_batch_size and is_dynamic_op parameters are not valid for the V2 API\r\nso they have been removed.", "comments": ["@vbardiovskyg Can you please take a look on this PR? Thanks!", "Added @aaroey as reviewer, since according to file history, he added it to saved_model_cli (and I have no knowledge of TensorRT).", "Also @sanjoy @pooyadavoodi ", "LGTM.\r\n\r\nIt would be good to also add the API method `build` as an option.", "> It would be good to also add the API method `build` as an option.\r\n\r\nAs the `build` API requires input data to run inference on, how would that input data be handled in the generic command line?", "@wdirons could you help to fix the sanity errors:\r\n```\r\nFAIL: Found 4 non-whitelisted pylint errors:\r\ntensorflow/python/tools/saved_model_cli.py:768: [C0330(bad-continuation), ] Wrong hanging indentation (remove 2 spaces).\r\n\r\ntensorflow/python/tools/saved_model_cli.py:769: [C0330(bad-continuation), ] Wrong hanging indentation (remove 2 spaces).\r\n\r\ntensorflow/python/tools/saved_model_cli.py:770: [C0330(bad-continuation), ] Wrong hanging indentation (remove 2 spaces).\r\n\r\ntensorflow/python/tools/saved_model_cli.py:772: [C0301(line-too-long), ] Line too long (85/80)\r\n\r\n```", "Sorry for the lint errors, I will fix them, rebuild and re-test. I didn't get to it today but I should be able tomorrow. "]}, {"number": 34944, "title": "SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool>]", "body": "Hi,\r\n I am writing Encoder-Decoder architecture with Bahdanau Attention using tf.keras with TensorFlow 2.0. Below is my code This is working with TensorFlow 1.15 but getting the error in 2.0. you can check the code in colab notebook [here](https://colab.research.google.com/drive/12Vq1t9xOtrPmXV2Sj_GSoE9bxH7FgaKs). \r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Dropout, GRU, Embedding\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras import activations\r\nfrom tensorflow.keras.layers import Layer\r\nfrom tensorflow.keras import layers\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import GRU, concatenate, Lambda\r\n\r\nENCODER_SEQ_LEN = 30\r\nDECODER_SEQ_LEN = 20\r\nVOCAB_SIZE = 500\r\nunits = 16\r\n\r\ntf.keras.backend.clear_session()\r\nclass Encoder(Model):\r\n    def __init__(self, vocab_size, embedding_dim, input_length, units):\r\n        super(Encoder, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embedding_dim = embedding_dim\r\n        self.input_length = input_length\r\n        self.units = units\r\n        self.embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=50, input_length=self.input_length,\r\n                           mask_zero=False, name=\"embedding_layer_encoder\")\r\n        self.gru = GRU(self.units, return_state=True, return_sequences=True, name=\"Encoder_GRU\")\r\n    @tf.function\r\n    def call(self, inputs, training=True):\r\n        x_embedd = self.embedding(inputs)\r\n        gru_output, gru_state = self.gru(x_embedd)\r\n        return gru_output, gru_state\r\n    \r\nclass BahdanauAttention(tf.keras.layers.Layer):\r\n    def __init__(self, units):\r\n        super(BahdanauAttention, self).__init__()\r\n        self.W1 = tf.keras.layers.Dense(units)\r\n        self.W2 = tf.keras.layers.Dense(units)\r\n        self.V = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, query, values):\r\n        # hidden shape == (batch_size, hidden size)\r\n        # # hidden_with_time_axis shape == (batch_size, 1, hidden size)\r\n        # # we are doing this to perform addition to calculate the score\r\n        hidden_with_time_axis = tf.expand_dims(query, 1)\r\n        # score shape == (batch_size, max_length, 1)\r\n        # # we get 1 at the last axis because we are applying score to self.V\r\n        # # the shape of the tensor before applying self.V is (batch_size, max_length, units)\r\n        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\r\n        # attention_weights shape == (batch_size, max_length, 1)\r\n        attention_weights = tf.nn.softmax(score, axis=1)\r\n        # context_vector shape after sum == (batch_size, hidden_size)\r\n        context_vector = attention_weights * values\r\n        context_vector = tf.reduce_sum(context_vector, axis=1)\r\n        return context_vector\r\n\r\nclass onestepDecoder(Model):\r\n    def __init__(self, vocab_size, embedding_dim, dec_units, att_units):\r\n        super(onestepDecoder, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embedding_dim = embedding_dim\r\n        self.dec_units = dec_units\r\n        self.att_units = att_units\r\n        self.embedd = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim,\r\n                      input_length=1, mask_zero=False, name=\"Decoder_Embedding_layer\")\r\n        self.att_layer = BahdanauAttention(units=self.att_units) #name='Attention')\r\n        self.dense = Dense(self.vocab_size, activation=\"softmax\", name=\"DenseOut\")\r\n        self.gru = GRU(units=self.dec_units, return_state=True, name=\"DecGRU\")\r\n    @tf.function\r\n    def call(self, input_decoder, input_state, encoder_outputs, training=True):\r\n        x_embedd = self.embedd(input_decoder)\r\n        context_vector = self.att_layer(input_state, encoder_outputs )\r\n        concat = tf.concat([tf.expand_dims(context_vector, 1), x_embedd], axis=-1)\r\n        decoder_output, Decoder_state = self.gru(concat, initial_state=input_state)\r\n        output = self.dense(decoder_output)\r\n        return (output, Decoder_state)\r\nclass Decoder(Model):\r\n    def __init__(self, vocab_size, embedding_dim, dec_units, att_units):\r\n        super(Decoder, self).__init__()\r\n        self.vocab_size = vocab_size\r\n        self.embedding_dim = embedding_dim\r\n        self.dec_units = dec_units\r\n        self.att_units = att_units\r\n        self.stepdec = onestepDecoder(self.vocab_size, self.embedding_dim, self.dec_units, self.att_units)\r\n    @tf.function\r\n    def call(self, input_decoder, input_state, encoder_outputs):\r\n        all_outputs= tf.TensorArray(tf.float32, size=input_decoder.shape[1], name=\"output_arrays\")\r\n        for timestep in range(input_decoder.shape[1]):\r\n            output, input_state = self.stepdec(input_decoder[:,timestep:timestep+1], input_state, encoder_outputs)\r\n            all_outputs = all_outputs.write(timestep, output)\r\n        all_outputs = tf.transpose(all_outputs.stack(), [1, 0, 2])\r\n        return all_outputs\r\n\r\nencoder_input = Input(shape=(ENCODER_SEQ_LEN,), name='encoder_input_final')\r\ndecoder_input = Input(shape=(DECODER_SEQ_LEN,), name=\"Decoder_inout_final\")\r\nencoder = Encoder(vocab_size=VOCAB_SIZE, embedding_dim=50, input_length=ENCODER_SEQ_LEN, units=16)\r\nx_gru_out, x_gru_state = encoder(encoder_input)\r\ndecoder = Decoder(vocab_size=VOCAB_SIZE, embedding_dim=50, dec_units=16, att_units=20)\r\nall_outputs = decoder(decoder_input, x_gru_state, x_gru_out)\r\nencoder_decoder = Model([encoder_input, decoder_input], outputs=all_outputs)\r\nencoder_decoder.compile(optimizer='adam',loss='sparse_categorical_crossentropy')\r\n\r\nx = np.random.randint(0, 499, size=(2000, ENCODER_SEQ_LEN))\r\ny = np.random.randint(0, 499, size=(2000, DECODER_SEQ_LEN))\r\n\r\nencoder_decoder.fit(x=[x,y], y=y, epochs=1,verbose=1,batch_size=32)\r\n```\r\n**Error**:\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     60                                                op_name, inputs, attrs,\r\n---> 61                                                num_outputs)\r\n     62   except core._NotOkStatusException as e:\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: keras_learning_phase:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n_SymbolicException                        Traceback (most recent call last)\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     73       raise core._SymbolicException(\r\n     74           \"Inputs to eager execution function cannot be Keras symbolic \"\r\n---> 75           \"tensors, but found {}\".format(keras_symbolic_tensors))\r\n     76     raise e\r\n     77   # pylint: enable=protected-access\r\n\r\n_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool>]\r\n", "comments": ["Error is self-explainatory. If running under eager mode ,tensorflow op will check if the inputs are of type \"_tensorflow.python.framework.ops.EagerTensor_\" and keras ops are implemented as DAGs. So the inputs to the eagermode will be of \"_tensorflow.python.framework.ops.Tensor_\" and this throws the error\r\n\r\nYou can change the input type to EagerTensor by explicity telling tensorflow to run in eager mode for keras. \r\n\"_tf.config.experimental_run_functions_eagerly(True)_\" \r\n\r\nAdding this statement should solve your issue. Although note that there will be significant performance hits since you are running now in eager mode and recommended only for debugging,profiling etc.", "can we change any code to work without \"tf.config.experimental_run_functions_eagerly(True)\" ? @Athul8raj \r\nand also If i am removing tf.function, i am getting this error. \r\n\r\nTypeError                                 Traceback (most recent call last)\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py in _num_elements(grad)\r\n    615   if isinstance(grad, ops.IndexedSlices):\r\n--> 616     return functools.reduce(operator.mul, grad.values._shape_tuple(), 1)  # pylint: disable=protected-access\r\n    617   raise ValueError(\"`grad` not a Tensor or IndexedSlices.\")\r\n\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\n<ipython-input-7-fa4707952deb> in <module>\r\n----> 1 encoder_decoder.fit(x=[x,y], y=y, epochs=1,verbose=1,batch_size=32)\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in distributed_function(input_iterator)\r\n     71     strategy = distribution_strategy_context.get_strategy()\r\n     72     outputs = strategy.experimental_run_v2(\r\n---> 73         per_replica_function, args=(model, x, y, sample_weights))\r\n     74     # Out of PerReplica outputs reduce or pick values to return.\r\n     75     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    759                                 convert_by_default=False)\r\n--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    761 \r\n    762   def reduce(self, reduce_op, value, axis):\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1785       kwargs = {}\r\n   1786     with self._container_strategy().scope():\r\n-> 1787       return self._call_for_each_replica(fn, args, kwargs)\r\n   1788 \r\n   1789   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2130         self._container_strategy(),\r\n   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2132       return fn(*args, **kwargs)\r\n   2133 \r\n   2134   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n    262       y,\r\n    263       sample_weights=sample_weights,\r\n--> 264       output_loss_metrics=model._output_loss_metrics)\r\n    265 \r\n    266   if reset_metrics:\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n    309           sample_weights=sample_weights,\r\n    310           training=True,\r\n--> 311           output_loss_metrics=output_loss_metrics))\r\n    312   if not isinstance(outs, list):\r\n    313     outs = [outs]\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    266           model._backwards(tape, scaled_total_loss)\r\n    267         else:\r\n--> 268           grads = tape.gradient(scaled_total_loss, trainable_weights)\r\n    269           if isinstance(model.optimizer,\r\n    270                         loss_scale_optimizer.LossScaleOptimizer):\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1012         output_gradients=output_gradients,\r\n   1013         sources_raw=flat_sources_raw,\r\n-> 1014         unconnected_gradients=unconnected_gradients)\r\n   1015 \r\n   1016     if not self._persistent:\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     74       output_gradients,\r\n     75       sources_raw,\r\n---> 76       compat.as_str(unconnected_gradients.value))\r\n\r\nD:\\Softwares\\Anaconda3\\envs\\dlt2\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py in _aggregate_grads(gradients)\r\n    596   assert gradients, \"No gradients to aggregate\"\r\n    597 \r\n--> 598   if len(gradients) == 1:\r\n    599     return gradients[0]\r\n    600   if all(isinstance(g, ops.Tensor) for g in gradients):\r\n\r\nSystemError: <built-in function len> returned a result with an error set\r\n", "For the first part of your question, yes it is possible to run in Eagermode without that statement, but instead of the fit function from keras you have to calculate gradients with the help of a gradient tape which will take care of loss gradient and track all the trainable variables\r\n\r\nFor the second part, I am not sure why you are getting the error, because I am not facing that .\r\nCan you specify the TF version you are running and also the OS Specs.\r\nActually you should include these details in the subject of this issue. That should be the way tensorflow issues should be raised\r\n\r\nThe template shown below: \r\n\r\n\"_System information_\r\n\r\n_Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No_\r\n_OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home_\r\n_Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:_\r\n_TensorFlow installed from (source or binary): binary_\r\n_TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0_\r\n_Python version: 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)]_\r\n_Bazel version (if compiling from source): NA_\r\n_GCC/Compiler version (if compiling from source): NA_\r\n_CUDA/cuDNN version: NA_\r\n_GPU model and memory: NA_\r\n_You can collect some of this information using our environment capture_\r\n_script_\r\n_You can also obtain the TensorFlow version with: 1. TF 1.0: python -c \"import tensorflow as tf_ _print(tf.GIT_VERSION, tf.VERSION)\" 2. TF 2.0: python -c \"import tensorflow as tf_ _print(tf.version.GIT_VERSION, tf.version.VERSION)\"_\r\n\r\n_Describe the current behavior_\r\n_The code snippets works fine on Colab but gives the following error on Windows:_\"", "@Athul8raj Below is my system information. \r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: Python 3.7.3\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA toolkit 10.0.130\r\n- GPU model and memory: GTX 1050 Ti\r\n\r\nDescribe the current behavior\r\nThe above code snippet with/without @tf.function is not working in colab as well. I am getting the same error.  ", "Can you provide a collab gist?.", "below are the two links, please check\r\n[error code without tf.function](https://gist.github.com/UdiBhaskar/d7492b7be034db1480db717ceb6e767e). \r\n[Error code with tf.function](https://gist.github.com/UdiBhaskar/613e671d8785538b1380a7f3909899ed)", "If you are not providing \"_tf.config.experimental_run_functions_eagerly(True)_\". then you have to give the model to gradient tape as shown in here\r\n\r\nhttps://www.tensorflow.org/tutorials/text/nmt_with_attention", "I saw documentation for tf.config.experimental_run_functions_eagerly(True) and it was saying useful while debugging and read the functional API specs and in that it was saying that, if a model built by a functional API is compiled without any error, you can use that model to train and this won't give any error. \r\ni understand if i am writing the code with tf.functions, i have to give eager tensor as input not keras but if am removing the tf.function, it has to work right? it is working fine with 1.15 version and in above comment you told, it is working on your system with 2.0 alpha. is there any bug in auto graph with keras .fit? \r\n\r\ncan you provide any referance for \"If you are not providing \"tf.config.experimental_run_functions_eagerly(True)\". then you have to give the model to gradient tape as shown in here\" ? i am not able to find any documentation which says this. ", "As per TF 2.0 docs,there are two ways to create models, one is through object oriented way and one is using the keras functional api. While there are examples where they have called layer subclass in a keras functional model, there are no instances of models subclasses being called in a keras functional api. This may be the reason you are encountering the issue.\r\n\r\nOne way to go, if you want to stick with the functional api would be to change the model subclasses to layer subclasses and write a single model function. \r\nFor more info please refer : https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class\r\n\r\nPS: please forgive my assertion in the previous reply. I didnt mean you have to, its just another way of doing it", "@Athul8raj I checked with the above cases as well and getting the same error. I am not getting any error if i am using a 1.15 version or below. \r\n\r\ncase-1 : Total model subclassing API  - Got same error. you can check [here](https://gist.github.com/UdiBhaskar/8f96a48dbe3afe9b969b4d46e835a93b)\r\ncase-2 : Custom layers(not model subclassing) and functional API - Got same error. You can check [here](https://gist.github.com/UdiBhaskar/e795757f6b07f70275ab37739d07e411)\r\ncase-3 : Custom layers and model subclassing API - Got same error. You can check [here](https://gist.github.com/UdiBhaskar/000af58031c6b95c046a8365df466ed9)", "You are not getting error in TF 1.15 and below because in tf 2.0 eager execution is enabled by default.\r\n\r\nSo I thought of disabling the eager execution and your code worked perfectly.\r\nthis is the statement to disable eager execution \"_tf.compat.v1.disable_eager_execution()_\"\r\nHere are the docs : https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_eager_execution\r\n\r\nHere is the example code with gradient tape and eager execution : https://www.tensorflow.org/tutorials/text/nmt_with_attention", " I got that if I disable my eager execution, I can run my model without error or if I use gradient tape then also I can run the model without error.  but I will get the error if I use .fit method with my Keras model but it has to work even in TensorFlow 2.0 right. do you think of any bug in .fit method with the autograph in 2.0 with eager execution? ", "I don't believe it is a bug rather TF gives us freedom in choosing each method. While we can mix match the layer subclass with keras functional api, I guess we can't make the model subclass work with the Model api of keras. This is where, in my opinion the distinction between eager execution and keras graph mode comes into conflict giving rise to this \"_SymbolicException_\". \r\nMaking TF aware beforehand what mode it should execute solves it.", "> I got that if I disable my eager execution, I can run my model without error or if I use gradient tape then also I can run the model without error. but I will get the error if I use .fit method with my Keras model but it has to work even in TensorFlow 2.0 right. do you think of any bug in .fit method with the autograph in 2.0 with eager execution?\r\n\r\nhey~ I encountered the same problem. Inspired by [your code](https://gist.github.com/UdiBhaskar/e795757f6b07f70275ab37739d07e411) I run the `.fit` method with the autograph in tf2.0 with eager execution successfully. You can check the [code](https://github.com/Douboo/tf_env_debug/blob/master/custom_layers_and_model_subclassing_API.ipynb)", "Closing this issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34944\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34944\">No</a>\n", "how?", "> Error is self-explainatory. If running under eager mode ,tensorflow op will check if the inputs are of type \"_tensorflow.python.framework.ops.EagerTensor_\" and keras ops are implemented as DAGs. So the inputs to the eagermode will be of \"_tensorflow.python.framework.ops.Tensor_\" and this throws the error\r\n> \r\n> You can change the input type to EagerTensor by explicity telling tensorflow to run in eager mode for keras.\r\n> \"_tf.config.experimental_run_functions_eagerly(True)_\"\r\n> \r\n> Adding this statement should solve your issue. Although note that there will be significant performance hits since you are running now in eager mode and recommended only for debugging,profiling etc.\r\n\r\nsolved my problem, thanks", "In order to evade the performance degradation when setting eager execution to True, you could run the code on colab with \r\n```python\r\n%tensorflow_version 1.x\r\n```"]}, {"number": 34943, "title": "Usage example using IRIS dataset to tf.data.experimental.make_csv_dataset()", "body": "Add usage example using IRIS dataset to tf.data.experimental.make_csv_dataset()\r\n\r\nGCI 2019", "comments": ["@boronhub Few changes. Rest LGTM.", "please check now @nikochiko ", "Wait, so I remove the >>> notation everywhere, and just use \r\n```python\r\n<indented code here without `>>>` >\r\n```", "I mean in the style of the other example code in the file, [for example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/readers.py#L627-L638). Thanks!", "Will the output be preceded by >> ?", "@boronhub Could you please address Ubuntu Sanity errors? Thanks!", "@boronhub Could you please help fix Ubuntu Sanity failures? Thanks!", "How do I fix the sanity errors? All local tests and pylint is passing on my system. ", "```\r\ntensorflow/python/data/experimental/ops/readers.py:350: [C0301(line-too-long), ] Line too long (116/80)\r\n\r\ntensorflow/python/data/experimental/ops/readers.py:351: [C0301(line-too-long), ] Line too long (93/80)\r\n\r\ntensorflow/python/data/experimental/ops/readers.py:363: [C0301(line-too-long), ] Line too long (100/80)\r\n```\r\n\r\nThe lines have to be shortened, split into multiple lines.", "Do I lint using regular pylint, or should I use the TensorFlow config file ?", "You should use TensorFlow's config.", "@mihaimaruseac builds passing", "It should get imported shortly.", "@mihaimaruseac ETA on merge ?", "Sorry for the delay, just out of a 3-days weekend.", "Can you try rebasing on master please?", "I tried, but there are a lot of conflict errors in files I haven't even edited.", "Probably you started from a bad branch.\r\n\r\nYou can fix this by:\r\n\r\n1. Download https://patch-diff.githubusercontent.com/raw/tensorflow/tensorflow/pull/34943.diff (add `.diff` to the end of the PR URL)\r\n2. Close this PR, delete the branch\r\n3. `git checkout master`; `git pull --rebase`; `git checkout -b your_new_branch`\r\n4. `git am /path/to/downloaded/diff`\r\n5. Make a new PR.", "@boronhub Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 34942, "title": "Memory leak with tf.shuffle, doesn't release buffer memory", "body": "**System information**\r\n- OS Platform and Distribution Linux Ubuntu 16.04\r\n- Python: 2.7.17 |Anaconda, Inc.| (default, Oct 21 2019, 19:04:46) [GCC 7.3.0]\r\n- Tensorflow: 1.12.0\r\n- Numpy: 1.16.5\r\n- GPU: GeForce RTX 2080 Ti\r\n- CUDA: 9.2\r\n\r\n**Describe the current behavior**\r\nCPU memory gradually increase after each epoch until the program restarts, i suspect that dataset.shuffle doesn't release the buffer memory. Tested with tf 1.15, same situation.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nclass ASRDataGenerator(object):\r\n    def __init__(self,num):\r\n        self.num = num\r\n    def __call__(self):\r\n        for i in range(self.num):\r\n            for j in range(106):\r\n                yield 'a','b',np.random.randn(100,120)\r\n\r\nclass TFASRDataSet(object):\r\n    def __init__(self,num,batch_size):\r\n\r\n        self.num = num\r\n        self.batch_size = batch_size\r\n        self.asrDataGenerator = ASRDataGenerator(num)\r\n        \r\n    def setDataSetIterator(self):\r\n        \r\n        dataset = tf.data.Dataset.from_generator(self.asrDataGenerator, (tf.string,tf.string,tf.float32))\r\n        dataset = dataset.shuffle(30000)\r\n        dataset = dataset.map(lambda s1,s2,feat: [s1,s2,feat])\r\n        dataset = dataset.batch(self.batch_size, drop_remainder=True)\r\n        self.iterator = dataset.make_initializable_iterator()\r\n        \r\ntest_tfASRDataSet = TFASRDataSet(248,192)\r\ntest_tfASRDataSet.setDataSetIterator()\r\ntest_iter = test_tfASRDataSet.iterator\r\ntest_next = test_iter.get_next()   \r\n\r\nrun_config = tf.ConfigProto()\r\nrun_config.gpu_options.allow_growth = True\r\nrun_config.allow_soft_placement = True\r\n\r\nwith tf.Session(config=run_config) as sess:\r\n\r\n    for i in range(100):\r\n\r\n        sess.run(test_iter.initializer)\r\n        \r\n        while True:\r\n            try:\r\n                loss_list = sess.run([test_next])\r\n                print(len(loss_list[0]))\r\n            except tf.errors.OutOfRangeError:\r\n                print(\"train epoch %d finish\" % (i+1))\r\n                break\r\n```", "comments": ["I could replicate the issue with Tf 1.15.\r\nPlease take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/9f8ff40136953326a476bfadc15f1e46/untitled297.ipynb). Thanks!", "@gadagashwini the user program does not provide evidence of the issue, so I am not sure what do you mean by \"replicating\" the issue.\r\n\r\nWhen I modify the program with tracking of memory use:\r\n\r\n```\r\n...\r\ndef view_used_mem():\r\n  used_mem = psutil.virtual_memory().used\r\n  print(\"used memory: {} Mb\".format(used_mem / 1024 / 1024))\r\n\r\n\r\ndef main(argv):\r\n  del argv\r\n\r\n  test_tfASRDataSet = TFASRDataSet(248, 192)\r\n  test_tfASRDataSet.setDataSetIterator()\r\n  test_iter = test_tfASRDataSet.iterator\r\n  test_next = test_iter.get_next()\r\n\r\n  run_config = tf.ConfigProto()\r\n  run_config.gpu_options.allow_growth = True\r\n  run_config.allow_soft_placement = True\r\n\r\n  with tf.Session(config=run_config) as sess:\r\n\r\n    for i in range(100):\r\n\r\n      sess.run(test_iter.initializer)\r\n\r\n      while True:\r\n        try:\r\n          loss_list = sess.run([test_next])\r\n        except tf.errors.OutOfRangeError:\r\n          print('train epoch %d finish' % (i + 1))\r\n          view_used_mem()\r\n          break\r\n...\r\n```\r\nand run it using internal TF 1.15 build, the memory is not increasing between epochs.\r\n\r\nFurthermore, I have confirmed that the shuffle buffer is properly disposed of between epochs by enabling logging of tf.data iterator constructors / destructors (which can be done by passing the `--vmodule=dataset=2` command-line flag to the program):\r\n\r\n```\r\n...\r\nI0102 13:32:09.063783   34490 dataset.h:887] Iterator::Model constructor\r\nI0102 13:32:09.063821   34490 dataset.h:887] Iterator::Model::MapAndBatch constructor\r\nI0102 13:32:09.063834   34490 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle constructor\r\nI0102 13:32:09.064474   34490 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle destructor\r\nI0102 13:32:09.064498   34490 dataset.h:891] Iterator::Model::MapAndBatch destructor\r\nI0102 13:32:09.064547   34490 dataset.h:891] Iterator::Model destructor\r\nI0102 13:32:09.100376   34570 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap constructor\r\nI0102 13:32:09.100427   34570 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor constructor\r\nI0102 13:32:09.101287   34570 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap[0]::Generator constructor\r\nI0102 13:32:19.100455   34570 shuffle_dataset_op.cc:185] Filling up shuffle buffer (this may take a while): 10188 of 30000\r\nI0102 13:32:29.100570   34570 shuffle_dataset_op.cc:185] Filling up shuffle buffer (this may take a while): 20548 of 30000\r\nI0102 13:32:34.942694   34570 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap[0]::Generator destructor\r\nI0102 13:32:34.944426   34570 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor destructor\r\nI0102 13:32:34.944455   34570 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap constructor\r\nI0102 13:32:34.944461   34570 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap destructor\r\nI0102 13:32:34.944477   34570 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor constructor\r\nI0102 13:32:34.946282   34570 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor destructor\r\nI0102 13:32:34.946300   34570 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap destructor\r\nI0102 13:32:34.946307   34570 shuffle_dataset_op.cc:234] Shuffle buffer filled.\r\ntrain epoch 2 finish\r\nused memory: 17154.9375 Mb\r\nI0102 13:32:36.206103   34490 dataset.h:887] Iterator::Model constructor\r\nI0102 13:32:36.206153   34490 dataset.h:887] Iterator::Model::MapAndBatch constructor\r\nI0102 13:32:36.206179   34490 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle constructor\r\nI0102 13:32:36.206920   34490 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle destructor\r\nI0102 13:32:36.206948   34490 dataset.h:891] Iterator::Model::MapAndBatch destructor\r\nI0102 13:32:36.206960   34490 dataset.h:891] Iterator::Model destructor\r\nI0102 13:32:36.213317   34616 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap constructor\r\nI0102 13:32:36.213364   34616 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor constructor\r\nI0102 13:32:36.214274   34616 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap[0]::Generator constructor\r\nI0102 13:32:46.213390   34616 shuffle_dataset_op.cc:185] Filling up shuffle buffer (this may take a while): 10597 of 30000\r\nI0102 13:32:56.214073   34616 shuffle_dataset_op.cc:185] Filling up shuffle buffer (this may take a while): 20791 of 30000\r\nI0102 13:33:01.736376   34616 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap[0]::Generator destructor\r\nI0102 13:33:01.736460   34616 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor destructor\r\nI0102 13:33:01.736486   34616 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap constructor\r\nI0102 13:33:01.736495   34616 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap destructor\r\nI0102 13:33:01.736521   34616 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor constructor\r\nI0102 13:33:01.736606   34616 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor destructor\r\nI0102 13:33:01.736621   34616 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap destructor\r\nI0102 13:33:01.736632   34616 shuffle_dataset_op.cc:234] Shuffle buffer filled.\r\ntrain epoch 3 finish\r\nused memory: 17118.4609375 Mb\r\nI0102 13:33:02.945749   34490 dataset.h:887] Iterator::Model constructor\r\nI0102 13:33:02.945802   34490 dataset.h:887] Iterator::Model::MapAndBatch constructor\r\nI0102 13:33:02.945827   34490 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle constructor\r\nI0102 13:33:02.946734   34490 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle destructor\r\nI0102 13:33:02.946763   34490 dataset.h:891] Iterator::Model::MapAndBatch destructor\r\nI0102 13:33:02.946774   34490 dataset.h:891] Iterator::Model destructor\r\nI0102 13:33:02.973794   34643 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap constructor\r\nI0102 13:33:02.973845   34643 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor constructor\r\nI0102 13:33:02.974667   34643 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap[0]::Generator constructor\r\nI0102 13:33:12.974602   34643 shuffle_dataset_op.cc:185] Filling up shuffle buffer (this may take a while): 10238 of 30000\r\nI0102 13:33:22.973838   34643 shuffle_dataset_op.cc:185] Filling up shuffle buffer (this may take a while): 20354 of 30000\r\nI0102 13:33:28.369214   34643 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap[0]::Generator destructor\r\nI0102 13:33:28.369300   34643 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor destructor\r\nI0102 13:33:28.369320   34643 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap constructor\r\nI0102 13:33:28.369330   34643 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap destructor\r\nI0102 13:33:28.369357   34643 dataset.h:887] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor constructor\r\nI0102 13:33:28.369455   34643 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap::FromTensor destructor\r\nI0102 13:33:28.369468   34643 dataset.h:891] Iterator::Model::MapAndBatch::Shuffle::FlatMap destructor\r\nI0102 13:33:28.369478   34643 shuffle_dataset_op.cc:234] Shuffle buffer filled.\r\ntrain epoch 4 finish\r\nused memory: 17145.87109375 Mb\r\n```", "The same issue is also with TensorFlow 2.0. After each iteration of the dataset, the buffer get filled again and doesn't release the old buffer. ", "Can confirm the same issue exists with Tensorflow 2.3", "@kamalkraj and @evancasey please create a separate issue with instructions on how to reproduce and evidence that leads you to believe that the shuffle buffer is not released. You can for instance run your workload with `--vmodule=dataset=2` to check whether the shuffle dataset iterator (which owns the buffer) is destructed at the end of each epoch. As per my response from January 2nd, I am not able to reproduce the issue with the instructions posted on this issue.", "@jsimsa could you please provide information about how to use this flag in tf2 or where we can find information about it? Thanks", "@jsimsa we would like to reproduce the issue using `vmodule=dataset=2` but can not figure out how to provide that flag in tensorflow 2.0. Please advise.", "Based on [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/logging.h), I believe that executing your program with the environment variable `TF_CPP_VMODULE` set to `dataset=2` should have the desired effect.", "@jsimsa here's a google collab where our (@evancasey and I) issue can be confirmed by looking at the runtime logs. No 'Shuffle buffer filled message' appears and there are some constructors that seem to not be destroyed. https://colab.research.google.com/drive/1cjRs3FjEXtJBnhELkbsf_ueiS2U3pZ_C?usp=sharing", "@vipermu your colab runs a single epoch of training, which means the shuffle transformation (and its underlying buffer) will be created once at the beginning and then destroyed (so there should certainly not be any leaking of shuffle buffer) ... when I ran your repro using a standalone Python binary with logging enabled, I receive the following log output:\r\n\r\n```\r\nI0903 16:01:32.081323 1038857 dataset.cc:495] Iterator::Model constructor\r\nI0903 16:01:32.081376 1038857 dataset.cc:495] Iterator::Model::Prefetch constructor\r\nI0903 16:01:32.081398 1038857 dataset.cc:495] Iterator::Model::Prefetch::BatchV2 constructor\r\nI0903 16:01:32.081406 1038857 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle constructor\r\nI0903 16:01:32.100963 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap constructor\r\nI0903 16:01:32.100995 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 constructor\r\nI0903 16:01:32.101010 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle constructor\r\nI0903 16:01:32.301694 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 constructor\r\nI0903 16:01:32.301746 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice constructor\r\nI0903 16:01:32.402904 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip constructor\r\nI0903 16:01:32.402943 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap constructor\r\nI0903 16:01:32.402955 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap::TensorSlice constructor\r\nI0903 16:01:32.406277 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[1]::TensorSlice constructor\r\nI0903 16:01:32.406596 1039316 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap[0]::TFRecord constructor\r\nI0903 16:01:32.419646 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap[0]::TFRecord destructor\r\nI0903 16:01:32.419682 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap::TensorSlice destructor\r\nI0903 16:01:32.419698 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[1]::TensorSlice destructor\r\nI0903 16:01:32.419709 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap destructor\r\nI0903 16:01:32.419722 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip destructor\r\nI0903 16:01:32.422107 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 constructor\r\nI0903 16:01:32.422194 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice destructor\r\nI0903 16:01:32.422208 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 destructor\r\nI0903 16:01:32.422235 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice constructor\r\nI0903 16:01:32.422360 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice destructor\r\nI0903 16:01:32.422374 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 destructor\r\nI0903 16:01:32.623497 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[0]::TensorSlice constructor\r\nI0903 16:01:32.915468 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[0]::TensorSlice destructor\r\nI0903 16:01:32.956434 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[1]::TensorSlice constructor\r\nI0903 16:01:33.213744 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[1]::TensorSlice destructor\r\nI0903 16:01:33.216261 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle destructor\r\nI0903 16:01:33.216274 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 destructor\r\nI0903 16:01:33.216282 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap constructor\r\nI0903 16:01:33.216284 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap destructor\r\nI0903 16:01:33.216294 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 constructor\r\nI0903 16:01:33.216301 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle constructor\r\nI0903 16:01:33.384797 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle destructor\r\nI0903 16:01:33.384828 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 destructor\r\nI0903 16:01:33.384833 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap destructor\r\nI0903 16:01:33.879366 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle destructor\r\nI0903 16:01:33.882599 1038857 dataset.cc:499] Iterator::Model::Prefetch::BatchV2 destructor\r\nI0903 16:01:33.882624 1038857 dataset.cc:499] Iterator::Model::Prefetch destructor\r\nI0903 16:01:33.882669 1038857 dataset.cc:499] Iterator::Model destructor\r\n```\r\n\r\nwhich indicates that every `shuffle` constructor is paired with destructor.", "Hi @jsimsa, the issue happens over multiple epochs / passes over the dataset.\r\nBut also there are multiple shuffles happening here (one over the record\r\nnames, and one over the flattened batches). Looking at the logs, I count 3\r\nshuffle constructors and only 2 shuffle destructors. I'm not sure why there\r\nare 3 shuffle constructors in the first place, but it appears that one of\r\nthem is not being destructed.\r\n\r\nOn Thu, Sep 3, 2020 at 7:05 PM Jiri Simsa <notifications@github.com> wrote:\r\n\r\n> @vipermu <https://github.com/vipermu> your colab runs a single epoch of\r\n> training, which means the shuffle transformation (and its underlying\r\n> buffer) will be created once at the beginning and then destroyed (so there\r\n> should certainly not be any leaking of shuffle buffer) ... when I ran your\r\n> repro using a standalone Python binary with logging enabled, I receive the\r\n> following log output:\r\n>\r\n> I0903 16:01:32.081323 1038857 dataset.cc:495] Iterator::Model constructor\r\n> I0903 16:01:32.081376 1038857 dataset.cc:495] Iterator::Model::Prefetch constructor\r\n> I0903 16:01:32.081398 1038857 dataset.cc:495] Iterator::Model::Prefetch::BatchV2 constructor\r\n> I0903 16:01:32.081406 1038857 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle constructor\r\n> I0903 16:01:32.100963 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap constructor\r\n> I0903 16:01:32.100995 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 constructor\r\n> I0903 16:01:32.101010 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle constructor\r\n> I0903 16:01:32.301694 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 constructor\r\n> I0903 16:01:32.301746 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice constructor\r\n> I0903 16:01:32.402904 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip constructor\r\n> I0903 16:01:32.402943 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap constructor\r\n> I0903 16:01:32.402955 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap::TensorSlice constructor\r\n> I0903 16:01:32.406277 1039313 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[1]::TensorSlice constructor\r\n> I0903 16:01:32.406596 1039316 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap[0]::TFRecord constructor\r\n> I0903 16:01:32.419646 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap[0]::TFRecord destructor\r\n> I0903 16:01:32.419682 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap::TensorSlice destructor\r\n> I0903 16:01:32.419698 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[1]::TensorSlice destructor\r\n> I0903 16:01:32.419709 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip[0]::FlatMap destructor\r\n> I0903 16:01:32.419722 1039316 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4[0]::Zip destructor\r\n> I0903 16:01:32.422107 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 constructor\r\n> I0903 16:01:32.422194 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice destructor\r\n> I0903 16:01:32.422208 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 destructor\r\n> I0903 16:01:32.422235 1039272 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice constructor\r\n> I0903 16:01:32.422360 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4::TensorSlice destructor\r\n> I0903 16:01:32.422374 1039272 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle::ParallelInterleaveV4 destructor\r\n> I0903 16:01:32.623497 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[0]::TensorSlice constructor\r\n> I0903 16:01:32.915468 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[0]::TensorSlice destructor\r\n> I0903 16:01:32.956434 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[1]::TensorSlice constructor\r\n> I0903 16:01:33.213744 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap[1]::TensorSlice destructor\r\n> I0903 16:01:33.216261 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle destructor\r\n> I0903 16:01:33.216274 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 destructor\r\n> I0903 16:01:33.216282 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap constructor\r\n> I0903 16:01:33.216284 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap destructor\r\n> I0903 16:01:33.216294 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 constructor\r\n> I0903 16:01:33.216301 1039269 dataset.cc:495] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle constructor\r\n> I0903 16:01:33.384797 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2::Shuffle destructor\r\n> I0903 16:01:33.384828 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap::ParallelMapV2 destructor\r\n> I0903 16:01:33.384833 1039269 dataset.cc:499] Iterator::Model::Prefetch::BatchV2::Shuffle::FlatMap destructor\r\n>\r\n> which indicates that every shuffle constructor is paired with destructor.\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/34942#issuecomment-686807529>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAOP756RQ6HC2OVDGLCBBZLSEAOMLANCNFSM4JX5N3SA>\r\n> .\r\n>\r\n\r\n\r\n-- \r\nCadmium\r\ncadmium.app | evan@cadmium.app\r\n+1 (303) 808-2955\r\n", "Oops, my mistake there I just noticed the third shuffle destructor. Disregard that last comment. Anyway, can you try running this for more than one epoch? I have a feeling that the issue only happens some of the time, but it will always happen at least once over a few epochs", "There are multiple shuffles per epoch because your input pipeline contains multiple shuffles -- one before `flat_map` and one after `flat_map`. The one before `flat_map` will result in a shuffle buffer being created and destroyed for each input element of `flat_map` (as opposed to once per epoch).\r\n\r\nI ran your example for 100 epochs using my standalone binary repro and do not see memory increase. In other words, I cannot reproduce your issue.\r\n\r\nHave you confirmed that the issue goes away when you remove the shuffle buffer? If so, can you extend your colab to run the epoch within a loop and print the amount of memory allocated at the end of each loop?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "I have recently investigated the memory growth observed for OSS version of TensorFlow when shuffle is used. The conclusion of my investigation is that the memory growth is because of poor performance of the memory allocator (TensorFlow OSS uses system malloc by default). In my experiments, switching to use TCMalloc (details below) resulted in constant memory usage (and program speedup).\r\n\r\nFor the evaluation, I used the following simple input pipeline:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport psutil\r\n\r\ndataset = tf.Dataset.range(int(1e7))\r\niterator = dataset.shuffle(int(1e7)).batch(int(1e6))\r\n\r\nfor _ in iterator:\r\n  used_mem = psutil.virtual_memory().used\r\n  print(\"used memory: {} Mb\".format(used_mem / 1024 / 1024))\r\n```\r\n\r\nWhen executed on workstation, it produces the following output:\r\n\r\n```\r\n$ python example.py\r\n\r\nused memory: 19853.52734375 Mb\r\nused memory: 19905.6484375 Mb\r\nused memory: 19958.109375 Mb\r\nused memory: 20014.796875 Mb\r\nused memory: 20064.8359375 Mb\r\nused memory: 20061.375 Mb\r\nused memory: 20117.23828125 Mb\r\nused memory: 20172.8515625 Mb\r\nused memory: 20228.18359375 Mb\r\nused memory: 20278.62890625 Mb\r\n```\r\n\r\nI then installed tcmalloc using `sudo apt-get install libtcmalloc-minimal4` and used it for the same program, as follows:\r\n\r\n```\r\n$ LD_PRELOAD=/path/to/libtcmalloc_minimal.so.4 python example.py\r\n\r\nused memory: 19291.0859375 Mb\r\nused memory: 19307.90234375 Mb\r\nused memory: 19315.859375 Mb\r\nused memory: 19315.859375 Mb\r\nused memory: 19315.875 Mb\r\nused memory: 19317.8671875 Mb\r\nused memory: 19311.14453125 Mb\r\nused memory: 19317.3515625 Mb\r\nused memory: 19317.34765625 Mb\r\nused memory: 19316.96484375 Mb\r\n``` \r\n\r\nNot only the gradual memory growth disappeared, but the program also ran 2x faster.", "did TF 2.5 release solved this issue for anyone ? ", "@azzeddineCH\r\nI am on 2.7 and still have the problem."]}, {"number": 34941, "title": "Add documentation for undocumented exception", "body": "The undocumented exception was:\r\nNotImplementedError\r\n\r\nThe exception is raised when conversion in\r\nthe function \u00b4convert_entity_to_ast\u00b4 is not supported", "comments": ["> This looks great, thank you for the fix! While reviewing the main code body, it strikes me that it's raising two different types of error for the same pattern (the input has some characteristic that is not yet supported). So I wonder if the should raise the same kind of error in both cases.\r\n\r\nYes I agree, I can fix it as well. Should I do it in this PR or should I open up a new one?\r\n\r\nI also assume here that `NotImplementedError` > `ValueError` ?", "Either way sounds good. It takes a day or two for PRs to be pulled in, so it might be quicker to do it in the same PR.\r\n\r\nI Think `NotImplementedError` is fine here, even though it might take a while until an implementation will actually be added."]}, {"number": 34940, "title": "Autograph issues with TF documented examples", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)** No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux 5.4.2-arch1-1 x86_64\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 2.0.0\r\n- **Python version**: 3.8.0\r\n- **Bazel version (if compiling from source)**: 1.1.0\r\n- **GCC/Compiler version (if compiling from source)**: 9.2.0\r\n- **CUDA/cuDNN version**: 10.1.168-4 / 7.6.5.32-1\r\n- **GPU model and memory**: Nvidia GTX 1050 Ti, 4096 Mb\r\n- **Exact command to reproduce**: \r\nimport tensorflow as tf\r\n\r\ntf.autograph.set_verbosity(10, alsologtostdout=True)\r\n\r\n\r\n@tf.function\r\ndef square_if_positive(x):\r\n    if x > 0:\r\n        x = x * x\r\n    else:\r\n        x = 0\r\n    return x\r\n\r\n\r\nprint('square_if_positive(2) = {}'.format(square_if_positive(tf.constant(2))))\r\nprint('square_if_positive(-2) = {}'.format(square_if_positive(tf.constant(-2))))\r\n\r\n### Describe the problem\r\n\"OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\" pops up anywhere I try to use a conditional, not just this example.\r\n\r\n### Source code / logs\r\n2019-12-08 10:56:40.479540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-08 10:56:41.527181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-08 10:56:41.543972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:41.544502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.455GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2019-12-08 10:56:41.544518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-08 10:56:41.545902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-08 10:56:41.547153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-08 10:56:41.547367: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-08 10:56:41.548737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-08 10:56:41.549551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-08 10:56:41.552590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-08 10:56:41.552690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:41.553243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:41.553652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-08 10:56:41.554122: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-12-08 10:56:41.573643: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 3101990000 Hz\r\n2019-12-08 10:56:41.573908: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561688dfa0c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-08 10:56:41.573927: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-08 10:56:41.574088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:41.574746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.455GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2019-12-08 10:56:41.574774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-08 10:56:41.574795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-08 10:56:41.574808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-08 10:56:41.574820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-08 10:56:41.574832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-08 10:56:41.574844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-08 10:56:41.574857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-08 10:56:41.574913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:41.575579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:41.576106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-08 10:56:41.576137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-08 10:56:42.040932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-08 10:56:42.040956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2019-12-08 10:56:42.040967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2019-12-08 10:56:42.041100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:42.041410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:42.041696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-08 10:56:42.041971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3386 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-12-08 10:56:42.043375: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56169a227030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-08 10:56:42.043385: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\nConverted call: <function square_if_positive at 0x7f4849b981f0>\r\n    args: (<tf.Tensor 'x:0' shape=() dtype=int32>,)\r\n    kwargs: {}\r\n\r\nEntity <function square_if_positive at 0x7f4849b981f0> is not cached for subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f4810de6d00>, frozenset())\r\nConverting <function square_if_positive at 0x7f4849b981f0>\r\nSource code of <function square_if_positive at 0x7f4849b981f0>:\r\n\r\n@tf.function\r\ndef square_if_positive(x):\r\n    if x > 0:\r\n        x = x * x\r\n    else:\r\n        x = 0\r\n    return x\r\n\r\n\r\nError transforming entity <function square_if_positive at 0x7f4849b981f0>\r\nWARNING: AutoGraph could not transform <function square_if_positive at 0x7f4849b981f0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for keyword: 1, expecting 2\r\nCaught error in <function square_if_positive at 0x7f4849b981f0> (converted=False)\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 539, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 358, in convert\r\n    converted_entity_info = _convert_with_cache(entity, program_ctx,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 273, in _convert_with_cache\r\n    nodes, converted_name, entity_info = convert_entity_to_ast(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 506, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 705, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 735, in node_to_graph\r\n    node = converter.apply_(node, context, function_scopes)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 399, in apply_\r\n    node = converter_module.transform(node, context)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 132, in transform\r\n    return FunctionBodyTransformer(ctx).visit(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 345, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 114, in visit_FunctionDef\r\n    wrapped_body = templates.replace(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 270, in replace\r\n    node = ReplaceTransformer(replacements).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 436, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 445, in generic_visit\r\n    new_node = self.visit(old_value)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 436, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 201, in visit_Name\r\n    new_nodes = self._prepare_replacement(node, node.id)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 140, in _prepare_replacement\r\n    new_nodes = ast_util.copy_clean(repl, preserve_annos=self.preserved_annos)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 76, in copy_clean\r\n    return CleanCopier(preserve_annos).copy(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 54, in copy\r\n    new_fields[f] = self.copy(getattr(node, f))\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 41, in copy\r\n    return [self.copy(n) for n in node]\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 41, in <listcomp>\r\n    return [self.copy(n) for n in node]\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 55, in copy\r\n    new_node = type(node)(**new_fields)\r\n  File \"/usr/lib/python3.8/site-packages/gast/gast.py\", line 10, in create_node\r\n    assert nbparam in (0, len(Fields)), \\\r\nAssertionError: Bad argument number for keyword: 1, expecting 2\r\nWARNING:tensorflow:AutoGraph could not transform <function square_if_positive at 0x7f4849b981f0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for keyword: 1, expecting 2\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 539, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 358, in convert\r\n    converted_entity_info = _convert_with_cache(entity, program_ctx,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 273, in _convert_with_cache\r\n    nodes, converted_name, entity_info = convert_entity_to_ast(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 506, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 705, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 735, in node_to_graph\r\n    node = converter.apply_(node, context, function_scopes)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 399, in apply_\r\n    node = converter_module.transform(node, context)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 132, in transform\r\n    return FunctionBodyTransformer(ctx).visit(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 345, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 114, in visit_FunctionDef\r\n    wrapped_body = templates.replace(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 270, in replace\r\n    node = ReplaceTransformer(replacements).visit(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 436, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 445, in generic_visit\r\n    new_node = self.visit(old_value)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/ast.py\", line 436, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/usr/lib/python3.8/ast.py\", line 360, in visit\r\n    return visitor(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 201, in visit_Name\r\n    new_nodes = self._prepare_replacement(node, node.id)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 140, in _prepare_replacement\r\n    new_nodes = ast_util.copy_clean(repl, preserve_annos=self.preserved_annos)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 76, in copy_clean\r\n    return CleanCopier(preserve_annos).copy(node)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 54, in copy\r\n    new_fields[f] = self.copy(getattr(node, f))\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 41, in copy\r\n    return [self.copy(n) for n in node]\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 41, in <listcomp>\r\n    return [self.copy(n) for n in node]\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/ast_util.py\", line 55, in copy\r\n    new_node = type(node)(**new_fields)\r\n  File \"/usr/lib/python3.8/site-packages/gast/gast.py\", line 10, in create_node\r\n    assert nbparam in (0, len(Fields)), \\\r\nAssertionError: Bad argument number for keyword: 1, expecting 2\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 344, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/home/michal/Projects/RLtat/the-mayor/l2tf.py\", line 8, in square_if_positive\r\n    if x > 0:\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py\", line 760, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py\", line 525, in _disallow_bool_casting\r\n    self._disallow_when_autograph_enabled(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py\", line 511, in _disallow_when_autograph_enabled\r\n    raise errors.OperatorNotAllowedInGraphError(\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\nTraceback (most recent call last):\r\n  File \"/home/michal/Projects/RLtat/the-mayor/l2tf.py\", line 15, in <module>\r\n    print('square_if_positive(2) = {}'.format(square_if_positive(tf.constant(2))))\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py\", line 496, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py\", line 2375, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py\", line 2706, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py\", line 2586, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in converted code:\r\n\r\n    /home/michal/Projects/RLtat/the-mayor/l2tf.py:8 square_if_positive\r\n        if x > 0:\r\n    /usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py:760 __bool__\r\n        self._disallow_bool_casting()\r\n    /usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py:525 _disallow_bool_casting\r\n        self._disallow_when_autograph_enabled(\r\n    /usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py:511 _disallow_when_autograph_enabled\r\n        raise errors.OperatorNotAllowedInGraphError(\r\n\r\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n", "comments": ["I have tried on colab with TF version 2.0  and i am not seeing any issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/ca061375db9e18e0334d60db7ae204ba/untitled442.ipynb) Thanks!", "I think this might be an issue with Python 3.8, as the Colab runtime uses Python 3.6.9.", "I've downgraded to python 3.7.4, downloaded TF binaries and the code is working properly. Does TF 2.0 simply not support python 3.8?", "Downgrading to python 3.7.4 and building from source with the same config as above works as well. The issue will most likely be with python 3.8.0.\r\nUsed ./configure:\r\n\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3.7/site-packages\r\n\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:  \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/opt/cuda/extras/CUPTI/lib64'\r\n        '/opt/cuda/lib64'\r\n        '/opt/cuda/nvvm/lib64'\r\n        '/usr'\r\n        '/usr/lib'\r\n        '/usr/lib/libfakeroot'\r\n        '/usr/lib32'\r\nAsking for detailed CUDA configuration...\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.1]: \r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n\r\nPlease specify the TensorRT version you want to use. [Leave empty to default to TensorRT 6]: \r\n\r\nPlease specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: 2.5.6\r\n\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /opt/cuda/lib64,/opt/cuda/bin,/opt/cuda/include,/usr/include,/usr/lib,/usr/bin,/opt/tensorrt/include,/opt/tensorrt/bin,/opt/tensorrt/lib,/opt/cuda\r\n\r\nFound CUDA 10.1 in:\r\n    /opt/cuda/lib64\r\n    /opt/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/lib\r\n    /usr/include\r\nFound TensorRT 6 in:\r\n    /opt/tensorrt/lib\r\n    /opt/tensorrt/include\r\nFound NCCL 2 in:\r\n    /usr/lib\r\n    /usr/include\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc-8]: \r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: --config=opt\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished", "We can expect python 3.8 support with TF 2.2 \r\nSee https://github.com/tensorflow/tensorflow/issues/33374#issuecomment-562663993", "There is a compatibility issue with 3.8 in autograph itself that will be resolved in 2.2 as well: #34433 ", "Great, thanks for clarifying.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34940\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34940\">No</a>\n"]}, {"number": 34939, "title": "Title", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@1Warcraft ,\r\nHi,can you please provide information for all the fields present in the standard template ?Thanks!", "Looks like spam account"]}, {"number": 34938, "title": "Error when adding tensorflow op with cmake using c ++ 17", "body": "Error when adding tensorflow op with cmake using c ++ 17\uff0chow do I set tensorflow to support c ++ 17?\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/user/Desktop/sum_each_op/lib/libsum_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefESt17basic_string_viewIcSt11char_traitsIcEESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteISA_EE\r\n", "comments": ["@lijinjiang001 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nBazel is the only supported way to build TensorFlow, sorry. Since contrib is being removed as a part of TF 2.0, the cmake files will go away at the same time. Please try out building from source with [Bazel](https://www.tensorflow.org/install/source_windows#install_bazel). Thanks!", "Thanks for your reply!\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No.\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: 1.13.2\r\nPython version: 3.6\r\nInstalled using virtualenv? pip? conda?: conda\r\nBazel version (if compiling from source): bazel 0.19.2\r\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 8.3.0-6ubuntu1~18.04.1) 8.3.0 \r\nCUDA/cuDNN version: no\r\nGPU model and memory: no\r\n\r\nWhen I implemented tensorflow op, the calling program used the characteristics of c ++ 17. During the process of compiling and generating the dynamic library, the above error occurred.\r\n", "Apologies for the delay in getting back. Is this still an issue?\r\nPlease refer similar issue https://github.com/tensorflow/tensorflow/issues/31568", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 34937, "title": "ZeroDivisionError: integer division or modulo by zero", "body": "I am using Ubuntu 17.10 and python 2.7 \r\nTrained the neural network and now trying to predict the unknown data using predict.py but getting an error as stated below: \r\nTraceback (most recent call last):\r\nFile \"predict.py\", line 68, in <module>\r\n    main(dataset_npz_filepath, result_csv_filepath)\r\n  File \"predict.py\", line 36, in main\r\n    for output, pfd_filepath in zip(output_generator, pfd_filepaths):\r\n  File \"/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 617, in predict\r\n    input_fn, ModeKeys.PREDICT)\r\n  File \"/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 993, in _get_features_from_input_fn\r\n    result = self._call_input_fn(input_fn, mode)\r\n  File \"/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1113, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/numpy_io.py\", line 198, in input_fn\r\n    num_epochs=num_epochs)\r\n  File \"/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 484, in _enqueue_data\r\n    num_epochs=num_epochs))\r\n  File \"/home/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py\", line 220, in __init__\r\n    self._epoch_end = (self._trav - 1) % self._max\r\nZeroDivisionError: integer division or modulo by zero\r\n", "comments": ["@VSGD ,\r\nCan you share a standalone code to reproduce the error reported here?\r\nAlso please refer to these links [1](https://github.com/tensorflow/tensorflow/issues/2072) and [2](https://github.com/google/emoji-scavenger-hunt/issues/28)and let us know if it helped.Thanks!", "Hi,\nI am working on classification problem where the neural network is trained\nwith 225 images and prediction is to be carried out for 23 images.\ncode for predict.py is attached herewith, predict(original).py is one and\nthe same file.\n\n\nOn Mon, Dec 9, 2019 at 11:43 AM oanush <notifications@github.com> wrote:\n\n> @VSGD <https://github.com/VSGD> ,\n> Can you share a standalone code to reproduce the error reported here?\n> Also please refer to these links 1\n> <https://github.com/tensorflow/tensorflow/issues/2072> and 2\n> <https://github.com/google/emoji-scavenger-hunt/issues/28>and let us know\n> if it helped.Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34937?email_source=notifications&email_token=AGHAP6HCO3RPK6EHZESGDGLQXXOX7A5CNFSM4JXYFYH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGH6Z7A#issuecomment-563080444>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGHAP6G2MCQCLRXUU4S2S2TQXXOX7ANCNFSM4JXYFYHQ>\n> .\n>\n", "@VSGD ,\r\nHi, I don't see the code file. Can you please check?Thanks!", "Hi,\nsharing code in .txt file. PFA\n\nThanks\n\nOn Thu, Dec 12, 2019 at 2:08 PM oanush <notifications@github.com> wrote:\n\n> @VSGD <https://github.com/VSGD> ,\n> Hi, I don't see the code file. Can you please check?Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34937?email_source=notifications&email_token=AGHAP6ECUUBZ2QEX5BAGOK3QYHZ7ZA5CNFSM4JXYFYH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGV4OWI#issuecomment-564905817>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGHAP6E7274OZSIJIZIULELQYHZ7ZANCNFSM4JXYFYHQ>\n> .\n>\n\nfrom __future__ import print_function\n\nimport sys\nimport pathlib2\nimport csv\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom cnn import cnn_model_fn\nfrom config import CLASS_ID_TO_LABEL\nfrom local_settings import MODEL_DIR\n\n\ndef main(dataset_npz_filepath, result_csv_filepath):\n\n    data = np.load(dataset_npz_filepath)\n    pfd_filepaths = data['pfd_filepaths']\n\n    classifier = tf.estimator.Estimator(\n        model_fn=cnn_model_fn,\n        params={'class_weights': None, 'return_all_layers': False},\n        model_dir=MODEL_DIR)\n\n    input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={k: v for k, v in data.items() if k != 'labels'},\n        num_epochs=2,\n        shuffle=False)\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    output_generator = classifier.predict(input_fn=input_fn)\n\n    with open(str(result_csv_filepath), 'wb') as result_csv_file:\n        csv_writer = csv.writer(result_csv_file)\n        for output, pfd_filepath in zip(output_generator, pfd_filepaths):\n            predicted_class_id = output['class']\n            predicted_class_label = CLASS_ID_TO_LABEL[predicted_class_id]\n            prediction_confidence = output['probabilities'][predicted_class_id]\n            csv_writer.writerow([\n                str(pfd_filepath),\n                predicted_class_label,\n                prediction_confidence])\n\n\nif __name__ == '__main__':\n\n    if len(sys.argv) != 3:\n        print('Invalid invocation.\\nUsage: {} {} {}'.format(\n            sys.argv[0],\n            '/path/to/dataset_file.npz',\n            '/path/to/prediction_result.csv'))\n        exit(1)\n\n    dataset_npz_filepath = pathlib2.Path(sys.argv[1]).absolute()\n    if not dataset_npz_filepath.exists() or dataset_npz_filepath.is_dir():\n        print('Bad path: \"{}\"'.format(dataset_npz_filepath))\n        exit(1)\n\n    result_csv_filepath = pathlib2.Path(sys.argv[2]).absolute()\n    if result_csv_filepath.exists() and result_csv_filepath.is_dir():\n        print('Bad path: \"{}\"'.format(result_csv_filepath))\n        exit(1)\n\n    with open(str(result_csv_filepath), 'w') as f:\n        f.write('Just testing writability...')\n\n    main(dataset_npz_filepath, result_csv_filepath)\n", "@VSGD ,\r\nWhen tried running the given code  `ModuleNotFoundError: No module named 'cnn' `   error was faced , kindly refer the [gist of colab](https://colab.sandbox.google.com/gist/oanush/754d9162b82f48c7b44effe4e3b22493/34937.ipynb) for reference.Thanks!", "hi,\n\nThis is the entire code that I am trying to run.\nIt also needs PRESTO software.\nInstructions are given in the manual. Plz try again to reproduce the error.\n\nhttps://drive.google.com/file/d/1Wsdxsj6XnzpnvtXhYsCIMqjevCQJcT4m/view?usp=drivesdk\n\n\nThanks and Regards,\nVaidehi\n\nOn Thu, 19 Dec 2019, 11:33 oanush, <notifications@github.com> wrote:\n\n> @VSGD <https://github.com/VSGD> ,\n> When tried running the given code ModuleNotFoundError: No module named\n> 'cnn' error was faced , kindly refer the gist of colab\n> <https://colab.sandbox.google.com/gist/oanush/754d9162b82f48c7b44effe4e3b22493/34937.ipynb>\n> for reference.Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34937?email_source=notifications&email_token=AGHAP6CRWF4KWTYLGTM7IA3QZMFDPA5CNFSM4JXYFYH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHIQXBY#issuecomment-567348103>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGHAP6BQ2IWPHNZM5MQTVILQZMFDPANCNFSM4JXYFYHQ>\n> .\n>\n", "Please fill in template, use markdown for code blocks, provide a minimal reproducer in the issue text itself", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34936, "title": "Train time per sample grows with dataset tensor size, even though it is always cropped to the same size", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen using `dataset.from_tensor_slices` on a tensor with shape (1, 10000, 1000) then `.map(tf.image.random_crop((20, 20)))`, training is 100 times faster par one iteration than when using  `dataset.from_tensor_slices` on a tensor with shape (1, 100000, 10000)\r\n\r\n**Describe the expected behavior**\r\nI expect that as long as the tensor can be kept in RAM, its size would not affect performance (putting aside tensors small enough to be kept in CPU cache)\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis is the simplest form of data and model that reproduces this.. not very interesting, but I really can't see any reason why this would be so much slower when the ones and zeros are bigger.\r\n\r\n```python\r\nones = tf.ones((1, 10000, 1000))\r\nzeros = tf.zeros((1, 10000, 1000))\r\ndata = tf.data.Dataset.from_tensor_slices(\r\n    (\r\n        ones,\r\n        tf.ones((1, 1))\r\n    )\r\n).concatenate(\r\n    tf.data.Dataset.from_tensor_slices(\r\n        (\r\n            zeros,\r\n            tf.zeros((1, 1))\r\n        )\r\n    )\r\n).map(lambda x, y: (tf.image.random_crop(x, (20, 20)), y)).repeat().batch(10)\r\n\r\nx = tf.keras.layers.Input((20, 20))\r\ny = tf.keras.layers.Dense(1, tf.keras.activations.sigmoid)(x)\r\nmodel = tf.keras.models.Model(inputs=[x], outputs=[y])\r\nmodel.compile(loss='mean_squared_error', optimizer='ADAM')\r\nmodel.fit(x=data, epochs=100, steps_per_epoch=100, validation_data=validation, validation_steps=3)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Tried running the code in colab, did not see any performance issue. kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/93f113270cda46e525b36721366afa79/34936.ipynb) of colab.Thanks!", "@oanush Did you try changing the size of ones and zeros? It drastically affects the training time..\r\n\r\nI have added another cell to your gist, the only difference being, zeros and one in the first cell have 100 times more values. Since we randomly crop them before processing them in our network, the original size should not affect performance. But try comparing the training performance of these two cells, and you'll find that one take 100 times more time to train.\r\n\r\nCheck this modified [gist](https://colab.research.google.com/gist/feature-engineer/e5234588ecb1abb02ec05cdecfd18995/34936.ipynb)", "The problem seems to stem from dataset's map method, not from the random_crop... I've added a timeit on the random_crop, and changing the input size of that function has no repercussions, as expected.\r\n\r\n(As an aside, removing the size check makes random_crop about 30% faster, but that's beside the point)\r\n\r\nI have also added a cell where this inefficiency is demonstrated by just mapping to a lambda that just slices the first (20,20) elements of the first tensor, and I've also checked that it happens when just creating a new tensor of shape (20, 20) without even doing anything with the big tensor.\r\n\r\nSo it would seem that the map method is the inefficient link here. ", "I have opened a new issue, since this one's title, as well as example code distracts from the root cause:  see [issue #35135](https://github.com/tensorflow/tensorflow/issues/35135)", "The issue was due to the use of `from_tensor_slices`, which apparently means the tensors passed as arguments to `from_tensor_slices` are being copied on each iteration over the dataset. \r\n\r\nReplacing `from_tensor_slices` with `from_tensors` fixed the performance issue."]}, {"number": 34935, "title": "Tutorial for Data Augmentation using tf.image", "body": "Hi, my name is Rachin Kalakheti and i am a participant of Google Code-in 2019. I felt overwhelmed to know Tensorflow is also one of the organization for this year. So, there was a task to create a notebook tutorial on Data Augmentation using tf.image. I see that currently there is no tutorial regarding the same topic. So, I would like to contribute to the community by adding my tutorial to the  Tensorflow repo. Therefore I am seeking guidance as to discuss this further.\r\nLink to my notebook tutorial: https://colab.research.google.com/drive/1skGIQhwifJY6HWO6ZnbFe4un-VuJ3VW5\r\n\r\nThank you!", "comments": ["@akalakheti  Does this task is included in GCI 2019. Note that you get points for tasks included in the portal. Confirm this before doing this. ", "Yes, this task is included in GCI 2019 and I have completed this task.", "any update in this issue?\r\n", "Why is there no response on this issue?", "Is this issue still open?", "@swghosh: Yes. let me go check the status of that PR.", "I am closing this issue as the related PR already merged. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 34934, "title": "Added a Usage Example to flip_left_right()", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34934) for more info**.\n\n<!-- need_sender_cla -->", "@jgulian please sign CLA's and also, I think you should try and make changes against the master branch"]}, {"number": 34933, "title": "Gast 0.2.2 working, 0.3.3 not working", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but tested for both\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version (use command below): 1.14, 1.15\r\n- Python version: 3.6,3.7,3.8\r\n- CUDA/cuDNN version: Conda\r\n- GPU model and memory: Geforce GTX 1070 & 1080ti, 8GB/11GB respectivelyl\r\n\r\n\r\n**Describe the current behavior**\r\nUsing Gast 0.3.3, what happens is that the training will hang randomly when using tensorflow-gpu. This can be fixed by pip install gast==0.2.2\r\n**Describe the expected behavior**\r\nTraining and not hanging\r\n**Code to reproduce the issue**\r\nAll ML training that utilizes gpu\r\n\r\nThis issue still hasn't been dealt with.\r\n", "comments": ["training still seems to hang even using gast 0.2.2. This was momentarily fixed.", "@MayhemGang ,\r\nPlease note that  tensorflow 1.15.0 has requirement `gast==0.2.2` , `gast 0.3.2` is incompatible. Thanks!", "I know, I had to install gast 0.2.2 manually. However, even with gast 0.2.2, the training halts. No limits, no logging, just image saving. ", "@MayhemGang ,\r\nCan you provide a standalone code to reproduce the error reported here?Thanks!", "Unable as my code mostly draws on datasets. Would an entire zip file help?", "Unfortunately no. You need to provide a minified example, not a full code, to help developers narrow down the problem. It's similar to finding a spelling mistake in a page versus finding it in all the books in a city.", "Sigh, I will integrate Mnist I guess", "Will be updated in like a few days, anyone else has hanging issues meanwhile? ", "@MayhemGang ~yea hanging issues on 1.14 on one computer. Tried multiple reinstalls but no luck. NVIDIA Titan X (pascal)~ So silly, I just worked out freezing was due to windows console stopping if you selected text in it", "Can you please try TF 1.15 or TF 2.0 or the release candidate for TF 2.1 or the master's `tf-nightly`? I would recommend testing them all if minified example is hard to provide (and especially not clinging to 1.14 as that is extremely hard to patch now as it has been more than 6 months since it was released)\r\n\r\nWe really need a way to debug this, so please try to provide a minified example.", "@mihaimaruseac \r\n\r\nFor me (on Windows), I can't do 1.15 because I get some error about pxtas (https://github.com/tensorflow/models/issues/7640) and I notice keras model.predict is slow (like it is running on CPU) and the times to do an epoch vary wildly (e.g. up to 5 x times variation).\r\n\r\nI can't do v.2.X because tf.contrib.image (which I need) got moved to tf.addons but isn't implemented for windows yet.\r\n\r\nOT: but my problems are weird - one computer with Titan X (pascal) and one with RTX 2080 Ti. Since 1.14, the TITAN X pascal computer trains as though it is on CPU, even though it generates all the normal GPU messages. nvidia-cpu shows the GPU memory is occupied, but GPU utilization is 0. It would also ~randomly just freeze~ (Ignore.. freezing was because clicked console window). Same on 1.15. Also training of a simple two layer network with model.fit is taking almost as long as a 13 layer CNN, although that is with model.fit_generator. Reverting to 1.13.1 seems to have fixed it. The RTX 2080 trains fine.\r\n\r\nAnyway I know that story is not really helpful to you, but I'm currently troubleshooting by changing everything to tf.multiply / tf.add / tf.subtract etc to check if it is autograph, as well a few other things, and try to isolate the problem for a minified version.\r\n\r\n", "Exactly same as your issue @geometrikal, haven't been able to replicate with a stand out model yet.  I don't get any errors, and the training literally just halts. Ofc I have quick edit disabled, and GPU memory is being used, while usage is 0%, and CPU is not even being used.", "Very strange behavior detected. This run seems to train fine. but no images are saving. Perhaps the images saved are causing the hanging?", "In fact, with even Image saving fixed, the training is going perfectly. This is a rare occurence.\r\nThe only things I have changed are the config options,\r\n\r\n```\r\nconfig = tf.ConfigProto(allow_soft_placement=True)\r\nconfig.gpu_options.allow_growth = True\r\nconfig.gpu_options.polling_inactive_delay_msecs = 10\r\n```\r\n\r\nIt seems the polling delay fixes the issue. I am using dual xeon x5690s, which are of a very old architecture, and that may be causing the issue in the first place.", "@MayhemGang Just a reminder, you haven't shared your code so it's not clear what you are referring to.", "Closing this issue since we cannot reproduce the reported behavior. Feel free to reopen when new information is available. Thanks!"]}, {"number": 34932, "title": "Severe TPU/CPU behaviour discrepency", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):No\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0-dev20191203\r\n- Python version: 3.5\r\n- GPU model and memory: TPU (nightly-2.x)\r\n\r\n**Describe the current behavior**\r\nWhen training using a TPU backend, if a `@tf.function` `function` code is defined before connecting to a TPU cluster. Calling the function (as `Dataset.map(function)`) results in a python kernel crash, without any errors or other info. \r\n\r\nThis is especially severe (IMHO), since, as the training code base grows, more autograph functions are defined in modules instead of on the main program. As it's natural to import modules at the start of the program, if the TPU connection is initiated after the imports, using the @tf.fucntion code defined in the modules results in a kernel crash.\r\nIf the same code is, however, being run just on the CPU, it works as expected.\r\nThis leads to a somewhat frustrating experience of everything working on a CPU dev env, and then crashing inexplicably when connecting to a TPU.\r\nThe unintuitive solution is to run the TPU connection boilerplate before any imports.\r\n\r\n**Describe the expected behavior**\r\n1. connecting to a TPU shouldn't create an implicit scope, if a scope is required it should be with a `with` idiom\r\n2. It should be better documented if all autograph function definitions should be defined after connecting to a TPU\r\n3. If a code executed within a TPU scope depends on a code defined outside, it should fail gracefully and informatively (not crash the kernel)\r\n\r\n**Code to reproduce the issue**\r\n\r\n---\r\nFail case:\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef test_func(a):\r\n    return a**3\r\n\r\ntrain, test = tf.keras.datasets.fashion_mnist.load_data()\r\nimages, labels = train\r\nimages = images/255\r\nds = tf.data.Dataset.from_tensor_slices((images))\r\n\r\nTPU_IP = '10.0.3.2' #this requires a working nightly-2.x TPU cluster (2v-8)\r\ntpu_address = 'grpc://' + TPU_IP + ':8470'\r\nresolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\ndsf=ds.map(test_func)\r\n```\r\n---\r\nWorking case:\r\n```\r\nimport tensorflow as tf\r\n\r\nTPU_IP = '10.0.3.2' #this requires a working nightly-2.x TPU cluster (2v-8)\r\ntpu_address = 'grpc://' + TPU_IP + ':8470'\r\nresolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\ntrain, test = tf.keras.datasets.fashion_mnist.load_data()\r\nimages, labels = train\r\nimages = images/255\r\nds = tf.data.Dataset.from_tensor_slices((images))\r\n\r\n@tf.function\r\ndef test_func(a):\r\n    return a**3\r\n\r\ndsf=ds.map(test_func)\r\n```\r\n---\r\nThe reproduction code doesn't use imports, but \r\n```\r\n@tf.function\r\ndef test_func(a):\r\n    return a**3\r\n```\r\nWould typically run as part of the import code, and not the main program.\r\n\r\n**Other info / logs**\r\nNo error logs produced", "comments": ["Thank you for the update!\r\nAs for myself, I can get by with using the aforementioned solution of initiating connection before imports.\r\nBut its good to know that a formal fix is coming soon.\r\nBest regards.\r\n", "Sorry I did not take a closer look; your problem seems a different one. Btw I\u2019m not related to Google so take my advice at your discretion.", "To my knowledge, strategy = tf.distribute.experimental.TPUStrategy(resolver) will help you get into the correct device scope because TPU is always the remote client.\r\n@rxsang as expert on this.\r\nProbably need creating a TPUStrategy at the beginning.\r\n", "The reason is because in TPU case is a 2VM environment (client + Cloud TPU), there are lots of things happening in the initialization time. E.g. in experimental_connect_to_cluster we will enter into the tpu worker device scope, because we expect everything to happen on the tpu worker rather the coordinator/client, otherwise the performance may be bad. Thus we actually expect user always put the TPUStrategy creation and initialization code in the beginning of the program. I will try to make the documentation better. ", "@rxsang , Thank you for the information.\r\nI don't doubt that the interaction between the TPU VM and the client VM is very complicated. but that was my meaning of 'implicit' scope. \r\nIn this scenario, I believe it would be better to force all commands to run within an explicit scope, with the pythonic `with` idiom, this greatly reduces the chance for strange errors that fall between the cracks of the 2 VMs.\r\n\r\nRight now, the documentation only mentions running the model definition and compilation inside the `strategy` scope. but if I understood correctly, this is another inner scope.", "Your observation is correct. There are two scopes here: the strategy scope and the default device scope. For strategy scope, we expose it to the user and would like user to handle that. For default device scope, currently it is set in tf.config.experimental_connect_to_cluster(resolver), and you can always set tf.config.experimental_connect_to_cluster(resolver, make_master_device_default=False) to disable entering into scope automatically. But that means you may have to put your code in a \"with tf.device(\"/job:worker:1\"):\" scope, and that has to be after initialization code as well. Because before the experimental_connect_to_cluster, you don't see any remote devices. That's why we handle it for users, because we think it is troublesome for users to handle the default device scope themselves.\r\n\r\nI'm still not clear why this is a scope issue, my understanding is that you are trying to build something like dataset/tf.function before TPU initialization, the expectation is users should always do TPUStrategy creation and initialization first.", "@rxsang \r\nI think some of it comes down to miscommunication.\r\nI'm a relatively new TF user, joined after 2.0, so I may be unaware of some of the intricacies of TF, especially in distributed settings. I followed the documentation guides, and though I might have missed it, I haven't seen such an expectation (\"the expectation is users should always do TPUStrategy creation and initialization first.\"). I only realized it after encountering these crashes.\r\nWhile I personally would prefer an explicit scope definition if its required, I can certainly understand the design decision. but it should be better communicated that it should be done first, and before even the imports.\r\n\r\nBut the other part of the issue is that it crashes (the whole python kernel) ungracefully and without any indication of the reason.\r\nIf I had some exception information, indicating the unavailability of certain definitions on the device, it would have been much faster to deal with.\r\nThanks!", "Thanks Mike for filing the issue. We should improve our documentation https://www.tensorflow.org/guide/tpu and improve the error message, so users can be less confused by those issues. Close this as the root cause is found.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34932\">No</a>\n"]}, {"number": 34931, "title": "TensorFlow broadcasting results in error of Incompatible shapes", "body": "Greetings,\r\n\r\nAnother day, another potentially \"deep\" bug with TF I found! :-) \r\n\r\nI have three tensors: A, B, C. Here A is of shape (None, 64, 64), B is of shape: (None, 64) and C is of shape (None, 64). What I wanted to do is to have an element-wise multiplication followed by an element-wise addition.\r\n\r\nThanks to TF broadcasting, the code I needed to write is simply:\r\n\r\nA * B + C\r\n\r\nWith this, the output should have shape  (None, 64, 64). All these things are a part of my model, and everything works well. Specifically, by working well I mean I can compile the model and print model summary() successfully.\r\n\r\nUnfortunately, when I fit the the model to my training data, I got a problem of Incompatible shapes as bellows.\r\n\r\n```\r\n\r\n  File \"/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 363, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/cuong.hoang/anaconda2/envs/py36_env_tensor_gpu_pip_local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: Incompatible shapes: [32,64] vs. [32,64,64]\r\n\t [[{{node encoder_layer/mul_5}}]]\r\n\t [[loss/mul/_5175]]\r\n  (1) Invalid argument: Incompatible shapes: [32,64] vs. [32,64,64]\r\n\t [[{{node encoder_layer/mul_5}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nI am pretty sure my code correct, and the problem, I think is because of broadcasting. Moving forward, I decided to write by myself the code to make B and C has size (None, 64, 64) by repeating. The code is pretty simple, for B, as example:\r\n```\r\nB = K.expand_dims(B, axis=-1)\r\nB = K.repeat_elements(B, 64, axis=2)\r\n```\r\n\r\nWith this, the model compiles OK, and more importantly, when I fit the model to data, it runs OK as well.\r\n\r\nSo, I believe this is a bug of TensorFlow broadcasting. I am curious why it happens and did anyone get any similar error before?\r\n\r\nThis is with Tensorflow 1.14. If you want the code to reproduce the error just let me know.", "comments": ["@hoangcuong2011 \r\n\r\nCan you please help us with the code to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!", "@ravikyram: Thx. Below is the simplest code to reproduce the error. I removed C and kept only A and B with shape as follows:\r\n```\r\n\r\nA Tensor(\"MatMul:0\", shape=(?, 64, 64), dtype=float32)\r\nB Tensor(\"Cast:0\", shape=(?, 64), dtype=float32)\r\n```\r\nThen:\r\n`A * B Tensor(\"mul:0\", shape=(?, 64, 64), dtype=float32)`\r\n\r\nBut when I fit the model, I got this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/hoangcuong2011/Desktop/translation/error.py\", line 33, in <module>\r\n    main()\r\n  File \"/Users/hoangcuong2011/Desktop/translation/error.py\", line 29, in main\r\n    model.fit(dataset, dataset, epochs=200)\r\n  File \"/Users/hoangcuong2011/anaconda3/envs/py36_good_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 727, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Users/hoangcuong2011/anaconda3/envs/py36_good_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 675, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/Users/hoangcuong2011/anaconda3/envs/py36_good_env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 394, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/Users/hoangcuong2011/anaconda3/envs/py36_good_env/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 3476, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/Users/hoangcuong2011/anaconda3/envs/py36_good_env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [32,64,64] vs. [32,64]\r\n\t [[{{node training/Adam/gradients/gradients/mul_grad/BroadcastGradientArgs}}]]\r\n```\r\n\r\n\r\n\r\nFull code to reproduce the error:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef main(max_seq_length=64):\r\n    def create_model():\r\n        sequence = tf.keras.layers.Input(dtype='int32', shape=(max_seq_length,))\r\n        embedding_layer \\\r\n            = tf.keras.layers.Embedding(100, 512, input_length=max_seq_length)\r\n        embedding = embedding_layer(sequence)\r\n        embedding_reverse = tf.keras.backend.permute_dimensions(embedding, pattern=(0, 2, 1))\r\n        A = tf.keras.backend.batch_dot(embedding, embedding_reverse)\r\n        print(\"A\", A)\r\n        B = tf.dtypes.cast(tf.keras.backend.not_equal(0, sequence), dtype='float32')\r\n        print(\"B\", B)\r\n        A = A * B\r\n        print(\"new A\", A)\r\n        output = tf.keras.activations.softmax(A, axis=-1)\r\n        model = tf.keras.models.Model(inputs=[sequence], outputs=output)\r\n        model.compile(\r\n            tf.keras.optimizers.Adam(),\r\n            loss=tf.keras.losses.sparse_categorical_crossentropy)\r\n        return model\r\n\r\n    model = create_model()\r\n    print(model.summary())\r\n\r\n    dataset = np.random.randint(10, size=(100, max_seq_length))\r\n    \r\n    model.fit(dataset, dataset, epochs=200)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n", "I have tried on colab with TF version 1.14, 1.15 and was able to reproduce the issue.\r\n\r\nI tried with TF version 2.0, 2.1.0-rc0 and i am seeing below error message.\r\n```\r\nInvalidArgumentError:  Incompatible shapes: [32,64,64] vs. [32,64]\r\n\t [[node BroadcastGradientArgs_4 (defined at <ipython-input-3-ad43ee668564>:28) ]] [Op:__inference_distributed_function_743]\r\n```\r\nPlease, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/02d8fdddb062c6118d3c2409834d320e/untitled465.ipynb). Thanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34931\">No</a>\n", "Did anyone find workarounds for this issue?", "Getting the same issue when using custom layers in keras, using mean-squared-error loss function. The output shape is [1, 164, 2] in this example. The error I get is:\r\n\r\n`\r\nInvalidArgumentError:  Incompatible shapes: [1,2,164,164] vs. [1,164,2]\r\n\t [[node gradient_tape/mean_squared_error_1/BroadcastGradientArgs (defined at <ipython-input-29-970d8f40700d>:7) ]] [Op:__inference_train_function_42509]\r\n`"]}, {"number": 34930, "title": "Fix deprecated args for count_nonzero", "body": "reduction_indices is deprecated, not axis", "comments": []}, {"number": 34929, "title": "Distributed training not working properly when i got 1 ps 2 ro more workers", "body": "tf 1.15  python 3.7.2\r\nHello,\r\nWhen I am running simple distributed demo with 1 ps, 2 workers.\r\nthe worker 2 can stop properly, but the chef worker can't stop properly, here is error from chef worker 'RuntimeError: Run called even after should_stop requested.'\r\n\r\nhere is my code, \r\n\r\nimport tensorflow as tf\r\nfrom absl import app as absl_app\r\nfrom absl import flags\r\nimport numpy as np\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nflags.DEFINE_string(\"ps_hosts\", \"localhost:2222\", \"ps hosts\")\r\nflags.DEFINE_string(\"worker_hosts\", \"localhost:2223,localhost:2224\", \"worker hosts\")\r\nflags.DEFINE_string(\"job_name\", \"worker\", \"'ps' or'worker'\")\r\nflags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef main(argv):\r\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n    global_step = tf.train.get_or_create_global_step()\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    is_chief = (FLAGS.task_index == 0)\r\n    with tf.device(\r\n            tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:{0}\".format(FLAGS.task_index),\r\n                ps_device=\"/job:ps/cpu:0\",\r\n                cluster=cluster)):\r\n        x = tf.placeholder(tf.float32, shape=[None, 1], name='x')\r\n        y = tf.placeholder(tf.float32, shape=[None, 1], name='y')\r\n        w = tf.Variable(tf.constant(0.0))\r\n        learning_rate = tf.train.exponential_decay(0.1, global_step, 10, 2, staircase=False)\r\n        loss = tf.pow(w * x - y, 2)\r\n        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\r\n        with tf.train.MonitoredTrainingSession(\r\n                master=server.target,\r\n                is_chief=is_chief,\r\n                hooks=[tf.train.StopAtStepHook(last_step=2000)]\r\n        ) as mon_sess:\r\n            while not mon_sess.should_stop():\r\n                mon_sess.run(train_op, feed_dict={x: np.linspace(1, 2, 10).reshape([10, 1]),\r\n                                                  y: np.linspace(1, 2, 10).reshape([10, 1])})\r\n                print(mon_sess.run(learning_rate))\r\n                print(mon_sess.run(global_step))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    absl_app.run(main)\r\n\r\n\r\n\r\n\r\n'''python demo.py --job_name=ps task_index=0\r\npython demo.py --job_name=worker task_index=0\r\npython demo.py --job_name=worker task_index=1'''\r\n\r\nIs there anyone can offer me a one true simple with no error tf distributed demo?\r\nthanks~!\r\n\r\n", "comments": ["@XueCangQiuYe ,\r\nHi, can you please refer these [Distribution Strategy](https://www.tensorflow.org/guide/distributed_training) and Also [Custom Training](https://www.tensorflow.org/tutorials/distribute/custom_training) ?Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@XueCangQiuYe ,\r\nAny update on the issue ?Thanks!", "> @XueCangQiuYe ,\r\n> Any update on the issue ?Thanks!\r\n\r\nHi, Thank you for your reply!\r\ni'm swich to use tf.train.Supervisor, it works for my distributed training. tf.train.MonitoredTrainingSession may not work....", "@XueCangQiuYe ,\r\nIs the issue resolved?please close it if works with use of `tf.train.Supervisor `.\r\nThanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34928, "title": "Tensorflow 1.14.0 and python3.6.9 compatibility issues\uff1f", "body": "when I write `import tensorflow as tf`,the tensorflow version is 1.14.0,and python version is 3.6.9,the result show exception:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE\r\n\r\nBut use tensorflow 1.14.0 and python 3.6.8,the result is normal.\r\n\r\n", "comments": []}, {"number": 34927, "title": "Bad example in Sparse_Categorical_CrossEntropy", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/losses.py#L493\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe example will cause errors:\r\n\r\ncce = tf.keras.losses.SparseCategoricalCrossentropy()\r\nloss = cce(\r\n  [0, 1, 2],\r\n  [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\nprint('Loss: ', loss.numpy())  # Loss: 0.3239\r\n\r\nNeed to change to\r\n\r\ncce = tf.keras.losses.SparseCategoricalCrossentropy()\r\nloss = cce(\r\n  [0, 1, 2],\r\n  tf.constant([[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]]))\r\nprint('Loss: ', loss.numpy())  # Loss: 0.3239\r\n\r\nIn addition [.5, .89, .6] should be [0.05, 0.89, 0.06] to be consistent with similar example (\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy?hl=en). Thus Loss should be updated to 0.0945\r\n\r\n", "comments": ["@wzzhu I am not able to observe the error with the latest tf-nightly. Can you show the version of the tf you are using?", "I first found the problem in Tensorflow 2.0 stable version. When I tried running with nightly (in colab), it still had errors:\r\n\r\n!pip install tf-nightly-2.0-preview\r\n\r\nimport tensorflow as tf\r\ncce = tf.keras.losses.SparseCategoricalCrossentropy()\r\nloss = cce([0, 1, 2], [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\r\nprint('Loss: ', loss.numpy())  # Loss: 0.3239\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in sparse_categorical_crossentropy(target, output, from_logits, axis)\r\n   4523   if not from_logits:\r\n   4524     if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\r\n-> 4525         output.op.type != 'Softmax'):\r\n   4526       epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\r\n   4527       output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\r\n\r\nAttributeError: 'list' object has no attribute 'op'", "@wzzhu tf-nightly-2.0-preview has not been updated even before 2.0 released as it has been switched to tf-nightly. The last version of tf-nightly-2.0-preview was released in 10/02:\r\nhttps://pypi.org/project/tf-nightly-2.0-preview/#history\r\n\r\nPlease use tf-nightly instead.", "Right, it was TensorFlow Ver:  2.0.0-dev20191002.\r\nI tried tf-nightly and it works. Thanks.\r\nBut the numbers still need updating to as [.5, 0.89, .6] is logits instead of a prob distribution as in categorical crossentropy example which is [.05, .89, .06]", "Could reproduce the issue with TF version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/6019a161a256e9a461093b9ea53279fd/34927.ipynb). Thanks!", "@wzzhu Looks like the docstring has been fixed in  commit c7061962262c8cacc4017b352f32d2134216ca4a", "Closing this issue since its resolved. Thanks!"]}, {"number": 34926, "title": "tensorflow lite build issue on Windows", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows-10-10.0.18362-SP0\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): 8.1.0\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI was trying to build tensorflow lite from source with `bazel build //tensorflow/lite:libtensorflowlite.so`\r\nAfter build error I've tried to add `features = [\"windows_export_all_symbols\"]` to build_def.bzl, but nothing changes\r\n\r\n**Any other info / logs**\r\nError without verbose with this command `bazel build //tensorflow/lite:libtensorflowlite.so`:\r\n```\r\nLINK : warning LNK4044: \u044d\u0445\u0401\u0440\u0451\u044f\u044e\u0447\u044d\u0440\u044d\u044d\u221a\u0449 \u044f\u0440\u0401\u0440\u044c\u0445\u0404\u0401 \"/s\"; \u0448\u0443\u044d\u044e\u0401\u0448\u0401\u0454\u0445\u0404\u0451\u00a0\r\nERROR: C:/users/hell_/documents/tensorflow/tensorflow/lite/BUILD:452:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created\r\nERROR: C:/users/hell_/documents/tensorflow/tensorflow/lite/BUILD:452:1: not all outputs were created or valid\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 0.943s, Critical Path: 0.38s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nAnd error running `bazel build -c opt - verbose_failures //tensorflow/lite:libtensorflowlite.so`:\r\n```\r\nSkipping '\u0427': Bad target pattern '\u0427': package names may contain A-Z, a-z, 0-9, or any of ' !\"#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\\')\r\nERROR: Bad target pattern '\u0427': package names may contain A-Z, a-z, 0-9, or any of ' !\"#$%&'()*+,-./;<=>?[]^_`{|}~' (most 7-bit ascii characters except 0-31, 127, ':', or '\\')\r\nINFO: Elapsed time: 0.272s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\nThank you", "comments": ["Hello @TheArheus, seems like something is corrupt in your repository. Are you able to build any other targets in TensorFlow? Can you try cloning a fresh copy and re-trying the build?\r\n", "I've tried to delete tensorflow repository and cloned it again. After that I tried to build tensorflow lite again and this error again.\r\nI tried to clone tensorflow repository to \"C:/\" and I got the same error after trying to build.\r\n```\r\nFrom Linking tensorflow/lite/libtensorflowlite.so:\r\nLINK : warning LNK4044: \u044d\u0445\u0401\u0440\u0451\u044f\u044e\u0447\u044d\u0440\u044d\u044d\u221a\u0449 \u044f\u0440\u0401\u0440\u044c\u0445\u0404\u0401 \"/s\"; \u0448\u0443\u044d\u044e\u0401\u0448\u0401\u0454\u0445\u0404\u0451\u00a0\r\nERROR: C:/tensorflow/tensorflow/lite/BUILD:452:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created\r\nERROR: C:/tensorflow/tensorflow/lite/BUILD:452:1: not all outputs were created or valid\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\n```", "@TheArheus could you try it again with changing system language to English? I'm wondering if your issue is the same with #34833 or not.", "I changed my system language to english, and now I have this error:\r\n```\r\nINFO: From Linking tensorflow/lite/libtensorflowlite.so:\r\nLINK : warning LNK4044: unrecognized option '/s'; ignored\r\nERROR: C:/tensorflow/tensorflow/lite/BUILD:452:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created\r\nERROR: C:/tensorflow/tensorflow/lite/BUILD:452:1: not all outputs were created or valid\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\n```", "@TheArheus I guess there is another error before the error. Could you check it? Or can you dump full build log?", "No, thats all. I tried to build with `build -c opt --verbose_failures //tensorflow/lite:libtensorflowlite.so`:\r\n```\r\nINFO: Writing tracer profile to 'C:/users/1/_bazel_1/xv6zejqw/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/1/AppData/Local/Programs/Python/Python36/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/1/AppData/Local/Programs/Python/Python36/python.exe --action_env PYTHON_LIB_PATH=C:/Users/1/AppData/Local/Programs/Python/Python36/lib/site-packages --python_path=C:/Users/1/AppData/Local/Programs/Python/Python36/python.exe --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/w --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --incompatible_windows_native_test_wrapper --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (1 packages loaded, 1 target configured).\r\nINFO: Found 1 target...\r\nINFO: From Linking tensorflow/lite/libtensorflowlite.so:\r\nLINK : warning LNK4044: unrecognized option '/s'; ignored\r\nERROR: C:/tensorflow/tensorflow/lite/BUILD:452:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created\r\nERROR: C:/tensorflow/tensorflow/lite/BUILD:452:1: not all outputs were created or valid\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 2.796s, Critical Path: 1.19s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@TheArheus I've reproduced this. I'll dig more.", "ERROR: C:/tensorflow/tensorflow/lite/BUILD:452:1: output 'tensorflow/lite/libtensorflowlite.so.if.lib' was not created\r\n\r\n@jdduke please let us know if have some idea on this error.", "The same problem here. \r\nDespite the error I found libtensorflow.so in bazel-bin/tensorflow/lite/, but the library is only 92KB and doesn't seem to be correct.", "Just to confirm, are y'all using the latest master checkout? or the 2.0 branch?", "@terryheo we might look to the core TensorFlow libs for guidance here, see also https://github.com/tensorflow/tensorflow/blob/master/tensorflow/BUILD#L728", "@jdduke yeah, definetly used 2.0 new branch", "So, in newer versions I don't have this issue anymore and build correctly. I was using newest versions of tensorflow(downloaded 24/1/2020) and bazel version 1.2.1. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34926\">No</a>\n"]}, {"number": 34925, "title": "sparse_matrix_sparse_matmul gives wrong result when run on GPU", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.1.0-rc0\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 10.1.243\r\n- **GPU model and memory**: Titan X\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nWhen using sparse matmul with 2 sparse matrices on GPU, the result is wrong (has bad indices and some entries are 0 that should not be). The code below fails with  \r\n\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[256] = [0,0,0] is out of \r\norder. Many sparse ops require sorted indices.\r\n>    Use `tf.sparse.reorder` to create a correctly ordered copy.\r\n\r\n  The same code runs without any errors on CPU. \r\n\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.linalg.sparse import sparse\r\n\r\n\r\na = tf.random.uniform([5, 300, 200])\r\na = tf.nn.dropout(a, 0.5)\r\nsa = tf.sparse.from_dense(a)\r\ncsr_sa = sparse.CSRSparseMatrix(sa)\r\nsmm = sparse.matmul(csr_sa, csr_sa, transpose_b=True)\r\nsmm = sparse.csr_sparse_matrix_to_sparse_tensor(smm._matrix, type=tf.dtypes.float32)\r\nsmm = tf.sparse.SparseTensor(indices=smm.indices, values=smm.values, dense_shape=smm.dense_shape)\r\nmm = tf.linalg.matmul(a, a, transpose_b=True)\r\ntf.debugging.assert_near(mm, tf.sparse.to_dense(smm))\r\n```", "comments": ["Issue replicating when the code is run for TF- 2.1rc0 on GPU, kindly find [gist](https://colab.sandbox.google.com/gist/oanush/ed04be796abab02ae5728162de540f45/34925.ipynb) for the same. Works fine when run on CPU-[gist](https://colab.sandbox.google.com/gist/oanush/2597a2e5b62425631fbc8a2c009d9131/34925.ipynb).Thanks!", "In case it helps, here are the operations that are executed on the GPU in the failing run.  I'd expect one or more of them to be breaking the \"indices are sorted\" invariant.\r\n\r\n * `BatchMatMulV2`\r\n * `Cast`\r\n * `CSRSparseMatrixToSparseTensor`\r\n * `Fill`\r\n * `GatherNd`\r\n * `GreaterEqual`\r\n * `Mul`\r\n * `NotEqual`\r\n * `RandomUniform`\r\n * `Shape`\r\n * `SparseMatrixSparseMatMul`\r\n * `SparseTensorToCSRSparseMatrix`\r\n * `Where`\r\n", "@chpohl,\r\nLooks like the issue has been fixed. I was able to run the code on TF v2.3 without any error, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9d0077ac5118cb0ece628dd16ceaa909/34925.ipynb). Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have verified that the issue is gone. I'm closing the issue. Please feel free to reopen it if there are further problems.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34925\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34925\">No</a>\n"]}, {"number": 34923, "title": "Update readers.py", "body": "Add usage example using IRIS dataset to tf.data.experimental.make_csv_dataset()\r\n\r\nGCI 2019", "comments": []}, {"number": 34922, "title": "Tensorflow Lite does not include GPU ", "body": "**System information**\r\n- OS Platform and Distribution: Linux Mint 19\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: current master (094da7eaa9d802ee676170f2df58ded528da52d9)\r\n- Installed using: make (as shown here: https://www.tensorflow.org/lite/guide/build_arm64)\r\n- GCC/Compiler version (if compiling from source): gcc 7.4.0\r\n- CUDA/cuDNN version: V9.1.85\r\n- GPU model and memory: GeForce GTX 1660 6GB\r\n\r\n**Describe the problem**\r\nThe Makefile (tensorflow/tensorflow/lite/tools/make/Makefile) for TF Lite does not include any possibility to include gpu delegates for x86.\r\n\r\n```bash\r\n>> ./download_dependencies.sh\r\n>> ./build.lib.sh\r\n```\r\n\r\nThen e.g. this will result in an undefined reference error: \r\n```cpp\r\nTfLiteGpuDelegateV2Create()\r\n```\r\n\r\n**What I have tried**\r\nI adapted some files and tried to get it working, but with little success:\r\n```bash\r\n# for cl support\r\n>> sudo apt-get install nvidia-cuda-toolkit\r\n# egl\r\n>> sudo apt-get install libegl1-mesa-dev\r\n# gles\r\n>> sudo apt-get install libgles2-mesa-dev\r\n\r\n# add fp16 to download_dependencies.sh\r\nFP16_URL=\"https://github.com/Maratyszcza/FP16/archive/master.zip\"\r\n...\r\ndownload_and_extract \"${FP16_URL}\" \"${DOWNLOADS_DIR}/fp16\"\r\n\r\n# add to Makefile\r\nINCLUDES := \\\r\n...\r\n-I$(MAKEFILE_DIR)/downloads/fp16/include \\\r\n...\r\n\r\nCORE_CC_ALL_SRCS := \\\r\n...\r\n$(wildcard tensorflow/lite/delegates/*.cc) \\\r\n$(wildcard tensorflow/lite/delegates/gpu/**/*.cc) \\\r\n...\r\n```\r\n\r\nResults in error:\r\n```\r\nIn file included from /usr/include/EGL/eglplatform.h:124:0,\r\n                 from /usr/include/EGL/egl.h:39,\r\n                 from ./tensorflow/lite/delegates/gpu/cl/egl_sync.h:19,\r\n                 from tensorflow/lite/delegates/gpu/cl/egl_sync.cc:16:\r\n./tensorflow/lite/delegates/gpu/common/status.h:47:7: error: expected identifier before \u2018int\u2019\r\n class Status {\r\n```\r\n", "comments": ["Hi @impjdi, could you help answer this question? Thanks!", "EGL by default pulls in X11 headers.  X11 includes a\r\n\r\n> typedef int Status;\r\n\r\nsomewhere.  You need to find the flag that disables that.  It's different for each platform, so I can't pinpoint which flag you need to set.  One of the name's I've seen in the past is something like `MESA_EGL_NO_X11_HEADER`, but really, you have to dig deeper and find the flag that does the job.", "Thanks for the feedback!\r\nI am still a bit lost. Is this expected? Is the code within `tensorflow/lite/delegates/gpu` ever used then e.g. when using bazel? Could not find any docu about that.", "That part is usually built with proper EGL for mobile devices, not Mesa.\r\n\r\nTFLite GPU is not friendly with non-mobile setting (yet?).", "Ah ok, that might explain why I have trouble getting it work. Then I guess I am just using the wrong tool for my usecase.\r\nNevertheless, would be a cool feature."]}, {"number": 34921, "title": "Cann't transform TF.EagerTensor to python datatype in map_fun", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:GTX1060 6Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI want to use `dataset = tf.data.Dataset.list_files('./*.mat')` to create a dataset, when i using `next(iter(dataset))` and `map_fun` to parse the data, it is ok. The `map_fun` is as below:\r\n```python\r\ndef map_fun(filename):\r\n\tfilename = filename.numpy()\r\n\tdata = sio.loadmat(filename)\r\n\tx = tf.cast(data['X'],dtype = tf.float32)\r\n\tlabel = tf.cast(data['label'].reshape(-1,), dtype = tf.int8)\r\n\treturn x,label\r\n```\r\nBecause tensorflow doesn't support to read .mat file directly,so l need to use `scipy.io.loadmat` to load .mat file.But when I use the `map` method to process lots of .mat files such as:\r\n```python\r\ndb = dataset.map(map_fun)\r\n```\r\nit is not work due to `.numpy()` method shouldn't be used based on eager mode.\r\n\r\n**Describe the expected behavior**\r\nCan using tf.data.Dataset.map to process a lot of .mat files so that can be create a dataset.\r\nI think i should transfrom the `TF.EagerTensor` to python data type(in my case, str) so that `scipy.io.loadmat` could work.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@HsinJhao  can you link a colab example or a gist?\r\n\r\nAlso here is one of the useful links that describes why we can't use .numpy method during eager execution and what is the remedy to solve it:\r\n\r\nhttps://stackoverflow.com/questions/56075037/how-to-convert-tensor-to-numpy-array-in-tensorflow", "@dubesar Thank your reply! Sorry i don't have a colab example or a gist, but there is not have much code so i add these code to this comment. \r\n\r\nI am noticed that we can't use .numpy method during eager execution,so I do try some tf built-in function to load .mat file,but it failed.\r\n\r\nThere is some example or thought I've tried(or not know how to implement):\r\n\r\n1. Using .numpy() method-->Not Work during eager execution\r\nthe code is same as the issue description.\r\n\r\n2. Using eval() method\r\nI think if i can use .eval method to get filename as string datatype,but i don't how to define default sess to .eval() in tensorflow2.0.\r\n```python\r\nimport tensorflow as tf\r\nimport scipy.io as sio\r\nimport glob\r\ndef map_fun(filename):\r\n\twith tf.compat.v1.Session().as_default() as sess:\r\n\t\tfilename = tf.cast(filename,tf.string).eval(session = sess) #eval method need a default sess\r\n\tdata = sio.loadmat(filename) #data.keys() = ['X','label'] \r\n\tx = tf.cast(data['X'],dtype = tf.float32) # data['X'].shape = (num_timestamp,num_feature)\r\n\tlabel = tf.cast(data['label'].reshape(-1, ),dtype = tf.int8) # data['label'].shape = (num_timestamp,)\r\n\treturn x, label\r\nfilelist = glob.glob('./<file_pattern>*.mat')\r\ndb = tf.data.Dataset.from_tensor_slices(filelist)\r\ndataset = db.map(map_fun)\r\n```\r\nThere is some error message as below:\r\n```python\r\nFile \"E:/Programs/PythonProjects/TFRecordAndTFDataset/data_loader.py\", line 66, in <module>\r\n    dataset = db.map(map_fun)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 1211, in map\r\n    return MapDataset(self, map_func, preserve_cardinality=True)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 3416, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 2695, in __init__\r\n    self._function = wrapper_fn._get_concrete_function_internal()\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1854, in _get_concrete_function_internal\r\n    *args, **kwargs)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 837, in func_graph_from_py_func\r\n    args, arg_names, flat_shapes=arg_shapes)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 1062, in _get_defun_inputs_from_args\r\n    args, names, structure=args, flat_shapes=flat_shapes)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 1130, in _get_defun_inputs\r\n    name=requested_name)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\graph_only_ops.py\", line 51, in graph_placeholder\r\n    attrs={\"dtype\": dtype_value, \"shape\": shape}, name=name)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"D:\\Python\\virtualenvs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1751, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n\r\n3. Use tf.io.read_file to load .mat file-->Do not know how to parse the data\r\n  If i can use `tf.io.read_file` to load .mat file directly and correctly,i will not need to load .mat file using `scipy.io.loadmat`, but i can't understand the data using `tf.io.read_file` to load .mat file and don't know how to parse the data to a dict(the data using `scipy.io.loadmat` to load .mat file will be a dict).\r\nThere is some information about the difference using tf.io.read_file or scipy.io.loadmat to load .mat file:\r\n```python\r\nfilename = next(iter(db))\r\n# filename = <tf.Tensor: id=10, shape=(), dtype=string, numpy=b'test.mat'>\r\ndata1 = tf.io.read_file(filename)\r\n# data1 = <tf.Tensor: id=11, shape=(), dtype=string, numpy=b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri Dec  6 16:54:07 2019      \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01IM\\x0f\\x00\\x00\\x00\\xe8......\r\n# type(data_tfio) = <class 'tensorflow.python.framework.ops.EagerTensor'>\r\ndata2 = scipy.io.loadmat(filename.numpy())\r\n# data2 = {'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri Dec  6 16:54:07 2019', '__version__': '1.0', '__globals__': [], 'X': array([[  7.8698893 , -10.556351  , ...],[...]]),'label': array([[2],[2],[2],...])\r\n# data_scipy.keys() = dict_keys(['__header__', '__version__', '__globals__', 'X', 'label'])\r\n```\r\n\r\nHope it helps!Thank your reply again!", "@HsinJhao Actually I got why are you getting the error in loading the ```tf.io.read_file```  as you have not defined the session after reading the file:\r\nTake this for example:\r\n``` python3\r\nimage_file = tf.read_file(filename)\r\nimage_decoded = tf.image.decode_jpeg(image_file, channels=3)\r\nwith tf.Session() as sess:\r\n     f, img = sess.run([image_file, image_decoded])\r\n     print(f)\r\n     print(img)\r\n```\r\n\r\nYou can do this to remove the error. I am pretty much sure this will remove the error.", "@dubesar Thanks your reply!\r\nI'm confused about using tf.image.decode_jpeg function to decode .mat file.The error message in my last reply is about using .eval() method of TF.EagerTensor to transform the datatype to string,not about tf.io.read_file. \r\n\r\nMaybe what I said is not clear enough.\r\n\r\n**My file is not images but time series and the length of time series is uncertain, but i am sure that one .mat file contains a time series  data and corresponding labels. The length of data in different .mat files is not necessarily the same, you can see my data as speech data.**\r\n\r\nAs far as I know, the tf.io.decode_jpeg is not supported to decode .mat file,it will cause the error as below:\r\n```python\r\n>>> filelist = glob.glob('test**.mat')\r\n>>> db = tf.data.Dataset.from_tensor_slices(filelist)\r\n>>> filename = next(iter(db))\r\n>>> file = tf.io.read_file(filename)\r\n>>> file_decoded = tf.image.decode_jpeg(file)\r\n# the error message:\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected image (JPEG, PNG, or GIF), got unknown format starting with 'MATLAB 5.0 MAT-f' [Op:DecodeJpeg]\r\n```\r\nMaybe we should pay attention to **loading .mat file using only the Tensorflow primitives** rather than scipy.io or hdf5storarge packages.\r\n\r\nThank you for your reply\uff01", "I have tried on colab with TF version 2.0,2.1.0-rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/40fe1d02697fa6b6111378b1062eac6d/untitled470.ipynb). Thanks!", "@HsinJhao I think it should be possible as shown in this [resource](https://stackoverflow.com/questions/46890387/how-to-read-mat-file-format-in-tensorflow). \r\nI updated that code to match your code. here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/f955420a6def4632eb922bb8ff390f55/untitled714.ipynb).\r\n\r\nPlease provide more data if you have any further questions. Please close the issue if it was resolved. Thanks!\r\n\r\n\r\n\r\n", "@jvishnuvardhan I have tried your solution but not resolved!\r\nIt raise the error as below:\r\n```python\r\nOperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph is disabled in this function. Try decorating it directly with @tf.function.\r\n```\r\nThe error is also shown in this issue:#35108\r\n\r\nHere is my [gist](https://colab.research.google.com/gist/HsinJhao/9a6ec62ce8eb3fb32fcc720f222abe1b/createdtfdatasetfrommatfiles.ipynb) and it contains my .mat example files.\r\n\r\nThanks your reply!", "@HsinJhao Can you please share the files `1.mat` and `2.mat`. Thanks!", "@jvishnuvardhan \r\nThese is the files:\r\n[matdata.zip](https://github.com/tensorflow/tensorflow/files/3992507/matdata.zip)\r\n\r\nThanks!\r\n\r\n\r\n", "@HsinJhao I updated your code. Please check the updated [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/4778937cff52a4e4a50889673ac39cfe/createdtfdatasetfrommatfiles.ipynb). This updated code works well without any error. Thanks!\r\n\r\nPlease post further questions in Stackoverflow. We will resolve it there. GitHub is mainly for bugs/performance related issues. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34921\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34921\">No</a>\n", "Hi @HsinJhao, wondering if you ever got this working? I attempted to implement the final version posted here (I am attempting to do something similar) and got the same error that it expects a str, not an EagerTensor.", "@jvishnuvardhan Do you know how to read the data back from the dataset in this case, i.e. reading mat files? I get UnicodeDecodeError when I take one and want to print it."]}, {"number": 34920, "title": "Unrolled LSTM terrible performance when tf.function is used", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from : binary\r\n- TensorFlow version : 2.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: no CUDA\r\n- GPU model and memory: no GPU\r\n\r\nWell I thought I knew what tf.function does, but these two pieces of code confused me:\r\n\r\nThis does not use tf.function:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\n\r\nrnn = tf.keras.layers.LSTM(256, return_state=True, unroll=True)\r\n\r\ndef go():\r\n    return rnn(tf.zeros((1, 300, 256)))\r\n\r\nst = time.time()\r\nwith tf.GradientTape() as tape:\r\n    x = go()\r\nprint(f'graph: {time.time() - st}') # --> graph: 0.20923495292663574\r\nst = time.time()\r\ngradient = tape.gradient(x[-1], rnn.trainable_weights) # --> gradient: 0.4629485607147217\r\nprint(f'gradient: {time.time() - st}')\r\n```\r\n\r\nWhile this does:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\n\r\nrnn = tf.keras.layers.LSTM(256, return_state=True, unroll=True)\r\n\r\n@tf.function\r\ndef go():\r\n    return rnn(tf.zeros((1, 300, 256)))\r\n\r\nst = time.time()\r\nwith tf.GradientTape() as tape:\r\n    x = go()\r\nprint(f'graph: {time.time() - st}') # --> graph: 12.843713283538818\r\nst = time.time()\r\ngradient = tape.gradient(x[-1], rnn.trainable_weights) # --> gradient: 2.7147161960601807\r\nprint(f'gradient: {time.time() - st}')\r\n```\r\n\r\nThis difference in performance only happens when unroll=True. Is this expected?\r\n", "comments": ["I have tried on colab with TF version 2.0,2.1.0-dev20191203 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d3b253675ac7770576cfeeaecb36646e/untitled449.ipynb). Thanks!", "Thanks for reporting the issue.\r\n\r\nI think the performance dip is caused by the tf.function tracing when the function is invoked first time. During the tracing, tf.function will try to exam all the ops and variables, and create graph for execution. This process will take some time, but its a one time cost to pay, which means all the follow up invocations of the function will be very fast.\r\n\r\nYou could try time the go() with tf.function multiple times, and from the experiment I did, the follow up runs takes 0.05 sec. This aligns with our suggested usage for tf function, that user will feed different data tensor to the function, but we only need to create the graph once.\r\n\r\nOn a side note, we are spending effort to improve the eager performance, both for tracing and execution. You could expect some speedups in the 2.x releases.\r\n\r\nI am closing this bug as working as intended, feel free to reopen it if you feel otherwise. Thanks."]}]