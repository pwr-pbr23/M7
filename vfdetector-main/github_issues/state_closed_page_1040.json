[{"number": 22113, "title": "Build of tensorflow r1.11 fails on ubuntu 18.04 (r1.10 was OK)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.11\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\n- **CUDA/cuDNN version**: 9.2/7.2.1\r\n- **GPU model and memory**: NVIDIA GeForce 940MX\r\n- **Exact command to reproduce**: bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n### Describe the problem\r\nWhile building the tensorflow from the branch r1.11 with GPU support, the build fails with the following error:\r\nERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n\r\n### Source code / logs\r\nThe following command line is available:\r\nINFO: From Compiling tensorflow/compiler/xla/service/gpu/outfeed_manager.cc [for host]:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/compiler/xla/status.h:19,\r\n                 from ./tensorflow/compiler/xla/array.h:33,\r\n                 from ./tensorflow/compiler/xla/array2d.h:29,\r\n                 from ./tensorflow/compiler/xla/literal.h:31,\r\n                 from ./tensorflow/compiler/xla/service/gpu/outfeed_manager.h:19,\r\n                 from tensorflow/compiler/xla/service/gpu/outfeed_manager.cc:16:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\n./tensorflow/compiler/xla/shape_util.h:117:5:   required from here\r\n./tensorflow/core/platform/default/logging.h:232:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\r\n./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n                                                 ^\r\n./tensorflow/core/platform/default/logging.h:232:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\r\n ^~~~~~~~~~~~~~~~~~~~~~~\r\nERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd ~/.cache/bazel/_bazel_~/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/host/genfiles/external/grpc -iquote bazel-out/host/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/host/genfiles/external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote bazel-out/host/bin/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/curl -iquote bazel-out/host/genfiles/external/curl -iquote bazel-out/host/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/host/genfiles/external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/genfiles/external/aws -iquote bazel-out/host/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/host/genfiles/external/grpc/include -isystem bazel-out/host/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/host/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/host/genfiles/external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem bazel-out/host/bin/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/com_google_absl -isystem bazel-out/host/genfiles/external/com_google_absl -isystem bazel-out/host/bin/external/com_google_absl -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/genfiles/external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/genfiles/external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -c tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc -o bazel-out/host/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o)\r\nIn file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/common_runtime/device.h:43,\r\n                 from ./tensorflow/core/common_runtime/device_mgr.h:24,\r\n                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,\r\n                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,\r\n                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,\r\n                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\n./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':\r\n./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {\r\n                                         ~~~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'\r\n   string key(std::move(parsed.FullKey().ToString()));\r\n                                         ^~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/refcount.h:22,\r\n                 from ./tensorflow/core/platform/tensor_coding.h:21,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/types.h:31,\r\n                 from ./tensorflow/contrib/verbs/verbs_util.h:21,\r\n                 from ./tensorflow/contrib/verbs/rdma.h:30,\r\n                 from ./tensorflow/contrib/verbs/rdma_mgr.h:24,\r\n                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21,\r\n                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1566.447s, Critical Path: 150.61s\r\nINFO: 1352 processes: 1352 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["@vyepishov I think the issue is the same as #21999. A PR #22003 has been created and ready to be merged into the master soon.", "Hi @yongtang,\r\n\r\nThanks for the good news!\r\nLooking forward to the merge.\r\n\r\nBest Regards,\r\nVadym", "Is the solution going to be backported to the r1.11 release so that this will build?"]}, {"number": 22112, "title": "Update rpi.md", "body": "corrected path for the static library in cross compiling", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Please fix the CLA, then reopen this PR.\r\nYou can find the instructions to sign the CLA above, in Googlebot's comments."]}, {"number": 22111, "title": "File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 153, in create_inference_graph     int(msg[0]))", "body": " 'tensorrt_dir', ['L2Loss'])\r\n  File \"demo.py\", line 88, in get_trt_graph\r\n    precision_mode=precision_mode)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 153, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\r\n\t [[Node: BatchMultiClassNonMaxSuppression/ones = Fill[T=DT_INT32](BatchMultiClassNonMaxSuppression/ones/shape, BatchMultiClassNonMaxSuppression/ones/Const)]]\r\n\r\nList of Pakages\r\nTensorRT:4.0.1.6\r\nCuda:9.0\r\nLinux:16.0.4\r\nPython :3.5\r\nTensorflow:1.10", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@Akhtar303nu it looks like your graphdef was generated with an earlier version of TF. If you can update it to the tensorflow that you are using it should go away. Updating is just loading graphdef from the file into a graphdef object and saving serialization of graphdef object back to a new file."]}, {"number": 22110, "title": "[Bug] tf.nn.depthwise_conv2d fails with AttributeError", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: v1.10.1-0-g4dcfddc5d1\r\n- **Python version**: 2.7.12\r\n- **CUDA/cuDNN version**: 9.0/7.1.4\r\n- **GPU model and memory**: GTX 1070\r\n\r\n### Describe the problem\r\n`tf.nn.depthwise_conv2d` fails when the shape of the input is not known statically and the data format is 'NCHW' and the dilation rate is larger than 1.\r\n\r\n### Source code / logs\r\nReproducible test case:\r\n```\r\nimport tensorflow as tf\r\nsh = tf.placeholder(dtype=tf.int32, shape=[4])\r\nimg = tf.ones(sh)\r\nk = tf.ones([1, 1, 1, 1])\r\nt = tf.nn.depthwise_conv2d(img, k, [1, 1, 1, 1], 'VALID', rate=[2, 1], data_format='NCHW')\r\n```\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py\", line 461, in depthwise_conv2d\r\n    op=op)\r\n  File \"/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 364, in with_space_to_batch\r\n    return new_op(input, None)\r\n  File \"/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 514, in _with_space_to_batch_call\r\n    output_shape[1] = filter.shape[-1]\r\nAttributeError: 'NoneType' object has no attribute 'shape'\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce\nMobile device", "Added a PR #22139 for the fix.", "As requested, system information with all fields filled out.\r\n\r\n### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **Mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: v1.10.1-0-g4dcfddc5d1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.1.4\r\n- **GPU model and memory**: GTX 1070\r\n- **Exact command to reproduce**: N/A\r\n"]}, {"number": 22109, "title": "TocoConverter: permute layer after dim-reducing reshape / squeeze: error on conversion", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n- **Python version**: 3.5.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: V9.0.176\r\n- **GPU model and memory**: NVIDIA TITAN Xp, 12196MiB\r\n- **Exact command to reproduce**: see minimum example code below\r\n\r\n\r\n### Describe the problem\r\nThe code example below containing a model with an input, \"transformation\" and permute layer compiles fine on pc and it is also possible to call `predict()`on it and getting the expected results.\r\nThe input layer expects the last dimension to be 1, the transform layer being either a reshape or a squeeze layer gets rid of the 1, and the permutation layer just pushes the first dimension after the batch size to the end.\r\nThe native Keras reshape layer (A) is the only one that doesn't work because it doesn't support dimensionality reduction. Its output shape would be `(None, None, 10, 42)`.\r\nHowever, lines (B), (C) and (D) all work fine and it doesn't matter which one is commented in.\r\nIn the same way it doesn't matter whether the Keras permute layer (E) or the backend version (F) is commented in.\r\n\r\nIf all transform layers (A-D) are commented out and only one permute layer (E) or (F) is commented in, directly gets passed `input` and an input shape of `(2,3,1)` for (E) or `(0,2,3,1)` for (F), the model compiles and the `TocoConverter` produces a tflite-file.\r\nIf all permute layers (E, F) are commented out and only one transform layer (B) or (C) or (D) is commented in and the `outputs` of `Model` is set to `[transform]`, the model also compiles and the `TocoConverter` produces a tflite-file.\r\n\r\nBut for each combination of (B-C) and (E, F) being commented in so that one transform layer and one permute layer is there, the TocoConverter doesn't produce a tflite-file although the model compiles and the model summary looks as it should.\r\n\r\nThe expected result is the minimum example code running without errors and producing a tflite model file from the converted keras model file.\r\n\r\n---------------\r\n**Edit: For the time being and for everyone with the same problem, I wrote an updated minimum example code with an \"emulated\" permute layer as a workaround. It uses split, concatenate and reshape operations and you find it at the end of this page after the tracebacks.**\r\n\r\n---------------\r\n\r\n### Source code / logs\r\n#### Minimum Example Code:\r\n```python\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.models import save_model\r\n\r\nfrom tensorflow.keras.layers import Input, Lambda, Permute, Reshape\r\nfrom tensorflow.keras.backend import permute_dimensions\r\nfrom tensorflow.keras.backend import squeeze as b_squeeze\r\nfrom tensorflow import reshape, squeeze\r\n\r\nfrom tensorflow.contrib import lite\r\n\r\n\r\ninput = Input(shape=(10, 42, 1))\r\n\r\n# transform = Reshape((-1, 10, 42))(input)\t\t\t\t\t# (A)\r\ntransform = Lambda(lambda x: reshape(x, (-1, 10, 42)))(input)\t\t\t# (B)\r\n# transform = Lambda(lambda x: squeeze(x, axis=-1))(input)\t\t\t# (C)\r\n# transform = Lambda(lambda x: b_squeeze(x, axis=-1))(input)\t\t\t# (D)\r\n\r\npermute = Permute((2,1))(transform)\t\t\t\t\t\t# (E)\r\n# permute = Lambda(lambda x: permute_dimensions(x, (0,2,1)))(transform)\t\t# (F)\r\n\r\nkeras_model = Model(inputs=[input], outputs=[permute])\r\nkeras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n\r\n\r\nprint(keras_model.summary())\r\nsave_model(model=keras_model, filepath=\"keras_model.hdf5\",\r\n\toverwrite=True, include_optimizer=True)\r\n\r\nconverter = lite.TocoConverter.from_keras_model_file(\"keras_model.hdf5\")\r\ntflite_model = converter.convert()\r\nwith open(\"tflite_model.tflite\", \"wb\") as f: f.write(tflite_model)\r\n```\r\n\r\n#### keras_model.summary() for each of the valid six combinations:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         (None, 10, 42, 1)         0\r\n_________________________________________________________________\r\nlambda (Lambda)              (None, 10, 42)            0\r\n_________________________________________________________________\r\npermute (Permute)            (None, 42, 10)            0\r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n#### Traceback for (B)+(E) and (B)+(F):\r\n```\r\nTraceback (most recent call last):\r\n  File \"minimal.py\", line 32, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 374, in convert\r\n    dump_graphviz_video=self.dump_graphviz_video)\r\n  File \"/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 246, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 106, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-09-06 10:40:25.228797: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 5 arrays (0 quantized)\\n\r\n2018-09-06 10:40:25.228949: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 5 arrays (0 quantized)\\n\r\n2018-09-06 10:40:25.229050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)\\n\r\n2018-09-06 10:40:25.229088: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1402] Check failed: axis < input_shape.dimensions_count() (1211997096 vs. 4)\\n\r\nAborted (core dumped)\\n'\r\nNone\r\n```\r\nThe number 1211997096 is always randomly different and even can be negative.\r\n\r\n#### Traceback for (C)+(E), (D)+(E), (C)+(F) and (D)+(F):\r\n```\r\nTraceback (most recent call last):\r\n  File \"minimal.py\", line 32, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 374, in convert\r\n    dump_graphviz_video=self.dump_graphviz_video)\r\n  File \"/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 246, in toco_convert\r\n    input_data.SerializeToString())\r\n  File \"/usr/stud/staab/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 106, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-09-06 10:44:46.550688: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 4 arrays (0 quantized)\\n\r\n2018-09-06 10:44:46.550805: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 4 arrays (0 quantized)\\n\r\n2018-09-06 10:44:46.550870: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1395] Check failed: perm.size() == input_shape.dimensions_count() (3 vs. 4)Transpose permutation input permute/transpose/perm must be same length as input dimensions\\n\r\nAborted (core dumped)\\n'\r\nNone\r\n```\r\n\r\n#### Minimum Example Code with Workaround:\r\n```python\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.models import save_model\r\n\r\nfrom tensorflow.keras.layers import Input, Lambda, Permute, Reshape, concatenate\r\nfrom tensorflow.keras.backend import permute_dimensions\r\nfrom tensorflow.keras.backend import squeeze as b_squeeze\r\nfrom tensorflow import reshape, squeeze, split, expand_dims\r\n\r\nfrom tensorflow.contrib import lite\r\n\r\n\r\ninput = Input(shape=(10, 42, 1))\r\n\r\n# transform = Reshape((-1, 10, 42))(input)\t\t\t\t\t# (A)\r\ntransform = Lambda(lambda x: reshape(x, (-1, 10, 42)))(input)\t\t\t# (B)\r\n# transform = Lambda(lambda x: squeeze(x, axis=-1))(input)\t\t\t# (C)\r\n# transform = Lambda(lambda x: b_squeeze(x, axis=-1))(input)\t\t\t# (D)\r\n\r\n# Emulated permute to come around a bug: split, concatenate, reshape\r\n#------------------------------------------------------------------------------\r\nsplit_p = Lambda(lambda x: split(expand_dims(x, axis=-1), num_or_size_splits=10,\r\n                                                            axis=1))(transform)\r\nmerge_p = concatenate(split_p)\r\nsqueeze_p = Lambda(lambda x: reshape(x, (-1, 42, 10)))(merge_p)\r\n#------------------------------------------------------------------------------\r\n\r\nkeras_model = Model(inputs=[input], outputs=[squeeze_p])\r\nkeras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n\r\n\r\nprint(keras_model.summary())\r\nsave_model(model=keras_model, filepath=\"keras_model.hdf5\",\r\n\toverwrite=True, include_optimizer=True)\r\n\r\nconverter = lite.TocoConverter.from_keras_model_file(\"keras_model.hdf5\")\r\ntflite_model = converter.convert()\r\nwith open(\"tflite_model.tflite\", \"wb\") as f: f.write(tflite_model)\r\n```", "comments": ["In propagate_fixed_sizes.cc:\r\nhttps://github.com/tensorflow/tensorflow/blob/54a01684b1cedaa93272aba4169da845155aae9b/tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc#L1713\r\n\r\nWe haven't yet checked for negative axis values. In your example, you passed axis=-1 to squeeze function. A walk-around might be using this instead:\r\ntransform = Lambda(lambda x: squeeze(x, axis=3))(input)\t\t\t# (C)\r\n\r\nI'm working on a fix of this issue right now.", "Still reproduces with permute layer.\r\n", "Which TF lite source code are you using? Could you download the latest source code?", "> Which TF lite source code are you using? Could you download the latest source code?\r\n\r\nCan i use nightly build?\r\n", "Sure, you can use the nightly build to verify if this fixes for you.", "I install lastest version (try both : \"pip install tensorflow==1.14.1rc1\" and \"pip install tf-nightly\") and reproduce the code \r\n\r\n```\r\nimport tensorflow\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.models import save_model\r\n\r\nfrom tensorflow.keras.layers import Input, Lambda, Permute, Reshape\r\nfrom tensorflow.keras.backend import permute_dimensions\r\nfrom tensorflow.keras.backend import squeeze as b_squeeze\r\nfrom tensorflow import reshape, squeeze\r\n\r\nfrom tensorflow import lite\r\n\r\nprint(tensorflow.__version__)\r\n\r\ninput = Input(shape=(10, 42, 1))\r\ntransform = Lambda(lambda x: reshape(x, (-1, 10, 42)))(input)\r\n\r\npermute = Permute((2,1))(transform)\r\n\r\nkeras_model = Model(inputs=[input], outputs=[permute])\r\nkeras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n\r\n\r\nprint(keras_model.summary())\r\nsave_model(model=keras_model, filepath=\"keras_model.hdf5\",\r\n\toverwrite=True, include_optimizer=True)\r\n\r\nconverter = lite.TocoConverter.from_keras_model_file(\"keras_model.hdf5\")\r\ntflite_model = converter.convert()\r\nwith open(\"tflite_model.tflite\", \"wb\") as f: f.write(tflite_model)\r\n```\r\n\r\nstill get error:\r\n```\r\n2019-06-11 19:57:49.674155: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2 operators, 5 arrays (0 quantized)\r\n2019-06-11 19:57:49.674215: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2 operators, 5 arrays (0 quantized)\r\n2019-06-11 19:57:49.674253: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (0 quantized)\r\n2019-06-11 19:57:49.674269: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1812] Check failed: axis >= 0 (-14836872 vs. 0)\r\nFatal Python error: Aborted\r\n```\r\n\r\ntensorflow.__version__ = 1.14.1-dev20190611", "I have the same error with permutation when converting. Are there any solutions of this problem? Why is this happening?", "Thanks for the feedback. I can reproduce this issue with the tf-nightly builds, I will take a look on this issue soon.", "@haozha111, I have the same problem when try to convert `tf.keras.applications.MobileNetV2` in TFLite in tensorflow 2.0 beta. Do we have any workaround for this now?", "Are there any fixes? If not, will this bug be fixed in the near future?\r\n\r\nUpdate: I checked this code on tf-nightly build, conversion works, thanks. ", "Is there an update for this?\r\n\r\nI had this problem in TF 1.12, managed to solve it by modifying my inputs to remove a transpose. Updating to TF 1.15.0 brought the issue back and I haven't figured out a workaround.\r\n\r\nIn my environment (Anaconda `tensorflow-gpu 1.15.0`, Ubuntu 18.04), I can reproduce both the problem and workaround in the original post (except changing the `lite` import from contrib).", "hey, sorry for late reply.\r\n\r\nWe will deprecate TOCO soon in favor of the new MLIR converter. Could you download tf-nightly and then enable the new converter via `converter.experimental_new_converter = True`. hope the new converter could solve this issue. \r\n\r\nlet me know if the issue still persists.", "I got the same problem,anybody solved this problem?", "I've bumped into this problem a few times. \r\n\r\nIf it's possible for you to upgrade to TF 2, install the TF 2.2-rc2 and use the new converter. That seems to be the real solution. Performance is better with the new runtime as well.\r\n\r\nIf that's not a reasonable solution, randomly rearranging any reshaping operations until it works is the only way that has worked for me. ", "This is fixed with latest tf-nightly 2.4.0-dev20200730 version \r\nSee [github gist](https://colab.research.google.com/gist/ymodak/0911752c0577873ff699e18eb0f85faa/github_issue22109.ipynb) for reference.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 22107, "title": "Add float16 support for CTCLoss", "body": "This fix tries to address the issue raised in #22096 where there was no float16 support for CTCLoss.\r\n\r\nThis fix added template to CTCLoss, and registered the op in tensorflow/core/ops/ctc_ops.cc.\r\n\r\nThis fix fixes #22096.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Wow, this was fast. Thanks @yongtang !", "Do you not need to update the [ops.pbtxt](https://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/core/ops/ops.pbtxt#L3568) file? ", "See my comments in that issue.", "Reopen if there's a good argument for float16 CTC loss on CPU."]}, {"number": 22106, "title": "Toco/TFLite_Convert for TFLite Problem", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Pixel 1\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9.0 (commit r1.9)\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n1.\r\n```bazel run -c opt tensorflow/python/tools/optimize_for_inference -- --input=$ORIGINAL_PB  --output=$STRIPPED_PB --frozen_graph=True --input_names=Preprocessor/sub --output_names=concat,concat_1 --alsologtostderr```\r\n\r\n2.\r\n```bazel run tensorflow/contrib/lite/toco:toco -- --input_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/new_model.tflite  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --inference_type=QUANTIZED_UINT8 --logtostderr --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_values=127 --allow_custom_opps```\r\n\r\nor\r\n\r\n```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps```\r\n(which fails)\r\n\r\nor\r\n\r\n```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --input_shapes=1,300,300,3```\r\n(which succeeds)\r\n\r\n3. Change TF_OD_API_MODEL_FILE and append new file to the assets list in BUILD\r\n\r\n4.\r\n```bazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo```\r\n\r\n5.\r\n```adb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk```\r\n\r\n6. Open TFL Detect\r\n\r\n### Describe the problem\r\nI'm attempting to import [ssd_mobilenet_v1](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz), [ssd_mobilenet_v2](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz) and [ssdlite](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) from the (model zoo)[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md] into the TFLite Android example. Ultimately I'm aiming to retrain either the ssdlite or ssd_mobilenet_v2 models, but for right now all models I use trigger runtime errors. All of the errors imply that the models are changed by the `optimize_for_inference` and `toco`/`tflite_convert` commands in a way that makes them incompatible with r1.9.\r\n\r\nNow, it's most likely that my command for `toco`/`tflite_convert` are to blame, but since these commands seem to be well formed I'm elevating this to github.\r\n\r\n### Source code / logs\r\nFirstly, according to (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md] we're only supposed to use `tflite_convert` once we're in r1.9. When I try to actually specify all of the fields that the command has in the help (aka the tflite_convert command I put in above) I get the following log and no file is produced:\r\n\r\n```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps\r\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n INFO: Analysed target //tensorflow/contrib/lite/python:tflite_convert (0 packages loaded).\r\n INFO: Found 1 target...\r\n Target //tensorflow/contrib/lite/python:tflite_convert up-to-date:\r\n   bazel-bin/tensorflow/contrib/lite/python/tflite_convert\r\n INFO: Elapsed time: 0.254s, Critical Path: 0.00s\r\n INFO: 0 processes.\r\n INFO: Build completed successfully, 1 total action\r\n INFO: Running command line: bazel-bin/tensorflow/contrib/lite/python/tflite_convert '--graph_def_file=/home/bryan/Downloads/ssd_mobilenet_v1_coco_2018_01_28/stripped' '--output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite' '--input_arrays=Preprocessor/sub' '--output_arrays=concat,concat_1' '--output_format=TFLITE' '--input_shapes=1,300,3INFO: Build completed successfully, 1 total action\r\n /home/bryan/.local/lib/python2.7/site-packages/scipy/__init__.py:114: UserWarning: Numpy 1.8.2 or above is recommended for this version of scipy (detected version 1.8.0)\r\n   UserWarning)\r\n usage: tflite_convert.py [-h] --output_file OUTPUT_FILE\r\n                          (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)\r\n                          [--output_format {TFLITE,GRAPHVIZ_DOT}]\r\n                          [--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n                          [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\r\n                          [--input_arrays INPUT_ARRAYS]\r\n                          [--input_shapes INPUT_SHAPES]\r\n                          [--output_arrays OUTPUT_ARRAYS]\r\n                          [--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n                          [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\r\n                          [--std_dev_values STD_DEV_VALUES]\r\n                          [--mean_values MEAN_VALUES]\r\n                          [--default_ranges_min DEFAULT_RANGES_MIN]\r\n                          [--default_ranges_max DEFAULT_RANGES_MAX]\r\n                          [--drop_control_dependency DROP_CONTROL_DEPENDENCY]\r\n                          [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]\r\n                          [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]\r\n                          [--allow_custom_ops ALLOW_CUSTOM_OPS]\r\n tflite_convert.py: error:\r\n```\r\n\r\nWhen I strip the tflite_convert params to just include the bare minimum (graph_def_file, output_file, input_arrays, output_arrays, input_shapes) it does create an unquantized tflite model. When I load an unquantized tflite model generated with either command, TFL Detect exits with the following log:\r\n\r\n```\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: FATAL EXCEPTION: inference\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 25024\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: java.lang.IllegalArgumentException: Output error: Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4].\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Tensor.copyTo(Tensor.java:44)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:156)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.handleCallback(Handler.java:873)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:99)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:193)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:65)\r\n09-06 00:00:51.049   914  2995 W ActivityManager:   Force finishing activity org.tensorflow.lite.demo/org.tensorflow.demo.DetectorActivity\r\n```\r\n\r\nThe closest I've found as a solution is in (this stackoverflow page)[https://stackoverflow.com/questions/50388330/java-lang-illegalargumentexception-output-error-shape-of-output-target-1-191] which suggests modifying the `TFLiteObjectDetectionAPIModel` itself (which runs into problems when you get similar errors on the outputClassification array).\r\n\r\nIf we use a quantized model, it crashes with this error:\r\n```\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: java.lang.IllegalArgumentException: Input error: DataType (1) of input data does not match with the DataType (3) of model inputs.\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:123)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:144)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\r\n```\r\n\r\nGiven similar error messages in (this test)[https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/contrib/lite/java/src/test/java/org/tensorflow/lite/NativeInterpreterWrapperTest.java#L228] and how commits after r1.9 give the `TFLiteObjectDetectionAPIModel` class an `isQuantized` flag this makes me think that r1.9 may not support quantization. If so, is there a definitive source for this? There are several sources that are imperfect in different ways (for the most official sources (fixed point quantization)[https://www.tensorflow.org/performance/quantization] page seems geared towards classification instead of the object detection, the required output arrays in the (medium page)[https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193] are not found when we run toco, and both of them are supposedly out of date because of (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md]).", "comments": ["I'm not sure how extensible the object detection harness is to using custom models. At the very least you'd need to modify the output tensor shape as dictated by [outputLocations](https://github.com/tensorflow/tensorflow/blob/b390be62ad0514f4fd3347b9db3446c84e08a38e/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L62) in `TFLiteObjectDetectionAPIModel`.\r\n\r\nIf you're mostly interested in classification, have you considered using the [ImageClassifer](https://github.com/tensorflow/tensorflow/blob/b390be62ad0514f4fd3347b9db3446c84e08a38e/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java) sample as a template?", "@jdduke Using a custom model is the next step - currently I'm just testing with the default models on the zoo.  Also good suggestion to look for alternate approaches, I just need to have the location of the object in the frame (so using just a classifier doesn't fit).", "@BryanRansil The default models in the Detection Model Zoo will work. The tflite_convert in the latest nightly build corrected a bug that was on our end.", "Please use the steps we detailed in this [blog post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)", "@achowdhery Good to know they should work - tried the toco command in the blog post on `ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb` and got the error\r\n```\r\n2018-09-06 11:21:59.289737: F tensorflow/contrib/lite/toco/tooling_util.cc:807] Check failed: model.HasArray(output_array) Output array not found: TFLite_Detection_PostProcess\r\n```\r\nI then tried substituting the output array with `concat,concat_1` and got the error\r\n```\r\n2018-09-06 12:04:52.844845: F tensorflow/contrib/lite/toco/tooling_util.cc:1613] Array Preprocessor/map/while/NextIteration, which is an input to the Merge operator producing the output array Preprocessor/map/while/Merge, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\n```\r\nThis matches the error in https://github.com/tensorflow/tensorflow/issues/19014#issuecomment-405696749, but that thread is on custom graphs.\r\n\r\nI then put in values for the two params, getting the following error\r\n```\r\nUnimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Enter) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nAborted (core dumped)\r\n```\r\nAgain, this is all on r1.9. Is quantization unsupported on that release?", "@BryanRansil \r\nCan you please the latest checkpoints: http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\r\nhttp://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_18.tar.gz\r\n?\r\nWe added native op for postprocessing that is available in r1.10. So, for your own model, you do need to go through export_tflite_ssd_graph.py scripts with steps we detailed in this [blog post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193).", "So the latest checkpoints are able to be loaded. Unfortunately they create the following error:\r\n```\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime: java.lang.ArrayIndexOutOfBoundsException: length=1280; index=-2147483647\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at java.util.Vector.elementData(Vector.java:734)\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at java.util.Vector.get(Vector.java:750)\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:214)\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:247)\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:873)\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:193)\r\n09-06 14:52:23.341  8812  8829 E AndroidRuntime:        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n09-06 14:52:23.345   914  1786 W ActivityManager:   Force finishing activity org.tensorflow.lite.demo/org.tensorflow.demo.DetectorActivity\r\n```\r\nI assume this is from using the incorrect labels file, when I use the pets label file in the blog post the index that triggers this error is smaller than in the error above.\r\n\r\nHowever, I also updated tensorflow/models to master and ran export_tflite_ssd_graph.py on ssd_mobilenet_v2. This produced and error saying that `batch_norm_trainable` had been removed (which can be verified by looking at the logs). Therefore is ssd_mobilenet_v2 still supported? If it is, what was the most recent set of commits (for tensorflow/tensorflow and tensorflow/models) that it's been tested on? If not, A. is the model zoo going to be updated with this info and B. how do we determine which models are supported with which versions of TensorFlow?", "@BryanRansil The app TFLiteObjectDetectionAPIModel.java references a COCO dataset pre-trained Mobilenet V1 SSD by default. Did the app build with pre-trained model?\r\nIn the blog post, we provided instructions to finetune weights for Pets dataset and provided the corresponding TF Lite files.\r\nOn your last question, we can double check if V2 SSD is working.", "@achowdhery Thanks for checking V2 SSD! What's a reasonable timeline to expect for that checking to be done?\r\n\r\nThe app builds with the default model that's pointed to in the app, without crashing as I described in the last comment. Otherwise I'll work with the instructions on finetuning the latest models and post when that's successful.", "Nagging Assignee @achowdhery: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\r\n1. I checked out r1.10 for both the models and tensorflow repos\r\n2. Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. **This by far is most likely what solved my problem**, since I was trying different versions of tensorflow as I was dealing with this issue.\r\n3. I trained using a command similar to this:\r\n```bash\r\npython ~/tensorflow/models/research/object_detection/model_main.py \\\r\n       --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n       --model_dir=${MODEL_DIR} \\\r\n       --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n       --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n       --alsologtostderr\r\n```\r\n\r\n4. For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\r\n```bash\r\npython ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n--pipeline_config_path=$CONFIG_FILE \\\r\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n--output_directory=$EXPORT_OUTPUT_DIR \\\r\n--add_postprocessing_op=true\r\n```\r\n\r\n5. Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\r\n```bash\r\n./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=$INPUT_PB_GRAPH \\\r\n  --output_file=$OUTPUT_TFLITE_FILE \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shapes=\"1,300, 300,3\" \\\r\n  --input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n  --std_values=128.0 --mean_values=128.0 \\\r\n  --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\r\n```\r\n\r\n6. Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\r\n```\r\ngit diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\ndiff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\nindex 9eb21de..2cfa7e0 100644\r\n--- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n+++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n@@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n       // in label file and class labels start from 1 to number_of_classes+1,\r\n       // while outputClasses correspond to class index from 0 to number_of_classes\r\n       int labelOffset = 1;\r\n-      recognitions.add(\r\n-          new Recognition(\r\n-              \"\" + i,\r\n-              labels.get((int) outputClasses[0][i] + labelOffset),\r\n-              outputScores[0][i],\r\n-              detection));\r\n+        final int classLabel = (int) outputClasses[0][i] + labelOffset;\r\n+        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\r\n+            recognitions.add(\r\n+                    new Recognition(\r\n+                            \"\" + i,\r\n+                            labels.get(classLabel),\r\n+                            outputScores[0][i],\r\n+                            detection));\r\n+        }\r\n     }\r\n     Trace.endSection(); // \"recognizeImage\"\r\n     return recognitions;\r\n   }\r\n \r\n+  private boolean inRange(float number, float max, float min) {\r\n+    return number < max && number >= min;\r\n+  }\r\n+\r\n```\r\n\r\nAnd then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!", "I was getting an issue on \r\n26829-27149/org.tensorflow.lite.demo W/System.err: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648\r\n\r\n1) i went into C:\\Users\\username\\tensorflow\\tensorflow\\lite\\examples\\android in which i loaded it into android studio \r\n\r\n2) I made the necessary changes in the DetectorActivity.java file=\r\n     2a) private static final String TF_OD_API_MODEL_FILE = \"detect_1.tflite\"; ## the model is converted into an tflite file\r\n      2b)   private static final String TF_OD_API_LABELS_FILE = \"label.txt\"; ##the label file for detection\r\n\r\n3)clean the project in android studio\r\n\r\n4) rebuild the project\r\n5) run it in an android device by intsalling the apks\r\n\r\nHere in the log console i am finding the java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648\r\n", "> I was getting an issue on\r\n> 26829-27149/org.tensorflow.lite.demo W/System.err: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648\r\n\r\n@aravindchaluvadi This error is what step 6 above deals with. It comes from the lines\r\n\r\n>      recognitions.add(\r\n>          new Recognition(\r\n>              \"\" + i,\r\n>              labels.get((int) outputClasses[0][i] + labelOffset),\r\n>              outputScores[0][i],\r\n>              detection));\r\n\r\nso I put the guards in (the whole \"if (inRange...) {\" part) and didn't get this error again.", "Even if I prevent this error by using inRange(), the detection results are full of false positives. Any one can explain why this would produce weirdly out of bound indexes.", "why do i get an error saying `Specified output array \u201cTFlite_Detection_PostProcess\u201d is not produced by any op in this graph\" even though it exist in graph` eventhough it exist in my graph as specified by this? \r\n![image](https://user-images.githubusercontent.com/40930782/56849180-ddf36b00-6923-11e9-8a92-72bb4797c528.png) @achowdhery  @BryanRansil \r\n", "> why do i get an error saying `Specified output array \u201cTFlite_Detection_PostProcess\u201d is not produced by any op in this graph\" even though it exist in graph` eventhough it exist in my graph as specified by this?\r\n> ![image](https://user-images.githubusercontent.com/40930782/56849180-ddf36b00-6923-11e9-8a92-72bb4797c528.png) @achowdhery @BryanRansil\r\n I have exactly the same problem and I couldn't find any solution for that \r\n", "While following the instructions [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md), I was getting this error when trying to run the bazel command: `Specified output array \"'TFLite_Detection_PostProcess'\" is not produced by any op in this graph`\r\n\r\nI was able to resolve it by removing the ' characters around each of the TFLite_Detection_PostProcess entries in output_arrays portion of the command, like so:\r\n\r\n```\r\n --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\r\n```\r\n\r\nI am doing this on Windows, so it may just be a weird way of how the command line interprets string characters.\r\n\r\n", "Removing ' characters like @EdjeElectronics said and, depending on which model I'm trying to convert via tflite, using `--inference_type=FLOAT` instead of `--inference_type=QUANTIZED_UINT8` did the trick for me.", "> So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\r\n> \r\n> 1. I checked out r1.10 for both the models and tensorflow repos\r\n> 2. Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. **This by far is most likely what solved my problem**, since I was trying different versions of tensorflow as I was dealing with this issue.\r\n> 3. I trained using a command similar to this:\r\n> \r\n> ```shell\r\n> python ~/tensorflow/models/research/object_detection/model_main.py \\\r\n>        --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n>        --model_dir=${MODEL_DIR} \\\r\n>        --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n>        --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n>        --alsologtostderr\r\n> ```\r\n> \r\n> 1. For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\r\n> \r\n> ```shell\r\n> python ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n> --pipeline_config_path=$CONFIG_FILE \\\r\n> --trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n> --output_directory=$EXPORT_OUTPUT_DIR \\\r\n> --add_postprocessing_op=true\r\n> ```\r\n> \r\n> 1. Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\r\n> \r\n> ```shell\r\n> ./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n>   --input_file=$INPUT_PB_GRAPH \\\r\n>   --output_file=$OUTPUT_TFLITE_FILE \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n>   --inference_type=QUANTIZED_UINT8 \\\r\n>   --input_shapes=\"1,300, 300,3\" \\\r\n>   --input_arrays=normalized_input_image_tensor \\\r\n> --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n>   --std_values=128.0 --mean_values=128.0 \\\r\n>   --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\r\n> ```\r\n> \r\n> 1. Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\r\n> \r\n> ```\r\n> git diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> diff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> index 9eb21de..2cfa7e0 100644\r\n> --- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> +++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> @@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n>        // in label file and class labels start from 1 to number_of_classes+1,\r\n>        // while outputClasses correspond to class index from 0 to number_of_classes\r\n>        int labelOffset = 1;\r\n> -      recognitions.add(\r\n> -          new Recognition(\r\n> -              \"\" + i,\r\n> -              labels.get((int) outputClasses[0][i] + labelOffset),\r\n> -              outputScores[0][i],\r\n> -              detection));\r\n> +        final int classLabel = (int) outputClasses[0][i] + labelOffset;\r\n> +        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\r\n> +            recognitions.add(\r\n> +                    new Recognition(\r\n> +                            \"\" + i,\r\n> +                            labels.get(classLabel),\r\n> +                            outputScores[0][i],\r\n> +                            detection));\r\n> +        }\r\n>      }\r\n>      Trace.endSection(); // \"recognizeImage\"\r\n>      return recognitions;\r\n>    }\r\n>  \r\n> +  private boolean inRange(float number, float max, float min) {\r\n> +    return number < max && number >= min;\r\n> +  }\r\n> +\r\n> ```\r\n> \r\n> And then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!\r\n\r\nThank you Bryan! Spend me 2 days to find your post! I tried many solutions but just cannot get over the [1,10,4] input tensor of the Tensorflow lite Android OD example.\r\nFor the SSD_MobileNet_quatiazed_v2_coco model (that is my case), it should be ' int labelOffset = 0'.\r\nAnd, good to see you here Evan. \ud83d\udcaf @EdjeElectronics ", "@innovimax  How to get this picture of the graph?", "> Even if I prevent this error by using inRange(), the detection results are full of false positives. Any one can explain why this would produce weirdly out of bound indexes.\r\n\r\nFrom inspecting class index predictions coming out of MobileNet V2, it looks like the values start off as whole numbers (i.e. floats that are actually integers), and then turn into true floats. This makes me think that the prediction arrays are being initialized (or overwritten) with garbage, and then being filled back in with non-background detections, with unused predictions (since you have to specify a fixed number of predictions to return) being left as garbage floats. So, in addition to checking if the output class is in range, I also check if it is a whole number. This removed the noise for me.", "> @innovimax How to get this picture of the graph?\r\n\r\ntry Netron", "> > So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\r\n> > \r\n> > 1. I checked out r1.10 for both the models and tensorflow repos\r\n> > 2. Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. **This by far is most likely what solved my problem**, since I was trying different versions of tensorflow as I was dealing with this issue.\r\n> > 3. I trained using a command similar to this:\r\n> > \r\n> > ```shell\r\n> > python ~/tensorflow/models/research/object_detection/model_main.py \\\r\n> >        --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n> >        --model_dir=${MODEL_DIR} \\\r\n> >        --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n> >        --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n> >        --alsologtostderr\r\n> > ```\r\n> > \r\n> > \r\n> > \r\n> > 1. For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\r\n> > \r\n> > ```shell\r\n> > python ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n> > --pipeline_config_path=$CONFIG_FILE \\\r\n> > --trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n> > --output_directory=$EXPORT_OUTPUT_DIR \\\r\n> > --add_postprocessing_op=true\r\n> > ```\r\n> > \r\n> > \r\n> > \r\n> > 1. Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\r\n> > \r\n> > ```shell\r\n> > ./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n> >   --input_file=$INPUT_PB_GRAPH \\\r\n> >   --output_file=$OUTPUT_TFLITE_FILE \\\r\n> >   --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n> >   --inference_type=QUANTIZED_UINT8 \\\r\n> >   --input_shapes=\"1,300, 300,3\" \\\r\n> >   --input_arrays=normalized_input_image_tensor \\\r\n> > --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n> >   --std_values=128.0 --mean_values=128.0 \\\r\n> >   --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\r\n> > ```\r\n> > \r\n> > \r\n> > \r\n> > 1. Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\r\n> > \r\n> > ```\r\n> > git diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > diff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > index 9eb21de..2cfa7e0 100644\r\n> > --- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > +++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > @@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n> >        // in label file and class labels start from 1 to number_of_classes+1,\r\n> >        // while outputClasses correspond to class index from 0 to number_of_classes\r\n> >        int labelOffset = 1;\r\n> > -      recognitions.add(\r\n> > -          new Recognition(\r\n> > -              \"\" + i,\r\n> > -              labels.get((int) outputClasses[0][i] + labelOffset),\r\n> > -              outputScores[0][i],\r\n> > -              detection));\r\n> > +        final int classLabel = (int) outputClasses[0][i] + labelOffset;\r\n> > +        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\r\n> > +            recognitions.add(\r\n> > +                    new Recognition(\r\n> > +                            \"\" + i,\r\n> > +                            labels.get(classLabel),\r\n> > +                            outputScores[0][i],\r\n> > +                            detection));\r\n> > +        }\r\n> >      }\r\n> >      Trace.endSection(); // \"recognizeImage\"\r\n> >      return recognitions;\r\n> >    }\r\n> >  \r\n> > +  private boolean inRange(float number, float max, float min) {\r\n> > +    return number < max && number >= min;\r\n> > +  }\r\n> > +\r\n> > ```\r\n> > \r\n> > \r\n> > And then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!\r\n> \r\n> Thank you Bryan! Spend me 2 days to find your post! I tried many solutions but just cannot get over the [1,10,4] input tensor of the Tensorflow lite Android OD example.\r\n> For the SSD_MobileNet_quatiazed_v2_coco model (that is my case), it should be ' int labelOffset = 0'.\r\n> And, good to see you here Evan. \ud83d\udcaf @EdjeElectronics\r\n\r\nSo, may i ask which step solves the problem?", "> > > So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\r\n> > > \r\n> > > 1. I checked out r1.10 for both the models and tensorflow repos\r\n> > > 2. Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. **This by far is most likely what solved my problem**, since I was trying different versions of tensorflow as I was dealing with this issue.\r\n> > > 3. I trained using a command similar to this:\r\n> > > \r\n> > > ```shell\r\n> > > python ~/tensorflow/models/research/object_detection/model_main.py \\\r\n> > >        --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n> > >        --model_dir=${MODEL_DIR} \\\r\n> > >        --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n> > >        --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n> > >        --alsologtostderr\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > \r\n> > > 1. For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\r\n> > > \r\n> > > ```shell\r\n> > > python ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n> > > --pipeline_config_path=$CONFIG_FILE \\\r\n> > > --trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n> > > --output_directory=$EXPORT_OUTPUT_DIR \\\r\n> > > --add_postprocessing_op=true\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > \r\n> > > 1. Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\r\n> > > \r\n> > > ```shell\r\n> > > ./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n> > >   --input_file=$INPUT_PB_GRAPH \\\r\n> > >   --output_file=$OUTPUT_TFLITE_FILE \\\r\n> > >   --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n> > >   --inference_type=QUANTIZED_UINT8 \\\r\n> > >   --input_shapes=\"1,300, 300,3\" \\\r\n> > >   --input_arrays=normalized_input_image_tensor \\\r\n> > > --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n> > >   --std_values=128.0 --mean_values=128.0 \\\r\n> > >   --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > \r\n> > > 1. Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\r\n> > > \r\n> > > ```\r\n> > > git diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > > diff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > > index 9eb21de..2cfa7e0 100644\r\n> > > --- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > > +++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> > > @@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n> > >        // in label file and class labels start from 1 to number_of_classes+1,\r\n> > >        // while outputClasses correspond to class index from 0 to number_of_classes\r\n> > >        int labelOffset = 1;\r\n> > > -      recognitions.add(\r\n> > > -          new Recognition(\r\n> > > -              \"\" + i,\r\n> > > -              labels.get((int) outputClasses[0][i] + labelOffset),\r\n> > > -              outputScores[0][i],\r\n> > > -              detection));\r\n> > > +        final int classLabel = (int) outputClasses[0][i] + labelOffset;\r\n> > > +        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\r\n> > > +            recognitions.add(\r\n> > > +                    new Recognition(\r\n> > > +                            \"\" + i,\r\n> > > +                            labels.get(classLabel),\r\n> > > +                            outputScores[0][i],\r\n> > > +                            detection));\r\n> > > +        }\r\n> > >      }\r\n> > >      Trace.endSection(); // \"recognizeImage\"\r\n> > >      return recognitions;\r\n> > >    }\r\n> > >  \r\n> > > +  private boolean inRange(float number, float max, float min) {\r\n> > > +    return number < max && number >= min;\r\n> > > +  }\r\n> > > +\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > And then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!\r\n> > \r\n> > \r\n> > Thank you Bryan! Spend me 2 days to find your post! I tried many solutions but just cannot get over the [1,10,4] input tensor of the Tensorflow lite Android OD example.\r\n> > For the SSD_MobileNet_quatiazed_v2_coco model (that is my case), it should be ' int labelOffset = 0'.\r\n> > And, good to see you here Evan. \ud83d\udcaf @EdjeElectronics\r\n> \r\n> So, may i ask which step solves the problem?\r\n\r\nStep 6.  changes to compile or get past runtime errors and other behaviour", "These commands worked for me for `ssd_mobilenet_v2_coco_2018_03_29` under Windows 10 (Tensorflow 1.13)\r\n\r\n1.\r\n`python export_tflite_ssd_graph.py --pipeline_config_path training/ssd_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-XXXX --output_directory inference_graph_mobile --add_postprocessing_op=true`\r\n\r\nMake sure to replace model.ckpt-XXXX\r\n\r\n2.\r\n`tflite_convert --graph_def_file=inference_graph_mobile/tflite_graph.pb --output_file=tflite/detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3  --inference_type=FLOAT --allow_custom_ops`", "@HaFred  \r\nI'm also same problem while running segmentation model.\r\n\r\nI don't have .txt file for this. Model is running good on IoS app but in android it gives exception\r\n`java.lang.ArrayIndexOutOfBoundsException: length=66049; index=66049`\r\n\r\nAny suggestion how I resolve this problem ?\r\n ", "> @HaFred\r\n> I'm also same problem while running segmentation model.\r\n> \r\n> I don't have .txt file for this. Model is running good on IoS app but in android it gives exception\r\n> `java.lang.ArrayIndexOutOfBoundsException: length=66049; index=66049`\r\n> \r\n> Any suggestion how I resolve this problem ?\r\n\r\nSeems like it is not the same issue we encountered here. The answer I quoted above is to solve `shape does not match` problem.", "> So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\r\n> \r\n> 1. I checked out r1.10 for both the models and tensorflow repos\r\n> 2. Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. **This by far is most likely what solved my problem**, since I was trying different versions of tensorflow as I was dealing with this issue.\r\n> 3. I trained using a command similar to this:\r\n> \r\n> ```shell\r\n> python ~/tensorflow/models/research/object_detection/model_main.py \\\r\n>        --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n>        --model_dir=${MODEL_DIR} \\\r\n>        --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n>        --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n>        --alsologtostderr\r\n> ```\r\n> \r\n> 1. For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\r\n> \r\n> ```shell\r\n> python ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n> --pipeline_config_path=$CONFIG_FILE \\\r\n> --trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n> --output_directory=$EXPORT_OUTPUT_DIR \\\r\n> --add_postprocessing_op=true\r\n> ```\r\n> \r\n> 1. Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\r\n> \r\n> ```shell\r\n> ./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n>   --input_file=$INPUT_PB_GRAPH \\\r\n>   --output_file=$OUTPUT_TFLITE_FILE \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n>   --inference_type=QUANTIZED_UINT8 \\\r\n>   --input_shapes=\"1,300, 300,3\" \\\r\n>   --input_arrays=normalized_input_image_tensor \\\r\n> --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n>   --std_values=128.0 --mean_values=128.0 \\\r\n>   --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\r\n> ```\r\n> \r\n> 1. Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\r\n> \r\n> ```\r\n> git diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> diff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> index 9eb21de..2cfa7e0 100644\r\n> --- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> +++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> @@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n>        // in label file and class labels start from 1 to number_of_classes+1,\r\n>        // while outputClasses correspond to class index from 0 to number_of_classes\r\n>        int labelOffset = 1;\r\n> -      recognitions.add(\r\n> -          new Recognition(\r\n> -              \"\" + i,\r\n> -              labels.get((int) outputClasses[0][i] + labelOffset),\r\n> -              outputScores[0][i],\r\n> -              detection));\r\n> +        final int classLabel = (int) outputClasses[0][i] + labelOffset;\r\n> +        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\r\n> +            recognitions.add(\r\n> +                    new Recognition(\r\n> +                            \"\" + i,\r\n> +                            labels.get(classLabel),\r\n> +                            outputScores[0][i],\r\n> +                            detection));\r\n> +        }\r\n>      }\r\n>      Trace.endSection(); // \"recognizeImage\"\r\n>      return recognitions;\r\n>    }\r\n>  \r\n> +  private boolean inRange(float number, float max, float min) {\r\n> +    return number < max && number >= min;\r\n> +  }\r\n> +\r\n> ```\r\n> \r\n> And then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!\r\n\r\nRight but in other github issue someone suggested that this Step 6 solve the problem statement `java.lang.ArrayIndexOutOfBoundsException: length=66049; index=66049` \r\n\r\nAnd make sense because Step 6 solving in indexing problem which link with the label file. But in my case I don't have label file. \r\nCode is working fine with other models and these models also have only .tflite file not .txt label file.\r\nAnd the model which I'm using working fine with the IoS app.  ", "I have not played with iOS development so there is nothing I could help on this. Maybe you could try doing the tensorflow lite Android flow from scartch\n\nFrederick\n________________________________\nFrom: Ehtasha <notifications@github.com>\nSent: Wednesday, February 26, 2020 8:11:29 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Frederick HONG <haurunis@outlook.com>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Toco/TFLite_Convert for TFLite Problem (#22106)\n\n\nSo as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\n\n  1.  I checked out r1.10 for both the models and tensorflow repos\n  2.  Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. This by far is most likely what solved my problem, since I was trying different versions of tensorflow as I was dealing with this issue.\n  3.  I trained using a command similar to this:\n\npython ~/tensorflow/models/research/object_detection/model_main.py \\\n       --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\n       --model_dir=${MODEL_DIR} \\\n       --num_train_steps=${NUM_TRAIN_STEPS} \\\n       --num_eval_steps=${NUM_EVAL_STEPS} \\\n       --alsologtostderr\n\n  1.  For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\n\npython ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\n--pipeline_config_path=$CONFIG_FILE \\\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\n--output_directory=$EXPORT_OUTPUT_DIR \\\n--add_postprocessing_op=true\n\n  1.  Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\n\n./bazel-bin/tensorflow/contrib/lite/toco/toco \\\n  --input_file=$INPUT_PB_GRAPH \\\n  --output_file=$OUTPUT_TFLITE_FILE \\\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\n  --inference_type=QUANTIZED_UINT8 \\\n  --input_shapes=\"1,300, 300,3\" \\\n  --input_arrays=normalized_input_image_tensor \\\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n  --std_values=128.0 --mean_values=128.0 \\\n  --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\n\n  1.  Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\n\ngit diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\ndiff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\nindex 9eb21de..2cfa7e0 100644\n--- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\n+++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\n@@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\n       // in label file and class labels start from 1 to number_of_classes+1,\n       // while outputClasses correspond to class index from 0 to number_of_classes\n       int labelOffset = 1;\n-      recognitions.add(\n-          new Recognition(\n-              \"\" + i,\n-              labels.get((int) outputClasses[0][i] + labelOffset),\n-              outputScores[0][i],\n-              detection));\n+        final int classLabel = (int) outputClasses[0][i] + labelOffset;\n+        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\n+            recognitions.add(\n+                    new Recognition(\n+                            \"\" + i,\n+                            labels.get(classLabel),\n+                            outputScores[0][i],\n+                            detection));\n+        }\n     }\n     Trace.endSection(); // \"recognizeImage\"\n     return recognitions;\n   }\n\n+  private boolean inRange(float number, float max, float min) {\n+    return number < max && number >= min;\n+  }\n+\n\n\nAnd then I was able to run the tflite example on my phone! Thanks to @achowdhery<https://github.com/achowdhery> and @jdduke<https://github.com/jdduke> for responding and the help!\n\nRight but in other github issue someone suggested that Step 6 solve the problem statement java.lang.ArrayIndexOutOfBoundsException: length=66049; index=66049\n\nAnd make sense because Step 6 solving in indexing problem which link with the label file. But in my case I don't have label file.\nCode is working fine with other models and these models also have only .tflite file not .txt label file.\nAnd the model which I'm using working fine with the IoS app.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/22106?email_source=notifications&email_token=ADKX3ARF2FBWEBQELN22E5LREZL7DA5CNFSM4FTRNUE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM77PYI#issuecomment-591394785>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ADKX3AWDEX645YO4GYDJNE3REZL7DANCNFSM4FTRNUEQ>.\n", "Okey", "@BryanRansil I am trying to convert yolov3-tiny to tflite how we can achieve it.\r\nI followed your commands for ssd model its working fine.\r\nIt will be helpful for all others as well, if we can successfully convert yolo model to tflite for android applications", "> So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\r\n> \r\n> 1. I checked out r1.10 for both the models and tensorflow repos\r\n> 2. Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. **This by far is most likely what solved my problem**, since I was trying different versions of tensorflow as I was dealing with this issue.\r\n> 3. I trained using a command similar to this:\r\n> \r\n> ```shell\r\n> python ~/tensorflow/models/research/object_detection/model_main.py \\\r\n>        --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n>        --model_dir=${MODEL_DIR} \\\r\n>        --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n>        --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n>        --alsologtostderr\r\n> ```\r\n> \r\n> 1. For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\r\n> \r\n> ```shell\r\n> python ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n> --pipeline_config_path=$CONFIG_FILE \\\r\n> --trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n> --output_directory=$EXPORT_OUTPUT_DIR \\\r\n> --add_postprocessing_op=true\r\n> ```\r\n> \r\n> 1. Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\r\n> \r\n> ```shell\r\n> ./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n>   --input_file=$INPUT_PB_GRAPH \\\r\n>   --output_file=$OUTPUT_TFLITE_FILE \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n>   --inference_type=QUANTIZED_UINT8 \\\r\n>   --input_shapes=\"1,300, 300,3\" \\\r\n>   --input_arrays=normalized_input_image_tensor \\\r\n> --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n>   --std_values=128.0 --mean_values=128.0 \\\r\n>   --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\r\n> ```\r\n> \r\n> 1. Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\r\n> \r\n> ```\r\n> git diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> diff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> index 9eb21de..2cfa7e0 100644\r\n> --- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> +++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n> @@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n>        // in label file and class labels start from 1 to number_of_classes+1,\r\n>        // while outputClasses correspond to class index from 0 to number_of_classes\r\n>        int labelOffset = 1;\r\n> -      recognitions.add(\r\n> -          new Recognition(\r\n> -              \"\" + i,\r\n> -              labels.get((int) outputClasses[0][i] + labelOffset),\r\n> -              outputScores[0][i],\r\n> -              detection));\r\n> +        final int classLabel = (int) outputClasses[0][i] + labelOffset;\r\n> +        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\r\n> +            recognitions.add(\r\n> +                    new Recognition(\r\n> +                            \"\" + i,\r\n> +                            labels.get(classLabel),\r\n> +                            outputScores[0][i],\r\n> +                            detection));\r\n> +        }\r\n>      }\r\n>      Trace.endSection(); // \"recognizeImage\"\r\n>      return recognitions;\r\n>    }\r\n>  \r\n> +  private boolean inRange(float number, float max, float min) {\r\n> +    return number < max && number >= min;\r\n> +  }\r\n> +\r\n> ```\r\n> \r\n> And then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!\r\n\r\n\r\n\r\nHi @BryanRansil  I used step 6 and change the **TFLiteObjectDetectionAPIModel.java** .But I still got the following error:\r\n\r\n> 2020-08-05 16:09:30.781 28332-28442/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n>     Process: org.tensorflow.lite.examples.detection, PID: 28332\r\n>     java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_input_tensor:0) with 3 bytes from a Java Buffer with 1080000 bytes.\r\n>         at org.tensorflow.lite.Tensor.throwIfSrcShapeIsIncompatible(Tensor.java:444)\r\n>         at org.tensorflow.lite.Tensor.setTo(Tensor.java:189)\r\n>         at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)\r\n>         at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:347)\r\n>         at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:196)\r\n>         at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)\r\n>         at android.os.Handler.handleCallback(Handler.java:883)\r\n>         at android.os.Handler.dispatchMessage(Handler.java:100)\r\n>         at android.os.Looper.loop(Looper.java:237)\r\n>         at android.os.HandlerThread.run(HandlerThread.java:67)\r\n\r\n\r\n\r\n**Any help is appreciated.** ", "> Cannot copy to a TensorFlowLite tensor (serving_default_input_tensor:0) with 3 bytes from a Java Buffer with 1080000 bytes.\r\n\r\nWhat is the shape of this tensor in your converted .tflite model? You might need to manually resize it (using `Interpreter.resizeInput()`). It looks like the buffer you're providing doesn't match the expected size for that tensor."]}, {"number": 22105, "title": "[Bug] Sometimes CUDA check failed", "body": "### System information\r\n- **Code**: a keras example (use tf.keras instead) https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\r\n- **OS Platform and Distribution**: Windows server 2016\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version**: 3.6.6\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0\r\n\r\n- **Have I written custom code**: some I/O code to adapt data format, which shouldn't involved\r\n- **Bazel version** : N/A\r\n- **GPU model and memory**: Quadro P600, 2GiB\r\n- **Exact command to reproduce**: python lstm_seq2seq.py\r\n- **Mobile device**: N/A\r\n\r\n### Describe the problem\r\nI try to run the code mentioned above.\r\nIf I run the script successfully, the script will raise a error in next several runs.\r\nKeep retry and I can successfully run it again.\r\n\r\nI only know the check failed, but can't see the error code.\r\nCould you provide some help on it?\r\n\r\nAnother problem: only changing latent_dim from 128 to 256 results in 5e-8 training loss and a bad model. But CPU training gives a good result.\r\n\r\n### Source code / logs\r\nEpoch 1/100\r\n2018-09-06 10:51:48.513539: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-09-06 10:51:48.726860: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: Quadro P600 major: 6 minor: 1 memoryClockRate(GHz): 1.5565\r\npciBusID: 0000:21:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.62GiB\r\n2018-09-06 10:51:48.731423: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-06 10:51:49.568323: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-06 10:51:49.573347: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\r\n2018-09-06 10:51:49.576631: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\r\n2018-09-06 10:51:49.582844: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name:\r\nQuadro P600, pci bus id: 0000:21:00.0, compute capability: 6.1)\r\n2018-09-06 10:51:49.872995: I T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:159] Creating CudaSolver handles for stream 000001DDA2412050\r\n**2018-09-06 10:51:50.120523: F T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\cuda_solvers.cc:94] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.**\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I got the same problem. It's just because I've ran out of GPU memory. Decreasing model parameters and it works again.", "same problem. guess it's memory because it's architecture search ... but it would be cool for this to fail more gracefully", "My colleague had this problem once. Solved by setting `allow_growth=True` when creating session so I'll record it here in case someone need it.\r\n\r\nShe had 2 GPUs in her machine, GPU 0 was empty and GPU 1 had graphics processes such as Xorg. The model trained well on GPU 0 only but displayed such problem when GPU 1 only. "]}, {"number": 22104, "title": "tf.keras.layers.CuDNNGRU in eager mode can cause OOM ", "body": "tensorflow 1.10.1 \r\nCode like below, with inputs dim 1 can be dynamic length.\r\n  \r\n    class Model(keras.Model):\r\n      def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = keras.layers.Embedding(vocab_size, FLAGS.emb_dim)\r\n        self.encode = keras.layers.CuDNNGRU(units=FLAGS.rnn_hidden_size, \r\n                                          return_sequences=True, \r\n                                          return_state=False, \r\n                                          recurrent_initializer='glorot_uniform')\r\n        self.pooling = keras.layers.GlobalMaxPool1D()\r\n        self.logits = keras.layers.Dense(NUM_CLASSES, activation=None)\r\n\r\n      def call(self, inputs, training=False):\r\n        x = inputs.comment\r\n        x = self.embedding(x)\r\n        x = self.encode(x)\r\n        x = self.pooling(x)\r\n        x = self.logits(x)\r\n      return x\r\n\r\n    def calc_loss(model, inputs, training=False):\r\n      y_ = model(inputs, training=training)\r\n      y = inputs.classes\r\n      return tf.losses.sigmoid_cross_entropy(y, y_)   \r\n\r\n\r\n1. Using keras.layers.GRU will not OOM\r\n2. Using keras.layers.CuDNNGRU but not in eager mode will not OOM\r\n3. Using keras.layers.CuDNNGRU + eager mode  will OOM (complain self.embedding OOM after training a few steps)  \r\n\r\n    optimizer.apply_gradients(zip(grads, model.variables))\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 605, in apply_gradients\r\n    update_ops.append(processor.update_op(self, grad))\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 165, in update_op\r\n    g.values, self._v, g.indices)\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 958, in _resource_apply_sparse_duplicate_indices\r\n    return self._resource_apply_sparse(summed_grad, handle, unique_indices)\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 218, in _resource_apply_sparse\r\n    grad, var, indices, self._resource_scatter_add)\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 200, in _apply_sparse_shared\r\n    lr * m_t / (v_sqrt + epsilon_t),\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 850, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 958, in _truediv_python3\r\n    return gen_math_ops.real_div(x, y, name=name)\r\n  File \"/home/gezi/py3env/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5904, in real_div\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[151627,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RealDiv] name: Adam/update_/truediv/\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this since, now it run ok. (The problem is eager mode will use more gpu mem, seems also pytorch use more gpu mem then tf graph mode, so maybe it's by design)"]}, {"number": 22103, "title": "TensorFlow profiler timeline range", "body": "Hello,\r\n\r\nI'm following the tf.profiler.Profiler guide to profile my code.\r\nWhen generating the timelines, all of them except the first one (for the first batch) are scaled in hundreds of years, rather than seconds.\r\n\r\nThere is something happening at year 0, then something going on after 583 years.\r\nNo, my DNN code is not that slow. \r\n![screenshot_2018-09-05_23-19-38](https://user-images.githubusercontent.com/6018251/45128332-5c24b680-b175-11e8-96f6-c5cc9abee2d4.png)\r\nRunning the latest stable version of everything.\r\n\r\nCould you please recommend me a fix?\r\nThank you", "comments": ["Looks like a duplicate of #20750.", "Indeed, apologies. I will close this one.", "The repro in #20750 only works on 2 GPUs somehow. Are you only using 1 GPU? If you can give a small repro with 1 GPU maybe that'll be easier to debug.", "There are two physical GPUs in my machine, but the launch script sets the `CUDA_VISIBLE_DEVICES` environment variable to use only one of them. Will post the code to reproduce the issue later this week.", "Hi @ppwwyyxx, the code is here: https://github.com/georgesterpu/Sigmedia-AVSR\r\nYou only have to set the `profiling` flag to True when instantiating the AVSR class.", "same here after upgrade to CUDA 9.0"]}, {"number": 22102, "title": "raw_rnns not working with xla jit compile", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7.4.1708\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.4.5\r\n- **Bazel version (if compiling from source)**: 0.16\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nTrying to run char_rnn using raw_rnns with LSTMCell using TF estimators on the shakespeare dataset. It trains when I don't use the xla jit compile. When I do try to add the xla jit compile, I run into errors.  \r\n\r\nfirst error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Resource arguments cannot be constant (argument 3)\r\n\tEncapsulateSubgraphsPass failed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"basic_rnn.py\", line 332, in <module>\r\n    classifier.train(input_fn, steps=args.num_steps)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 1145, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 1173, in _train_model_default\r\n    saving_listeners)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 1451, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 583, in run\r\n    run_metadata=run_metadata)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 1059, in run\r\n    run_metadata=run_metadata)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 1150, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 1135, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 1207, in run\r\n    run_metadata=run_metadata)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py\", line 987, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/net/server5/srv/nfs/vishal-data/ws/venv_tf110/lib/python3.4/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Resource arguments cannot be constant (argument 3)\r\n\tEncapsulateSubgraphsPass failed\r\n```\r\nI fixed this by modifying the tensorflow/compiler/tf2xla/const_analysis.cc with\r\n```\r\nStatus BackwardsConstAnalysis(const Graph& g,\r\n         int index;\r\n         status = GetNodeAttr(node->attrs(), \"index\", &index);\r\n         if (!status.ok()) return;\r\n-        compile_time_const_args->at(index) = true;\r\n+       DataType dt;\r\n+       status = GetNodeAttr(node->attrs(), \"T\", &dt);\r\n+       if (!status.ok()) return;\r\n+       if (dt != DT_RESOURCE) {\r\n+         VLOG(1) << \"HIHIH \"<< SummarizeNodeDef(node->def());\r\n+         compile_time_const_args->at(index) = true;\r\n+       }\r\n         return;\r\n       }\r\n       for (const Edge* pred : node->in_edges()) {\r\n``` \r\nI then run into this error:\r\n```\r\n (No registered '_Retval' OpKernel for XLA_CPU_JIT devices compatible with node test_conv_gradients_test_conv_rnn_while_select_1_grad_select_f_acc_0_retval_RetVal = _Retval[T=DT_RESOURCE, index=0](test_conv/gradients/test_conv/rnn/while/Select_1_grad/Select/f_acc)\r\n         (OpKernel was found, but attributes didn't match)\r\n        .  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='GPU'; T in [DT_STRING]\r\n  device='GPU'; T in [DT_RESOURCE]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_BOOL]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_INT8]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_UINT16]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_HALF, DT_UINT32, DT_UINT64]\r\n)\r\n```\r\n\r\nIs there a way for me to fix this? Or for the tensorflow folks to fix this? Or is there an example of using raw_rnns with the xla jit compile that I can look at? ", "comments": ["Variables should be handled specially around XLA compilation blocks, XLA doesn't have variables and instead the interaction with variables are handled by way of a rewrite (e.g., the variable's value is read before the part compiled by XLA and written upon return). The rewritting of resource variable read/use/writes should be taken care of for you by the new xla.compile. Could you try with the xla.compile?", "Thanks for the response. When I tired xla.compile on a VGGNet it took over an hour, so i'm guessing im not using it correctly. Do you have dummy example with how xla.compile is meant to be used? (I followed the comments in the code to use it, but I'm probably providing information I dont need to, in the tensors I am passing.). Also is it not possible to do it without xla.compile? ", "So I did try again, with xla.compile,  and it didnt work, this was my stack trace: \r\n```\r\n2018-10-10 11:33:05.589418: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3311865000 Hz\r\n2018-10-10 11:33:05.591668: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f3600060dd0 executing computations on platform Host. Devices:\r\n2018-10-10 11:33:05.591699: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-10-10 11:33:05.618642: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at xla_ops.cc:290 : Invalid argument: Detected unsupported operations when trying to compile graph cluster_7643397997199028266_f15n_0[] on XLA_CPU_JIT: _Arg (No registered '_Arg' OpKernel for XLA_CPU_JIT devices compatible with node {{node iteratortostringhandle_0_arg}} = _Arg[T=DT_STRING, index=2]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE]\r\n  device='XLA_CPU'; T in [DT_STRING]\r\n  device='XLA_CPU'; T in [DT_RESOURCE]\r\n  device='XLA_CPU'; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\r\n  device='CPU'\r\n  device='GPU'; T in [DT_STRING]\r\n  device='GPU'; T in [DT_RESOURCE]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_BOOL]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_INT8]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_UINT16]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='GPU'; T in [DT_HALF]\r\n){{node iteratortostringhandle_0_arg}}\r\nTraceback (most recent call last):\r\n  File \"/home/vishal/venv_master_xla/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/vishal/venv_master_xla/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/vishal/venv_master_xla/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_7643397997199028266_f15n_0[] on XLA_CPU_JIT: _Arg (No registered '_Arg' OpKernel for XLA_CPU_JIT devices compatible with node {{node iteratortostringhandle_0_arg}} = _Arg[T=DT_STRING, index=2]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE]\r\n  device='XLA_CPU'; T in [DT_STRING]\r\n  device='XLA_CPU'; T in [DT_RESOURCE]\r\n  device='XLA_CPU'; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\r\n  device='CPU'\r\n  device='GPU'; T in [DT_STRING]\r\n  device='GPU'; T in [DT_RESOURCE]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_BOOL]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_INT8]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_UINT16]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='GPU'; T in [DT_HALF]\r\n){{node iteratortostringhandle_0_arg}}\r\n\t [[{{node cluster}} = XlaLaunch[Nresources=9, Targs=[DT_FLOAT, DT_FLOAT, DT_STRING, DT_INT64, DT_INT64, ..., DT_INT32, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_FLOAT], Tconstants=[], Tresults=[DT_FLOAT, DT_INT32, DT_INT32, DT_INT32, DT_INT32, ..., DT_INT32, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], function=cluster_7643397997199028266_f15n_0[], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorGetNext, IteratorGetNext:1, IteratorToStringHandle, batch_size, count, drop_remainder, tensors/component_0, tensors/component_1, test_conv/func/lstm_cell/bias/Initializer/zeros, test_conv/func/lstm_cell/bias/IsInitialized/VarIsInitializedOp, test_conv/func/lstm_cell/bias/Read/ReadVariableOp, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/RandomUniform, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/max, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/min, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/mul, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/shape, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/sub, test_conv/func/lstm_cell/kernel/Initializer/random_uniform, test_conv/func/lstm_cell/kernel/IsInitialized/VarIsInitializedOp, test_conv/func/lstm_cell/kernel/Read/ReadVariableOp, test_conv/rnn_final/final_layer/bias/Initializer/zeros, test_conv/rnn_final/final_layer/bias/IsInitialized/VarIsInitializedOp, test_conv/rnn_final/final_layer/bias/Read/ReadVariableOp, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/RandomUniform, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/max, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/min, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/mul, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/shape, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/sub, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform, test_conv/rnn_final/final_layer/kernel/IsInitialized/VarIsInitializedOp, test_conv/rnn_final/final_layer/kernel/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Initializer/zeros, test_conv/test_conv/func/lstm_cell/bias/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Read_1/ReadVariableOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Initializer/zeros/Const, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Initializer/zeros/shape_as_tensor, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Initializer/zeros, test_conv/test_conv/func/lstm_cell/kernel/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Read_1/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Initializer/zeros, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Read_1/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Initializer/zeros/Const, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Initializer/zeros/shape_as_tensor, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Initializer/zeros, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Read_1/ReadVariableOp, OneShotIterator, test_conv/func/lstm_cell/bias, test_conv/func/lstm_cell/kernel, test_conv/rnn_final/final_layer/bias, test_conv/rnn_final/final_layer/kernel, test_conv/test_conv/func/lstm_cell/bias/Momentum, test_conv/test_conv/func/lstm_cell/kernel/Momentum, test_conv/test_conv/rnn_final/final_layer/bias/Momentum, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"basic_rnn.py\", line 241, in <module>\r\n    sess.run(result)\r\n  File \"/home/vishal/venv_master_xla/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/vishal/venv_master_xla/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/vishal/venv_master_xla/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/vishal/venv_master_xla/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_7643397997199028266_f15n_0[] on XLA_CPU_JIT: _Arg (No registered '_Arg' OpKernel for XLA_CPU_JIT devices compatible with node {{node iteratortostringhandle_0_arg}} = _Arg[T=DT_STRING, index=2]()\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE]\r\n  device='XLA_CPU'; T in [DT_STRING]\r\n  device='XLA_CPU'; T in [DT_RESOURCE]\r\n  device='XLA_CPU'; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\r\n  device='CPU'\r\n  device='GPU'; T in [DT_STRING]\r\n  device='GPU'; T in [DT_RESOURCE]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_BOOL]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_INT8]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_UINT16]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='GPU'; T in [DT_HALF]\r\n){{node iteratortostringhandle_0_arg}}\r\n\t [[{{node cluster}} = XlaLaunch[Nresources=9, Targs=[DT_FLOAT, DT_FLOAT, DT_STRING, DT_INT64, DT_INT64, ..., DT_INT32, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_FLOAT], Tconstants=[], Tresults=[DT_FLOAT, DT_INT32, DT_INT32, DT_INT32, DT_INT32, ..., DT_INT32, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], function=cluster_7643397997199028266_f15n_0[], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorGetNext, IteratorGetNext:1, IteratorToStringHandle, batch_size, count, drop_remainder, tensors/component_0, tensors/component_1, test_conv/func/lstm_cell/bias/Initializer/zeros, test_conv/func/lstm_cell/bias/IsInitialized/VarIsInitializedOp, test_conv/func/lstm_cell/bias/Read/ReadVariableOp, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/RandomUniform, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/max, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/min, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/mul, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/shape, test_conv/func/lstm_cell/kernel/Initializer/random_uniform/sub, test_conv/func/lstm_cell/kernel/Initializer/random_uniform, test_conv/func/lstm_cell/kernel/IsInitialized/VarIsInitializedOp, test_conv/func/lstm_cell/kernel/Read/ReadVariableOp, test_conv/rnn_final/final_layer/bias/Initializer/zeros, test_conv/rnn_final/final_layer/bias/IsInitialized/VarIsInitializedOp, test_conv/rnn_final/final_layer/bias/Read/ReadVariableOp, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/RandomUniform, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/max, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/min, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/mul, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/shape, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform/sub, test_conv/rnn_final/final_layer/kernel/Initializer/random_uniform, test_conv/rnn_final/final_layer/kernel/IsInitialized/VarIsInitializedOp, test_conv/rnn_final/final_layer/kernel/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Initializer/zeros, test_conv/test_conv/func/lstm_cell/bias/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/bias/Momentum/Read_1/ReadVariableOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Initializer/zeros/Const, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Initializer/zeros/shape_as_tensor, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Initializer/zeros, test_conv/test_conv/func/lstm_cell/kernel/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/func/lstm_cell/kernel/Momentum/Read_1/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Initializer/zeros, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/bias/Momentum/Read_1/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Initializer/zeros/Const, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Initializer/zeros/shape_as_tensor, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Initializer/zeros, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/IsInitialized/VarIsInitializedOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Read/ReadVariableOp, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum/Read_1/ReadVariableOp, OneShotIterator, test_conv/func/lstm_cell/bias, test_conv/func/lstm_cell/kernel, test_conv/rnn_final/final_layer/bias, test_conv/rnn_final/final_layer/kernel, test_conv/test_conv/func/lstm_cell/bias/Momentum, test_conv/test_conv/func/lstm_cell/kernel/Momentum, test_conv/test_conv/rnn_final/final_layer/bias/Momentum, test_conv/test_conv/rnn_final/final_layer/kernel/Momentum)]]\r\n```\r\n\r\nThe code of the model I am trying to run is found [here](https://github.com/tensorflow/tensorflow/issues/22216) (issue #22216)", "Saw this issue while looking into #22216.\r\n\r\nSeems like the suggestion in https://github.com/tensorflow/tensorflow/issues/22216#issuecomment-433159156 should apply here as well.\r\n\r\nClosing this as well but let us know if I am mistaken.", "@smit-hinsu \r\nI don't think this has been resolved. 2 similar issues (the other can be seen [here](https://github.com/tensorflow/tensorflow/issues/22216#issuecomment-433562028)). So I think both should be reopened.", "To use xla.compile with the model from issue #22216, I'd recommend using the estimator_model_fn decorator as seen [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/compiler/xla.py#L606).\r\n\r\nNote that this requires all variables to be resource variables.\r\n\r\nPlease let us know if you run into issues with this approach.\r\n", "fixed in tf1.12 :) \r\nCorrection: allow_soft_placement is being set to True by default, when set to false, still fails :( ", "in tf1.13 using dynamic_rnns use xla entirely :) "]}, {"number": 22101, "title": "NTC CHIP Build Script", "body": "Added the build script for compiling Tensorflow for NTC [C.H.I.P. $9 Computer](https://en.wikipedia.org/wiki/CHIP_(computer)). \r\n\r\nModified from instructions and build script for [Raspberry pi](https://www.tensorflow.org/install/install_raspbian)\r\n\r\nBy default it compiles with vfpv3 FPU instructions; compile by:\r\n```\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\" \\\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 tensorflow/tools/ci_build/chip/build_ntc_chip.sh\r\n```\r\n \r\nCan be switched to neon instructions as well.\r\n```\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\" \\\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 tensorflow/tools/ci_build/chip/build_ntc_chip.sh NEON\r\n```\r\n\r\nTested and working on actual CHIP using:\r\n1. Tensorflow hello world.\r\n2. Training Boston house prices [Keras tutorial](https://www.tensorflow.org/tutorials/keras/basic_regression)\r\n3. Saving and loading of Boston house price model.\r\n4. Tested with loading my custom Keras model, trained on 64bit Ubuntu.", "comments": ["This is very cool! I don't think we want to add this to TF proper, since we have no way of maintaining it. @petewarden may disagree.\r\n\r\nI would create your own repo to contain this. @ewilderj is there a place we can highlight such things?\r\n\r\nI will close this PR for now.", "My company, Source Parts, is creating new CHIP derivatives. We are using Tensorflow in a CHIP-based Donkeycar platform. We would be willing to maintain it.", "I recommend a separate repo. IIUC there's no technical need to have this live in the main TensorFlow repo. "]}, {"number": 22100, "title": "Cherrypick fix to tags for internal testing infrastructure", "body": "", "comments": []}, {"number": 22099, "title": "Issue with Tensorflow softmax on gpus", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nnone\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.10.0 gpu\r\n- **Python version**:\r\n3.5.5\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **GCC/Compiler version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0/cuDNN v7.2.1.38 for CUDA 9.0 \r\n- **GPU model and memory**:\r\nGTX 980 4GB\r\n- **Exact command to reproduce**:\r\nn/a\r\n\r\n\r\n### Describe the problem\r\nThere is still a huge issue with not included GPU kernels in the latest tensorflow python version. When you try to run \r\n\r\n> import tensorflow as tf\r\n> with tf.device('/{}:{}'.format('gpu','0')):\r\n>     x = [0., -1., 2., 3.]\r\n>     softmax_x = tf.nn.softmax(x)\r\n>     session = tf.Session()\r\n>     print(session.run(softmax_x))\r\n\r\nit's exiting with the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1261, in _run_fn\r\n    self._extend_graph()\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1295, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Softmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: Softmax = Softmax[T=DT_FLOAT, _device=\"/device:GPU:0\"](Reshape)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tools\\check_tensorflow.py\", line 7, in <module>\r\n    print(session.run(softmax_x))\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Softmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: Softmax = Softmax[T=DT_FLOAT, _device=\"/device:GPU:0\"](Reshape)]]\r\n\r\nCaused by op 'Softmax', defined at:\r\n  File \"tools\\check_tensorflow.py\", line 5, in <module>\r\n    softmax_x = tf.nn.softmax(x)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1738, in softmax\r\n    return _softmax(logits, gen_nn_ops.softmax, axis, name)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1680, in _softmax\r\n    output = compute_op(logits)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7672, in softmax\r\n    \"Softmax\", logits=logits, name=name)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\xxx\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Softmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n\r\n         [[Node: Softmax = Softmax[T=DT_FLOAT, _device=\"/device:GPU:0\"](Reshape)]]\r\n```\r\n\r\nThe softmax cuda kernel seems not to be included in the package. This means that when you don't specify explicitly to use the gpu, it will just run softmax on cpu, which leads to cpu usages of 100% and very low gpu usage from 10 to 20%, and if you do it crashes.\r\nEspecially for beginners this can be very confusing and hard to find. Also it has a huge impact on overall performance.\r\n\r\nSo are there any solutions for this problem?\r\n", "comments": ["I don't have Windows access though I tried on Linux the Softmax is available on GPU:\r\n```\r\n$ python2\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> with tf.device('/{}:{}'.format('gpu','0')):\r\n...   x = [0., -1., 2., 3.]\r\n...   softmax_x = tf.nn.softmax(x)\r\n...   session = tf.Session()\r\n...   print(session.run(softmax_x))\r\n... \r\n2018-09-05 22:09:50.785901: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-05 22:09:52.634968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-05 22:09:52.635365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-09-05 22:09:52.635392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-05 22:09:52.940328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-05 22:09:52.940375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-05 22:09:52.940384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-05 22:09:52.940659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n[0.03467109 0.01275478 0.25618663 0.69638747]\r\n>>> \r\n```\r\n\r\nI think that might be a Windows specific issue?", "This is the strange thing: there is also another issue, where someone used complicated building instructions to build the windows version with softmax gpu support: https://github.com/tensorflow/tensorflow/issues/15254\r\nBut apparently his suggestions were not picked up again\r\nI'm working on replicating his instructions, but still I think this should be correctly implemented into the official version\r\n", "So are you working on this?", "@TGithubbr I don't have access to Windows so I may not be able to provide much help for Windows-related issues. Would be happy to take a further look if the issue could be reproduced on Linux.", "I just tested it on linux and it works perfectly alright. So if you at any time get the chance to test the windows build for yourself please check this bug out. I have not enough experience in this field to find its location by myself.", "/CC @gunan, do you know why this is occurring? ", "@meteorcloudy @mrry to help\r\nI cannot see any problems.\r\nOnly potential is, is it possible when we override EIGEN_STRONG_INLINE on windows something weird happens here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/softmax_op_gpu.cu.cc#L38", "@TGithubbr I notice you were using v1.10, which is still built by CMake.\r\n\r\nI think this problem no longer exists when building TensorFlow on Windows with Bazel, which will be default from v1.11.\r\n\r\nI can also confirm the `//tensorflow/python/kernel_tests:softmax_op_test` is passing on Windows on TF CI.\r\n\r\nHere's my result with a recent tensorflow nightly build:\r\n```\r\npcloudy@TENSORLOW-JENKI C:\\tools\\msys64\\home\\pcloudy\\workspace\\tf_gpu_test\r\n# type test.py\r\nimport tensorflow as tf\r\nwith tf.device('/{}:{}'.format('gpu','0')):\r\n    x = [0., -1., 2., 3.]\r\n    softmax_x = tf.nn.softmax(x)\r\n    session = tf.Session()\r\n    print(session.run(softmax_x))\r\n\r\npcloudy@TENSORLOW-JENKI C:\\tools\\msys64\\home\\pcloudy\\workspace\\tf_gpu_test\r\n# C:\\python36\\python test.py\r\n2018-09-26 15:23:05.448428: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-09-26 15:23:07.100434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:04.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.208930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:05.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.318715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 2 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:06.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.435113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:07.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.563203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 4 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:08.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.677694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 5 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:09.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.797429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 6 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:0a.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.911700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 7 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:0b.0\r\ntotalMemory: 11.18GiB freeMemory: 10.87GiB\r\n2018-09-26 15:23:07.940870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6, 7\r\n2018-09-26 15:23:12.710532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-26 15:23:12.723927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3 4 5 6 7\r\n2018-09-26 15:23:12.732976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y Y Y N N N N\r\n2018-09-26 15:23:12.745232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N Y Y N N N N\r\n2018-09-26 15:23:12.756203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   Y Y N Y N N N N\r\n2018-09-26 15:23:12.766825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   Y Y Y N N N N N\r\n2018-09-26 15:23:12.777259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 4:   N N N N N Y Y Y\r\n2018-09-26 15:23:12.788947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 5:   N N N N Y N Y Y\r\n2018-09-26 15:23:12.801252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 6:   N N N N Y Y N Y\r\n2018-09-26 15:23:12.812655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 7:   N N N N Y Y Y N\r\n2018-09-26 15:23:12.827793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10534 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2018-09-26 15:23:12.861306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10534 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)\r\n2018-09-26 15:23:12.898461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10534 MB memory) -> physical GPU (device: 2, name: Tesla K80, pci bus id: 0000:00:06.0, compute capability: 3.7)\r\n2018-09-26 15:23:12.927800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10534 MB memory) -> physical GPU (device: 3, name: Tesla K80, pci bus id: 0000:00:07.0, compute capability: 3.7)\r\n2018-09-26 15:23:12.956151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 10534 MB memory) -> physical GPU (device: 4, name: Tesla K80, pci bus id: 0000:00:08.0, compute capability: 3.7)\r\n2018-09-26 15:23:12.986229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 10534 MB memory) -> physical GPU (device: 5, name: Tesla K80, pci bus id: 0000:00:09.0, compute capability: 3.7)\r\n2018-09-26 15:23:13.015754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 10534 MB memory) -> physical GPU (device: 6, name: Tesla K80, pci bus id: 0000:00:0a.0, compute capability: 3.7)\r\n2018-09-26 15:23:13.047423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:7 with 10534 MB memory) -> physical GPU (device: 7, name: Tesla K80, pci bus id: 0000:00:0b.0, compute capability: 3.7)\r\n[0.03467109 0.01275478 0.25618663 0.69638747]\r\n```\r\n\r\n", "I tested 1,11,0rc2 and it really works! Thanks!", "I would not bother to reopen this issue, but for Cuda Compute Capability 3.0 this is actually the last build which is supported from the pre-built packages on Windows (i.e. when you query tensorflow for the devices, it won't promptly say that you have a Cuda too low).\r\nWhile I understand that officially TF is not supporting this Cuda version, I would still be interested in what is the problem, or if there is a workaround for this (apart from training on CPU). ", "You could try to build it with bazel which I tried but was never able to get to work. They won't be able to do anything because they don't have Windows (google to cheap for that) and don't test their builds"]}, {"number": 22098, "title": "Memory issue when inter_op_parallelism_threads > 1 on Ubuntu 16.04", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n```\r\n== cat /etc/issue ===============================================\r\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ml 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.3)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\ntensorflow-tensorboard (0.4.0)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = v1.10.0-0-g656e7a2b34\r\ntf.COMPILER_VERSION = v1.10.0-0-g656e7a2b34\r\nSanity check: array([1], dtype=int32)\r\n/home/btabanpour/mlpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/home/anaconda3/lib/\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n```\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\nSave this file as `reproduce.py`:\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\nfrom memory_profiler import profile\r\n\r\nimage = np.random.random((1, 299, 299, 3))\r\n\r\n@profile\r\ndef predict():\r\n    m.predict(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n    m = InceptionV3()  # or any other large network like VGG19()\r\n    for _ in range(100):\r\n        predict()\r\n```\r\n\r\nI run `python reproduce.py` and memory increases from ~500MiB to ~1200MiB after 100 calls of `predict()`, **running on CPU**. Here is truncated output:\r\n\r\n```\r\n > python reproduce.py\r\n2018-09-05 14:41:10.229405: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7    493.0 MiB    493.0 MiB   @profile\r\n     8                             def predict():\r\n     9    524.8 MiB     31.8 MiB       m.predict(image)\r\n\r\n\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7    524.8 MiB    524.8 MiB   @profile\r\n     8                             def predict():\r\n     9    557.2 MiB     32.4 MiB       m.predict(image)\r\n\r\n\r\n...\r\n\r\n\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   1283.4 MiB   1283.4 MiB   @profile\r\n     8                             def predict():\r\n     9   1283.6 MiB      0.2 MiB       m.predict(image)\r\n\r\n\r\nFilename: reproduce.py\r\n\r\nLine #    Mem usage    Increment   Line Contents\r\n================================================\r\n     7   1283.6 MiB   1283.6 MiB   @profile\r\n     8                             def predict():\r\n     9   1285.9 MiB      2.3 MiB       m.predict(image)\r\n\r\n```\r\n\r\n### Describe the problem\r\n\r\nIn short, memory keeps increasing after each predict call. I tried the above script with other networks such as `tensorflow.keras.applications.vgg19` and pure tensorflow convnets and I see the same issue.\r\n\r\nThis issue seems to be resolved when I set `inter_op_parallelism_threads=1` in the `tf.ConfigProto` as such:\r\n\r\n```python\r\n    config = tf.ConfigProto(\r\n        inter_op_parallelism_threads=1)\r\n    sess = tf.Session(config=config)\r\n```\r\n\r\nAnd re-appears when I start increasing `inter_op_parallelism_threads` to 32 (I'm running on 32 CPU cores).\r\n\r\n### Another issue:\r\n\r\nAs I mentioned previously, setting `inter_op_parallelism_threads=1` stops the memory from increasing on each predict call. Here is a script that sets the config `inter_op_parallelism_threads=1` and does not exhibit a memory leak:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\n\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\nfrom memory_profiler import profile\r\n\r\n\r\nimage = np.random.random((1, 299, 299, 3))\r\n\r\n\r\n@profile\r\ndef predict():\r\n    m.predict(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n    config = tf.ConfigProto(\r\n        inter_op_parallelism_threads=1)\r\n    sess = tf.Session(config=config)\r\n\r\n    print(sess._config)\r\n    K.set_session(sess)\r\n\r\n    m = InceptionV3()\r\n    for _ in range(100):\r\n        predict()\r\n```\r\n\r\nHowever, I see memory increasing again if I add an extra call to `tf.Session()` before calling the one with the `tf.ConfigProto`:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\n\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\nfrom memory_profiler import profile\r\n\r\nimage = np.random.random((1, 299, 299, 3))\r\n\r\n\r\n@profile\r\ndef predict():\r\n    m.predict(image)\r\n\r\n\r\nif __name__ == '__main__':\r\n    s = tf.Session()  # Added an extra tf.Session() call here!!!\r\n    s.close()\r\n\r\n    config = tf.ConfigProto(\r\n        inter_op_parallelism_threads=1)\r\n    sess = tf.Session(config=config)\r\n\r\n    print(sess._config)\r\n    K.set_session(sess)\r\n\r\n    m = InceptionV3()\r\n\r\n    for _ in range(100):\r\n        predict()\r\n```\r\n\r\nIt seems like the ConfigProto from the first `tf.Session()` is overwriting the config from the next session I create.\r\n\r\n---\r\n\r\nHere is a graph of the script above for 1000 predict calls:\r\n\r\n![tf_issue](https://user-images.githubusercontent.com/7320238/45115233-3cfa3a80-b11d-11e8-9b5e-781508fc8583.png)\r\n", "comments": ["This is a pretty big performance issue.", "@shivaniag Any chance someone could look at this just to confirm that this is a bug?", "I meet similar question, under watching.\r\n\r\n@tensorflow-jenkins ", "bumpiddy", "I am running into the same issue - I can set `inter_op_parallelism_threads=1` to stop the leak but that affects the latency of my model.", "Here's how I solve this issue: use placeholder - there is a make_initialize_iterator() function in dataset, use that and update your tensor in each iteration.", "Have this problem too during training on CPU. Setting inter_op_parallelism_threads=1 as @btaba proposed gets rid of the leak. \r\nBTW we've never noticed this issue when training the same model with GPU.\r\nEdit1:\r\nI am using tensorflow 1.12(python-tensorflow)  on Manjaro linux installed from the distro repo.\r\n\r\n\r\n ", "Facing the same issue. After around 500 predictions , memory is fully occupied. can't do any more task. ", "Same issue I am facing. Any update will be helpful? \r\nThanks ", "Seems it is related not only 16.04 ubuntu. 18.04.2 has this issue as well. \r\nIt might not depend on OS", "Met the same problem, under watching.", "@fchollet  could you take a look.", "Same problem", "I've met the same problem, set inter_op_parallelism_threads to 1 is ok, but the latency will increase in high load.", "I am seeing a memory leak that looks like the same issue. I get it with anaconda::tensorflow from conda, but not with conda-forge::tensorflow nor the tensorflow from pypi.", "same issue. TF2.0, running multiple model.predict() in a loop balloons memory (GPU) and I get OOM errors. ", "I can confirm that I am experiencing this issue with TF 2.0.0, as well.", "Got this issue with TF 1.13.1", "Any updates?", "Still got this with TF 2.3.1 when training in a docker container. Solved it with\r\n\r\n`set_inter_op_parallelism_threads(1)`\r\n\r\nat the expense of a slower training. I still have to check if setting it to a higher value can result in better performance without memory leak", "As @btaba shows in http://blog.tabanpour.info/projects/2018/09/07/tf-docker-kube.html it's a problem of over-comitting threads to limited CPU resources, typically in docker or k8s. Both inter & intra thread count should be set to fit within the CPU limit. A detection routine is available in the article above but could be adapted to better handle for fractional CPU limits.", "@btaba  It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.6.0) and let us know if the issue still persists? Please have a look at the [link](http://blog.tabanpour.info/projects/2018/09/07/tf-docker-kube.html) and let us know if it helps ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22098\">No</a>\n"]}, {"number": 22097, "title": "Failed to load the native TensorFlow runtime (TF 1.10.0, Windows Platform Only)", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Windows 10 and Windows Server 2016\r\n- **TensorFlow installed from**: Anaconda Distribution\r\n- **TensorFlow version**: 1.10.0\r\n- **Python version**: 3.6.6\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nAfter updating TF to v1.10.0 (through conda update), it cannot be imported. Kindly note that this issue is specific to Windows platform only (tested on both 10 and Server 2016). On Linux platform and using the same setting (TF installed using Anaconda Distribution), the same code is working fine (tested on Ubuntu 18.04.1 LTS).\r\n\r\nI also tried running a stock example but the same issue occured (TF cannot be imported).\r\n\r\n### Source code / logs\r\n<details>\r\n<summary>Full Traceback (click to expand)</summary>\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n</details>\r\n<br />\r\n\r\nThanks!", "comments": ["Anaconda distribution is not maintained/owned by us.\r\nPlease reach out to the owners of the anaconda package for support."]}, {"number": 22096, "title": "CTCLoss does not support half precision (float16/fp16)", "body": "I was trying to use `CTCLoss` with float16, and encountered a type exception in `python.framework.op_def_library._apply_op_helper `. In the [op definition](https://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/core/ops/ops.pbtxt#L3568), it only lists `DT_FLOAT`.  Is there a particular reason that this op wasn't updated to support half precision in #1300?\r\n\r\n```\r\n  File \"/nix/store/rnfj7g778synyrkr7qa8nx67sdmsr4xg-python3.6-tensorflow-1.7.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 533, in _apply_op_helper\r\n    (prefix, dtypes.as_dtype(input_arg.type).name))\r\nTypeError: Input 'inputs' of 'CTCLoss' Op has type float16 that does not match expected type of float32.\r\n```\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.7\r\nBazel version: 0.10.1\r\nCUDA/cuDNN version: 9.1\r\nGPU model and memory: Tesla V100\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Added PR #22107 for the float16 support of CTCLoss.", "@tensorflowbutler \r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.7\r\nBazel version: 0.10.1\r\nCUDA/cuDNN version: 9.1\r\nGPU model and memory: Tesla V100\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "CTC loss calculation can be numerically unstable.  So it's better to keep it at least 32 bits.  Since it's only implemented on CPU, there's essentially no benefit to having a float16 version."]}, {"number": 22094, "title": "Possible bug with map_fn in Graph mode during training, works fine with Eager execution", "body": "Tensorflow 1.10 on Ubuntu 16.04\r\n\r\nA simplified code example, see also stackoverflow: https://stackoverflow.com/questions/52187269/tensorflow-map-fn-does-not-work-in-graph-mode-during-training-in-eager-mode-it\r\n\r\nThis loss function works:\r\n```\r\ndef discriminative_loss_working(y_true, y_pred):\r\n    # Compute the loss for only the first image in the batch\r\n    \r\n    prediction = y_pred[0]\r\n    label = y_true[0]\r\n\r\n    # Number of clusters in ground truth\r\n    clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n    # Compute cluster means and variances for each cluster\r\n    def compute_mean(c):\r\n        mask = tf.equal(label[:,:,0], c)\r\n        masked_pixels = tf.boolean_mask(prediction, mask)\r\n        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n        return cluster_mean\r\n\r\n    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n    return tf.reduce_mean(cluster_means)\r\n```\r\n\r\nHowever, when inserting an extra map_fn to work with batch sizes > 1 it doesnot work:\r\n```\r\ndef discriminative_loss(y_true, y_pred):\r\n    \"\"\"Computes loss for a batch of images\r\n    Args:\r\n        y_true: (n, h, w) where each elements contains the ground truth instance id\r\n        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch\r\n    Returns:\r\n        loss\r\n    \"\"\"\r\n    # Compute the loss for each image in the batch\r\n    def compute_loss(input):\r\n        prediction = input[1]\r\n        label = input[0]\r\n\r\n        # Number of clusters in ground truth\r\n        clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n        # Compute cluster means and variances for each cluster\r\n        def compute_mean(c):\r\n            mask = tf.equal(label[:,:,0], c)\r\n            masked_pixels = tf.boolean_mask(prediction, mask)\r\n            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n            return cluster_mean\r\n\r\n        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n        return tf.reduce_mean(cluster_means)\r\n        \r\n    # We want to know the loss for each image in the batch\r\n    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))\r\n    return losses\r\n```\r\n\r\nThe error is:\r\n\r\n> 018-09-05 16:07:24.740128: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at tensor_array_ops.cc:121 : Not found: Resource __per_step_6/_tensor_arraysloss/output_1_loss/map/while/map/TensorArray_1_3/N10tensorflow11TensorArrayE does not exist.\r\n> Traceback (most recent call last):\r\n>   File \"instancesegmenter/test.py\", line 90, in <module>\r\n>     model.fit(train_dataset, epochs=5, steps_per_epoch=2)\r\n>   File \"/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1363, in fit\r\n>     validation_steps=validation_steps)\r\n>   File \"/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 205, in fit_loop\r\n>     outs = f(ins)\r\n>   File \"/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\", line 2914, in __call__\r\n>     fetched = self._callable_fn(*array_vals)\r\n>   File \"/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1382, in __call__\r\n>     run_metadata_ptr)\r\n>   File \"/home/derk/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 519, in __exit__\r\n>     c_api.TF_GetCode(self.status.status))\r\n> tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_6/_tensor_arraysloss/output_1_loss/map/while/map/TensorArray_1_3/N10tensorflow11TensorArrayE does not exist.\r\n> \t [[Node: training/SGD/gradients/loss/output_1_loss/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 = TensorArrayGradV3[_class=[\"loc:@train...ad/truediv\"], source=\"training/SGD/gradients\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training/SGD/gradients/loss/output_1_loss/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2, training/SGD/gradients/loss/output_1_loss/map/while/map/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3/StackPopV2_1)]]\r\n> \t [[Node: training/SGD/gradients/loss/output_1_loss/map/while/map/while/Mean_grad/truediv/_161 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_608_t...ad/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](^_clooptraining/SGD/gradients/loss/output_1_loss/map/while/map/while/boolean_mask/Reshape/Enter_grad/Switch/_36)]]\r\n\r\nNote that this works fine in Eager mode and also when only doing a forward pass. However the backward pass when in graph mode gives the error.\r\n\r\nThe full code example to reproduce the issue:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef discriminative_loss(y_true, y_pred):\r\n    \"\"\"Computes loss for a batch of images\r\n    Args:\r\n        y_true: (n, h, w) where each elements contains the ground truth instance id\r\n        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch\r\n    Returns:\r\n        loss\r\n    \"\"\"\r\n    # Compute the loss for each image in the batch\r\n    def compute_loss(input):\r\n        prediction = input[1]\r\n        label = input[0]\r\n\r\n        # Number of clusters in ground truth\r\n        clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n        # Compute cluster means and variances for each cluster\r\n        def compute_mean(c):\r\n            mask = tf.equal(label[:,:,0], c)\r\n            masked_pixels = tf.boolean_mask(prediction, mask)\r\n            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n            return cluster_mean\r\n\r\n        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n        return tf.reduce_mean(cluster_means)\r\n        \r\n    # We want to know the loss for each image in the batch\r\n    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))\r\n    return losses\r\n\r\ndef discriminative_loss_working(y_true, y_pred):\r\n    # Compute the loss for only the first image in the batch\r\n    \r\n    prediction = y_pred[0]\r\n    label = y_true[0]\r\n\r\n    # Number of clusters in ground truth\r\n    clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n    # Compute cluster means and variances for each cluster\r\n    def compute_mean(c):\r\n        mask = tf.equal(label[:,:,0], c)\r\n        masked_pixels = tf.boolean_mask(prediction, mask)\r\n        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n        return cluster_mean\r\n\r\n    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n    return tf.reduce_mean(cluster_means)\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, input_shape):\r\n        super(MyModel, self).__init__()\r\n        self.conv = tf.keras.layers.Conv2D(filters=4, kernel_size=(1,1))\r\n\r\n    def call(self, input):\r\n        return self.conv(input)\r\n\r\ninput_shape = (1,128,128,3)\r\ndef my_gen():\r\n    while True:\r\n        x = np.random.rand(1,input_shape[1], input_shape[2],3)\r\n        y = np.random.randint(11000, 11015, (input_shape[1], input_shape[2],1))\r\n        yield x,y\r\n\r\ntrain_dataset = tf.data.Dataset.from_generator(my_gen, (tf.float32, tf.float32))\r\ntrain_dataset = train_dataset.batch(1)\r\ntrain_dataset = train_dataset.repeat()\r\n\r\nmodel = MyModel(input_shape=input_shape)\r\n\r\n# This is a fix to make loading weights possible\r\n# x = tf.zeros((1,) + input_shape)\r\nx = tf.zeros(input_shape)\r\ny = model(x)\r\n\r\noptimizer = tf.keras.optimizers.SGD(lr=0.0001)\r\nmodel.compile(loss=discriminative_loss,optimizer=optimizer)\r\nmodel.fit(train_dataset, epochs=5, steps_per_epoch=2)\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@skye wdyt?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I am facing a similar error mentioned above. I will try my best to help resolve this issue as it benefits my work as well. Please reopen the issue @Harshini-Gadige, @drpngx .\r\n\r\nOS Platform and Distribution: Linux Ubuntu x86_64 - 4.15.0-52-generic (kernel)\r\nTensorFlow installed from: conda 4.7.5\r\nTensorFlow version: 1.13.1\r\nBazel version: N/A\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: Tesla V100-SXM2-16GB\r\nExact command to reproduce: \r\nMobile device: N/A\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\ndef discriminative_loss(y_true, y_pred):\r\n    \"\"\"Computes loss for a batch of images\r\n    Args:\r\n        y_true: (n, h, w) where each elements contains the ground truth instance id\r\n        y_pred: (n, h, w, d) d-dimensional vector for each pixel for each image in the batch\r\n    Returns:\r\n        loss\r\n    \"\"\"\r\n    # Compute the loss for each image in the batch\r\n    def compute_loss(input):\r\n        prediction = input[1]\r\n        label = input[0]\r\n\r\n        # Number of clusters in ground truth\r\n        clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n        # Compute cluster means and variances for each cluster\r\n        def compute_mean(c):\r\n            mask = tf.equal(label[:,:,0], c)\r\n            masked_pixels = tf.boolean_mask(prediction, mask)\r\n            cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n            return cluster_mean\r\n\r\n        cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n        return tf.reduce_mean(cluster_means)\r\n\r\n    # We want to know the loss for each image in the batch\r\n    losses = tf.map_fn(compute_loss, (y_true,y_pred), dtype=(tf.float32))\r\n    return losses\r\n\r\ndef discriminative_loss_working(y_true, y_pred):\r\n    # Compute the loss for only the first image in the batch\r\n\r\n    prediction = y_pred[0]\r\n    label = y_true[0]\r\n\r\n    # Number of clusters in ground truth\r\n    clusters,_ = tf.unique(tf.reshape(label, [-1]))\r\n\r\n    # Compute cluster means and variances for each cluster\r\n    def compute_mean(c):\r\n        mask = tf.equal(label[:,:,0], c)\r\n        masked_pixels = tf.boolean_mask(prediction, mask)\r\n        cluster_mean = tf.reduce_mean(masked_pixels, axis=0)\r\n\r\n        return cluster_mean\r\n\r\n    cluster_means = tf.map_fn(compute_mean, clusters, dtype=(tf.float32))\r\n    return tf.reduce_mean(cluster_means)\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, input_shape):\r\n        super(MyModel, self).__init__()\r\n        self.conv = tf.keras.layers.Conv2D(filters=4, kernel_size=(1,1))\r\n\r\n    def call(self, input):\r\n        return self.conv(input)\r\n\r\ninput_shape = (1,128,128,3)\r\ndef my_gen():\r\n    while True:\r\n        x = np.random.rand(1,input_shape[1], input_shape[2],3)\r\n        y = np.random.randint(11000, 11015, (input_shape[1], input_shape[2],1))\r\n        yield x,y\r\n\r\ntrain_dataset = tf.data.Dataset.from_generator(\r\n                    my_gen,\r\n                    (tf.float32, tf.float32),\r\n                    (tf.TensorShape([1,128,128,3]),\r\n                     tf.TensorShape([128,128,1])))\r\ntrain_dataset = train_dataset.batch(1)\r\ntrain_dataset = train_dataset.repeat()\r\n\r\nmodel = MyModel(input_shape=input_shape)\r\n\r\n# This is a fix to make loading weights possible\r\n# x = tf.zeros((1,) + input_shape)\r\nx = tf.zeros(input_shape)\r\ny = model(x)\r\n\r\nwith tf.Session(config=config):\r\n    optimizer = tf.keras.optimizers.SGD(lr=0.0001)\r\n    model.compile(loss=discriminative_loss,optimizer=optimizer)\r\n    model.fit(train_dataset, epochs=5, steps_per_epoch=2)\r\n```\r\nAttached is the error log.\r\n[tf_error.log](https://github.com/tensorflow/tensorflow/files/3412842/tf_error.log)\r\n"]}, {"number": 22093, "title": "AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI am using the stock scripts with a custom training dataset\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nn/A\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled via Anaconda command line\r\n- **TensorFlow version (use command below)**:\r\nb'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version**:\r\nanaconda                  5.1.0                    py36_2\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\npython train.py --logtostderr --train_dir=training\\\\ --pipeline_config_path=training\\\\test.config\r\n\r\n### Describe the problem\r\nI'm trying to train various models using a custom dataset.  I am able to train using the Faster RCNN Inception V2 without issue.  However, all of the other models I try to use throw the same error:\r\n\r\nAttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'\r\n\r\nI've tried with ssd_mobilenet_v1_coco,ssd_mobilenet_v2_coco, and ssd_inception_v2_coco.  All three produce the same error.\r\n\r\n### Source code / logs\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 168, in <module>\r\n    tf.app.run()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 164, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\trainer.py\", line 240, in train\r\n    detection_model = create_model_fn()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\builders\\model_builder.py\", line 98, in build\r\n    add_background_class)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\builders\\model_builder.py\", line 178, in _build_ssd_model\r\n    ssd_config.anchor_generator)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\builders\\anchor_generator_builder.py\", line 59, in build\r\n    if ssd_anchor_generator_config.height_stride:\r\nAttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'\r\nPS C:\\Users\\timothy.molner\\Desktop\\tensorflow\\research\\object_detection> python train.py --logtostderr --train_dir=training\\\\ --pipeline_config_path=training\\\\test.config", "comments": ["Bumping", "This is a stale issue. Also, we are no longer supporting TF1.x version. \r\n\r\nIn the recent years, There were lot of improvements in the model garden. I think this might have got resolved with the recent TF version. If this is still an issue with recent TF2.x version, please feel free to share a simple standalone code to reproduce the issue.\r\n\r\nThere is a good resource to install and test recent TF models in model garden by following the tutorials in [this resource](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model.html#sphx-glr-auto-examples-plot-object-detection-saved-model-py). Thanks!\r\n\r\nI am closing this issue. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22093\">No</a>\n"]}, {"number": 22091, "title": "Failed to load the native TensorFlow runtime.", "body": "Traceback (most recent call last):\r\n  File \"test_pixel_link.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/aashish/anaconda3/envs/pixel_link/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["I have installed CUDA on a non-GPU machine ", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). **Please provide all the information it asks**. Thank you.\r\n\r\nIn particular, just going by the error message: `ImportError: libcuda.so.1: cannot open shared object file: No such file or directory`, it would appear that either CUDA isn't installed in a way that is expected, or perhaps there is a version mismatch. The issue template asks for information about the TensorFlow version, CUDA/CUDNN version etc., and that would be helpful in diagnosing the problem.", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22090, "title": "import tensorflow as tf", "body": "import tensorflow as tf\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-64156d691fe5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/19584"]}, {"number": 22089, "title": "ResourceExhaustedError", "body": "Hi, I have trained CNN model on overall 3021 images (train data) which belongs to 9 classes (labeled data). I have 453 test images to verify my model predictions. My training model built fine with no issues (~95% accuracy) but while I am predicting it on test data, after running on some 40-45 images(also printing their predictions), it does not able to load and process further data/images to make prediction and shows below error.\r\nI have tried reducing the batch size, but problems still persist.\r\n\r\n\r\nError:\r\nResourceExhaustedError: OOM when allocating tensor with shape[64,64,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: block1_conv2_77/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block1_conv1_77/Relu, block1_conv2_77/kernel/read)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n*****************************************************************\r\n\r\n\r\nFor the reference, I am attaching my source code(please check model.zip for code) and error screenshot and I am using Tensorflow  1.10.0  with Python 3.6.5\r\n\r\n\r\nNote: My system configuration -\r\nIntel i7, 32GB RAM, NVIDIA 1080 GPU, Windows 10.\r\n![error](https://user-images.githubusercontent.com/24571705/45087543-e5ab9880-b123-11e8-9bae-ebaa1cda965c.png)\r\n![gpu](https://user-images.githubusercontent.com/24571705/45087544-e6442f00-b123-11e8-9a13-11ea42f5c0b6.PNG)\r\n\r\n\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/2352317/model.zip)\r\n\r\n\r\n\r\n-----\r\nRegards,\r\nAjay Sharma\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hi, Please find the details below.\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: Windows 10 Version 1803\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.10.0\r\nBazel version: not using\r\nCUDA/cuDNN version: CUDA v9.0\r\nGPU model and memory: Nvidia GTX 1080 (8GB memory)\r\nExact command to reproduce: python model.py\r\nMobile device : not using\r\n\r\n> Please check the code once attached in model.zip file if there is any issue.\r\n\r\n\r\nRegards,\r\nAjay Sharma\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22088, "title": "distribute.MirroredStrategy fails with Resource exhausted: OOM when allocating tensor with shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:  source r1.10\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nWe have a training code based on `tf.Estimator` that works well on single GPU with `tf.contrib.distribute.OneDeviceStrategy(\"device:GPU:0\")`. But when we add another GPU and change the distribution type to `tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)` the training code doesn't run anymore and raise an ugly memory allocation error. Even if we reduce drastically the batch size (from 128 to 64). Below you'll find a sample of the output error.\r\n\r\n### Source code / logs\r\n\r\n```\r\n2018-09-05 12:06:36.826713: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa445486000 of size 2013265920\r\n2018-09-05 12:06:36.826719: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa4bd486000 of size 2013265920\r\n2018-09-05 12:06:36.826725: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x7fa535486000 of size 2013265920\r\n2018-09-05 12:06:36.826730: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7fa5ad486000 of size 2013501696\r\n2018-09-05 12:06:36.826735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0x7fa6254bf900 of size 1855833856\r\n2018-09-05 12:06:36.826741: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size:\r\n2018-09-05 12:06:36.826748: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 17 Chunks of size 256 totalling 4.2KiB\r\n2018-09-05 12:06:36.826754: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1280 totalling 2.5KiB\r\n2018-09-05 12:06:36.826760: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 2048 totalling 6.0KiB\r\n2018-09-05 12:06:36.826766: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 2304 totalling 2.2KiB\r\n2018-09-05 12:06:36.826772: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 9728 totalling 19.0KiB\r\n2018-09-05 12:06:36.826778: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 73728 totalling 144.0KiB\r\n2018-09-05 12:06:36.826784: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 147456 totalling 288.0KiB\r\n2018-09-05 12:06:36.826791: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 13516800 totalling 12.89MiB\r\n2018-09-05 12:06:36.826797: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2013265920 totalling 3.75GiB\r\n2018-09-05 12:06:36.826803: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 2013501696 totalling 1.88GiB\r\n2018-09-05 12:06:36.826809: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.64GiB\r\n2018-09-05 12:06:36.826818: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:\r\nLimit:                 11922948096\r\nInUse:                  6054027520\r\nMaxInUse:               9980828416\r\nNumAllocs:                     153\r\nMaxAllocSize:           3507027968\r\n\r\n2018-09-05 12:06:36.826832: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *_______________***********************************________________******************_______________\r\n2018-09-05 12:06:36.826879: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at nccl_ops.cc:96 : Resource exhausted: OOM when allocating tensor with shape[503375361] and type float on /job:localhost/replica:0/task:0/device:GPU:1 by allocator GPU_1_bfc\r\n\r\n...\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[503375361] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[Node: NcclAllReduce = NcclAllReduce[T=DT_FLOAT, _class=[\"loc:@Reshape_28\"], num_devices=2, reduction=\"sum\", shared_name=\"c0\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\r\n```", "comments": ["@josh11b Can you help here?", "@jrabary can you provide code to reproduce this? \r\n\r\n@chsigg looks like the OOM is coming from the Nccl op, could you help take look as well? ", "@guptapriya here is an example of code to reproduce this. Tested on Nvidia Titan X. The model is big enough to completely fill the memory of a single GPU but it still works in one device strategy. \r\nYou get the problem when you change the number of GPU to 2 for example.\r\n\r\n```\r\nimport tensorflow as tf\r\nlayers = tf.keras.layers\r\n\r\nclass MyModel(tf.keras.Model):\r\n    \"\"\"\r\n    Simple CNN model.\r\n    \"\"\"\r\n\r\n    def __init__(self, name=''):\r\n        super(MyModel, self).__init__(name=name)\r\n\r\n        self.flatten = layers.Flatten()\r\n\r\n        kernel_initializer = tf.variance_scaling_initializer(scale=1.0 / 3.0, distribution='uniform')\r\n\r\n        self.conv1 = layers.Conv2D(32, (3, 3), name='conv1', activation=tf.nn.relu6,\r\n                                   kernel_initializer=kernel_initializer)\r\n        self.conv2 = layers.Conv2D(64, (3, 3), name='conv2', activation=tf.nn.relu6,\r\n                                   kernel_initializer=kernel_initializer)\r\n        self.conv3 = layers.Conv2D(64, (3, 3), name='conv3', activation=tf.nn.relu6,\r\n                                   kernel_initializer=kernel_initializer)\r\n        self.fc1 = layers.Dense(512, name='fc1', activation=tf.nn.relu6)\r\n        self.steer_predictor = layers.Dense(1, name='steer_predictor')\r\n\r\n    def call(self, inputs, training=True):\r\n        y = self.conv1(inputs)\r\n\r\n        y = self.conv2(y)\r\n\r\n        y = self.conv3(y)\r\n        y = self.flatten(y)\r\n\r\n        y = self.fc1(y)\r\n        y = self.steer_predictor(y)\r\n\r\n        return y\r\n\r\n\r\ndef get_distribution_strategy(num_gpus, all_reduce_alg=None):\r\n    \"\"\"Return a DistributionStrategy for running the model.\r\n    Args:\r\n      num_gpus: Number of GPUs to run this model.\r\n      all_reduce_alg: Specify which algorithm to use when performing all-reduce.\r\n        See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.\r\n        If None, DistributionStrategy will choose based on device topology.\r\n    Returns:\r\n      tf.contrib.distribute.DistibutionStrategy object.\r\n    \"\"\"\r\n    if num_gpus == 0:\r\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:CPU:0\")\r\n    elif num_gpus == 1:\r\n        return tf.contrib.distribute.OneDeviceStrategy(\"device:GPU:0\")\r\n    else:\r\n        if all_reduce_alg:\r\n            return tf.contrib.distribute.MirroredStrategy(\r\n                num_gpus=num_gpus,\r\n                cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(\r\n                    all_reduce_alg, num_packs=num_gpus))\r\n        else:\r\n            return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    \"\"\" Model function to be used by the estimator.\r\n\r\n    Returns:\r\n      An EstimatorSpec object\r\n    \"\"\"\r\n\r\n    is_training = mode == tf.estimator.ModeKeys.TRAIN\r\n\r\n    model = MyModel()\r\n\r\n    predictions = model(features, training=is_training)\r\n\r\n    loss = tf.losses.mean_squared_error(labels, predictions)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n\r\n        global_step = tf.train.get_or_create_global_step()\r\n\r\n        learning_rate = tf.train.linear_cosine_decay(0.0001,\r\n                                                     global_step,\r\n                                                     10000,\r\n                                                     beta=0.01)\r\n\r\n        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\r\n\r\n        train_op = tf.contrib.training.create_train_op(loss,\r\n                                                       optimizer,\r\n                                                       global_step,\r\n                                                       summarize_gradients=False)\r\n        # summaries\r\n        tf.summary.image('inputs', features, max_outputs=6)\r\n        tf.summary.scalar('training/learning_rate', learning_rate)\r\n\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=None,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=None)\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        eval_metric_ops = {\r\n            'mae': tf.metrics.mean_absolute_error(labels, predictions),\r\n        }\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            loss=loss,\r\n            eval_metric_ops=eval_metric_ops\r\n        )\r\n\r\n\r\ndef create_input_fn():\r\n\r\n    def input_fn():\r\n        features = tf.random_uniform([100, 88, 200, 3])\r\n        labels = tf.random_uniform([100, 1])\r\n        data = tf.data.Dataset.from_tensor_slices((features, labels)).repeat().batch(128)\r\n        return data\r\n\r\n    return input_fn\r\n\r\n\r\n\r\ndef main(_):\r\n\r\n    num_gpus = 2\r\n\r\n    # run configuration\r\n    distribution = get_distribution_strategy(num_gpus)\r\n\r\n    # tf session config\r\n    session_config = tf.ConfigProto(inter_op_parallelism_threads=64,\r\n                                    intra_op_parallelism_threads=64,\r\n                                    allow_soft_placement=True)\r\n\r\n    run_config = tf.estimator.RunConfig(save_summary_steps=100,\r\n                                        train_distribute=distribution,\r\n                                        session_config=session_config)\r\n\r\n    # Create estimator that trains and evaluates the model\r\n    ml_estimator = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        model_dir='/tmp/model',\r\n        config=run_config,\r\n        params={}\r\n    )\r\n\r\n    ml_estimator.train(input_fn=create_input_fn(), steps=100)\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Set tensorflow verbosity\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n    # Run the experiment\r\n    tf.app.run()\r\n```", "@yuefengz I have encountered a similar problem, any updates?", "@jrabary thank you for sharing the code. Could you try setting `num_packs=0` when you define the `cross_tower_ops` using `AllReduceCrossTowerOps`? My hypothesis is that it is packing all the gradients into 2 tensors (with num_packs=2) and this is too big for nccl to handle.\r\n\r\n", "@jrabary @fanshiqing did any of you get a chance to try out the suggestion [above](https://github.com/tensorflow/tensorflow/issues/22088#issuecomment-430835141)?", "Hi @guptapriya, I did and setting `num_packs=0` seems to work. So what does it really mean if we set num_packs to zero ? And what are the side effects w.r.t optimisation results ?  ", "@jrabary thanks for trying it out. num_packs=0 means that we will reduce each gradient separately, instead of trying to combine all of them into a small number of tensors first. Performance impact will depend on the use case. In the cases where the gradient tensors are large though (like it seems in your use case), it is not feasible to combine them given the limited memory. \r\n\r\n@yuefengz @dubey it seems like we should not try to combine gradients if there isn't enough memory. Can we do this in MirroredStrategy? Does CollectiveAllReduceStrategy check this?\r\n", "From what I can understand, packing and splitting shouldn't affect overall memory usage.  This logic concats many (small) tensors into `num_packs` (larger) tensors.  But overall memory usage should remain the same.\r\n\r\nI'm not aware of a size limitation when using `nccl`.\r\n\r\nLooking at the logs above, the `op_kernel.cc` failure reports OOM at `GPU:1`, but the `ResourceExhaustedError` reports OOM at `GPU:0`.  @yuefengz could there be an issue with placing the concat and split ops on correct devices?  I noticed that we use `ops.colocate_with` which was recently deprecated.\r\n\r\nCollectives has this logic built into the C++ backend via the `ScopedAllocator`.  Conceptually it does a similar thing.  We haven't seen any OOMs due to `ScopedAllocator`.", "The `concat` op has to create a memory block to store the concatenated result. In our our nccl packing algorithm, we concat all gradients into one large tensor. We can switch to a different tensor aggregation method (specifying non-zero `agg_small_grads_max_bytes` and `agg_small_grads_max_group` and set `num_packs` to 0) or even disable tensor aggregation. `CollectiveAllReduceStrategy` is another option which I think avoids creating large concatenated tensors as well. ", "Does the physical second GPU that you are adding have the same amount of memory as the first?", "It's been a while and we have suggested work-arounds.  Please re-open if you still have this problem at newer versions of TF.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=22088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=22088\">No</a>\n", "It is still failing. ", "> The `concat` op has to create a memory block to store the concatenated result. In our our nccl packing algorithm, we concat all gradients into one large tensor. We can switch to a different tensor aggregation method (specifying non-zero `agg_small_grads_max_bytes` and `agg_small_grads_max_group` and set `num_packs` to 0) or even disable tensor aggregation. `CollectiveAllReduceStrategy` is another option which I think avoids creating large concatenated tensors as well.\r\n\r\nI met the same issue twice when training with 7-gpus using tf.distribute.MirroredStrategy in tf1.15.  The first time was happened when it runs at 1,3000+steps, and the second time it happens at 380,000+steps. I am curious why the program can run at first time, but crushes in the middle? If it was the issue that concat all the gradients into a large tensor, it should raise error at first place. @yuefengz "]}, {"number": 22087, "title": "Provide wheels for multiple versions of CUDA officially", "body": "Ubuntu 18.04, the latest LTS version, only supports CUDA 9.2, but there is no wheel available for that combination of CUDA and Linux.  This prevents servers where Ubuntu 16.04 are installed from upgrading to 18.04.\r\n\r\nOn the other hand, PyTorch and CuPy officially provides wheels multiple versions of CUDA.  TF may be the only deep learning framework that is available in PyPI and confines the version of CUDA to specific one.\r\n\r\nLet Ubuntu 18.04 use TF without building from source and Ubuntu 16.04 upgrade to 18.04!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS: Ubuntu 16.04/18.04\r\nTF installed from: wheel (via pip)\r\nCUDA: 9.0/9.2(/8.0)\r\ncuDNN: 7.2", "I agree that would be fantastic. However, changing CUDA versions often means changing drivers and code. We strongly believe that we should only release official builds that are thoroughly tested, and I we cannot properly test and release several CUDA/cuDNN versions. \r\n\r\nNote that while 18.04 switched the default CUDA version, you can simply request a different version of CUDA when installing with apt-get. Please see our installation instructions."]}, {"number": 22086, "title": "Lack of support for half precision type in linear algebra operators", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: Python 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Tesla V-100 16160MiB\r\n- **Exact command to reproduce**:\r\nMinimal example to reproduce the issue\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import gen_linalg_ops\r\n\r\na = tf.placeholder(tf.float16, shape=[2, 2])\r\ngen_linalg_ops.qr(a)\r\n```\r\n\r\n### Problem\r\nThere seems to be no support for float16 type in [linalg_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/linalg_ops.cc) at all. Is there any reason as to why that is the case? Or a way around it?\r\n\r\nI have run into this issue when trying to use keras with float16 (by editing `.keras/keras.json`), sequential model with GRU cells. Code included for completeness:\r\n```\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.GRU(128, recurrent_dropout=0.75, input_shape=(200, 4)))\r\n```\r\n\r\n### Log\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".../python3.6/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 1494, in qr\r\n    \"Qr\", input=input, full_matrices=full_matrices, name=name)\r\n  File \".../python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \".../python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: float64, float32, complex64, complex128\r\n```", "comments": ["I don't believe there is any fundamental reason to not support it, would just have to make sure that the kernel implementations work for float16.\r\n\r\nMarking as \"Contributions Welcome\", as we'd welcome PRs updating the op definitions and adding unittests.\r\n\r\nCC @rmlarsen ", "I will try to look at this issue.", "I am new to open source. Can someone guide me to help me solve this issue. I have good amount of c++ knowledge and practice.\r\nThank You.", "Ideally all of the ops in [linalg_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/linalg_ops.cc) unless there's a good reason not to."]}, {"number": 22085, "title": "How to convert to .pb when there are multiple sess.run in graph", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @robieta: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow ](https://stackoverflow.com/questions/tagged/tensorflow)since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 22084, "title": "Fix MPI build failure caused by StringPiece -> absl::string_view", "body": "This fix tries to fix the MPI build failure caused by `StringPiece` -> `absl::string_view`.\r\n\r\nThis fix fixes #22376.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@jlebar have we caught these already?", "> @jlebar have we caught these already?\r\n\r\nLooks to me like we have not.", "Updated the description to include `This fix fixes #22376.`, as issue #22376 is related.\r\n\r\n/cc @jlebar @martinwicke ", "Looks like you clang-formatted the whole file.  That's not the right thing to do: clang-format is not a stable format, so if you clang-format the entirety of every file you touch, we're going to introduce a lot of irrelevant whitespace changes.\r\n\r\nThe correct thing is to apply the diff that the clang-format test was saying to apply, or to use the `git-clang-format` tool that you can find in the LLVM repository, e.g. https://llvm.org/svn/llvm-project/cfe/trunk/tools/clang-format/git-clang-format.\r\n\r\nWhitespace, I know...\r\n\r\n@yifeif IIRC we're now clang-formatting patches when we import them to be submitted.  Does that allow us to turn off the external clang-format check, so we don't have to go through this with folks anymore?", "Thanks @jlebar. I have updated the PR and now only applied the clang-format change as is shown in `Experimental clang-format Check`. Please take a look and let me know if there are any issues.", "So was MPI related ToString issue addressed? I still see it with the latest master pull.", "> So was MPI related ToString issue addressed? I still see it with the latest master pull.\r\n\r\nIsn't the fix you're looking for in this PR, which is still unsubmitted?", "> > So was MPI related ToString issue addressed? I still see it with the latest master pull.\r\n> \r\n> Isn't the fix you're looking for in this PR, which is still unsubmitted?\r\n\r\nI don't know. I guess I was hoping that my question might prod someone to submit it. Wishful thinking?\r\n", "@phalexo This PR addresses the issue you encounter. The PR is not merged into the master yet, but it will go into master soon.", "This is ready for @martinwicke to submit.", "@jlebar, in that case, just add the \"ready to pull\" label.", "(just did for this one)", "What about r1.11 branch? Is it going to get the patch?", "@EFanZh I created a PR #22474 to cherry pick the fix on r1.11 branch. As r1.11 is already in release candidate 2, the PR might not be accepted though."]}, {"number": 22083, "title": "add gradient for broadcast_to", "body": "Fix #21901", "comments": ["@alextp Could you take a  look?", "Hi, more tests are added. Could you take a look again? Thanks.", "It seems like one of these tests is not safe on the GPU. I think there is a bug with broadcast_to on GPU from scalars; can you make that test cpu-only by adding a `with ops.device(\"cpu:0\"):` block around it?", "I have made this scalar test cpu-only, but I didn't test it on my machine. Could you restart all tests? Thanks.", "@alextp Thanks for your review. I think those failed tests are unrelated.", "Indeed, they look like flakes. Rerunning.", "Thank you, @alextp."]}, {"number": 22082, "title": "Build failes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux powerai 4.4.0-109-generic #132-Ubuntu SMP Tue Jan 9 20:00:40 UTC 2018 ppc64le ppc64le ppc64le GNU/Linux\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource (R1.11)\r\n- **TensorFlow version (use command below)**:\r\nR1.11\r\n- **Python version**:\r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n1.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0 20160609 \r\n- **CUDA/cuDNN version**:\r\nCUDA 9.2, cuDNN 7.1\r\n- **GPU model and memory**:\r\n4 x  Tesla P100-SXM2-16GB\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n### Describe the problem\r\nBuild failes at the end:\r\n\r\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\r\nERROR: /root/tensorflow/tensorflow/BUILD:592:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2927, in <module>\r\n    @_call_aside\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2913, in _call_aside\r\n    f(*args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in _initialize_master_working_set\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 956, in subscribe\r\n    callback(dist)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in <lambda>\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2515, in activate\r\n    declare_namespace(pkg)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2097, in declare_namespace\r\n    _handle_ns(packageName, path_item)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2047, in _handle_ns\r\n    _rebuild_mod_path(path, packageName, module)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2066, in _rebuild_mod_path\r\n    orig_path.sort(key=position_in_sys_path)\r\nAttributeError: '_NamespacePath' object has no attribute 'sort'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 7114.719s, Critical Path: 1859.33s\r\nINFO: 4737 processes: 4737 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["It looks like to be related to https://github.com/pypa/setuptools/issues/1282\r\n\r\nWondering if upgrade setuptools will resolve the issue?", "So that shouldn't happen with the 40.2.0 version?", "I agree with @yongtang \r\nThe stacktrace points to an issue outside tensorflow.\r\nCould you try the recommendation on pypa/setuptools#1282", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Similar issue is https://github.com/pypa/setuptools/issues/1321 and it's solved in setuptools-40.3.0, so I hope that with latest setuptools build will succeed.", "upgrading setuptools==40.6.3 works perfectly and build succeeds.\r\nOS - ubuntu 16.04 \r\ntensorflow 1.12, python-3.5\r\ncuda 10.0, cudnn-7.3.1, nccl-2.3.5\r\nIf anyone requires the built .whl you can download from here\r\n[https://drive.google.com/open?id=1ukL8M93LBPsBcERWPmL3VIzLSkFbCL_R](url)\r\n"]}, {"number": 22081, "title": "GDR Cannot register memory region", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:1.10\r\n- **Python version**: 2.7 \r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: K40c 12G\r\n- **Exact command to reproduce**: \r\n**Worker :** python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr\r\n**PS :** CUDA_VISIBLE_DEVICES='' python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=0 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=ps --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr\r\n\r\n## Describe the problem\r\nI tried to use GDR, but it seems no performance improvement compared to native RDMA, and logs are the following:\r\n\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=/home/shuai/imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr\r\n2018-09-05 15:05:58.062100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-05 15:05:58.211502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-05 15:05:58.374791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 2 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-05 15:05:58.553152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 3 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-05 15:05:58.553735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-09-05 15:05:59.918282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-05 15:05:59.918335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 2 3\r\n2018-09-05 15:05:59.918344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y N N\r\n2018-09-05 15:05:59.918349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N N N\r\n2018-09-05 15:05:59.918354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 2:   N N N Y\r\n2018-09-05 15:05:59.918359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 3:   N N Y N\r\n2018-09-05 15:05:59.919609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11473 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2018-09-05 15:06:00.088804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 11473 MB memory) -> physical GPU (device: 1, name: Tesla K40c, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\n2018-09-05 15:06:00.288448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 11473 MB memory) -> physical GPU (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)\r\n2018-09-05 15:06:00.487520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 11473 MB memory) -> physical GPU (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0, compute capability: 3.5)\r\n2018-09-05 15:06:00.688704: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.10.6:2222}\r\n2018-09-05 15:06:00.688752: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}\r\n2018-09-05 15:06:00.697485: I tensorflow/contrib/gdr/gdr_memory_manager.cc:254] RDMA server is listening on 10.10.10.5:3333\r\n2018-09-05 15:06:00.697568: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cuda_host_bfc\r\n2018-09-05 15:06:00.697588: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cpu_pool\r\n2018-09-05 15:06:00.697607: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cpu_rdma_bfc\r\n2018-09-05 15:06:00.697830: I tensorflow/contrib/gdr/gdr_memory_manager.cc:95] NUMA node for device: mlx4_0 is 1\r\n2018-09-05 15:06:00.734616: W tensorflow/contrib/gdr/gdr_memory_manager.cc:705] Cannot register memory region\r\n2018-09-05 15:06:00.771893: W tensorflow/contrib/gdr/gdr_memory_manager.cc:705] Cannot register memory region\r\n2018-09-05 15:06:00.771954: I tensorflow/contrib/gdr/gdr_memory_manager.cc:314] Instrumenting GPU allocator with bus_id 2\r\n2018-09-05 15:06:00.772783: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:375] Started server with target: grpc://localhost:3333\r\n```\r\nand here is GPU topology:\r\n\r\n```\r\n$ nvidia-smi topo -m\r\n        GPU0    GPU1    GPU2    GPU3    mlx4_0  CPU Affinity\r\nGPU0     X      PHB     SYS     SYS     SYS     0-7,16-23\r\nGPU1    PHB      X      SYS     SYS     SYS     0-7,16-23\r\nGPU2    SYS     SYS      X      PHB     PHB     8-15,24-31\r\nGPU3    SYS     SYS     PHB      X      PHB     8-15,24-31\r\nmlx4_0  SYS     SYS     PHB     PHB      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing a single PCIe switch\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\nThe logs said,\" cannot register memory region\" in GPU 2 and GPU 3. Is it a bug?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "@tensorflowbutler \r\nHi,\r\n I have updated exact command to reproduce, and I use the benchmark source code from tensorflow/benchmarks.\r\n\r\nThanks", "@tensorflowbutler \r\n\r\nDo you have any ideas about this? Is it caused by a bug ?", "Have you tried to install `nv_peer_mem` driver to enable GDR?\r\n\r\nBy the way, what is the output for command `ulimit -l`? If you have inherit limit for page-locked memory, the RDMA memory allocator might not be able to register enough memory region. It is thus recommended to modify the limits in `/etc/security/limits.conf`.", "@byronyi \r\n\r\nThe output for command ```ulimit -l``` is ```unlimited```, and I have installed ```nv_peer_mem``` according to the guide in [Mellanox/nv_peer_memory](https://github.com/Mellanox/nv_peer_memory). Here is the output for command ``` /etc/init.d/nv_peer_mem status``` : ```nv_peer_mem module is loaded.```", "Then you could try adding `perror(\"rdma_reg_read\");` here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gdr/gdr_memory_manager.cc#L703 and see what is the detailed error message.", "@byronyi \r\n\r\nHi,\r\nAs you have said, I modified the code to \r\n```\r\nelse {\r\n    perror(\"rdma_reg_read\");\r\n    LOG(WARNING) << \"Cannot register memory region\";\r\n  }\r\n```\r\n\r\nAnd here is the relative output of GDR:\r\n\r\n```\r\n2018-09-15 15:40:24.807709: I tensorflow/contrib/gdr/gdr_memory_manager.cc:255] RDMA server is listening on 12.12.10.15:3333\r\n2018-09-15 15:40:24.807797: I tensorflow/contrib/gdr/gdr_memory_manager.cc:303] Instrumenting CPU allocator cuda_host_bfc\r\n2018-09-15 15:40:24.807822: I tensorflow/contrib/gdr/gdr_memory_manager.cc:303] Instrumenting CPU allocator cpu_pool\r\n2018-09-15 15:40:24.807842: I tensorflow/contrib/gdr/gdr_memory_manager.cc:303] Instrumenting CPU allocator cpu_rdma_bfc\r\n2018-09-15 15:40:24.808077: I tensorflow/contrib/gdr/gdr_memory_manager.cc:96] NUMA node for device: mlx4_0 is 1\r\naddr = 0x1305c40000\r\nrdma_reg_read: Bad address\r\n2018-09-15 15:40:24.842781: W tensorflow/contrib/gdr/gdr_memory_manager.cc:709] Cannot register memory region\r\n2018-09-15 15:40:24.842828: I tensorflow/contrib/gdr/gdr_memory_manager.cc:315] Instrumenting GPU allocator with bus_id 2\r\n```\r\n\r\n It seems that the address is too large to exceed the capacity of my GPU, 12GB. Is that right?", "@wangshuaizs Wait until my PR https://github.com/tensorflow/tensorflow/pull/22559 is merged, or you could try that out  you self if you can\u2019t wait.", "Nagging Assignee @tatatodd: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@wangshuaizs Should be fixed now. Could you try again?", "Hi, @byronyi \r\n\r\nSorry for late reply. I tried to replace the original code with the modified code in gdr_memory_manager.cc, based on tf r 1.10.1. But it failed. Then I downloaded the master source code and tried to install it, but it failed again with the following error messages:\r\n```\r\nERROR: /home/shuai/tensorflow-master/tensorflow/contrib/gdr/BUILD:61:1: C++ compilati\r\ntensorflow/contrib/gdr/gdr_worker.cc: In constructor 'tensorflow::GdrWorker::GdrWorke\r\ntensorflow/contrib/gdr/gdr_worker.cc:46:45: error: no matching function for call to '\r\n       recv_tensor_recent_request_ids_(100000) {}\r\n                                             ^\r\nIn file included from ./tensorflow/contrib/gdr/gdr_worker.h:22:0,\r\n                 from tensorflow/contrib/gdr/gdr_worker.cc:16:\r\n./tensorflow/core/distributed_runtime/rpc/grpc_worker_service.h:36:3: note: candidate\r\n   GrpcWorker(WorkerEnv* env, const ConfigProto& config);\r\n   ^\r\n./tensorflow/core/distributed_runtime/rpc/grpc_worker_service.h:36:3: note:   candida\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "Could you try master? Currently there aren't sufficient resources for me to port GDR patches to every single TF release.", "Yes, I have tried master, and the error message is generated from master.", "It seems 462a79b7fe98ad71dccbcf691a06a4a7f48ee382 breaks GDR by introducing an additional config parameter.\r\n\r\nPing @poxvoculi. I will send a PR soon.\r\n\r\nEDIT: Looking at the commit, it seems trying to prevent repeated allocation/copying of gRPC buffers on the receiver side. What do you think about [zero-copy socket messaging](https://netdevconf.org/2.1/papers/netdev.pdf)? Seems worth to investigate as [Willem de Bruijn](mailto:willemb@google.com) mentioned in the paper that his team is working on porting MSG_ZEROCOPY to TF. Could be a good candidate for optimised TCP networking plugin. ", "@wangshuaizs You could try \r\n\r\n```curl -L https://github.com/tensorflow/tensorflow/pull/23480.patch | git apply```\r\n\r\n and see if it works.", "@byronyi \r\nIt compiled successfully this time. but it seems my training source code is not compatible with master. Here is the log:\r\n```\r\n2018-11-07 12:49:40.514512: I tensorflow/contrib/gdr/gdr_memory_manager.cc:228] RDMA server is listening on 10.10.11.7:3333\r\n2018-11-07 12:49:40.514610: I tensorflow/contrib/gdr/gdr_memory_manager.cc:90] NUMA node for device: mlx4_0 is 1\r\n2018-11-07 12:49:40.514643: I tensorflow/contrib/gdr/gdr_memory_manager.cc:266] Instrumenting CPU allocator(s)\r\n2018-11-07 12:49:43.439704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-11-07 12:49:43.439772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-07 12:49:43.441047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-07 12:49:43.441076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-07 12:49:43.441088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-07 12:49:43.441642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11519 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)\r\n2018-11-07 12:49:43.446125: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.11.2:2222}\r\n2018-11-07 12:49:43.446158: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}\r\n2018-11-07 12:49:43.452238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:3333\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /home/shuai/test/yolo_tensorflow/yolo/yolo_net.py:346: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nWARNING:tensorflow:From train.py:239: __init__ (from tensorflow.python.training.sync_replicas_optimizer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe `SyncReplicaOptimizer` is deprecated. For synchrononous training, please use [Distribution Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py:346: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From train.py:251: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nWorker 0: Initializing session...\r\n2018-11-07 12:49:45.947153: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session cdc6fedbc4db30df with config: gpu_options { } allow_soft_placement: true\r\n2018-11-07 12:49:51.843039: I tensorflow/contrib/gdr/gdr_memory_manager.cc:685] RDMA endpoint connected to rdma://10.10.11.2:2222\r\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\r\nWorker 0: Session initialization complete.\r\nStarting chief queue runner and running init_tokens_op\r\nStart distributed training ...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 358, in <module>\r\n    main()\r\n  File \"train.py\", line 353, in main\r\n    distributed_train(params)\r\n  File \"train.py\", line 315, in distributed_train\r\n    sess.run(train_op, feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'reduction_type' not in Op<name=ConditionalAccumulator; signature= -> handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; is_stateful=true>; NodeDef: sync_replicas/conditional_accumulator = ConditionalAccumulator[_class=[\"loc:@sync_replicas/AccumulatorApplyGradient\"], container=\"\", dtype=DT_FLOAT, reduction_type=\"MEAN\", shape=[7,7,3,64], shared_name=\"yolo/conv_2/weights:0/grad_accum\", _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n```\r\nThough no ```Cannot register memory region``` is printed, I don't know if it works.\r\nps: ```verbs``` failed to compile with tf-master. Would you mind making a patch based on r1.10? \r\n\r\nthanks", "@wangshuaizs Sorry, AFAIK we do not have necessary resources to back port patches to contrib to branch other than master."]}]