[{"number": 14663, "title": "DropoutWrapper and dynamic_rnn with parallel iterations not reproducible", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: CPU Binary (pip wheel)\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6.3 and 3.5.2\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\nThe following script fails with an assertion error, even though I've explicitly set the random seed:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef run():\r\n    tf.reset_default_graph()\r\n    tf.set_random_seed(0)\r\n\r\n    sess = tf.Session()\r\n    x = tf.placeholder(tf.float32, [None, None, 1])\r\n    cell = tf.nn.rnn_cell.DropoutWrapper(\r\n        tf.nn.rnn_cell.LSTMCell(100), input_keep_prob=0.5)\r\n    output, state = tf.nn.dynamic_rnn(\r\n        cell, x, dtype=tf.float32, parallel_iterations=100)\r\n    # if parallel_iterations=1, then everything works\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    return sess.run([output, state], {x: np.arange(100).reshape(1, 100, 1)})\r\n\r\no1, (c1, h1) = run()\r\no2, (c2, h2) = run()\r\n  \r\nassert (o1 == o2).all()\r\nassert (c1 == c2).all()\r\nassert (h1 == h2).all()\r\n```\r\n\r\n### Describe the problem\r\nIt looks like using parallel iterations options creates some non-determinism when using `DropoutWrapper` (and `parallel_iterations=32` by default). Ideally, when setting the random seed, all TensorFlow operations should be deterministic and reproducible (or the non-determinism should at least be documented).\r\n\r\ncc @tudorgt @adamAlnatsheh", "comments": ["While I agree with the ideal about deterministic and reproducible, it's not clear to me that we meet that level of guarantee with our current code. @mrry do you have any suggestions? ", "I'm pretty sure that it's impossible to get deterministic results from using an RNG op (like `tf.random_uniform()`) inside a `tf.while_loop()` with `parallel_iterations > 1`, unless there is a data or control dependency between the RNG ops in successive iterations. I can think of two conceptual workarounds, but neither of them is likely to work out of the box with the `tf.rnn.rnn_cell` APIs:\r\n\r\n1. Compute the sequence of dropout masks outside the loop using a single `tf.random_uniform()` call, and access the appropriate slice inside the loop.\r\n2. Somehow add control dependencies from the input loop variables (and hence the output values of the previous iteration) to the `tf.random_uniform()` inside `DropoutWrapper`. Technically you only need the control dependency between the `tf.random_uniform()` ops, and the more precise you can make it, the higher the parallelism you'll achieve. (For example, you could add the random mask as a loop variable, purely to make it possible to add a control dependency on it.)\r\n\r\nThere may be simpler ways to do it, but those are the only approaches I can think of.\r\n\r\n/cc @ekelsen for deterministic execution/reproducibility and @ebrevdo for `tf.nn.dynamic_rnn()`.", "if you want exact reproducibility, use the randomness operations in [`tf.contrib.stateless`](https://www.tensorflow.org/api_docs/python/tf/contrib/stateless).  You'll still have to feed some function of time (i.e., pass it through your state) in order to get proper random seeds for every time step.\r\n\r\nI agree we should document nondeterminism of the RNGs when the same op is executed within flow control like while loops but without control dependencies to ensure ordering.\r\n\r\nAnother question is, can we add some control dependencies in `DropoutWrapper` to ensure ordered execution of all randomness-generating ops.  The answer is maybe, but it's not high priority because not enough folks have considered this to be an issue - and because adding control dependencies generally slows down execution.  I'll leave it to @ekelsen to see if he wants to rifle through the `DropoutWrapper` code to ensure consistency of this sort.", "I think it is unlikely that I'll get to this.  There exists a WAR (use parallel_iterations = 1).  Higher priority is non-determinism in individual ops.\r\n\r\nAlso to the original poster's comment \"when setting the random seed, all TensorFlow operations should be deterministic and reproducible (or the non-determinism should at least be documented).\"  This would be a nice state of the world, but far from the world that we live in today.  Many TF operations are not deterministic and there is generally no documentation describing that.  Doing this now for all the ops that exist on the CPU and GPU is a rather daunting task.  I've done work on fixing the most common GPU cases.", "I think the main remaining source of nondeterminism may be Eigen's float32\nmatmuls.\n\nOn Mon, Nov 20, 2017 at 8:06 AM, ekelsen <notifications@github.com> wrote:\n\n> I think it is unlikely that I'll get to this. There exists a WAR (use\n> parallel_iterations = 1). Higher priority is non-determinism in individual\n> ops.\n>\n> Also to the original poster's comment \"when setting the random seed, all\n> TensorFlow operations should be deterministic and reproducible (or the\n> non-determinism should at least be documented).\" This would be a nice state\n> of the world, but far from the world that we live in today. Many TF\n> operations are not deterministic and there is generally no documentation\n> describing that. Doing this now for all the ops that exist on the CPU and\n> GPU is a rather daunting task. I've done work on fixing the most common GPU\n> cases.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14663#issuecomment-345741575>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyOeGkn9Axdx0v1Puc4gcY7PM41cks5s4aNogaJpZM4QifZ->\n> .\n>\n", "I also came across this issue. I am using MPI_Allreduce to do distributed training of my model. I noticed that my workers were not converging because of this issue. Parallel_iterations == 1 seems to have solved it. ", "@ebrevdo what's the state of this issue? Should it be closed since it seems infeasible to implement? Or marked as contributions welcome?", "This is a pretty nontrivial problem to solve.  We can add some dependency constraints to ensure that at least state dropout is performed in a deterministic way, but input dropout and (possibly) output dropout cannot be enforced this way.\r\n\r\nProbably the best way to ensure we do deterministic randomness inside a while_loop is to use the random ops from `tf.contrib.stateless` (soon to be moved to core).  This requires a lot more manual management of random seed, and will make the DropoutWrapper more complex.  It's still worth doing, but will require careful thought to do it correctly.\r\n\r\nIf anyone is interested in implementing a `StatelessDropoutWrapper` using these primitives, I am happy to work with you to make it happen.  cc @girving.", "cc @qlzh727 "]}, {"number": 14659, "title": "Feature request: tf.nn.ctc_loss lacks the API to handle sequences with all blanks", "body": "It is trivial to calculate the CTC loss of a sequence with all blanks. But tf.nn.ctc_loss cannot handle the situation that one or more sequences in the batch have no non-blank labels. This is a big limitation.   \r\n\r\nP.S.: I pulled these requests before:\r\nI reported this previously at\r\nhttps://github.com/tensorflow/tensorflow/issues/13457\r\nI asked this on stackoverflow\r\nhttps://stackoverflow.com/questions/46652720/how-to-calculate-ctc-loss-of-a-sequence-with-all-blanks-using-tf-nn-ctc-loss  ", "comments": ["@xilinli I'm sorry we and the community haven't been able to get you an effective answer. \r\n\r\n@ebrevdo and @andrewharp is @xilinli doing something different with his inputs than the usual way that ctc_ops.py is used? Might there be a workaround? Or do we anticipate making things more robust for his case?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Sounds like this is a edge case bug... Happy to review a bugfix pr.\n\nOn Mon, Feb 5, 2018, 11:55 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Awaiting TensorFlower: It has been 14 days with no activity and\n> the awaiting tensorflower label was assigned. Please update the label\n> and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14659#issuecomment-363338141>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimziTdThZpF_V5nxq_S9P7fzA0wP_ks5tSAOWgaJpZM4QiarT>\n> .\n>\n", "is anyone working on this issue or has it been fixed in a different issue or PR?\r\nif it is still open i would like to review it and try to create a PR.", "No one is working on it afaik\n\nOn Thu, Sep 6, 2018, 5:01 AM Pinaki Nath Chowdhury <notifications@github.com>\nwrote:\n\n> is anyone working on this issue or has it been fixed in a different issue\n> or PR?\n> if it is still open i would like to review it and try to create a PR.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14659#issuecomment-419067578>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim36X_FFFj3momlSrVYplDWnVzxukks5uYQ6bgaJpZM4QiarT>\n> .\n>\n", "I just hit the same bug. Has anyone found a workaround?", "I wonder how many people actually hitting this one without even releazing it.\r\n", "+1 Thanks for your attention. This seems to make tensorflow CTC unusable in a common use case.", "This is fixed for pytorch. Probably similar changes needed here.\r\nhttps://github.com/pytorch/pytorch/pull/23298\r\nhttps://github.com/pytorch/pytorch/pull/21910"]}, {"number": 14584, "title": "Contradicting behaviour in variations of tf.cond usage with tf.nn.static_state_saving_rnn", "body": "[Model.txt](https://github.com/tensorflow/tensorflow/files/1474854/Model.txt)\r\n[Training.txt](https://github.com/tensorflow/tensorflow/files/1474855/Training.txt)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.2/1.3/1.4 (tested on all)\r\n- **Bazel Version** : N/A\r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**: Cuda 8, CuDNN 6\r\n- **GPU model and memory**: GeForce GTX 1080 , 12 GB\r\n- **Exact command to reproduce**: python Training.py\r\n\r\n### Problem\r\nI am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, I am using tf.contrib.training.batch_sequences_with_states API with tf.nn.static_state_saving_rnn API to transfer RNN state information to subsequent segments of the same sequence. I am using tf.RandomShuffleQueue() to store my data and to decouple the I/O from training I am running the enqueue operations asynchronously in a different thread. \r\n\r\nTo facilitate a testing run after each training epoch I am using two separate `tf.RandomShuffleQueue()` structures and hence two different `tf.contrib.training.batch_sequences_with_states()` and `tf.nn.static_state_saving_rnn()` instances for train/test data correspondingly. Just the RNN cell which is passed to `tf.nn.static_state_saving_rnn` instances remains the same, so that the modified set of weights are used at test time.\r\n\r\nMoreover, I use a placeholder which is a boolean flag using which the appropriate nodes in the computation graph are switched at train/test time. This switching is done using `tf.cond()` operation.\r\n\r\n#### Situation 1\r\n\r\nThe problem is that of a deadlock situation at a specific stage between the enqueue operations and training operations, both running in separate threads. The enqueue operation timeouts mostly because the queue has reached the maximum capacity and for some reason training operation never returns and is waiting to get some more data and hence no dequeue operation is called.\r\n\r\n#### Situation 2\r\n\r\nIn file Model.py, if I uncomment the lines from 97-101 and comment line 104, then there is no such deadlock situation. The only difference is in the way that specific `tf.cond()` operation is written. One is in a declarative form(working code) and other is in an inline form(broken/deadlock code).\r\n\r\n#### Situation 3\r\n\r\nIn file - Training.py, dummy data is generated by the `gen_data()` procedure(lines - 43-48) and called on line 61. The second parameter to this function is the number of time steps for each sequence. If this number is fixed to a value which is less than the unroll length parameter of `tf.contrib.training.batch_sequences_with_states()` instances(i.e. each sequence can very well fit in one batch itself), this deadlock does not occur irrespective of situation 1 or situation 2 described above.\r\n\r\nHence, we suspect there is some minute intricacy in `tf.cond()` and `tf.nn.static_state_saving_rnn()` which gives rise to such a deadlock.\r\n\r\n### Source code / logs\r\nTwo files are attached(.txt files since the interface does not allow attaching files with extension .py) - \r\n\r\n1. Model.txt - Contains the Model class and the inference() method where the majority of the computation graph is built.\r\n2. Training.txt - Contains the client code which generates dummy data and calls to sess.run()\r\n\r\nThe current code is according to Situation 1 as described above and the behaviour can be seen by running the command - `python Training.py`\r\n\r\nRegards,\r\nDaksh\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "@tensorflowbutler Updated the post.", "Have you thought about using the `dataset` API? That would be our preferred solution. Queues are a little bit harder to diagnose without taking a deeper look.", "@tensorflowbutler @drpngx we tried the datasetAPI too, but stuck in the same problem.", "I see. That looks like a `cond` problem.\r\n\r\n/CC @skye ", "@tensorflowbutler  @drpngx  Any update on this?", "It seems to be a tf cond issue. @skye added a new tf cond implementation based on the functional op. Maybe you can try that?"]}, {"number": 14566, "title": "Feature request for making dynamic gradient clipping ", "body": "That I know about gradient, tf has \r\nhttps://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_clipping\r\n\r\nBut I need the clipping gradient can change at training, and I see the all clip gradient doc, not found this function.\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: python pip \r\n- **TensorFlow version (use command below)**: v 1.3.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:Cuda compilation tools, release 8.0, V8.0.61\r\n- **GPU model and memory**: 1080Ti 8G\r\n- **Exact command to reproduce**:\r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi @tensorflowbutler \r\nI update the infomation\r\nthx", "What do you mean by dynamic? Can you describe the API?", "like the learning rate can decay by the step , because the clipping is fixed when begining, \r\nmaybe the clipping can adjust when training like learning", "Sounds good. Do you want to send a PR?", "@kweisamx You can clip by a tensor which changes with the steps. Why do you think the clipping value __has to be__ fixed at the beginning?", "@ppwwyyxx yep, that i have tried, but the clipping not changing by step, ", "Then it sounds more like a bug report and you'd better post your code so people can find out the problems. From the documentation of `tf.clip_by_*` this feature is definitely supported.", "Hi @kweisamx, any update on this issue?\r\n\r\nI test a minimal example for `tf.clip_by_value` and it works fine.\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.reset_default_graph()\r\n\r\nW = tf.get_variable('W', shape=[2, 4], dtype=np.float32)\r\nclip_min_tf = tf.get_variable(\"clip_min_tf\", shape=[], dtype=tf.float32)\r\nclip_max_tf = tf.get_variable(\"clip_max_tf\", shape=[], dtype=tf.float32)\r\n\r\nop = tf.clip_by_value(W, clip_min_tf, clip_max_tf)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    \r\n    sess.run(tf.assign(W, [[1, 1, 2, 4], [3, 4, 8, 5]]))\r\n    \r\n    sess.run(tf.assign(clip_min_tf, 2))\r\n    sess.run(tf.assign(clip_max_tf, 5))\r\n    sess.run(op)\r\n    print(op.eval())\r\n    # >> [[2. 2. 2. 4.], [3. 4. 5. 5.]]\r\n\r\n    sess.run(tf.assign(clip_min_tf, 4))\r\n    sess.run(tf.assign(clip_max_tf, 7))\r\n    sess.run(op)\r\n    print(op.eval())\r\n    # >> [[4. 4. 4. 4.], [4. 4. 7. 5.]]\r\n```"]}, {"number": 14518, "title": "DataSet user provided shuffled order", "body": "It would be very useful if a user will be able to provide the order of examples within a dataset (with repetitions allowed, as only indices are shuffled).\r\nThis would allow having a more complicated logic (which involves balancing data of different types).\r\n\r\nI assume it can be somehow supported by zip-ing together different DataSets, but it would be MUCH easier and more flexible if we could just pass a list of indices. Probably light as well as it shouldn't be a big deal passing one list per epoch.\r\n\r\nPlease tell me if this feature already exists, and if not, please add it :)", "comments": ["Can you sketch the kind of syntax you'd like for this feature? Do you assume that the entire dataset to be shuffled fits in memory?\r\n\r\nI'm not sure whether this should be part of the API or a \"standard recipe\" that we document, but that would depend on how complicated the recipe ends up being....", "For example, if we required the data to fit into memory, something like the following would work:\r\n\r\n```python\r\nx = ...      # Tensor of shape [n, ...] containing all the feature values\r\ny = ...      # Tensor of shape [n] containing all the labels\r\norder = ...  # Tensor of shape [?] containing the desired order\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(order).map(lambda i: (x[i], y[i]))", "Thanks for the quick response!\r\nI do not assume a dataset that fits entirely in memory, actually, my main usage scenario is datasets which are far from that.\r\n\r\nIn the current syntax, to shuffle your elements, you can do something like:\r\n\r\n```python\r\nds = TFRecordDataset([filename])\r\nds = ds.Shuffle(...)\r\n```\r\nBut that's not useful for unbalanced samples classes scenarios.\r\n\r\n\r\nMy thought was adding something like this:\r\n```python\r\nds = TFRecordDataset([filename])\r\nexamples_order = [0,1,2,4,5,0,3,2,0,3,4,2,0,................] #this was calculated by the user as the desired order (notice the repetition of examples)\r\nds = ds.SetExamplesOrder(examples_order)\r\n```\r\nAn example of a common scenario where this is useful:\r\nIn total in your dataset, you have 100m samples of class A, 10k of class B, 10k of class C and 10k of class D. \r\nLet's say that while training, you want minibatches to be balanced.\r\nSo let's say that your minibatch is of size 10, and you want 2 samples of each class.\r\n\r\nUsing this syntax, you can manually use whatever logic you prefer, and all that needs to be passed is the indices list.\r\n\r\nAnother option is to (also?) allow the user to provide custom shuffling python function. This function can be activated before every new epoch if requested.\r\n```python\r\ndef my_foo(seed):\r\n    ... any user logic that returns a list of indices ...\r\n    return examples_order\r\n\r\nds = TFRecordDataset([filename])\r\nds = ds.CustomShuffling(my_foo)\r\n```\r\n\r\nNow, as I mentioned previously, you might say that this is achievable by creating few DataSet instances and zip-ing them together, however, I feel that it complicates things a bit too much, especially since it's comfortable to have all data in a single TFRecords file, and I don't see why not give the user the ability to provide whatever custom indices order he/she wants.\r\n\r\nI believe that this would make the API much more useful especially for people who are outside the \"natural images\" domain. For example - the medical domain which is in its nature highly unbalanced.\r\n\r\n\r\n\r\n\r\n\r\n", "Also, does DataSet.shuffle(buffer_size) require that buffer_size elements will fit into memory? (the actual outputs of the iterator) because if it does, it makes it not very useful in a tfrecords scenario of a big file.", "In case you need a workaround for balancing, here is what I did:\r\n- put your data into folders, each folder is a class\r\n- shuffle the folders\r\n- get one shuffled folders, shuffle the content of the folder and pick as many elements you wish\r\n\r\nYou can then do stuff like give me a batch of 10 classes with 5 samples per class for instance.\r\n\r\nBut this should definitely be handled better in Tensorflow.", "Thanks @jmaye  !\r\nActually, in my case I can't really do that (without bloating the storage), because classes are a bit more complicated.\r\nA class can be a combination of multiple traits - for the sake of example, a single class that I want to detect can be (shape=round AND color=red AND margin=clear)\r\nOn different experiment I may use different combinations and class definitions.\r\nThat's why I strongly prefer to work at the indices level, while having the data stored in a single place.\r\n\r\nFor now I went with something a bit similar to your suggestion, I'm storing the data on a single directory, in raw format (all inputs and outputs are numpy tensors serialized and stored as raw files).\r\nI create a single big filenames list, which already has the oversampling and in general the sampling proportions that I need. \r\nI store one tf.constant() of the input list and one tf.constant() per output.\r\nThen I use Dataset.from_tensor_slices() and use the dataset.map() api to shuffle them, actually read the raw tensors, augment etc.\r\n\r\nThe down side of my approach is that:\r\n1. I don't guarantee minibatch exact balance, just that the overall samples sampled during 1 epoch are balanced (but could theoretically get extreme cases like one minibatch containing only one class)\r\n2. I didn't find a tensorflow operation to load compressed raw files (WITHOUT using tfrecords) so this is probably slower and takes more disk space than needed\r\n\r\nTo solve #1 I can probably switch to zip-ing datasets.\r\nI don't know yet how to solve #2 - I could probably write a tensorflow C++ operation for it but currently have too many things prioritized before that.\r\n\r\nThanks again for the assistance! :)\r\n\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Since I'm not working on this feature (and still not exactly sure what it would entail), I'm going remove my assignment and open it up to \"contributions welcome\".\r\n\r\nA couple of notes though:\r\n* `tf.contrib.data.sample_from_datasets()` has recently been added, and might be useful here (as a substitute for zipping datasets).\r\n* @saeta is looking at better support for random access to datasets (by maintaining an index over them) and might end up adding support that could be useful here too.", "I am contemplating implementing a dataset using an index file to allow random access to (uncompressed) TFRecord files and came across this issue.\r\n\r\n- @saeta  Did you have TFRecords in mind for index support? (and if so - did you get a chance to implement anything?)\r\n\r\n- As for the original issue: Changing the operation from shuffling a dataset to transforming an index dataset by reading the corresponding records  would fit better in the dataset data  streaming model.\r\n```python \r\n    ds = CustomIndexGenerator()\r\n    ds = ds.LookupTFRecordByIndex(\"baseFileName\") # baseFileName.idx contains index\r\n```", "Hi @suphoff ! Great question. Unfortunately, while I wanted to add indexing to TFRecords, I never got around to it. :-( Sorry! -Brennan"]}, {"number": 14489, "title": "sess.run hangs forever despite operation_timeout_in_ms being set", "body": "`sess.run` waits indefinitely on worker when one of the parameter server machines in the cluster fails despite `operation_timeout_in_ms` being set.\r\n\r\nBecause waiting happens inside TensorFlow runtime, there's no way for client to regain control to provide a helpful error message/fix the situation.\r\n\r\n```\r\nsess=tf.Session(config=...operation_timeout_in_ms=1000)  # succeeds even though TF cluster is partially dead\r\nsess.run(remote_op)       # hangs forever\r\n```\r\nYou see this during worker's `session.run`\r\n```\r\n2017-11-11 16:06:12.524442: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-11 16:06:22.525582: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-11 16:06:32.525718: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n```\r\n\r\nSuggestion: since `CreateSession` happens during `session.run`, make it subject to `.operation_timeout_in_ms` deadline\r\n\r\ncc @mrry \r\n", "comments": ["@mrry would you please comment?", "Indeed, it looks like the cancellation isn't propagated to the `WorkerService.ListDevices()` RPCs, which will attempt to reconnect forever if the host is unavailable. It seems reasonable to add that, and hopefully someone will volunteer to do it.\r\n\r\nJust to check though, @sherrym: was there any reason you didn't add a timeout to this operation in fa07bfda6896748b747334e750763f6feea91129? (My recollection is that that change was to add a timeout that would mitigate run-time hangs, rather than startup hangs, for `SyncReplicasOptimizer`.)\r\n\r\n/cc @saeta @suharshs as the two people who've been in there most recently, and might enjoy implementing the feature :). I'll mark as contributions welcome in case anybody else fancies a go."]}, {"number": 14482, "title": "[Fetaure Request] Bicubic interpolation using map coordinates", "body": "I would like to train a two CNN in single graph. The architecture of model is \r\n![capture](https://user-images.githubusercontent.com/18436807/32688705-3ea9e54c-c6d6-11e7-84ba-fb01a3be2755.PNG)\r\n\r\nThis one is not like end to end training. In the warping of images(refer attachment), I use scipy for mapping coordinates from the disparity map to an input image and I am unable to build a model(graph) in tensorflow using scipy. Tensorflow supports only resize of images using bicubic/bilinear interpolation. [https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.interpolation.map_coordinates.html](url)\r\n\r\nFor reference, original paper is implemented using matconvnet in [matlab.](http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/SIGASIA16/) \r\n\r\n\r\n", "comments": ["We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!", "Thanks for your time. I have one question.\r\n\r\nI will train a CNN1 using extracted depth features and freeze a graph. Then I will do an bicubic interpolation with disparity map in images using SciPy. Later, I use that warped images to train a CNN2 with tensorflow. Will auto-differentiation work in this case ? \r\n\r\nI read that tensorflow follows static graph construction. So probably it works only in an end to end training. (Not sure ! )"]}, {"number": 14441, "title": "GPU support for sparse_dense_binary_op_shared.cc", "body": "Are you planning to add GPU support for the operations defined in \r\ntensorflow/tensorflow/core/kernels/sparse_dense_binary_op_shared.cc\r\n\r\nI see that there is a GPU version for the sparse to dense Matrix-Multiplication version \r\ntensorflow/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc \r\n\r\nbut being able to use the GPU for Component-Wise operations would be very helpful.\r\n\r\nThank you.\r\n\r\nRegards,\r\nPierre", "comments": ["@ebrevdo, can you comment on this?", "No current plans; will leave this as contributions welcome.", "Hi, the issue is interesting for me. I'd like to take a try.", "That would be really useful. Let me know if you try to do it and need some help.", "Yes, I'd like working on it. The issue seems not difficult, however it might still take a little long time (perhaps two weeks) for me, because I'm a novice in gpu programming. Is it OK? \r\n\r\nAnyway, I'll tell you if I get in trouble. Thanks. ", "Hi, I'm a little busy this week. So if anyone is interested, you can take over the issue  :-) \r\nIf I start, I will post here. If someone else starts first, hopefully they'll post here."]}, {"number": 14428, "title": "error when I try mpi_collectives features", "body": "I want to try mpi_collectives features in tensorflow 1.4, but when I run mpi_allreduce_test.py example using python2.7 in contrib/mpi_collectives, I got error.\r\n\r\nI installed tensorflow with openmpi.\r\n\r\nmpirun -n 2 python mpi_allreduce_test.py\r\n======================================================================\r\nERROR: test_mpi_allreduce (__main__.AllreduceTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"mpi_allreduce_test.py\", line 82, in test_mpi_allreduce\r\n    average=average_allreduce)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/mpi_collectives/__init__.py\", line 160, in allreduce\r\n    mpi_size = tf.cast(size(), tensor.dtype)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py\", line 71, in size\r\n    return MPI_LIB.mpi_size(name=name)\r\nAttributeError: 'module' object has no attribute 'mpi_size'\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.034s\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "sorry for incomplete information. \r\nI installed tensorflow r1.4 from source code. \r\nBazel version is  0.5.4.\r\nhere is output that I use tf_env.sh.\r\n\r\n====bazel version==============================================\r\nuild label: 0.5.4- (@non-git)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Nov 7 04:33:30 2017 (1510029210)\r\nBuild timestamp: 1510029210\r\nBuild timestamp as int: 1510029210\r\n\r\n== cat /etc/issue ===============================================\r\nLinux gpu01 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux gpu01 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.4.0)\r\ntensorflow-gpu (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc2)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libmpi.so.40: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Nov 10 21:00:53 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K20m          Off  | 0000:84:00.0     Off |                    0 |\r\n| N/A   24C    P8    15W / 225W |      0MiB /  4742MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n\r\n", "We currently don't have anybody working on mpi_collectives, which is a community contribution. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!"]}, {"number": 14425, "title": "Support Windows builds through clang", "body": "Right now, the tested configurations for Linux, Mac and Windows use gcc, clang and MSVC, as seen at \r\nhttps://www.tensorflow.org/install/install_sources#tested_source_configurations\r\n\r\nLinux can be made to compile with clang, too, if you use the right magic trick (`--config=cuda_clang`). All that's left is Windows, which is probably the trickiest of the three.\r\n\r\nBonus points for allowing to cross-build for Windows under Linux. The main problem there might be with CUDA and getting its SDK installed on a Linux system (just a wild guess).\r\n\r\n(Forked from an existing discussion under https://github.com/tensorflow/tensorflow/issues/12052)", "comments": ["This sounds potentially useful for development, but it's unlikely anyone on the team will take this on soon, because we already have a supported configuration that works well enough. Marking it as contributions welcome, in the hope that somebody wants to take it on.", "sounds greatly interesting, I'll try to investigate into this.", "I see that @gunan has written a windows clang toolchain under third_party/toolchains/preconfig/win/bazel_211, I'll probably assemble a new configuration with that and appropriate platform with new constraints. Right now I'm stuck on getting bazel working on my local machine, similar to #31608, where undname.exe couldn't be found under VC directory ", "That toolchain was constructed for MSVC 2019. AFAICR, all clang references to it are placeholders\r\n@artem-b may be aware if we have windows clang toolchains checked in.", "I don't think you need one. I've been using the default one provided by bazel.\r\nI've basically followed these instructions:\r\nhttps://docs.bazel.build/versions/master/windows.html#build-c-with-clang\r\n\r\n", "@Artem-B Just that will do? I had seen those instructions, but was wondering if that's too simple for a \"tricky\" problem, then I realized this issue was opened back in 2017, perhaps Bazel didn't support this option back then.\r\n\r\nBut the platform target still need to be manually added by users afterwards right? I don't see it written anywhere in the source code. Do you think it would be still useful for me to add it somewhere and make a new config for clang on windows by incorporating those options and modifying the `config.py` ?\r\n\r\nAnother thing, MSVC seems to be the only option for CUDA compiler, I'm not sure how relatable is it to this issue or if it's already an open issue somewhere, but it might also be useful to include clang in CUDA configuration for windows?", "BUILD/WORKSPACE files will need to have these modifications, though I think they will eventually make it into TF.\r\nAdding it all to the config.py may be a bit premature -- people will run with it and will start complaining when things don't quite work, and builds on Windows do have a lot of sharp corners. \r\n\r\nAs for CUDA compilation, clang should be able to do it in principle, but TF build needs a bit of work to put things together.\r\nBasically the issue is that windows build with clang uses `clang-cl` which is a drop-in replacement for MSVC's `cl` and uses different command-line syntax. As far as bazel is concerned, CUDA compilation with clang is another C++ compilation with a few extra options. The problem is that TF currently only constructs these extra options to be used by regular `clang`  and will need to prefix all of them with `/clang` (I think) if they were to be used with `clang-cl` on windows.\r\n\r\n> MSVC seems to be the only option for CUDA compiler, \r\n\r\nDo you mean NVCC? MSVC can't compile CUDA. That said, if you build with clang, your only choice is to whether enable CUDA or not. You can't (well, shouldn't, really) use NVCC in this case. I vaguely recall having to tweak config.py to force-enable CUDA for the build w/ clang."]}, {"number": 14369, "title": "configure.py should remember previous session as defaults", "body": "### System information\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1454666/tf_env.txt)\r\n\r\nBranch tensorflow-r1.4\r\n\r\n### Describe the problem\r\n\r\nUsing instructions at https://www.tensorflow.org/install/install_sources to build from source.  When run configure.py a second time, defaults should be the options chosen the previous time.  Filling out all the questions again is exceedingly tedious.\r\n", "comments": ["Two work-arounds:\r\n1. with https://github.com/tensorflow/tensorflow/pull/13495 , you don't need to run configure. Just run it once, and then pulling/switch branches will remember the old values\r\n2. Use expect script like https://github.com/yaroslavvb/stuff/blob/ab45a0463f12942aef4561799523a2f80e56d7de/configure_tf_cpu.sh to automatically answer the questions", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes.\n\nThe Nettles: Celtic. Eclectic. Folkadelic.\nwww.TheNettles.com <http://www.thenettles.com/>\n\nOn Thu, Feb 8, 2018 at 11:21 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> Nagging Awaiting Response: It has been 14 days with no activityand the awaiting\n> response label was assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14369#issuecomment-364218815>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOw_oVvknzR1biB6kSo1wpiD584xCaAnks5tS0kCgaJpZM4QWr8_>\n> .\n>\n"]}, {"number": 14221, "title": "bayeslfow.hmc - provide option for skipping the MH step", "body": "Currently `tf.contrib.bayesflow.hmc.kernel` returns directly the update x and value of the potential after the Metropolis-Hasting steps. It would be useful to have an option to omitting the MH step. This, for instance, is required for implementing HVI [1], where we want to propagate gradients through the HMC step and not reject any samples:\r\n\r\n[1] https://arxiv.org/pdf/1410.6460.pdf\r\n", "comments": ["@jvdillon : Any thoughts on this?\r\n\r\n@botev : You may want to consider sending a pull request."]}, {"number": 14165, "title": "Equivalent of caffe iter_size in TF", "body": "\r\nCan we get the equivalent of caffe's iter_size parameter in TF? This accumulates gradient calcs over several GPU cycles before doing the weight update. It effectively allows a larger batch size. TensorFlow doesn't natively have this but some ppl seem to have implemented sth like it themselves, e.g\r\n\r\nhttps://stackoverflow.com/questions/42156957/how-to-update-model-parameters-with-accumulated-gradients\r\n\r\nI think it'd be a useful parameter to have as part of official TF ... or is there some easy way to implement this functionality already?\r\n\r\nShaun", "comments": ["You can achieve this using tf control flows. .I'm going to mark this as \"contributions welcome\" since I think it would be a nice feature and would benefit the whole community.\r\n", "I really want an iter_size, or I have to turn to pytorch\r\nAnyone could do something?", "> Anyone could do something?\r\n\r\nIt's easy to implement and you can take the code from here:\r\nhttps://github.com/tensorpack/tensorpack/blob/45ebac959f34507f29176fc12d327f3cc9ff7468/tensorpack/tfutils/optimizer.py#L133-L202", "> > Anyone could do something?\r\n> \r\n> It's easy to implement and you can take the code from here:\r\n> https://github.com/tensorpack/tensorpack/blob/45ebac959f34507f29176fc12d327f3cc9ff7468/tensorpack/tfutils/optimizer.py#L133-L202\r\n\r\nThank you for providing so excellent repo. Now I have installed and I am preparing for testing on my previous code. If there is no problem, I'd like to introduce it to others.", "Thanks. To use this optimizer it takes only one extra line of code:\r\n```python\r\n        from tensorpack.tfutils.optimizer import AccumGradOptimizer\r\n        myopt = tf.train.GradientDescentOptimizer(0.01)\r\n        myopt = AccumGradOptimizer(myopt, niter=5)\r\n        train_op = myopt.minimize(loss)\r\n```", "> Thanks. To use this optimizer it takes only one extra line of code:\r\n> \r\n> ```python\r\n>         from tensorpack.tfutils.optimizer import AccumGradOptimizer\r\n>         myopt = tf.train.GradientDescentOptimizer(0.01)\r\n>         myopt = AccumGradOptimizer(myopt, niter=5)\r\n>         train_op = myopt.minimize(loss)\r\n> ```\r\n\r\nOK,  got it and I am running a program with AccumGradOptimizer. I worry that batch norm will go wrong because someone before gave his code with the same target but I came across many bugs. \r\nSo I am testing.\r\nAnother problem is learning rate. I should enlarge learning rate according to niter, or not?", "> I worry that batch norm will go wrong \r\n\r\nIf your per-iteration batch size is too small and make BatchNorm fail, `iter_size` will not help you.\r\nIn that case you'll need cross-GPU BatchNorm (which is also supported in tensorpack), or GroupNorm as a replacement (which I authored).\r\n\r\n>  I should enlarge learning rate according to niter, or not?\r\n\r\nWhat my optimizer does is written in the code:\r\n>   An optimizer which accumulates gradients across :math:`k` :meth:`minimize` calls,\r\n and apply them together in every :math:`k`th :meth:`minimize` call.\r\n This is roughly the same as using a :math:`k` times larger batch size plus a\r\n :math:`k` times larger learning rate, but uses much less memory.\r\n\r\nWhether you want to adjust learning rate on top of that is up to you. Based on common folk wisdom you do not need to adjust learning rate further.", "> > I worry that batch norm will go wrong\r\n> \r\n> If your per-iteration batch size is too small and make BatchNorm fail, `iter_size` will not help you.\r\n> In that case you'll need cross-GPU BatchNorm (which is also supported in tensorpack), or GroupNorm as a replacement (which I authored).\r\n> \r\n> > I should enlarge learning rate according to niter, or not?\r\n> \r\n> What my optimizer does is written in the code:\r\n> \r\n> > An optimizer which accumulates gradients across :math:`k` :meth:`minimize` calls,\r\n> > and apply them together in every :math:`k`th :meth:`minimize` call.\r\n> > This is roughly the same as using a :math:`k` times larger batch size plus a\r\n> > :math:`k` times larger learning rate, but uses much less memory.\r\n> \r\n> Whether you want to adjust learning rate on top of that is up to you. Based on common folk wisdom you do not need to adjust learning rate further.\r\n\r\nYour repo helps me a lot. But I find that **Global Step** is not supported while some repos train networks with dynamic learning rate, or other tricks, where Global Step is necessary. So I would like to know whether it's difficult to implement with Global Step? Or other reasons?", "> But I find that Global Step is not supported while some repos train networks with dynamic learning rate, or other tricks, where Global Step is necessary. So I would like to know whether it's difficult to implement with Global Step? Or other reasons?\r\n\r\nIt takes about [three lines of code](https://github.com/tensorpack/tensorpack/commit/d8dd7ca93640c7299d2c2da064d57982fe56ff69) to support it.\r\nIt was not there earlier because tensorpack already manages global_step by other methods, rather than by the optimizer.", "> > But I find that Global Step is not supported while some repos train networks with dynamic learning rate, or other tricks, where Global Step is necessary. So I would like to know whether it's difficult to implement with Global Step? Or other reasons?\r\n> \r\n> It takes about [three lines of code](https://github.com/tensorpack/tensorpack/commit/d8dd7ca93640c7299d2c2da064d57982fe56ff69) to support it.\r\n> It was not there earlier because tensorpack already manages global_step by other methods, rather than by the optimizer.\r\n\r\nWOW! So fast updating! What an enthusiastic man!Thank you."]}, {"number": 14081, "title": "Feature Request: C++ gradient for Cast", "body": "Implement the gradient for Cast in C++ so that it is available for TF_AddGradients.\r\n\r\nThis is the Python code that I believe would need to be ported:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/python/ops/math_grad.py#L1109-L1120\r\n\r\nWill be asking @bpiel for guidance if I get stuck!", "comments": ["FYI @suharshs @skye @andydavis1 \r\n\r\nContributions are welcome! See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/README.md for some more information on adding gradients.", "@DavidYKay Are you working on that ?", "@theflofly I think the dibs has expired on this one if you were looking to take it. \r\nI was thinking about taking it but it looks really hard =)\r\nThis is a high priority grad for Guildsman so if there isn't any activity on it by the end of the week I'll volunteer as tribute", "@gdeer81 I have some work in progress for now so you can take it, I will eventually review your work if it can help.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you."]}, {"number": 14075, "title": "Momentum SGD is very slow with large embedding layer", "body": "Hi, I have an (256 col* 500000row) embedding layer in my model and I found that Momentum SGD is 10x slower than adam optimizer. When I printed the global variables using API :tf.global_variables(), I found a large Momentum  variable created by optimizer, does that means Momentum SGD have not  supported sparse update yet?", "comments": ["I'm assuming you mean that AdamOptimizer is slower than MomentumOptimizer; Adam applies momentum to the whole variable, Momentum only applies it to slices affected. Does [tf.contrib.opt.LazyAdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/contrib/opt/LazyAdamOptimizer) work for you?", "@allenlavoie sorry, I mean AdamOptimizer is faster than MomentumOptimizer.In my case, \r\nAdamOptimizer can run 11.58 batch per second, while MomentumOptimizer can only run 5 batch per second.I run TF 1.3.1 in K40 GPU.\r\n\r\n", "Can you post code to reproduce? It sounds like this is not a sparsity issue, since MomentumOptimizer handles sparse updates more quickly than Adam (in theory). Maybe a device placement issue?", "The code is complex and has a lot preprocessing code(using Dataset API and TF data), I can write a  test code for it.Could you give some advice to test whether it is a placement  issue? I just changed the code from AdamOptimizer to MomentumOptimizer.The ways how the optimizer placed should be same.\r\n![image](https://user-images.githubusercontent.com/25046619/32200558-3c588e72-bda0-11e7-91e3-893c52f63c05.png)\r\n![image](https://user-images.githubusercontent.com/25046619/32200586-6355a2bc-bda0-11e7-9971-e57da1662071.png)\r\n\r\n", "And tf.contrib.opt.LazyAdamOptimizer can speed up training speed, It really help.", "I'd start with [log_device_placement](https://www.tensorflow.org/tutorials/using_gpu#logging_device_placement); maybe sparse Momentum updates don't have a GPU kernel (or don't have one for the dtype you're using)?", "![image](https://user-images.githubusercontent.com/25046619/32200877-eb4c38f6-bda1-11e7-9ea7-251a75f4c6a5.png)\r\nIt seems sparse Momentum updates don't have a GPU kernel?", "Oh, right, Adam's sparse updates are in terms of other ops rather than being fused. Looks like none of the other optimizers have GPU kernels implemented for the sparse updates (they do have GPU kernels for dense updates).\r\n\r\nOne approach would be to re-implement [MomentumOptimizer's sparse updates](https://github.com/tensorflow/tensorflow/blob/1f582aa3b24d59400492cc74876f0e075e7c106c/tensorflow/python/training/momentum.py#L97) in terms of the [same ops Adam is using](https://github.com/tensorflow/tensorflow/blob/1f582aa3b24d59400492cc74876f0e075e7c106c/tensorflow/python/training/adam.py#L171) rather than using the custom fused kernel. May be difficult without also using Adam's dense update behavior (which is more correct, but would upset lots of people).\r\n\r\nOr it may be possible to implement the sparse fused GPU kernel efficiently using CUB.\r\n\r\nOr you could make a simple copy of MomentumOptimizer which always does dense updates, since those will run on the GPU and be faster in your case. This is more of a hack, but certainly contributions welcome for either of the other options.", "Thanks @allenlavoie ~"]}, {"number": 14025, "title": "compile tensorflow-1.4.0-rc1 failed with error: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\nIt must be a bug or a feature request.\r\nThe form below must be filled out.\r\nIt shouldn't be a TensorBoard issue. Those go here.\r\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10 beta2\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): TensorFlow 1.4.0-rc1\r\nPython version: Python 2.7.14\r\nBazel version (if compiling from source): bazel 0.6.0\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\nbazel build --config=mkl --copt=\"-g\" --copt=\"-DEIGEN_USE_VML\" --copt=\"-mavx2\" --copt=\"-mfma\" --copt=\"-O3\" --verbose_failures --copt=\"-L/opt/intel/gcc/lib64\" -s -c opt //tensorflow/tools/pip_package:build_pip_package\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nDescribe the problem\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nFailed to use bazel 0.6.0 to compile tensorflow 1.4.0-rc1 on Ubuntu 17.10beta2\r\n\r\ninstall Bazel\r\ndownload bazel-0.6.0-installer-linux-x86_64.sh\r\nbash bazel-0.6.0-installer-linux-x86_64.sh to install bazel\r\nsource /usr/local/lib/bazel/bin/bazel-complete.bash\r\n\r\ncompile tensorflow\r\n./configure\r\n\r\nbazel build --config=mkl --copt=\"-g\" --copt=\"-DEIGEN_USE_VML\" --copt=\"-mavx2\" --copt=\"-mfma\" --copt=\"-O3\" --verbose_failures --copt=\"-L/opt/intel/gcc/lib64\" -s -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\ncompile failed\r\nSource code / logs\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n_ERROR: /home/automation/tensorflow-1.4.0-rc1/tensorflow/python/BUILD:2953:1: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command\r\n(cd /root/.cache/bazel/_bazel_root/35d546f7441fd09e73ff30ea3d9aa112/execroot/org_tensorflow && \r\nexec env - \r\nbazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i): swig failed: error executing command\r\n(cd /root/.cache/bazel/bazel_root/35d546f7441fd09e73ff30ea3d9aa112/execroot/org_tensorflow && \r\nexec env - \r\nbazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i).", "comments": ["@martinwicke \r\nI have installed the swig\r\n _apt list swig\r\nListing... Done\r\nswig/artful,now 3.0.10-1.2 amd64 [installed]_\r\n\r\nBut still git the same issue when compiling the tensorflow 1.4.0rc1 on ubuntu17.10beta2\r\n", "Could you try forcing swig to version 3.0.8? We know it works with that, and because it's a segfault, I suspect swig.", "@martinwicke Thanks a lot. I have downloaded the swig 3.0.8, but how could I force to use this swig version when I build the tensorflow?", "Technically, it is supposed to be downloaded and used by bazel automatically. Can you attach the build log so we can see the actual error?", "@martinwicke Sure\uff0cof course. I have try the new release version 1.4.0\r\nConfigure:\r\nTensorflow 1.4.0\r\nOS Ubuntu17.10GA\r\nBazel 0.6.1\r\nswig version: \r\n _apt list installed swig\r\nswig/artful 3.0.10-1.2 amd64_\r\n\r\nThe build log with the error message is:\r\n\r\n_ERROR: /root/tensorflow-1.4.0/tensorflow/python/BUILD:2953:1: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/c4b1fa2cb5a1fd6ccec3f5694c872872/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i): swig failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/c4b1fa2cb5a1fd6ccec3f5694c872872/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i).\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build_\r\n\r\nBTW: I have downloaded the swig 3.0.8 package, but have no idea about how to use it\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm sorry, I have no idea what the problem is and I cannot reproduce this here."]}, {"number": 14012, "title": "tfdbg ps -b command does not work on Windows", "body": "Issue:\r\n\r\nThe tfdbg ps -b command does not annotate the source file source file beginning at the given line i.e. the output of ps -b 10 source.py is the same as ps source.py.\r\n\r\nSteps to reproduce:\r\n\r\n1. Open the command prompt\r\n2. Run python -m tensorflow.python.debug.examples.debug_mnist --debug\r\n3. Press r\r\n4. Type ps -b 10 path/to/debug_mnist\r\n\r\nSystem information:\r\n\r\n* Tensorflow 1.3.0 (installed using Anaconda)\r\n* OS: Windows-10-10.0.15063-SP0\r\n* Python: 3.6.1\r\n", "comments": ["@hsm207 You are right! I have tried to run `ps -b 10`  in `tf 1.3 & tf 1.5` . It's producing same output.I guess for windows OS it's not working.\r\nDid you tried for other OS?", "@pntdhruv I couldn't try it on other OS because I only have a windows laptop.", "@hsm207 sure.No problem.", "@hsm207 thanks for reporting this issue! Sorry for the inconvenience. \r\n\r\nThis command should be functioning on linux and mac, for which we have unit test coverage. \r\nFor windows we don't have the test coverage. I remember running into the issue, possibly due to some Windows CR/LF peculiarities. \r\n\r\nIn any case, we'll look into it. ", "@caisq I would like to help fix this bug. Would you mind mentoring me? I've never made an open source contribution before.\r\n\r\nCould you please tell me which file has the unit test coverage for Linux?"]}, {"number": 13989, "title": "Fused batch norm can be folded with atrous conv2d ", "body": "### Describe the problem\r\nAs we can fold batch norm with convolution, we should also fold batch norm with atrous convolution, which has not been implemented.\r\n", "comments": ["Marking contributions welcome while we wait for PR review.", "Hi, \r\n\r\nI am working on folding batch norm with atrous convolution for quantization experiments. It seems the Space to Batch -> Conv -> Batch to Space -> Batch Norm -> Activation isn't getting folded correctly. There seems to be a shape incompatibility. Has anyone made progress on this issue? "]}, {"number": 13796, "title": "Feature Request: use S3 for checkpoint loading/saving", "body": "EFS is not available in most AWS regions. For distributed TensorFlow in those regions one would have to rely on S3 to save/restore checkpoints", "comments": ["Perhaps the code in here is relevant?\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/51e5b692a5f8f6942cb43291ba9faab39e4b6104/tensorflow/contrib/s3", "@yaroslavvb The S3 support was added in #11089. I think it is supported in nightly build by default. Please let me know if there are any issues.", "@yongtang thanks, that code is definitely relevant. Checkpoint support would need Python wrappers and perhaps some extra features to hide the fact that S3 is not a real filesystem", "Thanks @yaroslavvb. I am also working in this area though haven't touched checkpoint yet. Please keep me posted and let me know if I could be of any help.", "Hello @yaroslavvb . I was one of the early engineers of Minio, we are keen on making this integration. The SageMaker-Python Tensorflow wrapper currently provides this support. If you could share your thoughts on the interface it would help us to get started. \r\n", "Hi Guys, Can anyone please let me know if I can use tf.train.latest_checkpoint or tf.train.load_checkpoint with an S3 url to load the model checkpoint? Whenever I use either of these to load a checkpoint file I get InvalidArgumentError: S3 path doesn't contain an object name: s3://url/", "@kgramm9026 \r\nYou can't load the checkpoint files directly from S3. Use the `boto` library to download the files from S3 to local storage on your target machine (in my case, it was AWS Lambda). Then I restore the model from the files (following example uses an old version of the tf api, there should be a better way to do it in newer versions)\r\n\r\n```\r\nckpt_dir = os.path.join( ... )\r\nckpt_state = tf.train.get_checkpoint_state(ckpt_dir, latest_filename)\r\nsaver.restore(session, ckpt_state.model_checkpoint_path)\r\n\r\n```", "@kgramm9026 It should be possible to use `s3://` directly though you have to use s3  `s3://bucket/object` format.", "For me saving checkpoints still gives errors however. \r\n\r\n`OSError: Unable to create file (unable to open file: name = 's3://bucket/key/20200120_102319968195_some_name/weights.01-0.4383.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)`\r\n", "> For me saving checkpoints still gives errors however.\r\n> \r\n> `OSError: Unable to create file (unable to open file: name = 's3://bucket/key/20200120_102319968195_some_name/weights.01-0.4383.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)`\r\n\r\nUse tf format instead of hdf5.", "any updates here?"]}, {"number": 13740, "title": "AV in nvcuda on Win10 amd64", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: example script startup\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 amd64 10.0.16291.0\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'unknown' 1.3.0\r\n- **Python version**:  3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0.61.2/6.0\r\n- **GPU model and memory**: nVidia 1080Ti\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\n```\r\n\r\n### Describe the problem\r\n\r\nAccess violation in nvcuda\r\n\r\n### Source code / logs\r\n\r\n```\r\n2017-10-15 22:51:24.306411: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-15 22:51:24.306463: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\n```\r\n0:000> kn\r\n # Child-SP          RetAddr           Call Site\r\n00 000000c2`28be6150 00007fff`ace93028 nvcuda!cuTexRefSetAddress+0x309622\r\n01 000000c2`28be6180 00007fff`ace92ac2 nvcuda!cuTexRefSetAddress+0x1759ea\r\n02 000000c2`28be61b0 00007fff`ad02abf2 nvcuda!cuTexRefSetAddress+0x175484\r\n03 000000c2`28be6250 00007fff`ace950d6 nvcuda!cuTexRefSetAddress+0x30d5b4\r\n04 000000c2`28beddd0 00007fff`ace5c2e0 nvcuda!cuTexRefSetAddress+0x177a98\r\n05 000000c2`28bedee0 00007fff`ace5acfe nvcuda!cuTexRefSetAddress+0x13eca2\r\n06 000000c2`28bedf10 00007fff`ace5aa77 nvcuda!cuTexRefSetAddress+0x13d6c0\r\n07 000000c2`28bedf50 00007fff`acec6617 nvcuda!cuTexRefSetAddress+0x13d439\r\n*** WARNING: Unable to verify checksum for C:\\Python36\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd\r\n*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\\Python36\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd - \r\n08 000000c2`28bedf80 00007fff`8d0e1b50 nvcuda!cuTexRefSetAddress+0x1a8fd9\r\n09 000000c2`28bee040 00007fff`8d0e185e _pywrap_tensorflow_internal!perftools::gputools::port::InternalError+0xe0\r\n0a 000000c2`28bee250 00007fff`8d0eebd4 _pywrap_tensorflow_internal!perftools::gputools::cuda::CUDADriver::Init+0x10e\r\n0b 000000c2`28bee2a0 00007fff`8b6225d0 _pywrap_tensorflow_internal!perftools::gputools::cuda::CudaPlatform::VisibleDeviceCount+0x14\r\n0c 000000c2`28bee2d0 00007fff`8b62043b _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds+0x120\r\n0d 000000c2`28bee5f0 00007fff`8b51dd9c _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::CreateDevices+0x25b\r\n0e 000000c2`28bee7a0 00007fff`8b8e96ee _pywrap_tensorflow_internal!tensorflow::DeviceFactory::AddDevices+0x24c\r\n0f 000000c2`28bee880 00007fff`8b71742c _pywrap_tensorflow_internal!tensorflow::DirectSessionFactory::NewSession+0xae\r\n10 000000c2`28beea30 00007fff`8b4c6d9f _pywrap_tensorflow_internal!tensorflow::NewSession+0x12c\r\n11 000000c2`28beeb90 00007fff`8b4aa72b _pywrap_tensorflow_internal!TF_NewDeprecatedSession+0x1f\r\n*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\\Python36\\python36.dll - \r\n12 000000c2`28beebc0 00000000`604aad02 _pywrap_tensorflow_internal!tensorflow::Scope::status+0x138ab\r\n13 000000c2`28beec10 00000000`604aa413 python36!PyCFunction_FastCallDict+0x182\r\n14 000000c2`28beec90 00000000`604888d8 python36!PyObject_CallFunctionObjArgs+0x383\r\n15 000000c2`28beed70 00000000`604ab284 python36!PyEval_EvalFrameDefault+0x3c8\r\n16 000000c2`28beef00 00000000`604aa87f python36!Py_CheckFunctionResult+0x314\r\n17 000000c2`28beefb0 00000000`6048a03c python36!PyObject_CallFunctionObjArgs+0x7ef\r\n18 000000c2`28bef090 00000000`604ab284 python36!PyEval_EvalFrameDefault+0x1b2c\r\n19 000000c2`28bef220 00000000`604a9f98 python36!Py_CheckFunctionResult+0x314\r\n1a 000000c2`28bef2d0 00000000`604a9d65 python36!PyFunction_FastCallDict+0x1b8\r\n1b 000000c2`28bef3a0 00000000`604ad33c python36!PyUnicode_Partition+0x745\r\n1c 000000c2`28bef450 00000000`604ab067 python36!PyType_GenericAlloc+0x72c\r\n1d 000000c2`28bef4d0 00000000`604aa76f python36!Py_CheckFunctionResult+0xf7\r\n1e 000000c2`28bef500 00000000`604888d8 python36!PyObject_CallFunctionObjArgs+0x6df\r\n1f 000000c2`28bef5e0 00000000`604ab284 python36!PyEval_EvalFrameDefault+0x3c8\r\n20 000000c2`28bef770 00000000`604b5ee3 python36!Py_CheckFunctionResult+0x314\r\n21 000000c2`28bef820 00000000`604b5e41 python36!PyEval_EvalCodeEx+0x9b\r\n22 000000c2`28bef8b0 00000000`604b5deb python36!PyEval_EvalCode+0x2d\r\n23 000000c2`28bef920 00000000`6060a864 python36!PyArena_Free+0xa7\r\n24 000000c2`28bef960 00000000`6060a514 python36!PyRun_InteractiveOneObject+0x2b8\r\n25 000000c2`28befa00 00000000`6060a279 python36!PyRun_InteractiveLoopFlags+0xe8\r\n26 000000c2`28befa30 00000000`6055b0b0 python36!PyRun_AnyFileExFlags+0x45\r\n27 000000c2`28befa60 00000000`604f9ea8 python36!Py_hashtable_size+0x5140\r\n*** ERROR: Module load completed but symbols could not be loaded for C:\\Python36\\python.exe\r\n28 000000c2`28befaa0 00000000`1cf8126d python36!Py_FatalError+0x2cb48\r\n29 000000c2`28befba0 00007fff`fab71fe4 python+0x126d\r\n2a 000000c2`28befbe0 00007fff`fc451eb1 KERNEL32!BaseThreadInitThunk+0x14\r\n2b 000000c2`28befc10 00000000`00000000 ntdll!RtlUserThreadStart+0x21\r\n```\r\n\r\n```\r\n0:000> r\r\nrax=0000000000000000 rbx=00000223d1788910 rcx=00000223d1788910\r\nrdx=0000000000000064 rsi=000000c228be61e8 rdi=00000000000003e7\r\nrip=00007fffad026c60 rsp=000000c228be6150 rbp=0000000000000000\r\n r8=00000223be8a0f00  r9=0000000000008000 r10=00000223d04c0030\r\nr11=0000000000000246 r12=0000000000000001 r13=000000005c000001\r\nr14=000000c228be62e0 r15=0000000000000000\r\niopl=0         nv up ei pl nz na po nc\r\ncs=0033  ss=002b  ds=002b  es=002b  fs=0053  gs=002b             efl=00010206\r\nnvcuda!cuTexRefSetAddress+0x309622:\r\n00007fff`ad026c60 488b5010        mov     rdx,qword ptr [rax+10h] ds:00000000`00000010=????????????????\r\n0:000> ub\r\nnvcuda!cuTexRefSetAddress+0x30960e:\r\n00007fff`ad026c4c cc              int     3\r\n00007fff`ad026c4d cc              int     3\r\n00007fff`ad026c4e cc              int     3\r\n00007fff`ad026c4f cc              int     3\r\n00007fff`ad026c50 4053            push    rbx\r\n00007fff`ad026c52 4883ec20        sub     rsp,20h\r\n00007fff`ad026c56 488b81d81b0000  mov     rax,qword ptr [rcx+1BD8h]\r\n00007fff`ad026c5d 488bd9          mov     rbx,rcx\r\n0:000> u\r\nnvcuda!cuTexRefSetAddress+0x309622:\r\n00007fff`ad026c60 488b5010        mov     rdx,qword ptr [rax+10h]\r\n00007fff`ad026c64 817a04d0070000  cmp     dword ptr [rdx+4],7D0h\r\n00007fff`ad026c6b 7c11            jl      nvcuda!cuTexRefSetAddress+0x309640 (00007fff`ad026c7e)\r\n00007fff`ad026c6d e89ec5ceff      call    nvcuda!cuTexRefDestroy+0x113 (00007fff`acd13210)\r\n00007fff`ad026c72 84c0            test    al,al\r\n00007fff`ad026c74 7408            je      nvcuda!cuTexRefSetAddress+0x309640 (00007fff`ad026c7e)\r\n00007fff`ad026c76 488bcb          mov     rcx,rbx\r\n00007fff`ad026c79 e837dfceff      call    nvcuda!cuTexRefSetFlags+0xc8 (00007fff`acd14bb5)\r\n0:000> u\r\nnvcuda!cuTexRefSetAddress+0x309640:\r\n00007fff`ad026c7e 33c0            xor     eax,eax\r\n00007fff`ad026c80 4883c420        add     rsp,20h\r\n00007fff`ad026c84 5b              pop     rbx\r\n00007fff`ad026c85 c3              ret\r\n00007fff`ad026c86 cc              int     3\r\n00007fff`ad026c87 cc              int     3\r\n00007fff`ad026c88 cc              int     3\r\n00007fff`ad026c89 cc              int     3\r\n```", "comments": ["This part of the stack doesn't make sense:\r\n\r\n```\r\n07 000000c2`28bedf50 00007fff`acec6617 nvcuda!cuTexRefSetAddress+0x13d439\r\n08 000000c2`28bedf80 00007fff`8d0e1b50 nvcuda!cuTexRefSetAddress+0x1a8fd9\r\n09 000000c2`28bee040 00007fff`8d0e185e _pywrap_tensorflow_internal!perftools::gputools::port::InternalError+0xe0\r\n0a 000000c2`28bee250 00007fff`8d0eebd4 _pywrap_tensorflow_internal!perftools::gputools::cuda::CUDADriver::Init+0x10e\r\n0b 000000c2`28bee2a0 00007fff`8b6225d0 _pywrap_tensorflow_internal!perftools::gputools::cuda::CudaPlatform::VisibleDeviceCount+0x14\r\n0c 000000c2`28bee2d0 00007fff`8b62043b _pywrap_tensorflow_internal!tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds+0x120\r\n```\r\n\r\n...because the [`port::InternalError()`](https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/stream_executor/lib/status.h#L40) function doesn't call any CUDA APIs. \r\n\r\nHowever, the presence of `InternalError()` on the stack makes it look like [`CUDADriver::Init()`](https://github.com/tensorflow/tensorflow/blob/27767d8e9c1325979cf32ff5b81c10df9006fd57/tensorflow/stream_executor/cuda/cuda_driver.cc#L414) is failing for some reason. Strangely, from reading the code, it seems like that particular call never fails with `port::InternalError()`, so I'm doubly surprised. \r\n\r\nSince it looks like you're comfortable using Windbg, would you be able to set a breakpoint on `CUDADriver::Init()` and step through it to see if there's any more information about the failure?", "> This part of the stack doesn't make sense:\r\n\r\nThis is probably a side effect of the optimizer, if the compiler knows it can reuse blocks of assembly, it'll jump into the middle of functions or reuse them in weirdo ways\r\n\r\n> Since it looks like you're comfortable using Windbg, would you be able to set a breakpoint on CUDADriver::Init() and step through it to see if there's any more information about the failure?\r\n\r\nI can try to but I'll have to dig around building from source", "Also, please try a new version perhaps."]}, {"number": 13644, "title": "Add support for Mel Generalized Cepstrum Analysis to tf.signal.", "body": "Requested via discuss@\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/k6EI-BxbCMg\r\n\r\nPlease :+1: if you would like to see this feature in tf.contrib.signal.", "comments": ["Hi, is this still open to contribute to? It seems like a very interesting module to help with."]}, {"number": 13641, "title": "Extend SVD gradient to support backpropagating through complex and (strongly) rectangular U and V", "body": "This initial version of SVD gradients has the following restrictions:\r\n  Only supports statically known inner matrix dimensions m and n.\r\n\r\nBackpropagating through U and V (i.e. backpropagating through SVD nodes with compute_uv=True) has further restrictions:\r\n  a) Only supports real tensors.\r\n  b) Only supports square and \"almost square\" matrices where the number of rows and columns differ by at most 1.\r\n  c) full_matrices must be true also. This does not currently have severe implications, given the restriction in b).\r\n\r\nSupport for dynamic shapes and a) (I think) is straightforward to fix.  But b) is probably a deeper issue having to do with the (lack of) uniqueness of the decomposition, and will require some analysis. I think that if we understand b), we can get around the restriction in c) as well.\r\n\r\nI'm marking this as contributions welcome, in the hope that somebody with better math skills than myself will help out :-)", "comments": ["It looks like the implementation in autograd is better in several respects. That is probably a good starting point for working on this: https://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L120", "This article studies the analytical computation of the Jacobian of the SVD decomposition, maybe it can be of use:\r\n\"Estimating the Jacobian of the Singular Value Decomposition : Theory and Applications \" by Theodore Papadopoulo and Manolis L.A. Lourakis\r\nhttps://hal.inria.fr/inria-00072686/document\r\nSection 2.3.1 analyzes the degenerate case with repeated singular values. Their suggestion is to use a least square approach to obtain the \"minimum norm\" Jacobian.\r\n\r\nThat's probably why the finite difference gradient doesn't match the analytical one: even if it's able to compute a gradient, it may be only one of many.", "@guivenca thanks for the reference! I was thinking that such a minimal norm solution might be the right approach, but didn't work out the math. I'll study this paper.", "@rmlarsen I just wanted to say that I really appreciate your work on this!", "Would a \"minimum norm\" Jacobian actually be the one we need?  It seems to me that we need the Jacobian of the specific SVD function that's used.  I suspect this is an obstacle not only to removing the \"almost square\" restriction but also to extending support to complex tensors, since in the complex case there are always infinitely many choices of U and V.\r\n\r\nEven in the real case, I could imagine the indeterminacy in U and V being a problem if someone tries to optimize over a function that depends on the signs of columns in U and V.  According to the paper @guivenca linked,\r\n> ...even state of the art algorithms for SVD computation (eg LAPACK \u2019s dgesvd family of routines [1]) are \u201cunstable\u201d with respect to small perturbations of the input. By unstable, we mean that the signs associated with the columns and can change arbitrarily even with the slightest perturbation.\r\n\r\nAnyway, it looks to me like it wouldn't be too hard to add support for `full_matrices=False`.  And I think it wouldn't need the restriction that the matrices are almost square.  If it seems useful, I'd be happy to work on a pull request for that; it would essentially be a port of the autograd code.", "@vnavkal I agree with your analysis. It would be super helpful if you would contribute a port of the autograd code. I would eventually get to it but am working on a few other projects right now.", "@davharris Thanks! I'm really hoping that we'll see many more uses of sophisticated statistical models implemented  in TensorFlow with these types of algorithms available.", "any update on this matter?", "I think rectangular SVD is already supported by the pull requests above. We have some work in progress on complex support, I'd guess it should be a few weeks to get it baked.", "any update on complex?", "I am confused that  I should write in both python backend and C++ backend  or just one backend is enough? (forgive my bad English)\r\nCan anyone help me? I am really want to known", "No updates of interest thus far. We have the derivation for complex SVD reverse mode sensitivity, but have not yet implemented in TF, and have reduced the priority of that work.", "@brianwa84 just as a question, I have derived the back prop for eigenvalue decomposition is that even needed? Since svd is more stable I think it is not needed ", "this is super complex work!", "> Would a \"minimum norm\" Jacobian actually be the one we need? It seems to me that we need the Jacobian of the specific SVD function that's used. \r\n\r\nAs far as I understand, the idea is to through out the direction which is \"parallel\" to the family of the SVDs, since it will not move you to another family. While this is not a gradient, neither equals to numerical gradient, it could possibly work well. Also I think it will get you out of the region with undefined gradient.\r\n\r\n> Even in the real case, I could imagine the indeterminacy in U and V being a problem if someone tries to optimize over a function that depends on the signs of columns in U and V.\r\n\r\n> That the signs associated with the columns and can change arbitrarily even with the slightest perturbation.\r\n\r\nI'm not sure why such an instability would exist. For distinct singular values we can always choose some convention for phase of `U`, for example, first non-zero element of each column is real positive, can't we?\r\n\r\n\r\nAnother idea I had is to try to determine the vectors in subspaces of degenerate singular values such that the we get terms of form 0/0 (i.e., if we have `F * M` (elementwise), and `F_ij = 1/0`, then `M_ij=0`), which then can be regarded as 0 to acquire meaningful gradient, similarly to [degenerated pertrubation theory](https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)#Effects_of_degeneracy). However, I didn't manage to acquire conditions on `U` columns in the subspace which would guarantee than (and not sure whether such conditions exist).", "Any news?", "@brianwa84, sorry to bother. You mentioned that you guys had derived the backprop formula for complex valued SVD. Do you have any technical reports or notes? or just some hints or a direct formula on this? Since I am into this question recently and find that the formula should be very different from the real one due to the possible imaginary diagonals for $U^\\dagger dU$. \r\n\r\nI didn't find many relevant useful resources on this topic. The derivation in [this](https://giggleliu.github.io/2019/04/02/einsumbp.html) definitely overlooked the diagonal part of $U^\\dagger dU$, a very common loophole. In [this paper](https://arxiv.org/pdf/1907.13422.pdf), the author also left backprop for complex SVD unsolved.\r\n\r\nAny help on this is appreciated. (I am also curious why tf team reduced the priority of the implementation when you have the derivations, I'd guess it is a super complex formula?)", "> @brianwa84, sorry to bother. You mentioned that you guys had derived the backprop formula for complex valued SVD. Do you have any technical reports or notes? or just some hints or a direct formula on this? Since I am into this question recently and find that the formula should be very different from the real one due to the possible imaginary diagonals for $U^\\dagger dU$.\r\n> \r\n> I didn't find many relevant useful resources on this topic. The derivation in [this](https://giggleliu.github.io/2019/04/02/einsumbp.html) definitely overlooked the diagonal part of $U^\\dagger dU$, a very common loophole. In [this paper](https://arxiv.org/pdf/1907.13422.pdf), the author also left backprop for complex SVD unsolved.\r\n> \r\n> Any help on this is appreciated. (I am also curious why tf team reduced the priority of the implementation when you have the derivations, I'd guess it is a super complex formula?)\r\n\r\nInteresting point, could you please show which part ignores the diagonal imaginary part in [this](https://giggleliu.github.io/2019/04/02/einsumbp.html) blog? This formula was tested in this repo: https://github.com/GiggleLiu/BackwardsLinalg.jl/blob/master/test/svd.jl . And tested in practice in this [GSoC project](https://github.com/under-Peter/TensorNetworkAD.jl).\r\n\r\nYou rightly that the loss function in these tests is carefully designed. it does not pass tests with a general U adjoint, as it shouldn\u2019t due to the gauge freedom issue. If you can offer a **gauge independent** loss that breaks this test, it will be very helpful for me to fix the derivation.", "Hi @GiggleLiu, \r\n<img width=\"342\"  src=\"https://user-images.githubusercontent.com/35157286/64085121-51d6d180-cd63-11e9-9e7b-2dcbdc60cfd4.png\">\r\nNote how the above formula is valid only for off-diagonal elements. There is one part of differentiation missing, namely the diagonal part $dC_{ii}$ (which is not zero in general but pure imaginary). Unless one can argue that under some gauge, $diag(dC)$ and $diag(dD)$ could be zero simultaneously or somehow cancelled with each other in dL formula.\r\n\r\nActually, we can show that if the loss function is a real gauge invariant function depending on U OR V only, then the formula above is enough (somehow the diagonal part of dC and dD can be cancelled in this case). However, we didn't have a general proof on why we don't need to consider the differentiation in diagonal part for dC and dD in the general case. If the loss function is some thing like `tf.real(tf.conj(V[0,0])*U[0,0])`,  then the numerical test seems failed. If I am understanding right, such an object function is gauge invariant (I'd guess the gauge freedom is some diagonal phase matrix P transforming U and V as UP and VP, or is there a larger gauge freedom?).\r\n\r\nIn sum, either the formula in your post is incomplete with one part missing, or we should have a general proof on why ignoring the diagonal part of dC and dD is safe.\r\n\r\nBest", "Hi, @refraction-ray \r\n\r\nI think you are right. Will look into it ASAP. Thanks!\r\n\r\nPS: I am in IOP, CAS  (\u4e2d\u79d1\u9662\u7269\u7406\u6240), welcome for visiting.", "Some updates:\r\n\r\n1. The test passes for `tf.real(tf.conj(V[0,0])*U[0,0])`, \r\n2. This statement in the blog is not correct\r\n\r\n> where dCS and SdD has zero diagonal elements. So that dS=diag(dP).`dS=diag(dP)`\r\n\r\nShould be `dS = real(diag(dP))`, the code is correct though. The rest are correct. Now the [blog](https://giggleliu.github.io/2019/04/02/einsumbp.html) has been updated.\r\nHere, only non-zero real part is important to the final result, since `dS` must be real.\r\n\r\nNice observation and thanks for correction, @refraction-ray ", "> Some updates:\r\n> \r\n> 1. The test passes for `tf.real(tf.conj(V[0,0])*U[0,0])`,\r\n> 2. This statement in the blog is not correct\r\n> \r\n> > where dCS and SdD has zero diagonal elements. So that dS=diag(dP).`dS=diag(dP)`\r\n> \r\n> Should be `dS = real(diag(dP))`, the code is correct though. The rest are correct. Now the [blog](https://giggleliu.github.io/2019/04/02/einsumbp.html) has been updated.\r\n> Here, only non-zero real part is important to the final result, since `dS` must be real.\r\n> \r\n> Nice observation and thanks for correction, @refraction-ray\r\n\r\nYes, for dS, we can still have the same derivation, but I am not sure about dC and dD part. What is the value for $$dD_{i,i}$$? The formula in the above figure captures only relation on off-diagonal parts (dC has no diagonal contribution to the equation). In other words, dC should be some thing as $dC=dC_0+dC_d$ where dC_0 is the similar part as real SVD case, dC_d part is differentiations on diagonal imaginary parts (unless one can find a proof showing this is zero).\r\n\r\nAnd I am also not sure about the numerical test, since I have some code snippets works for real case or complex case with object function $U[0,0]U*[0,0]$, but fails when loss function is $UV*$. (I will have a double check on this implementation part, though).\r\n\r\n**Update:** Please see [this gist](https://gist.github.com/refraction-ray/32ad9ce58d9bc332ef7a9d0dc537065a) for the quick implementation of complex squared SVD backprop, and I find there is inconsistence between numerical and AD results if the loss function is $R(UV*)$. (I might mess something up or implement something wrong, though)", "I see, indeed, the imaginary part of dC and dD is ignored. Is your test same as this one:\r\n\r\nhttps://github.com/GiggleLiu/BackwardsLinalg.jl/blob/adfcad724273f0403e20ac4712fb7273cbac6f8d/test/svd.jl#L42\r\n\r\nIn our case, it does not error, you'd better check your program again.\r\n\r\nI will look into the imaginary part later, it looks not trivil.", "Hi, @GiggleLiu. \r\n\r\nWe have derived the correct backprop formula for squared SVD today with diagonal part of dC and dD. Their contributions are indeed nonzero. And we also tested the new formula numerically, which matches finite differentiation for `tf.real(tf.conj(V[0,0])*U[0,0])`. We will write up the whole thing soon, and will definitely email you the preprint for discussion before we publish the result.\r\n\r\nBest", "@refraction-ray\r\nCongratuations! look forward to your derivation.\r\n\r\nIt is still not clear to me why the autodiff check passes in our Julia program for the same loss with incorrect formula. \ud83d\ude02  Hope after reading your preprint, I will be clear. PS: You can find my email on my github page.", "Please check the note at https://re-ra.xyz/misc/complexsvd.pdf for the correct backprop formula of complex valued SVD. (I'd like to post this on arXiv for a better reference in the future, but somehow the submission entered onhold status :( and I decided to share this technical report with the community first.)\r\n\r\nAlso remember to check the updated [blog](https://giggleliu.github.io/2019/04/02/einsumbp.html) by @GiggleLiu , which now has the correct backprop formula and derivation for complex SVD.", "A general remark, @refraction-ray just solves the notorious [gauge problem](https://arxiv.org/abs/1907.13422) in the bacpropgation of complex values SVD, it can be used in [some important applications](https://arxiv.org/abs/1903.09650) in physics.\r\n\r\nIn my previous program, I checked `f(x-a*g) = f(x)-a*|g|^2`, which is not enough, it only gaurantes the training moves to the correct direction, but can loss some components, like a projection of a high dimensional vector to a low dimension `[1, 1] -> [1, 0]`. The missing part is what @refraction-ray finds. With this missing part, we can have a better differential programming quantum world.", "So are we going to see it implemented in TF soon? :)", "@Randl I think so.\r\n@refraction-ray do you want to take this honor? Ping me if you need any help in submitting a pull request. \ud83d\ude04 ", "Happy to help with reviewing/getting it pulled in.\n\nOn Wed, Sep 4, 2019, 8:08 PM Leo <notifications@github.com> wrote:\n\n> @Randl <https://github.com/Randl> I think so.\n> @refraction-ray <https://github.com/refraction-ray> do you want to take\n> this honor? Ping me if you need any help in submitting a pull request. \ud83d\ude04\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13641?email_source=notifications&email_token=AFJFSI72QKIBZMPWJJKQTGDQIBEWTA5CNFSM4D6XKQX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD55MEFI#issuecomment-528138773>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFJFSIYDJ7ODDHHAVZFJAV3QIBEWTANCNFSM4D6XKQXQ>\n> .\n>\n", "> @Randl I think so.\r\n> @refraction-ray do you want to take this honor? Ping me if you need any help in submitting a pull request. \ud83d\ude04\r\n\r\nHi, can I ask If your new update is implemented in TF now already? It is very significant for my research. ", "@helo-nerd , yes, I believe AD on complex valued SVD is ready for tensorflow 2.1+", "> @helo-nerd , yes, I believe AD on complex valued SVD is ready for tensorflow 2.1+\r\n\r\nOhh that's awesome! is this new update the same code list here? Because my server has some problem for TF 2, I wanna continuely use TF 1 by modifying it.\r\nhttps://github.com/tensorflow/tensorflow/pull/32226/commits/124386cd8312c530097cf930be9cdb3544c1e361", "@helo-nerd , yes, the code should be the same as this commit, you can directly hack tf source as this commit, or use [``customize_gradient``](https://www.tensorflow.org/api_docs/python/tf/custom_gradient) in your app code."]}, {"number": 13627, "title": "HDFS user impersonation", "body": "TensorFlow  does support HDFS filesystem but there is no way to specify as which user to access the filesystem.\r\n\r\nThe native hdfs library provides a function to set the user name, hdfsBuilderSetUserName() similar to hdfsBuilderSetNameNode(). There is also hdfsConnectAsUser() where you can specify the user, builds the hdfsBuilder struct, set the username among other arguments and return the FileSystem handle.\r\n\r\nCurrently I don't see any way to provide a username in TensorFlow and more specifically in tensorflow/core/platform/hadoop/hadoop_file_system.cc\r\n\r\nI suppose that something like the following would be sufficient.\r\nhttps://github.com/kouzant/tensorflow/commit/eacef5cb81d09d0490403fde33de8e5526f212ad", "comments": ["We can install our own build of tensorflow. And submit a PR to tensorflow in the meantime.", "Marking as contributions weclome since it seems @jimdowling will submit a PR.\r\n\r\n/CC @jhseu in case he has any comments.", "Sorry, that was an internal comment for @kouzant  - who will submit the PR."]}, {"number": 13607, "title": "Building custom op instructions out of date", "body": "Following instructions here\r\nhttps://www.tensorflow.org/extend/adding_an_op\r\n\r\nTo try to rebuild this [op](https://github.com/yaroslavvb/max_align_bytes_op)\r\n\r\nFirst I ran into issue with nsync headers, fixed by following\r\nhttps://github.com/tensorflow/tensorflow/issues/12482#issuecomment-328829250\r\n\r\nThen while trying to load the `.so` file I run into \r\ntensorflow.python.framework.errors_impl.NotFoundError: ./max_align_bytes_op.so: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n\r\nSo the definition for `tensorflow::OpKernel` is missing\r\n\r\ntf commit: https://github.com/tensorflow/tensorflow/commit/22a886b\r\ncc @allenlavoie ", "comments": ["Sorry about that. The [\"adding an op\" docs are updated at head](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/adding_an_op.md) (and will go live once 1.4 is released), but right now there's a bit of a mismatch. The important bits:\r\n\r\n```TF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')```\r\n\r\nAnd add to the compiler flags:\r\n```-L$TF_LIB -ltensorflow_framework```\r\n\r\nIt is also possible to build with --config=monolithic if you want to load custom ops with RTLD_GLOBAL in Python (should work just as it did pre-1.4).", "@MarkDaoust maybe we should add a little warning on some of these docs that they're versioned, with a link to head?", "Thanks, that fixed it! Looks like desired alignment in latest tf is 32\r\nThere used to be a time when TF docs had a pull-down that allowed to select version of docs, including \"Master\". ", "@allenlavoie \r\n\r\nit did not work.\r\n\r\nthere is no `libtensorflow_framework.so` under `$TL_LIB`\r\n\r\nMy `$TL_LIB` is `/home/tumh/.pyenv/versions/tensorflow/lib/python2.7/site-packages/tensorflow/core`\r\n", "I add both TF_INC and TF_LIB but still cannot fix it.\r\nDoes someone has same problem?", "@twmht that library path comes from a version before TensorFlow 1.4; maybe you have a couple different Python installations with different TensorFlow versions?\r\n\r\n@JuiHsuan-Kuo which TensorFlow version, and what's the error? Note that the updated documentation for 1.4+ is at https://www.tensorflow.org/versions/master/extend/adding_an_op", "Are there relevant/accurate instructions for 1.3.0?  I am seeing the same issue as @twmht   with lack of tensorflow_framework from my 1.3.0 install.", "If I do upgrade to 1.4.0 (which my codebase cannot support) I am able to build the shared object and avoid the missing kernel call but then I'm faced with\r\n\r\n```\r\nImportError: libtensorflow_framework.so: cannot open shared object file: No such file or directory\r\n```", "@bearrito \r\nhttps://www.tensorflow.org/versions/r1.3/extend/adding_an_op are the 1.3 instructions. https://www.tensorflow.org/versions/master/extend/adding_an_op are for master.\r\n\r\nWhat's the context for that ImportError? Importing TensorFlow?", "After messing around w/ the DYLD and LD lib paths, I ended up just brute forcing it....\r\n\r\nbazel build --config=cuda //tensorflow/tools/lib_package:libtensorflow\r\ncp bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz /opt/tensorlibs\r\ntar xvfmz libtensorflow.tar.gz\r\n\r\nAnd then just used linkopt\r\n\r\nbazel build -c opt --config=cuda  //dragnn/core:gen_dragnn_bulk_ops_py_wrappers_cc --verbose_failures --linkopt=/opt/tensorlibs/lib/libtensorflow_framework.so\r\n", "How about the build instructions for GPU ops. Tensorflow tutorials even never mention it. @allenlavoie \r\n\r\nI find one [here](https://gist.github.com/Sergio0694/fc94fb14388ee4b7b92be6e33704e5b9), but it also meets the same problem", "I'm using tf.14. I get the same error when running a custom op: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n\r\nwhen I compile using this flag: -L$TF_LIB -ltensorflow_framework,\r\n\r\nI get this error:\r\n/usr/bin/ld: cannot find -ltensorflow_framework.\r\ncan someone help please?\r\n", "> I'm using tf.14. I get the same error when running a custom op: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n> \r\n> when I compile using this flag: -L$TF_LIB -ltensorflow_framework,\r\n> \r\n> I get this error:\r\n> /usr/bin/ld: cannot find -ltensorflow_framework.\r\n> can someone help please?\r\n\r\nI am having the exact same issue", "I resolved the issue after spending days of scraping the internet to compile a custom tf op. I am including the steps that I had to take for other people to use. \r\n- downgrade your python version to 3.6 \r\n- downgrade your tf to 1.10.0\r\n- use CUDA 9", "@kasrayazdani, sorry I missed your first post. That does not sound like fun.\r\n\r\nThere appears to be a new repository for this: \r\n\r\nhttps://github.com/tensorflow/custom-op\r\n\r\nI will see if I can delete the old doc and just leave a pointer to this new repo.", "yaroslavvb\r\nplease let us know if the comments above help resolve the issue.", "> TF : 1.13.2 (build from source) / cuda 10.0  , cudnn7.4.24.2 , gpu-driver-460 \r\n> OS : ubuntu 18.4\r\n> gcc : 7.5.0\r\n> cmake : 3.22.1\r\n>  python 3.6.9\r\n\r\nMy tensorflow construction order :\r\n\r\n> bazel build --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package\r\n\r\nI also made a custom op for myself, but\r\nwhile trying to load the .so file I run into \r\n\r\n>     _fast_oll = tf.load_op_library('obj_loader/cmake-build/libobj_loader.so')\r\n>   File \"/home/shyan/testtensorflow/azmoon/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 61, in load_op_library\r\n>     lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n> **tensorflow.python.framework.errors_impl.NotFoundError: obj_loader/cmake-build/libobj_loader.so: undefined symbol: _ZTIN10tensorflow8OpKernelE**\r\n\r\nCan anyone help me?"]}, {"number": 13460, "title": "Feature request: segment_argmax", "body": "Currently trying to return argmax from a tensor for selected slices (segments)\r\nslices do not have the same length, so reshaping can't be used.\r\nwas looking for a function similar to tf.segment_max, only with indices as the return value.\r\n\r\nfor e.g\r\na = [1, 2, 3, 4, 5, 6]\r\nseg = [0, 0, 0, 1, 1, 2]\r\ntf.segment_argmax  return value will be\r\n[2, 4, 5]\r\n", "comments": ["Contributions are welcome!", "I can try to implement this. \r\n\r\nPer the example by OP,  the segments are `[1,2,3], [4,5], [6]` and the segment_argmax function returns the indices of max element in each segment, but with a reference to the unsegmented list. \r\n\r\nIs my understanding correct ?", "yes, \r\nindices will be with reference to the unsegmented list.\r\n\r\nmy intention was to use these indices to slice the original tensor for all the max tensors\r\n", "I will go to work on this.", "Any update on this? I would also extend the feature request to support unsorted_segment_argmax, segment_argmax, unsorted_segment_argmin, and segment_argmin.", "Would also like to throw my vote on this as a feature ; this would be super useful. "]}, {"number": 13329, "title": "Auto-Parallel excludes update operators of sparse tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nr1.3\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\ncuda 8.0/cudnn 5.1.5\r\n- **GPU model and memory**:\r\nTesla P40 \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI'm trying to use auto_parallel in grappler, but I found it only controls dense tensors. In the code, only below operators update averaged gradients, but what about 'ScatterSub' for sparse tensors? Do you have a plan to implement it? \r\n\r\n```\r\nconst std::set<string> apply_gradients_ops = {\"ApplyGradientDescent\",\r\n                                                \"ApplyProximalGradientDescent\",\r\n                                                \"ApplyAdadelta\",\r\n                                                \"ApplyAdagrad\",\r\n                                                \"ApplyProximalAdagrad\",\r\n                                                \"ApplyAdagradDA\",\r\n                                                \"ApplyFtrl\",\r\n                                                \"ApplyMomentum\",\r\n                                                \"ApplyAdam\",\r\n                                                \"ApplyRMSProp\",\r\n                                                \"ApplyCenteredRMSProp\"};\r\n```\r\n\r\n### Source code / logs", "comments": ["Is there anyone to give me an answer?", "Sorry, we don't have plan on that yet. Marking as contribution welcome for now."]}, {"number": 13295, "title": "Pre-built binaries with symbol information?", "body": "What you you guys think about providing CI version of TensorFlow with built-in symbol information? This would make it easier to report bugs.\r\n\r\nI believe the following is sufficient to get optimized version with symbol tables.\r\n`blaze build --cxxopt=-g2 --linkopt=-g2 --strip never -c opt`\r\n\r\nMy current problem is that I'm occasionally hitting segfaults due to `tensorflow::strings::FloatToBuffer`. I can't reproduce this in any small example. If there were a version of tf with symbol tables, I could just gdb on the core file and do `info locals` to get the value of offending float that causes the crash.\r\n\r\n```\r\n#0  0x00007ffb54a4941d in std::_Hashtable<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, float>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, float> >, std::__detail::_Select1st, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node(unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long) const ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007ffb5745b76e in float tensorflow::(anonymous namespace)::locale_independent_strtonum<float>(char const*, char const**) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007ffb5745bf4c in tensorflow::strings::safe_strtof(char const*, float*)\r\n    ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007ffb5745bfe9 in tensorflow::strings::FloatToBuffer(float, char*) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007ffb5735d7ce in tensorflow::Tensor::SummarizeValue[abi:cxx11](long long) const ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007ffb5735e3b5 in tensorflow::Tensor::DebugString[abi:cxx11]() const ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007ffb572f21ab in tensorflow::(anonymous namespace)::SummarizeTensor(tensorflow::TensorProto const&) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007ffb572f5529 in tensorflow::SummarizeAttrValue[abi:cxx11](tensorflow::AttrValue const&) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007ffb57322b9d in tensorflow::SummarizeAttrsHelper(tensorflow::AttrSlice, tensorflow::StringPiece) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007ffb573231f3 in tensorflow::SummarizeNodeDef[abi:cxx11](tensorflow::NodeDef const&) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007ffb5732332f in tensorflow::SummarizeNode[abi:cxx11](tensorflow::Node const&) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007ffb55fc7886 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007ffb55fc93cf in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007ffb57442c51 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007ffb57440d37 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007ffb6174c260 in ?? ()\r\n   from /home/yaroslav/anaconda3/envs/sep22/lib/python3.5/site-packages/scipy/sparse/../../../../libstdc++.so.6\r\n#16 0x00007ffb6d9926ba in start_thread (arg=0x7ffaa4906700)\r\n    at pthread_create.c:333\r\n#17 0x00007ffb6cdb03dd in clone ()\r\n    at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n\r\n```", "comments": ["@gunan, @av8ramit WDYT? What would be the impact of building the nightlies with symbols?", "I am ok with this. We just need to see how this would affect our pip package size.\r\nCan we try it and see how our binary, especially GPU pip package sizes grow?", "This\u00a0might have a significant effect on size. An alternative would be a separate \"Linux GPU Debug\" CI build and pip package", "Looks like nice folks at TinyMind are gonna give CI debug builds a shot  -- https://github.com/mind/wheels/issues/2#issuecomment-334295262", "as we are having difficulty getting a hold of pypi, we are blocked on anything that may increase our wheel file sizes.\r\nEventually we may want to do this, but at the moment it looks difficult to do this for us.", "I remember several years ago we were facing similar situation. Need to keep symbol table but have to release a smaller binary.\r\n\r\nAt the end we use `-g` during the build for symbols. Then we use `strip` command on Linux and MacOSX to get the stripped version without symbols. We keep both copy on record but only release the stripped (smaller) version. The symbol version was used for debugging with gdb.\r\n\r\nI am not familiar with `bazel` though maybe it is possible to do it similarly as well?", "I had the same thing at Google, symbols are distributed as separate files. Not sure if it's part of bazel's capabilities, perhaps @damienmg would know? ", "I don't think it is but @mhlopko might prove me wrong.", "Link to our 1.3.1 debug builds: https://github.com/mind/wheels/releases/tag/tf1.3.1-cpu-debug. CPU-only for now - we can try to get some GPU ones if there's interest.\r\n\r\n(Let us know in mind/wheels#2)", "Sorry to be late to the party, is [--fission](http://docs.bazel.build/versions/master/user-manual.html#flag--fission) what you're thinking of?", "ah yes, Fission is what I was thinking, looks like bazel has --fission flag", "Yup, link in my previous comment points to the documentation, sorry for not being clearer.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@av8ramit, did @tensorflowbutler comment on this in error, since it's contributions welcome? ", "The updated reminder policy that accounts for that is on it's way!"]}, {"number": 13124, "title": "tensorflow.python.debug.cli.offline_analyzer failed to read debug data from HDFS filesys", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0 from master branch\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: null\r\n- **GPU model and memory**: null\r\n- **Exact command to reproduce**: python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_dir>\r\n\r\n### Issue description\r\nI saved debug data by `DumpingDebugHook` into hdfs filesys and then it failed to read the data by `python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://<debug_data_path>` with the error \"not a valid DFS filename\" as Invalid argument. But it works well for the local filesys by the same way.\r\n\r\n#### Error info:\r\n```\r\n# python -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\r\ntfdbg offline: FLAGS.dump_dir = hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\n^@hdfsOpenFile(/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244): FileSystem#open((Lorg/apache/hadoop/fs/Path;I)Lorg/apache/hadoop/fs/FSDataInputStream;) error:\r\njava.lang.IllegalArgumentException: Pathname /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 from /data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs:/gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244 is not a valid DFS filename.\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:197)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 78, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/cli/offline_analyzer.py\", line 41, in main\r\n    FLAGS.dump_dir, validate=FLAGS.validate_graph)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 692, in __init__\r\n    self._load_all_device_dumps(partition_graphs, validate)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 714, in _load_all_device_dumps\r\n    self._load_partition_graphs(partition_graphs, validate)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 983, in _load_partition_graphs\r\n    self._dump_graph_file_paths[device_name])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/debug_data.py\", line 145, in _load_graph_def_from_event_file\r\n    event.ParseFromString(f.read())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 119, in read\r\n    self._preread_check()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py\", line 79, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/hdfs://gpu1.hs.na61.tbsite.net:9000/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244; Invalid argument\r\n```\r\n\r\nIn fact, it was able to read the hdfs dir info, as below:\r\n\r\n```\r\n$hdfs dfs -ls hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0\r\nFound 7 items\r\n-rw-r--r--   3 root supergroup     141732 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/_tfdbg_graph_hash3076817912156706527_1505733016027244\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/gradients\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_1\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_2\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/layer_out\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/loss\r\ndrwxr-xr-x   - root supergroup          0 2017-09-18 19:10 hdfs://ns1/data/luchen.sk/tfdbg/tb_debug_data_dump/run_1505733015456409_100/_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0/train\r\n```\r\n\r\nAfter that, I tried to change the name of the dir as above from `_tfdbg_device_,job_localhost,replica_0,task_0,cpu_0` to `_tfdbg_device`, and then it was able to load the dir into CLI UI for debug, but there was nothing about debug info to show, as below:\r\n\r\n![image](https://user-images.githubusercontent.com/28526467/30541641-dc4329d2-9cae-11e7-8e07-c84dd1a3a7a3.png)\r\n", "comments": ["@caisq Do we have a schedule for this issue? Thanks.", "@luchensk HDFS is currently not supported by tfdbg. If you or any other open-source users have the interest, you are welcome to contribute such support. I do not have enough free cycles to fulfill this feature request this month.", "@caisq I would like to take up this issue and work on it.", "@nikhilrayaprolu Thanks a lot! Please go ahead! When you make a pull request, please add me as a reviewer. \r\n\r\ncc @jhseu re HDFS."]}, {"number": 13096, "title": "No OpKernel was registered to support Op 'QuantizeV2'", "body": "I was able to build tf 1.3 quantize_graph on windows 10 64 using bazel 0.53 and cuda 8 (i could not build it without cuda). when i try to run it i get:\r\nNo OpKernel was registered to support Op 'QuantizeV2' with these attrs.\r\n\r\n\r\nbazel-bin\\tensorflow\\tools\\quantization\\quantize_graph --input d:/export/saved_model.pb --output_node_names my_output --output d:/export/saved_model_quant.pb --mode=eightbit\r\n\r\n2017-09-17 10:48:28.292364: W C:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-17 10:48:28.292529: W C:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\client\\session.py\", line 1297, in _run_fn\r\n    self._extend_graph()\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\client\\session.py\", line 1358, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"C:\\Anaconda3\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'QuantizeV2' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n         [[Node: QuantizeV2 = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\"](QuantizeV2/input, QuantizeV2/min_range, QuantizeV2/max_range)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 1301, in <module>\r\n    app.run()\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 1292, in main\r\n    output_graph = rewriter.rewrite(FLAGS.output_node_names.split(\",\"))\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 420, in rewrite\r\n    self.eightbitize_nodes_recursively(output_node)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 600, in eightbitize_nodes_recursively\r\n    self.eightbitize_nodes_recursively(input_node)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 600, in eightbitize_nodes_recursively\r\n    self.eightbitize_nodes_recursively(input_node)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 600, in eightbitize_nodes_recursively\r\n    self.eightbitize_nodes_recursively(input_node)\r\n  [Previous line repeated 90 more times]\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 632, in eightbitize_nodes_recursively\r\n    for n in quantize_weight_eightbit(current_node, b\"MIN_FIRST\"):\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 299, in quantize_weight_eightbit\r\n    quint8_tensor = quantize_op[0].eval()\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\framework\\ops.py\", line 541, in eval\r\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\framework\\ops.py\", line 4085, in _eval_using_default_session\r\n    return session.run(tensors, feed_dict)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'QuantizeV2' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n         [[Node: QuantizeV2 = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\"](QuantizeV2/input, QuantizeV2/min_range, QuantizeV2/max_range)]]\r\n\r\nCaused by op 'QuantizeV2', defined at:\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 1301, in <module>\r\n    app.run()\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 1292, in main\r\n    output_graph = rewriter.rewrite(FLAGS.output_node_names.split(\",\"))\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 420, in rewrite\r\n    self.eightbitize_nodes_recursively(output_node)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 600, in eightbitize_nodes_recursively\r\n    self.eightbitize_nodes_recursively(input_node)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 600, in eightbitize_nodes_recursively\r\n    self.eightbitize_nodes_recursively(input_node)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 600, in eightbitize_nodes_recursively\r\n    self.eightbitize_nodes_recursively(input_node)\r\n  [Previous line repeated 90 more times]\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 632, in eightbitize_nodes_recursively\r\n    for n in quantize_weight_eightbit(current_node, b\"MIN_FIRST\"):\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\tools\\quantization\\quantize_graph.py\", line 298, in quantize_weight_eightbit\r\n    mode=quantization_mode)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2381, in quantize_v2\r\n    mode=mode, name=name)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"\\\\?\\C:\\Users\\user\\AppData\\Local\\Temp\\Bazel.runfiles_5goewho4\\runfiles\\org_tensorflow\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'QuantizeV2' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n         [[Node: QuantizeV2 = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\"](QuantizeV2/input, QuantizeV2/min_range, QuantizeV2/max_range)]]\r\n", "comments": ["Please upgrade to tensorflow 1.4 to see if you can reproduce this issue. Thanks.\r\n", "could you point me out to what was changed in this regard in 1.4?", "Does anyone solve this error?\r\nI am in trouble the same problem although my version 1.4.0", "I met the same problem.", "I got same problem. Is there anyone who can advice to solve this problem?\r\n\r\n-------------------------------\r\nNo OpKernel was registered to support Op 'QuantizeV2' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:  <no registered kernels>", "Has anyone solved this problem? Thank you~", "This is a bug in Windows environment. If you run your command in Ubuntu or Centos, it will be OK.", "In the 'tensorflow/core/BUILD' file, search for 'name = \"all_kernels_impl\",', the '//tensorflow/core/kernels:quantized_ops' is set to 'if_not_windows', change it into deps. Or use mkl."]}, {"number": 13061, "title": "Proposal: Making the cmake build distribution friendly", "body": "The following is a proposal, and I don't have it fully working yet, so a Pull Request is too early, and I want to gather some feedback. If an issue is not appropriate, let me know and I will look for a different medium.\r\n\r\n### System information\r\n\r\nAlmost any Linux distribution.\r\n\r\n### Describe the problem\r\n\r\nMost Linux distributions have similar policies/limitations:\r\n\r\n1. Not accepting bundling of system libraries (specially those security-sensitive).\r\n2. Requiring building the whole from source and allowing patching the sources.\r\n3. Requiring building offline (without Internet access).\r\n4. Not having enough manpower to package hundred of packages in a dependency chain just to get packages like maven built in these source-chains environments.\r\n\r\nBecause of 4., cmake makes things easier. It is a build tool with a few dependencies that does not need complex bootstrapping. Building bazel itself will bring you into the Java maven dependency chain which we already proved in openSUSE to expand into hundred of packages.\r\n\r\nBoth the bazel and cmake build files do not play well with 1. 2. 3., but cmake already improves 4.\r\n\r\nThe cmake build files, also pull the sources of different projects. That is because the top `CMakeLists.txt` includes:\r\n\r\n```cmake\r\nset(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/external)\r\n```\r\n\r\ninto the load path.\r\n\r\nthat external/ directory is full of snippets like:\r\n\r\n* `tensorflow/contrib/cmake/external/jpeg.cmake`\r\n* `tensorflow/contrib/cmake/external/gif.cmake`\r\n* ...\r\n* etc\r\n\r\nThose snippets use the [external project](https://cmake.org/cmake/help/v3.0/module/ExternalProject.html) cmake api to build directly from git or tarballs upstream. This clashes with\r\n1., 2., 3.\r\n\r\nIt is a bit sad that things like curl used to be looked up in the system and developers deliberately bundled them without making the cmake snippet first look for it, and if not, configure the bundled one.\r\n\r\n### Proposal\r\n\r\nThe proposal is to make the cmake build support both the Google/Mac user type of build with bundled sources, and the classical Linux distribution build.\r\nThis would be a gradual refactoring. Steps could be:\r\n\r\n* Add to CMakeLists.txt an option:\r\n\r\n```cmake\r\noption(tensorflow_SYSTEM_LIBRARIES \"Use system libraries\" ON)\r\n```\r\n\r\n* Replicate the external/ directory as platform/ and conditionally make it use one or the other:\r\n\r\n```cmake\r\nif(tensorflow_SYSTEM_LIBRARIES)\r\n set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/platform)\r\nelse()\r\n  set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/external)\r\nendif()\r\n```\r\n\r\nSo that then the part that does:\r\n\r\n```cmake\r\n# External dependencies\r\ninclude(zlib)\r\ninclude(gif)\r\ninclude(png)\r\ninclude(jpeg)\r\n```\r\n\r\nwould just work...\r\n\r\n* Replace incrementaly and one by one the $dep.cmake snippets in `platform/`.\r\n\r\nExample with gif.cmake:\r\n\r\nUsing the standard `/usr/share/cmake/Modules/FindGIF.cmake` included in cmake, which may use `pkg-config` for some modules. The module itself documents it would then set: `GIF_LIBRARIES`, `GIF_INCLUDE_DIR`, etc.\r\n\r\nUnfortunately the variable that the original set is called `gif_STATIC_LIBRARIES`, because it assumes it would be static. I think we could fix that later so that naming ends making sense, but lets not focus on that for now.\r\n\r\nThe `gif.cmake` I cited above would become a simple:\r\n\r\n```cmake\r\nfind_package(GIF REQUIRED)\r\n# dummy targets other targets depend on.\r\nadd_custom_target(gif)\r\n# These can be removed if we fix CMakeLists\r\nadd_custom_target(gif_copy_headers_to_destination)\r\n\r\nset(gif_STATIC_LIBRARIES ${GIF_LIBRARIES})\r\nset(gif_INCLUDE_DIR ${GIF_INCLUDE_DIR})\r\n```\r\n\r\nAnd that is more or less enough to move forward with this dependency... repeat.\r\nIt may need some hacks also in the top level cmake files however...\r\n\r\nI think this approach is doable, and could be turned into making the cmake build fully Linux distro enabled upstream. Slowly cleaning the naming, etc, removing these copy_headers targets so that the platform/ version of the .cmake files do not need to create dummy targets, etc.\r\n\r\n- Similarly, the sqlite one becomes:\r\n\r\n```cmake\r\nfind_package(SQLite3)\r\nset(sqlite_STATIC_LIBRARIES ${SQLITE3_LIBRARIES})\r\nset(sqlite_HEADERS ${SQLITE3_INCLUDE_DIR})\r\nadd_custom_target(sqlite)\r\nadd_custom_target(sqlite_copy_headers_to_destination)\r\n```\r\n\r\nWith the only difference is that the `Find` module was not included in cmake, so I just copied it into\r\nplatform from https://raw.githubusercontent.com/LuaDist/libsqlite3/master/cmake/FindSQLite3.cmake somewhere.\r\n\r\n* With this method I have been able to move forward and forward with the build.\r\n\r\n### Current blockers\r\n\r\n* cmake build does not work out of the box [PR](https://github.com/tensorflow/tensorflow/pull/12734)\r\n* [Issue](https://github.com/tensorflow/tensorflow/issues/12018) with `Missing tensorflow/core/debug/debug_service.grpc.pb.h`\r\n\r\n//cc @meaksh @dincamihai @ncounter", "comments": ["I second this proposal. ", "This is an area where we would warmly welcome contributions from the community. At present our only critical use for the `tensorflow/contrib/cmake` build is generating release binaries for TensorFlow on Windows. Anything that improves the general usefulness of that build, while preserving the ability to build our Windows release, would be most welcome.", "Also, We are very close to moving windows to bazel, so we will lower the amount of support we will have available for cmake once that happens.\r\nTherefore, I recommend using bazel instead for distributing TF.", "@gunan as I mentioned, given point 4.\u00b9 bazel I see hard to to build with the requirements 1. 2. and 3. that most distributions have (maven is already a pain), so even if the bazel build files get improved to look for system libraries first, the problems with bazel, its bootstrapping and dragging of the maven ecosystem will prevail.\r\n\r\nDo you see this different, that bazel can get properly packaged by Linux distributions? am I missing something here?.\r\n", "Bazel packaging for Debian seems stuck for a long time because bazel doesn't meet Debian's policy. One of the developers working on it has told me that it's hard to get it done.\r\n\r\nhttps://bugs.debian.org/cgi-bin/bugreport.cgi?bug=782654\r\n\r\nI think the only linux distribution that ships bazel, tensorflow (and cudnn) in its official archive is Archlinux. https://www.archlinux.org/packages/?sort=&q=tensorflow&maintainer=&flagged=", "@CDLuminate Archilinux has it because it looks like they [are not building it purely from the Archlinux tree, they are just running `compile.sh`](https://git.archlinux.org/svntogit/community.git/tree/trunk/PKGBUILD?h=packages/bazel), which means they may not be applying any distribution policies to the build process (build from source, build offline, ability to patch, legal review of the sources, etc). Which is *exactly* my point. The package you install may have been built pulling stuff from the internet built on someone else's workstation.\r\n\r\nEvery other distro will fall in the situation that as soon as you depend on a dependency built by maven, the dependency chain expands exponentially to hundred of packages, sometimes in multiple versions, and a very complicated bootstrap process. Bazel depends on itself _and_ on stuff build by maven.\r\n\r\ncmake can be built with the base system (which is what I would expect from a build tool) and plays well with distributions with strict policies like openSUSE or Debian (when you build inside of the [Open Build Service](http://openbuildservice.org/) there is no Internet access for the build process, you can only install other packages from the distro).\r\n", "Let me add a citation to the proposal: https://www.debian.org/doc/debian-policy/index.html#main-building-script-debian-rules\r\n\r\n```\r\nFor packages in the main archive, no required targets may attempt network access.\r\n```\r\n\r\nNote, the `required targets` include the whole package building process and more.", "I created a PR #13145 that should address the `Missing tensorflow/core/debug/debug_service.grpc.pb.h` issue on Linux for cmake. A couple of other issues (one is with `sqlite`, another is with `jpeg`) have also been addressed.", "I am not sure about bazel's proper packaging on linux, however we have to support bazel, because internally there is nothing but bazel for us.\r\nSupporting multiple build systems in parallel is also slowing down our development, in not so distant future we will stop actively maintaining the cmake files. Just stating this as our plan as a warning for you.\r\nYou are welcome to keep our cmake files working, but just warning you. If you depend on them you will need to maintain our cmake files.\r\n\r\nAnother warning is, soon we will split most of contrib out of our repository, so cmake files wont even live under core tensorflow repository. But again, you are welcome to maintain them under the new location they will be at.", "What ist about another platforms like Solaris or AIX. I think with bazel only build scripts there is no way to build Tensorflow except with CMake. Bazel seems to me Linux centroid.", "@yongtang Thanks a lot for PR #13145 \r\nI plan to upload my branch with support for system sqlite3, jpeg and others after some cleanups.\r\nIt can then be enhanced from there.\r\n\r\nOne cleanup that may be it makes sense to be done before adding support for system libraries is to rename variables like `jemalloc_STATIC_LIBRARIES` to something like `jemalloc_ext_LIBRARIES` as they will not always be dynamic.\r\n\r\nAnother is to cleanup targets like `gif_copy_headers_to_destination` so that they are not depended outside of the module, as the system library version of it would not have such thing.\r\n\r\n\r\n", "I think that this support would also need to be extended to Windows.  As referenced at https://github.com/tensorflow/tensorflow/issues/13962 Windows builds of TensorFlow are only supported on CMake, which is what Google uses to build the Windows Python packages. \r\n\r\nThe biggest problem with TensorFlow CMake builds on Windows is that they generate MSBuilds that do not do incremental builds but only full builds.  Even on my powerful development machine full builds take almost five hours to complete just to build a Python wheel. ", "@gunan  if you are removing CMake support, when are you going to add bazel builds for TF for Windows?\r\n https://github.com/tensorflow/tensorflow/issues/13962 states that CMake is what you use to build the Windows Python wheels.  That build takes fives hours and is not incremental.", "Started looking at the external library issue. As a probing point, I've chosen zlib and sent #15382 . I'll continue making other external libraries as \"system libraries\" if #15382 is satisfiable.", "@johnsrude Tensorflow can already be built with Bazel on Windows, incremental build supported and is much faster than CMake + MSBuild (Bazel build took me 3 hours in my low-end laptop while MSBuild took about 9 hours).\r\n\r\nCMake + Ninja will be much faster than CMake + MSBuild as Ninja can do incremental build, probably on par with Bazel. However, for some unknown reason Tensorflow's CMake build scripts are so tuned to MSBuild that Ninja simply does not work.", "If you want fast compilation with MS, the best thing to do is precompiled header. It will compile much much faster, as a lot of object files are just a few lines with a huge header.", "I'm working on the same thing, any updates? @dmacvicar ", "Since `TensorFlow` was one of the motivators for a talk I recently gave (at FOSDEM'18), I can't resist to mention it here... (thanks @dmacvicar for pointing out this issue to me)\r\n\r\n_\"How To Make Package Managers Cry\"_\r\n* abstract + slides at https://fosdem.org/2018/schedule/event/how_to_make_package_managers_cry/\r\n* recording at https://www.youtube.com/watch?v=NSemlYagjIU\r\n\r\nI sort of understand the reluctance of the TF developers to switch away from Bazel or trying to maintain two different build systems, but I think it's important to realise that the current build procedure is a significant burden to many people who have tried to build TensorFlow from source for whatever reason (better performance on non-GPU systems, installing TF on an old OS (e.g. CentOS 6), etc.). And then I'm not even talking about getting Bazel installed first...\r\n\r\nProof:\r\n\r\n* The EasyBuild project (http://easybuilders.github.io/easybuild/), which already supports over 1,400 different (scientific) software packages (over 2,200 if you're also counting Python packages, R libraries, etc.) is still working on supporting the installation a built-from-source TensorFlow, see https://github.com/easybuilders/easybuild-easyblocks/pull/1287.\r\n  As one of the lead developers and after spent many hours trying to get tame the TF build procedure using Bazel, I can safely say TensorFlow is in the top-10 of most difficult to install software from source, and that means a lot, trust me...\r\n\r\n* The Spack project (https://spack.io/) has looked into adding support for building TensorFlow from source, both with Bazel (see https://github.com/spack/spack/pull/2043) and CMake (https://github.com/spack/spack/pull/3244), but has given up on it for now it seems. Spack supports over 2,500 different (scientific) software packages, but has a lot of trouble properly supporting TensorFlow, which is again really telling...\r\n\r\nThere's a strong incentive to build TensorFlow from source especially on CPU-only systems (not everyone has a fancy expensive GPU): we saw a factor 7 (7x, not 7%) speedup with a TF 1.4 built from source compared to the provided binary wheels on an Intel Haswell system, because our build was using AVX2, while the generically built wheel did not. 7x is **HUGE**, that's like back to the stone age without building from source. Running on GPUs was still significantly faster, but 'only' 8.3x (on a single K40x) rather than 60x. I suspect the different on the new Intel Skylake systems is going to be even bigger (I will try that soon).\r\n\r\nTensorFlow is quickly becoming a running joke in the software packaging/installation community, and I'm fairly sure that's not the intention...", "@boegel If it brings any comfort, building Tensorflow with Bazel on Windows is even more painful. Regularly breaks, then few external contributors or someone from Bazel team will come to fix the build. Repeat cycle.\r\n\r\nSomeone made it possible to build Tensorflow with CMake + Ninja (see https://github.com/tensorflow/tensorflow/pull/16763)!", "I'll just chime in from the Spack side to say that everything @boegel said is true.  He also gave a nice talk -- I was there \ud83d\ude04.  We package software for Livermore Computing (i.e., all of [these machines](https://hpc.llnl.gov/hardware/platforms)), for other large DOE HPC centers, and for HPC sites outside the DOE.  We have a lot of people who want to use TensorFlow, but it's not maintainable, and our attempts to package TF stagnated because of all the work involved.  We basically need to be TF developers to package the thing.\r\n\r\nThe main issues we have are:\r\n1. Swapping compilers into the build with Bazel is a giant pain.  Spack wants to be able to customize that.  Bazel (last I checked) wants the compiler to be in *exactly* its preferred location, which is hardly ever the case on HPC machines.  There are a lot of things about Bazel that are incompatible with the way things need to be built for OSS projects.\r\n2. We tried the contributed CMake build to address this.  It's more manageable, but the way it used external project at the time didn't provide a way to use an existing installation of the external projects.  So we had to put some nasty hacks in to point the TF build at the spack-built dependencies (see spack/spack#3244).\r\n3. To further complicate things, TF relies not only on the *installed* artifacts from its dependencies, but also internal libraries and headers that aren't usually installed.  It builds its dependencies and grabs stuff out of the build directory, not the install directory.  So even if you have properly packaged versions of the dependencies, you may not have all the things that TF needs. \r\n\r\nMaybe some of this stuff has changed -- If so, I'd love to take a look at this again when I have some time.  I am currently hesitant to do this, since I read here that the TF team was going to ditch the contributed builds, and Bazel is too painful to support in Spack.  See [#6923 (comment)](https://github.com/tensorflow/tensorflow/issues/6923#issuecomment-273671241) where @jart describes what we'd need to do to use Bazel.\r\n\r\nFor now, we just have a person at Livermore Computing who maintains patches on the Bazel build and updates them (very tediously) when users ask for a newer TF version.", "Hi everyone on this thread: I want to let you know about a new group to address these issues: [tf-distribute](https://groups.google.com/a/tensorflow.org/forum/#!forum/tf-distribute). \r\n\r\nWe created it because we hear the pain on these issues and wanted to get the community together so we can come to a solution.\r\n\r\nThere are [two proposals for a way forward](https://docs.google.com/document/d/1yWIrKZ8-42ZSztYz-TUJQETe_UmSdg0KJzBXMitTQXE/edit) that preserves the ability to build with CMake, please read this doc and comment, and join the discussion on the mailing list.\r\n\r\nBasically, we want to address this pain, but we need community involvement too to help this work. I welcome your comments: we certainly don't want to be the ongoing joke in the packaging world, @boegel ;)", "A hacky work in progress https://github.com/tensorflow/tensorflow/pull/21699 . However I'm only able to take care of Debian unstable/experimental.", "So far, SIG Build found nobody interested in keeping CMake alive. Please see this thread https://groups.google.com/a/tensorflow.org/forum/#!topic/build/9vKi3ceP2ZI and others. If you are interested, it's the right time to jump into the discussion.", "@ewilderj Unfortunately I totally lost interest in the cmake build too. I was trying build TF with a python+ninja build system from scratch.", "It's worth taking a look at @perfinion's packaging work for Gentoo. I believe now that Bazel can take care of the use case in this bug. @dmacvicar I'd be interested to know if your requirements can be now met with bazel, and if not what more work needs to happen in SIG Build.\r\n", "I checked Gentoo's ebuild for tensorflow. It uses bazel as the build system. However the problem is that bazel doesn't meet many debian's requirement hence is very hard to enter debian.", "Yeah once you can get bazel into debian then packaging TF is not too bad, its way better now than even a couple months ago. bazel should be able to build from scratch tho, the first ./compile.sh script just runs javac to get the very basics up and goes from there. iirc it doesnt need the bundled JVM like it had before. Once you get to the TF part feel free to ping me and i'll help how I can. :-)", "Just wanted to add that FreeBSD Ports is with you on this boat. It is simply impossible to package TensorFlow for FreeBSD, except for pre-building a package manually and redistributing binaries.\r\n\r\nAnything has changed wrt to CMake build since last post?", "Nothing much has changed, but we'd be very excited if you joined the SIG Build community and shared FreeBSD's experience https://groups.google.com/a/tensorflow.org/forum/#!forum/build"]}]