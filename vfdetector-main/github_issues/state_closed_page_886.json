[{"number": 26904, "title": "Cannot add metric to Estimator with binary_classification_head", "body": "I created custom Estimator that uses binary_classification_head. During training I can see only loss and global step in tensorboard.\r\nI was trying to add metric using tf.contrib.estimator.add_metrics but it doesn't work (don't know why). \r\nI was also trying to add it using similar approach to this one but also with no luck:\r\nhttps://stackoverflow.com/questions/50120073/tensorflow-metrics-with-custom-estimator\r\nCould you please advise me how can I add some metrics to this? ", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26903, "title": "Cardinality() of SkipDataset may be not right in some cases", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): \r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe `Cardinality` function of `SkipDatasetOp` is implemented as below:\r\n```C++\r\n int64 Cardinality() const override {\r\n      int64 n = input_->Cardinality();\r\n      if (n == kInfiniteCardinality || n == kUnknownCardinality) {\r\n        return n;\r\n      }\r\n      return std::max(0LL, n - count_);\r\n    }\r\n```\r\nGiven `count_ = -1`  and `input_` is a `RangeDataset` with `start =0`, `end=10`, and `step=1`, then `n` will 10, and the output of `Cardinality()` will be `11`. However, there is no output elements in this case. Will it be more reasonable if the output of `Cardinality()` be `0` for this case?  \r\n", "comments": []}, {"number": 26902, "title": "tf_upgrade_v2 does not preserve file attributes and symbolic links", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):n/a\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): tf2 preview nightly yesterday\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a\r\n\r\n1. `tf_upgrade_v2` changes executable files to non-executable files. I expect executable files are still executable after the upgrade.\r\n\r\n2. `tf_upgrade_v2` always changes symbolic links to regular files. However I expect:\r\n(1)For in-place upgrade, modify the file the link points to, but the symbolic link should be the same.\r\n(2) For non-in-place upgrade, if `--intree` and `--outtree` is used, symbolic links which point to files within the tree should become symbolic links pointing to the new file in the outtree. Symbolic links which point to external files should become a regular file.\r\n(3) For non-in-place single-file upgrade, the output should be a regular file.", "comments": ["Hi @ppwwyyxx! Do you have a simple example where tf_upgrade_v2 changes a file to be non-executable?\r\n\r\nI just tried the following:\r\n```\r\ntouch test1/bar.sh\r\nchmod +x test1/bar.sh\r\npip install tf-nightly-2.0-preview\r\ntf_upgrade_v2 --intree=test1/ --outtree=test2/ --copyotherfiles=True\r\n```\r\n\r\nAfter these commands are run, `test2/bar.sh` does have executable permissions. Could it be instead that file ownership changed?\r\n\r\nSorry for delay looking at this issue.", "I submitted a change to fix symbolic link behavior. However, the behavior is slightly different than you proposed. Specifically, I keep symlink unchanged if it points to external file (as opposed to converting it to regular file).\r\n\r\nI was thinking that someone who has code in a different directory might want to update it separately, while keeping the symlink.\r\n\r\nI will keep this issue open for the executable permission. Per my previous comment, I need some example to reproduce it.", "@ppwwyyxx Can you check whether the issue resolved in the `TF2.0` or `tf-nightly`? If this was resolved, please close the issue. Thanks!", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to no other updates", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26902\">No</a>\n"]}, {"number": 26901, "title": "tf.range fails on integer tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04.6 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.0-rc2-2-gbade323\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0 20160609\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Describe the current behavior**\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> tf.range(10, dtype=tf.float32)\r\n<tf.Tensor: id=3, shape=(10,), dtype=float32, numpy=array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)>\r\n>>> tf.range(tf.convert_to_tensor(10), dtype=tf.float32)\r\n[...]\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'tf.Tensor(10, shape=(), dtype=int32)'\r\n```\r\n\r\n**Describe the expected behavior**\r\n`tf.range` should not raise ValueError when passing a Tensor argument instead of an integer literal.", "comments": ["TensorFlow has a strict type system the moment inputs are tensors.  If you use convert_to_tensor(10) you get an `int32` tensor; and tf.range will not let you perform the subtle conversion to `float32`.  If you pass a python integer 10, it will perform the conversion to `float32` for you.  This strict typing is a core part of TF's type system.  If you want your tensor to be a float, use `tf.range(tf.cast(tf.constant(10), tf.float32), tf.float32)`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26901\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26901\">No</a>\n", "It would be nice to have consistent behavior between literals and tensors. If it's not possible or desirable to implicitly convert the tensor type, is it possible at least to raise a ValueError instead of implicitly casting the integer literal? It's quite surprising that two semantically equivalent programs have different behaviors."]}, {"number": 26900, "title": "Converted .tflite file and it's output", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.13.1\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nI am have a tf.keras sequential model which is a binary image classifier as below.\r\n\r\n`import tensorflow as tf\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\r\nfrom sklearn.utils import class_weight\r\nimport numpy as np\r\n\r\nimg_width, img_height = 200, 200\r\n\r\ntrain_data_dir = 'augmentedImg/200/training_data'#=============================================================================\r\nvalidation_data_dir = 'augmentedImg/200/validation_data'#=============================================================================\r\nnb_train_samples = 9009\r\nnb_validation_samples = 2252\r\nepochs = 100\r\nbatch_size = 32\r\n\r\nlayer_size = 64\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    input_shape = (3, img_width, img_height)\r\nelse:\r\n    input_shape = (img_width, img_height, 3)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(layer_size, (3, 3), input_shape=input_shape))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(layer_size, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(layer_size, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(layer_size, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Flatten())\r\n\r\n#model.add(Dropout(0.5))\r\n\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1. / 255,\r\n    rotation_range=90,\r\n    width_shift_range=0.1,\r\n    height_shift_range=0.1,\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True\r\n    )\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_dir,\r\n    target_size=(img_width, img_height),\r\n    color_mode='grayscale',\r\n    batch_size=batch_size,\r\n    class_mode='binary')\r\n\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size=(img_width, img_height),\r\n    color_mode='grayscale',\r\n    batch_size=batch_size,\r\n    class_mode='binary')\r\n\r\nclass_weights = class_weight.compute_class_weight(\r\n               'balanced',\r\n                np.unique(train_generator.classes), \r\n                train_generator.classes)\r\n\r\nmodel.fit_generator(\r\n    train_generator,\r\n    class_weight=class_weights,\r\n    steps_per_epoch=nb_train_samples // batch_size,\r\n    epochs=epochs,\r\n    validation_data=validation_generator,\r\n    validation_steps=nb_validation_samples# // batch_size,\r\n    #callbacks=[tensorboard, reduce_lr]\r\n    )\r\n\r\nmodel.save_weights('model.h5')\r\nprint(\"End of program\")`\r\n\r\nI then convert this into a .tflite file, which I am trying to incorporate into an android app.\r\n\r\nWhen I run the tf.keras model (using model.predict on a new image), it returns a float of either 1 or 0 to signify the class.\r\n\r\nHowever in the .tflite file it returns a number between 0 and 1 (for example 0.20582036)\r\n What does this number mean? I presume if its 0.20582036 then it's a prediction for class[0] and the confidence in its prediction?  \r\n\r\nThere seems to be very little documentation regarding the running of converted .tflite files.\r\n\r\nAny help would be much appreciated", "comments": ["Not sure what exactly caused this, but seems it wasn't a .tflite issue. I did 'File-> Invalidate Caches/Restart' in Android Studio followed by 'File->Sync Project With Gradle Files' and I started getting the expected \"1.0\", \"0.0\" outputs from my model."]}, {"number": 26899, "title": "Error when importing tensorflow: DLL load failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: using CPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nOriginaly, when importing python module keras with tensorflow backend ar error ocurred when importing. To be sure, I simply imported tensorflow and same error was shown: \r\n\r\n> ImportError: DLL load failed: Specified module could not be found.\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nOpen python in conda prompt or using spyder console:\r\n`import tensorflow`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n> import tensorflow\r\n> Traceback (most recent call last):\r\n> \r\n>   File \"<ipython-input-2-d6579f534729>\", line 1, in <module>\r\n>     import tensorflow\r\n> \r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n> \r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n> \r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n>     raise ImportError(msg)\r\n> \r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n>     return load_dynamic(name, filename, file)\r\n>   File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.", "comments": ["From the error message, I suspect you have installed tf-gpu version. Please take a look at duplicate #22794\r\nAlso can you please share  TF installation process you followed? Thanks!", "I had looked at that issue, but since I' not using a GPU, I figured I [didn't need to install CUDA](https://www.quora.com/What-is-CUDA-If-we-can-perform-TensorFlows-computation-directly-from-a-GPU-then-why-do-we-actually-need-it). \r\nWhen I check my conda enviorment, it says I have tensorflow (not tensorflow-gpu) installed, as well as tensorboard, tensorflow-base and tensorflow-estimator (and keras, but I don't think that's an issue. However, I just notised it also says that the build used is gpu_py36h0fff12a_0, so maybe GUP?\r\nTo install I'm prety sure I simply did `conda install tensorflow`.\r\n\r\nEDIT: just uninstalled and reinstalled everything again to be sure: I'm _not_ installing `tensorflow-gpu`. However, he build anaconda uses _is_ the one I mentiones above. ", "Thanks for the confirmation. Can you please describe the procedure you followed to install TF?\r\nAlso did you get a chance to take a look at [install guide on TF website](https://www.tensorflow.org/install/pip)?", "I hadn't read it, but it doesn't quite apply, I think. My python instalation uses Anaconda and in this machine I have a single enviorment I'm using. In that enviorment I ran `conda install tensorflow` to install.\r\n\r\nThe linked installation guide suggests using `pip install --upgrade tensorflow` and a virtualenv. I was told that as long as I can install something using conda I should, since it is better at checking for conflicts and keeping a clean path. Instad of Visual Studio 2015, I have Visual studio 2017, version 1.27.1, to be specific. Also, I did \"Make sure long paths are enabled on Windows.\" \r\n\r\n", "Just to be sure I did the following in an anaconda prompt:\r\n\r\n- I tried `conda update tensorflow`, which did nothing since I have a fresh installation.\r\n\r\n- I tried `pip install --upgrade pip`, which did nothing.\r\n\r\n- I ran `conda update pip` to actually update it, since my pip is a conda installation.\r\n\r\n- I ran `pip install --update tensorflow`, which installed something, but I still get the same problem in python.", "Apologies for the delay in response.\r\n*TensorFlow release binaries (CPU/GPU) version 1.6 and higher are prebuilt with AVX instruction sets.*  \r\nSee [hardware requirements][1] to know more.  \r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:  \r\n\r\n* Try Google Colab to use TensorFlow.    \r\n * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.  \r\n    * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.  \r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\n\r\n  [1]: https://www.tensorflow.org/install/pip#hardware-requirements", "Hm... That's very good to know, thanks! However, for whatever reason, after reinstalling anaconda yet again (I had broken my installation doing something else), Keras (and tensorflow) ended up working no problem. I don't recall doing anything differently. Thanks!", "You are welcome. Closing this issue now since it's resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26899\">No</a>\n", "Anaconda packages are not built by the TF team.\r\nSo the packages you get by `pip install` or `conda install` are different.\r\nThe conda packages may be more compatible."]}, {"number": 26898, "title": "Added tan, sinh and cosh to keras backend", "body": "Functions Added:\r\n1) tan\r\n2) Hyperbolic sin\r\n3) Hyperbolic cos", "comments": ["This is not part of the Keras API.\r\nIf you want to make it part of the Keras API, please submit a PR on https://github.com/keras-team/keras/.\r\n\r\nAlso, this PR requires new unit tests. ", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "For TF high level API, we don't expect to add new API in keras/backend, which also is just a wrapper around the TF API. The keras backend API is there for historical reason, eg when we copy them from keras-team/keras to TF. If you feel that this API is useful in keras (for multi-backend purpose), please create a PR in keras-team/keras.\r\n\r\nThanks.", "As pre comment, I am going to close this PR now. Feel free to create PR on keras-team/keras if you feel it is necessary."]}, {"number": 26897, "title": "[ROCm] Added ROCm support for the extract_image_patches op", "body": "This minor mod adds ROCm support for the extract_image_patches ops.\r\n\r\n#### Background info ####\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/python/kernel_tests:extract_image_patches_op_test           PASSED in 2.9s\r\n```", "comments": []}, {"number": 26896, "title": "[ROCm] Added ROCm support for the extract_volume_patches op", "body": "This minor mod adds ROCm support for the extract_volume_patches op.\r\n\r\n#### Background info ####\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/python/kernel_tests:extract_volume_patches_op_test          PASSED in 2.2s\r\n```", "comments": []}, {"number": 26895, "title": "[ROCm] Added ROCm support for the fake_quant ops", "body": "This minor mod adds ROCm support for the fake_quant ops.\r\n\r\n#### Background info ####\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n```\r\n//tensorflow/core/kernels:fake_quant_ops_test                            PASSED in 0.2s\r\n```", "comments": []}, {"number": 26894, "title": "[ROCm] Added ROCm support for fft ops ", "body": "This minor mod adds ROCm support for fft ops.  While the test builds, note that this test is already bazel-tagged as 'no-rocm' to avoid running the unit test.\r\n\r\n#### Background info ####\r\nThese ops are fundamental to TensorFlow, and this mod has been running for more than 1 year on our ROCm port of TF.\r\n\r\nWe have published docker images at: https://hub.docker.com/r/rocm/tensorflow/tags\r\nAnd also PyPI packages: https://pypi.org/project/tensorflow-rocm/\r\n\r\nFor a sample ROCm test run you can refer to:\r\nhttp://ml-ci.amd.com:21096/job/tensorflow-upstream-unit-tests/721/console\r\n\r\n", "comments": []}, {"number": 26893, "title": "Compilation failed: Compilation failure: Ran out of memory in memory space vmem. Please file a bug against XLA.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NO\r\n- TensorFlow installed from (source or binary):Already installed on Colab \r\n- TensorFlow version (use command below):1.13.1\r\n- Keras version: 2.2.4\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI'm training my model on cloud TPU colab. My model has ~27M params.\r\nThe model is not compiling.\r\n**Describe the expected behavior**\r\nI've used others model using the same procedure and have trained successfully.\r\nBut facing problem with this model. Not compiling. \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\ndef seresnext_model(input_shape):\r\n  base_model = SEResNextImageNet(input_shape,include_top = False)\r\n  x = base_model.output\r\n  out1 = GlobalMaxPooling2D()(x)\r\n  out2 = GlobalAveragePooling2D()(x)\r\n  out3 = Flatten()(x)\r\n  out = concatenate([out1,out2,out3])\r\n  out = Dropout(0.3)(out)\r\n  out = Dense(256,activation = 'relu')(out)\r\n  out = Dropout(0.3)(out)\r\n  X = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform', bias_initializer='zeros')(out)\r\n  model =  Model(inputs=base_model.input, outputs=X)\r\n  return model\r\n\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(\r\n    seresnext_model,\r\n    strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n    )\r\n)\r\ntpu_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate = 3e-4), loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\r\nfilepath = '/content/model.h5'\r\n#clr = CyclicLR(base_lr=2e-4, max_lr=0.006,\r\n#                     step_size=1070.)\r\ncheckpoint = ModelCheckpoint(filepath,monitor='val_loss', verbose=1, \r\n                             save_best_only=True)\r\nhistory = tpu_model.fit_generator(train_gen,steps_per_epoch = train_steps,validation_data = val_gen,validation_steps = val_steps,epochs = 60,callbacks=[checkpoint],\r\n                                  )\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nINFO:tensorflow:Querying Tensorflow master (grpc://10.14.174.10:8470) for TPU system metadata.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 2624331463547489192)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8563543438446983684)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 16583072253587280753)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 3721588361095158121)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6301764238571034056)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12096784135351803975)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 8677796372356234775)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 18261783914960296885)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15034483779723062214)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 660265193242712443)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 14213068833784483275)\r\nWARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\r\nEpoch 1/60\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(128, 96, 96, 3), dtype=tf.float32, name='input_1_30'), TensorSpec(shape=(128, 1), dtype=tf.float32, name='dense_33_target_50')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for input_1\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nINFO:tensorflow:Started compiling\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-44-3196f76a2e41> in <module>()\r\n     12 checkpoint = ModelCheckpoint(filepath,monitor='val_loss', verbose=1, \r\n     13                              save_best_only=True)\r\n---> 14 history = tpu_model.fit_generator(train_gen,steps_per_epoch = train_steps,validation_data = val_gen,validation_steps = val_steps,epochs = 60,callbacks=[checkpoint],\r\n     15                                   )\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1424         use_multiprocessing=use_multiprocessing,\r\n   1425         shuffle=shuffle,\r\n-> 1426         initial_epoch=initial_epoch)\r\n   1427 \r\n   1428   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\r\n    189       progbar.on_batch_begin(step, batch_logs)\r\n    190 \r\n--> 191       batch_outs = batch_function(*batch_data)\r\n    192       if not isinstance(batch_outs, list):\r\n    193         batch_outs = [batch_outs]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1189       else:\r\n   1190         self._make_fit_function()\r\n-> 1191         outputs = self._fit_function(ins)  # pylint: disable=not-callable\r\n   1192 \r\n   1193     if reset_metrics:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __call__(***failed resolving arguments***)\r\n   1258     input_specs = infeed_instance.make_input_specs(input_tensors)\r\n   1259     tpu_model_ops = self._tpu_model_ops_for_input_specs(input_specs,\r\n-> 1260                                                         infeed_manager)\r\n   1261     infeed_dict = infeed_instance.make_feed_dict(tpu_model_ops)\r\n   1262 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _tpu_model_ops_for_input_specs(self, input_specs, infeed_manager)\r\n   1169                                                  infeed_manager)\r\n   1170       self._compilation_cache[shape_key] = new_tpu_model_ops\r\n-> 1171       self._test_model_compiles(new_tpu_model_ops)\r\n   1172 \r\n   1173     return self._compilation_cache[shape_key]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in _test_model_compiles(self, tpu_model_ops)\r\n   1112     if proto.status_error_message:\r\n   1113       raise RuntimeError('Compilation failed: {}'.format(\r\n-> 1114           proto.status_error_message))\r\n   1115 \r\n   1116     end_time = time.time()\r\n\r\nRuntimeError: Compilation failed: Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.\r\n\r\nLargest program allocations in vmem:\r\n\r\n  XLA label: register allocator spill slots\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.276 = (f32[256]{0}, f32[256]{0}, f32[128,24,24,256]{3,0,2,1}) fusion(f32[128,24,24,256]{3,0,2,1}, f32[256]{0}, f32[128,24,24,8]{3,0,2,1}, f32[128,24,24,8]{3,0,2,1}, ...(+33)), kind=kLoop, calls=%fused_computation.272\r\n  Allocation type: scoped\r\n```\r\n", "comments": ["@jlebar ", "@ymodak for TPU related-stuff we need to find a different point of contact.\r\n\r\nPerhaps @broune can forward this to the right person. ", "Thank you for the bug report and congratulations for finding a way to make a loop fusion run out of VMEM, that's a bit of an achievement. :) I'd guess that it has something to do with the 33 input operands to the fusion node - maybe if you stick something into your graph that will defeat fusion around that location, you can work around it that way until we fix it on our end (I'm not sure what's best for that - a conv that in effect does a 1x1 average pool, i.e. no effect, *might* do it). There should also be a flag to make fusion less aggressive on TPU, but I'm not sure if/how you can access that from open source.\r\n\r\n@majnemer is the right person for internal errors in the TPU backend.", "Thanks, @broune ... had to do coz for the first time the error itself told to report the bug. :)\r\nGrateful to Google for providing free access to Cloud TPU on Colab. It saves a lot of training time.\r\n\r\nMay I know what exactly VMEM is, and why is it not possible to run out of memory in memory space VMEM? I used other models with params range(5M-15M) and they trained successfully.\r\n\r\nBasically, I'm using Keras to build my model and importing it from Tensorflow. Here's my model notebook if it can help in any way: https://colab.research.google.com/drive/1Vze56xg06pUNqOsS3iIusA8v2R6jwXri\r\nCan anyone guide what changes I'll have to make(if any) in the model?\r\n", "I can't get into specifics on the architecture, but VMEM is a memory space that is managed entirely by XLA. XLA should always be able to ensure that you don't run out of VMEM, so whenever it happens that's a bug in XLA.", "I talked this over with Jim and he suggested that this might be in @ukoxyz 's domain.", "obsolete."]}, {"number": 26892, "title": "Disable keras:applications_test (128912960)", "body": "", "comments": []}, {"number": 26891, "title": "--define=grpc_no_ares=true to make MacOS builds work", "body": "", "comments": []}, {"number": 26890, "title": "Adding operator", "body": "Added SoftShrink operator and the related tests. Not really sure of how much of this is right, can I have a review? @rthadur @fchollet @pavithrasv ", "comments": ["What's the update here @rthadur ???", "@kyscg , thi PR is similar to one of the old PR #25189\r\n\r\nRegards\r\nAmit", "#25189 is approved. Closing this, thanks @amitsrivastava78 "]}, {"number": 26889, "title": "tf.keras.fit not working with my model, doesn't feed the output target to placeholder tensor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have custom code to build an image classifier network with the tf.keras API\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04 (Linux Mint)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `'v1.13.1-0-g6612da8951' 1.13.1\r\n`\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: `release 10.0, V10.0.130` cuDNN: 7.4.2\r\n- GPU model and memory: GeForce GTX 1070\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to use `model.fit_generator` or `model.fit_on_batch`, I get the following error: \r\n```python\r\n>>> for x,y in training_gen:\r\n...     break\r\n... \r\n>>> model.train_on_batch(x,y)\r\nWARNING:tensorflow:From /home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1188, in train_on_batch\r\n    outputs = self.train_function(ins)  # pylint: disable=not-callable\r\n  File \"/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3076, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\r\n    run_metadata_ptr)\r\n  File \"/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'dense_1_target' with dtype float and shape [?,?]\r\n  [[{{node dense_1_target}}]]\r\n  [[{{node metrics/acc/Mean_2}}]]\r\n```\r\n\r\nHere's the details about the model: It's a custom classifier which uses [xception](https://arxiv.org/abs/1610.02357) as the feature detector and a custom feedforward network for a classifier.\r\n```python\r\n>>> model.summary()\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nxception (Model)             (None, 8, 8, 2048)        20861480  \r\n_________________________________________________________________\r\nClassifier (Sequential)      (None, 1)                 540929    \r\n=================================================================\r\nTotal params: 21,402,409\r\nTrainable params: 540,929\r\nNon-trainable params: 20,861,480\r\n_________________________________________________________________\r\n>>> classifier.summary()\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ndense (Dense)                (None, 8, 8, 256)         524544    \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 16384)             0         \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 16384)             0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1)                 16385     \r\n=================================================================\r\nTotal params: 540,929\r\nTrainable params: 540,929\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nThe two models are combined with the following code:\r\n```python\r\nef build_full_model(feature_detector, classifier):\r\n  model = tf.keras.models.Sequential()\r\n  #First get the features from ptdnn\r\n  model.add(feature_detector)\r\n  # for layer in feature_detector.layers:\r\n  #   layer.trainable=False\r\n  #   model.add(layer)\r\n  #Then classify\r\n  model.add(classifier)  \r\n  # for layer in classifier.layers:\r\n  #   model.add(layer)\r\n  #feature_detector should not be trainable if fine_tuning_layers==0\r\n  feature_detector.trainable=False\r\n\r\n  # Display layer information for reference\r\n  print('[ASSEMBLE] Full Model Architecture:')\r\n  model.summary()\r\n\r\n  # classifier options including metrics and loss function\r\n  classifier.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-4),\r\n                loss='binary_crossentropy',\r\n                metrics=['acc',recall,f1])\r\n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\r\n                loss='binary_crossentropy',\r\n                metrics=['acc',recall,f1])\r\n\r\n  print('[ASSEMBLE] Models compiled successfully')\r\n  return model\r\n```\r\nThe issue seems to occur no matter what the input values are. It seems to me that my inputs x and y (coming from my generator) are the correct shape:\r\n```python\r\n>>> print(x.shape,y.shape)\r\n(32, 256, 256, 3) (32,)\r\n```\r\nFor the sake of completeness, the training data generator is set to read in images from a directory with some augmentation:\r\n```python\r\n# anonymous function for rotating 90 degrees randomly\r\n  random_90 = lambda im: np.rot90(im,k=np.random.choice(4))\r\n  #define the settings for loading in images including value rescale, \r\n  #  and random alterations such as scaling, zooming, and flipping\r\n  augmented_gen = ImageDataGenerator(\r\n    rescale=1./255,\r\n    zoom_range=0.1,\r\n    cval=0,\r\n    horizontal_flip=True,\r\n    vertical_flip = True,\r\n    preprocessing_function=random_90,\r\n    )\r\ntraining_gen = augmented_gen.flow_from_directory(\r\n    train_dir,\r\n    target_size=(256, 256),\r\n    batch_size=32,\r\n    class_mode='binary'\r\n    )\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the keras API to be able to train my model as is. Presumably this means that the placeholder Tensor should be populated correctly dense_1_target`  in \"/home/hepcats/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\"\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis should be able to reproduce my issue:\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# get xception\r\ninshape = (256,256,3)\r\nptdnn = tf.keras.applications.Xception(\r\n\tweights='imagenet',\r\n  \tinclude_top=False,\r\n  \tinput_shape=inshape)\r\n# build classifier\r\nclassifier = tf.keras.models.Sequential(name='Classifier')\r\n  # The classifier input is dense, or fully connected\r\nclassifier.add(tf.keras.layers.Dense(256, activation='relu',\r\n  input_shape=feature_shape[1:]))\r\n  # The flatten layer reduces the dimensions of the output\r\nclassifier.add(tf.keras.layers.Flatten())\r\n  # Dropout layer prevents overfitting\r\nclassifier.add(tf.keras.layers.Dropout(0.5))\r\n  # Output layer is a single neuron sigmoid\r\nclassifier.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n\r\n#Assemble composite model\r\nmodel = tf.keras.models.Sequential()\r\n  #First get the features from ptdnn\r\nmodel.add(ptdnn)\r\n  #Then classify\r\nmodel.add(classifier)  \r\n  #feature_detector should not be trainable if fine_tuning_layers==0\r\nptdnn.trainable=False\r\n\r\n  # Display layer information for reference\r\nprint('[ASSEMBLE] Full Model Architecture:')\r\nmodel.summary()\r\n\r\n  # classifier options including metrics and loss function\r\nclassifier.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-4),\r\n                loss='binary_crossentropy',\r\n                metrics=['acc'])\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\r\n                loss='binary_crossentropy',\r\n                metrics=['acc'])\r\n\r\n# try to fit on random data \r\nbatch_size=1\r\nmodel.train_on_batch(np.random.rand(batch_size,256,256,3),np.random.rand(batch_size,))\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nMy models use manually defined metrics for recall and F1 score:\r\n```python\r\ndef recall(y_true, y_pred):\r\n    \"\"\"Recall metric.\r\n\r\n    Only computes a batch-wise average of recall.\r\n\r\n    Computes the recall, a metric for multi-label classification of\r\n    how many relevant items are selected.\r\n    \"\"\"\r\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n    recall = true_positives / (possible_positives + K.epsilon())\r\n    return recall\r\n\r\ndef precision(y_true, y_pred):\r\n    \"\"\"Precision metric.\r\n\r\n    Only computes a batch-wise average of precision.\r\n\r\n    Computes the precision, a metric for multi-label classification of\r\n    how many selected items are relevant.\r\n    \"\"\"\r\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n    precision = true_positives / (predicted_positives + K.epsilon())\r\n    return precision\r\n\r\ndef f1(y_true, y_pred):\r\n    \"\"\" F1 metric.\r\n    The F1 metric is the harmonic mean of precision and recall\r\n    \"\"\"\r\n    p = precision(y_true, y_pred)\r\n    r = recall(y_true, y_pred)\r\n    return 2*((p*r)/(p+r+K.epsilon()))\r\n```\r\n", "comments": ["I figured out the exact issue and a workaround. The issue is that keras wasn't copying over the target values for the output of model into the output of classifier. This can be easily fixed by changing the structure of the model so that this step is unnecessary. I only had to change the `build_full_model()` function to do so. Here is the updated code:\r\n```python\r\ndef build_full_model(feature_detector, classifier):\r\n  model = tf.keras.models.Sequential()\r\n  #First get the features from ptdnn\r\n  model.add(feature_detector)\r\n  #Then classify\r\n!  #model.add(classifier)  \r\n!  for layer in classifier.layers:\r\n!    model.add(layer)\r\n  #feature_detector should not be trainable if fine_tuning_layers==0\r\n  feature_detector.trainable=False\r\n\r\n  # Display layer information for reference\r\n  print('[ASSEMBLE] Full Model Architecture:')\r\n  model.summary()\r\n\r\n  # classifier options including metrics and loss function\r\n  classifier.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-4),\r\n                loss='binary_crossentropy',\r\n                metrics=['acc',recall,f1])\r\n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\r\n                loss='binary_crossentropy',\r\n                metrics=['acc',recall,f1])\r\n\r\n  print('[ASSEMBLE] Models compiled successfully')\r\n  return model\r\n```\r\n\r\nThe change is on the lines with exclamation marks. Basically I just had to unroll the classifier model into the full model. I still think that the keras API was handling the original situation incorrectly, and should have been able to assign values to dense_1 target. Perhaps the issue could also be solved by introduction another layer after Classifier, but I am unsure how this would work.", "This seems to be a copy of https://github.com/tensorflow/tensorflow/issues/26927 and a TF 2.0 version of this problem https://github.com/tensorflow/tensorflow/issues/26738.", "659c981 should fix this issue. Please give it a try in the next nightly and let me know if it works as expected.\r\n\r\nThank you!", "The fix was verified by @foxik in #26738 . Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26889\">No</a>\n"]}, {"number": 26888, "title": "typo fixed for tf_env_collect.sh", "body": "", "comments": []}, {"number": 26887, "title": "removed whitespaces from MPI_LIB_IS_OPENMPI", "body": "when mvpaich2/mpich by configure.py MPI_LIB_IS_OPENMPI=True is\r\nreplaced with MPI_LIB_IS_OPENMPI=False which fails if\r\nwhitespaces are included", "comments": ["Yun, any idea why we need to remove whitespaces?\r\nI thought starlark is similar to python, and the spaces in this file should be a noop.\r\nIs this a bazel bug?", "> Yun, any idea why we need to remove whitespaces?\r\n> I thought starlark is similar to python, and the spaces in this file should be a noop.\r\n> Is this a bazel bug?\r\n\r\nThe problem here is that the configure.py does an exact string replacement for the MPI library type detection in the file ./third_party/mpi/mpi.bzl. So either in configure.py the whitespaces have to added or in ./third_party/mpi/mpi.bz the whitespaces have to be removed for the string/line MPI_LIB_IS_OPENMPI=False so that the replacement matches.\r\nI know this are both crude workarrounds for a even more crude build and configure system.\r\n", "Thanks for the clarification @mslacken \r\nIf that is the case, I would prefer modifying the string replacement in configure script, as the general style in the files have the spaces around the equal signs.\r\n\r\nIdeally, we would rewrite the mpi bazel files, but that may take a long time.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26886, "title": "Restoring a saved model and evaluating on a new Tensorflow Data object", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary (for GPU)\r\n- TensorFlow version: **GIT**: v1.9.0-0-g25c197e023, **TF**: 1.9.0\r\n- Python version: Python 3.5.2\r\n- CUDA/cuDNN version: 10.0, V10.0.130\r\n- GPU model and memory: 4 X Nvidia GeForce GTX 1080 Ti (11GB)\r\n\r\nI have this saved model and I want to restore it. After I restore, I want to evaluate it on a new dataset which I feeding with a Tensorflow Data input pipeline. For convenience I created an LSTM model using MNIST dataset and I am feeding data using reinitializable iterator of Tensorflow Data API.\r\n\r\n```\r\n#Ignore the warnings\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\nimport sys\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimport matplotlib.pyplot as plt\r\nplt.rcParams['figure.figsize'] = (8,7)\r\n%matplotlib inline\r\n\r\nold_v = tf.logging.get_verbosity()\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\n#Data parameters\r\nnum_inputs = 28\r\nnum_classes = 2\r\nnum_steps=28\r\n\r\nmnist = input_data.read_data_sets(\"MNIST_data/\")\r\n\r\nX_train = mnist.train.images[mnist.train.labels < 2].reshape((-1, num_steps, num_inputs))\r\ny_train = mnist.train.labels[mnist.train.labels < 2]\r\n\r\nX_val = mnist.validation.images[mnist.validation.labels < 2].reshape((-1, num_steps, num_inputs))\r\ny_val = mnist.validation.labels[mnist.validation.labels < 2]\r\n\r\nX_test = mnist.test.images[mnist.test.labels < 2].reshape([-1, num_steps, num_inputs])\r\ny_test = mnist.test.labels[mnist.test.labels < 2]\r\n\r\ntf.logging.set_verbosity(old_v)\r\n\r\nprint(X_train.shape)\r\n#(11623, 28, 28)\r\nprint(y_train.shape)\r\n#(11623,)\r\nprint(X_val.shape)\r\n#(1042, 28, 28)\r\nprint(y_val.shape)\r\n#(1042,)\r\nprint(X_test.shape)\r\n#(2115, 28, 28)\r\nprint(y_test.shape)\r\n#(2115,)\r\n\r\n# CREATE THE COMPUTATIONAL GRAPH\r\ninitial_learning_rate=0.0001\r\nnum_neurons = 128\r\nnum_layers = 1\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    \r\n    features_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, num_steps, num_inputs])\r\n    labels_placeholder = tf.placeholder(tf.int32, shape=[None])\r\n\r\n    batch_size = 128\r\n\r\n    # create the training dataset\r\n    Xtrain = tf.data.Dataset.from_tensor_slices(features_placeholder)\r\n    # apply a one-hot transformation to each label for use in the neural network\r\n    ytrain = tf.data.Dataset.from_tensor_slices(labels_placeholder).map(lambda z: tf.one_hot(z, 2))\r\n    # zip the x and y training data together and batch and Prefetch data for faster consumption\r\n    train_dataset = tf.data.Dataset.zip((Xtrain, ytrain)).batch(batch_size)\r\n\r\n    # create the validation dataset\r\n    Xval = tf.data.Dataset.from_tensor_slices(features_placeholder)\r\n    # apply a one-hot transformation to each label for use in the neural network\r\n    yval = tf.data.Dataset.from_tensor_slices(labels_placeholder).map(lambda z: tf.one_hot(z, 2))\r\n    # zip the x and y validation data together and shuffle, batch etc.\r\n    validation_dataset = tf.data.Dataset.zip((Xval, yval)).batch(batch_size)\r\n\r\n    # create the testing dataset\r\n    Xtest = tf.data.Dataset.from_tensor_slices(features_placeholder)\r\n    # apply a one-hot transformation to each label for use in the neural network\r\n    ytest = tf.data.Dataset.from_tensor_slices(labels_placeholder).map(lambda z: tf.one_hot(z, 2))\r\n    # zip the x and y testing data together and shuffle, batch etc.\r\n    testing_dataset = tf.data.Dataset.zip((Xtest, ytest)).batch(batch_size)\r\n\r\n    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\r\n    X, y = iterator.get_next()\r\n\r\n    training_init_op = iterator.make_initializer(train_dataset)\r\n    validation_init_op = iterator.make_initializer(validation_dataset)\r\n    testing_init_op = iterator.make_initializer(testing_dataset)\r\n\r\n    with tf.name_scope(\"graph_inputs\"):\r\n        output_keep_prob = tf.placeholder_with_default(1.0, shape=(), name =\"output_dropout\")\r\n\r\n    def build_lstm_cell(num_neurons, output_keep_prob):\r\n        \"\"\"Returns a dropout-wrapped LSTM-cell.\r\n        See https://stackoverflow.com/a/44882273/2628369 for why this local function is necessary.\r\n        Returns:\r\n        tf.contrib.rnn.DropoutWrapper: The dropout-wrapped LSTM cell.\r\n        \"\"\"\r\n        initializer = tf.contrib.layers.xavier_initializer()\r\n        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_neurons, initializer=initializer, forget_bias=1.0, state_is_tuple=True, name='LSTM_cell')\r\n        lstm_cell_drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=output_keep_prob)\r\n        return lstm_cell_drop\r\n    \r\n    with tf.name_scope(\"LSTM\"):\r\n        with tf.name_scope(\"Cell\"):\r\n            #kernel = tf.get_variable(name = 'kernel_0',shape=[156, 512], initializer=tf.contrib.layers.xavier_initializer(), trainable=False)\r\n            #bias = tf.get_variable(name='bias_0',shape=[512],initializer=tf.zeros_initializer(), trainable=False)\r\n            multi_layer_cell = tf.contrib.rnn.MultiRNNCell([build_lstm_cell(num_neurons, output_keep_prob) for _ in range(num_layers)], state_is_tuple=True)\r\n        with tf.name_scope(\"Model\"):\r\n            outputs, states = tf.nn.dynamic_rnn(cell=multi_layer_cell, inputs=X, swap_memory=False, time_major = False, dtype=tf.float32)#[Batch_size, time_steps, num_neurons]\r\n        with tf.name_scope(\"Graph_Outputs\"):\r\n            outputs = tf.transpose(outputs, [1, 0, 2]) # [num_timesteps, batch_size, num_neurons]\r\n            outputs = tf.gather(outputs, int(outputs.get_shape()[0]) - 1) # [batch_size, num_neurons]\r\n        with tf.variable_scope('Softmax'):\r\n            softmax_w = tf.get_variable(name=\"softmax_w\", initializer=tf.truncated_normal(shape=[num_neurons, num_classes], stddev=np.sqrt(2.0 /num_neurons), dtype=tf.float32))\r\n            softmax_b = tf.get_variable(name=\"softmax_b\", initializer=tf.constant(value= 2.0 / num_neurons, shape=[num_classes], dtype=tf.float32))\r\n            logits= tf.matmul(outputs, softmax_w) + softmax_b #[Batch_size X time_steps, num_classes]\r\n        with tf.name_scope('Predictions'):\r\n            predictions = tf.nn.softmax(logits, name=\"predictions\")  #[Batch_size, num_classes]\r\n        with tf.name_scope(\"Accuracy\"):\r\n                accuracy, accuracy_update_op  = tf.metrics.accuracy(labels = tf.argmax(y,1), predictions = tf.argmax(predictions, axis = 1), name = 'accuracy')\r\n                running_vars_accuracy = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"LSTM/Accuracy\")\r\n        with tf.name_scope(\"AUC\"):\r\n                auc, auc_update_op  = tf.metrics.auc(labels = tf.argmax(y,1), predictions = predictions[:,1], name = 'auc')\r\n                running_vars_auc = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"LSTM/AUC\")\r\n        with tf.name_scope('Loss'):\r\n            xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y, name='xentropy')\r\n            loss = tf.reduce_mean(xentropy, name=\"loss\")\r\n        with tf.name_scope('Train'):\r\n            optimizer= tf.train.AdamOptimizer(learning_rate=initial_learning_rate)\r\n            trainer=optimizer.minimize(loss, name=\"training_op\")\r\n        with tf.name_scope(\"Saver\"):\r\n            saver = tf.train.Saver(var_list=tf.trainable_variables()) \r\n        with tf.name_scope(\"init\"):\r\n            global_variables_init = tf.global_variables_initializer()\r\n            running_vars_initializer_accuracy = tf.variables_initializer(var_list=running_vars_accuracy)\r\n            running_vars_initializer_auc = tf.variables_initializer(var_list=running_vars_auc)\r\n\r\n#EXECUTE THE COMPUTATIONAL GRAPH\r\n#Network parameters\r\nnum_epochs = 20\r\nmax_checks_without_progress = 200\r\ncheck_without_progress = 0\r\nbest_loss = np.infty\r\noutput_keep_var = 0.5\r\n\r\ndef get_model_params():\r\n    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\r\n    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\r\n\r\ndef restore_model_params(model_params):\r\n    gvar_names = list(model_params.keys())\r\n    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\r\n                  for gvar_name in gvar_names}\r\n    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\r\n    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\r\n    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\r\n    \r\n    \r\nwith tf.Session(graph = graph) as sess:\r\n    global_variables_init.run()\r\n    final_model_path = \"./my_deep_model_MNIST.ckpt\"\r\n        \r\n    print(\"Initialized\")\r\n    # Training cycle\r\n    val_loss = []\r\n    val_accuracy = []\r\n    train_loss = []\r\n    train_accuracy = []\r\n    total_train_batch = int(X_train.shape[0]/ batch_size + 1)\r\n    total_val_batch = int(X_val.shape[0]/ batch_size + 1)\r\n        \r\n    for epoch in range(0, num_epochs):\r\n        avg_cost_train = 0.\r\n        avg_accuracy_train =0\r\n        avg_cost_val = 0.\r\n        avg_accuracy_val =0\r\n        \r\n        \r\n        running_vars_initializer_accuracy.run()\r\n        running_vars_initializer_auc.run()\r\n        sess.run(training_init_op, feed_dict={features_placeholder: X_train, labels_placeholder: y_train})\r\n        # Loop over all batches\r\n        for _ in range(total_train_batch):\r\n            _, miniBatchCost_train, _, _ = sess.run([trainer, loss, accuracy_update_op, auc_update_op], feed_dict={output_keep_prob: output_keep_var})\r\n            avg_cost_train += miniBatchCost_train / total_train_batch\r\n        accuracy_train = sess.run(accuracy)\r\n        AUC_train = sess.run(auc)\r\n        train_loss.append(avg_cost_train)\r\n        train_accuracy.append(avg_accuracy_train)\r\n        \r\n        running_vars_initializer_accuracy.run()\r\n        running_vars_initializer_auc.run()\r\n        sess.run(validation_init_op, feed_dict={features_placeholder: X_val, labels_placeholder: y_val})\r\n        for _ in range(total_val_batch):\r\n            miniBatchCost_val, _, _ = sess.run([loss, accuracy_update_op, auc_update_op])\r\n            avg_cost_val += miniBatchCost_val / total_val_batch\r\n        accuracy_val = sess.run(accuracy)\r\n        AUC_val = sess.run(auc)\r\n        val_loss.append(avg_cost_val)\r\n        val_accuracy.append(avg_accuracy_val)\r\n        \r\n        if avg_cost_val < best_loss:\r\n            save_path = saver.save(sess, final_model_path)\r\n            best_params = get_model_params()\r\n            best_loss = avg_cost_val\r\n            check_without_progress = 0\r\n            save_message = \"*\"\r\n        else:\r\n            check_without_progress +=1\r\n            save_message = \"\"\r\n            if check_without_progress > max_checks_without_progress:\r\n                print(\"Stopping Early! Loss has not improved in {} epochs\".format(max_checks_without_progress))\r\n                break\r\n    \r\n        print(\"Epoch: {:d}-\".format(epoch), \\\r\n              \"Training Loss: {:.6f}, \".format(avg_cost_train), \\\r\n              \"Training Accuracy: {:>.2%}, \".format(accuracy_train), \\\r\n              \"Training AUC: {:>.2%},\".format(AUC_train), \\\r\n              \"Validation Loss: {:.6f}, \".format(avg_cost_val), \\\r\n              \"Validation Accuracy: {:>.2%},\".format(accuracy_val), \\\r\n              \"Validation AUC: {:>.2%},\".format(AUC_val),\\\r\n              save_message)\r\n    print(\"Optimization Finished!\")\r\n\r\n    # If we used early stopping then rollback to the best model found\r\n    if best_params:\r\n        restore_model_params(best_params)\r\n    \r\n    total_test_batch = int(X_test.shape[0]/ batch_size + 1)\r\n    running_vars_initializer_accuracy.run()\r\n    running_vars_initializer_auc.run()\r\n    sess.run(testing_init_op, feed_dict={features_placeholder: X_test, labels_placeholder: y_test})\r\n    for _ in range(total_test_batch):\r\n        sess.run([accuracy_update_op, auc_update_op])\r\n    accuracy_test= sess.run(accuracy)\r\n    auc_test= sess.run(auc)\r\n    print(\"Final test accuracy: {:>.2%}\".format(accuracy_test), \"Final test AUC: {:>6.1%}\".format(auc_test))\r\n    \r\n    plt.plot(train_loss, label='Train Loss')\r\n    plt.plot(val_loss, label='Validation Loss')\r\n    plt.xlabel('Epoch')\r\n    plt.ylabel('Cost')\r\n    plt.title(\"Loss\")\r\n    plt.legend()\r\n    plt.show()\r\n```\r\n\r\nThe model is saved. I try to restore model and use it to evaluate on ANOTHER dataset object (assuming test set of MNIST is a new dataset).\r\n\r\n```\r\ntf.reset_default_graph()\r\nwith tf.Session() as sess:\r\n    new_saver = tf.train.import_meta_graph('my_deep_model_MNIST.ckpt.meta')\r\n    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\r\n    print(\"Restored Operations from MetaGraph\")\r\n    g = tf.get_default_graph()\r\n    \r\n    predictions = g.get_tensor_by_name('LSTM/Predictions/predictions:0')\r\n    \r\n    accuracy_update_op = g.get_tensor_by_name('LSTM/Accuracy/accuracy/update_op:0')\r\n    accuracy = g.get_tensor_by_name('LSTM/Accuracy/accuracy/value:0')\r\n    \r\n    auc_update_op = g.get_tensor_by_name('LSTM/AUC/auc/update_op:0')\r\n    auc = g.get_tensor_by_name('LSTM/AUC/auc/value:0')\r\n    \r\n    ################ Let's start another dataset object################\r\n    batch_size = 128\r\n    Xtest = mnist.test.images[mnist.test.labels < 2].reshape([-1, num_steps, num_inputs])\r\n    ytest = mnist.test.labels[mnist.test.labels < 2]\r\n\r\n    featuresplaceholder = tf.placeholder(dtype=tf.float32, shape=[None, num_steps, num_inputs])\r\n    labelsplaceholder = tf.placeholder(tf.int32, shape=[None])\r\n    \r\n\r\n    train = tf.data.Dataset.from_tensor_slices(featuresplaceholder)\r\n    train = tf.data.Dataset.from_tensor_slices(labelsplaceholder).map(lambda z: tf.one_hot(z, 2))\r\n    dataset = tf.data.Dataset.zip((train, train)).batch(batch_size)\r\n\r\n    iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)\r\n    X, y = iterator.get_next()\r\n    dataset_init_op = iterator.make_initializer(dataset)\r\n    ###############################################################\r\n    \r\n    total_test_batch = int(Xtest.shape[0]/ batch_size + 1)\r\n    tf.local_variables_initializer().run()\r\n    sess.run(dataset_init_op, feed_dict={featuresplaceholder: Xtest, labelsplaceholder: ytest})\r\n    for _ in range(total_test_batch):\r\n        sess.run([accuracy_update_op, auc_update_op])\r\n    accuracy_test= sess.run(accuracy)\r\n    auc_test= sess.run(auc)\r\n    print(\"Final test accuracy: {:>.2%}\".format(accuracy_test), \"Final test AUC: {:>6.1%}\".format(auc_test))\r\n```\r\n\r\nHowever, I am having an `FailedPreconditionError` error, even though I initialize the iterator using ` sess.run(dataset_init_op, feed_dict={featuresplaceholder: Xtest, labelsplaceholder: ytest})`. That is really strange because the same code works without using Tensorflow Data API and using only `feed_dict`. \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1321     try:\r\n-> 1322       return fn(*args)\r\n   1323     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1306       return self._call_tf_sessionrun(\r\n-> 1307           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1308 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1408           self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1409           run_metadata)\r\n   1410     else:\r\n\r\nFailedPreconditionError: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,28,28], [?,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\t [[Node: IteratorGetNext/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_17_IteratorGetNext\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-2-20929b658f9d> in <module>()\r\n     37     sess.run(dataset_init_op, feed_dict={featuresplaceholder: Xtest, labelsplaceholder: ytest})\r\n     38     for _ in range(total_test_batch):\r\n---> 39         sess.run([accuracy_update_op, auc_update_op])\r\n     40     accuracy_test= sess.run(accuracy)\r\n     41     auc_test= sess.run(auc)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    898     try:\r\n    899       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 900                          run_metadata_ptr)\r\n    901       if run_metadata:\r\n    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1133     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1134       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1135                              feed_dict_tensor, options, run_metadata)\r\n   1136     else:\r\n   1137       results = []\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1314     if handle is None:\r\n   1315       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1316                            run_metadata)\r\n   1317     else:\r\n   1318       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333         except KeyError:\r\n   1334           pass\r\n-> 1335       raise type(e)(node_def, op, message)\r\n   1336 \r\n   1337   def _extend_graph(self):\r\n\r\nFailedPreconditionError: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,28,28], [?,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\t [[Node: IteratorGetNext/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_17_IteratorGetNext\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\r\n    self.io_loop.start()\r\n  File \"/home/musara1/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 148, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\r\n    self._run_once()\r\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\r\n    handle._run()\r\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\r\n    self._callback(*self._args)\r\n  File \"/home/musara1/.local/lib/python3.5/site-packages/tornado/ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\r\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\r\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/minitornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-20929b658f9d>\", line 3, in <module>\r\n    new_saver = tf.train.import_meta_graph('my_deep_model_MNIST.ckpt.meta')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1960, in import_meta_graph\r\n    **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py\", line 744, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3563, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3563, in <listcomp>\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3450, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nFailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,28,28], [?,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\t [[Node: IteratorGetNext/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_17_IteratorGetNext\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```", "comments": ["@mmuratarat Is this still an issue? If yes, could you try with recent TF versions (`tf-nightly`, `TF1.14.0`, `TF2.0rc0`) and let us know whether the issue persists? \r\n\r\nPlease try to follow new tutorials [`Save and Restore`](https://www.tensorflow.org/guide/saved_model) and [`datasets`](https://www.tensorflow.org/guide/datasets). Also, try to provide a simplified standalone code to reproduce the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26886\">No</a>\n"]}, {"number": 26885, "title": "Lite: ResizeTensor Dim size check added to avoid reallocation if no c\u2026", "body": "If Tensor dimension is already same as new dimension, then avoid reallocation.", "comments": ["> Left comment\r\n\r\nResolved the comment, please check again, Thanks!", "@karimnosseir : Your last comment is handled, please check and conclude this PR, TIA!", "@karimnosseir : would you please check and approve the changes, TIA!", "@rthadur : i think this can be merged, TIA!", "@karimnosseir , @rthadur : Gentle reminder! \r\nNOTE: #27015 is already merged which depends on this PR, so please consider to merge this PR as well, TIA!", "@miaout17 : Gentle reminder!", "@rthadur : Please help proceed with this PR, Thanks!", "@rthadur : Gentle Reminder!", "@rthadur : Please help get this merged, Thanks!", "This failed some internal tests. Please be patient, landing these kinds of seemingly trivial changes is often non-trivial with the number of internal tests that we have.", "Can one of the admins verify this patch?", "@ANSHUMAN87 can you please fix build errors ?", "@rthadur : I am sorry i am unable to get the logs details for the failures. Would you please help, Thanks!", "@ANSHUMAN87 here are the logs https://source.cloud.google.com/results/invocations/25250522-3be5-45da-a4f1-413e624fedc5/log", "@rthadur : I have checked the logs, it is not related to my changes. Because of some build ENV issue it has failed. Please help. Thanks!", "Thanks for your patience on this. There were some external tests/apps making assumptions about the lifetime of the TfLiteIntArray used for the resized tensor, so had to make a small fix there."]}, {"number": 26884, "title": "Update", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26884) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 26883, "title": "Lite: Fully_connected op bug fixed and code refactored", "body": "1:> TF_LITE_MACRO_DISPATCH --> remove this macro unused\r\n2:> Unnecessary check removed as it is clearly evident from code above\r\n3:> Shift negation is not required explicitly when computed, it should be binded with code where it is used, if needed\r\n4:> Confusing comment removed\r\n5:> Error message improvised\r\n6:> Code refactored to accommodate multiple if() else ()\r\n7:> Move data type check to one central place, to improve code readability\r\n8:> 2 Bug fixed as part of above change\r\n       \r\n\r\n-  The code was prone to significant data loss, if user has given Input as int16 type which was explicitly type casted to int8/uint8.\r\n-  Null pointer access when Bias is not present at EvalShuffledQuantized()\r\n\r\n9:> If condition modified to imply quantized input", "comments": ["Please avoid coupling refactoring changes with actual behavior changes and/or bug fixes. Can you split this CL into different pieces accordingly (with the highest priority being the bug fix)? Thanks!", "> Please avoid coupling refactoring changes with actual behavior changes and/or bug fixes. Can you split this CL into different pieces accordingly (with the highest priority being the bug fix)? Thanks!\r\n\r\n@jdduke : Thanks for your suggestion, i will keep in mind for my future PRs. Because the PR contains multiple changes, i have consolidated the list of changes above with descriptions, if that helps. Otherwise should i close the current PR and raise multiple PRs, or raise multiple commits in one PR? Please let me know your convenience, TIA!", ">  Otherwise should i close the current PR and raise multiple PRs, or raise multiple commits in one PR? Please let me know your convenience, TIA!\r\n\r\nPrefer multiple PRs. In practice, once this change gets approved, it is landed internally as an atomic commit. Better to have it split across multiple (real) commits than a single one.", "Also, it makes it easier to review and land if it focuses on a single issue/fix.", "> Also, it makes it easier to review and land if it focuses on a single issue/fix.\r\n\r\n@jdduke : I am closing this PR, to raise again with multiple PRs as suggested by you.\r\n@shahzadlone : Thanks for your comments, i will consider those in my next PRs.", "Closing to raise again."]}, {"number": 26882, "title": "ERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_absl/absl/types/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/types:bad_variant_access' failed (Exit 1) on benchmark_model tool build", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\nlsb_release -a\r\n\tNo LSB modules are available.\r\n\tDistributor ID:\tUbuntu\r\n\tDescription:\tUbuntu 16.04.5 LTS\r\n\tRelease:\t16.04\r\n\tCodename:\txenial\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nBuild from source\r\n- TensorFlow version:\r\nFresh master\r\n```\r\ncommit 3b3da345340a4ff5f4c587ee38e5a468f252aee1 (HEAD -> master, origin/master, origin/HEAD)\r\nAuthor: Juhyun Lee <impjdi@google.com>\r\nDate:   Tue Mar 19 05:36:06 2019 -0700\r\n\r\n    Changed return type of run() from boolean to void.\r\n\r\n    PiperOrigin-RevId: 239171696\r\n```\r\n- Python version:\r\n```\r\npython -c \"import sys;print(sys.version)\"\r\n3.5.2 (default, Nov 23 2017, 16:37:01)\r\n[GCC 5.4.0 20160609]\r\n```\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n```\r\nbazel version\r\n\tWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n\tINFO: Invocation ID: 1f934444-2ee8-4d1c-9a2c-534fdf65195d\r\n\tBuild label: 0.22.0\r\n\tBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n\tBuild time: Mon Jan 28 12:58:08 2019 (1548680288)\r\n\tBuild timestamp: 1548680288\r\n\tBuild timestamp as int: 1548680288\r\n```\r\n- GCC/Compiler version (if compiling from source):\r\n```\r\ngcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/6/lto-wrapper\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 6.4.0-17ubuntu1~16.04' --with-bugurl=file:///usr/share/doc/gcc-6/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --with-as=/usr/bin/x86_64-linux-gnu-as --with-ld=/usr/bin/x86_64-linux-gnu-ld --program-suffix=-6 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-6-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-6-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-6-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 6.4.0 20180424 (Ubuntu 6.4.0-17ubuntu1~16.04)\r\n```\r\n- CUDA/cuDNN version:\r\n```\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n```\r\n- GPU model and memory:\r\nGeForce GTX TITAN X\r\nGeForce GTX 1080 Ti\r\n\r\nCommand to reproduce:\r\n\r\n```\r\ngit clone git@github.com:tensorflow/tensorflow.git\r\ncd tensorflow\r\nbazel build -c opt --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --config monolithic tensorflow/tools/benchmark:benchmark_model\r\n```\r\n\r\nOutput:\r\n```\r\n\tWARNING: /data/user/external_projects/tensorflow/tensorflow/core/BUILD:1794:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/cc/saved_model:loader.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n\tWARNING: /data/user/external_projects/tensorflow/tensorflow/core/BUILD:1794:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/distributed_runtime:server_lib.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\n\tINFO: Analysed target //tensorflow/tools/benchmark:benchmark_model (72 packages loaded, 4810 targets configured).\r\n\tINFO: Found 1 target...\r\n\tERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_absl/absl/types/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/types:bad_variant_access' failed (Exit 1)\r\n\tTarget //tensorflow/tools/benchmark:benchmark_model failed to build\r\n\tUse --verbose_failures to see the command lines of failed build steps.\r\n\tINFO: Elapsed time: 6.057s, Critical Path: 0.06s\r\n\tINFO: 0 processes.\r\n\tFAILED: Build did NOT complete successfully\r\n```\r\n\r\nLooks like error changes every time I run command:\r\n\r\nRun 2:\r\n```\r\n\t...\r\n\tINFO: Analysed target //tensorflow/tools/benchmark:benchmark_model (0 packages loaded, 0 targets configured).\r\n\tINFO: Found 1 target...\r\n\tERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_googletest/BUILD.bazel:57:1: C++ compilation of rule '@com_google_googletest//:gtest' failed (Exit 1)\r\n\tTarget //tensorflow/tools/benchmark:benchmark_model failed to build\r\n\tUse --verbose_failures to see the command lines of failed build steps.\r\n\tINFO: Elapsed time: 0.304s, Critical Path: 0.05s\r\n\tINFO: 0 processes.\r\n\tFAILED: Build did NOT complete successfully\r\n```\r\n\r\nRun 3:\r\n```\r\n\t...\r\n\tINFO: Analysed target //tensorflow/tools/benchmark:benchmark_model (0 packages loaded, 0 targets configured).\r\n\tINFO: Found 1 target...\r\n\tERROR: /home/user/.cache/bazel/_bazel_user/b4774fbdb8542988b4e302c9e073f145/external/com_google_absl/absl/strings/BUILD.bazel:32:1: C++ compilation of rule '@com_google_absl//absl/strings:strings' failed (Exit 1)\r\n\tTarget //tensorflow/tools/benchmark:benchmark_model failed to build\r\n\tUse --verbose_failures to see the command lines of failed build steps.\r\n\tINFO: Elapsed time: 0.236s, Critical Path: 0.02s\r\n\tINFO: 0 processes.\r\n\tFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Seems I forget `./configure`\r\n\r\n```\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y\r\nSearching for NDK and SDK installations.\r\n\r\nPlease specify the home path of the Android NDK to use. [Default is /home/user/Android/Sdk/ndk-bundle]: /usr/lib/android-sdk/android-ndk-r16b\r\n\r\nPlease specify the home path of the Android SDK to use. [Default is /home/user/Android/Sdk]: /usr/lib/android-sdk\r\n\r\nEither /usr/lib/android-sdk does not exist, or it does not contain the subdirectories \"platforms\" and \"build-tools\".\r\n```\r\nNot sure what `/usr/lib/android-sdk` should contain, in my case:\r\n\r\n```\r\ntree -L 1 android-sdk\r\nandroid-sdk\r\n\u251c\u2500\u2500 android-ndk-r16b\r\n\u251c\u2500\u2500 build-tools\r\n\u251c\u2500\u2500 platform-tools\r\n\u2514\u2500\u2500 tools\r\n```\r\n\r\n```\r\ntree -L 1 android-sdk/android-ndk-r16b/\r\nandroid-sdk/android-ndk-r16b/\r\n\u251c\u2500\u2500 build\r\n\u251c\u2500\u2500 CHANGELOG.md\r\n\u251c\u2500\u2500 meta\r\n\u251c\u2500\u2500 ndk-build\r\n\u251c\u2500\u2500 ndk-depends\r\n\u251c\u2500\u2500 ndk-gdb\r\n\u251c\u2500\u2500 ndk-stack\r\n\u251c\u2500\u2500 ndk-which\r\n\u251c\u2500\u2500 platforms\r\n\u251c\u2500\u2500 prebuilt\r\n\u251c\u2500\u2500 python-packages\r\n\u251c\u2500\u2500 README.md\r\n\u251c\u2500\u2500 shader-tools\r\n\u251c\u2500\u2500 simpleperf\r\n\u251c\u2500\u2500 source.properties\r\n\u251c\u2500\u2500 sources\r\n\u251c\u2500\u2500 sysroot\r\n\u2514\u2500\u2500 toolchains\r\n```\r\n\r\ni.e. android-sdk have `build-tools`, but `platforms` is under `android-sdk/android-ndk-r16b/`.\r\n", "Are you still able to repro on master? This looks more like a transient bazel issue than anything else.", "Closing due to lack of response, feel free to reopen if the issues still repros with the latest build.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26882\">No</a>\n"]}, {"number": 26881, "title": "ImportError: cannot import name 'audio' from 'tensorflow._api.v1'", "body": "Am new in Tensor flow,i was running my first example in tensor flow but found this issue.\r\nHow can i go about it    \r\n\r\n\"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n\r\nConverterError: TOCO failed. See console for info.\r\nTraceback (most recent call last):\r\n  File \"D:\\Software\\Scripts\\toco_from_protos-script.py\", line 6, in <module>\r\n    from tensorflow.lite.toco.python.toco_from_protos import main\r\n  File \"D:\\Software\\lib\\site-packages\\tensorflow\\__init__.py\", line 31, in <module>\r\n    from tensorflow._api.v1 import audio\r\nImportError: cannot import name 'audio' from 'tensorflow._api.v1' (D:\\Software\\lib\\site-packages\\tensorflow\\_api\\v1\\__init__.py)", "comments": ["Please provide following information. Thanks!\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 26880, "title": "checkpoint_management.py - custom checkpoint prefix", "body": "", "comments": ["I updated the relevant APIs. hope it was done correctly.\r\n\r\nThese are the results of the test:\r\n\r\nroot@comp:/mnt/dev/tensorflow# bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\nLimited tf.compat.v2.summary API due to missing TensorBoard installation\r\nRunning tests under Python 2.7.12: /usr/bin/python\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibility\r\nW0319 23:55:09.072984 140072935352064 deprecation.py:323] From /mnt/dev/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/python/util/decorator_utils.py:145: VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.GraphKeys.GLOBAL_VARIABLES` instead.\r\nE0319 23:55:12.594662 140072935352064 api_compatibility_test.py:246] TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\n\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n\r\nE0319 23:55:12.595136 140072935352064 api_compatibility_test.py:247] 1 differences found between API and golden.\r\nIssue 1 :\r\n  path: \"tensorflow.train.CheckpointManager\"\r\n  tf_class {\r\n    is_instance: \"<class \\'tensorflow.python.training.checkpoint_management.CheckpointManager\\'>\"\r\n    is_instance: \"<type \\'object\\'>\"\r\n    member {\r\n      name: \"checkpoints\"\r\n      mtype: \"<type \\'property\\'>\"\r\n    }\r\n    member {\r\n      name: \"latest_checkpoint\"\r\n      mtype: \"<type \\'property\\'>\"\r\n    }\r\n    member_method {\r\n      name: \"__init__\"\r\n-     argspec: \"args=[\\'self\\', \\'checkpoint\\', \\'directory\\', \\'max_to_keep\\', \\'keep_checkpoint_every_n_hours\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n+     argspec: \"args=[\\'self\\', \\'checkpoint\\', \\'directory\\', \\'max_to_keep\\', \\'keep_checkpoint_every_n_hours\\', \\'checkpoint_name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'ckpt\\'], \"\r\n?                                                                                                                +++++++++++++++++++++                                                  ++++++++++\r\n    }\r\n    member_method {\r\n      name: \"save\"\r\n      argspec: \"args=[\\'self\\', \\'checkpoint_number\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n    }\r\n  }\r\n\r\nW0319 23:55:12.595268 140072935352064 api_compatibility_test.py:254] Golden file update requested!\r\nAll test failures have been skipped, see the logs for detected diffs.\r\nThis test is now going to write new golden files.\r\nMake sure to package the updates together with your change.\r\n\r\nYou will need an explicit API approval. This may take longer than a normal\r\nreview.\r\n\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibility\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV1\r\nI0319 23:55:15.501441 140072935352064 api_compatibility_test.py:273] No differences found between API and golden.\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV1\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\nE0319 23:55:18.709964 140072935352064 api_compatibility_test.py:246] TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\n\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n\r\nE0319 23:55:18.710264 140072935352064 api_compatibility_test.py:247] 1 differences found between API and golden.\r\nIssue 1 :\r\n  path: \"tensorflow.train.CheckpointManager\"\r\n  tf_class {\r\n    is_instance: \"<class \\'tensorflow.python.training.checkpoint_management.CheckpointManager\\'>\"\r\n    is_instance: \"<type \\'object\\'>\"\r\n    member {\r\n      name: \"checkpoints\"\r\n      mtype: \"<type \\'property\\'>\"\r\n    }\r\n    member {\r\n      name: \"latest_checkpoint\"\r\n      mtype: \"<type \\'property\\'>\"\r\n    }\r\n    member_method {\r\n      name: \"__init__\"\r\n-     argspec: \"args=[\\'self\\', \\'checkpoint\\', \\'directory\\', \\'max_to_keep\\', \\'keep_checkpoint_every_n_hours\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n+     argspec: \"args=[\\'self\\', \\'checkpoint\\', \\'directory\\', \\'max_to_keep\\', \\'keep_checkpoint_every_n_hours\\', \\'checkpoint_name\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'ckpt\\'], \"\r\n?                                                                                                                +++++++++++++++++++++                                                  ++++++++++\r\n    }\r\n    member_method {\r\n      name: \"save\"\r\n      argspec: \"args=[\\'self\\', \\'checkpoint_number\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n    }\r\n  }\r\n\r\nW0319 23:55:18.710416 140072935352064 api_compatibility_test.py:254] Golden file update requested!\r\nAll test failures have been skipped, see the logs for detected diffs.\r\nThis test is now going to write new golden files.\r\nMake sure to package the updates together with your change.\r\n\r\nYou will need an explicit API approval. This may take longer than a normal\r\nreview.\r\n\r\n[       OK ] ApiCompatibilityTest.testAPIBackwardsCompatibilityV2\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessage\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV1\r\n[ RUN      ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[       OK ] ApiCompatibilityTest.testNoSubclassOfMessageV2\r\n[ RUN      ] ApiCompatibilityTest.test_session\r\n[  SKIPPED ] ApiCompatibilityTest.test_session\r\n----------------------------------------------------------------------\r\nRan 7 tests in 10.638s\r\n\r\nOK (skipped=1)", "Is there something i need to do? Or the tests just have a long queue?", "There's nothing you need to do. It's just waiting for an API review. "]}, {"number": 26879, "title": "TF 2.0: tf.stack can cause a segmentation fault", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Mobile device: N/A\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0.0-alpha0, 2.0.0-dev20190319\r\n- Python version: 3.6.7\r\n- Bazel version: N/A\r\n- GCC/Compiler version: N/A\r\n- CUDA/cuDNN version: 10.1 / 7.4.2\r\n- GPU model and memory: GeForce GTX 1080 Ti (11 GB)\r\n\r\n**Describe the current behavior**\r\nThe Python interpreter crashes with SIGSEGV (Segmentation Fault); according to gdb the fault occurs in ```EagerTensor_CheckExact(_object const*) ()```.\r\n\r\n**Describe the expected behavior**\r\nNo segmentation fault.\r\nIdeally a stacked tensor returned (I was adapting code I developed interactively in eager execution mode, where it worked, in a Jupyter notebook for addition to a Keras based model), or an error that the argument cannot be a tensor (this is the TF1 behavior):\r\n```\r\nTypeError: Expected list for 'values' argument to 'pack' Op, not <tf.Tensor 'input_1:0' shape=(?, 128, 128, 1) dtype=float32>.\r\n``` \r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\n\r\nprint(tf.__version__)\r\n\r\ninput_ = Input((128, 128, 1), dtype='float32')\r\nprint(input_)\r\noutput = tf.stack(input_, axis=1)\r\n```", "comments": ["@csachs @dynamicwebpaige  i found that there is no check about the type of values in tf2.0 but this check was there in tf 1.x , according to docs it should be a list or tuple.\r\nso, by adding these lines in `ops/gen_array_ops pack function.`\r\n`\r\n    if not isinstance(values, (list, tuple)):\r\n      raise TypeError(\r\n          \"Expected list for 'values' argument to \"\r\n          \"'pack' Op, not %r.\" % values)\r\n` \r\nsolves this issue.\r\nbut this file is machine generated @dynamicwebpaige mam, can you guide me with this.\r\n", "I think the issue is located in:\r\nhttps://github.com/tensorflow/tensorflow/blob/ca1f0ce1b9e82e9577a2439c1c40cf38d4694c40/tensorflow/python/eager/pywrap_tfe_src.cc#L1835-L1843\r\n\r\nThe `PySequence_Fast_GET_ITEM` tries to access item which should be wrapped with `PySequence_Fast` instead.\r\n\r\nCreated a PR #27133 to fix the segmentation fault issue.", "@yongtang thanks for the PR :+1: ", "It seems to still occur when calling the predict method on a model loaded with tf.keras.models import load_model, though if i import load_model from keras not in tf2 all works well.\r\n\r\nis there a way of solving this?", "@kenseii if the issue persist, can you open a new issue with all the details so that the issue could be reproduced reliably? That will help triage and debug the issue."]}, {"number": 26878, "title": "Changed subgraph_test_util to control_flow_test_util", "body": "", "comments": ["@yenchenlin I have done this modification based on \" TODO(ycling): This file should be renamed as `control_flow_test_util` to avoid confusion.\", could you please review this PR", "miaout17 Could you please review and approve this PR?", "Can one of the admins verify this patch?", "@joyalbin  Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 26877, "title": "TF 2.0.0a0 doesn't find attributes after fresh installation", "body": "**System information**\r\n- OS Platform and Distribution: **Windows 10 Pro x64**\r\n- TensorFlow installed from (source or binary): **binary (for GPU)** https://files.pythonhosted.org/packages/a3/a1/adff98d96de3d623f53f2e002d6e5b3936857c168a99efa632b7db89da05/tensorflow_gpu-2.0.0a0-cp37-cp37m-win_amd64.whl\r\n- TensorFlow version: **2.0.0a0 for GPU**\r\n- Python version: **3.7.2 64-bit (3.6.8 x64)**\r\n- Installed using: **virtualenv & pip**\r\n- CUDA/cuDNN version: **10.0 | 7.5.0 (7.4.2)**\r\n- GPU model and memory: **Nvidia RTX 2080 Ti (11GB)**\r\n- Others: **Visual Studio 2017 Community | Nvidia-Drivers 419.35**\r\n\r\n<details>\r\n  <summary>Package Versions</summary>\r\n- absl-py 0.7.1\r\n- astor 0.7.1\r\n- gast 0.2.2\r\n- google-pasta 0.1.4\r\n- grpcio 1.19.0\r\n- h5py 2.9.0\r\n- Keras-Applications 1.0.7\r\n- Keras-Preprocessing 1.0.9\r\n- Markdown 3.0.1\r\n- numpy 1.16.2\r\n- pip 19.0.3\r\n- protobuf 3.7.0\r\n- setuptools 40.8.0\r\n- six 1.12.0\r\n- tb-nightly 1.14.0a20190301\r\n- tensorflow-gpu 2.0.0a0\r\n- termcolor 1.1.0\r\n- tf-estimator-nightly 1.14.0.dev2019030115\r\n- Werkzeug 0.14.1\r\n- wheel 0.33.1\r\n</details>\r\n\r\n**Describe the problem**\r\nHi you guys! Thanks in advance!\r\nI'm trying to install tensorflow-gpu 2.0.0a0 in an venv using pip as described at https://www.tensorflow.org/install/pip via _'pip install tensorflow-gpu==2.0.0-alpha0'_.\r\nIt works fine with TF 1.13.1 (GPU/CPU) but it doesn't for 2.0.0a0 (GPU/CPU)! The GPU and CPU version of 2.0.0a0 has the same error.\r\n\r\nFirst I installed everything according to guides on a fresh system.\r\nAfter installation I test it with the recommended cmd-line:\r\n\r\n_python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"_\r\n\r\nAs for the error, what I get is:\r\n\r\n_Traceback (most recent call last):\r\n  File \"\\<string\\>\", line 1, in \\<module\\>\r\nAttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'_\r\n\r\nI tried using python 3.6.8 and cuDNN 7.4.2. Also I tried using the newest versions of tf-estimator-nightly and tb-nightly. Tried it outside the venv. Tried it without _'tf.enable_eager_execution();'_ but then it has no attribute _'random_normal'_. Tried using .whl from other sites.\r\n\r\nSince TF 1.13.1-gpu works and the error is so strangely unspecific, I have no idea what to try next. I saw issues with similar problems, but those solutions didn't work for me or the issue was similar but not this one. Please help!", "comments": ["It seems that different syntax in TF 2.0 is used.\r\nTry this code (it worked for me):\r\n```\r\npython -c \"import tensorflow as tf;     \\\r\n    print('Version:', tf.__version__);  \\\r\n    print(tf.reduce_sum(tf.random.normal([1000, 1000])));\"\r\n```\r\n", "Thank you foobar167, it works!\r\n\r\nIt seems I have to get used to the new API. You are right, it needs to be _tf.random.normal_ instead of _tf.random_normal_ and also needs to use _tf.executing_eagerly()_ instead of _tf.enable_eager_execution()_ (which is on in 2.0 by default).\r\n\r\nThe TF-install-Doc needs to be update there, so this confusion would happen again.\r\n\r\nThanks again!", "To whoever needs to figure this out. You can use below code to verify the installation of TensorFlow v2.0\r\n\r\npython -c \"import tensorflow as tf; tf.executing_eagerly(); print(tf.reduce_sum(tf.random.normal([1000, 1000])))\""]}, {"number": 26876, "title": "Compilation warnings removed from tensor_format.h", "body": "Removed compilation warnings", "comments": []}, {"number": 26875, "title": "Added Negative Axes support along with TCs.", "body": "Added the support for negative axes along with TCs", "comments": ["@renjie-liu , sorry for the oversight, i have updated the name of the TC, kindly check and approve.", "@renjie-liu , thanks for the comments, i have updated as suggested by you.Kindly check and approve.\r\n\r\nRegards\r\nAmit", "@rthadur , can you pls help to get this merged, as this is approved and all checks have passed  and i am afraid that if it stays longer again some merge conflicts might come.\r\n\r\nRegards\r\nAmit", "@rthadur , this PR is approved, for long, can you please help me to get this merged.\r\n\r\nRegards\r\nAmit", "@renjie-liu , thanks for approving the PR, this PR is not yet merge, can you please help me to get this merge.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "working on it, thank you for your patience.", "@jianlijianli , thanks for approving the PR\r\n\r\n@rthadur , can you please help to merge this PR.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to merge this PR.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to meger this PR.\r\n\r\n\r\nRegards\r\nAmit", "@renjie-liu , @jianlijianli @rthadur , i have updated the code and now everything seems to be passing, kindly check and approve, hope to see this PR merge soon.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "@amitsrivastava78 Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]