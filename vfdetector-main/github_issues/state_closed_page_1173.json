[{"number": 18004, "title": "Adding Installation instructions for TensorRT", "body": "This PR updates installation instructions for GPU packages with an optional step.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Tagging @gunan and @MarkDaoust ."]}, {"number": 18003, "title": "Feature Request: No U-Turn Sampler (NUTS) in tf.contrib.bayesflow", "body": "The NUTS sampler (http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf) would be a great addition to the HMC code already in tf.contrib.bayesflow.\r\n\r\nA reference implementation is at: https://github.com/stan-dev/stan/blob/develop/src/stan/mcmc/hmc/nuts/base_nuts.hpp, and this has already been mentioned by @jvdillon and @dustinvtran in https://github.com/tensorflow/tensorflow/issues/4965. The HMC code has been brought over, but I don't see NUTS in there.\r\n\r\nIs someone already working on this? Is there any way I could help?\r\n\r\n---\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi,\r\n  Great question! We have some work progressing on this (and I hope we can give an update in the near future on this).\r\n\r\nMight I ask you to close this issue, and file it again in https://github.com/tensorflow/probability? We've been focusing / moving our code and efforts in to this repo (as you might notice the HMC code has moved there), and it would be good to keep the issue tracking there (and also since this is an important feature request). \r\n\r\nThanks!\r\n  Srinivas", "Awesome- thanks Srinivas! I opened the issue over at https://github.com/tensorflow/probability/issues/8, and will close this one out."]}, {"number": 18002, "title": "Branch 190479555", "body": "", "comments": []}, {"number": 18001, "title": "R1.1", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 18000, "title": "Android NDK r16 support ", "body": "While building TensorFlow with Android NDK r16, it failed finding system headers, such as `stdlib.h`\r\n\r\nFull error log:\r\n\r\n```\r\n                                                             ^\r\nIn file included from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/cstdlib:72:0,\r\n                 from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/bits/stl_algo.h:59,\r\n                 from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/algorithm:62,\r\n                 from ./google/protobuf/stubs/common.h:38,\r\n                 from ./google/protobuf/stubs/atomicops.h:59,\r\n                 from google/protobuf/stubs/atomicops_internals_x86_msvc.cc:37:\r\n/wd/android-ndk-r16//sources/android/support/include/stdlib.h:32:25: fatal error: stdlib.h: No such file or directory\r\n #include_next <stdlib.h>\r\n                         ^\r\ncompilation terminated.\r\nIn file included from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/cwchar:44:0,\r\n                 from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/bits/postypes.h:40,\r\n                 from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/bits/char_traits.h:40,\r\n                 from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/string:40,\r\n                 from ./google/protobuf/stubs/bytestream.h:54,\r\n                 from google/protobuf/stubs/bytestream.cc:31:\r\n/wd/android-ndk-r16//sources/android/support/include/wchar.h:32:24: fatal error: wchar.h: No such file or directory\r\n #include_next <wchar.h>\r\n                        ^\r\ncompilation terminated.\r\nIn file included from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/cstdlib:72:0,\r\n                 from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/bits/stl_algo.h:59,\r\n                 from /wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/algorithm:62,\r\n                 from ./google/protobuf/stubs/common.h:38,\r\n                 from ./google/protobuf/message_lite.h:43,\r\n                 from google/protobuf/stubs/common.cc:33:\r\n/wd/android-ndk-r16//sources/android/support/include/stdlib.h:32:25: fatal error: stdlib.h: No such file or directory\r\n #include_next <stdlib.h>\r\n                         ^\r\ncompilation terminated.\r\nIn file included from google/protobuf/stubs/atomicops_internals_x86_gcc.cc:34:0:\r\n/wd/android-ndk-r16//sources/cxx-stl/gnu-libstdc++/4.9/include/cstring:42:20: fatal error: string.h: No such file or directory\r\n #include <string.h>\r\n                    ^\r\nmake[3]: *** [google/protobuf/stubs/common.lo] Error 1\r\ncompilation terminated.\r\nmake[3]: *** Waiting for unfinished jobs....\r\nmake[3]: *** [google/protobuf/stubs/atomicops_internals_x86_gcc.lo] Error 1\r\nmake[3]: *** [google/protobuf/stubs/atomicops_internals_x86_msvc.lo] Error 1\r\nmake[3]: *** [google/protobuf/stubs/bytestream.lo] Error 1\r\nmake[3]: Leaving directory `/wd/tensorflow_0314/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src'\r\nmake[2]: *** [all] Error 2\r\nmake[2]: Leaving directory `/wd/tensorflow_0314/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src'\r\nmake[1]: *** [all-recursive] Error 1\r\nmake[1]: Leaving directory `/wd/tensorflow_0314/tensorflow/tensorflow/contrib/makefile/downloads/protobuf'\r\nmake: *** [all] Error 2\r\n```", "comments": ["They moved the paths in this build so someone has to go in and fix the include paths. When are we moving to CMake?", "Can you please confirm if this is still an open issue. There is another github issue tracking this which has been closed. Here is the link to that one\r\nhttps://github.com/bazelbuild/bazel/issues/4068", "As @anitha-v says, this is should already be fixed.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17999, "title": "Problem installing demo app on Android Studio because of Bazel", "body": "Hi I am trying to build the demo project on my linux pc and am getting this error. I opened the project after from the examples folder from the tensor flow library.\r\n\r\nERROR: /home/riya/.cache/bazel/_bazel_riya/e4d251743a4c03f32e2f3f7357bd97d7/external/bazel_tools/tools/android/BUILD:288:1: Executing genrule @bazel_tools//tools/android:no_android_sdk_repository_error failed (Exit 1)\r\nThis build requires an Android SDK. Please add the android_sdk_repository rule to your WORKSPACE.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 84.049s, Critical Path: 0.35s\r\nFAILED: Build did NOT complete successfully\r\n\r\nIn my Android project I am getting the same error as this :\r\nError:Execution failed for task ':buildNativeBazel'.\r\n\r\nA problem occurred starting process 'command '/usr/local/bin/bazel''", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Apologies for the same\r\nHave I written custom code :  I am trying to just run the demo project\r\nOS Platform and Distribution : Linux Ubuntu 16.04\r\nTensorFlow installed from : tensorflow github repository\r\nBazel version : Build label: 0.11.1\r\n", "Are you following all the instructions at:\r\nhttps://www.tensorflow.org/mobile/android_build\r\n\r\nthe error message suggests that either you have not installed the Android SDK or you haven't updated the WORKSPACE file with its location.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17998, "title": "android demo? mask_rcnn_inception_v2_coco?", "body": "\u5728android demo\u4e2d\uff0c\u4f7f\u7528mask_rcnn_inception_v2_coco\u83b7\u53d6\u5230\u7684detection_masks\u5e76\u4e0d\u662f[num_masks, mask_height, mask_width]\uff0c\u8bf7\u95ee\u5982\u4f55\u5728android\u4e2d\u6dfb\u52a0mask?\r\nHave I written custom code: YES\r\nOS Platform and Distribution: Android\r\nTensorFlow installed from: 1.7\r\nTensorFlow version: 1.7\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Please provide the code get output detection_masks with mask rcnn on mobile. Thanks"]}, {"number": 17997, "title": "csv_with_header", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 17996, "title": "Unable to import frozen graph with RMSProp", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:0.5.2\r\n- **GCC/Compiler version (if compiling from source)**:5.3.0\r\n- **CUDA/cuDNN version**:8/5\r\n- **GPU model and memory**:GTX 1080Ti\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n Some errors occured when loading the frozen graph:\r\n\r\n`\r\nValueError: graph_def is invalid at node 'RMSProp/update_InceptionV1/Conv2d_1a_7x7/weights/ApplyRMSProp': Input tensor 'InceptionV1/Conv2d_1a_7x7/weights:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\n`\r\n", "comments": ["Could you share some more details, specifically, steps to reproduce the problem? ", "@asimshankar \r\nThanks!\r\nCommand I get the pb file:\r\n`\r\nbazel-bin/tensorflow/python/tools/freeze_graph --input_graph=model.pbtxt --input_checkpoint=model.ckpt --input_binary=false --output_graph=frozen_model.pb --output_node_names=<more than 1000 nodes>\r\n`\r\nWhen I load the pb file, it occurs such errors as above.", "@PumayHui : Thanks, but I can't reproduce the problem without knowing what `model.pbtxt` is. How did you generate it? Or can you share the existing one? And what are the values of `output_node_names`\r\n\r\nIt's possible there is a bug in freeze-graph, CCing @suharshs to take a look.\r\nInstructions to reproduce will be helpful.\r\n\r\nThanks.", "I should also add: It seems that you're trying to freeze a graph where the path from inputs to outputs involves the RMSPropOptimizer kicking in.\r\n\r\nNote that Optimizers change weights (variables) in a model. While \"freeze_graph\" is meant for \"freezing\" weights, i.e., converting all variables to constants. So, if the computation involves changing weights, the graph cannot be frozen.\r\n\r\nAre you trying to apply \"freeze graph\" to a training step or something?\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17995, "title": "tf.igamma (lower regularized incomplete Gamma function) returns the incorrect derivative", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary (pypi)\r\n- **TensorFlow version (use command below)**: v1.6.0-0-gd2e24b6039\r\n- **Python version**: 3.6\r\n- **Bazel version**: using a precompiled version, not sure\r\n- **CUDA/cuDNN version**: not using a GPU\r\n- **GPU model and memory**: not using a GPU\r\n- **Exact command to reproduce**: `python igamma_test.py`\r\n\r\n### Describe the problem\r\n\r\n`tf.igamma`, which is the lower regularized incomplete Gamma function, returns an incorrect derivative with respect to `a`\r\n\r\nThis is probably very low down on the list of things to fix, but I wanted to highlight it since I spent something like 5 hours trying to understand why my model wasn't converging. I was fitting a Gamma distribution, and deep into the code it turns out that `tf.igamma` doesn't return the right derivative.\r\n\r\nI suspect the derivative wrt `a` isn't supported, but I would have much rather seen an exception being thrown. \r\n\r\nMy workaround ended up being not fitting `a` with gradient descent, but instead just perturbing it by epsilon (luckily I only had one single value that I tried to fit)\r\n\r\nFiling this issue mostly in the hope that anyone in the future doesn't waste the same amount of time that I spent.\r\n\r\n### Source code / logs\r\n\r\n```\r\na = tf.placeholder(dtype=tf.float32, shape=[None])\r\nx = tf.placeholder(dtype=tf.float32, shape=[None])\r\ny = tf.igamma(a, x)\r\ny_grad_a = tf.gradients(y, a)  # returns None, should return a tensor\r\ny_grad_x = tf.gradients(y, x)  # returns a tensor\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler I updated the fields", "I was facing the same issue. The derivative wrt a is indeed not implemented: https://github.com/tensorflow/tensorflow/blob/d8f9538ab48e3c677aaf532769d29bc29a05b76e/tensorflow/python/ops/math_grad.py", "Are there any updates on this? This would be really great to have.", "@erikbern,\r\nSorry for the delayed response. Your code works fine with **`Tensorflow Version 1.15.2`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/b2d7b9b7cf5831876a5bc1a6b9482868/gh_17995.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17995\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17995\">No</a>\n"]}, {"number": 17994, "title": "tensorflow-gpu OOM with Geforce GTX980", "body": "Hello there!\r\nSorry for the noob question, I'm new in the world of deep learning, especially with GPU processing.\r\n\r\nI'm trying to run Tensorflow-GPU but probably I'm doing something wrong.\r\nI tried many scripts with always the same result without any solution.\r\nI think I installed Cuda and cuDNN properly, as well tensorflow.\r\n\r\nThis is my system:\r\nUbuntu 16.04\r\nGPU: NVIDIA Geforce GTX 980\r\nRAM: 16 GB\r\nCuda 9.0, cuDNN 7.0.\r\n\r\nconda environment in the example reported below:\r\npython==3.6\r\npathlib==1.0.1\r\nscandir==1.6\r\nh5py==2.7.1\r\nKeras==2.1.2\r\nopencv-python==3.3.0.10\r\ntensorflow-gpu==1.5.0\r\nscikit-image\r\ndlib\r\nface_recognition\r\ntqdm\r\n\r\nand basically this is what I get everytime I try to start some kind of training. In this example faceswap.py training with https://github.com/deepfakes/faceswap.\r\nI can't see any change on nvidia-smi while this is going on, so I think the GPU is not actually used.\r\n\r\nThank you in advance for any kind of help, I'm going mad about this :(\r\n\r\n\r\n```\r\nLoading Trainer from Model_Original plugin...\r\nStarting. Press \"Enter\" to stop training and save model\r\n2018-03-26 02:29:53.146441: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-03-26 02:29:53.146998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-03-26 02:29:53.147230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.291\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 2.72GiB\r\n2018-03-26 02:29:53.147248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2018-03-26 02:29:54.731329: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.21GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.788791: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.20GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.832066: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 288.00MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.832100: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.29GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.837129: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 220.50MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:30:04.837415: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 3.00MiB**.  Current allocation summary follows.\r\n2018-03-26 02:30:04.837533: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (256): \tTotal Chunks: 49, Chunks in use: 48. 12.2KiB allocated for chunks. 12.0KiB in use in bin. 504B client-requested in use in bin.\r\n2018-03-26 02:30:04.837564: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (512): \tTotal Chunks: 6, Chunks in use: 6. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 3.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837593: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1024): \tTotal Chunks: 13, Chunks in use: 13. 13.2KiB allocated for chunks. 13.2KiB in use in bin. 13.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837620: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2048): \tTotal Chunks: 12, Chunks in use: 12. 25.5KiB allocated for chunks. 25.5KiB in use in bin. 24.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837646: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4096): \tTotal Chunks: 17, Chunks in use: 17. 68.0KiB allocated for chunks. 68.0KiB in use in bin. 68.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837673: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8192): \tTotal Chunks: 6, Chunks in use: 6. 48.0KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837700: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16384): \tTotal Chunks: 9, Chunks in use: 8. 168.8KiB allocated for chunks. 150.0KiB in use in bin. 150.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837727: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (32768): \tTotal Chunks: 6, Chunks in use: 6. 225.0KiB allocated for chunks. 225.0KiB in use in bin. 225.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837755: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837779: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.837817: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 256.0KiB allocated for chunks. 256.0KiB in use in bin. 256.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837854: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 597.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.837892: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1048576): \tTotal Chunks: 6, Chunks in use: 6. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837931: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2097152): \tTotal Chunks: 8, Chunks in use: 8. 23.62MiB allocated for chunks. 23.62MiB in use in bin. 22.75MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837971: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4194304): \tTotal Chunks: 13, Chunks in use: 13. 58.25MiB allocated for chunks. 58.25MiB in use in bin. 53.62MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838012: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8388608): \tTotal Chunks: 12, Chunks in use: 12. 123.00MiB allocated for chunks. 123.00MiB in use in bin. 123.00MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838046: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16777216): \tTotal Chunks: 13, Chunks in use: 13. 222.00MiB allocated for chunks. 222.00MiB in use in bin. 222.00MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838075: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (33554432): \tTotal Chunks: 11, Chunks in use: 11. 461.50MiB allocated for chunks. 461.50MiB in use in bin. 442.00MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838100: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (67108864): \tTotal Chunks: 23, Chunks in use: 23. 1.50GiB allocated for chunks. 1.50GiB in use in bin. 1.47GiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838124: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.838147: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.838172: I tensorflow/core/common_runtime/bfc_allocator.cc:644] Bin for 3.00MiB was 2.00MiB, Chunk State: \r\n2018-03-26 02:30:04.838203: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0000 of size 1280\r\n2018-03-26 02:30:04.838229: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0500 of size 256\r\n2018-03-26 02:30:04.838254: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0600 of size 256\r\n[...]\r\n2018-03-26 02:30:04.842321: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7949c2300 of size 67108864\r\n2018-03-26 02:30:04.842337: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7989c2300 of size 67108864\r\n2018-03-26 02:30:04.842353: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79c9c2300 of size 3145728\r\n2018-03-26 02:30:04.842371: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79ccc2300 of size 79944960\r\n2018-03-26 02:30:04.842390: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ed32200 of size 19200\r\n2018-03-26 02:30:04.842407: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72edaed00 of size 611328\r\n2018-03-26 02:30:04.842423: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ee45700 of size 256\r\n2018-03-26 02:30:04.842442: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size: \r\n2018-03-26 02:30:04.842469: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 48 Chunks of size 256 totalling 12.0KiB\r\n2018-03-26 02:30:04.842490: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 512 totalling 3.0KiB\r\n2018-03-26 02:30:04.842509: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 12 Chunks of size 1024 totalling 12.0KiB\r\n2018-03-26 02:30:04.842527: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.2KiB\r\n2018-03-26 02:30:04.842547: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 11 Chunks of size 2048 totalling 22.0KiB\r\n2018-03-26 02:30:04.842565: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 3584 totalling 3.5KiB\r\n2018-03-26 02:30:04.842585: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 17 Chunks of size 4096 totalling 68.0KiB\r\n2018-03-26 02:30:04.842604: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 8192 totalling 48.0KiB\r\n[...]\r\n2018-03-26 02:30:04.843025: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 7 Chunks of size 75497472 totalling 504.00MiB\r\n2018-03-26 02:30:04.843044: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 79944960 totalling 76.24MiB\r\n2018-03-26 02:30:04.843062: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 2.38GiB\r\n2018-03-26 02:30:04.843086: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: \r\nLimit:                  2555510784\r\nInUse:                  2554880000\r\nMaxInUse:               2554880256\r\nNumAllocs:                     303\r\nMaxAllocSize:            364109056\r\n\r\n2018-03-26 02:30:04.843151: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ****************************************************************************************************\r\n2018-03-26 02:30:04.843199: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: **OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc**\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 181, in processThread\r\n    raise e\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 161, in processThread\r\n    trainer.train_one_step(epoch, self.show if (save_iteration or self.save_now) else None)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Trainer.py\", line 26, in train_one_step\r\n    loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/training.py\", line 1839, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2357, in __call__\r\n    **self.session_kwargs)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'model_2/conv2d_9/convolution', defined at:\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 884, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 147, in processThread\r\n    model = PluginLoader.get_model(trainer)(get_folder(self.arguments.model_dir), self.arguments.gpus)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/AutoEncoder.py\", line 16, in __init__\r\n    self.initModel()\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Model.py\", line 22, in initModel\r\n    self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2061, in call\r\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2212, in run_internal_graph\r\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 164, in call\r\n    dilation_rate=self.dilation_rate)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3195, in conv2d\r\n    data_format=tf_data_format)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 754, in convolution\r\n    return op(input, filter)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 838, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 502, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 190, in __call__\r\n    name=self.name)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 639, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n[[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n[[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n```", "comments": ["I'm not sure, but I think that Tensorflow 1.5 is not compatible with CUDA 9.0, cuDNN 7.0. Try using Tensorflow 1.6.", "What's your network like? And what is your input tensor dimensions?", "You are running out of memory.", "thank you guys for the support,\r\n\r\nI tried many configurations of my environment, the one posted here is the one suggested in the requirements-gpu-python36-cuda9.txt, but I alsto tried tensorflow 1.7 (from source), 1.6 (installed from pip), and conda too, with no different results.\r\n\r\nI also tried:\r\ndifferent versions of tensorflow-gpu, installed from pip, from conda, and from source code\r\ndifferent versions of python (Anaconda)\r\nreducing batch size until 2\r\ndifferent versions of cuda / cudnn\r\nuninstall/reinstall nvidia cuda and cudnn and drivers\r\nreinstall Ubuntu\r\n\r\ntried to run many codes found on github and many tutorials, but I always get this kind of OOM issue.\r\n\r\nwhile training my nvidia-smi always looks like:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980     Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   46C    P0    58W / 196W |   3868MiB /  4040MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1136      G   /usr/lib/xorg/Xorg                           640MiB |\r\n|    0      1781      G   /opt/teamviewer/tv_bin/TeamViewer             17MiB |\r\n|    0      1958      G   compiz                                       456MiB |\r\n|    0      2391      G   ...-token=DAF6E1B7D2D5B5E9DFDF4E432544C3CF    63MiB |\r\n|    0     17774      C   python                                      2672MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nCan it be an hardware problem (maybe I'm asking to much and I need a more powerful GPU) or I'm missing something else?", "Looks like you're just out of memory.\r\n\r\nFirst step I'd try would be to turn off all other processes (/usr/lib/xorg/Xorg, compiz, in particular), as they are taking a non-trivial amount of your GPU memory.", "Unfortunately, this seems to work as expected. Memory is finite. Closing for now.", "@Hunterwolf88 Can you let me know how u resolved this issue ?. I am also facing this kind of problem. ", "@goutham-nekkalapu The problem was (for the GTX 980) an actual OOM, although I initially thought it was a computational issue due to some misconfiguration in my Python environment or in the Training script, seeing the GPU unused during all the process.. In some cases you can set a different network or reduce the batch size and get it working. I ended with replacing the GPU.", "@Hunterwolf88 Thanks for the reply. I found my problem, it was with my input length and batch size.", "glad it helped :)", "cpu  model i3 or much more thene\r\n"]}, {"number": 17993, "title": "deprecation warnings in contrib.graph_editor", "body": "Calling the method `tf.contrib.graph_editor.graph_replace` like\r\n```\r\nC = tf.contrib.graph_editor.graph_replace(C, {A: B})\r\n```\r\nreturns a ton of warnings of the type:\r\n```\r\nWARNING:tensorflow:Operation._node_def is private, use Operation.node_def instead. Operation._node_def will eventually be removed.\r\nWARNING:tensorflow:Operation._op_def is private, use Operation.op_def instead. Operation._op_def will eventually be removed.\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n```\r\nand later a ton of warnings about `_control_input`\r\n```\r\nWARNING:tensorflow:Operation._control_inputs is private, use Operation.control_inputs instead. Operation._control_inputs will eventually be removed.\r\nWARNING:tensorflow:Operation._control_inputs is private, use Operation.control_inputs instead. Operation._control_inputs will eventually be removed.\r\n```\r\nI'm guessing this doesn't actually require too many changes but is mostly a question of updating a few function calls to use the suggested names?\r\n\r\nThe error originally came up in 1.5, as can be seen from the system info extract below, but I also just tried it in 1.6. From inspecting the source, I also do not think it has been fixed in 1.7. \r\n\r\nedit: okay, some of the issues seem to have been fixed in the master branch, is this verifiable?\r\n\r\n\r\n------------------------\r\n== cat /etc/issue ===============================================\r\nLinux rabo-Latitude-E7450 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux rabo-Latitude-E7450 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named 'tensorflow'\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_issue_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rabo-Latitude-E7450 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux rabo-Latitude-E7450 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.1)\r\ntensorflow (1.5.0)\r\ntensorflow-tensorboard (1.5.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.5.0\r\ntf.GIT_VERSION = b'v1.5.0-1846-ga58f26d'\r\ntf.COMPILER_VERSION = b'v1.5.0-1846-ga58f26d'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_issue_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/MATLAB/R2015b/bin/glnxa64/libcudart.so.7.0.28\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Reassigning to contrib package owner @purpledog. This is an issue pertaining to deprecation warnings and logging.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "fixed in the master branch"]}, {"number": 17992, "title": "contrib/factorization: minor spelling tweaks", "body": "", "comments": ["Test failure is unrelated. Merging PR.\r\n\r\nThanks, @brettkoonce "]}, {"number": 17991, "title": "Tensorflow sequence mask without reducing dimensions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.4.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: not compiled from source\r\n- **GCC/Compiler version (if compiling from source)**: not compiled from source\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: 8GB x 4 GTX 1080\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\nI have a sample code to run:\r\n    \r\n    import numpy a np\r\n    import tensorflow as tf\r\n    import tensorflow.contrib.eager as tfe\r\n    tfe.enable_eager_execution()\r\n    \r\n    x = np.random.randint(100,size=(4,4))\r\n    indexes =tf.sequence_mask([1,2,2,4],4)\r\n    \"\"\"\r\n    indexes = [\r\n    \t[True,False,False,False],\r\n    \t[True,True,False,False],\r\n    \t[True,True,False,False],\r\n    \t[True,True,True,True],\r\n    ]\r\n    \"\"\"\r\n    \r\n    y = tf.boolean_mask(x,indexes)\r\n    # y = array([43, 78, 68, 54, 46, 28, 15, 52,  3])\r\n\r\nNow, I don't want this as the spatial information of the original tensor is lost and I want to keep the shape intact. How can do that in tensorflow since I work with RNN data so my tensor size is = `[batch_size, max_time, feature_length]` where I would slice it such that:\r\n\r\n`indexes = tf.sequence_mask([x_1, x_2, x_3, ..., x_batch_size], max_time)`\r\n\r\nbut still want to keep the shape intact. If its not possible, is there a way to sequence mask on multiple tensors of such size while also concatenating them so that only the extracted sequence would remain and not the masked out paddings? Paddings could be applied on the end of the concatenation.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@rohan100jain Thanks for the response, I have an another query related to tensorflow if you're quick enough to sort it out for me...I can ask in this thread instead of waiting for 10 days to get a response.\r\n\r\nAlso, I don't get much reply on StackOverflow for days so if you're here just to curate and filter on the questions, it's not really helping either of us. \r\n\r\nFor the record, I HAVE asked it on StackOverflow on the same day, and here's the link, with zero responses: https://stackoverflow.com/questions/49477417/tensorflow-sequence-mask-without-reducing-dimensions"]}, {"number": 17990, "title": "Fix missing interpretation of document", "body": "Add missing interpretation of the mobile optimizing section of the documentation.", "comments": []}, {"number": 17989, "title": "Updating install_golang.sh - bumping to 1.10", "body": "", "comments": ["@asimshankar the kokoro team handles that for us. They try to keep it as up to date, but acknowledge they sometimes are a bit behind. "]}, {"number": 17988, "title": "make rnn cell build with flexible inputs_shape", "body": "In case someone manually call them and provide a list or tuple.", "comments": ["Nagging Reviewer @ebrevdo: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 30 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 45 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 60 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "No longer accepting commits to 1.6."]}, {"number": 17987, "title": "graph.pb.h missing in cc tutorial", "body": "In tensorflow > cc > tutorials >>graph.pb.h , the code includes the header file `graph.pb.h` . However in the path it specifies (`tensorflow/core/framework/)` there is no such file.\r\n\r\nWhere is this header file located? I could not find it inside the tensorflow master branch.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It's auto generated by the grah.proto file.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/graph.proto"]}, {"number": 17986, "title": "Tensorflow 1.6.0 on ARMv7 import error", "body": "Hi there,\r\n\r\nI compiled Bazel and Tensorflow 1.6.0 on an ARMv7 board (OrangePI Zero 512MB) successfully without any error and built the python wheel, Here are related info : \r\n\r\n### System information\r\n- Have I written custom code : no\r\n- OS Platform and Distribution : Armbian 5.41 (Ubuntu 16.04.3 LTS - Kernel 4.14.18-sunxi)\r\n- TensorFlow installed from : source\r\n- TensorFlow version : 1.6.0\r\n- Bazel version : 0.11.1\r\n- CUDA/cuDNN version : N/A\r\n- GPU model and memory : N/A\r\n- Exact command to reproduce : import tensorflow as tf\r\n- Build command : \r\n`bazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 512,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package`\r\n\r\nIn python when I import tensorflow I get this error : \r\n`\r\nImportError: /home/user/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\r\n`\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Updated the first post", "Hi! I encountered a similar error and I was able to solve it. Check it out here: [https://github.com/tensorflow/tensorflow/issues/17790](url)", "Ok, thanks for the tip I removed concat functions from that file and currently building tensorflow again which on my board will take 2 days :) lets hope this time it works !", "It worked ! actually it didn't take 2 days because it only compiled changed files. thanks a lot !", "@Lexicographical thanks very much!\r\n@rezaxdi closing, thanks to @Lexicographical 's help."]}, {"number": 17985, "title": "name_scope problem of \"tensorflow/python/layers/normalization.py\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64bit\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.7.0-rc1 (GPU version from pip)\r\n- **Python version**:  3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GTX 1080Ti 11GB\r\n- **Exact command to reproduce**: See below\r\n\r\n\r\n### Describe the problem\r\nI found this problem when I am using `tf.layers.batch_normalization()`. For example, the following code:\r\n```python\r\nimport tensorflow as tf\r\n\r\ninData = tf.placeholder(shape = [1,1], name = 'input', dtype = tf.float32) \r\nwith tf.name_scope('layer1'):\r\n    with tf.name_scope('fully_connected'):\r\n        W = tf.Variable( [[0.]] ,name='W')\r\n        b = tf.Variable( [0.], name='bias')\r\n        x = tf.matmul( inData, W ) + b\r\n\r\n    normalized = tf.layers.batch_normalization(x, fused = True, reuse = False)\r\n    \r\nloss = tf.reduce_sum(normalized)\r\n\r\nwith tf.Session() as sess:\r\n    # record computation graph\r\n    writer = tf.summary.FileWriter('BN_test', sess.graph)\r\n    writer.close()\r\n```\r\nWill generate the following computation graph:\r\n![bn_problem](https://user-images.githubusercontent.com/8580553/37874558-ae939870-3063-11e8-82f1-72349b32f369.png)\r\n\r\nWhile what I expect is: the variables \"gamma\", \"beta\", \"moving_mean\" etc. should all inside the name_scope \"layer1/batch_normalization/\" but not \"batch_normalization/\"\r\n\r\nSo I  refer to the code \"[tensorflow/python/layers/normalization.py](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/layers/normalization.py)\"\r\nAround line 301 ~ 361, I found:\r\n![image](https://user-images.githubusercontent.com/8580553/37874572-041a6dbe-3064-11e8-98b7-860e7dc157eb.png)\r\n\r\nIt seems that here the name_scope was cleaned in order to support the 'reuse' option. But this should only affect \"moving_mean\" and \"moving_variance\", not \"beta\" and \"gamma\". However, as shown above, all of them was affected.\r\n In fact, if we add `` print(self._scope.partitioner) `` just before line 302, we will found that self._scope.partitioner is always ``None``.\r\n\r\n### Summary\r\n1. self._scope is probably not set correctly.\r\n2. Suggestion: I want an option to tell the program \"I will never reuse the weights\", thus do not clean the name_scope of \"moving_mean\" and \"moving_variance\". Otherwise, the graph in tensorboard is too messy......\r\n", "comments": ["Is there anyone can solve this problem?", "@fchollet could you please take a look?", "Thank you for the bug report. I believe the issue is resolved for layers from `tf.keras.layers` at HEAD. You could try with `tf.keras.layers.BatchNormalization` with the nightly release.\r\n\r\nUnfortunately we cannot touch the variable names of the of layers from `tf.layers` since this would affect existing checkpoints. But going forward we're recommending the usage of `tf.keras.layers` instead, for consolidation.", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 17984, "title": "Tensorflow Lite not support Siamese network with parameter sharing", "body": "Hi,\r\n\r\nI found that the Tensorflow Lite did not support Siamese network with parameter sharing of conv2d layers and batch normalization layers yet. (e.g. EnsureBiasVectors will fail to deal with different names of conv output layer with the same kernel.) Is there any plan to support it?\r\n\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi, sorry for that. And I found the problems were caused by the transformation (e.g. removing of original kernel of convolution after transformation) of operators. \r\n\r\nHave I written custom code\r\nNo\r\n\r\nOS Platform and Distribution\r\nMac OS\r\n\r\nTensorFlow installed from\r\nCompilation from github\r\n\r\nTensorFlow version\r\nr1.6\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\nCPU only\r\n\r\nGPU model and memory\r\nN/A\r\n\r\nExact command to reproduce\r\nbazel-bin/tensorflow/contrib/lite/toco/toco", "@guohengkai, it's difficult to help you without a reproducible test case. Could you provide one?", "@aselle Sorry for the late response. The following codes will save the model into \"test.pbtxt\", which will cause this problem.\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    # Input\r\n    input_placeholder = tf.placeholder(tf.float32, shape=(1, 20, 20, 2))\r\n    input_split_1, input_split_2 = tf.split(input_placeholder, 2, axis=3)\r\n\r\n    # Stream 1\r\n    conv1 = tf.layers.conv2d(input_split_1, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv1')\r\n    bn1 = tf.layers.batch_normalization(conv1, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn1')\r\n    relu1 = tf.nn.relu(bn1)\r\n    conv2 = tf.layers.conv2d(relu1, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv2')\r\n    bn2 = tf.layers.batch_normalization(conv2, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn2')\r\n    relu2 = tf.nn.relu(bn2)\r\n\r\n    # Stream 2\r\n    conv1_2 = tf.layers.conv2d(input_split_2, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv1',\r\n                            reuse=True)\r\n    bn1_2 = tf.layers.batch_normalization(conv1_2, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn1',\r\n                                        reuse=True)\r\n    relu1_2 = tf.nn.relu(bn1_2)\r\n    conv2_2 = tf.layers.conv2d(relu1_2, 8, kernel_size=3, strides=1,\r\n                            padding='SAME', activation=None, name='conv2',\r\n                            reuse=True)\r\n    bn2_2 = tf.layers.batch_normalization(conv2_2, axis=3, momentum=0.997,\r\n                                        epsilon=1e-5, center=True, scale=True,\r\n                                        training=True, fused=True, name='bn2',\r\n                                        reuse=True)\r\n    relu2_2 = tf.nn.relu(bn2_2)\r\n\r\n    # Final\r\n    concat = tf.concat(axis=3, values=[relu2, relu2_2])\r\n    flatten = tf.layers.flatten(concat)\r\n    fc = tf.layers.dense(flatten, 200)\r\n\r\n    tf.train.write_graph(graph.as_graph_def(), '.', 'test.pbtxt')\r\n```", "It looks like you will have to freeze your graph. Does https://www.tensorflow.org/extend/tool_developers/#freezing help?", "@andrehentz Thanks for your kind response. Indeed I have frozen my graph. The codes I provided are just for network structure. ", "We will try this, can you please share the error message if you have it handy ?", "@anitha-v Thanks for your response. I do not have any error message handy now. Sorry for that. The errors are something like dimension inconsistent with conv due to the trimming operation. But I think it's easy to reproduce.", "I'll take a look when I can. You may be able to work around this, by creating a new network that duplicates the weights.", "@aselle OK. Thanks. ", "Have you created an eval graph? Your example seems to use batch normalization with training. Can you try with Training=False?", "@raghuraman-k Hi, the provided example is just a demo to show the network structure I used because my codes are more complicated. I have converted it into an eval graph.", "Nagging Assignee @aselle: It has been 109 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi there! I'll close this issue for now. Please open a separate issue if you are still encountering this problem with the latest code."]}, {"number": 17983, "title": "OpKernel   for unknown op: HashTable  And   incompatible with expected float_ref", "body": "OS Platform : Cestos7\r\npython:2.7\r\ngcc:4.8\r\nTensorFlow installed from source \r\ntf version :  ('v1.4.0-19-ga52c8d9', '1.4.1')\r\nBazel version:N/A\r\nCUDA Version 8.0.61\r\nCUDNN_MAJOR  : 6\r\nGPU model and memory: 22912MiB*4\r\nExact command to reproduce:\r\n\r\nfreeze:CUDA_VISIBLE_DEVICES=\"1\" python freeze_graph.py --checkpoint_dir='./models/dpner/checkpoints/' --graph_pb='./models/dpner/checkpoints/graph.pb' --output_node_names='forward/unary_scores/shape,forward/unary_scores,loss/transitions,loss/transitions/Assign,loss/transitions/read' --output_dir='./models'\r\n\r\n-----------------------------------------------\r\n\r\n2018-03-25 07:49:05.497590: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LeftShift\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: LeftShift\r\n2018-03-25 07:49:05.497623: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"LookupTableInsert\" device_type: \"CPU\"') for unknown op: LookupTableInsert\r\n2018-03-25 07:49:05.497679: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"HashTable\" device_type: \"CPU\" constraint { name: \"key_dtype\" allowed_values { list { type: DT_INT32 } } } constraint { name: \"value_dtype\" allowed_values { list { type: DT_INT32 } } }') for unknown op: HashTable\r\n\r\n\r\n---------------------------------------------\r\n\r\n\r\nInvalid argument: Input 0 of node loss/transitions/Assign was passed float from loss/transitions:0 incompatible with expected float_ref.\r\n\r\n\r\n---------------------------------------------\r\n\r\n**In my model**: \r\n\r\nall variable is similar\r\n\r\n`with tf.variable_scope(\"softmax\") as scope:\r\n            self.W = tf.get_variable(\r\n                shape=[hidden_dim * 2, num_classes], \r\n                initializer=tf.truncated_normal_initializer(stddev=0.01), \r\n                name=\"weights\",\r\n                regularizer=tf.contrib.layers.l2_regularizer(self.l2_reg_lambda))\r\n\r\nsaver = tf.train.Saver(tf.global_variables())\r\n\r\nsaver.save(sess, checkpoint_prefix, global_step=current_step)\r\n`\r\n\r\n**In freeze**, \r\n[freeze_graph.py.log](https://github.com/tensorflow/tensorflow/files/1845141/freeze_graph.py.log)\r\n\r\nand _**in c++**_ is :\r\n`\r\nSession* session;\r\n\r\nStatus status = NewSession(SessionOptions(), &session);\r\n\r\nGraphDef graph_def;\r\n\r\nstatus = ReadBinaryProto(Env::Default(),  path, &graph_def);\r\n\r\nstatus = session->Create(graph_def);\r\n`\r\n\r\n\r\n----------------------------------------------------\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler     have updated"]}, {"number": 17982, "title": "Add parallel implementation of CTC greedy decoder", "body": "Used work_sharder to shard the computation of each batch according to #17136", "comments": []}, {"number": 17981, "title": "AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'leaky_relu'", "body": "Using https://github.com/hizhangp/yolo_tensorflow I get the following error. How should I fix it?\r\n```\r\n[jalal@goku yolo_tensorflow]$ /scratch/sjn/anaconda/bin/python train.py \r\nTraceback (most recent call last):\r\n  File \"train.py\", line 164, in <module>\r\n    main()\r\n  File \"train.py\", line 151, in main\r\n    yolo = YOLONet()\r\n  File \"/scratch2/body_pose/yolo_tensorflow/yolo/yolo_net.py\", line 41, in __init__\r\n    is_training=is_training)\r\n  File \"/scratch2/body_pose/yolo_tensorflow/yolo/yolo_net.py\", line 69, in build_network\r\n    net, 64, 7, 2, padding='VALID', scope='conv_2')\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/scratch/sjn/anaconda/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1039, in convolution\r\n    outputs = activation_fn(outputs)\r\n  File \"/scratch2/body_pose/yolo_tensorflow/yolo/yolo_net.py\", line 244, in op\r\n    return tf.nn.leaky_relu(inputs, alpha=alpha, name='leaky_relu')\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'leaky_relu'\r\n\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have you filed an issue with the upstream project?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please file an issue with the upstream project, which might need to upgrade to TF's latest APIs.", "TensorFlow version: '1.3.0'\r\n\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'leaky_relu'"]}, {"number": 17979, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid JPEG data or crop window, data size 15022 ", "body": "On the terminal window, it runs fine when an image is passed to tensorflow for image object recognition using:\r\n\r\n`python run.py http://image_url.jpg`\r\n\r\nHowever, with JSON data that contains stream of imageURL, it failed with the following main error:\r\n\r\n    InvalidArgumentError: Invalid JPEG data or crop window, data size 15022\r\n\t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DecodeJpeg/contents_0_0)]]\r\n    Caused by op u'DecodeJpeg'\r\n\r\nAnother error encountered:\r\n\r\n`ValueError: GraphDef cannot be larger than 2GB.`\r\n\r\n\r\nBelow is my tensorflow source code as a function(again it runs with single ImageUrl passed as parameter):\r\n\r\n    import tensorflow as tf\r\n    import sys\r\n    import os\r\n    import urllib2\r\n    \r\n    def tensorflow_pred(imageUrl):\r\n    \r\n        #suppress TF log-info messages - remove to display TF logs \r\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n    \r\n        response = urllib2.urlopen(imageUrl)\r\n    \r\n        image_data = response.read()\r\n    \r\n        # Loads label file, strips off carriage return\r\n        label_lines = [line.rstrip() for line \r\n                        in tf.gfile.GFile(\"./retrained_labels.txt\")]\r\n    \r\n        # Unpersists graph from file\r\n        with tf.gfile.FastGFile(\"./retrained_graph.pb\", 'rb') as f:\r\n            graph_def = tf.GraphDef()\r\n            graph_def.ParseFromString(f.read())\r\n            _ = tf.import_graph_def(graph_def, name='')\r\n    \r\n        with tf.Session() as sess:\r\n            # Feed the image_data as input to the graph and get first prediction\r\n            softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\r\n            \r\n            predictions = sess.run(softmax_tensor, \\\r\n                    {'DecodeJpeg/contents:0': image_data})\r\n            \r\n            # Sort to show labels of first prediction in order of confidence\r\n            top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\r\n            \r\n            for node_id in top_k:\r\n                classification = label_lines[node_id]\r\n                score = predictions[0][node_id]\r\n                if (score >=0.5):\r\n                    return ('%s (score = %.5f)' % (classification, score))\r\n            ", "comments": ["What is the cause of this error? I was trying to have image object classification while streaming JSON data.", "I hope this problem would be ultimately solved in the long run. Meanwhile, I have had a workaround. For the main error, which is the InvalidArgumentError, I handled it in a try-catch block. This error often results from getting a wrong file format and even for a JPEG/JPG  which is derived from some corrupt file such as conversion from PNG to JPEG - so just think of it as a JPEG file that is not truly JPEG. Perhaps, having Tensorflow support other file types would finally nail this bug. The second error occurs when there is a real-time stream of data which at some point causes the `GraphDef to exceed its limit of 2GB`. This is handled by recreating the `graph` from the `tf.Graph` using `with graph.as_default()`\r\n\r\nSo here is my proposed solution:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nimport os\r\nimport urllib2\r\n\r\ndef tensorflow_pred(imageUrl):\r\n\r\n    #suppress TF log-info messages - remove to display TF logs \r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\n    response = urllib2.urlopen(imageUrl)\r\n\r\n    image_data = response.read()\r\n\r\n    # Loads label file, strips off carriage return\r\n    label_lines = [line.rstrip() for line \r\n                    in tf.gfile.GFile(\"./retrained_labels.txt\")]\r\n    try:\r\n        \r\n          graph = tf.Graph()\r\n\r\n          with graph.as_default(): \r\n                 # Unpersists graph from file\r\n                  with tf.gfile.FastGFile(\"./retrained_graph.pb\", 'rb') as f:\r\n                  graph_def = tf.GraphDef()\r\n                  graph_def.ParseFromString(f.read())\r\n                _ = tf.import_graph_def(graph_def, name='')\r\n\r\n         with tf.Session() as sess:\r\n              # Feed the image_data as input to the graph and get first prediction\r\n              softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\r\n\r\n              predictions = sess.run(softmax_tensor, \\\r\n                {'DecodeJpeg/contents:0': image_data})\r\n        \r\n            # Sort to show labels of first prediction in order of confidence\r\n             top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\r\n        \r\n         for node_id in top_k:\r\n              classification = label_lines[node_id]\r\n              score = predictions[0][node_id]\r\n              if (score >=0.5):\r\n                return ('%s (score = %.5f)' % (classification, score))\r\n  except tf.errors.InvalidArgumentError:\r\n            print ('Poor image quality, unable to predict')\r\n```\r\n\r\n**Ref:** [Tensorflow.python.framework.errors_impl.InvalidArgumentError](https://stackoverflow.com/questions/49414902/tensorflow-python-framework-errors-impl-invalidargumenterror) also raised by me.", "Men, you save my code. Thanks"]}, {"number": 17977, "title": "OOM after repeatedly evaluating inception scores", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 2.7\r\n\r\n\r\n\r\nHi, I run my generative model and evaluate the inception score of the generated images every now and then and encountered out of memory after a lot of iterations. I don't know what the cause is since I built the graph and just feed some values to it. \r\nMy code goes as follows:\r\n\r\n```\r\nsession=tf.InteractiveSession()\r\n\r\ninception_samples = tf.placeholder(tf.float32, shape=[INCEPTION_BATCHES*BATCH_SIZE, DATA_DIM])\r\n\r\ndef get_inception_score_op(inception_samples=inception_samples):\r\n    inception_samples=tf.reshape(inception_samples,[BATCH_SIZE*INCEPTION_BATCHES,NUM_CHANNELS,HEIGHT,WIDTH])\r\n    inception_samples=tf.transpose(inception_samples,[0,2,3,1])\r\n    return lib.classifier_score.get_inception_scores(\\\r\n    inception_samples,batch_size=BATCH_SIZE*INCEPTION_BATCHES, num_inception_images=BATCH_SIZE)\r\n\r\ninception_score=get_inception_score_op()\r\n\r\ndef get_inception_score():\r\n    all_samples = []\r\n    for i in xrange(INCEPTION_BATCHES):# inception score for num_batches of fake data\r\n        all_samples.append(session.run(fake_sample))     \r\n    all_samples = np.concatenate(all_samples, axis=0)\r\n    return inception_score.eval({inception_samples:all_samples})\r\nfor i in range(ITERATIONS):\r\n    print get_inception_score()\r\n```\r\n\r\nwhere  \"lib.classifier_score.get_inception_scores\" is from the file\r\n[# https://github.com/tensorflow/models/blob/master/research/gan/cifar/util.py](url)\r\nthat looks like this:\r\n```\r\ntfgan = tf.contrib.gan\r\n\r\ndef get_inception_scores(images, batch_size, num_inception_images):\r\n  \"\"\"Get Inception score for some images.\r\n\r\n  Args:\r\n    images: Image minibatch. Shape [batch size, width, height, channels]. Values\r\n      are in [-1, 1].\r\n    batch_size: Python integer. Batch dimension.\r\n    num_inception_images: Number of images to run through Inception at once.\r\n\r\n  Returns:\r\n    Inception scores. Tensor shape is [batch size].\r\n\r\n  Raises:\r\n    ValueError: If `batch_size` is incompatible with the first dimension of\r\n      `images`.\r\n    ValueError: If `batch_size` isn't divisible by `num_inception_images`.\r\n  \"\"\"\r\n  # Validate inputs.\r\n  assert images.shape[-1]==3\r\n  tf.TensorShape(images.shape)[0:1].assert_is_compatible_with([batch_size])\r\n  if batch_size % num_inception_images != 0:\r\n    raise ValueError(\r\n        '`batch_size` must be divisible by `num_inception_images`.')\r\n\r\n  # Resize images.\r\n  size = 299\r\n  resized_images = tf.image.resize_bilinear(images, [size, size])\r\n\r\n  # Run images through Inception.\r\n  num_batches = batch_size // num_inception_images\r\n  inc_score = tfgan.eval.inception_score(\r\n      resized_images, num_batches=num_batches)\r\n\r\n  return inc_score\r\n```\r\nexcept that I made some minor changes to it.\r\n\r\nI guess `tf.contrib.gan.eval.inception_score()` have been taking up my GPU memory.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 17976, "title": "AttributeError: '_LSTMModel' object has no attribute '_get_exogenous_embedding_shape'", "body": "### System information\r\n**Ubuntu 16.04**\r\n**Tensorflow 1.6**\r\n\r\n### Describe the problem\r\n\r\nWhen I try to execute \r\n\r\n> **tensorflow/tensorflow/contrib/timeseries/examples/lstm.py**\r\n\r\n the next error appears:\r\n\r\n```\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1pp2uy_k\r\nTraceback (most recent call last):\r\n  File \"lstm.py\", line 296, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"lstm.py\", line 284, in main\r\n    all_times, predictions) = train_and_predict()\r\n  File \"lstm.py\", line 219, in train_and_predict\r\n    estimator.train(input_fn=train_input_fn, steps=training_steps)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 314, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 743, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 725, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py\", line 225, in create_estimator_spec\r\n    model=self.model, input_statistics=input_statistics)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/state_management.py\", line 147, in initialize_graph\r\n    self._start_state = model.get_start_state()\r\n  File \"lstm.py\", line 114, in get_start_state\r\n    tf.zeros(self._get_exogenous_embedding_shape(), dtype=self.dtype),\r\nAttributeError: '_LSTMModel' object has no attribute '_get_exogenous_embedding_shape'\r\n```\r\n\r\nSeems that the pull is coming from @gunan, @tensorflower-gardener,  authored by @allenlavoie \r\n\r\n\r\nEDIT: If I hard-code the 'self._get_exogenous_emebedding_shape()' (line 114 in lstm.py file) to 0, I get the next error also:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"lstm.py\", line 296, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"lstm.py\", line 284, in main\r\n    all_times, predictions) = train_and_predict()\r\n  File \"lstm.py\", line 219, in train_and_predict\r\n    estimator.train(input_fn=train_input_fn, steps=training_steps)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 314, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 743, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 725, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py\", line 228, in create_estimator_spec\r\n    return self._train_ops(features)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py\", line 78, in _train_ops\r\n    self.model, features, estimator_lib.ModeKeys.TRAIN)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/state_management.py\", line 93, in define_loss\r\n    model=model, features=features, mode=mode)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/state_management.py\", line 182, in _define_loss_with_saved_state\r\n    mode=mode)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/state_management.py\", line 237, in _update_cached_states\r\n    state=looked_up_state)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/model.py\", line 588, in per_step_batch_loss\r\n    features={key: value for key, value in features.items()\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/timeseries/python/timeseries/model.py\", line 296, in _process_exogenous_features\r\n    trainable=True))\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py\", line 221, in input_from_feature_columns\r\n    cols_to_outs=cols_to_outs)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py\", line 104, in _input_from_feature_columns\r\n    check_feature_columns(feature_columns)\r\n  File \"/home/proto/anaconda3/envs/tensor/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py\", line 776, in check_feature_columns\r\n    key = f.key\r\nAttributeError: '_EmbeddingColumn' object has no attribute 'key'\r\n\r\n\r\n```", "comments": ["Try the version from the 1.6 branch: https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/contrib/timeseries/examples/lstm.py\r\n\r\nOr use a nightly if you're using the example at head."]}, {"number": 17975, "title": "add runOptions and metaGraph into LoadSavedModel", "body": "", "comments": ["Nagging Reviewer : It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17974, "title": "QuantizedConv2D dimension mismatch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch\r\n- **TensorFlow installed from (source or binary)**: binary / source for transform_graph\r\n- **TensorFlow version (use command below)**:  1.6\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: 0.11.1-1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: GTX  1060\r\n- **Exact command to reproduce**: \r\n\r\nI have a frozen model (frozen.pb) and followed the guideline to produce `quantized.pb`.\r\nInference with frozen.pb is ok but with quantized.pb it crashes on `tf.import_graph_def`.\r\n\r\nIf the quantized model expects the same shape of input/output, just replacing frozen.pb with quantized.pb should work.\r\n\r\n- I followed https://www.tensorflow.org/performance/quantization\r\n- quantized with this command:\r\n```\r\n../tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=graph_def/frozen.pb \\\r\n  --out_graph=graph_def/quantized.pb \\\r\n  --inputs=img \\\r\n  --outputs=out1,out2,out3,out4,out5,out6 \\\r\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,3,256,256\")\r\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\r\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\r\n    strip_unused_nodes sort_by_execution_order'\r\n```\r\n- backtrace:\r\n```py\r\nTraceback (Most recent call last):\r\n8    test_tf.py                                                                    <module>                --> detector = Detector()                              \r\n114  /home/user/project/net_tf.py                                             __init__                --> tf.import_graph_def(graph_def, name='')            \r\n432  /usr/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py        new_func                --> return func(*args, **kwargs)                       \r\n663  /usr/lib/python3.6/site-packages/tensorflow/python/framework/importer.py      import_graph_def        --> ops.set_shapes_for_outputs(op)                     \r\n2501 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           set_shapes_for_outputs  --> return _set_shapes_for_outputs(op)                 \r\n2474 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           _set_shapes_for_outputs --> shapes = shape_func(op)                            \r\n2404 /usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py           call_with_requiring     --> return call_cpp_shape_fn(op, require_shape_fn=True)\r\n627  /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py call_cpp_shape_fn       --> require_shape_fn)                                  \r\n691  /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py _call_cpp_shape_fn_impl --> raise ValueError(err.message)                      \r\nValueError: Dimensions must be equal, but are 32 and 64 for 'conv2_1/Conv2D/eightbit' (op: 'QuantizedConv2D') with input shapes: [1,3,?,32], [3,3,64,128], [], [], [], [].\r\n> /usr/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py(691)_call_cpp_shape_fn_impl()\r\n```\r\n\r\n- related model code:\r\n```py\r\n    max_pool = tf.contrib.layers.max_pool2d\r\n\r\n    x = tf.placeholder(tf.float32, shape=[1, 3, None, None], name='img')\r\n\r\n    x = relu(conv2d(x, 64, kernel_size=3, padding='same', data_format='channels_first', name='conv1_1'))\r\n    x = relu(conv2d(x, 64, kernel_size=3, padding='same', data_format='channels_first', name='conv1_2'))\r\n    x = max_pool(x, kernel_size=2, data_format='NCHW')\r\n\r\n    x = relu(conv2d(x, 128, kernel_size=3, padding='same', data_format='channels_first', name='conv2_1'))\r\n    x = relu(conv2d(x, 128, kernel_size=3, padding='same', data_format='channels_first', name='conv2_2'))\r\n    x = max_pool(x, kernel_size=2, data_format='NCHW')\r\n...\r\n```\r\n\r\nThis is fully convolutional, and the channel number goes from 3 to 64 and 128.\r\nSo 32 in the error message comes out of nowhere. (Is [1,3,?,32] a NCHW shape or conv2d kernel shape?)\r\n\r\nCan it be related to NCHW? Somehow max_pool halves the channel number instead of spatial dimensions, then it explains how 32 appears (64/2=32).\r\n", "comments": ["@petewarden Could you take a look at this? ", "I converted my model from NCHW to NHWC, and the error changed.\r\n\r\n```py\r\nTraceback (Most recent call last):\r\nself.sess.run(self.outs, feed_dict={self.img: img})\r\n905  /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py run      --> run_metadata_ptr)\r\n1140 /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py _run     --> feed_dict_tensor, options, run_metadata)                   \r\n1321 /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py _do_run  --> run_metadata)                                              \r\n1340 /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py _do_call --> raise type(e)(node_def, op, message)                       \r\nInvalidArgumentError: requested_output_max must be >= requested_output_min, but got -nan and 0\r\n\t [[Node: mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mul/eightbit, mul/eightbit:1, mul/eightbit:2, mul/eightbit/requant_range, mul/eightbit/requant_range:1)]]\r\n\t [[Node: Relu_17/_27 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_657_Relu_17\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op 'mul/eightbit/requantize', defined at:\r\n  File \"/home/user/...py\", line 108, in __init__\r\n    tf.import_graph_def(graph_def, name='')\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 577, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): requested_output_max must be >= requested_output_min, but got -nan and 0\r\n\t [[Node: mul/eightbit/requantize = Requantize[Tinput=DT_QINT32, out_type=DT_QUINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mul/eightbit, mul/eightbit:1, mul/eightbit:2, mul/eightbit/requant_range, mul/eightbit/requant_range:1)]]\r\n\t [[Node: Relu_17/_27 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_657_Relu_17\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\n> /usr/lib/python3.6/site-packages/tensorflow/python/client/session.py(1340)_do_call()\r\n-> raise type(e)(node_def, op, message)\r\n```", "@elbaro so QuantizedConv2D doesn't support NCHW, or did you figure out how to solve this? I ran into the same problem as well.", "Closed by mistake.", "Actually this does't come with gpu implementation, our net becomes 200x slower after reordering from NCHW to NHWC? Is there any plan to implement on gpu and NCHW in the near future? thank?\r\n", "@zheng-xq Can you comment?", "Hi, here is the patch for [fold_batch_norms.py](https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/contrib/quantize/python/fold_batch_norms.py), it may be useful.\r\n```shell\r\n# diff fold_batch_norms.py{,.bk}\r\n126,127d125\r\n<       new_layer_tensor = _swap_nchw_nhwc(new_layer_tensor, match.layer_op)\r\n<\r\n137,138d134\r\n<       # NHWC back to NCHW\r\n<       bias_add_tensor = _swap_nchw_nhwc(bias_add_tensor, match.layer_op, False)\r\n145,150d140\r\n< def _swap_nchw_nhwc(tensor, op, flag=True):\r\n<   # NCHW to NHWC if flag = True, default\r\n<   # NHWC back to NCHW else\r\n<   if op.type == 'Conv2D' and op.get_attr('data_format') == b'NCHW':\r\n<       tensor = array_ops.transpose(tensor, [0, 2, 3, 1] if flag else [0, 3, 1, 2])\r\n<   return tensor\r\n459,460d448\r\n<   xop = x.op\r\n<   x = _swap_nchw_nhwc(x, xop)\r\n465c453\r\n<   return _swap_nchw_nhwc(dmean_dx + dvar_dx, xop, False), None, None, None, None\r\n---\r\n>   return (dmean_dx + dvar_dx), None, None, None, None\r\n```", "Hi @elbaro! We are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17974\">No</a>\n"]}, {"number": 17973, "title": "Losing Output Shape information of conv2d_transpose layer when importing from .pbtxt file", "body": "- [x] This is a custom code\r\n- [x] I'm on Mac OSX 10.11.16\r\n- [x] TensorFlow version 1.6.0\r\n- [x] Bazel version N/A\r\n- [x]  Tensorflow installed from source\r\n- [x] CUDA/cuDNN version N/A\r\n- [x] GPU model and memory N/A\r\n\r\n- [x] Exact command to reproduce\r\n\r\n\r\n\r\nThe output shape of the node with node. Type `Conv2DBackpropInput` seems to lose height and width information as in this:\r\nThis is denconv layer definition and conv3 layer after it\r\n`deconv = tf.layers.conv2d_transpose(pool2 , filters = 32 ,kernel_size = [2,2],strides=(1, 1) , padding = \"same\")\r\n\r\n\r\nconv3 = tf.layers.conv2d(inputs= deconv,filters=64,kernel_size=[1, 1],padding=\"same\", activation=tf.nn.relu)`\r\n\r\n`graph = tf.get_default_graph()` is built in the notebook\r\n\r\n    for node in graph.get_operations():\r\n      if node.type == \"Conv2DBackpropInput\":\r\n        print(node.type,\"---->>>>>\", \"outputsshape ----\" , node.outputs[0].get_shape())\r\n\r\n**Output:**\r\n \r\n    Conv2DBackpropInput ---->>>>> outputsshape ---- (?, 7, 7, 32)\r\n\r\n\r\nSame graph converted to .pbtxt by:\r\n\r\n    tf.train.write_graph(graph, \"graphx/\" , \"sample_this_1.pbtxt\")\r\n\r\nReading it again from .pbtxt\r\n\r\n    tf.reset_default_graph()\r\n    gram = tf.get_default_graph()\r\n    gram.get_operations()\r\n    from tensorflow.core.framework import graph_pb2\r\n    from google.protobuf import text_format as pbtf\r\n    \r\n    gdef = graph_pb2.GraphDef()\r\n    \r\n    with open('graphx/sample_this_1.pbtxt', 'r') as f:\r\n        graph_str = f.read()\r\n    \r\n    pbtf.Merge(graph_str, gdef)\r\n    \r\n    tf.import_graph_def(gdef)\r\n\r\n\r\nNow doing the same for `gram` I get,\r\n\r\n    for node in gram.get_operations():\r\n      if node.type == \"Conv2DBackpropInput\":\r\n        print(node.type,\"---->>>>>\", \"outputsshape ----\" , node.outputs[0].get_shape())\r\n\r\n**Output:**\r\n\r\n    Conv2DBackpropInput ---->>>>> outputsshape ---- (?, ?, ?, 32)\r\n\r\n\r\nSo, the `outputs --- shape` went from (? ,7 ,7 ,32) to (? , ? , ? ,32) after reading from .pbtxt\r\n\r\nWhy is that? Is it a bug because output_shape is a part of model definition I guess?\r\n\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler done.", "I think this might have something to do with the C_API vs. non C_API stuff. Shape inference tends to be better with the new C API.\r\n\r\nTry setting environment variable TF_C_API_GRAPH_CONSTRUCTION_SHAPES and TF_C_API_GRAPH_CONSTRUCTION to 1 and then see.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}]