[{"number": 16993, "title": "Fix broken link in tfmobile documentation", "body": "This fix fixes the broken link in tfmobile documentation as\r\n`android/` had been moved to `android/tfmobile/` in the repo\r\n`googlecodelabs/tensorflow-for-poets-2`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Re @chluke20an7 friendly pinging about review.", "Nagging Reviewer : It has been 48 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer : It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer : It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 16992, "title": "Android CUDA cherrypicks for 1.6", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 16991, "title": "Branch 185565363", "body": "", "comments": []}, {"number": 16990, "title": "[Feature request] Adding scaffold parameter estimator heads", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen creating custom estimators, using the Heads defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py) reduces an important part of the boilerplate as well as guarantee uniform evaluation against canned estimator instances. However, one key parameter is missing in in `create_estimator_spec` function, which is the `scaffold` that one can pass into an `EstimatorSpec`. One example where that is needed is if you want to initialize a large tensor with a numpy array, for example for loading an embedding file from word2vec. This [StackOverflow question](https://stackoverflow.com/questions/44680769/loading-pre-trained-word2vec-to-initialise-embedding-lookup-in-the-estimator-mod) also describes the issue.\r\n\r\n### Source code / logs\r\nThis is my normal code, currently impossible to use with heads. Actually, it might not be impossible, but this seems to me like the cleaner solution. I have the [tiny PR](https://github.com/eisenjulian/tensorflow/commit/9e6c004f07d2ba5c17d58007092f513b54077198) ready if you think this is a valuable change. Thanks a lot for your time!\r\n\r\n```python\r\ndef model_fn(mode, features, labels, hparams):\r\n  embed_ph = tf.placeholder(\r\n      shape=[hparams.vocab_size, hparams.embedding_size], \r\n      dtype=tf.float32)\r\n  embeddings = tf.Variable(embed_ph)\r\n  # Define your model\r\n  return tf.estimator.EstimatorSpec(\r\n      ..., # normal EstimatorSpec args\r\n      scaffold=tf.train.Scaffold(init_feed_dict={embed_ph: my_embedding_numpy_array})\r\n  )\r\n```\r\n", "comments": ["@martinwicke @ispirmustafa should we mark \"contributions welcome\"?", "Mustafa, this looks correct to me, but your call.", "playing with scaffold is an power user category. for power users I think following is a better approach:\r\nest_spec = head.create_estimator_spec(...)\r\nreturn est_spec._replace(scaffold=...)\r\nI'm in favor of not adding scaffold into head interface.", "@eisenjulian will @ispirmustafa 's suggestion get you, as a power user, going? If so, it's my inclination to close the discussion. But please advise (or close yourself if you're satisfied). ", "Thanks for the suggestion: indeed I found about that approach by digging through the code and got to a working solution \ud83d\udc4d. I can go ahead and close this.\r\n\r\nDo you think there is value in updating the docs on [creating custom estimators](https://www.tensorflow.org/get_started/custom_estimators) regarding this idea? I mean both using `scaffold` for initializing large tensors and the `head` module. I am happy to help with that if you do. Loading pre-trained embeddings seems like a common enough use-case to account for and explain IMO.\r\n\r\nFor some context I'm a Google Developer Expert, working in collaboration with Google Dev Advocates to write a blog post on using estimators for NLP tasks.", "for pre-trained embedding, I think warm_start_from is a better approach. Could you please check it out. it's a new argument we added to Estimator interface.", "Interesting, didn't know about this upcoming change, does this allow initializing with a `numpy` array? Or should we create dummy checkpoints from embeddings `.bin` or `.txt` files in advanced?", "it works on checkpoint files. if you have numpy then scaffold is a better approach.", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this can be closed. For the record, I ended up finding even a better way of loading pre-trained embeddings this by using `tf.contrib.layers.embed_sequence` and passing in a custom initializer.", "Hi,\r\n\r\nI just wanted to thanks @eisenjulian on his suggestion on loading pre-trained embeddings using a custom initializer. You saved me a lot of time!\r\nI didn't expect doing this is that hard with tf estimator."]}, {"number": 16989, "title": "Fix a bug in tf.strided_slice()", "body": "Current implementation modifies `TfLiteNode::builtin_data` every time when a loaded graph is executed. The three masks in params (please see the patch) will continually flipping, and causing the op produce incorrect result every two executions.\r\n\r\nThe bug can be reproduced by `m.Invoke()` twice in most axis-shrinking unit tests from *strided_slice_test.cc*. I am not sure if I should add one extra `m.Invoke()` as it will really look like typo.", "comments": ["Thanks for the contribution."]}, {"number": 16988, "title": "TensorFlow compile from source with GPU support error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master version or 1.6.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:  gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n- **GPU model and memory**: NVIDIA Tesla K80\r\n- **Exact command to reproduce**: bazel build  --config=opt --config=cuda tensorflow/tools/pip_package:build_pip_package --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"\r\n\r\n### Describe the problem\r\nI have tried to compile the master version from source, I've added all the env variables, been through stackoverflow and github issues, nothing works, I think it's a bug.\r\n\r\n### Source code / logs\r\nERROR: /home/ubuntu/work/master/tensorflow/tensorflow/contrib/periodic_resample/BUILD:40:1: Linking of rule '//tensorflow/contrib/periodic_resample:gen_gen_periodic_resample_op_py_py_wrappers_cc' failed (Exit 1)\r\n/usr/bin/ld: warning: libcublas.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\n/usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\n/usr/bin/ld: warning: libcurand.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Speriodic_Uresample_Cgen_Ugen_Uperiodic_Uresample_Uop_Upy_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.0'\r\n\r\nand a lot more going", "comments": ["@ciprianflow have you gotten other TF builds to work from src on this machine, and it's just this particular version that isn't building for you?", "Yes I have, I've actually downgraded to CUDA 8.0 and I managed to successfully build. It seems to be a problem with CUDA 9.0 only.", "I'm experiencing the same issue in very similar circumstances. I've read all the related issues and stack overflow posts I can find, and employed the workaround to no avail.\r\n\r\nI'm trying to build on a machine running `Deep Learning AMI (Ubuntu) Version 8.0 (ami-5d7c5024) in eu-west-1` on AWS. I'm using `python 3.6` and `TensorFlow 1.8.0`. It is using `cuda 9` and `cudnn 7`, which is in the `LD_LIBRARY_PATH`, both at configure and build time.\r\n\r\nThe shared objects which `ld` claims are missing are definitely present, and I have run with and without the workaround (`--action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"`) suggested in this issue and elsewhere.\r\n\r\n@ciprianflow and @cy89 could we reopen?\r\n", "\r\n* UPDATE: Workaround posted below *\r\n\r\nI am also experiencing this issue. I've tried TF v.1.8 and the latest Github master. My system:\r\nLinux Redhat 7.5\r\nCUDA 9.1 -- installed as root user\r\ncuDNN 7.1.3  -- located in my user directory\r\nappropriately set paths in ./configure script, and confirmed by looking at tensorflow-master/.tf_configure.bazelrc\r\n\r\nI have tried with/without the --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" as suggested. Failure occurs in various places (this is a 32-core machine, so there's some parallelism-related non-determinism with which files cause a fault-out first). It gets to about 11,000 of 12,462 tasks.\r\n\r\n\"\"\"\"\"  log with verbose errors \"\"\"\"\r\nERROR: $HOME/tensorflow_source/tensorflow-master/tensorflow/python/BUILD:1568:1: Linking of rule '//tensorflow/python:gen_set_ops_py_wrappers_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd $HOME/.cache/bazel/_bazel_$USER/0adab69598361a8f35bbb0e3835cbfb8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/python/gen_set_ops_py_wrappers_cc '-Wl,-rpath,$ORIGIN/../../_solib_local/_U_S_Stensorflow_Spython_Cgen_Uset_Uops_Upy_Uwrappers_Ucc___Utensorflow' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../_solib_local/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib' -Lbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Uset_Uops_Upy_Uwrappers_Ucc___Utensorflow -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/host/bin/_solib_local/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-z,notext -Wl,-z,notext -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -B/usr/bin/ -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/python/gen_set_ops_py_wrappers_cc-2.params)\r\n/usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Uset_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Spython_Cgen_Uset_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreate@libcudnn.so.7'\r\n\r\n<other files>\r\n\"\"\"\"\"\r\n\r\n\r\n", "I was able to work around this issue by  modifying:\r\nCROSSTOOL_nvcc.tpl\r\n\r\nadding this under toolchain \"local_linux\" (I used my full pathnames.. but I put in $HOME for reference here)\r\n\r\nlinker_flag: \"-Wl,-rpath=$HOME/.local/cuda/lib64\"\r\n\r\nand below that under %host_compiler_includes:\r\ncxx_builtin_include_directory: \"$HOME/.local/cuda/include\"\r\n", "@dan-1d Thanks!", "dan1-d I had modified CROSSTOOL_nvcc.tpl but it did not help. can you post the complete modified cross tool file pls?"]}, {"number": 16987, "title": "fixup 1baac78627: Underlying allocator must be a VisitableAllocator too", "body": "Signed-off-by: Sylvain Gault <sylvain.gault@road-b-score.com>", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "Probably should close this PR in favor of #17000.", "Closing, since #17000 has been merged."]}, {"number": 16986, "title": "ParseSingleExample op is missing from op_def_registry.get_registered_ops()", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Pip install\r\n- **TensorFlow version (use command below)**: cpu 1.5.0\r\n- **Python version**: 3.0\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\n**ParseSingleSample** is available as tf.parse_single_example() method call, but it is not listed in the op_def_registry.get_registered_ops()\r\n\r\nWhich cause freeze_graph.py call to fail with following error:\r\nValueError: No op named ParseSingleExample in defined operations.\r\n\r\nWhen converting mobilenet model with training information.\r\n\r\n", "comments": ["The specialized op for `tf.parse_single_example()` was added in 96f3023b6a8b154c3840776c5feff3e028860a36, which is not part of TensorFlow 1.5.0. It sounds like the mobilenet module that you're trying to freeze was generated by a later version of TensorFlow.  To use that op or freeze a graph using it, you will need to upgrade to TF 1.6.0rc0 or later."]}, {"number": 16985, "title": "undefined symbol: PyUnicodeUCS4_FromString   ", "body": "I tried to install tensorflow cpu version using python 2.7 on Ubuntu16.04 under virtualenv. but when I want to import tensorflow, the error is :\r\n\r\n`(My_python2) yuan@ubuntu:~$ python\r\nPython 2.7.13 (default, Feb 13 2018, 14:17:11) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /home/yuan/Documents/My_python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: PyUnicodeUCS4_FromString\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nPlease help! Many thanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks, Here is the details:\r\nHave I written custom code: \r\n**No**\r\nOS Platform and Distribution: \r\n**Ubuntu 16.04 , 64-bit**\r\nTensorFlow installed from:  \r\n **https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.5.0-cp27-none-linux_x86_64.whl**\r\nTensorFlow version:\r\n**1.5.0**\r\nBazel version: N/A\r\nCUDA/cuDNN version:N/A\r\nGPU model and memory:N/A\r\nExact command to reproduce:\r\n**just follow the steps to install with virtualenv, and :pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.5.0-cp27-none-linux_x86_64.whl**", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16984, "title": "New roadmap", "body": "Revised Roadmap document with draft changes from Feb 2018.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Nice to see the RFC process is on TF\u2019s roadmap. Btw, what would be the preferred migration path for current contrib projects? I\u2019d be curious as owner of one (gdr).", "Mind to change the visibility of `core/distributed_runtime` to public? This will allow external implementation of TF distributed runtime to live outside contrib when coming to v2.0.\r\n\r\nPing @poxvoculi and @mrry."]}, {"number": 16983, "title": "Importing graph with tf.contrib.resampler.resampler fails", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nOSx High Sierra\r\n- **TensorFlow installed from (source or binary)**:\r\npip install\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**: \r\n3.5.4\r\n- **CUDA/cuDNN version**:\r\nCPU\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nSee below\r\n\r\n\r\n### Describe the problem\r\nImporting a graph def with a  `tf.contrib.resampler.resampler` op fails iff `tf.contrib` is not imported first.\r\n\r\nExecute:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef export_model(filename, sess, output_node_names):\r\n    from tensorflow.python.framework import graph_util\r\n    output_graph_def = graph_util.convert_variables_to_constants(sess,\r\n                                                                 sess.graph.as_graph_def(add_shapes=True),\r\n                                                                 output_node_names)\r\n    with tf.gfile.GFile(filename, \"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n        \r\ndef read_frozen_protobuf(path):\r\n    with tf.gfile.FastGFile(str(path), 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        return graph_def\r\n\r\n    \r\ndef export(filename):\r\n    tf.reset_default_graph()\r\n    g = tf.Graph()\r\n    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n        images = tf.placeholder(dtype=tf.float64, shape=[32, 32], name='images')\r\n        points = tf.placeholder(dtype=tf.float64, shape=[32, 2], name='points')\r\n        resampled = tf.contrib.resampler.resampler(images, points, name='resampled')\r\n        output_node_names = ['resampled/Resampler']\r\n        export_model(filename, sess, output_node_names)\r\n        \r\ndef load(filename):\r\n    import numpy as np\r\n    tf.reset_default_graph()\r\n    g = tf.Graph()\r\n    with tf.Session(graph=g, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n        images = np.zeros((32, 32), dtype=np.float64)\r\n        points = np.zeros((32, 2), dtype=np.float64)\r\n        graph_def = read_frozen_protobuf(filename)\r\n        tf.import_graph_def(graph_def, \r\n                            input_map={'images': images,\r\n                                       'points': points},\r\n                            return_elements=['resampled/Resampler:0'])\r\n        \r\n######################################################\r\nfrozen_graph_def = '/tmp/test.frozen'\r\nexport(frozen_graph_def)\r\nload(frozen_graph_def)\r\n```\r\n\r\nThen, in a new interpreter (where the load(..) function is defined), execute:\r\n```\r\nfrozen_graph_def = '/tmp/test.frozen'\r\nload(frozen_graph_def)\r\n```\r\n\r\nThis give the error message:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-64251e160f7d> in <module>()\r\n     42 frozen_graph_def = '/tmp/test.frozen'\r\n     43 # export(frozen_graph_def)\r\n---> 44 load(frozen_graph_def)\r\n\r\n<ipython-input-1-64251e160f7d> in load(filename)\r\n     37                             input_map={'images': images,\r\n     38                                        'points': points},\r\n---> 39                             return_elements=['resampled/Resampler:0'])\r\n     40 \r\n     41 ######################################################################\r\n\r\n/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    314                 'in a future version' if date is None else ('after %s' % date),\r\n    315                 instructions)\r\n--> 316       return func(*args, **kwargs)\r\n    317     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    318                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\n/lib/python3.5/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    539         # Set any default attr values that aren't present.\r\n    540         if node.op not in op_dict:\r\n--> 541           raise ValueError('No op named %s in defined operations.' % node.op)\r\n    542         op_def = op_dict[node.op]\r\n    543         for attr_def in op_def.attr:\r\n\r\nValueError: No op named Resampler in defined operations.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory", "Duplicate from https://github.com/tensorflow/tensorflow/issues/17014"]}, {"number": 16982, "title": "dnnConversionCreate_F32 fails when running TF with optimized MKL", "body": "### System information\r\n- **Have I written custom code**: Yes, (https://github.com/jakubkarczewski/AlexNetF/blob/master/alexnet.py)\r\n- **OS Platform and Distribution**: Linux Centos 7\r\n- **TensorFlow installed from**: compiled from source from https://github.com/tensorflow/tensorflow/releases\r\n- **TensorFlow version**: 1.6.0-rc0\r\n- **Python version**:  2.7\r\n- **Bazel version**: 0.10.0\r\n- **GCC/Compiler version**: stock Centos 7 gcc\r\n- **Compilation command**: ```bazel build --config=mkl --copt=\"-DINTEL_MKL_ML\" --copt=\"-mfma\" --copt=\"-mavx2\" --copt=\"-march=broadwell\" --copt=\"-O3\" -s -c opt //tensorflow/tools/pip_package:build_pip_package;;```\r\n- **Exact command to reproduce**: python alexnet.py --training_epoch=1 --model_version=1 output/\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\n\r\n### Describe the problem\r\nRunning following training with Tensorflow compiled with command specified above results in error: ```2018-02-12 23:40:38.088756: F tensorflow/core/kernels/mkl_lrn_op.cc:595] Check failed: dnnConversionCreate_F32( &convert_input, static_cast<dnnLayout_t>(inimage_shape.GetCurLayout()), lt_internal_input) == E_SUCCESS (-1 vs. 0) ``` as opossed to running without any error and training properly on Tensorflow version available under ```pip install tensorflow```.\r\nFor training data I used Imagenet 60gb dataset (http://www.image-net.org/challenges/LSVRC/2012/) with 1000 classes.\r\nWhat's more - following error can be found when running with Tensorflow from precompiled wheel files for both versions of python. This makes me think that the way I compile TF is not the problem here.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory", "@tensorflowbutler of course, it should be ok now.", "@jakubkarczewski so to clarify, is it the set of MKL-enabling flags that you think breaks your case? I.e., can you please give us the minimally contrasting build command that doesn't fail? ", "Yes, I think the problem lies in those MKL flags. I've tried to compile Tensorflow in the same environment as mentioned above but with command ```bazel build --config=mkl --copt=\u201d-DEIGEN_USE_VML\u201d -c opt //tensorflow/tools/pip_package:\r\nbuild_pip_package``` and got in return ``` ./tensorflow/core/util/mkl_util.h:710] Non-OK-status: Status(error::Code::UNIMPLEMENTED, \"Unimplemented conversion function\") status: Unimplemented: Unimplemented conversion function ```. It was a mistake on my side (haven't passed which MKL I actually wanted to use with ```--copt=\"-DINTEL_MKL_ML\"``` ) but it proves the problem is MKL related. I think the version of TF available on pip is build with ```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package``` so what differs from my command is the MKL flag.", "@jakubkarczewski, one quick thing to double-check: you can run \"python alexnet.py --training_epoch=1 --model_version=1 output/\" without problems on the non-MKL build, right? \r\n", "A colleague suggests: The error appears to be E_INCORRECT_INPUT_PARAMETER.  It sounds to me like it might be a shape mismatch of some kind - so perhaps adding logging to see what the two layouts are?\r\n\r\n(they traced through dnnConversionCreate_F32 to the Intel site which has https://software.intel.com/en-us/mkl-developer-reference-c-dnnconversioncreate, and then in turn found dnnError_t, which has the following cases:\r\n\r\ntypedef enum {\r\n    E_SUCCESS                   =  0,\r\n    E_INCORRECT_INPUT_PARAMETER = -1,\r\n    E_UNEXPECTED_NULL_POINTER   = -2,\r\n    E_MEMORY_ERROR              = -3,\r\n    E_UNSUPPORTED_DIMENSION     = -4,\r\n    E_UNIMPLEMENTED             = -127\r\n} dnnError_t;", "@cy89 yes non-MKL build works just fine, I will follow your advice with adding more logging and will report results as soon as possible. Thanks for help!", "Good luck!", "@cy89 I am having trouble with adding logging info because I can't seem to find the structure of ```dnnLayout_t```. Searching on github (https://github.com/search?p=3&q=_dnnLayout_s&type=Code&utf8=%E2%9C%93) I noticed it is assigned to empty structure which (**I guess**) means that the type of this variable is not strictly defined. Is there any way I can extract and print the layout's dimensions?\r\n\r\nMaybe there is a way to do it from Python level? What I intended to do was to modify the  ```mkl_lrn_op.cc``` file so that it prints the dimensions of layouts to ```stderr``` and then compile the tensorflow with this modification. Although if it's possible to do it from Python script, it would be great ", "@jakubkarczewski I'm sorry, I'm not an MKL expert. I agree that it looks like dnnLayout_t starts off as an empty structure. ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Thank you @jakubkarczewski  for reporting and for going deeper into this debugging. We will reproduce and provide a fix for this. Please stay tuned. ", "@cy89 Can you keep an eye on the resolution of this issue?", "@jakubkarczewski Could you please list the contents of the \"/data/i1k-extracted/train\" (10 lines of the output is fine). I am having difficulty make your code run with my imagenet dataset, so I wonder maybe you are using raw images? ", "@angersson happy to keep an eye out; I think we're mostly relying on Wei for help here.", "@wei-v-wang yes, I am using raw images. Directory `i1k-extracted/train/` contains directories named `0/, 1/, 2/, ..., 999/`. Each directory represents data for one particular class of images. In each one of them there are images in `.JPEG` format. Example name of 1st image from `0/` directory: `n02119789_10007.JPEG ` where `n02119789` stands for some kind of class id and `10007` is (I guess) image id.\r\n\r\nThe listing is as follows:\r\n``` drwxrwxr-x 1002 jkarczew jkarczew 20480 Feb  9 09:52 ./\r\ndrwxrwxr-x    3 jkarczewski jkarczewski  4096 Feb  9 09:51 ../\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 0/\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 1/\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 10/\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 100/\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 101/\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 102/\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 103/\r\ndrwxrwxr-x    2 jkarczewski jkarczewski  4096 Feb  9 09:52 104/\r\n```\r\n\r\nDon't mind the `4096` size - I listed the `i1k-extracted/val/` because they have identical subdirectories and I don't have the `train/` now on my laptop.", "Ah I see. Thank you for the info. Looks like maybe each year the imagenet are named differently -- e.g. my raw images had n02119789 being the directory names and yours are numbers like 0,1,10,... \r\nI will work on preparing identical dataset to yours. Thanks for the info!", "I might have used a preprocessed ImageNet dataset because I got it from a friend of mine. I bet he used mapping for subdirectories such as: https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57", "@jakubkarczewski Please know that I was able to get the preprocessed ImageNet dataset identical to yours and was able to reproduce the error (e.g. use --config=mkl, by default MKL-DNN) as well as --config=mkl --copt=\"-DINTEL_MKL_ML\" (MKL_ML). \r\n2018-04-05 23:28:14.468026: F ./tensorflow/core/util/mkl_util.h:710] Non-OK-status: Status(error::Code::UNIMPLEMENTED, \"Unimplemented conversion function\") status: Unimplemented: Unimplemented conversion function\r\nIt seems using MKL_ML our error message differs a little bit -- maybe it was caused by the commit difference. Hopefully fixing on MKL-DNN will also fix on the MKL_ML side. I will post the outcome now that I have reproduced the bug. Thanks for your patience!\r\n", "As you originally reported: \r\ntensorflow/core/kernels/mkl_lrn_op.cc:595] Check failed: dnnConversionCreate_F32( &convert_input, static_cast<dnnLayout_t>(inimage_shape.GetCurLayout()), lt_internal_input) == E_SUCCESS (-1 vs. 0) \r\n\r\nThe bug lies in LRN op as commenting out the two LRN layers in your script avoids the error. We will fix the LRN MKL ops. Please stay tuned. Thank you!", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "(FYI @tatianashp )", "@jakubkarczewski @cy89 @tatianashp @asimshankar \r\n\r\nPlease see below for my newest findings regarding this issue - it seems the issue disappeared if we use Intel MKL DNN backend (with recent commits) rather than Intel MKL ML backend. \r\n\r\nAlexNet]$ export OMP_NUM_THREADS=4 (may need to do this to avoid OpenMP resource issue)\r\nAlexNet]$ python alexnet.py   testoutput/\r\n/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nLoading data\r\nData loaded.\r\n1281167\r\n1000\r\nWARNING:tensorflow:From alexnet.py:329: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee tf.nn.softmax_cross_entropy_with_logits_v2.\r\n\r\nStart time is: 1524900472.01\r\n2018-04-28 00:27:52.015040: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n Iter 0, Minibatch Loss= 6.898850, Training Accuracy= 0.0 Elapsed time:8.87320780754acc_up=[0.0, 0.0]\r\n Iter 20, Minibatch Loss= 6.916332, Training Accuracy= 0.0 Elapsed time:87.2397489548acc_up=[0.0, 0.0]\r\n\r\n@jakubkarczewski Is it OK for you to use Intel MKL DNN? In order to use Intel MKL DNN backend, you\r\ncan just use the previous command you tried \r\nbazel build --config=mkl --copt=\u201d-DEIGEN_USE_VML\u201d -c opt //tensorflow/tools/pip_package: build_pip_package  (*please do not specify   --copt=\"-DINTEL_MKL_ML\" * , this way by default will be INTEL_MKL_DNN). Sorry for the confusion. \r\n\r\nCould you please confirm? ", "@jakubkarczewski FYI, I used a commit that is very much like this public commit: https://github.com/tensorflow/tensorflow/commit/a175841eb549f069ac205fb32bf55314a387fe6d ", "@jakubkarczewski May I suggest trying v1.8.0 (https://github.com/tensorflow/tensorflow/commit/93bc2e2072e0daccbcff7a90d397b704a9e8f778) and  v170 tensorflow tagged version (https://github.com/tensorflow/tensorflow/releases/tag/v1.7.0)  and build with --config=mkl (by default it would be -DIntel_MKL_DNN, not -DIntel_MKLML) . I tried from my side, they do not have the issue as you reported anymore, please confirm. Thank you!\r\n\r\nAfter you confirm, I suggest trying AlexNet in tf_cnn_benchmark (https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/models/alexnet_model.py) instead - they run significantly faster than the keras one. ", "Nagging Assignee @cy89: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16981, "title": "module 'tensorflow.python._pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions'", "body": "I have followed the TensorFlow tutorial, \"[Simple Audio Recognition](https://www.tensorflow.org/versions/master/tutorials/audio_recognition)\"\r\n\r\nWhen I was running **train.py**, I got this error message:\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-61266873bc5f> in <module>()\r\n     77 import numpy as np\r\n     78 from six.moves import xrange  # pylint: disable=redefined-builtin\r\n---> 79 import tensorflow as tf\r\n     80 \r\n     81 import input_data\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     56     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_LOCAL)\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n     60   from tensorflow.python.pywrap_tensorflow_internal import __git_version__\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n    102 def TFE_NewContextOptions():\r\n    103     return _pywrap_tensorflow_internal.TFE_NewContextOptions()\r\n--> 104 TFE_NewContextOptions = _pywrap_tensorflow_internal.TFE_NewContextOptions\r\n    105 \r\n    106 def TFE_ContextOptionsSetConfig(options, proto, proto_len, status):\r\n\r\nAttributeError: module 'tensorflow.python._pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions'\r\n```\r\nSo, I checked out **pywrap_tensorflow_internal.py** in the pyhton directory. \r\nThis is the part of **pywrap_tensorflow_internal.py** that defines TFE_NewContextOptions.\r\n\r\n```\r\ndef TFE_ContextOptionsSetConfig(options, proto, proto_len, status):\r\n    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig(options, proto, proto_len, status)\r\nTFE_ContextOptionsSetConfig = _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig\r\nTFE_DEVICE_PLACEMENT_EXPLICIT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT\r\nTFE_DEVICE_PLACEMENT_WARN = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_WARN\r\nTFE_DEVICE_PLACEMENT_SILENT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT\r\nTFE_DEVICE_PLACEMENT_SILENT_FOR_INT32 = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have figured out what the problem was!!!\r\nI didn't upgrade the tensorflow version\r\n\r\nThanks a lot.", "@magicmutal  I met with the same problem after I upgrade tensorflow from 1.4 to 1.7 .  How to deal with it? Go back to the previous version?", "could you please tell me how to deal with the problem?I upgraded tf to 1.7 and i have the same problem.", "I updated my tensorflow from version 1.4.0 to 1.11.0. After that, when I tried \"import tensorflow as tf\", I ran into following problem:\r\n\r\nFile \"C:\\Users\\MyName\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 96, in <module>\r\n    TFE_NewContextOptions = _pywrap_tensorflow_internal.TFE_NewContextOptions\r\nAttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TFE_NewContextOptions'\r\n\r\nMay I know if you could give me any clue to sort this out? Thanks!\r\n\r\n", "I also got the same problem after installing tf_nightly using \"pip install tf_nightly\" with administrator privileges on windows 10. There was another related issue that while installing tf_nightly some WIN ERROR 5 was raised for which the problem stated here occured. I got to know that it was because of an active session running tensorflow. I solved it by shutting down any python programs currently running and reinstalling tensorflow and tf_nightly and it worked like a charm. I hope it helps someone with the same issue.", "@zwei2007 @manupillai308 \r\n\r\nConsider adding the installation directory (usually **\"Python36\\Scripts\"**) to your environment variables. \r\n\r\nIt should solve the issue.", "I am also facing  the same problem after I upgrade tensorflow   1.12.0. Please suggest how to fix this issue?", "\u8c22\u8c22", "I had similar issue.  My problem was that my tensorflow and tensorflow-gpu installation was mixing up.  To solve this, I ran `pip uninstall tensorflow tensorflow-gpu` and removed those tensorflow folders manually in `python36\\lib\\site-packages`.  Then, reinstall again."]}, {"number": 16980, "title": "Tensorboard Error 404, path /[[_dataImageSrc]] not found", "body": "I am using the object detection API\r\nand training a new ssd_mobilenet from scratch with my dataset.\r\nI already did a successful retraining, but now i want to try a complete new training without checkpoint.\r\n\r\nThe training and evaluation job seem to run normal, but when i start tensorboard and open port 6006 in my browser,  the terminal where its running shows following error:\r\n`\r\nW0213 11:04:23.869396 Thread-2 application.py:273] path /[[_dataImageSrc]] not found, sending 404\r\n`\r\nand no scalars, except of the learning rate, are visualized in tensorboard.\r\nIn contrast to that Evaluation images, the graph, distributions and histogram are all shown correctly.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "although the error is still shown, now everything works and is displayed as it should.\r\nIdk what was the reason but doesn't seem to be really problematic.\r\nThanks, closing!"]}, {"number": 16979, "title": "[Critical some questions] Tensorflow-lite, Neural Networks API", "body": "Hi.\r\n\r\n**Some question.**\r\n\r\n **1.**\r\n\r\n> Current i use  tensorflow library `  compile 'org.tensorflow:tensorflow-lite:+'`\r\n> and i want to use Neural Networks Api, to reduce inference time.\r\n> \r\n> but wrapper function is not existing in `Interpreter.java` class \r\n> `  private static native void useNNAPI(long var0, boolean var2);`\r\n> so i can't use it..\r\n> \r\n> If I create a wrapper function, can I use Neural Networks API? (my device level >  8.1)\r\n\r\nWhat is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/testhelper/java/org/tensorflow/lite/TestHelper.java\r\n?\r\n\r\n**2.**\r\n\r\n> i'm converting pb to tflite. mobilenet_v1_224.pb (17 mb) mobilenet_v1_224_uint8.tflite (4 mb) \r\n> inference speed about 290 ms -> 73ms decreased\r\n\r\n> Command line(mobileNet)\r\n\r\n<pre><code>\r\nbazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_graph.pb \\\r\n  --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint_graph.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shape=1,224,224,3 \\\r\n  --input_array=input \\\r\n  --output_array=MobilenetV1/Predictions/Reshape_1 \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6 \\\r\n  --mean_value=127.5 \\\r\n  --std_value=127.5\r\n</pre></code>\r\n\r\n> In a similar way,\r\n> But frozen_cpm.pb(120 mb) convert to frozen_cpm_uint8.tflite(30 mb)\r\n> inference speed about 11000 ms -> 11000ms. \r\n>  **If the model size is reduced, does not the inference time generally decrease?**\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler \r\nOS Platform and Distribution :  Ubuntu 16.04 LTS\r\nTensorFlow installed from : official\r\nTensorFlow version : 1.5 version\r\nBazel version : 0.9.0 \r\nCUDA/cuDNN version : V 8.0.61 (8.0)\r\nGPU model and memory : GeForce GTX 1060 / 3G\r\nExact command to reproduce : \r\n```\r\nbazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_cpm.pb \\\r\n  --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint8_cpm.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,368,368,3 \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_array=inference_images/Placeholder \\\r\n  --output_array=stage6/confidence_maps/BiasAdd \\\r\n  --logtostderr\r\n```", "@cy89 , @aselle ", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think there's already an official way to turn on the NNAPI in the JAVA Api of TF Lite. It will not make much of a difference unless your device provides NNAPI acceleration."]}, {"number": 16978, "title": "[XLA] Allow 3rd party backends to subclass the generic transfer manager", "body": "I would like to subclass the generic transfer manager, but it does not allow itself to be subclassed by anything but the 3 whitelisted backends.\r\n\r\nThis change removes that restriction.\r\n", "comments": ["LGTM. Thanks!", "@kayzhu i think you have to say something like 'googlebot test this' or something like that.", "jenkins test this please\r\n\r\n"]}, {"number": 16977, "title": "[XLA] Add header and macros to allow these tests to be disabled in a manifest", "body": "Change to the XLA specific macro which allows tests to be disabled in a manifest file.\r\n\r\nWithout this, tests which use types that are not supported by a backend cannot be run.", "comments": ["Looks like there are some conflicts to resolve.", "done - one of the changes was already done by someone else.\r\n"]}, {"number": 16976, "title": "df982b8de - Split gpu_id.h and GpuIdManager out from build target breaks build for verbs and GDR", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: TF not compiling. (master)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: Any\r\n- **Exact command to reproduce**: \r\n\r\n1. ./configure ... with GDR (and/or verbs)\r\n2. bazel build -c opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nCommit df982b8de breaks the build for GDR and verbs. \r\n> ERROR: /home/eladw/google/tensorflow/tensorflow/contrib/gdr/BUILD:52:1: undeclared inclusion(s) in rule '//tensorflow/contrib/gdr:gdr_memory_manager':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/gdr/gdr_memory_manager.cc':\r\n  '/home/eladw/google/tensorflow/tensorflow/core/common_runtime/gpu/gpu_id.h'.\r\n", "comments": ["@aaroey is fixing this within your wheelhouse? ", "Thanks @eladweiss :)", "Thanks for pointing out. Working on the fix.", "Fixed by febed14."]}, {"number": 16975, "title": "libcublas.so 9.0 cannot open shared object file { CUDA 8.0 + GeForce 940MX}", "body": "python\r\nPython 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 17:05:42) \r\n[GCC 7.2.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/abhay/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n", "comments": ["Which tensorflow version is this? Did you build from source or with binaries, e.g. pip? The binaries obtained via pip install for version 1.5 are built with CUDA\u00ae Toolkit 9.0 and cuDNN v7.0.", "If your CUDA is in 9.1 then you'll need to reinstall tensorflow from sources.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes its still an issue\n\nOn 09-Mar-2018 7:08 PM, \"Alfred Sorten Wolf\" <notifications@github.com>\nwrote:\n\n> Nagging Awaiting Response: It has been 14 days with no activityand the awaiting\n> response label was assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16975#issuecomment-371813487>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AUU08ZejDfh1b4vdbycjiT10ovydGFmMks5tcoXmgaJpZM4SDadF>\n> .\n>\n", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 16974, "title": "Fix typo in datasets/imdb.py", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Adding @gunan as a reviewer as this is a PR to a non-master branch (`r1.5`)."]}, {"number": 16973, "title": "Added controlled logging to estimator.py", "body": "Added `logging_every_n_iter` argument to Estimator.fit and _train_model to control the display of metrics (e.g. loss and accuracy) during training.", "comments": ["I missed a comma there in the parameters for train. My bad.", "Re @martinwicke friendly pining about reviewing PR.", "You can control logging frequency using [`RunConfig.log_step_count_steps`](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig). \r\n\r\nI'll close this, assuming that the above is sufficient."]}, {"number": 16972, "title": "Tensorflow inference interface cannot be compiled from build.gradle", "body": "When I enter the dependency named `compile 'org.tensorflow:tensorflow-android:+'` it produces the error named \r\n\r\n> Error:(44, 0) Could not find method compile() for arguments [org.tensorflow:tensorflow-android:+] on object of type org.gradle.api.internal.artifacts.dsl.dependencies.DefaultDependencyHandler\r\n\r\nEven after adding all the required libraries and dependencies, the error still persists.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16971, "title": "Go API - graph.get_tensor_by_name", "body": "\r\nWhat is the equivalent of the Python graph.get_tensor_by_name in Go?\r\n\r\nthanks\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code\r\n- no\r\n\r\nOS Platform and Distribution\r\n- x86_64, Fedora Core 27 \r\n\r\nTensorFlow installed from\r\n- python\r\nTensorFlow version\r\n- 1.5.0\r\nBazel version\r\n- n/a\r\nCUDA/cuDNN version\r\n- n/a\r\nGPU model and memory\r\n- n/a\r\nExact command to reproduce\r\n- want to use equivalent of graph.get_tensor_by_name in Go", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I thought about posting here first because in Python, get_tensor_by_name() is a method of the Graph type but in Golang the API seems to be quite different and the design doesn't seem to be documented anywhere. \r\nAny pointers that would also be useful (apart from godoc.org) ?\r\n", "I'd still recommend posting on StackOverflow for a few reasons. (a) To keep this focused on bugs and feature requests, (b) To make such support questions more accessible to others.\r\n\r\nAnyway, the short answer would be `Graph.Operation(<name>).Output(<index>)` in Go. \r\nFor more details, again, I'd recommend StackOverflow.\r\n\r\nThanks!"]}, {"number": 16970, "title": "Add release note: sparse tensor support in `tf.data`", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Thank you for catching this in the release notes.  Too much time has likely passed although it could still be merged.  If you go back and sign the CLA please tag me and I can reopen and try to get this merged.  \r\n\r\nI think you get an email but if not this link should make it easy and the official links are above in the comments from the bot.  \r\n\r\nhttps://cla.developers.google.com/clas"]}, {"number": 16969, "title": "Tensorflow in tornado", "body": "The process stop in calculating and did not give any response.\r\njust like\r\n`[->] restore model\r\n2018-02-13 15:27:49.235484: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 566c31076b9c7519 with config:`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Platform:Ubuntu 16.04\r\nTensorflow-cpu:1.4, install from pip\r\n\r\n---\r\nOur project used `tornado` as service\r\n\r\nModel like this:  \r\n  \r\nClass Model:  \r\n\r\n    def __init__(self):\r\n        ....\r\n        self.create_model()  # create placeholder, graph, etc.\r\n        self.resotre_model() # resotre model from cpkt\r\n\r\n    def create_model():\r\n        self.graph = tf.Graph()\r\n        with self.graph.as_default():\r\n            ...\r\n    def restore_model():\r\n        with tf.Session(graph=self.graph) as sess:\r\n            ...\r\n\r\n    def predict(self):\r\n        with tf.Session(graph=self.graph) as sess:\r\n            sess.run(...)   \r\n\r\n\r\nProblem is that if i restored model while the model created, the `sess.run` didn't give any response, it seems that the program was in await status.If I moved `resotre_model` into `predict` which means restore model every time, it worked well but waste time.\r\n\r\n", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It seems that this is a design defect (maybe?). The program work fine with **bottle** but unsupported with **tornado** because of the **fork-safe** in Python2.7.", "This is a clear design defect in tensorflow in that tf.session objects are not fork safe, thus crippling any kind of parallelisation you might wish to do with tensorflow.\r\n\r\nIt's existed for a few years and so far each issue mentioning it has been closed with no solution. There currently exists no way around this that I or anyone else has found so far.\r\n\r\nYet to hear from any tensorflow contributors why exactly this is, either.", "One solution is that using the **tensorflow.train.Server**, but we don't do that because it makes the projects complex.We just change the frame to **bottle**   :(", "Tornado can  work with tensorflow well in Python 3.6.5 .(we tested !)So I sugesst you have a try.", "yeah, we tested it tonight, and it worked fine! Thx !"]}, {"number": 16968, "title": "Fix typo in build_and_run_inception_hexagon.sh", "body": "Signed-off-by: MyungSung Kwak <yesmung@gmail.com>", "comments": ["Dear @gunan \r\nThis typo can cause confusion when the developer builds it.  So I fixed the typo with the correct name. \r\nPlease review my commit.", "Thank you for your support."]}, {"number": 16967, "title": "No package nasm", "body": "On Release version v1.5.0.\r\nIn the file tensorflow/workspace.bzl.\r\nAt the line 208.\r\n\r\nThe link [http://pkgs.fedoraproject.org/repo/pkgs/nasm/nasm-2.12.02.tar.bz2/d15843c3fb7db39af80571ee27ec6fad/nasm-2.12.02.tar.bz2]() is not exist any more.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: source\r\nTensorFlow version: v1.5.0\r\nBazel version: 0.10.0\r\nCUDA/cuDNN version: 9.1/7.0.5\r\nGPU model and memory: GTX 1050 4Gbs", "Please use link at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L214 instead. Thanks"]}, {"number": 16966, "title": "Fix compiler error mentioned in #16960 introduced by commit 1baac78627", "body": "Addressing #16960.\r\nThe commit https://github.com/tensorflow/tensorflow/commit/1baac7862739525351d25202800dc04e8ec3868b introduced member functions `MklSubAllocator::AddAllocVisitor` and `MklSubAllocator::AddFreeVisitor` which respectively use `allocator_->AddAllocVisitor` and `allocator_->AddFreeVisitor` but `allocator_` is of type `Allocator *` which doesn't have these member functions. \r\n\r\nI am guessing the intention was to change the type of `allocator_` to `BFCAllocator*` to make this work.", "comments": []}, {"number": 16965, "title": "Bug - Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)  Aborted (core dumped)", "body": "Hello ~\r\n\r\nI got this error when use `tf.layers.conv2d`.\r\nI use systems as following.\r\n\r\nubuntu 16.04\r\ntensorflow 1.5.0\r\ncuda 9.0\r\ncudnn 7\r\n\r\nand I executed my code on nvidia-docker conteiner.\r\n(nvidia/cuda:9.0-cudnn7-runtime-ubuntu16.04)\r\n\r\n", "comments": ["Same issue. \r\n\r\nSystem details are:\r\nWindows 10\r\nPython 3.6\r\nTensorflow 1.5.0\r\nCuda 9.0 and cuDNN 7.0.5\r\n\r\nExample Code is Tensorflow's MNIST code - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py\r\n\r\nThe debug info:\r\n```\r\nname: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.00GiB freeMemory: 2.47GiB\r\n2018-02-13 12:24:03.845670: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2018-02-13 12:24:05.082021: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018-02-13 12:24:05.087986: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:444] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2018-02-13 12:24:05.590109: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2018-02-13 12:24:05.595021: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018-02-13 12:24:05.600328: F C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)\r\n```\r\n", "This bug is still present with Keras program:\r\n```\r\nimport numpy as np\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras.optimizers import SGD\r\n\r\n\r\nx_train = np.random.random((100,100,100,3))\r\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\r\nx_test = np.random.random((20,100,100,3))\r\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(20,1)), num_classes=10)\r\n\r\n\r\nmodel = Sequential()\r\n\r\n\r\nmodel.add(Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)))\r\nmodel.add(Conv2D(32, (3,3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(256, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(10, activation='softmax'))\r\n\r\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\r\n\r\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\r\n\r\n```\r\nThe below snipped from stackoverflow fixes it in MNIST example\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config, ...)\r\n```\r\n\r\nThis fix from https://stackoverflow.com/questions/41117740/tensorflow-crashes-with-cublas-status-alloc-failed\r\n\r\nWhat's the problem here?", "@wang-yang can you please take a look at?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The answer appears to be on stack overflow.\r\n\r\n```py\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config, ...)\r\n```\r\n\r\nIf there's a bug in TensorFlow that we can fix here, please help us learn more and I'll reopen this issue.", "When I was using Tenosorflow ver 1.2.1, I can not observe this error.\r\nI seem that some system can observe this error from  Tensorflow ver 1.3.0. or Cuda9.0.\r\nAnd this error occurs only in convolutional operation.\r\n\r\nSystem detail \r\nnvidia drivver varsion: 384.125\r\ngpu : tesla v100 x4\r\ncpu : Intel(R) Xeon(R) CPU E5-2623 v3 @ 3.00GHz\r\ntensorflow version: 1.5.0\r\nCuda version: 9.0\r\nNvidia Docker version: 2", "I'm having the same issue, even after applying the changes posted by Jart back in April. The code I'm executing is from Udacity deep learning tutorial assignment #4. Any help would be greatly appreciated. \r\n\r\nSystem Details: \r\nSystem: Windows 10 home 64-bit, x64-based processor\r\nCuda: v 9.0.176\r\nCUDNN: v 9.0 win10x64 7.3.1.2\r\ntf-gpu: v 1.5.0 via PIP\r\nNVIDIA: GTX 1060 6 GiB\r\nNVIDIA DRIVER VERSION: 417.35\r\npython v: 3.6.7\r\n\r\nThe output does not change after applying Jart's suggestion. \r\nOutput:\r\n~\\Documents\\Udacity\\Deep Learning\\Assignment 4 (CNN's)> python main.py\r\nTraining set (200000, 28, 28) (200000,)\r\nValidation set (10000, 28, 28) (10000,)\r\nTest set (10000, 28, 28) (10000,)\r\nTraining set (200000, 28, 28, 1) (200000, 10)\r\nValidation set (10000, 28, 28, 1) (10000, 10)\r\nTest set (10000, 28, 28, 1) (10000, 10)\r\n2019-01-04 15:40:09.714793: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2019-01-04 15:40:10.003545: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1105] Found device 0 with properties:\r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.97GiB\r\n2019-01-04 15:40:10.013346: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060, pci bus\r\nid: 0000:01:00.0, compute capability: 6.1)\r\nInitialized\r\n2019-01-04 15:40:12.584016: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:378] Loaded runtime CuDNN library: 7301 (compatibility version 7300) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2019-01-04 15:40:12.601433: F C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\kernels\\conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)"]}, {"number": 16964, "title": "[Build Failed] Convolutional pose machine converts tflite format", "body": "Hello.\r\n\r\nI converting CPM(convolutional pose machine) PB file to TFLITE file, encounter error(s).\r\n\r\nI success, converting from MobileNet v1 224 PB file to uint8 TFLITE file format (17.2 MB -> 4.3 MB)\r\n\r\n**Just only this log.**\r\n<pre><code>\r\nWARNING: Config values are not defined in any .rc file: opt\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/toco/toco\r\nINFO: Elapsed time: 0.205s, Critical Path: 0.00s\r\nINFO: Build completed successfully, 1 total action\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/danshin/tensorflow_lite/lite_model/frozen_cpm.pb' '--output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint8_cpm.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shape=1,368,368,3' '--input_array=images/Placeholder' '--output_array=stage6/confidence_maps/BiasAdd' '--default_ranges_min=0' '--default_ranges_max=6' '--mean_value=127.5' '--std_value=127.5' '--v=1'\r\n2018-02-13 11:07:56.107251: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 324 operators, 444 arrays (0 quantized)\r\n</code></pre>\r\n\r\n**Command line**\r\n<pre><code>\r\n- bazel run -c opt --copt=-msse4.1 --copt=-msse4.2 --config=opt \\\r\n- //tensorflow/contrib/lite/toco:toco -- \\\r\n- --input_file=/home/danshin/tensorflow_lite/lite_model/frozen_cpm.pb \\\r\n- --output_file=/home/danshin/tensorflow_lite/lite_model/frozen_uint8_cpm.tflite \\\r\n- --input_format=TENSORFLOW_GRAPHDEF \\\r\n- --output_format=TFLITE \\\r\n- --inference_type=QUANTIZED_UINT8 \\\r\n- --inference_input_type=QUANTIZED_UINT8 \\\r\n- --input_shape=1,368,368,3 \\\r\n- --input_array=images/Placeholder \\\r\n- --output_array=stage6/confidence_maps/BiasAdd \\\r\n- --default_ranges_min=0 \\\r\n- --default_ranges_max=6 \\\r\n- --mean_value=127.5 \\\r\n- --std_value=127.5 \\\r\n- --v=1\r\n</code></pre>\r\n\r\n**Input node name description**\r\n Dim is -1, could this be a problem?\r\n```\r\nname: \"images/Placeholder\"\r\nop: \"Placeholder\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"shape\"\r\n  value {\r\n    shape {\r\n      dim {\r\n        size: -1\r\n      }\r\n      dim {\r\n        size: 368\r\n      }\r\n      dim {\r\n        size: 368\r\n      }\r\n      dim {\r\n        size: 3\r\n      }\r\n    }\r\n  }\r\n}\r\n```", "comments": ["Resolved. nice!!\r\n\r\njust dim problem.\r\n@aselle why tensorflow-lite supported dim >= 1 ?", "@nanamare is your question to @aselle why tf-lite requires array dimensions to be static, not dynamic (which is the usual meaning of dim=-1)? ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}]