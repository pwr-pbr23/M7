[{"number": 365, "title": "Hamburger menu no longer appears on mobile", "body": "I noticed that the hamburger menu no longer appears on mobile:\n\n<img width=\"400\" alt=\"screen shot 2015-11-26 at 9 20 26 pm\" src=\"https://cloud.githubusercontent.com/assets/5283042/11434590/a97d28de-9483-11e5-9587-70a6a1b45e79.png\">\n", "comments": ["- @craigcitro \n", "@martinwicke probably knows more about the details here ...\n", "We disabled the more involved navigation features on mobile to expedite moving to the new website.\n", "Understood. I'll look forward to seeing the new website.\n", "You are seeing the new website, which we launched without the hamburger menu in order to launch it quicker. :) It'll come back, eventually.\n", "@martinwicke: Do we have hamburgers again?\n", "No. All we have is this issue and @danmane.\nOn Tue, Mar 8, 2016 at 13:41 Geoffrey Irving notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke: Do we have hamburgers\n> again?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/365#issuecomment-193979968\n> .\n", "Will likely happen once we move doc systems again.\n", "@martinwicke: Any update on this?\n", "Will be dealt with in our move to new website infrastructure. We can close this as obsolete -- we won't work on this.\n"]}, {"number": 364, "title": "Integer random_uniform", "body": "Feature request:\n\nSupport for dtype=tf.int32/int64 in tf.random_uniform.\n", "comments": ["Yep, we want this internally too.\n", "Would the upper bound be configurable? What sort of range would be a good default? \n", "This is done, and should be out in git soon.  `tf.random_uniform` now works for `int32` and `int64`.  Like the floating point case, it takes a half open range as `[minval, maxval)`.  In the integer case, `maxval` must be specified (it does not default to 1, since random zeros are not particularly useful).\n", "@michaelguia: That is, I don't know a good default range for the integer case (at least on the top), so I made it required.\n"]}, {"number": 363, "title": "No Rollback after \"cifar10_train.py\" canceled", "body": "Hi,\n\nI noticed than on both Ubuntu and Mac OSX there is no handling of cancelling (rollback?) the cifar10-set download process of the cifar10_train.py via ctrl+c.\n\nIf the process was once canceled you will be incapable of running the script again. The following error will be issued:\n\n```\nTraceback (most recent call last):\n  File \"cifar10_train.py\", line 138, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 11, in run\n    sys.exit(main(sys.argv))\n  File \"cifar10_train.py\", line 134, in main\n    train()\n  File \"cifar10_train.py\", line 69, in train\n    images, labels = cifar10.distorted_inputs()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10.py\", line 166, in distorted_inputs\n    raise ValueError('Failed to find file: ' + f)\nValueError: Failed to find file: /tmp/cifar10_data/cifar-10-batches-bin/data_batch_1.bin\n```\n", "comments": ["Checking for integrity of all the files would be tedious. We could provide an option to always re-download, which might make DOSing the source web site a little too easy if people get used to enabling it.\nThe simple 'rm -rf /tmp/cifar10_data' is a natural way to deal with this, so I'd be inclined not to do anything here. \n", "I think atleast a md5sum-check shouldn't be surcharged. \nBut this isn't the point.\nThere is on neither of my systems a /tmp/cifar10_data/ directory, despite the error. Deduce from this fact \"rm-rf /tmp/cifar10_train.py\" hasn't worked for me.\n\nSent from my iPhone\n\n> On Nov 26, 2015, at 21:32, Vincent Vanhoucke notifications@github.com wrote:\n> \n> Checking for integrity of all the files would be tedious. We could provide an option to always re-download, which might make DOSing the source web site a little too easy if people get used to enabling it.\n> The simple 'rm -rf /tmp/cifar10_data' is a natural way to deal with this, so I'd be inclined not to do anything here.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "I'm not sure exactly what the problem is:\n\nhttps://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/models/image/cifar10/cifar10.py#L484 \n\nCan you instrument that code and let us know if something is not behaving as expected?\n", "I am so sorry, \"rm -rf /tmp/cifar10_data\" didn't work due to a miss configuration on my system.\n\nHowever I still think a checksum check should me implemented. This scripts is mainly used by newbies following the tensorflow cifar tutorial. It should be beginner friendly and not require deep research and terminal working.\n", "Go to /tmp, where is all the way outside of your directory. There you could find cifar10_train and cifar10_data folders. Inside cifar10_data folder, damaged cifar-10-binary.tar.gz file is located. The cifar10_train.py keep recognize this damaged file as intact one, error message keep shown. \nTherefore you should remove the cifar10_train and cifar10_data folders and re-python cifar10_train.py.\n", "I\u2018m afraid that it's too late to say something. I think that maybe the data doesn't exists in your slave nodes.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "yeah I met the same issue as you.\r\nAnd I guess this is because I download it for a while but not perfectly,that means my computer have cifar-10 but it's bad.So,i get to\r\n/tmp/cifar10_data/\r\nand I delect anything about cifar-10\r\n\r\nafter that I get back to cifar10_train,it worked !"]}, {"number": 362, "title": "Some wrong code in get_started document", "body": "On your get started website http://www.tensorflow.org/get_started/basic_usage.html#basic-usage\nI found a small piece of wrong code:\nIn the sample code of Feeds section:\n\n```\ninput1 = tf.placeholder(tf.types.float32)\ninput2 = tf.placeholder(tf.types.float32)\n```\n\nI tried them and python gave the AttributeError: 'module' object has no attribute 'types'\nAfter reading the document http://www.tensorflow.org/resources/dims_types.html\nI think it should be `tf.float32` instead of `tf.types.float32`.  Probably it's a version issue?\n", "comments": ["Thanks for pointing this out! We have a fix in the pipeline, and it should appear on the website soon.\n", "Thanks for the prompt reply!\n", "Again I found very minor wrong number in the tutorial: http://www.tensorflow.org/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners\nIt mentions the shape of training images are [60000, 784], but actually [55000, 784]; also for the labels: [55000, 10] instead of [60000, 10].\n", "Thanks! it does help!!!!\r\nNow I'm studying the newly tensorflow version based on the windows platform, and some demo codes of the guide TENSORFLOW.ORG gave do have some error, probably this guide is for python version 2.x, but the new tensorflow is based on python version 3.5.x.....", "@Careenn If you have links to the tutorials that don't work in Python 3.5.x, please send them in a new issue (or sending a pull request that fixes them would be even better!). "]}, {"number": 361, "title": "wget ...mnist/input_data.py from tensorflow.googlesource.com results in jargon", "body": "`wget https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/input_data.py` \nresult is html formatted, unusable. \n\n`wget https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/input_data.py?format=TEXT` \nresults in jargon, unusable. \n\n`wget https://github.com/tensorflow/tensorflow/raw/master/tensorflow/g3doc/tutorials/mnist/input_data.py`\nworks\n", "comments": ["Sadly, nothing we can do.  https://code.google.com/p/gitiles/issues/detail?id=7 seems relevant.\n"]}, {"number": 360, "title": "Warn when calling session.run() on a very large scatter", "body": "Scatter ops output a ref to the whole tensor for easier chaining. The problem is if one executes the op directly in Session.run() the whole tensor is copied back as a numpy array. This can create very annoying and hard to find performance problems. I think you should consider adding a warning.\n\nFor example:\nlarge_table = tf.Variable(... [1000000, 3])\nset3_op = tf.scatter_update(large_table, 3, [1,2,3])\n\nsession.run([set3_op])\n\nHere run() modifies just 3 elements of the table but returns the whole table as a numpy array that is then \nignored.\n", "comments": ["Sure, we'll take that into consideration.\n\nMost computational operations, when expressed in our python API, return a tf.Tensor object, and passing a Tensor object to session.run does indeed fetch the result back -- you can just run the operation by passing set3_op.op instead.\n", "I don't think it's practical to have a warning like this, unfortunately.  Calling `session.run()` on large tensors seems like pretty standard behavior for machine learning.\n", "@girving I was still learning Tensorflow at the time I filed this so I guess I didn't phrase it properly.\n\nThe problem is that tf.scatter_update/add/sub() Ops return the whole tensor although they are typically used to update only a small part of it. \n\nFor example:\n\nsession.run([tf.scatter_update(large_table, 3, [1,2,3]))\n\nwill return the whole value of \"large_table\" as a NumPy array although it updates a small part of it. This can cause a nasty hard-to-find performance problem. I think it is reasonable to have a warning when\non of the fetch operations in session.run is scatter_*.\n", "Fair enough, I was a bit misled by the title change.  Reopening.\n", "Incidentally, it would be better to fix the performance issue entirely.  There are a couple possible ways to do that, though I'm not sure if any are practical:\n1. Automatically replace `scatter` with `scatter.op` in `session.run`.\n2. Same as (1), but on the C++ side somehow.\n3. Return views into the tensor rather than the whole thing.\n\n@michaelisard: Do any of these seem sane / practical? \n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Is this still valid? Also ` tf.scatter_update` is deprecated.", "Hi @bdaskalov ,\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade your code base to 2.x versions following [migration document ](https://www.tensorflow.org/guide/migrate/migrate_tf2)and let us know if the issue still persists in newer versions. Thanks!", "You can close it. I don't have this problem any more\n\nOn Thu, Sep 23, 2021, 4:29 AM mohantym ***@***.***> wrote:\n\n> Hi @bdaskalov <https://github.com/bdaskalov> ,\n> We are checking to see if you still need help on this issue. We recommend\n> that you upgrade your code base to 2.x versions following migration\n> document <https://www.tensorflow.org/guide/migrate/migrate_tf2>and let us\n> know if the issue still persists in newer versions. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/360#issuecomment-925726195>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAVTZEBN2IJVVIV2G3OMWEDUDMFR5ANCNFSM4BVHLXNQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@bdaskalov ,Thanks for confirming the same!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 359, "title": "ADAM optimizer creates trainable variable", "body": "ADAM optimizer crates two variables that are marked as trainable for no good reason (at least not apparent to me). \n\nI sent the following path but no one seems to have looked at it:\nhttps://tensorflow-review.googlesource.com/#/c/1141/\n", "comments": ["Sorry, we'll look at it soon -- we are still working on our tooling to accept external contributions, so give us a little bit more time.\n", "First successfully merged contribution I believe. Congrats! :)\n", "Thanks.\n\nOn Fri, Dec 4, 2015 at 5:16 PM, Vincent Vanhoucke notifications@github.com\nwrote:\n\n> First successfully merged contribution I believe. Congrats! :)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/359#issuecomment-161990963\n> .\n"]}, {"number": 358, "title": "SWIG signature mismatch at RecordWriter", "body": "I tried to run the example \"tensorflow/g3doc/how_tos/reading_data/convert_to_records.py\", but there is an error saying function signature mismatch on the `bool WriteRecord(::tensorflow::StringPiece record);` call.\n\nMy swig version is `SWIG Version 1.3.40`\n\nA simple fix is to change `bool WriteRecord(::tensorflow::StringPiece record);` to `bool WriteRecord(const string &record);` for these two files,\n\n```\ntensorflow/tensorflow/python/lib/io/py_record_writer.h\ntensorflow/tensorflow/python/lib/io/py_record_writer.cc\n```\n", "comments": ["`StringPiece` is faster than `string` in some cases, so we don't want to change the signature in that way.  Can you add the full error message?  Also, what happens if you change `::tensorflow::StringPiece` to just `tensorflow::StringPiece` (i.e., remove the `::`)?\n", "Removing the \"::\" solves the problem. Thanks!\n\nAs a reference, following is the TypeError with \"::\".\n\n```\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 132, in WriteRecord\n    def WriteRecord(self, *args): return _pywrap_tensorflow.PyRecordWriter_WriteRecord(self, *args)\nTypeError: in method 'PyRecordWriter_WriteRecord', argument 2 of type '::tensorflow::StringPiece'\n```\n", "Thanks, I'll fix this in the code.\n"]}, {"number": 357, "title": "reduce_sum (complex) on GPU segfaults", "body": "When the `reduce_sum` operation is run on a complex tensor on my GPU, it segfaults. See the attached python bug demo script.\n- OS: Linux CentOS 7\n- GPU: GeForce GTX 750 ti\n\n[bug.txt](https://github.com/tensorflow/tensorflow/files/44567/bug.txt)\n", "comments": []}, {"number": 356, "title": "translate: unrecognized option --data_dir", "body": "I tried the 'translate' command described in the tutorial (http://www.tensorflow.org/tutorials/seq2seq/index.html#run_it). But it threw an error:\n\n```\n$ bazel run -c opt tensorflow/models/rnn/translate:translate --data_dir /tmp/data --train_dir /tmp/train  \nUnrecognized option: --data_dir \n```\n\nFor the records, this worked instead:\n\n```\n$ bazel build -c opt --config=cuda tensorflow/models/rnn/translate:translate            \n$ bazel-bin/tensorflow/models/rnn/translate/translate --data_dir /tmp/data --train_dir /tmp/train\n```\n", "comments": ["I think there's a missing '--' after \"tensorflow/models/rnn/translate:translate\".  We'll fix it soon!\n", "I believe this was fixed a few weeks ago.\n"]}, {"number": 355, "title": "The first name of Fran\u00e7oise Beaufays is misspelled in whitepaper2015.pdf", "body": "In the TensorFlow white paper (http://download.tensorflow.org/paper/whitepaper2015.pdf), on page 17, reference [6], the first name of Fran\u00e7oise Beaufays is misspelled as Franoise, missing a \"\u00e7\".\n", "comments": []}, {"number": 354, "title": "Tutorials are difficult to read", "body": "Greyish text on white background causes eye fatigue. Especially with longer text, and sanserif fonts.\n", "comments": ["@danmane, @martinwicke: I assume this is just a css style change?\n", "Not quite but almost. Dan is on it.\nOn Mon, Nov 30, 2015 at 22:13 Vijay Vasudevan notifications@github.com\nwrote:\n\n> @danmane https://github.com/danmane, @martinwicke\n> https://github.com/martinwicke: I assume this is just a css style\n> change?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/354#issuecomment-160866764\n> .\n", "Done.\n", "The contrast is much better now. So the readability.\n\nThank you very much.\n"]}, {"number": 353, "title": "cifar10 speed numbers", "body": "In the comments of [cifar10_train.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10_train.py), it says: \n\n```\nAccuracy:\ncifar10_train.py achieves ~86% accuracy after 100K steps (256 epochs of\ndata) as judged by cifar10_eval.py.\n\nSpeed: With batch_size 128.\n\nSystem        | Step Time (sec/batch)  |     Accuracy\n------------------------------------------------------------------\n1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n```\n\nThe math doesn't look right to me: \n5 x 60 x 60 / 60K=**0.3** sec/batch\n4 x 60 x 60 / 100K=**0.144** sec/batch\n\nAlso, on my machine (K40c, 28 CPU cores, SSD) I can only reach about half the speed (~0.6 sec/batch). Any idea what might be the factor that slows it down? Thanks! \n", "comments": ["I bet these numbers (both ours and yours) are probably out dated -- re-open if there's still a problem with newer releases.\n"]}, {"number": 352, "title": "Out of GPU Memory -- Only 1 LSTM layer on 980 TI", "body": "Hey TF,\n\nI have been using your translate model from your seq2seq tutorial and everything seems to work great. \n\nHowever, I have encountered a substantial problem. On my 980 TI with 6gb of memory:\n- 5 GRU layers, 512 cells, batch size 32 -- _works_\n- 1 GRU layer, 1024 cells, batch size 2 -- _barely works_ (tried batch size of 2, 4, 8, 16)\n\nIn Keras, I could run 2 GRU layers each with 2048 cells on my the 6gb memory. So the question I have is: how is this possible? What is taking up so much memory when you increase the cell size? \n\nI have a second 980 TI as well. I was really hoping I could put one layer 2048 cells on each card. Thanks again! \n", "comments": ["When it runs out of memory, it should print information about the memory usage  -- can you include that?\n", "Sure! Here is the pool allocation when it _does_ work -- GRU with 1024 cells at batch size 2:\n\n```\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 7923 get requests, put_count=3499 evicted_count=1000 eviction_rate=0.285796 and unsatisfied allocation rate=0.697211\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 100 to 110\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4623 get requests, put_count=2605 evicted_count=1000 eviction_rate=0.383877 and unsatisfied allocation rate=0.656284\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 176 to 193\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 6 get requests, put_count=2029 evicted_count=2000 eviction_rate=0.985707 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1034 evicted_count=1000 eviction_rate=0.967118 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 4 get requests, put_count=1044 evicted_count=1000 eviction_rate=0.957854 and unsatisfied allocation rate=0\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=1054 evicted_count=1000 eviction_rate=0.948767 and unsatisfied allocation rate=-nan\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 0 get requests, put_count=1087 evicted_count=1000 eviction_rate=0.919963 and unsatisfied allocation rate=-nan\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 7337 get requests, put_count=6254 evicted_count=2000 eviction_rate=0.319795 and unsatisfied allocation rate=0.437509\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 1400 to 1540\nW tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 12607 get requests, put_count=10862 evicted_count=1000 eviction_rate=0.0920641 and unsatisfied allocation rate=0.235583\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:239] Raising pool_size_limit_ from 2478 to 2725\n```\n\nHere is 2 GRU layers of 1024 batch size of 2. When it runs out of memory, the error message keeps repeating over and over again. I've included the print out right as it is about to compute. You can see that it starts to allocate and pool data but then it simply runs out:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 256 (256B) Pool: chunks: 2048 free: 247 cumulative malloc: 10736 cumulative freed: 8935\nNumber of chunks: 2048, in_use chunks: 1801\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 512 (512B) Pool: chunks: 64 free: 64 cumulative malloc: 360 cumulative freed: 360\nNumber of chunks: 64, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1024 (1.0KiB) Pool: chunks: 64 free: 62 cumulative malloc: 352 cumulative freed: 350\nNumber of chunks: 64, in_use chunks: 2\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2048 (2.0KiB) Pool: chunks: 128 free: 128 cumulative malloc: 414 cumulative freed: 414\nNumber of chunks: 128, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4096 (4.0KiB) Pool: chunks: 256 free: 2 cumulative malloc: 963 cumulative freed: 709\nNumber of chunks: 256, in_use chunks: 254\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8192 (8.0KiB) Pool: chunks: 128 free: 63 cumulative malloc: 1052 cumulative freed: 987\nNumber of chunks: 128, in_use chunks: 65\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 16384 (16.0KiB) Pool: chunks: 1344 free: 709 cumulative malloc: 14046 cumulative freed: 13411\nNumber of chunks: 1344, in_use chunks: 635\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 32768 (32.0KiB) Pool: chunks: 384 free: 247 cumulative malloc: 2187 cumulative freed: 2050\nNumber of chunks: 384, in_use chunks: 137\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 65536 (64.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 9 cumulative freed: 9\nNumber of chunks: 2, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 131072 (128.0KiB) Pool: chunks: 2 free: 1 cumulative malloc: 2 cumulative freed: 1\nNumber of chunks: 2, in_use chunks: 1\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 229376 (224.0KiB) Pool: chunks: 44 free: 44 cumulative malloc: 242 cumulative freed: 242\nNumber of chunks: 44, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 327680 (320.0KiB) Pool: chunks: 4 free: 3 cumulative malloc: 4 cumulative freed: 3\nNumber of chunks: 4, in_use chunks: 1\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 425984 (416.0KiB) Pool: chunks: 68 free: 68 cumulative malloc: 796 cumulative freed: 796\nNumber of chunks: 68, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 524288 (512.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 6 cumulative freed: 6\nNumber of chunks: 2, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1048576 (1.00MiB) Pool: chunks: 190 free: 69 cumulative malloc: 768 cumulative freed: 647\nNumber of chunks: 190, in_use chunks: 121\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2228224 (2.12MiB) Pool: chunks: 119 free: 119 cumulative malloc: 276 cumulative freed: 276\nNumber of chunks: 119, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4456448 (4.25MiB) Pool: chunks: 62 free: 0 cumulative malloc: 164 cumulative freed: 102\nNumber of chunks: 62, in_use chunks: 61\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8912896 (8.50MiB) Pool: chunks: 123 free: 0 cumulative malloc: 529 cumulative freed: 406\nNumber of chunks: 123, in_use chunks: 122\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 17825792 (17.00MiB) Pool: chunks: 61 free: 0 cumulative malloc: 226 cumulative freed: 165\nNumber of chunks: 61, in_use chunks: 60\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 41943040 (40.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 67108864 (64.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 2 cumulative freed: 2\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 134217728 (128.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 335544320 (320.00MiB) Pool: chunks: 7 free: 3 cumulative malloc: 22 cumulative freed: 18\nNumber of chunks: 7, in_use chunks: 4\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:345] Aggregate Region Memory: 5596086272 (5.21GiB)\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:347] Aggregate Chunk Memory: 5595824128 (5.21GiB)\nW tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:89] Out of GPU memory, see memory state dump above\nW tensorflow/core/kernels/matmul_op.cc:143] Resource exhausted: OOM when allocating tensor with shapedim { size: 2048 } dim { size: 1024 }\nW tensorflow/core/common_runtime/executor.cc:1027] 0x58dae900 Compute status: Resource exhausted: OOM when allocating tensor with shapedim { size: 2048 } dim { size: 1024 }\n     [[Node: gradients_3/model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/Linear/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/Linear/concat, gradients_3/model_with_buckets/embedding_attention_seq2seq_3/embedding_attention_decoder/attention_decoder/GRUCell_4/Candidate/add_grad/Reshape)]]\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 256 (256B) Pool: chunks: 2048 free: 247 cumulative malloc: 10736 cumulative freed: 8935\nNumber of chunks: 2048, in_use chunks: 1801\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 512 (512B) Pool: chunks: 64 free: 64 cumulative malloc: 360 cumulative freed: 360\nNumber of chunks: 64, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1024 (1.0KiB) Pool: chunks: 64 free: 62 cumulative malloc: 352 cumulative freed: 350\nNumber of chunks: 64, in_use chunks: 2\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2048 (2.0KiB) Pool: chunks: 128 free: 128 cumulative malloc: 414 cumulative freed: 414\nNumber of chunks: 128, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4096 (4.0KiB) Pool: chunks: 256 free: 2 cumulative malloc: 963 cumulative freed: 709\nNumber of chunks: 256, in_use chunks: 254\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8192 (8.0KiB) Pool: chunks: 128 free: 63 cumulative malloc: 1052 cumulative freed: 987\nNumber of chunks: 128, in_use chunks: 65\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 16384 (16.0KiB) Pool: chunks: 1344 free: 710 cumulative malloc: 14046 cumulative freed: 13412\nNumber of chunks: 1344, in_use chunks: 634\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 32768 (32.0KiB) Pool: chunks: 384 free: 248 cumulative malloc: 2187 cumulative freed: 2051\nNumber of chunks: 384, in_use chunks: 136\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 65536 (64.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 9 cumulative freed: 9\nNumber of chunks: 2, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 131072 (128.0KiB) Pool: chunks: 2 free: 1 cumulative malloc: 2 cumulative freed: 1\nNumber of chunks: 2, in_use chunks: 1\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 229376 (224.0KiB) Pool: chunks: 44 free: 44 cumulative malloc: 242 cumulative freed: 242\nNumber of chunks: 44, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 327680 (320.0KiB) Pool: chunks: 4 free: 3 cumulative malloc: 4 cumulative freed: 3\nNumber of chunks: 4, in_use chunks: 1\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 425984 (416.0KiB) Pool: chunks: 68 free: 68 cumulative malloc: 796 cumulative freed: 796\nNumber of chunks: 68, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 524288 (512.0KiB) Pool: chunks: 2 free: 2 cumulative malloc: 6 cumulative freed: 6\nNumber of chunks: 2, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1048576 (1.00MiB) Pool: chunks: 190 free: 69 cumulative malloc: 768 cumulative freed: 647\nNumber of chunks: 190, in_use chunks: 121\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2228224 (2.12MiB) Pool: chunks: 119 free: 119 cumulative malloc: 276 cumulative freed: 276\nNumber of chunks: 119, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4456448 (4.25MiB) Pool: chunks: 62 free: 0 cumulative malloc: 164 cumulative freed: 102\nNumber of chunks: 62, in_use chunks: 61\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 8912896 (8.50MiB) Pool: chunks: 123 free: 0 cumulative malloc: 529 cumulative freed: 406\nNumber of chunks: 123, in_use chunks: 122\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 17825792 (17.00MiB) Pool: chunks: 61 free: 0 cumulative malloc: 226 cumulative freed: 165\nNumber of chunks: 61, in_use chunks: 60\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 41943040 (40.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 67108864 (64.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 2 cumulative freed: 2\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 134217728 (128.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 335544320 (320.00MiB) Pool: chunks: 7 free: 3 cumulative malloc: 22 cumulative freed: 18\nNumber of chunks: 7, in_use chunks: 4\n```\n\nIt will keep repeating this over and over again.\n\nHas anyone had similar problems? It just doesn't seem right that a 1024 GRU should barely fit on 5gb of memory. I am purely doing text to text (50 words input and 50 words output), so the input data shouldn't be that heavy. Any help would be **immensely** appreciated! \n", "Ah, this is likely a result of fragmentation caused by our initial / older memory allocator.\n\nIf you build from source (at HEAD), you will by default use a better memory allocator that is less likely to run into these problems.  It will be in our next binary release, if you'd rather wait until then.\n", "Hey @vrv thanks for your help. I just built it from source and it did help some. Now, I can run a 1536 GRU layer at a batch size of 8 -- which is definitely an improvement! However, I can't run the desired 2048 unless I decrease the batch size to 4. \n\nI'll close this issue as things are certainly better. Hopefully, the gpu allocator can be further improved upon? It seems that multiple layers is great (I can use 6 GRU layers with 512 cells/layer with a batch size of 16). It just seems that these large layer sizes really sucks up memory compared to Theano. \n", "Cool, glad it helped at least a little.\n\n@rafaljozefowicz may have some other ideas too.   Memory improvements will certainly come as the project matures :)\n", "Could you try locating tf.gradients call in the model code and add a new argument: aggregation_method=1 (or 2 which might be better). This is an experimental feature that optimizes memory consumption of gradient updates for RNNs. I was able to train much larger models with this parameter. There is a chance it will slow down your code somewhat, though.\n", "@rafaljozefowicz I apologize for the late response. Thank you for letting me know about this! \n\nUnfortunately, I'm away from my computer because of the holidays, but on Sunday/Monday I will try this, and post back here with results. Again, I really do appreciate the help.\n\nIf it slows it down, that's okay. The main thing I'm hoping for is a GRU of 2048 cells with a batch size of hopefully 16. \n", "@rafaljozefowicz thank you for the help. Using `aggregation_method = 2` got me to a gru of 1536 with a batch size of 16 (which is double the size from what I had previously!). So this is a definite upgrade. \n\nI'm starting wonder if the softmax is sucking up alot of gpu memory. @vrv is there a way to see how much op is taking GPU memory? This would allow us to diagnose where GPU memory shortages are stemming from. \n", "@LeavesBreathe: not yet, but we're working on providing instrumentation so that it can be made available to end-users for debugging.\n", "That would be such a great upgrade. I'll close this thread, but I tested similar architecture in Theano\n\nTheano Max with 12gb: two GRU's with 2048 cells with a batch size of 256\n\nTensorFlow Max with 12gb: Two GRU's with 1536 cells with a batch size of 16\n", "Want to note, I have the same issue with current version on GTX 770 (translate.py)\n", "I think when the GPU runs out of its memory, the weights and maps storage is shifted onto the RAM. \nI observed this when I trained a 50 Layer ResNet on my GT 755m(running on 97%) laptop with 16GB(running on 32%) of RAM. Never faced any issues. \n", "In my case, the same, and I doubt if this Raising pool_size_limit_ will have effects on generator() ? My gosh....\n\n> jiapei:peijia$ python train.py \n> Using TensorFlow backend.\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.0.5 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\n> Loading folder c0...\n> Loading folder c1...\n> Reading training images...\n> /usr/local/lib/python3.5/dist-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint8\n>   \"%s to %s\" % (dtypeobj_in, dtypeobj))\n> Fold 1/(811,)\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n> name: GeForce GTX 980M\n> major: 5 minor: 2 memoryClockRate (GHz) 1.1265\n> pciBusID 0000:01:00.0\n> Total memory: 3.93GiB\n> Free memory: 3.50GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0)\n> Writing model architecture...\n> Starting Training...\n> fit...\n> Train on 1619 samples, validate on 811 samples\n> Epoch 1/32\n> 1024/1619 [=================>............] - ETA: 12s - loss: 0.8683 - acc: 0.8066I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3897 get requests, put_count=3885 evicted_count=1000 eviction_rate=0.2574 and unsatisfied allocation rate=0.285348\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n"]}, {"number": 351, "title": "Failed to load compute graph when executing label_image example", "body": "after cloning tensorflow on my OSX 10.10.4, I tried to run label_image example in tensorflow/examples/, and followed the instructions.\ncurrently seems fine after built even some warnings were occured but no error.\nbut when I typed 'bazel-bin/tensorflow/examples/label_image/label_image'\nit showed:\nE tensorflow/examples/label_image/main.cc:260] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/googlenet_graph.pb'\n\nI saw a file named tensorflow_inception_graph.pb in that folder, and I tried to rename it to googlenet_graph.pb and executed again, and it showed:\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/direct_session.cc:60] Direct session inter op parallelism threads: 4\nSegmentation fault: 11\n\nthen I don't know what I am supposed to try, please show me a way to solve it, thanks.\n", "comments": ["Have you seen the updated instructions from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md, and if so, do they still not help?\n", "yeah, actually I just cloned it today.\nthe only step different is wget, I downloaded it from browser and move it to the data folder, then I ran the unzip command below successfully.\n$ unzip tensorflow/examples/label_image/data/inception5h.zip -d tensorflow/examples/label_image/data/\n\nand two bazel command were ran, but still go wrong.\n", "Can you run it in gdb and paste a backtrace?\nOn Nov 25, 2015 4:25 PM, \"Sean\" notifications@github.com wrote:\n\n> yeah, actually I just cloned it today.\n> the only step different is wget, I downloaded it from browser and move it\n> to the data folder, then I ran the unzip command below successfully.\n> $ unzip tensorflow/examples/label_image/data/inception5h.zip -d\n> tensorflow/examples/label_image/data/\n> \n> and two bazel command were ran, but still go wrong.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/351#issuecomment-159764884\n> .\n", "After going through the new download process, it looks like the name of the file has changed from the expected one. I'll update the command-line flag default to point to the correct path here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc#L66\nWhen I run the following command passing the correct path with --graph:\n`bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb`\nI see the same SEGV fault, so it looks like there's something different between the Android graph file which we're now using here, and what the code here is expecting. I'll keep digging.\n", "It turns out there were two problems. I needed to update the default flag to point to the new labels file location, and I hadn't added error handling for the case when the labels file wasn't found, so it just crashed mysteriously when it tried to print the labels. We'll get that updated in our next push, but for now the workaround is to specify the correct labels file location on the command line. Here's one that I've tested:\n`bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb --labels=tensorflow/examples/label_image/data/imagenet_comp_graph_label_strings.txt`\n\nDoes that help?\n", "put graph and label files works for me now.\n", "@petewarden I run your command and it works well, thanks for helping me guys!\n", "same method i followed but i am getting error. the program is compiled successfully.\n![screenshot from 2015-12-10 02 19 25](https://cloud.githubusercontent.com/assets/10511526/11689044/0c07d79c-9eb6-11e5-9b39-fbee372d8fa5.png)\n", "The latest version of the imagenet graphdef changed the names of the input and output nodes.\n\n1) Fetch the latest source from git and it should work: we updated the source to use the new names.\n\n2) Add the following flags to your current build.\n\n --input_layer=Mul\n--output_layer=softmax\n", "Thanks. This solves my problem\n", "@vrv I have the same issue on MacOS X when running the java example\r\n\r\n```\r\n$ javac -cp libtensorflow-1.0.0-PREVIEW1.jar LabelImage.java\r\n$ java -cp libtensorflow-1.0.0-PREVIEW1.jar:. -Djava.library.path=./jni LabelImage\r\n$ java -cp libtensorflow-1.0.0-PREVIEW1.jar:. -Djava.library.path=./jni LabelImage ./ ./example-400x288.jpg \r\nFailed to read [./tensorflow_inception_graph.pb]: ./tensorflow_inception_graph.pb\r\n```\r\n\r\nI have the native dylib library and the inceptionv5 model in the path:\r\n\r\n```\r\n-rw-r--r--  1 loretoparisi  staff      2628 10 Mar 18:14 LabelImage$GraphBuilder.class\r\n-rw-r--r--  1 loretoparisi  staff      6849 10 Mar 18:14 LabelImage.class\r\n-rw-r--r--@ 1 loretoparisi  staff      7343 10 Mar 18:08 LabelImage.java\r\n-rw-r--r--  1 loretoparisi  staff       262 10 Mar 17:10 download.sh\r\n-rw-r--r--@ 1 loretoparisi  staff     35430 10 Mar 18:12 example-400x288.jpg\r\n-rw-r--r--@ 1 loretoparisi  staff  49937555 19 Nov  2015 inception5h.zip\r\ndrwxr-xr-x  4 loretoparisi  staff       136 10 Mar 17:10 jni\r\n-rw-r--r--@ 1 loretoparisi  staff     20314 10 Mar 16:58 libtensorflow-1.0.0-PREVIEW1.jar\r\n-rw-r--r--@ 1 loretoparisi  staff     20992 10 Mar 16:58 libtensorflow-src-1.0.0-PREVIEW1.jar\r\n-r-xr-xr-x@ 1 loretoparisi  staff  90199624  1 Gen  1970 libtensorflow_jni.dylib\r\n```", "Hi all and @vrv @petewarden \r\n\r\nI have installed Tensorflow gpu-support successfully, I test it with python example, it works fine, but I have problem when I test it with C++ example, I run \r\n\r\n`bazel build tensorflow/examples/label_image/...` this works fine\r\n\r\n`bazel-bin/tensorflow/examples/label_image/label_image` this give me the follwoing error \r\n\r\n`E tensorflow/examples/label_image/main.cc:285] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/tensorflow_inception_graph.pb'`\r\n\r\nI tried all your suggestions, but same error still there, please help me with it ?????\r\nI am using ubuntu 16.04, cuda 8, cudnn 5.1, Nvidia Geforce 1080\r\n\r\n```\r\njesse@jesse-System-Product-Name:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image\r\nE tensorflow/examples/label_image/main.cc:285] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/tensorflow_inception_graph.pb'\r\njesse@jesse-System-Product-Name:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb --labels=tensorflow/examples/label_image/data/imagenet_comp_graph_label_strings.txt\r\nE tensorflow/examples/label_image/main.cc:285] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/tensorflow_inception_graph.pb'\r\njesse@jesse-System-Product-Name:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image--input_layer=Mul\r\nbash: bazel-bin/tensorflow/examples/label_image/label_image--input_layer=Mul: No such file or directory\r\njesse@jesse-System-Product-Name:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image --input_layer=Mul\r\nE tensorflow/examples/label_image/main.cc:285] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/tensorflow_inception_graph.pb'\r\njesse@jesse-System-Product-Name:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb --labels=tensorflow/examples/label_image/data/imagenet_comp_graph_label_strings.txt--input_layer=Mul\r\nE tensorflow/examples/label_image/main.cc:285] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/tensorflow_inception_graph.pb'\r\njesse@jesse-System-Product-Name:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb --labels=tensorflow/examples/label_image/data/imagenet_comp_graph_label_strings.txt--input_layer=Mul --output_layer=softmax\r\nE tensorflow/examples/label_image/main.cc:285] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/tensorflow_inception_graph.pb'\r\njesse@jesse-System-Product-Name:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/tensorflow_inception_graph.pb --labels=tensorflow/examples/label_image/data/imagenet_comp_graph_label_strings.txt--input_layer=Mul--output_layer=softmax\r\nE tensorflow/examples/label_image/main.cc:285] Not found: Failed to load compute graph at 'tensorflow/examples/label_image/data/tensorflow_inception_graph.pb'\r\n```\r\n", "Sorry about that I just figured it out, the issue was, I have run\r\n\r\n```\r\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\r\n  tar -C tensorflow/examples/label_image/data -xz\r\n```\r\nwhich is an old version of the downloaded model, then I used the new following one.\r\n\r\n```\r\nwget https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip -O tensorflow/examples/label_image/data/inception_dec_2015.zip\r\n\r\nunzip tensorflow/examples/label_image/data/inception_dec_2015.zip -d tensorflow/examples/label_image/data/\r\n```\r\nThis works fine even without `-input_layer=Mul --output_layer=softmax` I run only this `bazel-bin/tensorflow/examples/label_image/label_image`\r\n\r\nbut I don't know why I could not run the old version, I don't even know why I got `military uniform (866): 0.684116` score only 0.6 not 0.843 as mention in the example", "@Abduoit i'm getting the same couldn't load graph error:\r\n\r\n@wing-artemis:~/tensorflow$ bazel build tensorflow/examples/label_imageINFO: Analysed target tensorflow/examples/label_image:label_image (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/examples/label_image:label_image up-to-date:\r\n  bazel-bin/tensorflow/examples/label_image/label_image\r\nINFO: Elapsed time: 16.417s, Critical Path: 15.79s\r\nINFO: 2 processes: 2 local.\r\nINFO: Build completed successfully, 3 total actions\r\n@wing-artemis:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image --graph=tensorflow/examples/label_image/data/frozen_inference_graph.pb --labels=tensorflow/examples/label_image/data/object_detection.txt\r\nFine to this point in main.\r\n2018-07-05 15:39:24.407219: E tensorflow/examples/label_image/main.cc:221] graph_path:~/tensorflow/tensorflow/examples/label_image/data/frozen_inference_graph.pb\r\n2018-07-05 15:39:24.407630: E tensorflow/examples/label_image/main.cc:224] LoadGraph ERROR!!!!Not found: Failed to load compute graph at '~/tensorflow/tensorflow/examples/label_image/data/frozen_inference_graph.pb'\r\n", "Not sure why it's striking through that comment above either. "]}, {"number": 350, "title": "CPU Version install successfull,but import tensorflow error", "body": "Python 2.7.3 (default, Jul 29 2015, 13:52:45) \n[GCC 3.4.5 20051201 (Red Hat 3.4.5-2)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/**init**.py\", line 4, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 22, in <module>\n> > >     from tensorflow.python.client.client_lib import *\n> > >   File \"/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n> > >     from tensorflow.python.client.session import InteractiveSession\n> > >   File \"/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n> > >     from tensorflow.python import pywrap_tensorflow as tf_session\n> > >   File \"/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n> > >     _pywrap_tensorflow = swig_import_helper()\n> > >   File \"/home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n> > >     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n> > > ImportError: /home/users/caohao/.jumbo/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: ELF file OS ABI invalid\n", "comments": ["install pip from source\uff1bthe error always exiting\n", "What do you get from the output of `uname -a` ?\n\n(We only support 64-bit platforms at the moment).\n", "uname -a output\uff1a\n2.6.32_1-17-0-0 #1 SMP Mon Aug 24 11:14:27 CST 2015 x86_64 GNU/Linux\n", "Do you know what version of glibc you have installed on your machine?  (I think `ldd --version` might tell you).\n\nI think we require something later than 2.14 or 2.17...\n", "ldd --version\nldd () 2.18\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nWritten by Roland McGrath and Ulrich Drepper\n", "Hmm :(.  All the info online suggests some kind of glibc version skew.  You might want to try the instructions to install from source in the meantime.\n", "Estimados,\n\nHay algun site o documentaci\u00f3n donde pueda ver la funcionalidad de\nTensorFlow, estoy iniciando mi conocimiento en esta herramienta. Me\ngustar\u00eda conocer:\n1.- su funcionalidad, para identificar su aplicaci\u00f3n en mis proyectos.\n2.- su instalaci\u00f3n e implementaci\u00f3n\n\nGracias,\nV+\n\nUn abrazo,\nV+\n\n2015-11-25 23:49 GMT-05:00 Vijay Vasudevan notifications@github.com:\n\n> Hmm :(. All the info online suggests some kind of glibc version skew. You\n> might want to try the instructions to install from source in the meantime.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/350#issuecomment-159804202\n> .\n", "Is this still an issue with the new release?\n", "I just tried with version 0.7.0 for Red Hat 4.4.7-1.\n\n`pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.0-py3-none-linux_x86_64.whl --no-deps`\n\nGot similiar `ELF file OS ABI invalid` error. :( \n", "Python 3.3.2 (default, Aug 14 2014, 14:25:52)  _(tried with python 2.7, 3, etc)_\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > > ImportError: No module named 'tensorflow'\n", "Closing due to likely GLIBC issues.  \n"]}, {"number": 349, "title": "Building from source, gcc issues", "body": "Hi everyone,\n\nI'm trying to build tensorflow from source but got stucked at the following:\n\n```\nroot@ubuntu:/tensorflow# ../bazel/output/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/kernels/cwise_op_add.cc:\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\nERROR: /tensorflow/tensorflow/core/BUILD:210:1: C++ compilation of rule '//tensorflow/core:kernels' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 72 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 126.561s, Critical Path: 123.83s\n```\n\n`gcc` should be fine:\n\n```\nroot@ubuntu:/tensorflow# gcc -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\nThread model: posix\ngcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) \n```\n\nEvery time i execute `../bazel/output/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package` i get the same:\n\n```\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\n[810 / 839] Still waiting for 27 jobs to complete:\n      Running (spawn):\n        Compiling tensorflow/core/kernels/transpose_op.cc, 10 s\n        Compiling tensorflow/core/kernels/cwise_op_add.cc, 10 s\n        Compiling tensorflow/core/kernels/slice_op.cc, 10 s\n        Compiling tensorflow/core/kernels/cwise_op_equal_to.cc, 10 s\n      Scheduling:\n        Compiling tensorflow/core/kernels/argmax_op.cc, 10 s\n        Compiling tensorflow/core/kernels/cwise_op_sub.cc, 10 s\n        Compiling tensorflow/core/kernels/reduction_ops_mean.cc, 10 s\n        Compiling tensorflow/core/kernels/reduction_ops_prod.cc, 10 s\n        Compiling tensorflow/core/kernels/matmul_op.cc, 10 s\n        Compiling tensorflow/core/kernels/reverse_sequence_op.cc, 10 s\n        Compiling tensorflow/core/kernels/segment_reduction_ops.cc, 10 s\n        Compiling tensorflow/core/kernels/aggregate_ops.cc, 10 s\n        Compiling tensorflow/core/kernels/cwise_op_mul.cc, 10 s\n        ... 14 more jobs\n\n```\n\nThose 27 missing packages never seem to finish. Any advice here?\n", "comments": ["You are likely running out of memory. Try reducing number of parallel builds by passing '--local_resources  2048,.5,1.0', which would instruct bazel to spawn no more than one compiler process at the time.\n", "That did it. Thanks @malfet!\n", "Thanks @malfet and @vmayoral \ud83d\udc4d \n", "An other solution is to limit the maximum jobs bazel should build. For example:\r\n\r\n```shell\r\nbazel build -c opt --jobs 1 --local_resources 2048,0.5,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```", "Thanks @malfet @dat-ai ", "> You are likely running out of memory. Try reducing number of parallel builds by passing '--local_resources 2048,.5,1.0', which would instruct bazel to spawn no more than one compiler process at the time.\r\n\r\nWorth noting that`--local_resources` refers to:\r\n\r\n> comma-separated available amount of RAM (in MB), CPU (in cores) and available I/O (1.0 being average workstation\r\n\r\nvia https://docs.bazel.build/versions/master/command-line-reference.html#flag--local_resources", "> You are likely running out of memory. Try reducing number of parallel builds by passing '--local_resources 2048,.5,1.0', which would instruct bazel to spawn no more than one compiler process at the time.\r\n\r\nThank you very much, bazel is a terrible build system https://archive.fosdem.org/2018/schedule/event/how_to_make_package_managers_cry/ "]}, {"number": 348, "title": "error: invalid command 'bdist_wheel'", "body": "ub1404@ub1404-A:~/github/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nWed Nov 25 02:32:35 EST 2015 : === Using tmpdir: /tmp/tmp.lVOy4pRiQ5\n/tmp/tmp.lVOy4pRiQ5 ~/github/tensorflow\nWed Nov 25 02:32:35 EST 2015 : === Building wheel\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: setup.py --help [cmd1 cmd2 ...]\n   or: setup.py --help-commands\n   or: setup.py cmd --help\n\nerror: invalid command 'bdist_wheel'\n", "comments": ["pip install wheel  \n", "ub1404@ub1404-A:~/github/tensorflow$ sudo pip install wheel\n[sudo] password for ub1404: \nThe directory '/home/ub1404/.cache/pip/log' or its parent directory is not owned by the current user and the debug log has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want the -H flag.\nThe directory '/home/ub1404/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want the -H flag.\nYou are using pip version 6.0.8, however version 7.1.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nThe directory '/home/ub1404/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want the -H flag.\nCollecting wheel\n  Downloading wheel-0.26.0-py2.py3-none-any.whl (63kB)\n    100% |################################| 65kB 837kB/s \nInstalling collected packages: wheel\n\nSuccessfully installed wheel-0.26.0\nub1404@ub1404-A:~/github/tensorflow$ \nub1404@ub1404-A:~/github/tensorflow$ \nub1404@ub1404-A:~/github/tensorflow$ sudo bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nWed Nov 25 03:06:54 EST 2015 : === Using tmpdir: /tmp/tmp.6aZe9fEjxd\n/tmp/tmp.6aZe9fEjxd ~/github/tensorflow\nWed Nov 25 03:06:54 EST 2015 : === Building wheel\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: setup.py --help [cmd1 cmd2 ...]\n   or: setup.py --help-commands\n   or: setup.py cmd --help\n\nerror: invalid command 'bdist_wheel'\nub1404@ub1404-A:~/github/tensorflow$ \n", "pip install wheel  can sovle this issue.\n\nI used pip2 install wheel since my installed both py2 and py3\n", "I encountered the same problem and worked it out. It seems that python can't find wheel module.\r\nDoes you use pyenv-virtualenv? you should install wheel on global python\r\n\r\n```\r\npyenv global 3.6.0\r\npip3 install wheel\r\n```", "anyone still having this problem? I have wheel installed in both pip2 and pip3, still the command fails\r\n\r\n```\r\npablo@batman tensorflow (master) $ pip2 list\r\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\r\ncertifi (2017.7.27.1)\r\nchardet (3.0.4)\r\ncryptop (0.1.0)\r\nidna (2.5)\r\nmercurial (4.3.1)\r\nnumpy (1.13.1)\r\npip (9.0.1)\r\nrequests (2.18.2)\r\nsetuptools (32.1.0)\r\nsix (1.10.0)\r\nurllib3 (1.22)\r\nwheel (0.29.0)\r\nyasm (0.0)\r\npablo@batman tensorflow (master) $ pip3 list\r\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\r\npip (9.0.1)\r\nsetuptools (32.2.0)\r\nvirtualenv (15.1.0)\r\nwheel (0.29.0)\r\npablo@batman tensorflow (master) $ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nTue Sep 12 21:52:39 -03 2017 : === Using tmpdir: /var/folders/p2/1n2xjbq93hn4sd7lgj9mv4c80000gn/T/tmp.XXXXXXXXXX.TBZdyy7q\r\n~/Documents/work/projects/forks/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/Documents/work/projects/forks/tensorflow\r\n~/Documents/work/projects/forks/tensorflow\r\n/var/folders/p2/1n2xjbq93hn4sd7lgj9mv4c80000gn/T/tmp.XXXXXXXXXX.TBZdyy7q ~/Documents/work/projects/forks/tensorflow\r\nTue Sep 12 21:52:41 -03 2017 : === Building wheel\r\nusage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\r\n   or: setup.py --help [cmd1 cmd2 ...]\r\n   or: setup.py --help-commands\r\n   or: setup.py cmd --help\r\n\r\nerror: invalid command 'bdist_wheel'\r\n```", "I was having this problem and for me it was not sufficient to simply execute \"`sudo pip install wheel`\".  \r\n\r\nTo investigate further I looked into the bazel-bin/tensorflow/tools/pip_package/build_pip_package script to see how it was loading python, where I found these lines:\r\n\r\n```\r\n# Before we leave the top-level directory, make sure we know how to\r\n# call python.\r\nsource tools/python_bin_path.sh\r\n```\r\nThe shell script sourced there is the one liner:  \r\n\r\nexport PYTHON_BIN_PATH=\"/Library/Frameworks/Python.framework/Versions/2.7/bin/python\"\r\n\r\nWhich is the python version I selected when configuring my tensorflow build.  \r\n\r\nThe pip on my path, however, was for python 3.5, which you can see by calling `pip --version` or `which pip`.   So my calls to `sudo pip install wheel` were only installing to the 3.5 version.  The solution for me was to explicitly call the version of pip for the version/install of python you are building tensorflow with.  In my case, that was:\r\n\r\n`sudo /Library/Frameworks/Python.framework/Versions/2.7/bin/pip install wheel`\r\n\r\nAfter that, the build_pip_package script worked for me.", "Maybe use sudo chown you_user_name /path/to/project/evn/* -R  \r\nit was good for me!", "if you're on ubuntu and it doesn't work, see this:\r\n\r\nhttps://stackoverflow.com/a/59596814/2353085\r\n\r\n> sudo apt-get install gcc libpq-dev -y\r\n> sudo apt-get install python-dev  python-pip -y\r\n> sudo apt-get install python3-dev python3-pip python3-venv python3-wheel -y\r\n> pip3 install wheel\r\n", "@relh \r\n\r\nI builded a script to have various environments in one project DEV, QA, etc, in each run, I delete the .temp folder, where is the venv for each environment, and I get the error error: `invalid command 'bdist_wheel'`, \u00a0I realize that the error disappears if I don't delete the folder, but then I add the install `pip3 install wheel` before install the other libs and that\u00a0resolved the problem too.\u00a0 \r\n\r\n\r\n```\r\n#!/bin/bash\r\n\r\n# Time tracker Start\r\necho START: `date`\r\n\r\n# ..:: DEV\r\n\r\nENVIRONMENT='dev'\r\n\r\nENVIRONMENT_DIR=\"/${ENVIRONMENT}\"\r\n\r\nENVIRONMENTS_DIR=\"/.temp\"\r\n\r\nPROJECT_DIR=\"/project\"\r\n\r\nPWD=\"$(pwd)\"\r\n\r\n# Define environment dependencies FOLDER\r\nVENV_DIR=\"${PWD}${ENVIRONMENTS_DIR}${ENVIRONMENT_DIR}\"\r\n\r\n# Clear environment dependencies FOLDER\r\nif [ -d \"$VENV_DIR\" ]; then\r\n    rm -rf $VENV_DIR\r\nfi\r\n\r\n# Create venv directory\r\npython3.6 -m venv $VENV_DIR\r\n\r\n# Activate venv directory\r\n.  .${ENVIRONMENTS_DIR}${ENVIRONMENT_DIR}/bin/activate\r\n\r\n# https://github.com/tensorflow/tensorflow/issues/348\r\n# Issue ref: https://stackoverflow.com/a/59596814/2513972\r\npip3.6 install wheel\r\n\r\n# Install requirements\r\npip3.6 install -r \"requirements_${ENVIRONMENT}.txt\"\r\n\r\n\r\nexport PYTHONPATH=\"${PYTHONPATH}:${PWD}${PROJECT_DIR}\"\r\n\r\n\r\n# Time tracker End\r\necho END: `date`\r\n\r\n```\r\n\r\nrepo: https://github.com/romelgomez/python-project-with-various-environments-example\r\n"]}, {"number": 347, "title": "\"constrastive\" in the document", "body": "In tensorflow/g3doc/tutorials/word2vec/index.md:\n\n> practice we approximate the expectation by drawing \\(k\\) constrastive words\n> from the noise distribution (i.e. we compute a\n\nI think \"constrastive\" would be typo, \"contrastive\" is correct?\n", "comments": ["Correct. Thanks!\n"]}, {"number": 345, "title": "conv_grad_ops.cc:623:46: error: invalid use of 'auto'", "body": "I'm getting a compile error when trying to compile the `example_trainer` using gcc 4.8.1:\n\n```\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer  --verbose_failures \n____From Compiling tensorflow/core/kernels/conv_grad_ops.cc:\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]\n::__lambda3':\ntensorflow/core/kernels/conv_grad_ops.cc:621:22:   required from 'struct tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; \nT = float]::__lambda3'\ntensorflow/core/kernels/conv_grad_ops.cc:632:7:   required from 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T =\n float]'\ntensorflow/core/kernels/conv_grad_ops.cc:1265:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__col_buffer_data' before deduction of 'auto'\n                     &size_A](int64 start, int64 limit) {\n                                                        ^\ntensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__col_buffer_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__input_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:621:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda3::__input_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:623:46: error: use of 'input_data' before deduction of 'auto'\n           auto input_data_shard = input_data + shard_id * input_offset;\n                                              ^\ntensorflow/core/kernels/conv_grad_ops.cc:623:46: error: invalid use of 'auto'\n```\n", "comments": ["It seems the compile error was introduced recently. It goes away when I downgrade to an older version; this version was still OK:\n\n```\ncommit 011e9baccd343eb943d25014c4e8aec53eac396b\nAuthor: Vijay Vasudevan <vrv@google.com>\nDate:   Thu Nov 12 18:34:45 2015 -0800\n```\n", "Thanks, we're looking at it now...\n", "This was fixed some time last week.\n", "I still see the issue: \n\nFrom Compiling tensorflow/core/kernels/conv_grad_ops.cc:\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4':\ntensorflow/core/kernels/conv_grad_ops.cc:705:22:   required from 'struct tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::**lambda4'\ntensorflow/core/kernels/conv_grad_ops.cc:716:7:   required from 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]'\ntensorflow/core/kernels/conv_grad_ops.cc:1368:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:705:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__col_buffer_data' before deduction of 'auto'\n                     &size_A](int64 start, int64 limit) {\n                                                        ^\ntensorflow/core/kernels/conv_grad_ops.cc:705:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__col_buffer_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:705:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__input_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:705:56: error: use of 'tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda4::__input_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:707:50: error: use of 'input_data' before deduction of 'auto'\n           const T_ input_data_shard = input_data + shard_id \\* input_offset;\n                                                  ^\ntensorflow/core/kernels/conv_grad_ops.cc:707:50: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:707:63: error: cannot convert 'auto_' to 'const float_' in initialization\n           const T\\* input_data_shard = input_data + shard_id \\* input_offset;\n                                                               ^\ntensorflow/core/kernels/conv_grad_ops.cc:708:47: error: use of 'col_buffer_data' before deduction of 'auto'\n           T\\* col_data_shard = col_buffer_data + shard_id \\* size_A;\n                                               ^\ntensorflow/core/kernels/conv_grad_ops.cc:708:47: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:708:60: error: cannot convert 'auto_' to 'float_' in initialization\n           T\\* col_data_shard = col_buffer_data + shard_id \\* size_A;\n                                                            ^\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'void tensorflow::Conv2DCustomBackpropFilterOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]':\ntensorflow/core/kernels/conv_grad_ops.cc:1368:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:702:20: error: invalid initialization of reference of type 'auto_&' from expression of type 'const float_'\n       auto shard = [&input_data, &col_buffer_data, &in_depth, &input_rows,\n                    ^\ntensorflow/core/kernels/conv_grad_ops.cc:702:20: error: invalid initialization of reference of type 'auto_&' from expression of type 'float_'\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0':\ntensorflow/core/kernels/conv_grad_ops.cc:487:24:   required from 'struct tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0'\ntensorflow/core/kernels/conv_grad_ops.cc:505:9:   required from 'void tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]'\ntensorflow/core/kernels/conv_grad_ops.cc:1368:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__filter_data' before deduction of 'auto'\n                       &size_C](int64 start, int64 limit) {\n                                                          ^\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__filter_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__out_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__out_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__col_buffer_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__col_buffer_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__input_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:487:58: error: use of 'tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]::__lambda0::__input_backprop_data' before deduction of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:489:45: error: use of 'col_buffer_data' before deduction of 'auto'\n             T\\* im2col_buf = col_buffer_data + shard_id \\* size_C;\n                                             ^\ntensorflow/core/kernels/conv_grad_ops.cc:489:45: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:489:58: error: cannot convert 'auto_' to 'float_' in initialization\n             T\\* im2col_buf = col_buffer_data + shard_id \\* size_C;\n                                                          ^\ntensorflow/core/kernels/conv_grad_ops.cc:490:49: error: use of 'input_backprop_data' before deduction of 'auto'\n             T\\* input_data = input_backprop_data + shard_id \\* input_offset;\n                                                 ^\ntensorflow/core/kernels/conv_grad_ops.cc:490:49: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:490:62: error: cannot convert 'auto_' to 'float_' in initialization\n             T\\* input_data = input_backprop_data + shard_id \\* input_offset;\n                                                              ^\ntensorflow/core/kernels/conv_grad_ops.cc:491:51: error: use of 'out_backprop_data' before deduction of 'auto'\n             const T\\* out_data = out_backprop_data + shard_id \\* output_offset;\n                                                   ^\ntensorflow/core/kernels/conv_grad_ops.cc:491:51: error: invalid use of 'auto'\ntensorflow/core/kernels/conv_grad_ops.cc:491:64: error: cannot convert 'auto_' to 'const float_' in initialization\n             const T\\* out_data = out_backprop_data + shard_id \\* output_offset;\n                                                                ^\ntensorflow/core/kernels/conv_grad_ops.cc:497:71: error: use of 'filter_data' before deduction of 'auto'\n             ConstMatrixMap B(filter_data, filter_total_size, out_depth);\n                                                                       ^\ntensorflow/core/kernels/conv_grad_ops.cc:497:71: error: no matching function for call to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::Map(auto_&, const int&, const int64&)'\ntensorflow/core/kernels/conv_grad_ops.cc:497:71: note: candidates are:\nIn file included from third_party/eigen3/Eigen/Core:411:0,\n                 from third_party/eigen3/unsupported/Eigen/CXX11/Core:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./tensorflow/core/public/tensor.h:19,\n                 from ./tensorflow/core/framework/device_base.h:28,\n                 from ./tensorflow/core/framework/op_kernel.h:24,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:21:\nthird_party/eigen3/Eigen/src/Core/Map.h:170:12: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, Eigen::Map<MatrixType, MapOptions, StrideType>::Index, Eigen::Map<MatrixType, MapOptions, StrideType>::Index, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float_; Eigen::Map<MatrixType, MapOptions, StrideType>::Index = long int]\n     inline Map(PointerArgType dataPtr, Index nbRows, Index nbCols, const StrideType& a_stride = StrideType())\n            ^\nthird_party/eigen3/Eigen/src/Core/Map.h:170:12: note:   no known conversion for argument 1 from 'auto_' to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::PointerArgType {aka const float_}'\nthird_party/eigen3/Eigen/src/Core/Map.h:156:12: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, Eigen::Map<MatrixType, MapOptions, StrideType>::Index, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float_; Eigen::Map<MatrixType, MapOptions, StrideType>::Index = long int]\n     inline Map(PointerArgType dataPtr, Index a_size, const StrideType& a_stride = StrideType())\n            ^\nthird_party/eigen3/Eigen/src/Core/Map.h:156:12: note:   no known conversion for argument 1 from 'auto_' to 'Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::PointerArgType {aka const float_}'\nthird_party/eigen3/Eigen/src/Core/Map.h:143:12: note: Eigen::Map<MatrixType, MapOptions, StrideType>::Map(Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType, const StrideType&) [with PlainObjectType = const Eigen::Matrix<float, -1, -1, 1, -1, -1>; int MapOptions = 0; StrideType = Eigen::Stride<0, 0>; Eigen::Map<MatrixType, MapOptions, StrideType>::PointerArgType = const float_]\n     inline Map(PointerArgType dataPtr, const StrideType& a_stride = StrideType())\n            ^\nthird_party/eigen3/Eigen/src/Core/Map.h:143:12: note:   candidate expects 2 arguments, 3 provided\nthird_party/eigen3/Eigen/src/Core/Map.h:104:79: note: Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::Map(const Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&)\n template<typename PlainObjectType, int MapOptions, typename StrideType> class Map\n                                                                               ^\nthird_party/eigen3/Eigen/src/Core/Map.h:104:79: note:   candidate expects 1 argument, 3 provided\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'void tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice; T = float]':\ntensorflow/core/kernels/conv_grad_ops.cc:1368:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:481:22: error: invalid initialization of reference of type 'auto_&' from expression of type 'float_'\n         auto shard = [&in_depth, &input_rows, &input_cols, &filter_rows,\n                      ^\ntensorflow/core/kernels/conv_grad_ops.cc:481:22: error: invalid initialization of reference of type 'auto_&' from expression of type 'float_'\ntensorflow/core/kernels/conv_grad_ops.cc:481:22: error: invalid initialization of reference of type 'auto_&' from expression of type 'const float_'\ntensorflow/core/kernels/conv_grad_ops.cc:481:22: error: invalid initialization of reference of type 'auto_&' from expression of type 'const float_'\nIn file included from ./tensorflow/core/framework/op_kernel.h:21:0,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:21:\n./tensorflow/core/framework/allocator.h: In member function 'virtual size_t tensorflow::Allocator::RequestedSize(void_)':\n./tensorflow/core/framework/allocator.h:86:3: warning: control reaches end of non-void function [-Wreturn-type]\n   }\n   ^\nIn file included from ./tensorflow/core/framework/op_kernel.h:24:0,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:21:\n./tensorflow/core/framework/device_base.h: In member function 'virtual tensorflow::Allocator\\* tensorflow::DeviceBase::GetAllocator(tensorflow::AllocatorAttributes)':\n./tensorflow/core/framework/device_base.h:146:3: warning: control reaches end of non-void function [-Wreturn-type]\n   }\n   ^\n./tensorflow/core/framework/device_base.h: In member function 'virtual const tensorflow::DeviceAttributes& tensorflow::DeviceBase::attributes() const':\n./tensorflow/core/framework/device_base.h:164:3: warning: control reaches end of non-void function [-Wreturn-type]\n   }\n   ^\nERROR: /home/local/ANT/arpgup/tensorflow/tensorflow/tensorflow.bzl:226:3: C++ compilation of rule '//tensorflow/core:kernels' failed: gcc failed: error executing command\n  (cd /home/local/ANT/arpgup/.cache/bazel/_bazel_arpgup/1d518f226ad034073e38f75799c6eef6/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/bin/X11:/usr/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem tools/cpp/gcc3 -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE**=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.d -fPIC -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command\n  (cd /home/local/ANT/arpgup/.cache/bazel/_bazel_arpgup/1d518f226ad034073e38f75799c6eef6/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/bin/X11:/usr/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem tools/cpp/gcc3 -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -pthread -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.d -fPIC -c tensorflow/core/kernels/conv_grad_ops.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/kernels/tensorflow/core/kernels/conv_grad_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n", "This is still an issue, I pulled from master today:\n\nhttps://travis-ci.org/peterbraden/node-tensorflow/builds/108240579\n\n```\nexternal/eigen_archive/eigen-eigen-fa22401ededc/Eigen/src/Core/Map.h:88:79: note: Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >::Map(const Eigen::Map<const Eigen::Matrix<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&)\n template<typename PlainObjectType, int MapOptions, typename StrideType> class Map\n                                                                               ^\nexternal/eigen_archive/eigen-eigen-fa22401ededc/Eigen/src/Core/Map.h:88:79: note:   candidate expects 1 argument, 3 provided\ntensorflow/core/kernels/conv_grad_ops.cc: In instantiation of 'void tensorflow::Conv2DCustomBackpropInputOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':\ntensorflow/core/kernels/conv_grad_ops.cc:1422:1:   required from here\ntensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'\n         auto shard = [&in_depth, &input_rows, &input_cols, &filter_rows,\n                      ^\ntensorflow/core/kernels/conv_grad_ops.cc:490:22: error: invalid initialization of reference of type 'auto*&' from expression of type 'float*'\n```\n"]}, {"number": 344, "title": "Dropout Appears to Lose Tensor Shape", "body": "While training a model, I encountered a strange issue around dropout.  I can't yet rule out user error and haven't yet produced the minimum viable product, but I wanted to get this on the board before I forgot.\n\n```\n# With dropout...\n        wc1 = tf.Variable(tf.random_normal([1, sample_vec_len, 1, 64]))\n        bc1 = tf.Variable(tf.random_normal([64,]))\n        conv1 = tf.nn.conv2d(inputs, wc1, strides=[1, 1, 1, 1], padding='SAME') + bc1\n        act1 = tf.nn.relu(conv1)\n        drop1 = tf.nn.dropout(act1, keep_prob=dropout_toggle)\n        # TensorShape([Dimension(None), Dimension(1), Dimension(4620), Dimension(64)])\n\n        wc2 = tf.Variable(tf.random_normal([1, char_sample_size, 64, 32]))\n        bc2 = tf.Variable(tf.random_normal([32,]))\n        conv2 = tf.nn.conv2d(drop1, wc2, strides=[1, 1, 1, 1], padding='SAME') + bc2\n        act2 = tf.nn.relu(conv2)\n        norm2 = tf.nn.lrn(act2, bitreader.get_sentence_vector_length(1), bias=1.0, alpha=0.001, beta=0.75)\n        # TensorShape([Dimension(None), Dimension(1), Dimension(4620), Dimension(32)])\n\n        # Conv -> FC\n        dims = act2.get_shape()\n        shape = [dims[0].value, dims[1].value, dims[2].value, dims[3].value] # dims[0].value -> None\n        c_fc = tf.reshape(act2, [-1, shape[1]*shape[2]*shape[3]])\n```\n\n(venv)jcatrambone-osx:nlp jcatrambone$ python ./nlp_tensorflow.py ../wikipedia_utf8_filtered_20pageviews.csv.gz 1\nBuilding model.\nTraceback (most recent call last):\n  File \"./nlp_tensorflow.py\", line 92, in <module>\n    encoder, decoder, weights, biases = build_model(\"ConvNLP\", x, keep_prob)\n  File \"./nlp_tensorflow.py\", line 56, in build_model\n    c_fc = tf.reshape(act2, [-1, shape[1]_shape[2]_shape[3]])\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\n\n```\n# Remove dropout...\n        wc1 = tf.Variable(tf.random_normal([1, sample_vec_len, 1, 64]))\n        bc1 = tf.Variable(tf.random_normal([64,]))\n        conv1 = tf.nn.conv2d(inputs, wc1, strides=[1, 1, 1, 1], padding='SAME') + bc1\n        act1 = tf.nn.relu(conv1)\n        # TensorShape([Dimension(None), Dimension(1), Dimension(4620), Dimension(64)])\n\n        wc2 = tf.Variable(tf.random_normal([1, char_sample_size, 64, 32]))\n        bc2 = tf.Variable(tf.random_normal([32,]))\n        conv2 = tf.nn.conv2d(act1, wc2, strides=[1, 1, 1, 1], padding='SAME') + bc2\n        act2 = tf.nn.relu(conv2)\n        norm2 = tf.nn.lrn(act2, bitreader.get_sentence_vector_length(1), bias=1.0, alpha=0.001, beta=0.75)\n        # TensorShape([Dimension(None), Dimension(1), Dimension(4620), Dimension(32)])\n\n        # Conv -> FC\n        dims = act2.get_shape()\n        shape = [dims[0].value, dims[1].value, dims[2].value, dims[3].value] # dims[0].value -> None\n        c_fc = tf.reshape(act2, [-1, shape[1]*shape[2]*shape[3]])\n```\n\n(venv)jcatrambone-osx:nlp jcatrambone$ python ./nlp_tensorflow.py ../wikipedia_utf8_filtered_20pageviews.csv.gz 1\nBuilding model.\nDefining loss functions and optimizer.\nGathering variables.\nBeginning training session.\ncan't determine number of CPU cores: assuming 4\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\ncan't determine number of CPU cores: assuming 4\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\nInitializing variables.\nSession, variables, and generator initialized.  Training.\n\nRuns fine...\n", "comments": ["Did you use the binary install?  I think we fixed a bug in dropout shape inference, so it is fixed in the source. \n\nI'll de-dupe with https://github.com/tensorflow/tensorflow/issues/184, since it is likely related.\n", "Yup.  Binary install on all my machines.  I'll do a pull and verify the fix.  Thanks for the help and sorry for the noise -- I really did search for (open) issues before posting.  :)\n", "Oh, had the same issue. Thanks\n"]}, {"number": 343, "title": "failed installing using virtualenv", "body": "(tensorflow)itay@ubuntu:~/tensorflow$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n  Downloading tensorflow-0.5.0-cp27-none-linux_x86_64.whl (10.9Mb): 10.9Mb downloaded\n  Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n    Traceback (most recent call last):\n      File \"<string>\", line 14, in <module>\n    IOError: [Errno 2] No such file or directory: '/tmp/pip-2LuRsH-build/setup.py'\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n\n  File \"<string>\", line 14, in <module>\n\nIOError: [Errno 2] No such file or directory: '/tmp/pip-2LuRsH-build/setup.py'\n\n---\n\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-2LuRsH-build\n\nany help?\n", "comments": ["http://www.tensorflow.org/get_started/os_setup.html#pip-installation-issues ?  Let us know if those instructions don't help\n", "they don't help , that's why I opened an issue.\nI was able to run it using docker.\n"]}, {"number": 342, "title": "Feed sparse array for placeholder ?", "body": "X = tf.placeholder(\"float\", [None, num_features]) # create symbolic variables\nY = tf.placeholder(\"float\", [None, 1])\n\npredicts = sess.run(predict_op, feed_dict={X: teX, Y: teY})\n\nWhat if I'am facing text classification problem, where every instance is extremely sparse, \nlike using scipy.sparse.csr_matrix or somethint else, what's the best to do this ?\n", "comments": ["It seems tesnsorflow right now does not support  general sparse matrix yet. Will probably have to convert each mini-batch data into dense matrix but that will make matrix vector operation much slower..\n", "You can probably construct a tf.SparseTensor from three placeholders (for the values, indices, and shape) and then feed dense numpy arrays for each of those.\nhttp://www.tensorflow.org/api_docs/python/sparse_ops.html#sparse-tensors\n", "@georgedahl   But seems many ops only support input to be tensor not sparse tensor. \nLike below\nh = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions\n\nreturn tf.matmul(h, w_o) \n\nIs it possible to pass sparse tensor ?\n", "It depends on what you are trying to do. If you have a sparse-dense product with sparse indicator variables tf.gather might be able to do what you want. Is w_h dense in your example?\n", "@georgedahl \nX is sparse,w_h, w_o is dense\n\nX = tf.placeholder(\"float\", [None, num_features]) # create symbolic variables\n\nY = tf.placeholder(\"float\", [None, 1])\n\nw_h = init_weights([num_features, hidden_size]) # create symbolic variables\n\nw_o = init_weights([hidden_size, 1])\n\ndef model(X, w_h, w_o):\n\nh = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions\n\nreturn tf.matmul(h, w_o) # note that we dont take the softmax at the end because our cost fn does that for us\n", "We have SparseTensor and embedding lookup ops that with with SparseTensor.\nCan you clarify your exact use case?\nOn Nov 25, 2015 5:15 PM, \"chenghuige\" notifications@github.com wrote:\n\n> It seems tesnsorflow right now does not support general sparse matrix yet.\n> Will probably have to convert each mini-batch data into dense matrix but\n> that will make matrix vector operation much slower..\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/342#issuecomment-159772556\n> .\n", "@ebrevdo  I want to use tensorflow doing text classification experiment. \nSay I may use the mini batch sgd implementation logistic regression for text classification.\nThen each mini batch is sparse matrix, like for each instance , will contain 400k features(where only 100 will be non zero), so for matmul(X,w) operation, X(mini batch input matrix) is sparse, w(weight vector) is dense. \n\nI find the code comment , seems only accept tensor nor sparse tensor, it has parameter a_is_sparse,\nbut I'm not sure how to use ? convert sparse mini batch to dense matrix(tensor), then set (a_is_sparse=True)\uff1f\ndef matmul(a, b,\n           transpose_a=False, transpose_b=False,\n           a_is_sparse=False, b_is_sparse=False,\n           name=None):\n  \"\"\"Multiplies matrix `a` by matrix `b`, producing `a` \\* `b`.\n\n  The inputs must be two-dimensional matrices, with matching inner dimensions,\n  possibly after transposition.\n\n  Both matrices must be of the same type. The supported types are:\n  `float`, `double`, `int32`, `complex64`.\n\n  Either matrix can be transposed on the fly by setting the corresponding flag\n  to `True`. This is `False` by default.\n\n  If one or both of the matrices contain a lot of zeros, a more efficient\n  multiplication algorithm can be used by setting the corresponding\n  `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n\n  For example:\n\n``` python\n  # 2-D tensor `a`\n  a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]\n                                                        [4. 5. 6.]]\n  # 2-D tensor `b`\n  b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]\n                                                           [9. 10.]\n                                                           [11. 12.]]\n  c = tf.matmul(a, b) => [[58 64]\n                          [139 154]]\n```\n\n  Args:\n    a: `Tensor` of type `float`, `double`, `int32` or `complex64`.\n    b: `Tensor` with same type as `a`.\n    transpose_a: If `True`, `a` is transposed before multiplication.\n    transpose_b: If `True`, `b` is transposed before multiplication.\n    a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n    b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n    name: Name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same type as `a`.\n  \"\"\"\n", "For this type of calculation, we have [embedding_lookup](http://www.tensorflow.org/api_docs/python/nn.html#embedding_lookup).  See the example in [word2vec_basic](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py#L145).  Your \"X\" would be the output of embedding_lookup.\n", "In fact, for X*w, see [embedding_lookup_sparse](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/embedding_ops.py#L108) which does much of the hard work for you.\n", "@ebrevdo  thanks! \u3000I will have a try:)\n", "@ebrevdo Seems will not solve my problem. embeddings_lookup will need ids to be int32, but for X*w,\nw is a weight vector, with float value.\n", "I think you should take another look. tf.gather or embeddinglookup or something similar will work to learn word embeddings. Only the ids need to be ints, the params are floats typically.\n", "@georgedahl \ndef embedding_lookup_sparse(params, sp_ids, sp_weights,\n                            name=None,\n                            combiner=\"mean\"):\n  Args:\n    params: A single tensor representing the complete embedding tensor,\n\nI suppose params to be single tensor, then for my problem input X must be a tensor which is dense,\nbut X actually should be sparse, converting to dense is not good..\n", "@chenghuige Since you have 400k features, in practice you must somewhere store a 400k x feature_depth matrix, no?  This is what you pass to embedding_lookup_sparse and it performs the sparse multiplication for you.\n", "@ebrevdo  Could you help me modify my simple example code ? I have posted my code and dataset.\n\n#binary classification of dense input data using logistic regression \npython ./logistic_regression.py  --train ./data/feature.normed.rand.12000.0_2.txt --test ./data/feature.normed.rand.12000.1_2.txt\nThis will be ok...\n0 auc: 0.776691260816 cost: 1439.02\n1 auc: 0.876033213903 cost: 538.457\n2 auc: 0.893921799102 cost: 495.259\n3 auc: 0.900204872568 cost: 480.429\n\n#binary classification of sparse input data using logistic regression \npython ./logistic_regression_sparse.py  --train ./data/feature.trate.0_2.normed.txt --test ./data/feature.trate.1_2.normed.txt \n\nThis one is really slow(batch size 50), and if setting to large batch size(500) will cause memroy error, could you help me modify it(logistic_regression_sparse.py) to work ? Thanks !\n\nhttps://github.com/chenghuige/tensorflow-example\n", "There are a few things in that example that I can't see, for example sparse2dense is not defined.\n\nAlso, this is a good question for stackoverflow; github issues is more focused towards reporting bugs.\n", "I see\uff0c sparse2dense is in melt_dataset.py. Right now there are similar question in stackoverflow but no answer\n", "Suppose you have a minibatch of 2 entries.  The first entry has sparse ids [53, 87, 101], values [0.1, 0.2, 0.3] and the second has sparse ids [34, 98], weights [-1.0, 3.5].  Suppose your total vocab size is 500.  Suppose also that the hidden layer has depth 25 (25 units).\n\nthen:\n\n``` python\nX = tf.Variable(tf.truncated_normal([500, 25], stddev=1/500.0))\nsp_indices = tf.placeholder(tf.int64)\nsp_shape = tf.placeholder(tf.int64)\nsp_ids_val = tf.placeholder(tf.int64)\nsp_weights_val = tf.placeholder(tf.float32)\nsp_ids = tf.SparseTensor(sp_indices, sp_ids_val, sp_shape)\nsp_weights = tf.SparseTensor(sp_indices, sp_weights_val, sp_shape)\ny = tf.nn.embedding_lookup_sparse(X, sp_ids, sp_weights, \"sum\")\ntf.initialize_all_variables().run()  # initialize values in X\n\ny_values = tf.run(y, feed_dict={\n  sp_indices: [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # 3 entries in minibatch entry 0, 2 entries in entry 1.\n  sp_shape: [2, 3],  # batch size: 2, max index: 2 (so index count == 3)\n  sp_ids_val: [53, 87, 101, 34, 98],\n  sp_weights_val: [0.1, 0.2, 0.3, -1.0, 3.5]})\n```\n\ny_values should be the output of X*[w1, w2], where w1 and w2 are the two minibatch entries.\n", "@ebrevdo Thanks very much for your  patient explanation ! I finally got your point.\nHowever I'm still a bit confused by the embedding_lookup_sparse result \nseems there is a bug , that \"sum\" just work the same as \"mean\".\nI modified your code a bit .\nimport tensorflow as tf\nimport numpy as np\n\nX = tf.placeholder(\"float\", [10, 1])\nx = np.array([[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]], dtype=np.float32)\n\nsp_indices = tf.placeholder(tf.int64)\nsp_shape = tf.placeholder(tf.int64)\nsp_ids_val = tf.placeholder(tf.int64)\nsp_weights_val = tf.placeholder(tf.float32)\nsp_ids = tf.SparseTensor(sp_indices, sp_ids_val, sp_shape)\nsp_weights = tf.SparseTensor(sp_indices, sp_weights_val, sp_shape)\ny = tf.nn.embedding_lookup_sparse(X, sp_ids, sp_weights, \"sum\")\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\ny_values = sess.run(y, feed_dict={\n  X: x,\n  sp_indices: [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # 3 entries in minibatch entry 0, 2 entries in entry 1.\n  sp_shape: [2, 3],  # batch size: 2, max index: 2 (so index count == 3)\n  sp_ids_val: [2, 5, 8, 3, 4],\n  sp_weights_val: [1.0, 1.5, 2.5, 3.5, 4.5]\n  })\n\nprint y_values\n\n#0 should be\n\n# 2 \\* 1 + 5 \\* 1.5 + 8 \\* 2.5  = 29.5\n\n#The result\n./sparse_tensor.py \nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 24\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 24\n[[ 5.9000001]\n [ 3.5625   ]]\n\nThe result is as 29.5/(1 + 1.5 + 2.5) = 5.9\n", "@ebrevdo  Well should be combiner = \"sum\", then ok. Thanks very much, ebrevdo and georgedahl.\nSince problem solved, closing this bug.\n", "@chenghuige @ebrevdo \r\nHey, I am also trying to do text classification - working with very sparse data. TF-IDF feature matrix in SVM light format. I understand the nn.embedding_lookup_sparse BUT your code for producing mini-batch sets does not randomly shuffle the dataset. Is there a good way to do it? Also, does your full_batch_set() have a bug because I get an error when I try to run that.  ", "I am also trying to model very sparse sequential input data with RNN (LSTM). Is there any approaches besides converting the sparse data into dense ones? I'm afraid that the input data may blow up the GPU memory ;)", "@chenghuige Hey, i am using the tensorflow to train a logistic regression model, \r\nif i feed the feed_dict with dense feature vector, the computation of gradient is very slow, how should i use the sparse tensor to speed up the training operation?", "I don't think we can feed SparseTensor to placeholder now. But we can feed\nthe dense `ids` and `values` to construct SparseTensor to train.\n\nHere is the example to train and inference with the sparse libsvm data and\nconvert to TFRecords. You can also export the model for TensorFlow Serving.\nRefer to\nhttps://github.com/tobegit3hub/deep_recommend_system/blob/master/sparse_classifier.py\n.\n\n\nRegards\n\n\nOn Wed, Jan 18, 2017 at 6:55 PM, oliverwang <notifications@github.com>\nwrote:\n\n> @chenghuige <https://github.com/chenghuige> Hey, i am using the\n> tensorflow to train a logistic regression model,\n> if i feed the feed_dict with dense feature vector, the computation of\n> gradient is very slow, how should i use the sparse tensor to speed up the\n> training operation?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/342#issuecomment-273444996>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AClteOocgcpLIQVgLBt_S4osaH7DAnO4ks5rTe-MgaJpZM4GoSbi>\n> .\n>\n", "@CasyWang you can find one example of training and predicting of libsvm format data file example here \r\n[https://github.com/chenghuige/hasky/tree/master/applications/text-regression](https://github.com/chenghuige/hasky/tree/master/applications/text-regression)\r\n[https://github.com/chenghuige/hasky/blob/master/applications/README.md](https://github.com/chenghuige/hasky/blob/master/applications/README.md)"]}, {"number": 341, "title": "Forking TensorFlow after You Already Pip Installed", "body": "Hey TF, love your code.\n\nI recently forked and cloned TF because I wanted to add some new features to it. \n\nHowever, previously I pip installed it, so my question is:\n\nHow do I change it so that when I call in `import TensorFlow`, it comes from my specified directory (cloned dir) and not the one in `dist-packages`? \n\nDo I need to re-install TensorFlow? Or can I just simply use the scripts found in my TensorFlow clone?\n\nThanks!\n", "comments": ["I would probably pip uninstall tensorflow's binary package if you're building from source, just to avoid any problems.  the binary install is for those who just want to use the library as is without modification.\n", "> How do I change it so that when I call import TensorFlow, it comes from my specified directory (cloned dir) and not the one in dist-packages? \n\nI think that's the more important part of the question.\n\nOften you can just `pip install -e ...` but I don't see any way to install tensorflow in an \"editable\" mode so you can easily try out changes, without rebuilding/reinstalling the wheel.\n\nAm I missing something? Is there a simple way?\n", "You can use this pip install option if you want to modify the python code\nlive. Any changes to the c++ build or dependencies requires a recompile.\nOn Nov 24, 2015 5:33 PM, \"Mark Daoust\" notifications@github.com wrote:\n\n> How do I change it so that when I call import TensorFlow, it comes from my\n> specified directory (cloned dir) and not the one in dist-packages?\n> \n> I think that's the more important part of the question.\n> \n> Often you can just pip install -e ... but I don't see any way to install\n> tensorflow in an \"editable\" mode so you can easily try out changes, without\n> rebuilding/reinstalling the wheel.\n> \n> Am I missing something? Is there a simple way?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/341#issuecomment-159458024\n> .\n", "Thanks @ebrevdo , but I don't see it.\n\nThere's no `setup.py` in the root directory\n\nIf I take the `setup.py` from `tensorflow/tools/pip_package` and run a `pip install -e` I still can't get past the `ImportError: Error importing tensorflow: you should not try to import\n  tensorflow from its source directory...`\n\nDoes it work for you? Could you be more specific on how to do it?\n", "Thank you for suggesting to pip uninstall it. I can do that, but I thought it wouldn't be favorable to do so. I'll give a try and see if it works! Thanks!\n"]}, {"number": 340, "title": "Fix compatibility with CUDA Compute Capability <3.2 (issue #320).", "body": "The current code is not compatible with CUDA devices withe CC <3.2 due to the use of __ldg intrinsic. A  preprocessor check for CC>=3.2, such as the one proposed here resolves the issue.\n", "comments": ["Closing the request. Didn't notice that the project doesn't use the pull request mechanism at the moment.\n", "Sorry about that -- we already have a change internally that addresses this though.  Will be pushed out soon.\n", "Thanks for letting me know, Vijay. And most of all, thanks for working on\ntensorflow and making it openly available!\n"]}, {"number": 339, "title": "segmentation fault during data loading of machine translation example on amazon ec2", "body": "Hello,\n\nI am running tensorflow on amazon ec2 with AMI id ami-cf5028a5 constructed as follows:\nhttps://gist.github.com/erikbern/78ba519b97b440e10640. The instance is a g2.2xlarge.\n\nThe only modification to the AMI was to move /models/rnn/translate from the ~/tensorflow dir, to the installation directory (/usr/local/lib/python2.7/dist-packages/tensorflow), so that the libs in translate.py are properly imported.\n\nThe command I run is:\n\n```\npython tensorflow/tensorflow/models/rnn/translate/translate.py --data_dir /mnt/translate/data --train_dir /mnt/translate/train --size=256 --num_layers=2 --steps_per_checkpoint=50\n```\n\nOutput looks like this:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties:\nname: GRID K520\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\npciBusID 0000:00:03.0\nTotal memory: 4.00GiB\nFree memory: 3.95GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927400448\nI tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 8\nCreating 2 layers of 256 units.\nCreated model with fresh parameters.\nReading development and training data (limit: 0).\n  reading data line 100000\n  reading data line 200000\n  reading data line 300000\n[...]\n  reading data line 15000000\nSegmentation fault (core dumped)\n```\n\nWhen I set --max_train_data_size 100000 everything is okay.\n", "comments": ["I don't kown how to learn with this project. For help.\n", "Did you examine the core dump? Likely you ran out of memory...\n", "Closing due to lack of activity.  Can you reopen if it doesn't seem to be a memory issue?\n"]}, {"number": 338, "title": "Compressed input?", "body": "Is there any support for reading/writing compressed input files, e.g. with gzip? I would like to use the \"standard\" file format, but the uncompressed TFRecords files I am generating are huge for my datasets.\n", "comments": ["None yet. gzipping will unlikely result in significant compression due to the high entropy of the weights, but we're very interested in enabling compressed model representations at rest in general, especially for models that have to fit on mobile devices. I'll leave this bug open to track the feature request.\n", "OP: have you tried using L1 regularization? How big are your model files\nafter that?\n\nOn Monday, November 23, 2015, Vincent Vanhoucke notifications@github.com\nwrote:\n\n> None yet. gzipping will unlikely result in significant compression due to\n> the high entropy of the weights, but we're very interested in enabling\n> compressed model representations at rest in general, especially for models\n> that have to fit on mobile devices. I'll leave this bug open to track the\n> feature request.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/338#issuecomment-159109382\n> .\n", "@vincentvanhoucke I've got a TFRecords file with ~9k Example protos that went from 115M to 1.7M after gzip. My input Features are (pretty sparse) float arrays, so I'm not surprised that compression helps a lot.\n\n@delip I'm focusing here on the input data, not the model checkpoints. The checkpoints do compress somewhat, but limiting the number and frequency of checkpoints is fine for now.\n", "@skearnes sorry, not sure why I talked about model files :8ball: \n\n+1 for this request. I would suggest using lzo for compression as opposed to gzip, as the former allows compressed files to be splittable. There's a reference use of lzo with protobufs in Twitter's [elephantbird](https://github.com/twitter/elephant-bird) library.\n", "I completely misunderstood your point, for some reason I assumed you meant checkpoints. FR stands, I agree that it might make a lot of sense to support something like that.\n", "@vincentvanhoucke I guess @skearnes meaning compressing the training data instead of the checkpoints. In some cases, the input feature vectors are extremely sparse and compression will reduce the IO dramatically. So I'm also wondering whether the compression of TFRecord file will be supported in the future?\n", "@delip Snappy is another choice: https://github.com/google/snappy\n", "Looks like support for zlib compressed TFRecords was added in 90bad51f97cc32dc9bc8e2ffb3e81b65cec37dfb\n", "Indeed 90bad51f97cc32dc9bc8e2ffb3e81b65cec37dfb added support for zlib compressed TFRecords.  Closing this issue out.\n", "Is it possible to read compressed raw files?"]}, {"number": 337, "title": "Fix for issue #117: allow relative paths for tensorboard logdir", "body": "This fixes issues [#117](https://github.com/tensorflow/tensorflow/issues/117) and [#321](https://github.com/tensorflow/tensorflow/issues/321).\n\nIt checks if paths passed to `tensorboard --logdir=logs/events` are relative or absolute, and properly resolves relative paths to the current directory instead of assuming they're relative to $HOME.\n", "comments": ["Aargh gerrit is such a pain... submitting through that in a bit.  Ya'll really need to get Github PRs working.\n", "https://tensorflow-review.googlesource.com/#/c/1161/\n", "@vrv \n", "Tested that and it didn't work for me on OS X, possibly because Tensorboard changes the cwd?\n", "Hm, interesting.  Could replace os.getcwd() with os.path.realpath('.').  (The main point was that passing that field through all of the functions isn't really necessary, since it's not going to be configured by external callers.\n", "Yup definitely, when I originally made this PR it was necessary because the current path was [changed in the launch function](https://github.com/tensorflow/tensorflow/blob/f41959ccb2d9d4c722fe8fc3351401d53bcf4900/tensorflow/tensorboard/tensorboard.py#L103).  Looks like we can remove that extra complexity and do what you suggest now that that bit was removed.  I'll commit it in a bit.\n", "Committed your suggestion and tested, looks good! Tested with both absolute and relative.\n\n<img width=\"730\" alt=\"screen shot 2015-12-23 at 2 11 09 pm\" src=\"https://cloud.githubusercontent.com/assets/511499/11983134/1e2a9292-a97f-11e5-8d02-4c77b55d9688.PNG\">\n\n<img width=\"1752\" alt=\"screen shot 2015-12-23 at 2 11 33 pm\" src=\"https://cloud.githubusercontent.com/assets/511499/11983135/1e2b1b4a-a97f-11e5-9e7a-02c6ce3c0ea5.PNG\">\n", "Thanks!  If you can squash the commits, I'll merge right after.\n"]}, {"number": 336, "title": "Cannot make tensorflow to  use a local gcc installation?", "body": "Hi,\nI am bound to an old installation on a machine. I had to install a local gcc 4.8.1 and make my way through a bazel compilation (thanks to the community for their help :) ). However, now I am unable to compile tensorflow. This is the error I get:\n\n```\ngcc: unrecognized option '-no-canonical-prefixes'\ncc1plus: error: unrecognized command line option \"-std=c++11\"\ncc1plus: warning: unrecognized command line option \"-Wno-free-nonheap-object\"\nERROR: $HOME/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... : com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n```\n\nI guess it is not getting the correct gcc, since if I run : \n\n```\n/path/to/local/gcc.4.8.1/bin/gcc -std=c++11  example.cc \n```\n\nAll is ok, but if I run \n\n```\n/usr/bin/gcc  -std=c++11 example.cc\n```\n\nI get the same error\"\n\n```\ncc1plus: error: unrecognized command line option \"-std=c++11\"\n```\n\nIf I add the `--verbose_failures` option, I get this additional information:\n\n```\n(cd $HOME/.cache/bazel/_bazel_$USER/2d4cdeea5be55811d371414ca0f7bd15/tensorflow && \\\n  exec env - \\\n    PATH=$HOME/jdk1.8.0_65/bin/:$HOME/local.gcc-4.8.1/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:$HOME/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/wire_format_lite.o' -MD -MF bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/wire_format_lite.d -c google/protobuf/src/google/protobuf/wire_format_lite.cc -o bazel-out/local_linux-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/wire_format_lite.o)\n```\n\n How can I make it use the correct gcc ?\n\nThanks,\n", "comments": ["This seems more like a bazel-related question -- can please you file an issue at their github repo?\n", "Hi,\nI think it is not a bazel exclusive problem.   I get the error when I execute the following: \n\n```\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n```\n\nThen I get the above error at this stage:\n\n```\n(cd $HOME/.cache/bazel/_bazel_$USER/2d4cdeea5be55811d371414ca0f7bd15/tensorflow && \\\n  exec env - \\\n    PATH=$HOME/jdk1.8.0_65/bin/:$HOME/local.gcc-4.8.1/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:$HOME/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' ... -- see above for full command --)\n```\n\nI execute:\n\n```\n(cd $HOME/.cache/bazel/_bazel_$USER/2d4cdeea5be55811d371414ca0f7bd15/tensorflow && \\\n  exec env - \\\n    PATH=$HOME/jdk1.8.0_65/bin/:$HOME/local.gcc-4.8.1/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:$HOME/bin \\\n  gcc  -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1'  ... )\n```\n\nThen this compilation step works. I think : \n\n```\nthird_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n```\n\nIs not getting the correct gcc. \nMoreover if  I remove the cuda flag (`--config=cuda`) and execute this:\n\n```\nbazel build -c opt  //tensorflow/cc:tutorials_example_trainer\n```\n\nThen the wrapper is different (`external/bazel_tools/tools/cpp/gcc_wrapper.sh` instead of `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n`  ) and it compiles (there was a librt.so problem I solved via addint `-lrt to LDFLAGS`) . So it seems to be a specific problem of tensorFlow + Cuda + the **crosstool_wrapper_driver_is_not_gcc** .  Note that the `gcc_wrapper.sh` is a wapper that I needed to write so that bazel gets the correct gcc and LD_LIBRARY_FLAGS:\n\n```\n#!/bin/bash\nset -e\nLD_LIBRARY_PATH=$HOME/local/lib64:/$HOME/jdk1.8.0_65/lib:/usr/local/cuda-7.0/lib64  $HOME/local/bin/gcc \"$@\"\n```\n\n*_How can I fix the compiler when `--config=cuda` is provided ? *_\nI have tried to modify the script `crosstool_wrapper_driver_is_not_gcc` to point to my gcc in the initial declarations:\n\n``` python\nCPU_COMPILER= ('/path/to/local/bin/gcc')\nGCC_HOST_COMPILER_PATH =  ('/path/to/local/bin/gcc')\n...\n```\n\nBut then I get another error:\n\n```\nERROR: $HOME/tensorflow/google/protobuf/BUILD:267:1: undeclared inclusion(s) in rule '//google/protobuf:protoc':\nthis rule is missing dependency declarations for the following files included by 'google/protobuf/src/google/protobuf/compiler/main.cc':\n  '$HOME/local/include/c++/4.8.4/string'\n  '$HOME/local/include/c++/4.8.4/x86_64-unknown-linux-gnu/bits/c++config.h'\n...\n  '$HOME/local/include/c++/4.8.4/bits/stl_iterator_base_funcs.h'\n...\n  '$HOME/local/include/c++/4.8.4/bits/stl_relops.h'.\n```\n", "Hi, \nfollowing in the https://github.com/bazelbuild/bazel/issues/649  , it seems a tensorflow configuration problem. It is not 100% solved, can anyone help ?\n", "FYI, this is still not solve -- using a local GCC does not work as OP mentioned. Somehow TF or bazel is picking up my /usr/bin/g++ instead of /usr/local/bin/g++ ... debugging...\n", "Any solution to this yet?\n", "Yes, \nfrom here  https://github.com/bazelbuild/bazel/issues/649, apply the comment fix by sethbruder , and then edit compile.sh to export your flags.\n", "Is there any way to do this without editing build files?\n", "This question is probably better asked over at bazel, they're more likely\nto be able to help with this.\n\nOn Wed, Feb 10, 2016 at 1:13 AM Peter Braden notifications@github.com\nwrote:\n\n> Is there any way to do this without editing build files?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/336#issuecomment-182267676\n> .\n", "I had counted such problem when compiling tensorflow from source in mac system:\ngcc: error: unrecognized command line option '-fcolor-diagnostics'\ngcc: error: unrecognized command line option '-Wthread-safety'\ngcc: error: unrecognized command line option '-Wself-assign'\nERROR: /Users/clhuang/Downloads/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: osx_gcc_wrapper.sh failed: error executing command external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException\n\nAnd my gcc version is 4.9,when I just run the command /my/path/gcc-4.9 -Wthread-safety  example.cc,\nit also shows gcc: error: unrecognized command line option '-Wthread-safety'.could anyone give me some advise ,thanks in advance.\n", "I have the same issue as ScottHCL. Anyone please any solution?\n", "same issue. did editing build files work for anyone else?\n", "Add the following to your path:\r\n/usr/lib/gcc/x86_64-linux-gnu/4.8\r\n\r\nI added the following to .bashrc\r\nexport PATH=/usr/lib/gcc/x86_64-linux-gnu/4.8${PATH:+:${PATH}}\r\n\r\nAll cc1 and cc1plus errors are now gone during basel compile.\r\n\r\nFYI, I also compiled basel 0.14.1 using gcc-4.8 as well, not sure if this is necessary. This alone did not cure this problem but may have cured others.\r\n\r\nUbuntu 18.04 LTS\r\nAnaconda3-5.2.0-Linux-x86_64.sh\r\ncuda_9.2.88_396.26_linux.run\r\ncudnn-9.2-linux-x64-v7.1.tgz\r\nopenjdk-11-jre-headless\r\ngcc-4.8, g++-4.8\r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 1806.409s, Critical Path: 138.18s\r\nINFO: 6930 processes, local.\r\nINFO: Build completed successfully, 7875 total actions\r\n\r\nHope it helps"]}, {"number": 335, "title": "Extend image summary so it allows displaying a label next to an image", "body": "It would be really neat if the tensorboard could additionally show the predicted and/or given label for an image in the 'images' tab. \n", "comments": ["@danmane: Thoughts?\n", "This seems pretty important. If labels come from a separate file and need to be joined with the proper images in tensor flow, then having a good and easy visualization would do wonders for being confident that the correct labels are matched with each image.\n", "From an API perspective, how do you imagine this working? The ImageSummary op would take in an image input and also (optionally?) a text label input? I've also heard requests for bounding boxes on the images. (What happens if you want to make multiple predictions on a single image?)\n\nThe image viewer is definitely one of the least-developed parts of TensorBoard at the moment. I want to improve it in the future, but it's not very high on my queue at the moment as I'm prioritizing features that are domain independent and thus useful to every user of TensorBoard (general usability, hyperparameter search, etc). If you want this to get fixed sooner, the best way is to come up with a plan for what the API should look like, get buy-in, and then submit a pull request - I'd be happy to help coordinate the process :)\n", "Once we have plugin support in TensorBoard, this will be a good candidate for a plugin. But I'm going to close it for now since it won't be easy to write plugins for a few months.\n", "It appears TensorBoard supports [plugins](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tensorboard/plugins) now. Will this issue be reopened soon?", "+1 for plot detection label, not only boxes", "Any updates?", "For anyone still interested in this issue, now that TensorBoard has a separate repository from TensorFlow, I have reopened the issue there. [Extend image summary so it allows displaying a label next to an image](https://github.com/tensorflow/tensorboard/issues/2204)"]}]