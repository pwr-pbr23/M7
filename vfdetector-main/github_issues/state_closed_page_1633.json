[{"number": 3918, "title": "global_step/sec is always zeros in the tensorflow distributed log", "body": "When I use the tensorflow distributed version to train a model of classify of the text.\nI find the global_step variable in the log of parameter server is always zero. At the same time, the worker log doesn't update any more.\nAfter 30 minutes, there is no change in the situation.\nBut when I use the nvidia-smi to check the situation of the gpu, I find the process which is working in the gpu.\n\nthis is my global_step code:\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n\nDoes anyone who know how to solve it?\n", "comments": ["This is a general question best suited to StackOverflow.  Can you please re-ask it there.\n", "OK, thanks\n", "@iamsile  hello\uff0ci also meet this problem,do you solve it?can you tell me how to solve it?", "hello\uff0ci also meet this problem,do you solve it?can you tell me how to solve it?"]}, {"number": 3917, "title": "Floating point exception (core dumped) when running tutorials_example_trainer using gpu", "body": "I'm trying build latest tensorflow but I got a Floating point exception as follow:\n\n> zhipeng@tu567:~/apps/tensorflow$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: \n> name: GeForce GTX 1070\n> major: 6 minor: 1 memoryClockRate (GHz) 1.7845\n> pciBusID 0000:82:00.0\n> Total memory: 7.92GiB\n> Free memory: 7.84GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 \n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y \n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> Floating point exception (core dumped)\n## Enviroment info\n\n> Ubuntu 14.04\n> GPU: GTX GeForce 1070\n> CUDA 8.0RC\n> Cudnn v5\n> bazel: 0.3.1-jdk7\n## gdb\n\n> (gdb) r --use_gpu\n> Starting program: /home/zhipeng/.cache/bazel/_bazel_zhipeng/97115ed19a1a63c4345ae364363ad69b/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n> [Thread debugging using libthread_db enabled]\n> Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n> [New Thread 0x7fffe1bf4700 (LWP 96027)]\n> [New Thread 0x7fffe13f3700 (LWP 96028)]\n> [New Thread 0x7fffe0bf2700 (LWP 96029)]\n> [New Thread 0x7fffe03f1700 (LWP 96030)]\n> [New Thread 0x7fffdfbf0700 (LWP 96031)]\n> [New Thread 0x7fffdf3ef700 (LWP 96032)]\n> [New Thread 0x7fffdebee700 (LWP 96033)]\n> [New Thread 0x7fffde3ed700 (LWP 96034)]\n> [New Thread 0x7fffddbec700 (LWP 96035)]\n> [New Thread 0x7fffdd3eb700 (LWP 96036)]\n> [New Thread 0x7fffdcbea700 (LWP 96037)]\n> [New Thread 0x7fffbffff700 (LWP 96038)]\n> [New Thread 0x7fffbf7fe700 (LWP 96039)]\n> [New Thread 0x7fffbeffd700 (LWP 96040)]\n> [New Thread 0x7fffbe7fc700 (LWP 96041)]\n> [New Thread 0x7fffbdffb700 (LWP 96042)]\n> [New Thread 0x7fffbd7fa700 (LWP 96043)]\n> [New Thread 0x7fffbcff9700 (LWP 96044)]\n> [New Thread 0x7fffaffff700 (LWP 96045)]\n> [New Thread 0x7fffaf7fe700 (LWP 96046)]\n> [New Thread 0x7fffaeffd700 (LWP 96047)]\n> [New Thread 0x7fffae7fc700 (LWP 96048)]\n> [New Thread 0x7fffadffb700 (LWP 96049)]\n> [New Thread 0x7fffad7fa700 (LWP 96050)]\n> [New Thread 0x7fffacff9700 (LWP 96051)]\n> [New Thread 0x7fffac7f8700 (LWP 96052)]\n> [New Thread 0x7fffabff7700 (LWP 96053)]\n> [New Thread 0x7fffab7f6700 (LWP 96054)]\n> [New Thread 0x7fffaaff5700 (LWP 96055)]\n> [New Thread 0x7fffaa7f4700 (LWP 96056)]\n> [New Thread 0x7fffa9ff3700 (LWP 96057)]\n> [New Thread 0x7fffa97f2700 (LWP 96058)]\n> [New Thread 0x7fffa8ff1700 (LWP 96059)]\n> [New Thread 0x7fffa87f0700 (LWP 96060)]\n> [New Thread 0x7fffa7fef700 (LWP 96061)]\n> [New Thread 0x7fffa77ee700 (LWP 96062)]\n> [New Thread 0x7fffa6fed700 (LWP 96063)]\n> [New Thread 0x7fffa67ec700 (LWP 96064)]\n> [New Thread 0x7fffa5feb700 (LWP 96065)]\n> [New Thread 0x7fffa57ea700 (LWP 96066)]\n> [New Thread 0x7fffa4fe9700 (LWP 96067)]\n> [New Thread 0x7fffa47e8700 (LWP 96068)]\n> [New Thread 0x7fffa3fe7700 (LWP 96069)]\n> [New Thread 0x7fffa37e6700 (LWP 96070)]\n> [New Thread 0x7fffa2fe5700 (LWP 96071)]\n> [New Thread 0x7fffa27e4700 (LWP 96072)]\n> [New Thread 0x7fffa1fe3700 (LWP 96073)]\n> [New Thread 0x7fffa17e2700 (LWP 96074)]\n> [New Thread 0x7fffa0fe1700 (LWP 96075)]\n> [New Thread 0x7fffa07e0700 (LWP 96076)]\n> [New Thread 0x7fff9ffdf700 (LWP 96077)]\n> [New Thread 0x7fff9f7de700 (LWP 96078)]\n> [New Thread 0x7fff9efdd700 (LWP 96079)]\n> [New Thread 0x7fff9e7dc700 (LWP 96080)]\n> [New Thread 0x7fff9dfdb700 (LWP 96081)]\n> [New Thread 0x7fff9d7da700 (LWP 96082)]\n> [New Thread 0x7fff9cfd9700 (LWP 96083)]\n> [New Thread 0x7fff9c7d8700 (LWP 96084)]\n> [New Thread 0x7fff9bfd7700 (LWP 96085)]\n> [New Thread 0x7fff9b7d6700 (LWP 96086)]\n> [New Thread 0x7fff9afd5700 (LWP 96087)]\n> [New Thread 0x7fff9a7d4700 (LWP 96088)]\n> [New Thread 0x7fff99fd3700 (LWP 96089)]\n> [New Thread 0x7fff997d2700 (LWP 96090)]\n> [New Thread 0x7fff98fd1700 (LWP 96091)]\n> [New Thread 0x7fff987d0700 (LWP 96092)]\n> [New Thread 0x7fff97fcf700 (LWP 96093)]\n> [New Thread 0x7fff977ce700 (LWP 96094)]\n> [New Thread 0x7fff96fcd700 (LWP 96097)]\n> [New Thread 0x7fff967cc700 (LWP 96098)]\n> [New Thread 0x7fff95fcb700 (LWP 96099)]\n> [New Thread 0x7fff957ca700 (LWP 96100)]\n> [New Thread 0x7fff94fc9700 (LWP 96101)]\n> [New Thread 0x7fff947c8700 (LWP 96102)]\n> [New Thread 0x7fff93fc7700 (LWP 96103)]\n> [New Thread 0x7fff937c6700 (LWP 96104)]\n> [New Thread 0x7fff92fc5700 (LWP 96105)]\n> [New Thread 0x7fff927c4700 (LWP 96106)]\n> [New Thread 0x7fff91fc3700 (LWP 96107)]\n> [New Thread 0x7fff917c2700 (LWP 96108)]\n> [New Thread 0x7fff90fc1700 (LWP 96109)]\n> [New Thread 0x7fff907c0700 (LWP 96110)]\n> [New Thread 0x7fff89fbf700 (LWP 96114)]\n> [New Thread 0x7fff897be700 (LWP 96115)]\n> [New Thread 0x7fff88fbd700 (LWP 96116)]\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: \n> name: GeForce GTX 1070\n> major: 6 minor: 1 memoryClockRate (GHz) 1.7845\n> pciBusID 0000:82:00.0\n> Total memory: 7.92GiB\n> Free memory: 7.84GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 \n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y \n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:868] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:82:00.0)\n> [New Thread 0x7fff83fff700 (LWP 96117)]\n> [New Thread 0x7fff82ffd700 (LWP 96119)]\n> [New Thread 0x7fff837fe700 (LWP 96118)]\n> [New Thread 0x7fff817fa700 (LWP 96121)]\n> [New Thread 0x7fff827fc700 (LWP 96120)]\n> [New Thread 0x7fff81ffb700 (LWP 96122)]\n> [New Thread 0x7fff79ffb700 (LWP 96126)]\n> [New Thread 0x7fff7b7fe700 (LWP 96124)]\n> [New Thread 0x7fff7bfff700 (LWP 96125)]\n> [New Thread 0x7fff80ff9700 (LWP 96123)]\n> [New Thread 0x7fff7a7fc700 (LWP 96128)]\n> [New Thread 0x7fff7affd700 (LWP 96127)]\n> [New Thread 0x7fff797fa700 (LWP 96129)]\n> [New Thread 0x7fff78ff9700 (LWP 96130)]\n> [New Thread 0x7fff710d3700 (LWP 96131)]\n> [New Thread 0x7fff708d2700 (LWP 96132)]\n> [New Thread 0x7fff700d1700 (LWP 96133)]\n> [New Thread 0x7fff6f8d0700 (LWP 96134)]\n> [New Thread 0x7fff6f0cf700 (LWP 96135)]\n> [New Thread 0x7fff6e8ce700 (LWP 96136)]\n> [New Thread 0x7fff6e0cd700 (LWP 96137)]\n> [New Thread 0x7fff6d8cc700 (LWP 96138)]\n> [New Thread 0x7fff6d0cb700 (LWP 96139)]\n> [New Thread 0x7fff6c8ca700 (LWP 96140)]\n> [New Thread 0x7fff6c0c9700 (LWP 96141)]\n> [New Thread 0x7fff6b8c8700 (LWP 96142)]\n> [New Thread 0x7fff6b0c7700 (LWP 96143)]\n> [New Thread 0x7fff6a8c6700 (LWP 96144)]\n> [New Thread 0x7fff6a0c5700 (LWP 96145)]\n> [New Thread 0x7fff698c4700 (LWP 96146)]\n> [New Thread 0x7fff690c3700 (LWP 96147)]\n> [New Thread 0x7fff688c2700 (LWP 96148)]\n> [New Thread 0x7fff680c1700 (LWP 96149)]\n> [New Thread 0x7fff678c0700 (LWP 96150)]\n> [New Thread 0x7fff670bf700 (LWP 96151)]\n> [New Thread 0x7fff668be700 (LWP 96152)]\n> [New Thread 0x7fff660bd700 (LWP 96153)]\n> [New Thread 0x7fff658bc700 (LWP 96154)]\n> [New Thread 0x7fff650bb700 (LWP 96155)]\n> [New Thread 0x7fff648ba700 (LWP 96156)]\n> [New Thread 0x7fff640b9700 (LWP 96157)]\n> [New Thread 0x7fff638b8700 (LWP 96158)]\n> [New Thread 0x7fff630b7700 (LWP 96159)]\n> [New Thread 0x7fff628b6700 (LWP 96160)]\n> [New Thread 0x7fff620b5700 (LWP 96161)]\n> [New Thread 0x7fff618b4700 (LWP 96162)]\n> [New Thread 0x7fff610b3700 (LWP 96163)]\n> [New Thread 0x7fff608b2700 (LWP 96164)]\n> [New Thread 0x7fff600b1700 (LWP 96165)]\n> [New Thread 0x7fff5f8b0700 (LWP 96166)]\n> [New Thread 0x7fff5f0af700 (LWP 96167)]\n> [New Thread 0x7fff5e8ae700 (LWP 96168)]\n> [New Thread 0x7fff5e0ad700 (LWP 96169)]\n> [New Thread 0x7fff5d8ac700 (LWP 96170)]\n> [New Thread 0x7fff5d0ab700 (LWP 96171)]\n> [New Thread 0x7fff5c8aa700 (LWP 96172)]\n> [New Thread 0x7fff5c0a9700 (LWP 96173)]\n> [New Thread 0x7fff5b8a8700 (LWP 96174)]\n> [New Thread 0x7fff5b0a7700 (LWP 96175)]\n> [New Thread 0x7fff5a8a6700 (LWP 96176)]\n> [New Thread 0x7fff5a0a5700 (LWP 96177)]\n> [New Thread 0x7fff598a4700 (LWP 96178)]\n> [New Thread 0x7fff590a3700 (LWP 96179)]\n> [New Thread 0x7fff588a2700 (LWP 96180)]\n> [New Thread 0x7fff580a1700 (LWP 96181)]\n> [New Thread 0x7fff578a0700 (LWP 96182)]\n> [New Thread 0x7fff5709f700 (LWP 96183)]\n> [New Thread 0x7fff5689e700 (LWP 96184)]\n> [New Thread 0x7fff5609d700 (LWP 96185)]\n> [New Thread 0x7fff5589c700 (LWP 96186)]\n> [New Thread 0x7fff5509b700 (LWP 96187)]\n> [New Thread 0x7fff5489a700 (LWP 96188)]\n> [New Thread 0x7fff54099700 (LWP 96189)]\n> [New Thread 0x7fff53898700 (LWP 96190)]\n> [New Thread 0x7fff53097700 (LWP 96191)]\n> [New Thread 0x7fff52896700 (LWP 96192)]\n> [New Thread 0x7fff52095700 (LWP 96193)]\n> [New Thread 0x7fff51894700 (LWP 96194)]\n> [New Thread 0x7fff51093700 (LWP 96195)]\n> [New Thread 0x7fff50892700 (LWP 96196)]\n> [New Thread 0x7fff50091700 (LWP 96197)]\n> [New Thread 0x7fff4f890700 (LWP 96198)]\n> [New Thread 0x7fff4f08f700 (LWP 96199)]\n> [New Thread 0x7fff4e88e700 (LWP 96200)]\n> [New Thread 0x7fff4e08d700 (LWP 96201)]\n> [New Thread 0x7fff4d88c700 (LWP 96202)]\n> [New Thread 0x7fff4d08b700 (LWP 96203)]\n> [New Thread 0x7fff4c88a700 (LWP 96204)]\n> [New Thread 0x7fff4c089700 (LWP 96205)]\n> [New Thread 0x7fff4b888700 (LWP 96206)]\n> [New Thread 0x7fff4b087700 (LWP 96207)]\n> [New Thread 0x7fff4a886700 (LWP 96208)]\n> [New Thread 0x7fff4a085700 (LWP 96209)]\n> [New Thread 0x7fff48882700 (LWP 96213)]\n> [New Thread 0x7fff49884700 (LWP 96210)]\n> [New Thread 0x7fff48081700 (LWP 96212)]\n> [New Thread 0x7fff49083700 (LWP 96211)]\n> [New Thread 0x7fff47880700 (LWP 96215)]\n> [New Thread 0x7fff4687e700 (LWP 96216)]\n> [New Thread 0x7fff4587c700 (LWP 96217)]\n> [New Thread 0x7fff4707f700 (LWP 96214)]\n> [New Thread 0x7fff4607d700 (LWP 96218)]\n> [New Thread 0x7fff4487a700 (LWP 96220)]\n> [New Thread 0x7fff4507b700 (LWP 96219)]\n> [New Thread 0x7fff44079700 (LWP 96221)]\n> [New Thread 0x7fff43878700 (LWP 96222)]\n> [New Thread 0x7fff43077700 (LWP 96223)]\n> [New Thread 0x7fff42876700 (LWP 96224)]\n> [New Thread 0x7fff42075700 (LWP 96225)]\n> [New Thread 0x7fff41874700 (LWP 96226)]\n> [New Thread 0x7fff41073700 (LWP 96227)]\n> [New Thread 0x7fff40872700 (LWP 96228)]\n> [New Thread 0x7fff40071700 (LWP 96229)]\n> [New Thread 0x7fff3f870700 (LWP 96230)]\n> [New Thread 0x7fff3f06f700 (LWP 96231)]\n> [New Thread 0x7fff3e86e700 (LWP 96232)]\n> [New Thread 0x7fff3d86c700 (LWP 96234)]\n> [New Thread 0x7fff3e06d700 (LWP 96233)]\n> [New Thread 0x7fff3d06b700 (LWP 96235)]\n> [New Thread 0x7fff3c069700 (LWP 96237)]\n> [New Thread 0x7fff3c86a700 (LWP 96236)]\n> [New Thread 0x7fff3b868700 (LWP 96238)]\n> [New Thread 0x7fff3b067700 (LWP 96239)]\n> [New Thread 0x7fff3a866700 (LWP 96240)]\n> [New Thread 0x7fff3a065700 (LWP 96241)]\n> [New Thread 0x7fff39864700 (LWP 96242)]\n> [New Thread 0x7fff39063700 (LWP 96243)]\n> [New Thread 0x7fff38862700 (LWP 96244)]\n> [New Thread 0x7fff37860700 (LWP 96246)]\n> [New Thread 0x7fff38061700 (LWP 96245)]\n> [New Thread 0x7fff3705f700 (LWP 96247)]\n> [New Thread 0x7fff3685e700 (LWP 96248)]\n> [New Thread 0x7fff3605d700 (LWP 96249)]\n> [New Thread 0x7fff3585c700 (LWP 96250)]\n> [New Thread 0x7fff3505b700 (LWP 96251)]\n> [New Thread 0x7fff3485a700 (LWP 96252)]\n> [New Thread 0x7fff34059700 (LWP 96253)]\n> [New Thread 0x7fff33057700 (LWP 96255)]\n> [New Thread 0x7fff33858700 (LWP 96254)]\n> [New Thread 0x7fff32856700 (LWP 96256)]\n> [New Thread 0x7fff32055700 (LWP 96257)]\n> [New Thread 0x7fff31854700 (LWP 96258)]\n> [New Thread 0x7fff31053700 (LWP 96259)]\n> [New Thread 0x7fff30852700 (LWP 96260)]\n> [New Thread 0x7fff30051700 (LWP 96261)]\n> [New Thread 0x7fff2f850700 (LWP 96262)]\n> [New Thread 0x7fff2f04f700 (LWP 96263)]\n> [New Thread 0x7fff2e84e700 (LWP 96264)]\n> [New Thread 0x7fff2e04d700 (LWP 96265)]\n> [New Thread 0x7fff2d84c700 (LWP 96266)]\n> [New Thread 0x7fff2d04b700 (LWP 96267)]\n> [New Thread 0x7fff2c84a700 (LWP 96268)]\n> [New Thread 0x7fff2c049700 (LWP 96269)]\n> [New Thread 0x7fff2b848700 (LWP 96270)]\n> [New Thread 0x7fff2b047700 (LWP 96271)]\n> [New Thread 0x7fff2a846700 (LWP 96272)]\n> [New Thread 0x7fff2a045700 (LWP 96273)]\n> [New Thread 0x7fff29043700 (LWP 96275)]\n> [New Thread 0x7fff29844700 (LWP 96274)]\n> [New Thread 0x7fff28842700 (LWP 96276)]\n> [New Thread 0x7fff28041700 (LWP 96277)]\n> [New Thread 0x7fff27840700 (LWP 96278)]\n> [New Thread 0x7fff2703f700 (LWP 96279)]\n> [New Thread 0x7fff2683e700 (LWP 96280)]\n> [New Thread 0x7fff2603d700 (LWP 96281)]\n> [New Thread 0x7fff2583c700 (LWP 96282)]\n> [New Thread 0x7fff2503b700 (LWP 96283)]\n> [New Thread 0x7fff2483a700 (LWP 96284)]\n> [New Thread 0x7fff07fff700 (LWP 96285)]\n> [New Thread 0x7fff077fe700 (LWP 96286)]\n> [New Thread 0x7fff06ffd700 (LWP 96287)]\n> [New Thread 0x7fff067fc700 (LWP 96288)]\n> [New Thread 0x7fff05ffb700 (LWP 96289)]\n> [New Thread 0x7fff057fa700 (LWP 96290)]\n> [New Thread 0x7fff04ff9700 (LWP 96291)]\n> [New Thread 0x7ffea7fff700 (LWP 96292)]\n> [New Thread 0x7ffea77fe700 (LWP 96293)]\n> [New Thread 0x7ffea6ffd700 (LWP 96294)]\n> [New Thread 0x7ffea67fc700 (LWP 96295)]\n> [New Thread 0x7ffea5ffb700 (LWP 96296)]\n> [New Thread 0x7ffea4ff9700 (LWP 96298)]\n> [New Thread 0x7ffea57fa700 (LWP 96297)]\n> [New Thread 0x7ffe27576700 (LWP 96299)]\n> [New Thread 0x7ffe26d75700 (LWP 96300)]\n> [New Thread 0x7ffe26574700 (LWP 96301)]\n> [New Thread 0x7ffe25d73700 (LWP 96302)]\n> [New Thread 0x7ffe25572700 (LWP 96303)]\n> [New Thread 0x7ffe24d71700 (LWP 96304)]\n> [New Thread 0x7ffd17fff700 (LWP 96305)]\n> Program received signal SIGFPE, Arithmetic exception.\n> [Switching to Thread 0x7fff4a886700 (LWP 96208)]\n> 0x000055555600f22a in tensorflow::functor::CastFunctor<Eigen::GpuDevice, float, int>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16>) ()\n\nAnd it can make successful execution without 'use_gpu' parameter, I can't find the problem.\n", "comments": ["Sounds like integer division by 0 somewhere, there was same error message when doing tf.div(1, 0), but @girving added some checks in the div/mod kernels\n", "@yaroslavvb Do floating point exceptions occur if you cast 1e100 to `int32`?\n", "Shouldn't....tf.cast(1e100, tf.int32) gives MAXINT on GPU and MININT on CPU\n", "Not sure what the problem is, then.  @czp1019 Can you compile in debug mode and investigate what exactly is causing the exception?\n", "I'll try compile in debug mode these days, but I'm not sure if this issue is caused by the version of Cudnn or Cuda. @girving \n", "I think so and I will check it, thanks! @yaroslavvb\n", "This seems to be a Cuda/Driver Issue. I tracked this down on my system and it fails in /external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h at line 250:\n\nconst int max_blocks = device.getNumCudaMultiProcessors() *\n                           device.maxCudaThreadsPerMultiProcessor() / block_size;\n\nIn some cases a block_size of 0 is returned from device.maxCudaThreadsPerBlock();\n", "@zheng-xq Do you know what might be happening?\n", "I am getting the same error as well, although it only occurs about of the third of the time. The rest of the time the program exits normally, but shows nan for many of the output values.\n\n**gdb - on failure**\n\n```\n(gdb) r --use_gpu\nStarting program: /home/reale/.cache/bazel/_bazel_reale/b77777b530a2fcea5d78d328048dc62b/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.1.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\n[New Thread 0x7fffe1a36700 (LWP 26488)]\n[New Thread 0x7fffe1235700 (LWP 26489)]\n[New Thread 0x7fffe0a34700 (LWP 26490)]\n[New Thread 0x7fffe0233700 (LWP 26491)]\n[New Thread 0x7fffdfa32700 (LWP 26492)]\n[New Thread 0x7fffdf231700 (LWP 26493)]\n[New Thread 0x7fffdea30700 (LWP 26494)]\n[New Thread 0x7fffde22f700 (LWP 26495)]\n[New Thread 0x7fffdda2e700 (LWP 26496)]\n[New Thread 0x7fffdd22d700 (LWP 26497)]\n[New Thread 0x7fffdca2c700 (LWP 26498)]\n[New Thread 0x7fffcffff700 (LWP 26499)]\n[New Thread 0x7fffcf7fe700 (LWP 26500)]\n[New Thread 0x7fffceffd700 (LWP 26501)]\n[New Thread 0x7fffce7fc700 (LWP 26502)]\n[New Thread 0x7fffcdffb700 (LWP 26503)]\n[New Thread 0x7fffcd7fa700 (LWP 26504)]\n[New Thread 0x7fffccff9700 (LWP 26505)]\n[New Thread 0x7fffc7fff700 (LWP 26511)]\n[New Thread 0x7fffc77fe700 (LWP 26512)]\n[New Thread 0x7fffc6ffd700 (LWP 26513)]\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:118] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:01:00.0\nTotal memory: 7.92GiB\nFree memory: 7.54GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:138] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:148] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:867] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n[New Thread 0x7fffc67fc700 (LWP 26514)]\n[New Thread 0x7fffc5ffb700 (LWP 26515)]\n[New Thread 0x7fffc57fa700 (LWP 26516)]\n[New Thread 0x7fffc4ff9700 (LWP 26517)]\n[New Thread 0x7fffbffff700 (LWP 26518)]\n[New Thread 0x7fffbf7fe700 (LWP 26519)]\n[New Thread 0x7fffbd7fa700 (LWP 26523)]\n[New Thread 0x7fffbdffb700 (LWP 26521)]\n[New Thread 0x7fffbcff9700 (LWP 26524)]\n[New Thread 0x7fffbeffd700 (LWP 26520)]\n[New Thread 0x7fffb7fff700 (LWP 26525)]\n[New Thread 0x7fffbe7fc700 (LWP 26522)]\n[New Thread 0x7fffb67fc700 (LWP 26526)]\n[New Thread 0x7fffb5ffb700 (LWP 26528)]\n[New Thread 0x7fffb4ff9700 (LWP 26527)]\n[New Thread 0x7fffb77fe700 (LWP 26531)]\n[New Thread 0x7fffb6ffd700 (LWP 26529)]\n[New Thread 0x7fffb57fa700 (LWP 26530)]\n[New Thread 0x7fffaffff700 (LWP 26533)]\n[New Thread 0x7fffaf7fe700 (LWP 26532)]\n[New Thread 0x7fffaeffd700 (LWP 26534)]\n[New Thread 0x7fffae7fc700 (LWP 26535)]\n[New Thread 0x7fffadffb700 (LWP 26536)]\n[New Thread 0x7fffad7fa700 (LWP 26537)]\n[New Thread 0x7fffacff9700 (LWP 26538)]\n[New Thread 0x7fff99fff700 (LWP 26539)]\n[New Thread 0x7fff997fe700 (LWP 26540)]\n[New Thread 0x7fff98ffd700 (LWP 26541)]\n[New Thread 0x7fff937fe700 (LWP 26542)]\n[New Thread 0x7fff927fc700 (LWP 26546)]\n[New Thread 0x7fff84d5f700 (LWP 26549)]\n[New Thread 0x7fff90ff9700 (LWP 26547)]\n[New Thread 0x7fff917fa700 (LWP 26548)]\n[New Thread 0x7fff93fff700 (LWP 26543)]\n[New Thread 0x7fff91ffb700 (LWP 26545)]\n[New Thread 0x7fff83d5d700 (LWP 26551)]\n[New Thread 0x7fff92ffd700 (LWP 26544)]\n[New Thread 0x7fff8455e700 (LWP 26550)]\n[New Thread 0x7fff8355c700 (LWP 26552)]\n[New Thread 0x7fff7ed53700 (LWP 26559)]\n[New Thread 0x7fff7f554700 (LWP 26560)]\n[New Thread 0x7fff82d5b700 (LWP 26553)]\n[New Thread 0x7fff8255a700 (LWP 26554)]\n[New Thread 0x7fff81d59700 (LWP 26557)]\n[New Thread 0x7fff80d57700 (LWP 26555)]\n[New Thread 0x7fff81558700 (LWP 26558)]\n[New Thread 0x7fff80556700 (LWP 26556)]\n[New Thread 0x7fff7fd55700 (LWP 26561)]\n[New Thread 0x7fff7e552700 (LWP 26562)]\n[New Thread 0x7fff7bd4d700 (LWP 26567)]\n[New Thread 0x7fff7a54a700 (LWP 26570)]\n[New Thread 0x7fff7dd51700 (LWP 26563)]\n[New Thread 0x7fff7d550700 (LWP 26564)]\n[New Thread 0x7fff7b54c700 (LWP 26568)]\n[New Thread 0x7fff79d49700 (LWP 26571)]\n[New Thread 0x7fff7cd4f700 (LWP 26565)]\n[New Thread 0x7fff7ad4b700 (LWP 26569)]\n[New Thread 0x7fff7c54e700 (LWP 26566)]\n[New Thread 0x7fff79548700 (LWP 26572)]\n[New Thread 0x7fff75d41700 (LWP 26602)]\n[New Thread 0x7fff78d47700 (LWP 26573)]\n[New Thread 0x7fff78546700 (LWP 26599)]\n[New Thread 0x7fff75540700 (LWP 26600)]\n[New Thread 0x7fff76542700 (LWP 26597)]\n[New Thread 0x7fff77544700 (LWP 26598)]\n[New Thread 0x7fff77d45700 (LWP 26596)]\n[New Thread 0x7fff76d43700 (LWP 26574)]\n[New Thread 0x7fff74d3f700 (LWP 26601)]\n[New Thread 0x7fff73d3d700 (LWP 26604)]\n[New Thread 0x7fff7453e700 (LWP 26603)]\n[New Thread 0x7fff7353c700 (LWP 26605)]\n[New Thread 0x7fff70d37700 (LWP 26606)]\n[New Thread 0x7fff72d3b700 (LWP 26611)]\n[New Thread 0x7fff7253a700 (LWP 26608)]\n[New Thread 0x7fff71d39700 (LWP 26607)]\n[New Thread 0x7fff71538700 (LWP 26609)]\n[New Thread 0x7fff70536700 (LWP 26610)]\n[New Thread 0x7fff6fd35700 (LWP 26612)]\n[New Thread 0x7fff6f534700 (LWP 26613)]\n[New Thread 0x7fff6cd2f700 (LWP 26615)]\n[New Thread 0x7fff6ed33700 (LWP 26618)]\n[New Thread 0x7fff6e532700 (LWP 26617)]\n[New Thread 0x7fff6dd31700 (LWP 26614)]\n[New Thread 0x7fff6d530700 (LWP 26616)]\n[New Thread 0x7fff6c52e700 (LWP 26622)]\n[New Thread 0x7fff6ad2b700 (LWP 26619)]\n[New Thread 0x7fff6bd2d700 (LWP 26621)]\n[New Thread 0x7fff6b52c700 (LWP 26620)]\n[New Thread 0x7fff6a52a700 (LWP 26623)]\n[New Thread 0x7fff69d29700 (LWP 26624)]\n[New Thread 0x7fff69528700 (LWP 26631)]\n[New Thread 0x7fff68526700 (LWP 26628)]\n[New Thread 0x7fff66d23700 (LWP 26629)]\n[New Thread 0x7fff68d27700 (LWP 26625)]\n[New Thread 0x7fff66522700 (LWP 26627)]\n[New Thread 0x7fff65d21700 (LWP 26630)]\n[New Thread 0x7fff67d25700 (LWP 26632)]\n[New Thread 0x7fff67524700 (LWP 26626)]\n[New Thread 0x7fff65520700 (LWP 26633)]\n[New Thread 0x7fff64d1f700 (LWP 26634)]\n[New Thread 0x7fff6451e700 (LWP 26635)]\n[New Thread 0x7fff6351c700 (LWP 26636)]\n[New Thread 0x7fff63d1d700 (LWP 26637)]\n[New Thread 0x7fff62d1b700 (LWP 26638)]\n[New Thread 0x7fff6251a700 (LWP 26642)]\n[New Thread 0x7fff61d19700 (LWP 26641)]\n[New Thread 0x7fff61518700 (LWP 26639)]\n[New Thread 0x7fff60d17700 (LWP 26640)]\n[New Thread 0x7fff60516700 (LWP 26643)]\n[New Thread 0x7fff5d510700 (LWP 26652)]\n[New Thread 0x7fff5fd15700 (LWP 26644)]\n[New Thread 0x7fff5dd11700 (LWP 26648)]\n[New Thread 0x7fff5cd0f700 (LWP 26650)]\n[New Thread 0x7fff5f514700 (LWP 26645)]\n[New Thread 0x7fff5ed13700 (LWP 26646)]\n[New Thread 0x7fff5bd0d700 (LWP 26651)]\n[New Thread 0x7fff5c50e700 (LWP 26649)]\n[New Thread 0x7fff5e512700 (LWP 26647)]\n[New Thread 0x7fff5b50c700 (LWP 26653)]\n[New Thread 0x7fff5ad0b700 (LWP 26654)]\n[New Thread 0x7fff5a50a700 (LWP 26657)]\n[New Thread 0x7fff59d09700 (LWP 26656)]\n[New Thread 0x7fff59508700 (LWP 26655)]\n[New Thread 0x7fff58d07700 (LWP 26662)]\n[New Thread 0x7fff58506700 (LWP 26659)]\n[New Thread 0x7fff57504700 (LWP 26660)]\n[New Thread 0x7fff57d05700 (LWP 26661)]\n[New Thread 0x7fff56d03700 (LWP 26658)]\n[New Thread 0x7fff56502700 (LWP 26663)]\n\nThread 48 \"tutorials_examp\" received signal SIGFPE, Arithmetic exception.\n[Switching to Thread 0x7fff99fff700 (LWP 26539)]\n0x000055555605c320 in tensorflow::functor::CastFunctor<Eigen::GpuDevice, float, int>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16>) ()\n```\n\n**last 20 lines from running outside gdb**\n\n```\n000009/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000005/000008 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000009/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000008/000001 lambda = 2.000146 x = [0.894449 -0.447170] y = [1.789007 -0.894449]\n000009/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000007/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000008/000001 lambda = 2.000073 x = [0.894438 -0.447192] y = [1.788931 -0.894438]\n000009/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000003/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000005/000008 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000009/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000009/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000009/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000008/000001 lambda = 2.000037 x = [0.894433 -0.447203] y = [1.788893 -0.894433]\n000007/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000009/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000007/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000009/000002 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000007/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000007/000004 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n```\n\n**Environment**\nUbuntu 16.04\nGTX 1080\nCUDA 8.0 RC + Patch\ncudnn 5.1\nbazel 0.3.1\ngcc 5.4.0\n", "Same intermittent error for me (though I don't see nans):\n\n**Environment**\nUbuntu 16.04\n2xTitan X (Pascal)\nCUDA 7.5\ncudnn 5.1\n\n**gdb**\n\n```\ngdb bazel-bin/tensorflow/cc/tutorials_example_trainer\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\nCopyright (C) 2016 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from bazel-bin/tensorflow/cc/tutorials_example_trainer...(no debugging symbols found)...done.\n(gdb) r --use-gpu\nStarting program: /home/maxim/.cache/bazel/_bazel_maxim/703ad90c2d94a2fe53038a3a66b484c5/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer --use-gpu\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally\nUnknown flag: --use-gpu\n[Inferior 1 (process 7289) exited with code 0377]\n(gdb) r --use_gpu\nStarting program: /home/maxim/.cache/bazel/_bazel_maxim/703ad90c2d94a2fe53038a3a66b484c5/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally\n[New Thread 0x7fffe632e700 (LWP 7295)]\n[New Thread 0x7fffe5b2d700 (LWP 7296)]\n[New Thread 0x7fffe532c700 (LWP 7297)]\n[New Thread 0x7fffe4b2b700 (LWP 7298)]\n[New Thread 0x7fffdffff700 (LWP 7299)]\n[New Thread 0x7fffdf7fe700 (LWP 7300)]\n[New Thread 0x7fffdeffd700 (LWP 7301)]\n[New Thread 0x7fffde7fc700 (LWP 7302)]\n[New Thread 0x7fffddffb700 (LWP 7303)]\n[New Thread 0x7fffdd7fa700 (LWP 7304)]\n[New Thread 0x7fffdcff9700 (LWP 7305)]\n[New Thread 0x7fffdc7f8700 (LWP 7306)]\n[New Thread 0x7fffdbff7700 (LWP 7307)]\n[New Thread 0x7fffd57f6700 (LWP 7315)]\n[New Thread 0x7fffd4ff5700 (LWP 7316)]\n[New Thread 0x7fffd47f4700 (LWP 7317)]\n[New Thread 0x7fffd3ff3700 (LWP 7318)]\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\npciBusID 0000:06:00.0\nTotal memory: 11.90GiB\nFree memory: 11.75GiB\n[New Thread 0x7fffd37f2700 (LWP 7319)]\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x7fffe0046af0\n[New Thread 0x7fffd2ff1700 (LWP 7320)]\n[New Thread 0x7fffd27f0700 (LWP 7321)]\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\npciBusID 0000:05:00.0\nTotal memory: 11.90GiB\nFree memory: 11.37GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:05:00.0)\n[New Thread 0x7fffd1fef700 (LWP 7322)]\n[New Thread 0x7fffd17ee700 (LWP 7323)]\n[New Thread 0x7fffd0fed700 (LWP 7324)]\n[New Thread 0x7fffc3fff700 (LWP 7325)]\n[New Thread 0x7fffc37fe700 (LWP 7326)]\n[New Thread 0x7fffc2ffd700 (LWP 7327)]\n[New Thread 0x7fffc27fc700 (LWP 7328)]\n[New Thread 0x7fffc1ffb700 (LWP 7329)]\n[New Thread 0x7fffc17fa700 (LWP 7330)]\n[New Thread 0x7fffc0ff9700 (LWP 7331)]\n[New Thread 0x7fffa608e700 (LWP 7332)]\n[New Thread 0x7fffa588d700 (LWP 7333)]\n[New Thread 0x7fffa508c700 (LWP 7334)]\n[New Thread 0x7fffa488b700 (LWP 7335)]\n[New Thread 0x7fffa408a700 (LWP 7336)]\n[New Thread 0x7fffa3889700 (LWP 7337)]\n[New Thread 0x7fffa3088700 (LWP 7338)]\n[New Thread 0x7fffa2887700 (LWP 7339)]\n[New Thread 0x7fffa2086700 (LWP 7340)]\n[New Thread 0x7fffa1885700 (LWP 7341)]\n[New Thread 0x7fffa1084700 (LWP 7342)]\n[New Thread 0x7fffa0883700 (LWP 7343)]\n[New Thread 0x7fffa0082700 (LWP 7344)]\n[New Thread 0x7fff9f881700 (LWP 7345)]\n[New Thread 0x7fff9f080700 (LWP 7346)]\n[New Thread 0x7fff9e87f700 (LWP 7347)]\n[New Thread 0x7fff9e07e700 (LWP 7348)]\n\nThread 36 \"tutorials_examp\" received signal SIGFPE, Arithmetic exception.\n[Switching to Thread 0x7fffa408a700 (LWP 7336)]\n0x00005555560df621 in tensorflow::functor::CastFunctor<Eigen::GpuDevice, float, int>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16>)\n    ()\n(gdb)\n```\n", "Same here:\n\n**Environment**\nUbuntu 16.04\n1xTitan X (Pascal)\nCUDA 8.0 RC, 367.35\ncudnn 5.1\n\n**gdb**\n\n```\ntjy@thresh:~/Libraries/tensorflow$ gdb bazel-bin/tensorflow/cc/tutorials_example_trainer\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\nCopyright (C) 2016 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from bazel-bin/tensorflow/cc/tutorials_example_trainer...(no debugging symbols found)...done.\n(gdb) r --use_gpu\nStarting program: /home/tjy/.cache/bazel/_bazel_tjy/3585b37c520dd48091f052dff46832b7/execroot/tensorflow/bazel-out/local_linux-py3-opt/bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcublas.so.8.0. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_blas.cc:2289] Unable to load cuBLAS DSO.\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcudnn.so.5. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_dnn.cc:3304] Unable to load cuDNN DSO\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcufft.so.8.0. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_fft.cc:343] Unable to load cuFFT DSO.\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcurand.so.8.0. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.\n[New Thread 0x7ffff5cc8700 (LWP 12891)]\n[New Thread 0x7ffff54c7700 (LWP 12892)]\n[New Thread 0x7ffff4cc6700 (LWP 12893)]\n[New Thread 0x7fffeffff700 (LWP 12894)]\n[New Thread 0x7fffef7fe700 (LWP 12895)]\n[New Thread 0x7fffeeffd700 (LWP 12896)]\n[New Thread 0x7fffee7fc700 (LWP 12897)]\n[New Thread 0x7fffedffb700 (LWP 12898)]\n[New Thread 0x7fffed7fa700 (LWP 12899)]\n[New Thread 0x7fffecff9700 (LWP 12900)]\n[New Thread 0x7fffec7f8700 (LWP 12901)]\n[New Thread 0x7fffebff7700 (LWP 12902)]\n[New Thread 0x7fffeb7f6700 (LWP 12903)]\n[New Thread 0x7fffeaff5700 (LWP 12904)]\n[New Thread 0x7fffea7f4700 (LWP 12905)]\n[New Thread 0x7fffe9ff3700 (LWP 12906)]\n[New Thread 0x7fffe97f2700 (LWP 12907)]\n[New Thread 0x7fffe8ff1700 (LWP 12908)]\n[New Thread 0x7fffe87f0700 (LWP 12909)]\n[New Thread 0x7fffe7fef700 (LWP 12910)]\n[New Thread 0x7fffe77ee700 (LWP 12911)]\n[New Thread 0x7fffe6fed700 (LWP 12912)]\n[New Thread 0x7fffe67ec700 (LWP 12913)]\n[New Thread 0x7fffe5feb700 (LWP 12914)]\n[New Thread 0x7fffe57ea700 (LWP 12915)]\n[New Thread 0x7fffe4fe9700 (LWP 12916)]\n[New Thread 0x7fffe47e8700 (LWP 12917)]\n[New Thread 0x7fffe3fe7700 (LWP 12918)]\n[New Thread 0x7fffe37e6700 (LWP 12919)]\n[New Thread 0x7fffe2fe5700 (LWP 12920)]\n[New Thread 0x7fffe27e4700 (LWP 12921)]\n[New Thread 0x7fffe1fe3700 (LWP 12922)]\n[New Thread 0x7fffe17e2700 (LWP 12923)]\n[New Thread 0x7fffdafe1700 (LWP 12929)]\n[New Thread 0x7fffda7e0700 (LWP 12930)]\n[New Thread 0x7fffd9fdf700 (LWP 12931)]\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: TITAN X\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\npciBusID 0000:09:00.0\nTotal memory: 11.90GiB\nFree memory: 11.76GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X, pci bus id: 0000:09:00.0)\n[New Thread 0x7fffd97de700 (LWP 12932)]\n[New Thread 0x7fffd8fdd700 (LWP 12933)]\n[New Thread 0x7fffd3fff700 (LWP 12934)]\n[New Thread 0x7fffd37fe700 (LWP 12935)]\n[New Thread 0x7fffd2ffd700 (LWP 12936)]\n[New Thread 0x7fffd27fc700 (LWP 12937)]\n[New Thread 0x7fffd1ffb700 (LWP 12938)]\n[New Thread 0x7fffd17fa700 (LWP 12939)]\n[New Thread 0x7fffd0ff9700 (LWP 12940)]\n[New Thread 0x7fffc0d4c700 (LWP 12941)]\n[New Thread 0x7fffc054b700 (LWP 12942)]\n[New Thread 0x7fffbfd4a700 (LWP 12943)]\n[New Thread 0x7fffbf549700 (LWP 12944)]\n[New Thread 0x7fffbed48700 (LWP 12945)]\n[New Thread 0x7fffbe547700 (LWP 12946)]\n[New Thread 0x7fffbdd46700 (LWP 12947)]\n[New Thread 0x7fffbd545700 (LWP 12948)]\n[New Thread 0x7fffbcd44700 (LWP 12949)]\n[New Thread 0x7fffbc543700 (LWP 12950)]\n[New Thread 0x7fffbbd42700 (LWP 12951)]\n[New Thread 0x7fffbb541700 (LWP 12952)]\n[New Thread 0x7fffbad40700 (LWP 12953)]\n[New Thread 0x7fffba53f700 (LWP 12954)]\n[New Thread 0x7fffb9d3e700 (LWP 12955)]\n[New Thread 0x7fffb953d700 (LWP 12956)]\n[New Thread 0x7fffb8d3c700 (LWP 12957)]\n[New Thread 0x7fffb853b700 (LWP 12958)]\n[New Thread 0x7fffb7d3a700 (LWP 12959)]\n[New Thread 0x7fffb7539700 (LWP 12960)]\n[New Thread 0x7fffb6d38700 (LWP 12961)]\n[New Thread 0x7fffb6537700 (LWP 12962)]\n[New Thread 0x7fffb5d36700 (LWP 12963)]\n[New Thread 0x7fffb5535700 (LWP 12964)]\n[New Thread 0x7fffb4d34700 (LWP 12965)]\n[New Thread 0x7fffb4533700 (LWP 12966)]\n[New Thread 0x7fffb3d32700 (LWP 12967)]\n[New Thread 0x7fffb3531700 (LWP 12968)]\n[New Thread 0x7fffb2d30700 (LWP 12969)]\n[New Thread 0x7fffb252f700 (LWP 12970)]\n[New Thread 0x7fffb1d2e700 (LWP 12971)]\n[New Thread 0x7fffb152d700 (LWP 12972)]\n[New Thread 0x7fffb0d2c700 (LWP 12973)]\n[New Thread 0x7fffb052b700 (LWP 12974)]\n[New Thread 0x7fffafd2a700 (LWP 12975)]\n[New Thread 0x7fffaf529700 (LWP 12976)]\n\nThread 69 \"tutorials_examp\" received signal SIGFPE, Arithmetic exception.\n[Switching to Thread 0x7fffb5d36700 (LWP 12963)]\n0x0000555556158c1e in tensorflow::functor::CastFunctor<Eigen::GpuDevice, float, int>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16>) ()\n(gdb) \n```\n", "Same here running a TITAN X (Pascal) on Ubuntu 16.04, CUDA 8 + patch, cuDNN 5.1 and Bazel 0.3.1\n\nAnd same kind of result when running gdb.\n", "Hello, \n\nSame problem here as well.  GTX 1080, Ubuntu 14.04, CUDA 8.0, CuDNN 5.1, nvidia driver 367.27.  `--num_concurrent_sessions=1` does _not_ seem to change anything for me, either.  With `--use_gpu`, roughly 1 in 10-15 runs finishes without error.  Without `--use_gpu`, it works as expected.\n", "I tried to use r0.10 instead of the head of the master branch, and it works correctly.\n", "The faulty values for the grid sizes are caused by a device not being initalized (but passed to the TensorExecutor). Can be fixed with the attached patch to the TensorExecutor.\n\nApply from tensorflow base directory:\n`patch ./bazel-tensorflow/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h < patch.txt`\n\nShould fix this issue for now.\n[patch.txt](https://github.com/tensorflow/tensorflow/files/473783/patch.txt)\n", "Adding @benoitsteiner to review this Eigen patch. \n", "@girving I encountered the same problem just like you guys. it came to me that the exceptions do not occur when the system is under heavy load. so either this Eigen package is assuming cuda initialization order without a guarantee or nvidia is changing their protocol.\n", "@fheide @yangsiwei880813 Can you try with the latest version ? The problem you are facing could be due to a race condition in the device initialization code that that is now fixed.  \n", "I'm assuming this is fixed. Please reopen if this isn't the case.\n", "I have the same issue while training. I tried reducing batch size and # of layers but throws the same error.\r\nThe core dumped message pops up just before completing one epoch.\r\ntensorflow version = 1.8\r\n\r\n`61/62 [============================>.] - ETA: 0s - loss: 0.4864 - dice_coef: 0.5136[1] 24623 floating point exception (core dumped)`", "exactly same problem with me .have you found any solution? @bibinmjose ", "@008karan This was happening when I divided the input-image/255. Pass input-image as it is. Let me know if it works", "In my case I provided a batch of size 0 by mistake."]}, {"number": 3916, "title": "some change to Mnist_softmax.py", "body": "### Environment info\n\nOperating System:\nubuntu 14.0\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n_ls -l /path/to/cuda/lib/libcud_\nIf installed from binary pip package, provide:\n_-rw-r--r-- 1 root root 322936  8\u6708 16  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16  8\u6708 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19  8\u6708 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336  8\u6708 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192  8\u6708 16  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a_\n1. Which pip package you installed.\n_pip 1.5.4 from /usr/lib/python2.7/dist-packages (python 2.7)_\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n_0.10.0rc0_\n### Steps to reproduce\n\n1.When i run the original ~/Tensorflow Modles/tensorflow-master/tensorflow/examples/tutorials/mnist/mnist_softmax.py ,i got accuray :91.9%\n2.But after i change follow code:\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x, W) + b)\nto:\nW1 = tf.Variable(tf.zeros([784, 1000]))\nb1 = tf.Variable(tf.zeros([1000]))\nW2 = tf.Variable(tf.zeros([1000, 10]))\nb2 = tf.Variable(tf.zeros([10]))\nh1=tf.nn.sigmoid(tf.matmul(x, W1) + b1)\ny = tf.nn.softmax(tf.matmul(h1, W2) + b2)\nthe accuracy was down to about 30%\nand it seemed that no or very little convergence happened even after changed the learning rate.\n\nwhy?what happened?Could somebody help me?\n", "comments": ["after i change the initial method for W1 and W2 to tf.random_normal , the problem is solved.\n But why?\nW1 was all zero,that means no gradient was pass to W1;\nbut b1 was not zero,but all same value;\nAnd there were so many same value in W2 or b2.\nWhy?\n", "Is it that the train time was no enough for my model to get rid of the local regoin?\n", "Oh,i see, such a struggle for my model!\n", "This is a general question best suited to StackOverflow.  Can you please re-ask it there.\n"]}, {"number": 3915, "title": "update message to ask user to also install scipy", "body": "Following the word2vec tutorial instructions I tried \"pip3 install sklearn matplotlib\" but this didn't install the required \"scipy\". Adding that to the ImportError message.\n\n```\n>>> from sklearn.manifold import TSNE\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/__init__.py\", line 57, in <module>\n    from .base import clone\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/base.py\", line 9, in <module>\n    from scipy import sparse\nImportError: No module named 'scipy'\n```\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "done.\n\nOn Fri, Aug 19, 2016 at 12:54 PM googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3915#issuecomment-240927566,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAHwCnY8V03Cmg5u0vGylWd-lbku5Exwks5qhTcQgaJpZM4JoJRv\n> .\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks!\n", "Can one of the admins verify this patch?\n"]}, {"number": 3914, "title": "Fix typo in documentation of `gate_gradients`", "body": "The documentation said `gate_gradient` in several places; however, the\noption is named `gate_gradients`. This PR fixes this in all the\nplaces that `gate_gradient` occurs.\n", "comments": ["Thanks!\n", "Can one of the admins verify this patch?\n"]}, {"number": 3913, "title": "Error during build: no such package '@paper_radio_group//'", "body": "Hi All. Building python bindings gives me error:\n\n`\nroot@host# bazel build -c opt --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package\n...\n\nWARNING: /root/.cache/bazel/_bazel_root/4b98d0d2e8f34611cfd0d274c46b2eaf/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/profiler.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nERROR: /data/github/google/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_group//': Error cloning repository: Unexpected end of file from server caused by Unexpected end of file from server caused by Unexpected end of file from server and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 1.857s\n`\nIs there a workaround for this problem?\n", "comments": ["can you provide tensorflow, bazel's version, and os info?\n", "tensorflow compiled from git source:\n\ncommit d8dddca5b11212ec6e8fe372d774d60f452dab24\nMerge: 858c170 1a9b107\n\nbazel installed via apt on ubuntu and shows in basel version:\n\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n", "I can't reproduce. I removed bazel cache directory in `~/.cache/bazel/`, and run `bazel clean`, then rebuild tensorflow, everything is ok, I use the latest tensorflow and bazel 0.3.1(also the latest) on ubuntu16.04. \nIt seems you failed to clone `paper_radio_group` from github.\n", "Try cleaning out your caches and trying a fresh rebuild (start from scratch in a new directory). Transient failures of git fetches have been known to happen if ssl certificates temporarily get out of whack. Let us know if that doesn't fix it. (can't find the issue where I saw this before at the moment)\n", "@dimon777 , I hope the advice above helped you resolve your build issue.  Closing this out as there hasn't been any activity for a while.  Please ping this issue if the problem persists.\n"]}, {"number": 3912, "title": "Tensorflow Data Corruption after Training", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Mac OSX El Capitan\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nThe latest tensorflow.\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n0.10.0rc0\n### Steps to reproduce\n1. In order to reproduce, I have used the main function:\n   \n   import Input\n   import Process\n   \n   import time\n   import numpy as np\n   import os\n   \n   import tensorflow as tf\n   from datetime import datetime\n   \n   FLAGS = tf.app.flags.FLAGS\n   \n   def train():\n   \n   ```\n   with tf.Session() as sess:\n   \n       images, labels = Process.inputs()\n   \n       forward_propgation_results = Process.forward_propagation(images)\n   \n       cost, train_loss = Process.error(forward_propgation_results, labels)\n   \n       image_summary_t = tf.image_summary(images.name, images, max_images = 2)\n   \n       summary_op = tf.merge_all_summaries()\n   \n       init = tf.initialize_all_variables()\n   \n       saver = tf.train.Saver()\n   \n       sess.run(init)\n   \n       saver = tf.train.Saver(tf.all_variables())\n   \n       tf.train.start_queue_runners(sess = sess)\n   \n       train_dir = \"/Users/Zanhuang/Desktop/NNP/model.ckpt\"\n   \n       summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)\n   \n       for step in range(100):\n           start_time = time.time()\n           print(sess.run([train_loss, cost]))\n           duration = time.time() - start_time\n           if step % 1 == 0:\n               num_examples_per_step = FLAGS.batch_size\n               examples_per_sec = num_examples_per_step / duration\n               sec_per_batch = float(duration)\n   \n               format_str = ('%s: step %d, (%.1f examples/sec; %.3f ''sec/batch)')\n               print (format_str % (datetime.now(), step, examples_per_sec, sec_per_batch))\n   \n               summary_str = sess.run(summary_op)\n               summary_writer.add_summary(summary_str, step)\n   \n   \n               if step % 2 == 0:\n                   checkpoint_path = os.path.join(FLAGS.data_dir, 'model.ckpt')\n                   saver.save(sess, checkpoint_path, global_step = step)\n   ```\n   \n   def main(argv = None):\n       train()\n   \n   if **name** == '**main**':\n     tf.app.run()\n\nThen the computation graph seems normal while running but after a few minutes after, the displayed images (in a similar to cifar10 format) shifts and turns into weird unrecognizable colors and the training loss which is normally descending normally is now displayed chaotically. This is all sudden after a few reloads a few minutes after training. I am absolutely sure there is nothing wrong with my hard drive.\n### What have you tried?\n1. I have tried retraining.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["weird unrecognizable colors like this? -- \n<img width=\"343\" alt=\"screen shot 2016-08-18 at 7 06 44 pm\" src=\"https://cloud.githubusercontent.com/assets/23068/17796639/f67a4eae-6576-11e6-9873-32c849611f0c.png\"> \n\n(from \nhttps://github.com/tensorflow/tensorflow/issues/3816 )\n", "No. It's supposed to show the original image but it suddenly turns into\nrandom pixels randomly scattered.\n\nOn Aug 18, 2016 10:09 PM, \"Yaroslav Bulatov\" notifications@github.com\nwrote:\n\n> weird unrecognizable colors like this? --\n> [image: screen shot 2016-08-18 at 7 06 44 pm]\n> https://cloud.githubusercontent.com/assets/23068/17796639/f67a4eae-6576-11e6-9873-32c849611f0c.png\n> (from\n> #3816 https://github.com/tensorflow/tensorflow/issues/3816 )\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3912#issuecomment-240909275,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AP-ZENTvNvH-mmmjwN-KtXVDe24_Nejuks5qhRBlgaJpZM4JoEpp\n> .\n", "One way to get \"random data\" is an uninitialized memory read. However, such bugs are quickly fixed and the thing to ask is what you are doing that is unusual so that it wouldn't get caught in the gazillion hours of training on Google computers. Some ideas to isolate:\n1. If this happens with CPU training, does it also reoccur in GPU training? And vica versa \n2. If you make your data order deterministic, does it happen after the same number of steps, ie on the same example?\n\nIt's also possible that it's some kind of weird interaction with your drivers and hardware. For instance, on my MacBook GPU I've encountered weird behavior at some point. I tracked it down to \"tf.add(1, 1)\" producing -10. It had something to do with CUDA/VideoCard interaction/OS because once I disabled Chrome hardware acceleration, the behavior disappeared, so one thing to try is upgrading your drivers/trying different hardware setup\n", "Have you managed to find the source of your problem @StructML?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 3911, "title": "Test DO NOT MERGE", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 3910, "title": "R0.10", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(assuming this is an accident)\n"]}, {"number": 3909, "title": "libstdc++ cannot be found", "body": "I receive the following error when executing:\n\n`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures`\n\n> ERROR: /home/XX/.cache/bazel/_bazel_XX/2e273cfba15d637070500a6c0bde03c6/external/protobuf/BUILD:331:1: Linking of rule '@protobuf//:protoc' failed: gcc failed: error executing command \n>   (cd /home/XX/.cache/bazel/_bazel_XX/2e273cfba15d637070500a6c0bde03c6/execroot/tensorflow && \\\n>   exec env - \\\n>   /software/gcc/4.9.3/bin/gcc -o bazel-out/host/bin/external/protobuf/protoc bazel-out/host/bin/external/protobuf/_objs/protoc/external/protobuf/src/google/protobuf/compiler/main.o bazel-out/host/bin/external/protobuf/libprotoc_lib.a bazel-out/host/bin/external/protobuf/libprotobuf.a bazel-out/host/bin/external/protobuf/libprotobuf_lite.a -lpthread -L/software/gcc/4.9.3/lib64/ -Wl,-rpath,/software/gcc/4.9.3/lib64/ -B/usr/bin/ '-lstdc++ -L/software/gcc/4.9.3/lib64/' -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,-S -Wl,--gc-sections): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n> /usr/bin/ld: cannot find -lstdc++ -L/software/gcc/4.9.3/lib64/\n\nI know for sure that in /software/gcc/4.9.3/lib64/ libstdc++ exists. I cannot find the source of the problem.\n\nHere is an excerpt of my CROSSTOOl\n\n> tool_path { name: \"ar\" path: \"/usr/bin/ar\" }\n>   tool_path { name: \"compat-ld\" path: \"/usr/bin/ld\" }\n>   tool_path { name: \"cpp\" path: \"/software/gcc/4.9.3/bin/cpp\" }\n>   tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n>   tool_path { name: \"gcc\" path: \"/software/gcc/4.9.3/bin/gcc\" }\n> \n>   cxx_flag: \"-std=c++11\"\n>   #linker_flag: \"-L/software/gcc/4.9.3/lib/\n>   linker_flag: \"-L/software/gcc/4.9.3/lib64/\"\n>   #linker_flag: \"-Wl,-rpath,/software/gcc/4.9.3/lib/\n>   linker_flag: \"-Wl,-rpath,/software/gcc/4.9.3/lib64/\"\n>   linker_flag: \"-B/usr/bin/\"\n>   #linker_flag: \"-B/software/gcc/4.9.3/bin/\"\n>   linker_flag: \"-lstdc++ -L/software/gcc/4.9.3/lib64/\"\n> \n> cxx_builtin_include_directory: \"/software/gcc/4.9.3/lib64/gcc/\"\n>   #\"/usr/lib/gcc/\"\n>   cxx_builtin_include_directory: \"/software/gcc/4.9.3/lib64/\"\n>   #cxx_builtin_include_directory: \"/software/gcc/4.9.3/lib/\"\n>   cxx_builtin_include_directory: \"/software/gcc/4.9.3/include/\"\n>   cxx_builtin_include_directory: \"/usr/local/include\"\n>   cxx_builtin_include_directory: \"/usr/include\"\n>   cxx_builtin_include_directory: \"/software/nvidia/7.5.18/cuda/include\"\n\nI tested every possible configuration but cannot figure out why it does not work. I am using the latest. I am using:\nbazel release 0.3.1 and TF 0.10\n", "comments": ["Could you please do the following and paste the output here:\n # file /software/gcc/4.9.3/bin/gcc\n # find /software/gcc/4.9.3/lib64/\n\nDepending on the answer, here are the things to try:\n- If your gcc is 32-bit, then maybe install the corresponding 32-bit libraries as well.\n- If your libstdc++.so doesn't exist, or it's pointing to a non-existent file, make the link directly. For example,\n  ln -s libstdc++.so libstdc++.so.6.0.19\n", "@sherrym \nI am using this gcc version: x86_64-redhat-linux\nand the libstdc++.so exists in the folder already.\n\nEdit:\nI just noticed that: Today (so  I guess all tmp files from yesterday are removed ) I had to include\n\n> cxx_builtin_include_directory: \"/software/gcc/4.9.3/lib/\"\n\notherwise it wouln't come to the error above but crash before with the error: missing dependencies. I am still receiveing the error\n\nEdit:\nthe commands from above\n\n`/software/gcc/4.9.3/bin/gcc: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.18, not stripped\n`\n\n`/software/gcc/4.9.3/lib64/\n/software/gcc/4.9.3/lib64/libgcj.la\n/software/gcc/4.9.3/lib64/libquadmath.so.0.0.0\n/software/gcc/4.9.3/lib64/libgcj-tools.so.15\n/software/gcc/4.9.3/lib64/libcilkrts.so.5\n/software/gcc/4.9.3/lib64/libssp.la\n/software/gcc/4.9.3/lib64/libvtv.la\n/software/gcc/4.9.3/lib64/libgfortran.la\n/software/gcc/4.9.3/lib64/libgomp.la\n/software/gcc/4.9.3/lib64/libquadmath.a\n/software/gcc/4.9.3/lib64/libstdc++.a\n/software/gcc/4.9.3/lib64/libasan.la\n/software/gcc/4.9.3/lib64/libssp.so\n/software/gcc/4.9.3/lib64/libstdc++.so.6.0.20-gdb.py\n/software/gcc/4.9.3/lib64/libobjc.a\n/software/gcc/4.9.3/lib64/security\n/software/gcc/4.9.3/lib64/security/classpath.security\n/software/gcc/4.9.3/lib64/libgcj.so\n/software/gcc/4.9.3/lib64/logging.properties\n/software/gcc/4.9.3/lib64/libcilkrts.la\n/software/gcc/4.9.3/lib64/libatomic.a\n/software/gcc/4.9.3/lib64/libstdc++.so.6.0.20\n/software/gcc/4.9.3/lib64/libgij.so.15\n/software/gcc/4.9.3/lib64/libquadmath.so.0\n/software/gcc/4.9.3/lib64/libatomic.la\n/software/gcc/4.9.3/lib64/libssp.a\n/software/gcc/4.9.3/lib64/libgij.la\n/software/gcc/4.9.3/lib64/libgomp.a\n/software/gcc/4.9.3/lib64/libssp_nonshared.la\n/software/gcc/4.9.3/lib64/liblsan.a\n/software/gcc/4.9.3/lib64/libtsan.la\n/software/gcc/4.9.3/lib64/libgfortran.so.3\n/software/gcc/4.9.3/lib64/libvtv.so.0\n/software/gcc/4.9.3/lib64/libgomp.spec\n/software/gcc/4.9.3/lib64/libasan.a\n/software/gcc/4.9.3/lib64/libssp_nonshared.a\n/software/gcc/4.9.3/lib64/libgcc_s.so.1\n/software/gcc/4.9.3/lib64/libvtv.so\n/software/gcc/4.9.3/lib64/libsupc++.a\n/software/gcc/4.9.3/lib64/libstdc++.la\n/software/gcc/4.9.3/lib64/liblsan.so.0.0.0\n/software/gcc/4.9.3/lib64/libitm.a\n/software/gcc/4.9.3/lib64/libtsan.so.0\n/software/gcc/4.9.3/lib64/libobjc.so\n/software/gcc/4.9.3/lib64/libobjc.la\n/software/gcc/4.9.3/lib64/pkgconfig\n/software/gcc/4.9.3/lib64/pkgconfig/libgcj-4.9.pc\n/software/gcc/4.9.3/lib64/libasan.so.1\n/software/gcc/4.9.3/lib64/libatomic.so\n/software/gcc/4.9.3/lib64/libcilkrts.spec\n/software/gcc/4.9.3/lib64/libgcj.so.15.0.0\n/software/gcc/4.9.3/lib64/liblsan.so.0\n/software/gcc/4.9.3/lib64/libstdc++.so.6\n/software/gcc/4.9.3/lib64/libsupc++.la\n/software/gcc/4.9.3/lib64/libvtv.so.0.0.0\n/software/gcc/4.9.3/lib64/libasan.so.1.0.0\n/software/gcc/4.9.3/lib64/libgcj-tools.la\n/software/gcc/4.9.3/lib64/libssp.so.0.0.0\n/software/gcc/4.9.3/lib64/libgij.so\n/software/gcc/4.9.3/lib64/libquadmath.la\n/software/gcc/4.9.3/lib64/libgcj-tools.so.15.0.0\n/software/gcc/4.9.3/lib64/libasan.so\n/software/gcc/4.9.3/lib64/libitm.so.1.0.0\n/software/gcc/4.9.3/lib64/libobjc.so.4\n/software/gcc/4.9.3/lib64/libgomp.so\n/software/gcc/4.9.3/lib64/libitm.la\n/software/gcc/4.9.3/lib64/libgcj_bc.so.1.0.0\n/software/gcc/4.9.3/lib64/libgfortran.spec\n/software/gcc/4.9.3/lib64/libatomic.so.1.1.0\n/software/gcc/4.9.3/lib64/libubsan.so\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15/libgjsmalsa.so\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15/libjavamath.so\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15/classmap.db\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15/libgjsmalsa.la\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15/libjavamath.la\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15/libjvm.la\n/software/gcc/4.9.3/lib64/gcj-4.9.3-15/libjvm.so\n/software/gcc/4.9.3/lib64/libasan_preinit.o\n/software/gcc/4.9.3/lib64/libobjc.so.4.0.0\n/software/gcc/4.9.3/lib64/libgcj-tools.so\n/software/gcc/4.9.3/lib64/libssp.so.0\n/software/gcc/4.9.3/lib64/libcilkrts.a\n/software/gcc/4.9.3/lib64/libubsan.so.0.0.0\n/software/gcc/4.9.3/lib64/libgfortran.so\n/software/gcc/4.9.3/lib64/libgomp.so.1.0.0\n/software/gcc/4.9.3/lib64/libgcc_s.so\n/software/gcc/4.9.3/lib64/libatomic.so.1\n/software/gcc/4.9.3/lib64/libgomp.so.1\n/software/gcc/4.9.3/lib64/libstdc++.so\n/software/gcc/4.9.3/lib64/libgcj_bc.so.1\n/software/gcc/4.9.3/lib64/libgcj.so.15\n/software/gcc/4.9.3/lib64/libquadmath.so\n/software/gcc/4.9.3/lib64/libvtv.a\n/software/gcc/4.9.3/lib64/libtsan.a\n/software/gcc/4.9.3/lib64/libtsan.so\n/software/gcc/4.9.3/lib64/libgfortran.a\n/software/gcc/4.9.3/lib64/libubsan.so.0\n/software/gcc/4.9.3/lib64/libcilkrts.so.5.0.0\n/software/gcc/4.9.3/lib64/liblsan.so\n/software/gcc/4.9.3/lib64/libitm.so\n/software/gcc/4.9.3/lib64/libsanitizer.spec\n/software/gcc/4.9.3/lib64/libcilkrts.so\n/software/gcc/4.9.3/lib64/libgfortran.so.3.0.0\n/software/gcc/4.9.3/lib64/libgij.so.15.0.0\n/software/gcc/4.9.3/lib64/libitm.spec\n/software/gcc/4.9.3/lib64/libitm.so.1\n/software/gcc/4.9.3/lib64/liblsan.la\n/software/gcc/4.9.3/lib64/libubsan.a\n/software/gcc/4.9.3/lib64/libubsan.la\n/software/gcc/4.9.3/lib64/libtsan.so.0.0.0\n/software/gcc/4.9.3/lib64/libgcj_bc.so\n`\n\nEdit: I have compiled bazel myself. Could it be a problem that already bazel is using a wrong version?\n\nEdit: I have avoided the problem by changing to Tensorflow 0.9. There I have other issues but that seems to be resolved?\n", "What version of Linux are you on?  e.g. run \n$cat /etc/issue\n", "Closing due to lack of recent activity. Please reopen if more information becomes available.\n"]}, {"number": 3908, "title": "Branch 130665817", "body": "", "comments": ["Jenkins, test this please.\n", "Please feel free to ignore the MAC issues for now.\nI am taking them off of presubmits now.\nYou should only look at the python3 test failures.\n"]}, {"number": 3907, "title": "Feature Request: Sparse Variables", "body": "I want to train a sparse model like this:\n\n```\nParam=tf.Variable(tf.SparceTensor(indices,values,shape))\ncost=some_function_of_Params\n```\n\ntf.Variable(tf.SparseTensor) generates error:\n\n`Expected binary or unicode string, got <tensorflow.python.framework.ops.SparseTensor object at 0x7f994788a550>`\n\nMy solution was to use dense matrices as variables, mask the gradients and then apply them (mask is a tensor containing zeros and ones):\n\n```\ninit=a_dense_matrix\n\nparam=tf.variable(init)\n\ncost=some_function_of_param\n\nmask=a_dense_matrix_shaped_just_like_param\n\nmask=tf.constant(mask)\n\noptimizer=tf.GradientDescentOptimizer(learning_rate=0.01)\n\ngrad=optimizer.compute_gradients(cost)\n\nmasked_grad=tf.mul(grad,mask)\n\nupdate_rule=optimizer.apply_gradients(masked_grad)\n```\n\nThis however includes lots of redundant gradient computations and zero_masking multiplications. Doing updates on sparse variables instead of dense variables can speedup the training. Is there a plan to make it possible that sparse tensors could be used as variables in the graph? \n", "comments": ["This feature is also needed when accumulating gradients. \n\nI want to accumulate the gradients of word embeddings over several iterations and update them together afterwards. Without sparse variable support, I can only initialize a huge zero tensor with the same size as the word embedding matrix first, accumulate the gradients with it and tf.assign_add this huge tensor to the word embedding matrix, which is very inefficient.\n\nDoes anyone have some workarounds for this kind of things?\n", "@bfluo Did you try `tf.scatter_{assign,add,sub}()`?\n\nAs for the original question: this is a feature that requires design and we don't have an immediate timeline.  Adding @alextp in case he has more information to comment.\n", "@concretevitamin I'm not sure what you mean about `tf.scatter_{assign,add,sub}()`.\n\nI accumulate the gradients using the method mentioned in issue #3994. To be more specific, in order to update variable `var`, I first create a zero initialized variable `accum_var` to hold the accumulated gradients, and I add the gradients to `accum_var` in every iteration. Finally I add `accum_var` to `var` after several iterations. \n\nThis method works fine except for the situation where `var` is the word embedding matrix. In this situation, I want `accum_var` to be something like `IndexedSlices` or `SparseTensor` so that I can use functions like `tf.scatter_add()` to update the word embedding matrix efficiently. However, I'm not sure how `tf.scatter_{assign,add,sub}()` alone can solve this problem. Do you mean evaluating `IndexedSlices.indices` and `IndexedSlices.values` of word embedding gradient every iteration, accumulate them outside of the graph and use `tf.scatter_add()` to update afterwards?\n", "I tried evaluating `IndexedSlices.indices` and `IndexedSlices.values` every iteration and accumulating them outside of the graph. This makes my program 2x faster. Although not elegant, this seems like a feasible workaround for now.\n", "@pfllo why do you want to accumulate sparse gradients and then add them to a dense variable? Why not having one call of the optimizer per iteration?\n\n@MohammadSamragh you can use a dense variable to store the weights and have efficient code if your gradients are sparse (and stored as indexedslices). What op is producing these gradients as SparseTensors which require a mask?\n", "@alextp I am actually doing multi-instance learning, where the gold label is inferred from a bag of instances rather than one instance. \n\nSince each bag usually contains a different number of instances, I can't feed a batch of bags directly to the graph in one iteration (I don't want to pad the bags so that each of them will have the same size. This will be very inefficient). Instead, to train with a batch of 16 bags, I need to process one bag per iteration, accumulate its gradients, and update the accumulated gradients after 16 iterations. \n", "@pfllo ah I see. Doing what you're doing then seems to be ideal. Do you think the API should be simpler? \n", "@alextp I'm not sure which API you are referring to, but I think it will be better if tensorflow support sparse variables. Although I can accumulate `IndexedSlices` out side of the graph, this workaround is still not very elegant. And sparse variable seems like a more natural way to solve @MohammadSamragh's problem, which is more efficient than using mask.\n", "@MohammadSamragh's problem seems solvable by using IndexedSlices as gradients. I don't understand why a mask is necessary here.\n\nI do think there should be a way of accumulating indexedslices gradients before adding them to a variable. I believe @jmchen-g is working on this.\n", "It is very nice to hear that someone is working on accumulating `IndexedSlices` gradients. As for @MohammadSamragh's problem, I'm not sure about his particular use case, but I think that he is expecting a more general sparsity that can not be represented by `IndexedSlices` (e.g. only position [0,0] and [2, 3] have non-zero values in a 4*5 matrix).\n", "Yes, but that problem can be solved by doing a sparse_tensor_+= operator, which is simpler to support than a sparse tensor variable.\n", "Haven't followed through the discussion -- maybe take a look at\ntf.sparse_add()?\n\nOn Thursday, September 8, 2016, Alexandre Passos notifications@github.com\nwrote:\n\n> Yes, but that problem can be solved by doing a sparse_tensor_+= operator,\n> which is simpler to support than a sparse tensor variable.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3907#issuecomment-245798838,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAkLHpElFUfwknlu7qwnk1YI2tFj6_VDks5qoMBxgaJpZM4Jnyrq\n> .\n", "I suppose @MohammadSamragh wants to have a sparse weight matrix `W`, and do something like `tf.matmul(input, W)`. Then the major problem is how to get the sparse gradient with regard to the non-zero values of `W` efficiently (without mask). I'm not sure how sparse_tensor_+= operator or `tf.sparse_add()` can solve this problem. However, I agree that implementing a new operation will be a better idea than supporting sparse variable. It will be helpful if @MohammadSamragh can elaborate his use case.\n", "Sorry for my late response guys, I had to get back to my codes to see what I have done.\n\n@pfllo You got my question well. Assume that `y=tf.matmul(input,W)`; if `W` is a dense matrix, then `tf.gradients(y,W)` returns a dense matrix. My question is how to make `tf.gradients(y,W)` ignore gradient computation of some indices in `W` and compute them only in some known indices. Basically I want to implement the Pruning idea in neural networks described in this [paper](https://arxiv.org/abs/1506.02626). They claim that they have implemented this using Caffe, but I'm not sure how you can implement it using TF. \n\n@alextp I have tried using `tf.gather` and `tf.gradients` together, I opened a Stackoverflow  question [here](http://stackoverflow.com/questions/38063430/using-tensorflows-tf-gather-and-tf-gradients-together)\nI used the following code:\n\n```\nsparse_X=tf.gather(W,indices)\ngrads_i_want_to_compute = tf.gradient(y, sparse_W)\n```\n\nthe problem is that `grads_i_want_to_compute` becomes a `None` object. I assume this happens because `sparse_W` is not considered as part of the graph and the gradients of `y` with respect to  `sparse_W` become `None`.\n", "Why is sparse_W not a part of the graph?\n", "@alextp I just know that `tf.gradients(y,sparse_W)` returns `None`; I don't know why that happens. I read somewhere that one reason for this might be that  `y` is independent from `sparse_W` as in the following code:\n\n```\ny=2*W\nf=tf.gradients(y,x)\n```\n\nwhere changing `x` does not affect `y`. Is there a way I could check if `y` and `sparse_W`  are dependent? \n", "I don't know many graph connectivity things which would let you check this other than the gradient code itself. You can try replacing sparse_w with a placeholder, evaluating y without feeding sparse_w (but feeding the other tensors which y depends on), and see if something fails.\n", "I ran into the same issue when I tried to use `tf.train.slice_input_producer` with a sparse input. I'm not sure if there is a better way to handle my use case currently (I'm working on a bag-of-words model and each example is a list of word indices)\n", "@MohammadSamragh looks here is a [workaround:](http://stackoverflow.com/questions/37001686/using-sparsetensor-as-a-trainable-variable)\n", "Closing due to inactivity. Let us know if the problem still persists and if @amxineohp fix above worked.", "Is there any updates on training on sparse variables? I have the same question and cannot find out whether the most updated tensorflow supports this kind of function. Thanks!", "Not as of august 2017\n\nOn Sat, Aug 12, 2017 at 11:38 AM, Yiru Shen <notifications@github.com>\nwrote:\n\n> Is there any updates on training on sparse variables? I have the same\n> question and cannot find whether the most updated tensorflow supports this\n> kind of function. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/3907#issuecomment-321998380>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZ9qL7XLiny9oePGHzu7rSCptsTIks5sXfE4gaJpZM4Jnyrq>\n> .\n>\n\n\n\n-- \n - Alex\n", "Any update? \r\ni have the same question to train on sparse variables?\r\nis there any solution of this problem  or not?\r\n", "Not yet.\n\nOn Thu, Feb 22, 2018 at 4:32 AM, Arbish <notifications@github.com> wrote:\n\n> Any update?\n> i have the same question to train on sparse variables?\n> is there any solution of this problem or not?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/3907#issuecomment-367665753>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXA445MvQP3F5LlNWv4XMjAA-_1-ks5tXV5kgaJpZM4Jnyrq>\n> .\n>\n\n\n\n-- \n - Alex\n", "same needs\r\n\r\nany update in latest version", "same needs + 1", "Same need", "Same, still needed", "I would need this feature too. I find that this thread contradicts some older threads, where people said that training sparse weight matrices with fixed indices is possible.", "Training with fixed indices is fine because then you have a _dense_\nvariable for the values and just make a SparseTensor out of your fixed\nindices and those values.\n\nOn Tue, Sep 3, 2019 at 1:48 AM L\u00e1szl\u00f3 M\u00e9r\u0151 <notifications@github.com> wrote:\n\n> I would need this feature too. I find that this thread contradicts some\n> older threads, where people said that training sparse weight matrices with\n> fixed indices is possible.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/3907?email_source=notifications&email_token=AAABHRIISLY3FUDE7MPJTGLQHYQFPA5CNFSM4CM7FLVKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5XO3TA#issuecomment-527363532>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROD5T65Y3EJ3ZGZEWDQHYQFPANCNFSM4CM7FLVA>\n> .\n>\n\n\n-- \n - Alex\n", "I found out that this is true for stable tensorflow, but for 2.0rc the\ngradients are lost somewhere, possibly the gradient op is not defined for\nsparse dense matmul.\n\nAlexandre Passos <notifications@github.com> ezt \u00edrta (id\u0151pont: 2019. szept.\n3., K, 18:14):\n\n> Training with fixed indices is fine because then you have a _dense_\n> variable for the values and just make a SparseTensor out of your fixed\n> indices and those values.\n>\n> On Tue, Sep 3, 2019 at 1:48 AM L\u00e1szl\u00f3 M\u00e9r\u0151 <notifications@github.com>\n> wrote:\n>\n> > I would need this feature too. I find that this thread contradicts some\n> > older threads, where people said that training sparse weight matrices\n> with\n> > fixed indices is possible.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/3907?email_source=notifications&email_token=AAABHRIISLY3FUDE7MPJTGLQHYQFPA5CNFSM4CM7FLVKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5XO3TA#issuecomment-527363532\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AAABHROD5T65Y3EJ3ZGZEWDQHYQFPANCNFSM4CM7FLVA\n> >\n> > .\n> >\n>\n>\n> --\n> - Alex\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/3907?email_source=notifications&email_token=AALT3PB7EXOQJUDMZTSKMJTQH2EOPA5CNFSM4CM7FLVKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5YWZFA#issuecomment-527527060>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALT3PC7CZYPN6QRHNMFHGTQH2EOPANCNFSM4CM7FLVA>\n> .\n>\n", "This would be really useful to use functionally as an expandable hashtable. I need to track data ids inside a tensorflow graph :/ "]}, {"number": 3906, "title": "unterminated substitute pattern in download_dependencies.sh", "body": "### Environment info\n\nOperating System:\nOS X (El capitan)\n\nI'm trying to build a static library with the end goal of using tensorflow for mobile apps. When I run the download_dependencies script I get this terminal log:\n<img width=\"775\" alt=\"screen shot 2016-08-18 at 2 10 30 pm\" src=\"https://cloud.githubusercontent.com/assets/12484426/17785182/b8263b96-654d-11e6-890b-44f9beb6750f.png\">\n\nTo an untrained eye it seems like a potentially easy fix but I have no experience with what would be required to fix it so if anyone has any suggestions/could point me in the right direction that would be dope.\n\nThanks\n", "comments": ["@satok16 can you take a look?\n", "Sure, let me take a look\n"]}, {"number": 3905, "title": "Revert \"Revert \"Merge branch 'r0.10' of https://github.com/tensorflow/tensorf\u2026\"", "body": "Reverts tensorflow/tensorflow#3781\nThis PR was born out of a confusion. should be reverted to reinstate the changes we want.\n", "comments": []}, {"number": 3904, "title": "Branch 130640571", "body": "", "comments": []}, {"number": 3903, "title": "conversion to graph_def lose tensor shapes", "body": "If i convert a tensorflow model to graph_def using\n`tf.get_default_graph().as_graph_def()`\nAnd later import it again, i lose the information regarding the tensor shapes\n### Code\n\n``` python\nimport tensorflow as tf\n\n# import any tensorflow model\nnet = __import__ ('mynet')\nimages = tf.placeholder(tf.float32, [32,28,28,3])\nnet = getattr(net, 'LeNet')({'data': images})\n\n# iterate through the graph\ngraph = tf.get_default_graph()\nfor i in graph.get_operations():\n    print i.name\n    print 'OUTPUTS ',\n    for j in i.outputs:\n        print j.get_shape(),\n    print \n\nprint \"*************** CONVERT MODEL TO GRAPH_DEF AND IMPORT AGAIN ***************\"\n\n# Graph as graph_def\ngraph_def = tf.get_default_graph().as_graph_def()\n\n# reset everything\ntf.reset_default_graph()\n\n# Import the graph_def\ntf.import_graph_def(graph_def,name='')\n\n# iterate through the graph again\ngraph = tf.get_default_graph()\nfor i in graph.get_operations():\n    print i.name\n    print 'OUTPUTS ',\n    for j in i.outputs:\n        print j.get_shape(),\n    print\n```\n### Output\n\n```\nPlaceholder\nOUTPUTS  (32, 28, 28, 3)\nconv1/weights\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/Initializer/random_uniform/shape\nOUTPUTS  (4,)\nconv1/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nconv1/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nconv1/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv1/weights/Initializer/random_uniform/mul\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/Initializer/random_uniform\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/Assign\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/read\nOUTPUTS  (5, 5, 3, 20)\nconv1/Conv2D\nOUTPUTS  (32, 24, 24, 20)\nconv1/biases\nOUTPUTS  (20,)\nconv1/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nconv1/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nconv1/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nconv1/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (20,)\nconv1/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv1/biases/Initializer/random_uniform/mul\nOUTPUTS  (20,)\nconv1/biases/Initializer/random_uniform\nOUTPUTS  (20,)\nconv1/biases/Assign\nOUTPUTS  (20,)\nconv1/biases/read\nOUTPUTS  (20,)\nconv1/BiasAdd\nOUTPUTS  (32, 24, 24, 20)\npool1\nOUTPUTS  (32, 12, 12, 20)\nconv2/weights\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/Initializer/random_uniform/shape\nOUTPUTS  (4,)\nconv2/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nconv2/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nconv2/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv2/weights/Initializer/random_uniform/mul\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/Initializer/random_uniform\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/Assign\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/read\nOUTPUTS  (5, 5, 20, 50)\nconv2/Conv2D\nOUTPUTS  (32, 8, 8, 50)\nconv2/biases\nOUTPUTS  (50,)\nconv2/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nconv2/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nconv2/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nconv2/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (50,)\nconv2/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv2/biases/Initializer/random_uniform/mul\nOUTPUTS  (50,)\nconv2/biases/Initializer/random_uniform\nOUTPUTS  (50,)\nconv2/biases/Assign\nOUTPUTS  (50,)\nconv2/biases/read\nOUTPUTS  (50,)\nconv2/BiasAdd\nOUTPUTS  (32, 8, 8, 50)\npool2\nOUTPUTS  (32, 4, 4, 50)\nip1/Reshape/shape\nOUTPUTS  (2,)\nip1/Reshape\nOUTPUTS  (32, 800)\nip1/weights\nOUTPUTS  (800, 500)\nip1/weights/Initializer/random_uniform/shape\nOUTPUTS  (2,)\nip1/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nip1/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nip1/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (800, 500)\nip1/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nip1/weights/Initializer/random_uniform/mul\nOUTPUTS  (800, 500)\nip1/weights/Initializer/random_uniform\nOUTPUTS  (800, 500)\nip1/weights/Assign\nOUTPUTS  (800, 500)\nip1/weights/read\nOUTPUTS  (800, 500)\nip1/biases\nOUTPUTS  (500,)\nip1/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nip1/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nip1/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nip1/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (500,)\nip1/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nip1/biases/Initializer/random_uniform/mul\nOUTPUTS  (500,)\nip1/biases/Initializer/random_uniform\nOUTPUTS  (500,)\nip1/biases/Assign\nOUTPUTS  (500,)\nip1/biases/read\nOUTPUTS  (500,)\nip1/ip1/MatMul\nOUTPUTS  (32, 500)\nip1/ip1/BiasAdd\nOUTPUTS  (32, 500)\nip1/ip1\nOUTPUTS  (32, 500)\nip2/weights\nOUTPUTS  (500, 10)\nip2/weights/Initializer/random_uniform/shape\nOUTPUTS  (2,)\nip2/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nip2/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nip2/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (500, 10)\nip2/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nip2/weights/Initializer/random_uniform/mul\nOUTPUTS  (500, 10)\nip2/weights/Initializer/random_uniform\nOUTPUTS  (500, 10)\nip2/weights/Assign\nOUTPUTS  (500, 10)\nip2/weights/read\nOUTPUTS  (500, 10)\nip2/biases\nOUTPUTS  (10,)\nip2/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nip2/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nip2/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nip2/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (10,)\nip2/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nip2/biases/Initializer/random_uniform/mul\nOUTPUTS  (10,)\nip2/biases/Initializer/random_uniform\nOUTPUTS  (10,)\nip2/biases/Assign\nOUTPUTS  (10,)\nip2/biases/read\nOUTPUTS  (10,)\nip2/ip2/MatMul\nOUTPUTS  (32, 10)\nip2/ip2\nOUTPUTS  (32, 10)\nprob\nOUTPUTS  (32, 10)\n*************** CONVERT MODEL TO GRAPH_DEF AND IMPORT AGAIN ***************\nPlaceholder\nOUTPUTS  (32, 28, 28, 3)\nconv1/weights\nOUTPUTS  <unknown>\nconv1/weights/Initializer/random_uniform/shape\nOUTPUTS  (4,)\nconv1/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nconv1/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nconv1/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv1/weights/Initializer/random_uniform/mul\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/Initializer/random_uniform\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/Assign\nOUTPUTS  (5, 5, 3, 20)\nconv1/weights/read\nOUTPUTS  <unknown>\nconv1/Conv2D\nOUTPUTS  (32, ?, ?, ?)\nconv1/biases\nOUTPUTS  <unknown>\nconv1/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nconv1/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nconv1/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nconv1/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (20,)\nconv1/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv1/biases/Initializer/random_uniform/mul\nOUTPUTS  (20,)\nconv1/biases/Initializer/random_uniform\nOUTPUTS  (20,)\nconv1/biases/Assign\nOUTPUTS  (20,)\nconv1/biases/read\nOUTPUTS  <unknown>\nconv1/BiasAdd\nOUTPUTS  (32, ?, ?, ?)\npool1\nOUTPUTS  (32, ?, ?, ?)\nconv2/weights\nOUTPUTS  <unknown>\nconv2/weights/Initializer/random_uniform/shape\nOUTPUTS  (4,)\nconv2/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nconv2/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nconv2/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv2/weights/Initializer/random_uniform/mul\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/Initializer/random_uniform\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/Assign\nOUTPUTS  (5, 5, 20, 50)\nconv2/weights/read\nOUTPUTS  <unknown>\nconv2/Conv2D\nOUTPUTS  (32, ?, ?, ?)\nconv2/biases\nOUTPUTS  <unknown>\nconv2/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nconv2/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nconv2/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nconv2/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (50,)\nconv2/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nconv2/biases/Initializer/random_uniform/mul\nOUTPUTS  (50,)\nconv2/biases/Initializer/random_uniform\nOUTPUTS  (50,)\nconv2/biases/Assign\nOUTPUTS  (50,)\nconv2/biases/read\nOUTPUTS  <unknown>\nconv2/BiasAdd\nOUTPUTS  (32, ?, ?, ?)\npool2\nOUTPUTS  (32, ?, ?, ?)\nip1/Reshape/shape\nOUTPUTS  (2,)\nip1/Reshape\nOUTPUTS  (?, 800)\nip1/weights\nOUTPUTS  <unknown>\nip1/weights/Initializer/random_uniform/shape\nOUTPUTS  (2,)\nip1/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nip1/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nip1/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (800, 500)\nip1/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nip1/weights/Initializer/random_uniform/mul\nOUTPUTS  (800, 500)\nip1/weights/Initializer/random_uniform\nOUTPUTS  (800, 500)\nip1/weights/Assign\nOUTPUTS  (800, 500)\nip1/weights/read\nOUTPUTS  <unknown>\nip1/biases\nOUTPUTS  <unknown>\nip1/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nip1/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nip1/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nip1/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (500,)\nip1/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nip1/biases/Initializer/random_uniform/mul\nOUTPUTS  (500,)\nip1/biases/Initializer/random_uniform\nOUTPUTS  (500,)\nip1/biases/Assign\nOUTPUTS  (500,)\nip1/biases/read\nOUTPUTS  <unknown>\nip1/ip1/MatMul\nOUTPUTS  (?, ?)\nip1/ip1/BiasAdd\nOUTPUTS  (?, ?)\nip1/ip1\nOUTPUTS  (?, ?)\nip2/weights\nOUTPUTS  <unknown>\nip2/weights/Initializer/random_uniform/shape\nOUTPUTS  (2,)\nip2/weights/Initializer/random_uniform/min\nOUTPUTS  ()\nip2/weights/Initializer/random_uniform/max\nOUTPUTS  ()\nip2/weights/Initializer/random_uniform/RandomUniform\nOUTPUTS  (500, 10)\nip2/weights/Initializer/random_uniform/sub\nOUTPUTS  ()\nip2/weights/Initializer/random_uniform/mul\nOUTPUTS  (500, 10)\nip2/weights/Initializer/random_uniform\nOUTPUTS  (500, 10)\nip2/weights/Assign\nOUTPUTS  (500, 10)\nip2/weights/read\nOUTPUTS  <unknown>\nip2/biases\nOUTPUTS  <unknown>\nip2/biases/Initializer/random_uniform/shape\nOUTPUTS  (1,)\nip2/biases/Initializer/random_uniform/min\nOUTPUTS  ()\nip2/biases/Initializer/random_uniform/max\nOUTPUTS  ()\nip2/biases/Initializer/random_uniform/RandomUniform\nOUTPUTS  (10,)\nip2/biases/Initializer/random_uniform/sub\nOUTPUTS  ()\nip2/biases/Initializer/random_uniform/mul\nOUTPUTS  (10,)\nip2/biases/Initializer/random_uniform\nOUTPUTS  (10,)\nip2/biases/Assign\nOUTPUTS  (10,)\nip2/biases/read\nOUTPUTS  <unknown>\nip2/ip2/MatMul\nOUTPUTS  (?, ?)\nip2/ip2\nOUTPUTS  (?, ?)\nprob\nOUTPUTS  (?, ?)\n```\n\nIt can be clearly seen that the shape of the last tensor (and others as well) in the model was `(32, 10)` and after conversion to graph_def and import again it becomes `(?, ?)`.\n\nIs there any other way to get output shapes out of any graph_def?\n", "comments": ["You can replace this line:\n\n``` python\ngraph_def = tf.get_default_graph().as_graph_def()\n```\n\n...with this line:\n\n``` python\ngraph_def = tf.get_default_graph().as_graph_def(add_shapes=True)\n```\n", "I want the code to work with any graph_def in general.\nIs it any how possible to get the shape info even if this requires computations?\n", "In general, some shape information is lost if you call `Graph.as_graph_def(add_shapes=False)`, because when you build the graph there are often cases where `Tensor.set_shape()` is called to provide extra information. This happens in particular for variables, queues, `tf.nn.embedding_lookup()`, and other cases where the Python graph construction function has additional information about the semantics of a composition of ops that is not available when you look at each op in turn (as the shape inference does).\n\nAssuming you create the graph yourself, calling `as_graph_def(add_shapes=True)` will add that information to the `GraphDef` so that you can restore it when it is imported. If the `GraphDef` was created by someone else, and they didn't add the inferred shapes to it, I'm not aware of anything you can do to infer that information.\n", "@mrry Okay. Thanks for your help\n", "How to preserve shapes when `strip_unused_lib.strip_unused` is used?\r\n\r\n```\r\nfrom tensorflow.python.tools import strip_unused_lib\r\nfrom tensorflow.python.framework import dtypes\r\n\r\ngdef = strip_unused_lib.strip_unused(\r\n        input_graph_def = original_gdef,\r\n        input_node_names = input_node_names,\r\n        output_node_names = output_node_names,\r\n        placeholder_type_enum = dtypes.float32.as_datatype_enum)\r\n```"]}, {"number": 3902, "title": "Issues running TensorFlow module in a Python sub-interpreter", "body": "I'm using the tensorflow Python module in a custom C++ application, by instantiating the Python interpreter and then running Python code that imports tensorflow.\n\nThe Python code is run in a sub-interpreter (created using Py_NewInterpreter). This works fine the first time around, but when I end the sub-interpreter and create a second one, the call to 'import tensorflow' fails with the exception shown below. Looks like the TensorFlow module is either not being cleaned up properly when it de-initializes, and/or it has some global state that is shared between multiple interpreters? In particular it looks it tries to load a shared object that has already been loaded.\n\nIs there a way to fix this and to make TensorFlow compatible with Python sub-interpreters, or is this just the tip of the iceberg and more issues would pop up once this particular one is resolved?\n\nThanks!\nPeter\n\n--- C++ code to reproduce the problem:\n\n```\n#include <cassert>\n#include <python/Python.h>\n\n// set this to the folder containing a virtual env with TensorFlow:\nstatic char g_pythonHome[] = \"/Users/peter/tf101\";\n\nvoid create_sub_interpreter_import_tensorflow()\n{\n    // acquire global interpreter lock and create new sub-interpreter:\n    PyEval_AcquireLock();\n    PyThreadState* pThreadState = Py_NewInterpreter();\n    assert(pThreadState);\n\n    // set sys.argv because TensorFlow needs it:\n    int ret = PyRun_SimpleString(\"import sys\\n\"\n                                 \"sys.argv = ['']\\n\");\n    assert(ret == 0);\n\n    // import TensorFlow, say hello.\n    // !!! this fails when it is called the second time:\n    ret = PyRun_SimpleString(\"import tensorflow\\n\"\n                             \"print 'hello world'\\n\");\n    assert(ret == 0);\n\n    // end sub-interpreter and release global interpreter lock:\n    Py_EndInterpreter(pThreadState);\n    PyEval_ReleaseLock();\n}\n\nint main(int argc, const char * argv[]) {\n    // set Python home to a virtual env containing TensorFlow:\n    Py_SetPythonHome(g_pythonHome);\n\n    // initialise Python and threads:\n    Py_Initialize();\n    PyEval_InitThreads();\n\n    // store current thread state, release global interpreter lock:\n    PyThreadState* pMainThreadState = PyEval_SaveThread();\n    assert(pMainThreadState);\n\n    create_sub_interpreter_import_tensorflow();\n    create_sub_interpreter_import_tensorflow();\n\n    // acquire global interpreter lock, restore main thread state, finalize Python:\n    PyEval_RestoreThread(pMainThreadState);\n    Py_Finalize();\n\n    return 0;\n}\n```\n\n--- Python error message:\n\n```\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 65, in <module>\n    import tensorflow.contrib as contrib\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 26, in <module>\n    from tensorflow.contrib import grid_rnn\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/grid_rnn/__init__.py\", line 27, in <module>\n    from tensorflow.contrib.grid_rnn.python.ops.grid_rnn_cell import *\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py\", line 28, in <module>\n    from tensorflow.contrib import layers\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/layers/__init__.py\", line 77, in <module>\n    from tensorflow.contrib.layers.python.layers import *\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/__init__.py\", line 22, in <module>\n    from tensorflow.contrib.layers.python.layers.feature_column import *\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py\", line 79, in <module>\n    from tensorflow.contrib.layers.python.ops import bucketization_op\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/layers/python/ops/bucketization_op.py\", line 25, in <module>\n    resource_loader.get_path_to_datafile(\"_bucketization_op.so\"))\n  File \"/Users/peter/tf101/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 71, in load_op_library\n    raise errors._make_specific_exception(None, None, error_msg, error_code)\ntensorflow.python.framework.errors.AlreadyExistsError: /Users/peter/tf101/lib/python2.7/site-packages/tensorflow/contrib/layers/python/ops/_bucketization_op.so has already been loaded\n```\n\n--- Environment:\nOS: OSX El Capitan (10.11.5)\ntensorflow version: 0.9.0\ninstalled using: pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py2-none-any.whl\n", "comments": ["@keveman - this seems to be tf.layers loading a custom bucketization_op via `load_op_library` for the second time.\nThe dynamic loader presumably returns a status indicating the shared library is already loaded - but this causes TF to fail rather then use the existing version.\n(Obviously this is an unanticipated use case!)\n", "Closing due to inactivity. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 3901, "title": "GPU version for OSX also available", "body": "as per: #664 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can you sign the CLA? Thanks.\n", "sorry, signed the CLA but forgot to reply to the email, just did that.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I signed it!\n\nOn Thu, Aug 18, 2016 at 1:39 PM googlebot notifications@github.com wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3901#issuecomment-240698399,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ACeGjdRAiLaNAwJ4VKY-P8fQ7PRzIN4iks5qhER9gaJpZM4JnZqT\n> .\n", "Jenkins, test this please.\n", "Thanks, I'll merge your PR as soon as the test results come back.\n", "There was a test failure, but given that this pull request only touches markdown I'm concluding the test failure was a flake. Merging, thanks for the fix!\n"]}, {"number": 3900, "title": "translate.py not found in tensorflow libs on osx", "body": "OSX 10.11.6 (15G31)\nTensorflow: 0.10.0rc0\nPython 3.5\n\nI'm following https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html but translate.py is not in the tensorflow/models/rnn/translate directory.\n\n```\ncd tensorflow/models/rnn/translate\npython translate.py --data_dir [your_data_directory]\n```\n\nWhen I ls in \"/usr/local/lib/python3.5/site-packages/tensorflow/models/rnn/translate\", I see the following files:\n`__init__.py        __pycache__     data_utils.py       seq2seq_model.py\n`\nIs my installation broken somehow? I installed tensorflow with this code:\n\n```\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py3-none-any.whl\nsudo pip3 install --upgrade $TF_BINARY_URL\n```\n", "comments": ["Actually, more files seem to be missing. Looking at https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html it references `models/rnn/ptb/ptb_word_lm.py` but I can't find that either. In that directory I see\n\n`__init__.py    __pycache__ reader.py\n`\n", "I have the same question.\nsee http://stackoverflow.com/questions/39683085/translate-py-cant-found-in-rnn-translate-folder\n", "Is this related to issue #4574 (which was already closed)? Seems they excluded the example files from the pip packages, but didn't update the tutorials... I guess cloning https://github.com/tensorflow/models/tree/master/tutorials will provide all necessary tutorial files.", "Right, we moved the example files into models/tutorials if memory serves. Is this still current?", "Feel free to file again if you still have the problem. Closing this to clean up."]}, {"number": 3899, "title": "where is the location of grpc library installed by pip ", "body": "Could you someone can tell me where the grpc c++ lib location?\n\nI can find the python part in site-packages, but cannot find where the grpc installed?\n\nMy system is centos 7, and install it by pip over  anaconda\n\n```\n[root@nodel]# pip show tensorflow\n\n---\nMetadata-Version: 2.0\nName: tensorflow\nVersion: 0.9.0\nSummary: TensorFlow helps the tensors flow\nHome-page: http://tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: opensource@google.com\nInstaller: pip\nLicense: Apache 2.0\nLocation: /usr/local/anaconda2/lib/python2.7/site-packages\nRequires: numpy, six, protobuf, wheel\nClassifiers:\n  Development Status :: 4 - Beta\n  Intended Audience :: Developers\n  Intended Audience :: Education\n  Intended Audience :: Science/Research\n  License :: OSI Approved :: Apache Software License\n  Programming Language :: Python :: 2.7\n  Topic :: Scientific/Engineering :: Mathematics\n  Topic :: Software Development :: Libraries :: Python Modules\n  Topic :: Software Development :: Libraries\nEntry-points:\n  [console_scripts]\n  tensorboard = tensorflow.tensorboard.tensorboard:main\n```\n\nThanks a lot!\n", "comments": ["After some try I know... it use SWIG tool to complied C or C++ with python as \n\n`_pywrap_tensorflow.so`\n", "That's correct: gRPC is statically linked into the `_pywrap_tensorflow.so` module which contains the TensorFlow runtime, the standard operation kernels, and the SWIG wrappers for interfacing between Python and C++.\n"]}, {"number": 3898, "title": "Inception model error on upgrading Tensorflow", "body": "I am loading the Inception model as,\n\n```\ndef load_network(png=False):\n        with gfile.FastGFile(CONFIG_PATH + '/data/network.pb', 'rb') as f:\n            graph_def = tf.GraphDef()\n            data = f.read()\n            graph_def.ParseFromString(data)\n            if png:\n                png_data = tf.placeholder(tf.string, shape=[])\n                decoded_png = tf.image.decode_png(png_data, channels=3)\n                _ = tf.import_graph_def(graph_def, name='', input_map={'DecodeJpeg': decoded_png})\n                return png_data\n            else:\n                _ = tf.import_graph_def(graph_def, name='')\n```\n\nHowever, after upgrading from Tensorflow 0.8 to 0.10, this is giving an error,\n\n```\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 241, in import_graph_def\n    raise ValueError('tf.import_graph_def() requires a non-empty `name` '\nValueError: tf.import_graph_def() requires a non-empty `name` if `input_map` is used.\n```\n\nI then removed the `name=''` part as,\n\n```\n                _ = tf.import_graph_def(graph_def, input_map={'DecodeJpeg': decoded_png})\n                return png_data\n            else:\n                _ = tf.import_graph_def(graph_def)\n\n```\n\nThis part then runs fine, however later on in my code when I am trying to get the pool3 layer's output I get the following error,\n\n```\n    pool3 = sess.graph.get_tensor_by_name('pool_3:0')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2531, in get_tensor_by_name\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2385, in as_graph_element\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2427, in _as_graph_element_locked\n    \"graph.\" % (repr(name), repr(op_name)))\nKeyError: \"The name 'pool_3:0' refers to a Tensor which does not exist. The operation, 'pool_3', does not exist in the graph.\"\n```\n\nWhat could be the problem?\n", "comments": ["according to the doc, [import_graph_def](https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#import_graph_def), the default value of `name` is `import`, and it says `A prefix that will be prepended to the names in graph_def`.\n", "@suiyuan2009 Yes, but that does not solve my problem?\n", "in my understanding, all tensor name's prefix will be `name`, if you don't set, it will be `import`\n", "So you're saying that the 'pool_3:0' layer I am after is actually 'importpool_3;0'? \n", "maybe import/pool_3:0, actually you can use tensorboard to see the graph, every tensor and op's information will show on tensorboard.\n", "Yes, thanks, this works!\n", " **I have same issue...\r\n I have doing real time face recognition using tensorflow.\r\n I'm follow this link \"https://github.com/btwardow/tf-face-recognition\"\r\n When I'm download clone repo it work in one system but try to run another one this show me error.\r\n Please help me out.**\r\n\r\n`Traceback (most recent call last):\r\n  File \"server.py\", line 49, in detect\r\n    faces = recognize(detection.get_faces(image, threshold))\r\n  File \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/recognition.py\", line 19, in recognize\r\n    X[i, :] = embedding(img_to_np(img))\r\n  File \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/embedding.py\", line 43, in embedding\r\n    images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3666, in get_tensor_by_name\r\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3490, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3532, in _as_graph_element_locked\r\n    \"graph.\" % (repr(name), repr(op_name)))\r\n**KeyError: \"The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.\"**\r\n[2019-02-26 11:28:06,532] ERROR in app: Exception on /detect [POST]\r\nTraceback (most recent call last):\r\n  File \"server.py\", line 49, in detect\r\n    faces = recognize(detection.get_faces(image, threshold))\r\n  File \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/recognition.py\", line 19, in recognize\r\n    X[i, :] = embedding(img_to_np(img))\r\n  File \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/embedding.py\", line 43, in embedding\r\n    images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3666, in get_tensor_by_name\r\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3490, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3532, in _as_graph_element_locked\r\n    \"graph.\" % (repr(name), repr(op_name)))\r\nKeyError: \"The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 2292, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1815, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1718, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/_compat.py\", line 35, in reraise\r\n    raise value\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1813, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1799, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"server.py\", line 69, in detect\r\n    print('POST /detect error: %e' % e)\r\nTypeError: must be real number, not KeyError`\r\n\r\n"]}, {"number": 3897, "title": "Tensorflow gradients are always zero!", "body": "Tensorflow gradients are always zero with respect to conv layers that are after first conv layer. I've tried different ways to check that but gradients are always zero! Here is the small reproducible code that can be run to check that.\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport random\nimport tflearn\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.000001\nnum_classes = 4\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 500\nnum_of_filters = 1000\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, num_of_filters]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.contrib.layers.xavier_initializer())\nconv_biases = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = conv + conv_biases\ntf_normalize = tflearn.layers.normalization.batch_normalization(normalize)\nrelu = tf.nn.elu(tf_normalize)\npooling = tf.reduce_max(relu, reduction_indices = 3, keep_dims = True)\noutputs_fed_lstm = pooling\n\nfilter_shape2 = [1, 1, 1, num_of_filters]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32, tf.contrib.layers.xavier_initializer())\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[num_of_filters]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = conv2 + conv_biases2\ntf_normalize2 = tflearn.layers.normalization.batch_normalization(normalize2)\nrelu2 = tf.nn.elu(tf_normalize2)\npooling2 = tf.reduce_max(relu2, reduction_indices = 3, keep_dims = True)\noutputs_fed_lstm2 = pooling2\n\nx = tf.squeeze(outputs_fed_lstm2, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 1])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \ngrads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1):\n         for j in range(1):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n```\n\nAnd here is the output\n\n```\n[array([[[[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  5.21326828,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n           0.        ,  0.        ,  0.        ,  0.        ,  0.        ]]]], dtype=float32)]\n```\n", "comments": ["can you give a smaller code, about 20 lines(you can delete some layers for example). you can ask on stackoverflow first, tensorflow official developers also answer questions on stackoverflow, and they always response very quickly.\n", "@omoindrot investigated [here](http://stackoverflow.com/questions/39015376/tensorflow-gradients-are-always-zero) and found that this gradient is the correct answer for this architecture\n", "This isn't an issue, there was problem with my architecture. Thanks @yaroslavvb and @suiyuan2009 \n"]}, {"number": 3896, "title": "Docs(FIFOQueue):Fix a misspelling in line #594", "body": "Fix a misspelling which may confuse readers. It should keep same as the line #604 and others:\n`first-in-first out` -> `first-in first-out`\n", "comments": ["Jenkins, test this please.\n"]}, {"number": 3895, "title": "MultiRNNCell with state_is_tuple=True got Error!", "body": "Running MultiRNN, when I set \n\n```\ncell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\ncell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n```\n\ngot following error:\n\n```\nTraceback (most recent call last):\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/RNN/lstm.py\", line 175, in <module>\n    train_network(g, 3)\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/RNN/lstm.py\", line 140, in train_network\n    [g['total_loss'], g['final_state'], g['train_step']], feed_dict)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 584, in _run\n    processed_fetches = self._process_fetches(fetches)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 540, in _process_fetches\n    % (subfetch, fetch, type(subfetch), str(e)))\nTypeError: Fetch argument (LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_2:0' shape=(32, 100) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_3:0' shape=(32, 100) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_4:0' shape=(32, 100) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_5:0' shape=(32, 100) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_6:0' shape=(32, 100) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_7:0' shape=(32, 100) dtype=float32>)) of (LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_2:0' shape=(32, 100) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_3:0' shape=(32, 100) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_4:0' shape=(32, 100) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_5:0' shape=(32, 100) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_6:0' shape=(32, 100) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_7:0' shape=(32, 100) dtype=float32>)) has invalid type <type 'tuple'>, must be a string or Tensor. (Can not convert a tuple into a Tensor or Operation.)\n```\n\nBut when I set state_is_tuple=False(Default), it works fine.\nHowever, state_is_tuple=True is recommended to faster LSTM states.\n", "comments": ["Please can you provide full details of Tnesorflow version and steps to reproduce, as requested in the issues reporting template we provided.\n", "Thanks for your help in advance.\n\nTensorflow version is v0.9,  the running code as follows:\n\n```\ndef build_multilayer_lstm_graph_with_dynamic_rnn(state_size=100, num_classes=vocab_size, batch_size=32,\n                                                 num_steps=200, num_layers=3, learning_rate=1e-4):\n    reset_graph()\n\n    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n\n    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n\n    # Note that our inputs are no longer a list, but a tensor of dims batch_size x num_steps x state_size\n    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n\n    cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n    init_state = cell.zero_state(batch_size, tf.float32)\n    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n\n    with tf.variable_scope('softmax'):\n        W = tf.get_variable('W', [state_size, num_classes])\n        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n\n    # reshape rnn_outputs and y so we can get the logits in a single matmul\n    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n    y_reshaped = tf.reshape(y, [-1])\n    logits = tf.matmul(rnn_outputs, W) + b\n    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_reshaped))\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n    return dict(x=x, y=y, init_state=init_state, final_state=final_state, total_loss=total_loss, train_step=train_step)\n```\n\nThe problem happens when I set  state_is_tuple=True.\n", "Try Tf 0.10 or newer. Do you still get the error?\n", "Error solved!\n\nThanks for your response.\n"]}, {"number": 3894, "title": "Failure to build tensorflow on centos 5.7 with gcc 4.8.5", "body": "### Environment info\n\nOperating System:\nCENTOS 5.7 with gcc 4.8.5\n\nBazel 0.1.3 without namespace-sandbox\n### Problems:\n\nWhen I build tensorflow using \n\n> bazel build -c opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package\n\nThe error occurs :\n\n> .............\n> Unhandled exception thrown during build; message: namespace-sandbox not in [embedded_tools/WORKSPACE, embedded_tools/src/tools/android/java/com/google/devtools/build/android/README, embedded_tools/src/tools/android/java/com/google/devtools/build/android/AndroidResourceProcessor.java, embedded_tools/src/tools/android/java/com/google/devtools/build/android/MergedAndroidData.java, embedded_tools/src/tools/android/java/com/google/devtools/build/android/Converters.java, embedded_tools/src/tools/android/java/com/google/devtools/build/androi\n\nIt seems that the problems is due to disabling namespace-sandbox of bazel, anyone can help me with this?\n", "comments": ["Generally speaking tensorflow and bazel is not supported before CentOS 7. I am aware that people have gotten tensorflow built on CentOS 6, and so it may be possible to also do on CentOS 5, but it will probably require some ddebugging effort on your part. You should also post the entire build log (as an attachment).\n", "Closing this issue, as building TensorFlow with any bazel version older than 0.4.1 is not possible anymore.\r\nFor a possible workaround for building tensorflow on RHEL, and possibly centos, you can check https://github.com/tensorflow/tensorflow/issues/7118"]}, {"number": 3893, "title": "fix sparse rmsprop check variable bug", "body": "fixes #3797 \na quick fix\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks, I suspect some of the other sparse optimizers are also not correct in the same way, but we can fix them separately.\n\n@tensorflow-jenkins test this please\n", "(Double checked, I think the others are okay).\n\nDo you know if there's a test we could write that would have caught this?\n", "currently grad and var in SparseRMSProp's test case have same shape, because grad.indexs contains all indexs, so it will pass test when I check whether var and grad shape are same. we can add test case that grad.indexs not contains all indexs. other SparseXXX ops have same problem.\n", "I modified one test case for sparsermsprop, it will catch this bug.\n", "Great, thanks, will test again.  test this please\n"]}, {"number": 3892, "title": "Merging ROI Pooling into master", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Sorry, this was a mistake!\n"]}, {"number": 3891, "title": "Very low accuracy in the mnist dataset with cnn when running on a GPU using tensorflow", "body": "hi everyone:\nI tried to run an examples of MNIST with cnn and when i only use cpu the code can work well  **but when i use gpu it is not working well and  it has very low accuracy**. \n## environment like this\n\n GPU:Geforce GTX1070\n Cuda toolkit version\uff1a7.5\n cuDNN version\uff1a7.0\ntensorflow version\uff1a0.9r\n## code like this:\n\n``` python\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ndef weight_varible(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nprint(\"Download Done!\")\n\nsess = tf.InteractiveSession()\n\n# paras\nW_conv1 = weight_varible([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\n# conv layer-1\nx = tf.placeholder(tf.float32, [None, 784])\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# conv layer-2\nW_conv2 = weight_varible([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# full connection\nW_fc1 = weight_varible([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# dropout\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# output layer: softmax\nW_fc2 = weight_varible([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\ny_ = tf.placeholder(tf.float32, [None, 10])\n\n# model training\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsess.run(tf.initialize_all_variables())\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n\n    if i % 100 == 0:\n        train_accuacy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n        print(\"step %d, training accuracy %g\"%(i, train_accuacy))\n    train_step.run(feed_dict = {x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n# accuacy on test\nprint(\"test accuracy %g\"%(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})))\n```\n### on cpu\n\nstep 0, training accuracy 0.06\nstep 100, training accuracy 0.8\nstep 200, training accuracy 0.94\nstep 300, training accuracy 0.9\nstep 400, training accuracy 0.92\nstep 500, training accuracy 0.84\nstep 600, training accuracy 1\nstep 700, training accuracy 0.96\nstep 800, training accuracy 0.94\nstep 900, training accuracy 1\nstep 1000, training accuracy 0.98\nstep 1100, training accuracy 0.92\nstep 1200, training accuracy 0.96\nstep 1300, training accuracy 1\nstep 1400, training accuracy 1\nstep 1500, training accuracy 1\n......\ntest accuracy 0.9671\n### on gpu\n\nstep 0, training accuracy 0.1\nstep 100, training accuracy 0.08\nstep 200, training accuracy 0.02\nstep 300, training accuracy 0.12\nstep 400, training accuracy 0.1\nstep 500, training accuracy 0.1\nstep 600, training accuracy 0.12\nstep 700, training accuracy 0.12\nstep 800, training accuracy 0.1\nstep 900, training accuracy 0.04\nstep 1000, training accuracy 0.12\nstep 1100, training accuracy 0.3\nstep 1200, training accuracy 0.1\nstep 1300, training accuracy 0.08\nstep 1400, training accuracy 0.12\nstep 1500, training accuracy 0.2\n......\ntest accuracy 0.1160\n", "comments": ["Sometimes there will be some mistakes, the gpu log like this(it looks like out of memory):\n\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (256):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (512):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (1024):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (2048):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (4096):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (8192):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (32768):     Total Chunks: 1, Chunks in use: 0 34.2KiB allocated for chunks. 4.79MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (65536):     Total Chunks: 1, Chunks in use: 0 125.0KiB allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (131072):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (262144):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (524288):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (1048576):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (2097152):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (4194304):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (8388608):   Total Chunks: 1, Chunks in use: 0 12.25MiB allocated for chunks. 390.6KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (16777216):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (33554432):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (67108864):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\n> I tensorflow/core/common_runtime/bfc_allocator.cc:656] Bin for 29.91MiB was 16.00MiB, Chunk State: \n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005800000 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005800100 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005800200 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005800300 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005800400 of size 4096\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005801400 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005801500 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005801600 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005801700 of size 3328\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005802400 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005802500 of size 204800\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005834500 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005834600 of size 12845056\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006474600 of size 4096\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006475600 of size 40960\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000647f600 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000647f700 of size 3328\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480400 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480500 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480600 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480700 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480800 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480900 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480a00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480b00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480c00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006480d00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006489700 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006489800 of size 4096\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000648a800 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000648a900 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000648aa00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000648ab00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000648ac00 of size 3328\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000648b900 of size 40960\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006495900 of size 3328\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006496600 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006496700 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10006496800 of size 204800\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x100064c8800 of size 204800\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x100064fa800 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x100064fa900 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x100064faa00 of size 12845056\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000713aa00 of size 12845056\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007d7aa00 of size 4096\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007d7ba00 of size 4096\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007d7ca00 of size 40960\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007d86a00 of size 40960\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007d90a00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007d90b00 of size 256\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007d90c00 of size 80128\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10007dc3900 of size 204800\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10008a35900 of size 12845056\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10009675900 of size 31360000\n> I tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x1000b45dd00 of size 1733108480\n> I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10006480e00 of size 35072\n> I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10007da4500 of size 128000\n> I tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10007df5900 of size 12845056\n> I tensorflow/core/common_runtime/bfc_allocator.cc:689]      Summary of in-use Chunks by size: \n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 31 Chunks of size 256 totalling 7.8KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 3328 totalling 13.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 5 Chunks of size 4096 totalling 20.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 40960 totalling 160.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 80128 totalling 78.2KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 204800 totalling 800.0KiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 12845056 totalling 49.00MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 31360000 totalling 29.91MiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 1733108480 totalling 1.61GiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 1.69GiB\n> I tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \n> Limit:                  1829961728\n> InUse:                  1816953600\n> MaxInUse:               1816953600\n> NumAllocs:                  114506\n> MaxAllocSize:           1733108480\n> \n> W tensorflow/core/common_runtime/bfc_allocator.cc:270] *************************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n> W tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 29.91MiB.  See logs for memory state.\n> W tensorflow/core/framework/op_kernel.cc:909] Resource exhausted: OOM when allocating tensor with shape[10000,1,28,28]\n", "you can compute memory need of  your model to see if it exceeds your GPU memory.\n", "If you are getting out of memory errors when using GPU then clearly something is wrong and it is unlikely to train properly.  \nPlease can you clarify what card you're using.  The log messages imply you only have 1.69GB in use - I'm assuming this is quite a small GPU?\nAlso, as @suiyuan2009 states, you should probably check what the memory requirements of your model are.\n\nThis seems unlikely to be a TensorFlow bug, and you will probably be better off asking for support on StackOverflow.\n", "@prb12 @suiyuan2009 thank you for your reply , my GPU is shared by 3 persons, so sometimes there has insufficient memory, but the low accuracy result is got when on exclusive resources.\nso my point is why a low accuracy  result in the mnist dataset is got when running on a GPU. the following message is from **mnist_cnn.py** (a cnn example from tensorflow)\n@vrv @vincentvanhoucke \n\n> (tensorflow)liyiran@seele:~/dllearning/mnist$ vim mnist_cnn.py \n> (tensorflow)liyiran@seele:~/dllearning/mnist$ python mnist_cnn.py \n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n> Using TensorFlow backend.\n> X_train shape: (60000, 28, 28, 1)\n> 60000 train samples\n> 10000 test samples\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n> name: GeForce GTX 1070\n> major: 6 minor: 1 memoryClockRate (GHz) 1.7845\n> pciBusID 0000:01:00.0\n> Total memory: 7.92GiB\n> Free memory: 7.10GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\n> Train on 60000 samples, validate on 10000 samples\n> Epoch 1/12\n> 60000/60000 [==============================] - 8s - loss: 2.3031 - acc: 0.1116 - val_loss: 2.3021 - val_acc: 0.1114\n> Epoch 2/12\n> 60000/60000 [==============================] - 3s - loss: 2.3051 - acc: 0.1120 - val_loss: 2.3011 - val_acc: 0.1135\n> Epoch 3/12\n> 60000/60000 [==============================] - 3s - loss: 2.3016 - acc: 0.1122 - val_loss: 2.3032 - val_acc: 0.1134\n> Epoch 4/12\n> 60000/60000 [==============================] - 2s - loss: 2.3028 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135\n> Epoch 5/12\n> 60000/60000 [==============================] - 2s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3025 - val_acc: 0.1134\n> Epoch 6/12\n> 60000/60000 [==============================] - 2s - loss: 2.3023 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135\n> Epoch 7/12\n> 60000/60000 [==============================] - 2s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3009 - val_acc: 0.1135\n> Epoch 8/12\n> 60000/60000 [==============================] - 3s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n> Epoch 9/12\n> 60000/60000 [==============================] - 3s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3022 - val_acc: 0.1135\n> Epoch 10/12\n> 60000/60000 [==============================] - 3s - loss: 2.3013 - acc: 0.1124 - val_loss: 2.3010 - val_acc: 0.1135\n> Epoch 11/12\n> 60000/60000 [==============================] - 3s - loss: 2.3012 - acc: 0.1123 - val_loss: 2.3028 - val_acc: 0.1135\n> Epoch 12/12\n> 60000/60000 [==============================] - 3s - loss: 2.3014 - acc: 0.1123 - val_loss: 2.3010 - val_acc: 0.1135\n> Test score: 2.30167105103\n> Test accuracy: 0.1135\n\nthe code of mnist_cnn.py is:\n\n``` python\nfrom __future__ import print_function\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\nimport tensorflow as tf\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\n\ndef run():\n  batch_size = 128\n  nb_classes = 10\n  nb_epoch = 12\n\n  # input image dimensions\n  img_rows, img_cols = 28, 28\n  # number of convolutional filters to use\n  nb_filters = 32\n  # size of pooling area for max pooling\n  nb_pool = 2\n  # convolution kernel size\n  nb_conv = 3\n\n\n  # the data, shuffled and split between train and test sets\n  (X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n  X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n  X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n  X_train = X_train.astype('float32')\n  X_test = X_test.astype('float32')\n  X_train /= 255\n  X_test /= 255\n  print('X_train shape:', X_train.shape)\n  print(X_train.shape[0], 'train samples')\n  print(X_test.shape[0], 'test samples')\n\n  # convert class vectors to binary class matrices\n  Y_train = np_utils.to_categorical(y_train, nb_classes)\n  Y_test = np_utils.to_categorical(y_test, nb_classes)\n\n  model = Sequential()\n\n  model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n                          border_mode='valid',\n                          input_shape=(img_rows, img_cols, 1), dim_ordering='tf'))\n  model.add(Activation('relu'))\n  model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n  model.add(Dropout(0.25))\n\n  model.add(Flatten())\n  model.add(Dense(128))\n  model.add(Activation('relu'))\n  model.add(Dropout(0.5))\n  model.add(Dense(nb_classes))\n  model.add(Activation('softmax'))\n\n  model.compile(loss='categorical_crossentropy',\n                optimizer='adadelta',\n                metrics=['accuracy'])\n\n  model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n            verbose=1, validation_data=(X_test, Y_test))\n  score = model.evaluate(X_test, Y_test, verbose=0)\n  print('Test score:', score[0])\n  print('Test accuracy:', score[1])\n\nif __name__ == \"__main__\":\n  with tf.device('/gpu:0'): \n  #with tf.device('/cpu:0'): \n    run()\n```\n\nwhen i replace gpu by cpu, then the result is like this:\n\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n> Using TensorFlow backend.\n> X_train shape: (60000, 28, 28, 1)\n> 60000 train samples\n> 10000 test samples\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n> name: GeForce GTX 1070\n> major: 6 minor: 1 memoryClockRate (GHz) 1.7845\n> pciBusID 0000:01:00.0\n> Total memory: 7.92GiB\n> Free memory: 7.10GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\n> Train on 60000 samples, validate on 10000 samples\n> Epoch 1/12\n> 60000/60000 [==============================] - 65s - loss: 0.3884 - acc: 0.8818 - val_loss: 0.1009 - val_acc: 0.9686\n> Epoch 2/12\n> 60000/60000 [==============================] - 71s - loss: 0.1462 - acc: 0.9561 - val_loss: 0.0661 - val_acc: 0.9793\n> Epoch 3/12\n> 60000/60000 [==============================] - 69s - loss: 0.1109 - acc: 0.9672 - val_loss: 0.0541 - val_acc: 0.9828\n> Epoch 4/12\n> 60000/60000 [==============================] - 66s - loss: 0.0903 - acc: 0.9734 - val_loss: 0.0498 - val_acc: 0.9840\n> Epoch 5/12\n> 60000/60000 [==============================] - 68s - loss: 0.0826 - acc: 0.9759 - val_loss: 0.0421 - val_acc: 0.9863\n> Epoch 6/12\n> 60000/60000 [==============================] - 67s - loss: 0.0734 - acc: 0.9786 - val_loss: 0.0383 - val_acc: 0.9871\n> Epoch 7/12\n> 60000/60000 [==============================] - 68s - loss: 0.0659 - acc: 0.9802 - val_loss: 0.0373 - val_acc: 0.9879\n> Epoch 8/12\n> 60000/60000 [==============================] - 66s - loss: 0.0622 - acc: 0.9818 - val_loss: 0.0348 - val_acc: 0.9883\n> Epoch 9/12\n> 60000/60000 [==============================] - 66s - loss: 0.0574 - acc: 0.9826 - val_loss: 0.0361 - val_acc: 0.9880\n> Epoch 10/12\n> 60000/60000 [==============================] - 66s - loss: 0.0538 - acc: 0.9843 - val_loss: 0.0331 - val_acc: 0.9888\n> Epoch 11/12\n> 60000/60000 [==============================] - 66s - loss: 0.0520 - acc: 0.9843 - val_loss: 0.0311 - val_acc: 0.9900\n> Epoch 12/12\n> 60000/60000 [==============================] - 64s - loss: 0.0495 - acc: 0.9849 - val_loss: 0.0311 - val_acc: 0.9901\n> Test score: 0.0310514848216\n> Test accuracy: 0.9901\n", "Try using CUDA 8.0 RC, I don't think TF works with 1070 or 1080 series GPUs unless you compile from source with CUDA 8.0.\n", "(Closing because I think this is a duplicate of a lot of existing issues that are solved by upgrading cuda)\n", "cuDNN needs to be 5.0 and CUDA needs to be 8.0 with latest Video driver 367.35.0\n", "@vrv @Mazecreator thank you very much!\n", "the problem is solved. TF 0.9/0.10 need CUDA 8.0, pip install can not support it, so we should compile it form source code and choose CUDA version 8.0.\n"]}, {"number": 3890, "title": "GPU Profiling: does the OP include the data transferring time from host to gpu device and gpu device to host?", "body": "@prb12 \nHi Paul,\nI just do the profiling following: https://github.com/tensorflow/tensorflow/issues/1824. And it works quite well. I detect that Conv2D costs a lot of time in my case, and it is run by nvidia GPU, does the timing for Conv2D include the H2D/D2H time or just the time run on GPU.\n\nThanks!\n", "comments": ["In future, please can you ask general questions like this on StackOverflow.  This is not a bug/issue.\n\nThe Conv2D timings shown in the Timeline do not include host/GPU transfers.  These always happen within Send/Recv ops.  They DMA timings will show up in the timelines labeled /gpu:0/memcpy (and also in the timeline for the stream which was used)\n"]}, {"number": 3889, "title": "Control flow gradient problem with `tf.float16` datatype", "body": "### Environment info\n\nOperating System:\nLinux\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   3cb39956e622b322e43547cf2b6e337020643f21\n2. The output of `bazel version`\n\n```\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n\n```\n### Steps to reproduce\n\n```\n# Minimal example to reproduce the error.\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import conv2d, batch_norm\nsess = tf.InteractiveSession()\nis_train_var = tf.placeholder(tf.bool)\n# If `tf.float32` is used here, it works flawlessly. Alternatively,\n# if a constant instead of a variable is used for `is_training`,\n# everything works as expected.\ninp = tf.placeholder(tf.float16, [1, 5, 5, 3], name='data')\nconv1 = conv2d(inp, 1, 5, padding='VALID',\n               normalizer_fn=batch_norm,\n               normalizer_params={'is_training': is_train_var})\noptimizer = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9)\noptimizer.compute_gradients(conv1)\n```\n\nThis results in the error\n\n```\nTraceback (most recent call last):\n  File \"repr.py\", line 10, in <module>\n    optimizer.compute_gradients(conv1)\n  File \"/lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/control_flow_grad.py\", line 69, in _SwitchGrad\n    return merge([good_grad, zero_grad], name=\"cond_grad\")[0], None\n  File \"/lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/control_flow_ops.py\", line 361, in merge\n    return gen_control_flow_ops._merge(inputs, name)\n  File \"/lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/ops/gen_control_flow_ops.py\", line 153, in _merge\n    result = _op_def_lib.apply_op(\"Merge\", inputs=inputs, name=name)\n  File \"/lustre/home/classner/git/tensorflow/_python_build/tensorflow/python/framework/op_def_library.py\", line 437, in apply_op\n    raise TypeError(\"%s that don't all match.\" % prefix)\nTypeError: Tensors in list passed to 'inputs' of 'Merge' Op have types [float16, float32] that don't all match.\n```\n### What have you tried?\n\nI am currently using constant variables as a workaround as described.\n", "comments": ["Should be fixed at HEAD by now.\n", "Closing since it should be fixed. Feel free to open a new github issue if the problem still persists in recent versions."]}]