[{"number": 14668, "title": "Revert \"Add missing conv1d in `tf.contrib.layers`\"", "body": "Reverts tensorflow/tensorflow#14513\r\n\r\nActually, we shouldn't be modifying tf.contrib.layers at this point. Reverting.", "comments": []}, {"number": 14667, "title": "`mean_relative_error` supports complex label", "body": "Fix #14658 \r\n\r\n\r\n### How to test\r\n\r\n+ [x] add test case\r\n+ [ ] pass all tests", "comments": ["Can one of the admins verify this patch?", "I'd rather we didn't do this. Performing division by zero raises the FP exception flag and makes for a very slow operation. Either we have a new safe operator in C++ or we should ask user-level awareness.", "Yes, I found `_safe_div` exists in many modules, for example, `losses`, `metrics` and so on. \r\nThose PR #15443, #14865 get trouble in the division by zero problem.\r\n\r\nCreating a general C++ op seems a better solution, at least for me.  Is it a good idea that I open a issue to discuss the new OP?", "It seems that `safe_div` has been implemented in C++ side, however it only supports integer now.  \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ac7387b48674fdeb11e7a4fa595c88b5e2afe16e/tensorflow/core/kernels/cwise_ops.h#L702-L705\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ac7387b48674fdeb11e7a4fa595c88b5e2afe16e/tensorflow/core/kernels/cwise_ops.h#L119-L137", "OK, then should we implement the float version in `C++` then?", "Yes, I think so. I'll open a issue later, and let's discuss the details there. Thank you for your help, @drpngx .", "Thanks!"]}, {"number": 14666, "title": "feature request in examples/image_retraining/retrain.py", "body": "Hi all, \r\nThis script is working great but please add projector / embedding and images in tensorboard. \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 14665, "title": "Use cub::ReduceByKey to count partition indices", "body": "This implements a suggestion made by @ekelsen in the comments for #13905.\r\nIt replaces the previously custom-made counting method, and is likely more efficient.\r\n\r\nIn order to use cub::ReduceByKey properly, I defined a specialization of TransformOutputIterator that only allows writes in a bounded interval. This is needed in the case of wrong inputs.\r\n\r\nI've also added tests for the GPU kernel, covering the case of wrong inputs.", "comments": ["Can one of the admins verify this patch?", "Hi @ekelsen,\r\n\r\nThank you for doing the review! I implemented the suggestions you made."]}, {"number": 14664, "title": "TensorFlow Lite Android example doesn't compile with Bazel", "body": "TensorFlow Lite Android example doesn't compile with Bazel, as explained in its README.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master (c0662f1620c2b97abb79b8ae6a8a30f7c7719475)\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: (not using cuda)\r\n- **GPU model and memory**: (not using GPU)\r\n- **Exact command to reproduce**: `bazel build --cxxopt='--std=c++11'   //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo`\r\n\r\n### Describe the problem\r\nWhen trying to compile the TF Lite Android demo with Bazel, it doesn't work, yielding:\r\n\r\n```\r\nWARNING: The major revision of the Android NDK referenced by android_ndk_repository rule 'androidndk' is 16. The major revisions supported by Bazel are [10, 11, 12, 13, 14]. Defaulting to revision 14.\r\nINFO: Analysed target //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/santiago/repos/tensorflow/tensorflow/contrib/lite/kernels/internal/BUILD:273:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels/internal:tensor_utils' failed (Exit 1)\r\nIn file included from tensorflow/contrib/lite/kernels/internal/tensor_utils.cc:24:\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/optimized/neon_tensor_utils.h:21:\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/optimized/cpu_check.h:21:\r\nexternal/androidndk/ndk/sources/android/cpufeatures/cpu-features.h:31:10: fatal error: 'sys/cdefs.h' file not found\r\n#include <sys/cdefs.h>\r\n         ^~~~~~~~~~~~~\r\n1 error generated.\r\nTarget //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 14.636s, Critical Path: 7.92s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI have already tried installing all APT packages that contain a file named `sys/cdefs.h`, including `g{cc,++}-{5,6,7}-multilib`. NDK 16 is installed, along with LLDB 3.0 and cmake.\r\n\r\n### Source code / logs\r\ntf_env.txt:\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux s.local 4.13.0-16-lowlatency #19-Ubuntu SMP PREEMPT Wed Oct 11 19:51:52 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"17.10 (Artful Aardvark)\"\r\nVERSION_ID=\"17.10\"\r\nVERSION_CODENAME=artful\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux s.local 4.13.0-16-lowlatency #19-Ubuntu SMP PREEMPT Wed Oct 11 19:51:52 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/santiago/torch/install/lib:/home/santiago/torch/install/lib:\r\nDYLD_LIBRARY_PATH /home/santiago/torch/install/lib:/home/santiago/torch/install/lib:\r\n\r\n== nvidia-smi ===================================================\r\n/dev/fd/63: l\u00ednea 105: nvidia-smi: orden no encontrada\r\n\r\n== cuda libs  ===================================================\r\n```", "comments": ["I have met this error also.\r\nTry different with NDK version,  get fatal error complaining different head file not found. \r\n#include <sys/xxx.h>\r\nAny one have a look at this?\r\n", "i am now trying to compile the demo by Android Studio,but i want to know how you get the libtensorflow_lite.so,the new source recompiled  just get libtensorflow_inference.so,but in this so file no model named org.tensorflow.lite.Interpreter,can anyone help me.", "I'm running into this problem too on Docker (I'm the one who wrote the build instructions). @bryant1410, could you also post the contents of your `WORKSPACE` file?\r\n\r\nThe build has been working in our internal Bazel tests, so my initial assumption is that this is an environmental configuration issue of some kind.", "I cloned HEAD on my work machine and successfully built the app:\r\n\r\n```shell\r\ngit clone http://github.com/tensorflow/tensorflow /tmp/tf\r\ncd /tmp/tf\r\nvim WORKSPACE\r\n./configure\r\nbazel build //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --cxxopt='--std=c++11' --config=android_arm\r\n```\r\n\r\nHere's what WORKSPACE looks like:\r\n\r\n```python\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 23,\r\n    build_tools_version = \"26.0.1\",\r\n    path = \"<my sdk path>\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name = \"androidndk\",\r\n    api_level = 14,\r\n    path = \"<my ndk path>\",\r\n)\r\n```\r\n\r\nAnd the output of `bazel version`:\r\n\r\n```\r\nBuild label: 0.8.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Nov 27 13:19:39 2017 (1511788779)\r\nBuild timestamp: 1511788779\r\nBuild timestamp as int: 1511788779\r\n```\r\n\r\nSo there's... some critical environmental difference going on.", "This is ultimately caused by https://github.com/bazelbuild/bazel/issues/4068. NDK version 16 rearranged important includes, and Bazel doesn't support it yet.\r\n\r\nYou should be able to work around this by manually downloading NDK version 15, which you'll have to search around online for (the sdk manager doesn't let you download older versions). You can also try messing with [the NDK developer recommendations for build toolchain changes](https://android.googlesource.com/platform/ndk/+/ndk-release-r16/docs/UnifiedHeaders.md), which I haven't had time to experiment with.", "Our official workaround at the moment is to download an older NDK and point to that instead. @bryant1410 , can you verify that works for you?", "@angersson yes, that works. Thanks!", "Thanks @angersson, reverting to NDK v14 worked for me!"]}, {"number": 14662, "title": "Distributed TF hangs because of \"CreateSession still waiting for response from worker....\"", "body": "**UPDATE:** The first 2 posts are no longer appropriate to describe the issue. Please jump to my 3rd post.\r\n\r\nHi,\r\n\r\nI followed the idea of this https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py#L1670 to implement a worker sync queue. Everything seemed to work fine except when I increased the size of the dataset or number of workers: all workers hang when they try to evaluate the sync op. Typical example of my code is as follow:\r\n\r\nFor each worker:\r\n```\r\n    def create_sync_queue_ops(self, op_prefix):\r\n        \"\"\"\r\n        op_prefix: a string that denote where the sync is being used.\r\n        \"\"\"\r\n        #Evenly distribute queues to all ps or workers\r\n        device_name = \"/job:ps/task:{}/cpu:0\".format(self.sync_queue_counter % self.num_ps_nodes)\r\n        self.sync_queue_counter += 1\r\n        with tf.device(device_name):\r\n            sync_queues = [tf.FIFOQueue(self.num_worker_nodes, [tf.bool], shapes=[[]], shared_name=\"{0}_{1}\".format(op_prefix,i)) for i in xrange(self.num_worker_nodes)]\r\n            token = tf.constant(False)\r\n            queue_ops = []\r\n            for i, q in enumerate(sync_queues):\r\n                if i == self.worker_id:\r\n                    queue_ops.append(tf.no_op())\r\n                else:\r\n                    queue_ops.append(q.enqueue(token))\r\n            #Drain tokens off queue for this worker after enqueuing ops\r\n            with tf.control_dependencies(queue_ops):\r\n                wait_ops = sync_queues[self.worker_id].dequeue_many(len(sync_queues)-1)\r\n            return wait_ops\r\n\r\n(some graph definition...)\r\nsess = tf.Session()\r\ndemo_sync_ops = self.create_sync_queue_ops(\"demo\")\r\nif self.is_chief_worker: #only execute by worker 0, other workers do nothing\r\n    sess.run(tf.global_variables_initializer())\r\nprint \"finishing message\"\r\nsess.run(demo_sync_ops)\r\n```\r\n\r\nI could **occasionally** see all workers hang after printing the \"finishing message\". \r\nMy observation so far is that this only happened when dataset is huge or number of worker is big. e.g. 10+TB dataset with 300-500 workers.\r\n\r\nI haven't been able to see why this occurred, not sure if it is a TF issue or some network bottleneck that I was not aware of. Any help would be much appreciated!\r\n  ", "comments": ["Some updates:\r\nI think the shared queue is inherently unstable, according to my experience and to some Googler who works on TF.\r\nSo I implemented the sync mechanism without using queues. But the hangs still happened, and it looked like this was due another problem than some servers just hung because the port it got assigned was taken away by others. (hung on the first sess.run())\r\n\r\nSo there were actually at least two things contributing to the hanging behavior.\r\nAnyone who has more insights please feel free to shed light.", "Yes, we are moving to the dataset API. Would that work for you?\r\n\r\n/CC @mrry ", "@drpngx @mrry Some updates:\r\nFound the direct cause of the hanging problem that happened right after the first sess.run(). It's because the there are some workers indefinitely waiting for the connections to other servers to be established. (\"CreateSession still waiting for response from worker....\"). The amount of bad connections was random, and this did not only happened in some particular hosts.\r\n\r\nThis happened quite often when I have large dataset (5TB+) and 300+ servers.\r\n\r\nMy guess was, with that large amount of servers, if I declare the clusterSpec as what TF suggested: (PS:[list of all ps ports], workers:[list of all worker ports]), there are way too many unnecessary connections for TF or the system to handle (I don't have network/OS savvy so not sure if I am using the correct words).", "@drpngx \r\nThank you for your response. The dataset we have is some huge .npy files. So what I did is first loading them into memory and splitting it into batches, then I enqueued those batches into RandomShuffleQueue. Could you elaborate why you suggested dataset API?", "@OscarDPan the dataset API is easier to analyze than the queues, which can be hard to debug when they freeze. As long as you're OK with loading into memory then splitting, I think it should work just fine.\r\n\r\nCould you try that instead?", "@drpngx were you suggesting me to use the dataset API to create a synchronization barrier? I am not sure if that helps with the problems I found as I mentioned in the post above.\r\n\r\nAnd FYI I had no problem enqueuing/dequeuing training data to/from a RandomShuffleQueue. As to whether it is ok to use FIFOQueue to create sync barrier, I am not sure since there's the server connection problems hindering the analysis.\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@OscarDPan I want to make sure it's not a queuing problem. Sorry, our time is limited and I am trying to isolate the problem and make 100% sure that it's not a queuing problem before we take a look.", "@drpngx \r\nI don't think it's a queuing problem. I updated the title that better describes the problem.", "@OscarDPan Which version of TF are you using? 1.5.0rc0 (released today) includes an upgraded version of gRPC that fixes a deadlock bug that causes some servers to hang after a random period of time (although more quickly under heavier request load/at larger scale). Could you confirm whether the same problem still applies with the latest code?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mrry thanks for the update.\r\nI was using TF1.3. There's been a lot of changes to my graph since I found the problem. I am still using 1.3 but now each worker don't see other workers and only see the necessary PS. This reduces the connections a lot and I haven't seen any hanging problem.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Okay, I'm going to close this issue for now, since you seem to have found a workaround. I suspect TF 1.5 (and the upgraded gRPC in particular) will fix the original program. If the problem comes back and it can't be solved by upgrading, please feel free to reopen the issue."]}, {"number": 14661, "title": "TensorFlowMaximum operator in TF Lite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Docker \r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI am trying to convert a graph from .pb to .lite format using toco, but I get this error:\r\n`2017-11-17 10:20:47.738777: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: TensorFlowMaximum`\r\n\r\nI had a look inside the code, and found `TensorFlowMaximum` operator defined inside `lite/kernels/internal/optimized/optimized_ops.h`, but apparently it's not linked or defined anywhere else.\r\n\r\nWhat's the fastest way to get this defined and linked?\r\n\r\nThanks a lot.\r\n", "comments": ["@aselle, any suggestions? ", "Should add, I also seem to be having a near identical error, in the form of:\r\n\r\n`2017-11-21 12:06:51.416795: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: TensorFlowMinimum`, even though it is also in the same aforementioned header file.", "any updates? :-)", "any updates?\r\n", "Could you provide a reproducible small tensorflow example. What kind of model have you converted? ", "Here is the command I use to convert from .pb to .tflite\r\n\r\n```bash\r\nbazel run --config=opt tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/path/to/tiny-yolo-voc.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=/path/to/tiny-yolo-voc.lite --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --output_arrays=output --input_shapes=1,416,416,3\r\n```\r\n\r\nand here is the [model file](https://www.dropbox.com/s/hwqam5eo167tcsx/tiny-yolo-voc.pb?dl=0) and here is the output I get:\r\n\r\n```bash\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/toco/toco\r\nINFO: Elapsed time: 0.571s, Critical Path: 0.01s\r\nINFO: Build completed successfully, 1 total action\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/path/to/tiny-yolo-voc.pb' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--output_file=/path/to/tiny-yolo-voc.lite' '--inference_type=FLOAT' '--input_type=FLOAT' '--input_arrays=input' '--output_arrays=output' '--input_shapes=1,416,416,3'\r\n2017-11-28 09:58:11.208146: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 74 operators, 125 arrays (0 quantized)\r\n2017-11-28 09:58:11.553490: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 58 arrays (0 quantized)\r\n2017-11-28 09:58:11.553743: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 58 arrays (0 quantized)\r\n2017-11-28 09:58:11.553908: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:312] Total transient array allocated size: 33226752 bytes, theoretical optimal value: 33226752 bytes.\r\n2017-11-28 09:58:11.554142: I tensorflow/contrib/lite/toco/toco_tooling.cc:255] Estimated count of arithmetic ops: 6.98837 billion (note that a multiply-add is counted as 2 ops).\r\n2017-11-28 09:58:11.554394: F tensorflow/contrib/lite/toco/tflite/export.cc:192] Unsupported operator: TensorFlowMaximum\r\n```", "inside  toco/tflite/operator.cc, most class are inherited from BuiltinOperator, but class Split : public CustomOperator<TensorFlowSplitOperator>. I noticed. Unfortunately, I have not figured out how to fix it.", "Assigning to @andrehentz for advising.", "You'll need to implement a wrapper to make the existing optimized_ops.h available to TF Lite, and you would need to implement the export code for toco. In both cases you can look at the MUL operation for inspiration. I'd say the steps would be:\r\n\r\n1. Propose a change to schema/schema.fbs to add the new operator along with any required options\r\n2. Change toco/tflite/operator.cc to export/inport the new op. Add a unittest or two.\r\n3. Implement parsing code in model.cc and, if necessary, builtin_op_data.h (SpaceToDepth is a better example than Mul here)\r\n4. Add kernels/max.cc (for TensorflowMaximum) with the proper checks and calls to optimized_ops.cc (and perhaps reference_ops.cc for completeness too). Add a unittest or two. If possible, provide a quantized implementation too.\r\n", "Thanks!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@andrehentz , @mohamedadaly \r\n\r\nWhile not production-ready and without any tests, I have created [a patch](https://gist.github.com/StanislawAntol/ce9fb48d112a133cabb92be1c093f1cd) on top of `TensorFlow v1.6.0-rc1` that adds a valid reference implementation for `tf.maximum` (i.e., doesn't assume input2 is a scalar) into `TensorFlowLite`. I used @andrehentz 's suggestions and [this recent commit](https://github.com/tensorflow/tensorflow/commit/90159c53b83527bd088452769ea4e1b98667860c) as a guide (note: I don't understand the TFLite codebase, so I relied a lot on pattern-matching). It seems to work fine on my small experiment (`tiny-yolo-voc` model on Android that outputs results consistent with the `TensorFlow` version), but it could probably use some tests and an optimized implementation.", "@StanislawAntol Would you like to make a PR and work together through the review?", "I just found this issue via a search; Toco gives me the below error - could you include minimum in this change as well?\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. Here is a list of operators for which you will need custom implementations: TensorFlowMinimum.", "@andrehentz , Am I supposed to copy over the `schema_generated.h` file from the `bazel-genfiles` directory for the commit? It seems without it, I can't built my test, but with it, I notice that I get a bunch of extra diffs, such as:\r\n```\r\n1154c1175\r\n<   explicit QuantizationParametersBuilder(flatbuffers::FlatBufferBuilder &_fbb)\r\n---\r\n>   QuantizationParametersBuilder(flatbuffers::FlatBufferBuilder &_fbb)\r\n```\r\nWhat do you recommend for this?", "I ended up manually removing any differences that aren't additions in `schema_generated.h`.\r\n\r\nYou can find my PR at #17074.\r\n\r\nI don't have that many tests (e.g., not sure if broadcasting works), but running `bazel test tensorflow/contrib/lite/kernels:maximum_test`, `bazel test tensorflow/contrib/lite/toco/tflite:operator_test`, and `bazel test //tensorflow/contrib/lite/testing:generated_examples_zip_test` all passed.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 14660, "title": "typo fixed", "body": "It appears that you are missing a comma after the introductory phrase In the last few years. Consider adding a comma", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14658, "title": "tf.metrics.mean_relative error doesn't handle complex inputs", "body": "The offending line is here: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9c4bdb865452e418a1d69cd5f5cdccb51d6a0e1d/tensorflow/python/ops/metrics_impl.py#L1057\r\n\r\n`array_ops.where` expects the second and third inputs to be of the same shape and **type**. Unfortunately, when the `labels` input is complex, `array_ops.zeros_like(labels)` returns an (unnecessarily) complex tensor, whereas the third input returns a float tensor. \r\n\r\nI haven't checked, but suspect similar problems might occur with other metrics (and perhaps elsewhere). ", "comments": ["Tagging my colleague @crjensen21. ", "Hi, @amir-mogh . I open a PR to fix the issue, would you like to take a look?", "Closing this issue since a PR has been pushed. Please use the latest version of TensorFlow and feel free to reopen the issue if it still persists. Thanks!"]}, {"number": 14657, "title": "FusedBatchNorm & Conv2D backwards doesn't support zero batch size", "body": "Most ops in TF work well with tensors with zero elements. However, <del>convolution</del> fusedbatchnorm with cudnn gives the following error:\r\n```\r\n2017-11-17 08:00:20.835113: F tensorflow/stream_executor/cuda/cuda_dnn.cc:444] could not convert BatchDescriptor {count: 0 feature_map_count: 1024 spatial: 28 28  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX} to cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM \r\n```\r\n\r\nI would expect it checks and returns a 4D tensor with zero batch-size. Currently I have to work around it by `tf.cond`.", "comments": ["@zheng-xq this seems like it falls between the boundaries of cuDNN and TF. Is this something we might fix, or should I set to \"contributions welcome\"?", "Perhaps doc of `cudnnSetTensorNdDescriptor` is helpful for discussion.\r\n\r\nReturn Value | Meaning\r\n----|---\r\nCUDNN_STATUS_BAD_PARAM | At least one of the elements of the array dimA was negative or zero, or dataType has an invalid enumerant value\r\n", "@ppwwyyxx \r\nHi, I'm facing the same issue using CUDA 8.0 with cuDNN 6. Could you show me how did you use tf.cond? Like which parameter did you put condition on?\r\n\r\nThank you!", "https://github.com/ppwwyyxx/tensorpack/blob/be50085f7e2d9ed2704a14cb4c2e6aaa20548972/examples/FasterRCNN/train.py#L123-L134", "@ppwwyyxx Could you point me how to use tf.cond on keras generator type of input? ", "I'm not sure I know what you mean. And I think it's already unrelated to the issue anyway. You can search for documentation/examples or ask on stackoverflow for usage questions.", "Turned out that `Conv2d` actually does support empty tensor, but `FusedBatchNorm` does not. That is probably why I saw this error. To reproduce:\r\n```python\r\nwith tf.device('/gpu:0'):\r\n    x = tf.random_normal([0, 16, 64, 64])\r\n    scale = tf.random_normal([16])\r\n    y = tf.nn.fused_batch_norm(x, scale, scale, data_format='NCHW')\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(y))\r\n```", "There is a problem with conv2d backward as well, though forward works:\r\n```python\r\nwith tf.device('/gpu:0'):\r\n    x = tf.random_normal([0, 16, 64, 64])\r\n    W = tf.random_normal([3, 3, 16, 16])\r\n    y = tf.nn.conv2d(x, W, [1, 1, 1, 1], padding='SAME', data_format='NCHW')\r\n    grad = tf.gradients(y, W)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(grad))\r\n```", "Anyone can take a look at my PR #15264? ", "Thanks for the PR @ppwwyyxx . I will take a look."]}, {"number": 14656, "title": "little_modify", "body": "Modify the list '[' ']' to the tuple '(' ')'", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Please handle the CLA before we can code review. Thanks!", "u need to pass cla ~ i was there, now i passed , just follow me:\r\n#14867", "@lizuoyue if you signed the CLA, please reply \"I signed it!\" to this thread. Thanks!", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing for now. Let us know if you have updates."]}, {"number": 14655, "title": "Support dynamic partition in loss function", "body": "\r\n\r\n### System information\r\n\r\nPython 3.6.1, Ubuntu 16.04, TF 1.3\r\n\r\n### Describe the problem\r\n\r\nI am trying to implement a loss function in Tensorflow similar to the Theano loss function described here: https://github.com/Lasagne/Lasagne/issues/767\r\n\r\nI have tried several code but they all lead to runtime error.  Seems that dynamically sized tensors aren't supported in loss function.  I have tried various optimizers, including adam, without success.\r\n\r\nWhen tested standalone, the function works fine and produces results similar to a numpy based implementation.\r\n\r\n### Source code / logs\r\n  Here is one code I tried:\r\n\r\n    import tensorflow as tf\r\n    \r\n    def pair_loss(y_true, y_pred):\r\n        y_true = tf.cast(y_true, tf.int32)\r\n        parts = tf.dynamic_partition(y_pred, y_true, 2)\r\n        y_pos = parts[1]\r\n        y_neg = parts[0]\r\n        y_pos = tf.expand_dims(y_pos, 0)\r\n        y_neg = tf.expand_dims(y_neg, -1)\r\n        out = tf.sigmoid(y_neg - y_pos)\r\n        return tf.reduce_mean(out, axis=-1)\r\n\r\nHere is the theano code for reference:\r\n\r\n    import theano\r\n    \r\n    def calc_auroc_loss(pred_vr, y_vr):\r\n        pos_pred_vr = pred_vr[y_vr.nonzero()]\r\n        neg_pred_vr = pred_vr[theano.tensor.eq(y_vr, 0).nonzero()]\r\n        pred_diffs_vr = pos_pred_vr.dimshuffle(0, 'x') - neg_pred_vr.dimshuffle('x', 0)\r\n        num_pairs_vr = theano.tensor.sum(theano.tensor.eq(y_vr, 1)) * theano.tensor.sum(theano.tensor.eq(y_vr, 0))\r\n        auroc_vr = theano.tensor.sum(theano.tensor.nnet.sigmoid(pred_diffs_vr)) / num_pairs_vr\r\n        return -auroc_vr", "comments": ["You can literally line-by-line translate your theano code. Maybe you missed `tf.boolean_mask`.", "boolean_mask does not work either.\r\n\r\nAgain, the function runs fine except if it used as a loss function.", "I guess you need to tell everyone what is \"does not work\". Post errors if you saw any, and better with some code.", "OK, I will extract code from my full code and post it here.", "I attach a code and its output that shows the issue.  I think the issue comes from the broadcast in line 16 of the attached code.\r\n\r\n[tf_bug.py.txt](https://github.com/tensorflow/tensorflow/files/1487749/tf_bug.py.txt)\r\n[tf_bug_output.txt](https://github.com/tensorflow/tensorflow/files/1487751/tf_bug_output.txt)\r\n\r\n", "It's just another constraint added by keras and not a tensorflow issue. It's complaining that your custom loss function return something of a different shape. Using normal tensorflow optimizer with this custom loss will be fine. Or you can ask in keras issues or stackoverflow about how to do this in keras.", "Not really, because the return shape is the same whether I have this broadcast in line 16 or something without broadcast. Yet the latter does not trigger an issue.  \r\n\r\nIf I use tf.keras instead of keras, does this qualify better as a tf issue?", "Line 16 in your code is `out = tf.sigmoid(y_neg - y_pos)`. I don't know what you mean by not doing broadcast, since y_neg and y_pos may have different shape. Maybe you can clarify that by providing a modification that makes it work.\r\nI'm sure gradient can pass your loss function in raw tensorflow, because if you use `tf.reduce_mean(out)` it can run. Why it does not work in keras is not something I'm able to help. But you can certainly leave it here for others to answer. At least now the problem is more clear.", "Your previous comment made me find the issue, thanks a lot.  The issue was in my code indeed.  I should have used tf.reduce_mean(out) instead of tf.reduce_mean(out, axis=-1).  ", "Oh great. I thought you want `axis=-1` on purpose, because Keras does allow you to return a vector in the loss function.", "Yes, Keras allows this, which is why the error was a bit obfuscated for me.  Thanks again for your help, and sorry for the distraction.  ", "This issue still appears in the open issues because of the tag awaiting response, but there is no more response to provide.  It can be closed.", "@jfpuget I tried using the corrected version of your pair_loss function above on a GRU model. It works fine for about 2 seconds after which I keep getting `loss: nan` during the remainder of the training. Curiously enough, the accuracy keeps increasing. Would you happen to know why it might be returning `nan`?", "Hi,  some of your gradient is exploding.  Try using a different optimizer, or try clipping gradient.  The net result is that some predictions are leading to overflow in computation, and therefore nan, but it seems that in most cases your predictions are getting better. ", "@jfpuget Thank you for responding. Gradient clipping and optimizer selection only delayed the nanification process, but could not prevent it. However, based on the discussion [here](https://github.com/keras-team/keras/issues/2134), simply increasing the batch size got it working again. ", "@ashimb9: You might be getting Nans when your mini-batch contains only a single class. I'm not sure of that function's behavior if either of the partitions is empty, or what a good default behavior would be. I don't believe that behavior is defined for RankNet or LambdaRank, where the assumption is that you are dealing with labeled pairs."]}, {"number": 14654, "title": "Unable to install Tensorflow on Ubuntu - Anaconda error trace", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.4.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**: \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n\r\n### Describe the problem\r\nI am installing the latest version of tensorflow 1.4.0 and I am getting an error when I try to install it inside a conda environment.\r\n\r\n`\r\nconda create -n py36-tensorflow python=3.6\r\n`\r\n`\r\nsource activate py36-tensorflow\r\n`\r\n`\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n`\r\n\r\nthe above are the commands I ran and below is the error trace I am getting.\r\n\r\n### Source code / logs\r\nLogs:\r\n\r\n\r\n`ubuntuvm@ubuntuvm:~$ conda create -n py36-tensorflow python=3.6`\r\n`Fetching package metadata ...........`\r\n`Solving package specifications: .`\r\n\r\nPackage plan for installation in environment /home/ubuntuvm/anaconda3/envs/py36-tensorflow:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    ca-certificates: 2017.08.26-h1d4fec5_0\r\n    certifi:         2017.7.27.1-py36h8b7b77e_0\r\n    libedit:         3.1-heed3624_0\r\n    libffi:          3.2.1-hd88cf55_4\r\n    libgcc-ng:       7.2.0-h7cc24e2_2\r\n    libstdcxx-ng:    7.2.0-h7a57d05_2\r\n    ncurses:         6.0-h9df7e31_2\r\n    openssl:         1.0.2m-h26d622b_1\r\n    pip:             9.0.1-py36h6c6f9ce_4\r\n    python:          3.6.3-h1284df2_4\r\n    readline:        7.0-ha6073c6_4\r\n    setuptools:      36.5.0-py36he42e2e1_0\r\n    sqlite:          3.20.1-hb898158_2\r\n    tk:              8.6.7-hc745277_3\r\n    wheel:           0.29.0-py36he7f4e38_1\r\n    xz:              5.2.3-h55aa19d_2\r\n    zlib:            1.2.11-ha838bed_2\r\n\r\nProceed ([y]/n)? y\r\n```\r\npython-3.6.3-h 100% |#################################################################| Time: 0:00:01  16.93 MB/s\r\npip-9.0.1-py36 100% |#################################################################| Time: 0:00:00  54.97 MB/s\r\n`#`\r\n`# To activate this environment, use:`\r\n`# > source activate py36-tensorflow`\r\n`#`\r\n`# To deactivate an active environment, use:`\r\n`# > source deactivate`\r\n`#`\r\n\r\n`ubuntuvm@ubuntuvm:~$ source activate py36-tensorflow`\r\n`(py36-tensorflow) ubuntuvm@ubuntuvm:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl`\r\n`Collecting tensorflow==1.4.0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl`\r\n  `Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl (41.2MB)`\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41.2MB 33kB/s\r\n`Collecting protobuf>=3.3.0 (from tensorflow==1.4.0)`\r\n  Using cached protobuf-3.5.0-py2.py3-none-any.whl\r\n`Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.0)`\r\n  Using cached tensorflow_tensorboard-0.4.0rc3-py3-none-any.whl\r\n`Collecting six>=1.10.0 (from tensorflow==1.4.0)`\r\n  Using cached six-1.11.0-py2.py3-none-any.whl\r\n`Collecting enum34>=1.1.6 (from tensorflow==1.4.0)`\r\n  Using cached enum34-1.1.6-py3-none-any.whl\r\n`Collecting wheel>=0.26 (from tensorflow==1.4.0)`\r\n  Using cached wheel-0.30.0-py2.py3-none-any.whl\r\n`Collecting numpy>=1.12.1 (from tensorflow==1.4.0)`\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/basecommand.py\", line 215,in main\r\n    status = self.run(options, args)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/commands/install.py\", line335, in run\r\n    wb.build(autobuilding=True)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/wheel.py\", line 749, in build\r\n    self.requirement_set.prepare_files(self.finder)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/req/req_set.py\", line 380,in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/req/req_set.py\", line 554,in _prepare_file\r\n    require_hashes\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/req/req_install.py\", line 278, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/index.py\", line 465, in find_requirement\r\n    all_candidates = self.find_all_candidates(req.name)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/index.py\", line 423, in find_all_candidates\r\n    for page in self._get_pages(url_locations, project_name):\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/index.py\", line 568, in _get_pages\r\n    page = self._get_page(location)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/index.py\", line 683, in _get_page\r\n    return HTMLPage.get_page(link, session=self.session)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/index.py\", line 792, in get_page\r\n    \"Cache-Control\": \"max-age=600\",\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py\", line 488, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/download.py\", line 386, inrequest\r\n    return super(PipSession, self).request(method, url, *args, **kwargs)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py\", line 475, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py\", line 596, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/_vendor/cachecontrol/adapter.py\", line 37, in send\r\n    cached_response = self.controller.cached_request(request)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/_vendor/cachecontrol/controller.py\", line 111, in cached_request\r\n    resp = self.serializer.loads(request, cache_data)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py\", line 114, in loads\r\n    return getattr(self, \"_loads_v{0}\".format(ver))(request, data)\r\n  File \"/home/ubuntuvm/anaconda3/envs/py36-tensorflow/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py\", line 176, in _loads_v2\r\n    cached = json.loads(zlib.decompress(data).decode(\"utf8\"))\r\nzlib.error: Error -5 while decompressing data: incomplete or truncated stream\r\n(py36-tensorflow) ubuntuvm@ubuntuvm:~$`\r\n```", "comments": ["looks to be simply a bad download?\r\nCould you retry installation, and see if it repeats exactly the same way?\r\n\r\nIf it still does not work, we recommend following one of the official installation methods here:\r\nhttps://www.tensorflow.org/install/install_linux", "@gunan If I simply do \r\n`pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl`\r\nthen it is installing (however without numpy), but I did \r\n`pip install numpy`\r\nI am getting requirement already satisfied. \r\nBut if I do \r\n`pip install numpy --upgrade`\r\nI am getting that exception again\r\nI think I am not able to retrieve the latest version of numpy package.\r\nMy numpy version is '1.13.3' Can I use this with the latest version of tensorflow 1.4.0?", "@gunan However when I import the tensorflow library. I am getting the below warning.\r\n\r\n`>>> import tensorflow\r\n/home/ubuntuvm/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n`\r\n\r\nI assume the above warning is just a normal warning because I tried in a diffrent ubuntu machine where it installed properly. And yes I used the 1.4.0 version. And upon importing the library in python I still got the same warning.\r\n", "@vickylance Thanks for trying out the suggestions.\r\nTF should work fine with numpy 1.13.3, that is why the first command installs without numpy.\r\n\r\nAbout the warning you are seeing. It looks like just a warning. can you use TF normally despite that?\r\nIn case you see issues, it should be fixed in our latest nightly packages.", "@gunan In windows I see no issues/warnings but its definitely persistent in ubuntu I tried in 3 different environments of ubuntu. And the warning is still there. But I think it should work fine with no issues.\r\n\r\nThanks for the help. I am closing this issue for now. Will reopen if I were to face some issue due to it."]}, {"number": 14653, "title": "Use the known fixed size of the tensor array instead of the dynamic one", "body": "For compatibility with XLA:  The tensorarray.size() operator produces graphs which are not caompatible with XLA.  The size of the tensor array is known explicitly, so by using that size we can avoid the incompatibility.\r\n\r\nThe other examples of stack() used in this file are not the same, and do not need adjusting.\r\n", "comments": ["Can one of the admins verify this patch?", "Closing as per @ebrevdo's comments. As he mentioned, we're actively working on this.", "@ebrevdo \r\n\r\nhello.\r\n\r\nthanks for the feedback. I have a working version of dynamic_rnn at the moment, although obviously it relies on changes to the XLA bridge, while_loop and dynamic_rnn that are not present in the public version of Tensorflow.  In an ideal world I would rather abandon my changes and have our version of TF the same as the public one.\r\n\r\ni am interested to hear more about what you are doing. can someone who is working on it contact me so I can get a better idea of what is going on?  for instance, are you going to have the python code for while_loop generate XlaWhile ops directly, or fix up the functionalize_loop and mark_for_compilation_pass code. also - will you change the clustering code so that the forward and backward pass loops (and the gather loop) are placed into the same cluster to be compiled together?\r\n\r\n thanks.\r\n\r\n\r\n", "Might be easier for you to chat with @jpienaar through e-mail.", "The shape of the input sequence is not known in general. The batch size and\nmax time steps can change from batch to batch.\n\nOn Wed, Nov 22, 2017, 6:39 AM David Norman <notifications@github.com> wrote:\n\n> *@DavidNorman* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/rnn.py\n> <https://github.com/tensorflow/tensorflow/pull/14653#discussion_r152582888>\n> :\n>\n> > @@ -810,7 +810,8 @@ def _time_step(time, output_ta_t, state):\n>\n>    # Unpack final output if not using output tuples.\n>    if in_graph_mode:\n> -    final_outputs = tuple(ta.stack() for ta in output_final_ta)\n> +    final_outputs = tuple(\n> +        ta.gather(range(time_steps)) for ta in output_final_ta)\n>\n> Can you explain why it is not? The time steps parameter (graph compile\n> time) is both the size of the output tensor array and the loop counter. It\n> comes from the shape of the input sequence, which is known.\n>\n> I'm not suggesting that you take this change, if there is an alternative\n> XLA compatible dynamic_rnn in the pipeline. Just curious why it isn't\n> static.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14653#discussion_r152582888>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzQwaByBg3r91WuainNYiweub2Lvks5s5DIngaJpZM4Qh57J>\n> .\n>\n"]}, {"number": 14652, "title": "a document bug", "body": "from https://www.tensorflow.org/mobile/prepare_models\r\nin the section: How do you get a model you can use on mobile?\r\nthe right path is \uff1a\r\n```\r\nbazel build tensorflow/python/tools:freeze_graph\r\n```\r\nnot \r\n```\r\nbazel build tensorflow/tools:freeze_graph\r\n```", "comments": ["@petewarden could you take a quick look into the doc.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 14651, "title": "crash", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I don't see any specifics here. Closing; please reopen if you've filled out the description. "]}, {"number": 14650, "title": "tensorflow lite: how to set  --input_shapes param with batchsize when gen the pb lite file", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: lastest master\r\n- **Python version**: 2.7\r\n\r\n### Describe the problem\r\n\r\nmy python train script input tensor define as below :\r\ntf.placeholder(dtype=tf.float32, shape=[None, 440], name=conf.input_features_tensor)\r\n\r\nwhen i execute \"bazel run --config=opt tensorflow/contrib/lite/toco:toco\r\n --others --input_shapes={None, 440} \"\r\n\r\nan error show as below :\r\n\r\n2017-11-17 17:00:10.285456: F tensorflow/contrib/lite/toco/model_cmdline_flags.cc:269] Check failed: absl::SimpleAtoi(dim_str, &size) Failed to parse input_shape: None,440.\r\n\r\nhow i can set input batch size?\r\n\r\n@andrehentz thx\r\n", "comments": ["If you know the batch size ahead of time, pass it in instead of None. For example --input_shape=3,440\r\n\r\nIf the batch size will change during inference, you can pass anything in and resize the input tensors during inference, when you know the size of the batch. Take a look at the apis.md file.", "@andrehentz thx", "Feel free to re-open if your issue hasn't been fully resolved.", "@andrehentz \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/14650#issuecomment-345325832\r\n\r\nit doesn't work for me. I did exactly this, and I resize input tensors and call for allocation and at some point I get the reshape op called and when it calculates the shape of the output tensor it reports back that it's different to the input shape\r\n\r\nreshape.cc line 58\r\nTF_LITE_ENSURE_EQ(context, num_input_elements, num_output_elements);\r\n\r\nAs far as I understand some op (custom or built in) has the shape as constant and doesn't allow modification and I cannot find out which one. Probably it's a built-in op because I went and modified all my custom ops to use dynamic tensors and still have the same error.\r\n\r\nI Googled this and many people have the same problem"]}, {"number": 14649, "title": "[XLA] Fix build issue with TensorShape constructor", "body": "The TensorShape requires a default provided constructor.  This change uses the same style of providing that as is seen elsewhere in the code.\r\n\r\nOS: possibly only OS/X\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "hi - thanks for the approval.  i suspect that the failures are nothing to do with my change.  looks like a tool failure."]}, {"number": 14648, "title": "labels produced by Dataset  API can not be well operated!  when use tf.reduce_sum or tf.split on labels ,it returnss the wrong result ", "body": "my system :\r\n     cuda 8.0\r\n     tensorflow-gpu (1.3.0)\r\n     tensorflow-tensorboard (0.1.7)\r\n      python 2.7\r\n       ubuntu 14.04\r\n### *************description\uff1a**************\r\n\r\n>  I use Dataset and Estimator to train my model.  When I test the  \r\n>  return value  of Dataset , the return value(images and labels) of input_fn is right.   \r\n>  But when I use tf.reduce_sum or tf.split to the labels, all results got wrong.  \r\n>  I can't undenstand what had happend.\r\n\r\n### **************key code*******\r\ndef dataset_parser(value,is_training): # return signal  imagel and label\r\ndef input_fn(is_training,num_epochs=1) # return batch imagel and label\r\n\r\nFLAGS.batch_size=5\r\nsess = tf.InteractiveSession()\r\nimages,labels = input_fn(True,1)\r\nsess.run(labels)\r\n\r\n> the labels of one batch\r\n>>array([[ 0.        ,  1.        ,  2.75984216,  5.86036015],\r\n       [ 1.        ,  0.        ,  3.82080388,  4.23745823],\r\n       [ 1.        ,  0.        ,  7.59959507,  4.93859673],\r\n       [ 0.        ,  1.        ,  3.29546738,  5.50357151],\r\n       [ 0.        ,  1.        ,  2.0612247 ,  6.73015881]], dtype=float32)\r\n\r\nlabel,weight,score = tf.split(value=labels,num_or_size_splits=[2,1,1],axis=1)\r\ni = 1\r\nprint labels\r\nprint sess.run(label[i]) \r\nprint sess.run(weight[i])\r\nprint sess.run(score[i])  \r\n\r\n>  we expected to get the split value of [1,0] [3.82080388,],[ 4.23745823] ,but the result is following\r\n>>Tensor(\"IteratorGetNext:1\", shape=(?, 4), dtype=float32)\r\n>>[ 0.  1.]\r\n>>[ 2.85865355]\r\n>>[ 5.57142878]\r\n\r\na = tf.reduce_sum(labels,0)\r\nprint  sess.run(a)\r\n> we expected to get the values of [  2. \u3000\u3000 , 3.\u3000\u3000\u3000,  19.53693319\u3000\u3000 , 27.27014543],but we get the following result.\r\n >>[  4.           1.          25.82047653  24.87050629]\r\n\r\n\r\n### # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"code detail \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\r\n\r\n```\r\ndef dataset_parser(value,is_training):\r\n    features = {}\r\n    features['image'] = tf.FixedLenFeature([], tf.string)\r\n    features['average_score'] = tf.FixedLenFeature((),tf.float32,default_value=0)\r\n    features['aes_tag'] = tf.FixedLenFeature((),tf.float32,default_value=0)\r\n \r\n    parsed = tf.parse_single_example(value, features=features)\r\n    image = tf.decode_raw(parsed['image'],tf.uint8)\r\n    image = tf.reshape(image,[256,256,3])\r\n\r\n    aes_tag = tf.one_hot(tf.cast(parsed['aes_tag'],tf.int32),FLAGS.num_classes)\r\n    if is_training:\r\n        weight = tf.where(tf.greater(score,5),FLAGS.beta,FLAGS.alpha)\r\n        label = tf.reshape(tf.concat([aes_tag,[weight],[score]],0),(4,))\r\n    \r\n    else:\r\n        weight = tf.ones(1,dtype=tf.float32)\r\n        label = tf.reshape(tf.concat([aes_tag,weight],0),(3,))\r\n\r\n    \r\n    return image,label\r\n\r\n\r\ndef input_fn(is_training,num_epochs=1):\r\n    \"\"\"Input function which provides batches for train or eval.\"\"\"\r\n    dataset = tf.contrib.data.TFRecordDataset([FLAGS.train_data_path if is_training  else FLAGS.test_data_path])\r\n\r\n    if is_training:\r\n        dataset = dataset.repeat(num_epochs)\r\n\r\n    dataset = dataset.map(lambda value: dataset_parser(value, is_training),\r\n        num_threads=5,output_buffer_size=FLAGS.batch_size)\r\n\r\n    if is_training:\r\n        buffer_size = 1250 + 2 * FLAGS.batch_size\r\n        dataset = dataset.shuffle(buffer_size=buffer_size)\r\n    iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()\r\n    images, labels = iterator.get_next()\r\n    return images, labels\r\n     \r\n```", "comments": ["Can you share the exact sequence of `sess.run()` calls? I can't quite tell from the snippets in the question, but if for example you do:\r\n\r\n```python\r\nprint sess.run(label[i])\r\nprint sess.run(weight[i])\r\nprint sess.run(score[i])\r\n```\r\n\r\n...each `sess.run()` call will return values from a different batch. If instead you do:\r\n\r\n```python\r\nprint sess.run([label[i], weight[i], score[i]])\r\n```\r\n\r\n...it will print a label, weight, and score from the same batch.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing due to lack of activity."]}, {"number": 14647, "title": "Move ptr_util to core/util.", "body": "Break the dependency of core onto XLA.\r\n\r\nFixes #14549", "comments": []}, {"number": 14646, "title": "tf.bitwise.bitwise_and and friends have bad shape functions", "body": "The bitwise ops are componentwise and do normal broadcasting at the kernel level.  However, they claim unchanged shape during op registration:\r\n\r\n    #define BINARY_BITWISE()                                                     \\\r\n      Input(\"x: T\")                                                              \\\r\n          .Input(\"y: T\")                                                         \\\r\n          .Output(\"z: T\")                                                        \\\r\n          .SetIsCommutative()                                                    \\\r\n          .Attr(\"T: {int8, int16, int32, int64, uint8, uint16, uint32, uint64}\") \\\r\n          .SetShapeFn(shape_inference::UnchangedShape)\r\n\r\nTo reproduce, do\r\n\r\n    >>> import tensorflow as tf\r\n    >>> tf.bitwise.bitwise_and(tf.zeros([3,1], dtype=tf.int32), tf.zeros([1,3], dtype=tf.int32))\r\n    <tf.Tensor 'BitwiseAnd:0' shape=(3, 1) dtype=int32>\r\n\r\nThe result shape should be `(3, 3)`, not `(3, 1)`.", "comments": ["@girving I think the shape function probably could be changed to `shape_inference::BroadcastBinaryOpShapeFn`? Added a PR #14678 for the fix."]}, {"number": 14645, "title": "Is tensorflow Lite support for detection, like SSD\uff1f", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Not currently, we will be adding support for more models in the future.\r\n\r\nCurrently supported models are Mobilenets, Inception V3 and Smart Reply.\r\nPlease look at the list of supported models here. \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md "]}, {"number": 14644, "title": "[Java] Add support for shape list attributes", "body": "This allows passing a list of shapes as an attribute to an operation. Some operation wrappers cannot be generated without it.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 14643, "title": "Can google publish checkpoint file for some common network:VGG, AlexNet", "body": "In tensorflow/models repository, developer maintains some pre-built model and checkpoint files from some famous network, it is very convenient  for some developer don't have too much computation resources, train a network from scratch can take a very long time.  The author of these networks do publish weights file but It is not for tensorflow, mostly for caffe. ", "comments": ["@wolffg is this the sort of thing we are planning to do, or can you ask someone who has perspective on it to comment?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 14642, "title": "Android tensorflow lite kernel_util.cc:34 input_product_scale < output_scale", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7.0\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: 3.0\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n1.down load Quantilized MobileNet model 0.75_224 from [here](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_0.75_224_frozen.tgz\r\n)\r\n\r\n2.Transform the frozen .pb model to .tflite file:\r\n bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=tmp/mobilenet_v1_0.75_224/frozen_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=tmp/mobilenet_v1_0.75_224/mobilenet_v1_0.75_224.tflite --inference_type=QUANTIZED_UINT8  --input_type=QUANTIZED_UINT8  --input_arrays=input --default_ranges_min=0 --default_ranges_max=6 --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3\r\n\r\n3.Change the \"MODEL_PATH\" in Tensorflow lite Android demo to \"mobilenet_v1_0.75_224.tflite\"\r\n\r\n### logs\r\n4.Then run the demo, make the error:\r\nFATAL EXCEPTION: CameraBackground\r\n                                                                                      Process: android.example.com.tflitecamerademo, PID: 13909\r\n                                                                                      java.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/kernel_util.cc:34 input_product_scale < output_scale was not true.\r\n                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)\r\n                                                                                          at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)\r\n                                                                                          at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)\r\n                                                                                          at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:109)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Camera2BasicFragment.java)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:558)\r\n                                                                                          at android.os.Handler.handleCallback(Handler.java:739)\r\n                                                                                          at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                                          at android.os.Looper.loop(Looper.java:135)\r\n                                                                                          at android.os.HandlerThread.run(HandlerThread.java:61)\r\n\r\nneed your help?\r\n\r\nFollowed the same steps , transform other mobilenet models, only the  \"mobilenet_v1_1.0_128\" can run successfully.\r\n", "comments": ["Thanks for the detailed description of the error.\r\n\r\nUnfortunately the quantization method used in the models you downloaded are currently incompatible with TF Lite's quantization. (Details: the weights were quantized, and a Dequantize op was introduced post-training, as opposed to retrained the model with quantization in mind). This is why the tensors lack min/max information.\r\n\r\nTo make this work you need to use pass:\r\n  '--default_ranges_min=0' '--default_ranges_max=6' '--std_values=128' '--mean_values=128'\r\nbut bear in mind that this will noticeably affect the accuracy of the model. The alternative is to use the corresponding float model and alter the demo app to provide floating-point input.", "@andrehentz Thanks for you detailed explanation.  \r\nSo, if I retrained the models with quantization, the error will be solved?\r\nMoreover,why add '--std_values=128' '--mean_values=128' will let it work? My test mobilenet model is mobilenet_v1_0.75_224?\r\n", "This appears on iOS as well (do you want a different bug issue?) -stack:  \r\n```\r\n/Library/Developer/CoreSimulator/Devices/10888914-22EB-4CA7-B019-F95D5A8A6F5C/data/Containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model resolved reportertensorflow/contrib/lite/kernels/kernel_util.cc:34 input_product_scale < output_scale was not true.Failed to allocate tensors!(lldb) \r\n```\r\n  \r\nthis also appears with the mentioned retraining parameters:  \r\n`'--default_ranges_min=0' '--default_ranges_max=6' '--std_values=128' '--mean_values=128'`", "No need for a different issue for iOS since this is platform-independent.\r\n\r\nThe conversion tool uses std_value and mean_value to map raw input values into real values:\r\n    real_value = (raw_input_value - mean_value) / std_value\r\nThe default values of 0 and 1 yield real_value = raw_input_value, which in conjunction with -\r\n inference_type=QUANTIZED_UINT8 means the input will range from 0.0 to 255.0, while the output will use the default ranges or 0.0 to 6.0.\r\n\r\nBy passing '--std_values=128' '--mean_values=128'  you are telling the conversion tool that the real input values are between -1.0 and 1.0 (for the corresponding uint8 0 and 255), which satisfies the quantization constraint that input_product_scale < output_scale. (That's the gist of it, the details are slightly  more complex).\r\n\r\nWe are in the process of documenting the details on how to retain a quantized model for TF Lite. Meanwhile, if you are interested there are more details here: https://stackoverflow.com/questions/47463204\r\n", "@andrehentz thanks a lot for your explanation! maybe I misunderstood, but as I read it `--std_values=128' '--mean_values=128` should resolve it, however at my end the error still persists. (I will try again later today.)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Please reopen and add details if this still happens.", "@andrehentz -I tried the above(std_values=128' '--mean_values=128) but still getting the error on ios:\r\n\r\n`` nnapi error: unable to open library libneuralnetworks.so\r\nLoaded model 1resolved reportertensorflow/contrib/lite/kernels/kernel_util.cc:37 input_product_scale < output_scale was not true.\r\nFailed to allocate tensors!2018-06-07 06:40:37.952396+1000 tflite_camera_example[9900:3643432] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /private/var/containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles\r\n2018-06-07 06:40:37.954062+1000 tflite_camera_example[9900:3643432] [MC] Reading from public effective user settings error: unable to open library libneuralnetworks.so\r\nLoaded model 1resolved reportertensorflow/contrib/lite/kernels/kernel_util.cc:37 input_product_scale < output_scale was not true.\r\nFailed to allocate tensors!2018-06-07 06:40:37.952396+1000 tflite_camera_example[9900:3643432] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /private/var/containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles\r\n2018-06-07 06:40:37.954062+1000 tflite_camera_example[9900:3643432] [MC] Reading from public effective user settings.\r\n\r\n@derdav3 -Were you able to resolve this for ios?"]}, {"number": 14641, "title": "Incorrect second derivative for softmax cross entropy", "body": "\r\n### System information\r\n-  I've written custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu==1.2.1\r\n- python version: 2.7.12\r\n- **CUDA/cuDNN version**: CUDA version: 8.0 and CUDNN version is: 8.0\r\n- **GPU model and memory**: GeForce GTX 750 2GB\r\n\r\nIncorrect second derivative for softmax cross entropy\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\n\r\ntfe.enable_eager_execution()\r\n\r\nlogits = [0.5, 0.5]\r\ny = [1, 0]\r\n\r\ndef loss_function(x):\r\n    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)\r\n    return loss2\r\n\r\ngrad_loss = tfe.gradients_function(loss_function)\r\nprint grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]\r\n\r\ngradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0])\r\nprint gradgrad_loss(logits)[0] # this should be [-0.25,  0.25], but it prints [0. 0.]\r\n```", "comments": ["@ebrevdo would you PTAL?", "cc @alextp @asimshankar", "I think this is working as intended. You're differentiating a vector-valued function and we differentiate only scalar-valued functions.\r\n\r\nWhen we build a scalar function out of a vector function to differentiate we reduce_sum the vector and so you are computing the second gradient of the sum of the components of the first gradient (so the derivative of the divergence of the softmax, if you will) which is 0.", "@alextp Hi, Thank you for your response. Do you have any suggestions how I can find second derivative for such kind of function (e.g. softmax crossentropy with logits)?", "You can implement the function yourself using primitive TF ops; then second\nderivatives will work.\n\nOn Sat, Nov 18, 2017 at 9:30 PM, Arman Zharmagambetov <\nnotifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Hi, Thank you for your response. Do\n> you have any suggestions how I can find second derivative for such kind of\n> function (e.g. softmax crossentropy with logits)?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14641#issuecomment-345493360>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_4yg8d3iL48lZIPuIwx7dZml6cZks5s37zYgaJpZM4QhZfY>\n> .\n>\n", "You can try this. It computes the hessian matrix for softmax cross entropy. Though it may be not efficient enough.\r\n```Python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\n\r\ntfe.enable_eager_execution()\r\n\r\nlogits = [0.5, 0.5]\r\ny = [1, 0]\r\n\r\ndef loss_function(*x):\r\n    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)\r\n    return loss2\r\n\r\ngrad_loss = tfe.gradients_function(loss_function)\r\nfor i in range(len(logits)):\r\n    print(tfe.gradients_function(lambda x: grad_loss(*x)[i])(logits))\r\n```", "@ebrevdo What do you mean by primitive TF ops? I've trie to use the following, but still it doesn't work :\r\n```\r\nlogits = [0.5, 0.5]\r\ny = [1, 0]\r\n\r\ndef loss_function_v2(x):\r\n    loss2 = tf.reduce_mean(-tf.multiply(y, tf.log(tf.nn.softmax(x))))\r\n    return loss2\r\n\r\ngrad_loss = tfe.gradients_function(loss_function_v2)\r\nprint grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]\r\n\r\ngradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0])\r\nprint gradgrad_loss(logits)[0] # this should be [-0.25,  0.25], but it prints [0. 0.]\r\n```\r\n", "@armanform what you could do to your code easily is look at the derivative of the first dimension of the gradient of the loss with respect to the logits, which gives you the numbers you were thinking of:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\n\r\ntfe.enable_eager_execution()\r\n\r\nlogits = [0.5, 0.5]\r\ny = [1, 0]\r\n\r\ndef loss_function(x):\r\n    loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=x)\r\n    return loss2\r\n\r\ngrad_loss = tfe.gradients_function(loss_function)\r\nprint grad_loss(logits)[0] # prints correct gradient [-0.5 0.5]\r\n\r\ngradgrad_loss = tfe.gradients_function(lambda x: grad_loss(x)[0][0])  # this should be +-0.25\r\n```", "@qmick  @alextp  Thank you! One more question. Is is possible to obtain only diagonal entries of hessian?", "We don't have a way to make it efficient to differentiate with respect to slices of a tensor so the easiest way to get the diagonal entries only is to compute the whole thing and slice it.\r\n\r\nYou could maybe start with individual scalar variables, stack them into a vector, and then you'll be able to get second derivatives with respect to them only.", "For example for VGG16 architecture I have to create 138M scalar variables? :)", "Yeah that won't be practical.", "@alextp Hi! I'm just wondering, are tf developers planning to implement diagonal Hessian implementation (either in eager mode or in tensorflow)? Thanks!", "I don't have such a plan yet. I'd accept contributions.\n\nOn Fri, Jan 5, 2018 at 3:20 PM, Arman Zharmagambetov <\nnotifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Hi! I'm just wondering, are tf\n> developers planning to implement diagonal Hessian implementation (either in\n> eager mode or in tensorflow)? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14641#issuecomment-355691540>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXcF5SLLH2lhswkC5sExdYNiaVZPks5tHq5UgaJpZM4QhZfY>\n> .\n>\n\n\n\n-- \n - Alex\n"]}, {"number": 14640, "title": "Fix up link to ios.md in docs.", "body": "Fix up the broken link so it points at the ios docs correctly instead of 404", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14639, "title": "Branch 176031889", "body": "", "comments": []}, {"number": 14638, "title": "Add LICENSES to gitignore for iOS example", "body": "Update gitignore file for ios to cover the license files that get\r\ninstalled following the install instructions.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 14637, "title": "macOS doesn't have wget", "body": "Make building easier on macOS by using curl instead of wget\r\nbecause wget isn't part of macOS, and wget installed from\r\nbrew has trouble resolving certificates appropriately.", "comments": ["Can one of the admins verify this patch?", "Thanks @dmaclach! Do you mind also resolving the conflicts?", "@dmaclach any luck with pull rebase and push?", "ping @dmaclach? ", "Looks like the issue was already fixed for contrib/lite/download_dependencies.\r\nreverted changes to that file, and we can either move forward with this PR, or just copy the changes under contrib/lite to contrib makefile."]}]