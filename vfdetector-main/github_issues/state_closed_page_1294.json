[{"number": 14296, "title": "Add support for grayscale bmp image", "body": "This fix tries to address the issue raised in #13942 to support grayscale bmp image. Previously only channels of 3 or 4 are supported in bmp decoding. This fix adds the support to have 1 channel grayscale image.\r\n\r\nThis fix fixes #13942.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "It looks like tests passed as well.\r\nOne is the timeout error of \r\n```\r\n//tensorflow/python/kernel_tests:slice_op_test\r\n```\r\nand `Ubuntu contrib`/`MacOs Contrib` error. It looks like they are very similar to the error in https://github.com/tensorflow/tensorflow/pull/13242#issuecomment-343700709\r\n\r\nMaybe they are unrelated as well?", "Thanks! I'm rerunning the GPU tests here: http://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/7497/\r\n\r\nThe slide_op_test is indeed unrelated.", "Passed. Thanks!"]}, {"number": 14295, "title": "Documentation: Java Tutorials", "body": "There is only one example for the Java API `LabelImage.java` that is also outdated. It would be great to add more examples for different tasks like text classification, sentence matching, seq2seq, etc.\r\n\r\nI have a small example that put the java api all together. See [tensorflow-java](https://github.com/loretoparisi/tensorflow-java)", "comments": ["@asimshankar how about more examples in Java tutorials?", "@bignamehyp That would be very helpful, thank you! \ud83d\udcaf ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@bignamehyp @asimshankar any news about providing more examples within the Java API tutorials? Thanks.", "Hi, I wish more examples in Java but also tutorials too to learn the tensorflow workflow in Java. Thanks", "Thanks very much for the interest. Unfortunately, at this time we don't have the bandwidth to invest in detailed tutorials, we would appreciate any community support (blogs/articles/examples repositories etc.).\r\n\r\nFor now, there are a few samples in: https://github.com/tensorflow/models/tree/master/samples/languages/java \r\n\r\nMore examples and/or documentation will be added there on a best effort basis. But I'm going to close this out, since in all honesty, no TensorFlow maintainer is actively looking into more detailed documentation or tutorials right now.", "@asimshankar ok thanks, not sure then why are you closing then.", "Hi guys,\r\nI implemented some example application using TensorFlow Java API with YOLOv2 as model and Gradle as build and dependency management tool. You can see my example here: https://github.com/szaza/tensorflow-example-java.\r\nThere is one more application where I implemented an image recognition web service by using Spring Framework, you can see the source code here: https://github.com/szaza/tensorflow-java-examples-spring.\r\nA live demo is available on the Google Cloud here: http://35.231.120.117:8080/"]}, {"number": 14294, "title": "Compilation progress indicator is useless", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: `master` from around now\r\n- **Python version**:  -\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM 9.0.0\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: Any compilation, e.g. `bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nWhenever I compile Tensorflow, Bazel shows a nice progress indicator, for example:\r\n\r\n    [1,428 / 1,437] Compiling tensorflow/core/kernels/cast_op_impl_int64.cc\r\n\r\nHowever that progress proceeds something like this:\r\n\r\n    [4 / 15]\r\n    [20 / 26]\r\n    ...\r\n    [264 / 275]\r\n    ...\r\n    [1,821 / 1,832]\r\n    ...\r\n    [3,456 / 3,470]\r\n\r\nPretty useless. Compiling Tensorflow takes about 45 minutes on my system and it seems to want to do a complete recompile at the drop of a hat so it would be useful at least if I know whether I should wait for it or go and cook dinner. Is there anything that could be done to improve this?", "comments": ["I agree it could be better. You can try filing a bug with the [Bazel project](https://github.com/bazelbuild/bazel), but this would be a tough nut to crack and I'm not sure how much help they would be able to provide.\r\n\r\nThe reason why that number grows over time is because Bazel starts building as it's traversing the build graph. It sort of discovers as it keeps going how much other stuff it needs to build. Each individual item could build quickly, like a .c file, or take a long time, like a complicated genrule. Whether or not they add time to build depends on whether or not they're in a critical path. So it would probably be very hard to predict the time it'll take.\r\n\r\nHowever we know from experience that TensorFlow takes a long time to build. The main reason why that's the case is because we pass a lot of `-msystem` flags to the compiler which adds a `stat()` call for each `#include`, and C/C++ includes are quadratic. Those stat calls are at least half the compiler wall time IIRC. I've done some work in the past to remove `cc_library.includes` attributes wherever possible. In order to go further, I would probably need to do either:\r\n\r\n1. Enhance Bazel's downloader to be able to regex transform C++ sources so includes can be relative to the repository root.\r\n2. Break apart the `tensorflow/core/BUILD` targets, e.g. `lib_internal` into more fine-grained targets that depend on fewer third party libraries.\r\n\r\nBoth of which would require a big time investment.\r\n\r\nIdeally, the C++ compiler would allow us to pass a file mapping includes to a specific location on disk, so we wouldn't need all these stat() calls. But compiler authors have never made this functionality available and I'm not sure why. Maybe it's because everyone is waiting for modules in C++20.\r\n\r\nEither way, we appreciate the feedback and wish we could do more.", "I don't think this particular issue was about a long build time so much as a lousy progress indicator.\r\n\r\nBut yes, this is a Bazel problem."]}, {"number": 14293, "title": "external symbol unresolved external symbol VS2015", "body": "I have one error while trying to build my program:\r\n\r\n### System information\r\nWindows 10 x64 bit\r\nTensorFlow version 1.3\r\nPython version 3.5\r\ncmake\r\nvisual studio 2015\r\n\r\n- **Additional Dependencies**:\r\nzlib\\install\\lib\\zlibstatic.lib\r\ngif\\install\\lib\\giflib.lib\r\npng\\install\\lib\\libpng12_static.lib\r\njpeg\\install\\lib\\libjpeg.lib\r\nlmdb\\install\\lib\\lmdb.lib\r\njsoncpp\\src\\jsoncpp\\src\\lib_json\\Release\\jsoncpp.lib\r\nfarmhash\\install\\lib\\farmhash.lib\r\nfft2d\\\\src\\lib\\fft2d.lib\r\nhighwayhash\\install\\lib\\highwayhash.lib\r\nlibprotobuf.lib\r\ntf_protos_cc.lib\r\ntf_cc.lib\r\ntf_cc_ops.lib\r\ntf_cc_framework.lib\r\ntf_core_cpu.lib\r\ntf_core_direct_session.lib\r\ntf_core_framework.lib\r\ntf_core_kernels.lib\r\ntf_core_lib.lib\r\ntf_core_ops.dir\\Release\\tf_core_ops.lib\r\nnsync\\src\\nsync\\Release\\nsync.lib\r\nsqlite\\src\\sqlite-build\\Release\\sqlite.lib\r\nsnappy\\src\\snappy\\Release\\snappy.lib\r\n\r\n- **Error**:\r\n`Severity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\nError\tLNK2019\tunresolved external symbol \"class tensorflow::Status __cdecl tensorflow::ops::BuildWhileLoop(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::function<class tensorflow::Status __cdecl(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class tensorflow::Output *)> const &,class std::function<class tensorflow::Status __cdecl(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *)> const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *,bool,class tensorflow::Output *)\" (?BuildWhileLoop@ops@tensorflow@@YA?AVStatus@2@AEBVScope@2@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@std@@AEBV?$function@$$A6A?AVStatus@tensorflow@@AEBVScope@2@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@std@@PEAVOutput@2@@Z@6@AEBV?$function@$$A6A?AVStatus@tensorflow@@AEBVScope@2@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@std@@PEAV45@@Z@6@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@6@PEAV56@_NPEAVOutput@2@@Z) referenced in function \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::AddBackPropLoopCounter(class tensorflow::WhileContext *,class tensorflow::Output const &,class tensorflow::Scope const &,class tensorflow::Output *)\" (?AddBackPropLoopCounter@?A0xb5093d1e@tensorflow@@YA?AVStatus@2@PEAVWhileContext@2@AEBVOutput@2@AEBVScope@2@PEAV52@@Z)\ttensorflow_test2\tC:\\Users\\mcuevas\\Documents\\Visual Studio 2015\\Projects\\tensorflow_test2\\tensorflow_test2\\tf_cc.lib(while_gradients.obj)\t1\t\r\n`", "comments": ["That symbol is defined in `tf_cc_while_loop.lib`. Can you try linking that into your project?"]}, {"number": 14292, "title": "Can't import contrib.boosted_trees", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n== cat /etc/issue ===============================================                                               \r\nLinux 5508912-0913 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux  \r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"                                                                            \r\nVERSION_ID=\"16.04\"                                                                                              \r\nVERSION_CODENAME=xenial                                                                                         \r\n                                                                                                                \r\n== are we in docker =============================================                                               \r\nNo                                                                                                              \r\n                                                                                                                \r\n== compiler =====================================================                                               \r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609                                                              \r\nCopyright (C) 2015 Free Software Foundation, Inc.                                                               \r\nThis is free software; see the source for copying conditions.  There is NO                                      \r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.                                     \r\n                                                                                                                \r\n                                                                                                                \r\n== uname -a =====================================================                                               \r\nLinux 5508912-0913 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux  \r\n                                                                                                                \r\n== check pips ===================================================                                               \r\nnumpy (1.13.3)                                                                                                  \r\nprotobuf (3.4.0)                                                                                                \r\ntensorflow (1.4.0)                                                                                              \r\ntensorflow-tensorboard (0.4.0rc1)                                                                               \r\n                                                                                                                \r\n== check for virtualenv =========================================                                               \r\nFalse                                                                                                           \r\n                                                                                                                \r\n== tensorflow import ============================================                                               \r\ntf.VERSION = 1.4.0                                                                                              \r\ntf.GIT_VERSION = v1.4.0-0-gd752244                                                                              \r\ntf.COMPILER_VERSION = v1.4.0-0-gd752244                                                                         \r\nSanity check: array([1], dtype=int32)                                                                           \r\n                                                                                                                \r\n== env ==========================================================                                               \r\nLD_LIBRARY_PATH is unset                                                                                        \r\nDYLD_LIBRARY_PATH is unset                                                                                      \r\n                                                                                                                \r\n== nvidia-smi ===================================================                                               \r\n../../tf_env_collect.sh: line 105: nvidia-smi: command not found                                                \r\n                                                                                                                \r\n== cuda libs  ===================================================                                               \r\n\r\n### Describe the problem\r\nCan't import the boosted_trees module.\r\nBoosted_trees isn't properly listed in contrib/__init__.py, so I get:\r\n>>> import tensorflow as tf\r\n>>> est = tf.contrib.boosted_trees.estimator_batch.estimator.GradientBoostedDecisionTreeClassifier()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py\", line 54, in __getattr__\r\n    return getattr(module, item)\r\nAttributeError: 'module' object has no attribute 'boosted_trees'\r\n\r\n\r\n### Source code / logs\r\nSee above.\r\n", "comments": ["@yk5 @tkoeppe Any particular reason why boosted_trees wouldn't be listed in `contrib/__init__.py`?\r\n", "ask thomaswc@, nponomoreva@, or soroush@.  There may still be some loose ends left in the migration from learning/lib to third_party.", "You can import the classifier the way in the [examples](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/examples/boston.py#L42) like:\r\n\r\n`from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor`\r\n\r\nBut I'm asking the team about whether there's any reason of being made not accessible directly.", "Understood.  It's worth pointing out that using the 'nightly-gpu' container from the [docker hub](https://hub.docker.com/r/tensorflow/tensorflow/tags/), I can't import the GBDTClassifier or the estimator python module:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/azfileshare/scripts/train.py\", line 9, in <module>\r\n    import models\r\n  File \"/mnt/batch/tasks/shared/LS_root/mounts/azfileshare/scripts/models.py\", line 4, in <module>\r\n    from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeClassifier\r\nImportError: No module named estimator\r\n\r\nUsing the '1.4.0-rc1' image works fine. I don't know if this is indicative of a problem introduced since the 1.4.0-rc1 image was cooked or not.  It says it was updated 4 days ago. ", "I don't have proper GPU/docker settings, so cannot test out that nightly, but can you try 1.4.0-gpu instead of rc1?\r\n1.4.0 is released last week, and the import works fine with non-gpu pip installation (not the docker).", "I think the issue is that with version 1.5 some of the python files are not included in the release. We have to investigate and add the proper dependencies. (Probably from boosted_trees:init_py to estimator_batch/estimator). \r\nI just tried the nightly docker image and verified that in estimator_batch directory it only has:\r\n`__init__.py  __init__.pyc  custom_export_strategy.py  custom_export_strategy.pyc  trainer_hooks.py  trainer_hooks.pyc`\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Well, I wrote a response but it seems lost so writing again (please forgive if there appears a dupe).\r\n\r\nI confirmed that the command I put on November 6 works for both tensorflow-1.4.1 and tensorflow-gpu-1.4.1, which are the latest versions.\r\n`from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor`\r\n\r\nMaybe the nightly would have been funky at the moment as I mentioned.\r\nClosing the issue.", "That wasn't what was asked.  I wanted:\r\nimport tensorflow as tf\r\nest = tf.contrib.boosted_trees.estimator_batch.estimator.GradientBoostedDecisionTreeRegressor()\r\n\r\nI realize there isn't a big difference, it's just more consistent to not have to say:\r\n1. Ok, I can import other estimators with tf.contrib....\r\n2. Huh, boosted_trees isn't found in tf.contrib, I wonder why\r\n3. <mucks around for a while>\r\n4. Oh, now I realize that I have to explicitly import it, unlike all the other estimators.", "Yes, my last comment is to your response (Nov 6 9:51AM PST) : \"in\nnightly-gpu, I can't even explicitly import it\" which sounds like a big\nissue.\nI confirmed that it's not the issue of the released tensorflow-gpu (nor\ntensorflow) package.\n\nFor another issue you raised, many contrib libraries are not preloaded,\nwhich is obvious for stability, and boosted_trees is one of them.\nThe reason of excluding boosted_trees had been Mac test failures, as I\nknow. I think the failures are resolved but it might take time to put it to\nthe list of preloaded packages.\n\nIf you want to keep the issue open for that reason, that's fine, and the\nissue becomes\n \"a feature request to include contrib.boosted_trees as a preloaded\npackage\".\n\n\nOn Fri, Jan 19, 2018 at 11:53 AM Gilbert Hendry <notifications@github.com>\nwrote:\n\n> That wasn't what was asked. I wanted:\n> import tensorflow as tf\n> est =\n> tf.contrib.boosted_trees.estimator_batch.estimator.GradientBoostedDecisionTreeRegressor()\n>\n> I realize there isn't a big difference, it's just more consistent to not\n> have to say:\n>\n>    1. Ok, I can import other estimators with tf.contrib....\n>    2. Huh, boosted_trees isn't found in tf.contrib, I wonder why\n>    3.\n>    4. Oh, now I realize that I have to explicitly import it, unlike all\n>    the other estimators.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14292#issuecomment-359071188>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AXihKvVL2BVT2eoi4gpLlhJabO9YiwIfks5tMPKvgaJpZM4QTYbC>\n> .\n>\n", "the command \"from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor\" also couldn't work in Windows.\r\n\r\nit raised error: \r\nImportError: No module named 'tensorflow.contrib.boosted_trees.python.training'\r\n\r\ncould it be fixed?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "> the command \"from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor\" also couldn't work in Windows.\r\n\r\neven using ubuntu `bash` on windows doesn't work", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@sshrdp Soroush, I found that the problem got worse since 1.5.0.\r\nNot even Linux python 2.7 can import boosted_trees.\r\nI installed 1.4.1 and 1.5.0 in different virtualenv, and checked the diffs.\r\n$ (cd tf-1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch && ls *.py)\r\ncustom_export_strategy.py  custom_loss_head.py  estimator.py  __init__.py  model.py  trainer_hooks.py\r\n$ (cd tf-1.5/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch && ls *.py)\r\ncustom_export_strategy.py  __init__.py  trainer_hooks.py\r\n\r\nMany files are taken out in tf-1.5.0, even though I cannot find significant changes regarding BUILD or __init__.py under boosted_trees between the two versions.\r\n\r\nCould it be due to some BUILD file changes like [this commit](https://github.com/tensorflow/tensorflow/commit/66b1615b6e2783c9ddce27e7b084fcc230c3a594#diff-133d898664bdd5d74926bf66659adb53) or [another](https://github.com/tensorflow/tensorflow/commit/966016b7f2382658e7c84baae0596d35f0a49bae#diff-133d898664bdd5d74926bf66659adb53)..?\r\n\r\n", "@yk5: Yes indeed, the removal of the deps from `init_py` had the effect of removing the files from the PIP package. I think you can restore functionality by adding them back in (in `contrib/boosted_trees/estimator_batch/BUILD`):\r\n\r\n```\r\npy_library(                                                                                                                                                                                                        \r\n    name = \"init_py\",                                                                                                                                                                                              \r\n    srcs = [\"__init__.py\"],                                                                                                                                                                                        \r\n    srcs_version = \"PY2AND3\",                                                                                                                                                                                      \r\n    deps = [                                                                                                                                                                                                       \r\n        \"custom_export_strategy\",                                                                                                                                                                                  \r\n        \":custom_loss_head\",                                                                                                                                                                                       \r\n        \":estimator\",                                                                                                                                                                                              \r\n        \":model\",                                                                                                                                                                                                  \r\n        \":trainer_hooks\",                                                                                                                                                                                          \r\n    ],                                                                                                                                                                                                             \r\n)\r\n```", "@ghendrymsft: Could you perhaps try to add `//tensorflow/contrib/boosted_trees/estimator_batch:init_py` to the dependencies of `//tensorflow/contrib:contrib_py`? (On top of fixing up what I said in the previous comment, which are already committed at head.)", "I'm away this week, but will continue to investigate on it later.\nAt this moment, other platforms should be okay (confirmed on linux with\nnightly), but I suspect cmake as the culprit for windows problem.\n\nOn Tue, Apr 3, 2018, 6:58 PM Rohan Jain <notifications@github.com> wrote:\n\n> Assigned #14292 <https://github.com/tensorflow/tensorflow/issues/14292>\n> to @yk5 <https://github.com/yk5>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14292#event-1555452971>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AXihKljaQ9d8qO2sTNbwvDOYNvNtgLZ7ks5tlCjXgaJpZM4QTYbC>\n> .\n>\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "any work around?I need this GradientBoostedDecisionTreeClassifier and could not import it.\r\nps. I am using tensorflow 1.5.0", "It should be back in 1.8.0 for Linux/Mac, and you can test it out with 1.8.0rc1.\r\n\r\nFor Windows, we have some package problem, but it's not easy to find what's the cause. We placed several patches but not successful. I have another, hopefully would more likely fix it this time.", "Despite the latest fix, I'm afraid TF on Windows will get contrib.boosted_trees in near future, after looking at the CMake documentation.\r\n[Known limitation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake#current-known-limitations-1) section explains that CMake doesn't support tf.load_op_library(), which we use to load custom ops for contrib.boosted_trees ([link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/python/ops/boosted_trees_ops_loader.py#L27))\r\nUntil that functionality is implemented in CMake, it might not be possible to load contrib.boosted_trees packages properly due to lack of ops.\r\n\r\nOTOH, we started to migrate contrib boosted_trees into core, and v0 implementation is pushed to 1.8.0 (e.g. [`BoostedTreesClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier)), so try it out.\r\n\r\nCurrent limitation is that it only accepts bucketized_columns, however other features are being actively developed.\r\n"]}, {"number": 14291, "title": "map_fn not working with string tensor on Android", "body": "It seems that the tensorflow Android version (1.4.0) does not include the kernel for TensorArrayScatterV3 for string tensors (see stacktrace below). This leads to the problem that I cannot use map_fn with string tensors on Android. \r\n\r\nThe same code and model runs fine with the desktop Java API in the same version. \r\n\r\nCan you please add the missing kernel?\r\n\r\nStacktrace\r\n\r\n```\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'TensorArrayScatterV3' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err:   device='CPU'; T in [DT_BOOL]\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err:   device='CPU'; T in [DT_FLOAT]\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err:   device='CPU'; T in [DT_INT32]\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err: \t [[Node: map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 = TensorArrayScatterV3[T=DT_STRING, _class=[\"loc:@image_strings\"]](map/TensorArray, map/TensorArrayUnstack/range, image_strings, map/TensorArray:1)]]\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err:     at java.util.concurrent.FutureTask.report(FutureTask.java:94)\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err:     at java.util.concurrent.FutureTask.get(FutureTask.java:164)\r\n11-06 15:14:45.760 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.neuronalnetwork.service.TaskWorkerLoop$Loop.run(TaskWorkerLoop.java:71)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at java.lang.Thread.run(Thread.java:762)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err: Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'TensorArrayScatterV3' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:   device='CPU'; T in [DT_BOOL]\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:   device='CPU'; T in [DT_FLOAT]\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:   device='CPU'; T in [DT_INT32]\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err: \t [[Node: map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 = TensorArrayScatterV3[T=DT_STRING, _class=[\"loc:@image_strings\"]](map/TensorArray, map/TensorArrayUnstack/range, image_strings, map/TensorArray:1)]]\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at org.tensorflow.Session.run(Native Method)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at org.tensorflow.Session.access$100(Session.java:48)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at org.tensorflow.Session$Runner.run(Session.java:248)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.brain.Api.run(Api.java:39)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.client.api.Evaluator.evaluateBatch(Evaluator.java:66)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.client.api.Evaluator.evaluate(Evaluator.java:54)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.client.api.EvaluationService.evaluateNN(EvaluationService.java:114)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.client.api.EvaluationService.evaluateSections(EvaluationService.java:73)\r\n11-06 15:14:45.761 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.neuronalnetwork.service.engine.NetworkEngine.lambda$calculate$0(NetworkEngine.java:59)\r\n11-06 15:14:45.762 32605-1644/de.test.android.fisheye.local W/System.err:     at de.test.neuronalnetwork.service.engine.NetworkEngine$$Lambda$1.call(Unknown Source)\r\n11-06 15:14:45.762 32605-1644/de.test.android.fisheye.local W/System.err:     at java.util.concurrent.FutureTask.run(FutureTask.java:237)\r\n11-06 15:14:45.762 32605-1644/de.test.android.fisheye.local W/System.err:     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:428)\r\n11-06 15:14:45.762 32605-1644/de.test.android.fisheye.local W/System.err:     at java.util.concurrent.FutureTask.run(FutureTask.java:237)\r\n11-06 15:14:45.762 32605-1644/de.test.android.fisheye.local W/System.err:     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1133)\r\n11-06 15:14:45.762 32605-1644/de.test.android.fisheye.local W/System.err:     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:607)\r\n11-06 15:14:45.762 32605-1644/de.test.android.fisheye.local W/System.err: \t... 1 more\r\n```", "comments": ["@petewarden Can you take a look at this?", "any news on this?", "@andrewharp can you comment on this?", "I have same problem also, is there any progress? \r\n@andreas-eberle did you find a solution? or a workaround?", "@pogamar: No, unfortunately not.", "I am also facing this issue while trying to run a model on mobile.  @bignamehyp any updates on when this will be fixed ? or any workarounds?", "@andrewharp can you comment on this?\r\n\r\n", "Nagging Assignee @andrewharp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Apologies for the delay, I'll take a look, thanks for your patience.", "If you need to support string types, you should make sure that you've built TensorFlow(Mobile) with the __ANDROID_TYPES_FULL__ build define (`--copt=-D__ANDROID_TYPES_FULL__`). See also [register_types.h](https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/framework/register_types.h#L89). Otherwise, building for Android defaults to only supporting float, int and bool types."]}, {"number": 14290, "title": "Adding connectivity check, compilation fix and some code refactoring to verbs ", "body": "1. Connectivity check - checking the correctness of the RDMA configuration parameters by pinging on each channel.\r\n2. Compilation with verbs and without CUDA fix (contrib/verbs only works on GPU #13466)\r\n3. Code refactoring: \r\n  Call done in case of not OK status fix  \r\n  Replace hardcoded 100 with RDMA_QP_QUEUE_DEPTH ", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "This PR fixes the #13466 (contrib/verbs only works on GPU) issue.\r\n", "@junshi can you please review?", "@noaezra , can you confirm here that you're ok with your commit being submitted?", "I'm okay with this change, but I don't have a well-informed opinion as to how appropriate it is.  @junshi15 should give his opinion.  Is there a background thread other than the cited bug discussing this change?", "@dariavel The patch looks good, except that I have one question above.", "@jhseu # \"noaezra , can you confirm here that you're ok with your commit being submitted?\"\r\nIt's ok that my commit is being submitted.", "@jhseu @poxvoculi @junshi15\r\ncan you please re-run the tests?\r\n", "Jenkins, test this please.", "Thanks @tatatodd . looks like the failure is not related to the PR.\r\nCan we merge or re-run ? ", "Jenkins, test this please."]}, {"number": 14289, "title": "Is something wrong about slim.learning.train?", "body": "[source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py)\r\n\r\nMy code is so complex and very very long, so I just describe my code logic.\r\n\r\nI add two parameters `run_tensor` and `run_placeholder` to `slim.learning.train`, the `run_tensor` is the tensor I want to run and the `run_placeholder` is placeholder I need to feed. And I also add two parameters `run_tensor` and `run_placeholder` to [`train_step`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L456), and `train_step` is passed to `slim.learning.train`'s `train_step_fn`. So every training step, I can run `run_tensor` in the  `train_step`  function, and I'm sure I have fed the `run_placeholder`, but when I run the program, it raise an error, like:\r\n\r\n```\r\nInvalid argument: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape\r\n[...]\r\n``` \r\n\r\n~~I debug for long time, and find it may be the [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L779)~~ \r\n\r\n~~I have updated tensorflow to 1.4.0, but I  find my `slim.learning.train` only use `sv.stop(threads, close_summary_writer=True)`, without `ignore_live_threads=ignore_live_threads`.~~\r\n\r\nI find what's wrong, it is `tf.summary.scalar`, slim doesn't run `summary` op with feed_dict.\r\nMaybe I think slim need to support feed_dict on every training step.\r\n", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 14288, "title": "Fix XLA compilation on OSX", "body": "The double versions of these functions are overloaded on OSX which means we need an explicit cast to disambiguate them. Fixes #14127.\r\n\r\nFor compilation to actually succeed, PR #14137 needs to be merged too.", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Is there any way to tell your author consent system \"commits from Timmmm may also be authored by this other email address...\"? I'd rather not have to create a separate work Github account.", "should I merge this into https://github.com/tensorflow/tensorflow/pull/14137?\r\n", "Hello @google-admin @googlebot .\r\n\r\nTim is on our list of contributors (both his github email address and his graphcore.ai address).  Could you let us know which address the googlebot CA system is objecting to, so we can investigate further.   \r\n\r\nDoes the googlebot work on email address, or display name, for entries in the googlegroup?\r\n\r\n", "ok - having read the detail of the error message, I can see that it is saying that both Tim's github account and graphcore account are listed in the group.  The issue is that you need Tim to confirm that his graphcore account is ok for submissions to the TF project.  I'll let him confirm that...\r\n", "I was running into \r\nUndefined symbols for architecture x86_64:\r\n  \"___xla_cpu_runtime_ExpV4F32NEON\", referenced from:\r\n      xla::cpu::SimpleOrcJIT::SimpleOrcJIT(llvm::TargetOptions const&, llvm::CodeGenOpt::Level, bool, bool, bool, std::__1::function<tensorflow::Status (llvm::Module const&)>, std::__1::function<tensorflow::Status (llvm::Module const&)>) in libsimple_orc_jit.a(simple_orc_jit.o)\r\n      xla::cpu::(anonymous namespace)::RegisterKnownJITSymbols() in libsimple_orc_jit.a(simple_orc_jit.o)\r\n  \"___xla_cpu_runtime_ExpV8F32AVX\", referenced from:\r\n      xla::cpu::SimpleOrcJIT::SimpleOrcJIT(llvm::TargetOptions const&, llvm::CodeGenOpt::Level, bool, bool, bool, std::__1::function<tensorflow::Status (llvm::Module const&)>, std::__1::function<tensorflow::Status (llvm::Module const&)>) in libsimple_orc_jit.a(simple_orc_jit.o)\r\n      xla::cpu::(anonymous namespace)::RegisterKnownJITSymbols() in libsimple_orc_jit.a(simple_orc_jit.o)\r\n  \"___xla_cpu_runtime_LogV4F32NEON\", referenced from:\r\n      xla::cpu::(anonymous namespace)::RegisterKnownJITSymbols() in libsimple_orc_jit.a(simple_orc_jit.o)\r\n  \"___xla_cpu_runtime_LogV8F32AVX\", referenced from:\r\n      xla::cpu::(anonymous namespace)::RegisterKnownJITSymbols() in libsimple_orc_jit.a(simple_orc_jit.o)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/compiler/aot:tfcompile failed to build\r\n\r\nand had to comment out \r\n\r\n\r\n@@ -102,8 +107,8 @@ llvm::StringRef GetHostCpuName() {\r\n CompilerFunctor::VectorIntrinsics GetAvailableIntrinsics() {\r\n   CompilerFunctor::VectorIntrinsics intrinsics;\r\n   intrinsics.sse_intrinsics = (&__xla_cpu_runtime_ExpV4F32SSE != nullptr);\r\n-  intrinsics.avx_intrinsics = (&__xla_cpu_runtime_ExpV8F32AVX != nullptr);\r\n-  intrinsics.neon_intrinsics = (&__xla_cpu_runtime_ExpV4F32NEON != nullptr);\r\n+  //intrinsics.avx_intrinsics = (&__xla_cpu_runtime_ExpV8F32AVX != nullptr);\r\n+  //intrinsics.neon_intrinsics = (&__xla_cpu_runtime_ExpV4F32NEON != nullptr);\r\n   return intrinsics;\r\n }\r\n \r\nand ..\r\n\r\n   REGISTER_CPU_RUNTIME_SYMBOL(EigenSingleThreadedMatMulF64);\r\n-  REGISTER_CPU_RUNTIME_SYMBOL(ExpV4F32NEON);\r\n+ // REGISTER_CPU_RUNTIME_SYMBOL(ExpV4F32NEON);\r\n   REGISTER_CPU_RUNTIME_SYMBOL(ExpV4F32SSE);\r\n-  REGISTER_CPU_RUNTIME_SYMBOL(ExpV8F32AVX);\r\n-  REGISTER_CPU_RUNTIME_SYMBOL(LogV4F32NEON);\r\n+ // REGISTER_CPU_RUNTIME_SYMBOL(ExpV8F32AVX);\r\n+ // REGISTER_CPU_RUNTIME_SYMBOL(LogV4F32NEON);\r\n   REGISTER_CPU_RUNTIME_SYMBOL(LogV4F32SSE);\r\n-  REGISTER_CPU_RUNTIME_SYMBOL(LogV8F32AVX);\r\n+ // REGISTER_CPU_RUNTIME_SYMBOL(LogV8F32AVX);\r\n\r\n\r\nfor it to be able to compile correctly on my 2015 13\" Macbook pro and 10.13.2 beta and xcode 9.1 (Apple LLVM version 9.0.0 (clang-900.0.38))\r\n\r\n", "I am the other of the commits. I hereby give myself permission to submit this pull request. I guess? :-)", "Cc @martinwicke", "Jenkins, test this please."]}, {"number": 14287, "title": "Add layer scope to tf.contrib.layers.spatial_softmax", "body": "`tensorflow.contrib.layers.spatial_softmax` is lacking a layer-level scope\r\n\r\n| before | after |\r\n|---|---|\r\n|![before](https://user-images.githubusercontent.com/10049601/32437377-6a3f56e4-c2e7-11e7-8e60-270f3e01daf5.png)|![after](https://user-images.githubusercontent.com/10049601/32437379-6f0ea800-c2e7-11e7-9388-18f4c53b450b.png)|", "comments": ["Can one of the admins verify this patch?", "This may break checkpoints by changing variable names (unless I misunderstand). You could use a name_scope instead, that is harmless.", "On one hand, yes, that would break checkpoints. On the other, wouldn't it be a bit against the general behavior of the `layers` api to create variables not in their own scope but in the global one?\r\n\r\nNo problem though if you'd prefer a `name_scope` however. \r\n", "That is true, variable_scope would have been the right thing to do. I am wary of breaking people though, so I'd rather stick with the safer fix. \r\n\r\n@sguada, do you agree with name_scope here? Or should we try for variable_scope.", "Since it creates a variable it should have a variable_scope.", "Are you ok with breaking checkpoints using this? That's what a variable_scope would do. If you're ok with it, I am too.", "Jenkins, test this please."]}, {"number": 14286, "title": "fix InvalidArgumentError when using cv2 with tf.py_func()", "body": "If we use `cv2.imread(filename, cv2.IMREAD_GRAYSCALE)` here, we'll get an `InvalidArgumentError: TypeError: bad argument type for built-in operation`, using `cv2.imread(filename.decode(), cv2.IMREAD_GRAYSCALE)` instead solves this issue.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "I notice that the PR still hasn't been merged, what else should I do?", "No, good to go."]}, {"number": 14285, "title": "TF 1.4.0 on MacOSX: crash, object was probably modified after being freed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13\r\n- **TensorFlow installed from (source or binary)**: binary, via `pip3.6 install tensorflow`\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**:  3.6.3, via Homebrew\r\n\r\n### Describe the problem\r\n\r\nTensorFlow crashes in some cases. This occurred only now with version TF 1.4.0. It is a test of my test suite ([this one](https://github.com/rwth-i6/returnn/blob/4a69d0a1e74fb1ac7f76fc8c27694d906f9a8642/tests/test_TFEngine.py#L459)). I can try to come up with a reduced test case but maybe the current information is already enough to identify the problem.\r\n\r\nOn the terminal, I see this:\r\n```\r\nPython(60770,0x70000fafb000) malloc: *** error for object 0x7fb861518b48: incorrect checksum for freed object - object was probably modified after being freed.\r\n*** set a breakpoint in malloc_error_break to debug\r\nfish: Job 1, 'python3 tests/test_TFEngine.py test_engine_train_simple_attention' terminated by signal SIGABRT (Abort)\r\n```\r\n\r\nThe crashed thread stacktrace:\r\n```\r\nThread 15 Crashed:\r\n0   libsystem_kernel.dylib        \t0x00007fff7c559fce __pthread_kill + 10\r\n1   libsystem_pthread.dylib       \t0x00007fff7c697150 pthread_kill + 333\r\n2   libsystem_c.dylib             \t0x00007fff7c4b632a abort + 127\r\n3   libsystem_malloc.dylib        \t0x00007fff7c5beb28 szone_error + 596\r\n4   libsystem_malloc.dylib        \t0x00007fff7c5c9ea5 tiny_free_no_lock + 2439\r\n5   libsystem_malloc.dylib        \t0x00007fff7c5ca254 free_tiny + 628\r\n6   libtensorflow_framework.so    \t0x00000001091927c2 tensorflow::Tensor::~Tensor() + 50\r\n7   libtensorflow_framework.so    \t0x00000001095d1b2e tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 5646\r\n8   libtensorflow_framework.so    \t0x00000001095d9d90 std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 80\r\n9   libtensorflow_framework.so    \t0x0000000109277fc2 Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 1922\r\n10  libtensorflow_framework.so    \t0x0000000109277734 std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 52\r\n11  libtensorflow_framework.so    \t0x000000010929a9c0 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 96\r\n12  libsystem_pthread.dylib       \t0x00007fff7c6946c1 _pthread_body + 340\r\n13  libsystem_pthread.dylib       \t0x00007fff7c69456d _pthread_start + 377\r\n14  libsystem_pthread.dylib       \t0x00007fff7c693c5d thread_start + 13\r\n```\r\n\r\nAlternatively, I sometimes get this crashed thread stacktrace:\r\n```\r\nThread 12 Crashed:\r\n0   libtensorflow_framework.so    \t0x000000010e9fd5b7 tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const + 71\r\n1   _pywrap_tensorflow_internal.so\t0x0000000108b16932 tensorflow::(anonymous namespace)::CheckNumericsOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*) + 98\r\n2   libtensorflow_framework.so    \t0x000000010ee6d88d tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 301\r\n3   libtensorflow_framework.so    \t0x000000010ee3c82b tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4875\r\n4   libtensorflow_framework.so    \t0x000000010ee44d90 std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 80\r\n5   libtensorflow_framework.so    \t0x000000010eae2fc2 Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 1922\r\n6   libtensorflow_framework.so    \t0x000000010eae2734 std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 52\r\n7   libtensorflow_framework.so    \t0x000000010eb059c0 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 96\r\n8   libsystem_pthread.dylib       \t0x00007fff7c6946c1 _pthread_body + 340\r\n9   libsystem_pthread.dylib       \t0x00007fff7c69456d _pthread_start + 377\r\n10  libsystem_pthread.dylib       \t0x00007fff7c693c5d thread_start + 13\r\n```\r\n\r\nMaybe the main thread stacktrace is also relevant:\r\n```\r\nThread 0:: Dispatch queue: com.apple.main-thread\r\n0   libsystem_kernel.dylib        \t0x00007fff7c559e7e __psynch_cvwait + 10\r\n1   libsystem_pthread.dylib       \t0x00007fff7c695662 _pthread_cond_wait + 732\r\n2   libc++.1.dylib                \t0x00007fff7a449cb0 std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n3   _pywrap_tensorflow_internal.so\t0x000000010dc2d23b nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 363\r\n4   _pywrap_tensorflow_internal.so\t0x000000010dc29c97 nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423\r\n5   _pywrap_tensorflow_internal.so\t0x000000010dc2a3d1 nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49\r\n6   _pywrap_tensorflow_internal.so\t0x000000010dc3771b tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) + 107\r\n7   _pywrap_tensorflow_internal.so\t0x000000010dc332a6 tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) + 38\r\n8   _pywrap_tensorflow_internal.so\t0x000000010dc2fe3e tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor> > > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) + 3438\r\n9   _pywrap_tensorflow_internal.so\t0x000000010bf0827e TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tensorflow::Tensor> > > const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, TF_Tensor**, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, TF_Buffer*, TF_Status*) + 750\r\n10  _pywrap_tensorflow_internal.so\t0x000000010bf07eb6 TF_Run + 1286\r\n11  _pywrap_tensorflow_internal.so\t0x000000010bc990b3 tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) + 1683\r\n12  _pywrap_tensorflow_internal.so\t0x000000010bc997e4 tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) + 52\r\n13  _pywrap_tensorflow_internal.so\t0x000000010bc5b885 _wrap_TF_Run(_object*, _object*) + 1861\r\n14  org.python.python             \t0x000000010797b5dd _PyCFunction_FastCallDict + 166\r\n...\r\n```\r\n\r\nThe full MacOSX crash report with the stacktrace of all threads can be seen [here](https://gist.github.com/albertz/6f92b691d6025f47f8ec3a738a8ba970).\r\n\r\nOn another run, I also got this stacktrace:\r\n```\r\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\r\n\r\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\r\nException Codes:       EXC_I386_GPFLT\r\nException Note:        EXC_CORPSE_NOTIFY\r\n\r\nTermination Signal:    Segmentation fault: 11\r\nTermination Reason:    Namespace SIGNAL, Code 0xb\r\nTerminating Process:   exc handler [0]\r\n\r\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\r\n0   libtensorflow_framework.so    \t0x0000000115d21a1e tensorflow::(anonymous namespace)::AddArgToSig(tensorflow::NodeDef const&, tensorflow::OpDef_ArgDef const&, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*) + 78\r\n1   libtensorflow_framework.so    \t0x0000000115d21942 tensorflow::InOutTypesForNode(tensorflow::NodeDef const&, tensorflow::OpDef const&, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*, tensorflow::gtl::InlinedVector<tensorflow::DataType, 4>*) + 82\r\n2   libtensorflow_framework.so    \t0x0000000115d22829 tensorflow::ValidateNodeDef(tensorflow::NodeDef const&, tensorflow::OpDef const&) + 1881\r\n3   _pywrap_tensorflow_internal.so\t0x0000000110f83e16 tensorflow::graph::ValidateGraphDef(tensorflow::GraphDef const&, tensorflow::OpRegistryInterface const&) + 134\r\n4   _pywrap_tensorflow_internal.so\t0x0000000110eef9c3 tensorflow::GraphExecutionState::Extend(tensorflow::GraphDef const&, std::__1::unique_ptr<tensorflow::GraphExecutionState, std::__1::default_delete<tensorflow::GraphExecutionState> >*) const + 2147\r\n5   _pywrap_tensorflow_internal.so\t0x0000000110d18e03 tensorflow::DirectSession::ExtendLocked(tensorflow::GraphDef const&) + 131\r\n6   _pywrap_tensorflow_internal.so\t0x0000000110d18eb3 tensorflow::DirectSession::Extend(tensorflow::GraphDef const&) + 67\r\n7   _pywrap_tensorflow_internal.so\t0x000000010eff04f6 TF_ExtendGraph + 102\r\n8   _pywrap_tensorflow_internal.so\t0x000000010ed43b71 _wrap_TF_ExtendGraph(_object*, _object*) + 273\r\n9   org.python.python             \t0x000000010d57d5dd _PyCFunction_FastCallDict + 166\r\n...\r\n```\r\n\r\nAnd yet another stacktrace:\r\n```\r\nThread 13 Crashed:\r\n0   _pywrap_tensorflow_internal.so\t0x0000000107a4016a tensorflow::TTypes<int, 3ul, long>::ConstTensor tensorflow::Tensor::bit_casted_shaped<int, 3ul>(tensorflow::gtl::ArraySlice<long long>) const + 42\r\n1   _pywrap_tensorflow_internal.so\t0x0000000107a45a6e void tensorflow::HandleStridedSliceGradCase<Eigen::ThreadPoolDevice, float, 3>(tensorflow::OpKernelContext*, tensorflow::gtl::ArraySlice<long long> const&, tensorflow::gtl::ArraySlice<long long> const&, tensorflow::gtl::ArraySlice<long long> const&, tensorflow::TensorShape const&, bool, tensorflow::Tensor*) + 302\r\n2   _pywrap_tensorflow_internal.so\t0x00000001079ff7ac tensorflow::StridedSliceGradOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*) + 2556\r\n3   libtensorflow_framework.so    \t0x000000010644788d tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 301\r\n4   libtensorflow_framework.so    \t0x000000010641682b tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4875\r\n5   libtensorflow_framework.so    \t0x000000010641ed90 std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 80\r\n6   libtensorflow_framework.so    \t0x00000001060bcfc2 Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 1922\r\n7   libtensorflow_framework.so    \t0x00000001060bc734 std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 52\r\n8   libtensorflow_framework.so    \t0x00000001060df9c0 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 96\r\n9   libsystem_pthread.dylib       \t0x00007fff7c6946c1 _pthread_body + 340\r\n10  libsystem_pthread.dylib       \t0x00007fff7c69456d _pthread_start + 377\r\n11  libsystem_pthread.dylib       \t0x00007fff7c693c5d thread_start + 13\r\n```\r\n\r\nOr this:\r\n```\r\nThread 12 Crashed:\r\n0   libsystem_kernel.dylib        \t0x00007fff7c55a1ea __semwait_signal_nocancel + 10\r\n1   libsystem_c.dylib             \t0x00007fff7c460097 nanosleep$NOCANCEL + 188\r\n2   libsystem_c.dylib             \t0x00007fff7c488931 usleep$NOCANCEL + 53\r\n3   libsystem_c.dylib             \t0x00007fff7c4b6334 abort + 137\r\n4   libsystem_malloc.dylib        \t0x00007fff7c5beb28 szone_error + 596\r\n5   libsystem_malloc.dylib        \t0x00007fff7c5b3658 tiny_malloc_from_free_list + 1155\r\n6   libsystem_malloc.dylib        \t0x00007fff7c5b2403 szone_malloc_should_clear + 422\r\n7   libsystem_malloc.dylib        \t0x00007fff7c5b2201 malloc_zone_malloc + 103\r\n8   libsystem_malloc.dylib        \t0x00007fff7c5b150b malloc + 24\r\n9   libc++abi.dylib               \t0x00007fff7a49b628 operator new(unsigned long) + 40\r\n10  _pywrap_tensorflow_internal.so\t0x0000000108c9bc16 tensorflow::ApplyAdamOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*) + 70\r\n11  libtensorflow_framework.so    \t0x000000010e45488d tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) + 301\r\n12  libtensorflow_framework.so    \t0x000000010e42382b tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) + 4875\r\n13  libtensorflow_framework.so    \t0x000000010e42bd90 std::__1::__function::__func<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&>, std::__1::allocator<std::__1::__bind<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long), tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode const&, long long&> >, void ()>::operator()() + 80\r\n14  libtensorflow_framework.so    \t0x000000010e0c9fc2 Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 1922\r\n15  libtensorflow_framework.so    \t0x000000010e0c9734 std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 52\r\n16  libtensorflow_framework.so    \t0x000000010e0ec9c0 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::function<void ()> > >(void*) + 96\r\n17  libsystem_pthread.dylib       \t0x00007fff7c6946c1 _pthread_body + 340\r\n18  libsystem_pthread.dylib       \t0x00007fff7c69456d _pthread_start + 377\r\n19  libsystem_pthread.dylib       \t0x00007fff7c693c5d thread_start + 13\r\n```\r\n\r\nThis might be related to our own C++ operation which has worked fine so far (we used it since TF 0.8), although of course this might be triggered only now by some race condition. Is there anything new I need to take care of? I think this NSync stuff is new?\r\n", "comments": ["I have had a use-after-free bug before when some tensors were the wrong size. They were fed into an Eigen operation that only checks the size in debug mode. When I rebuilt in debug mode I got an Eigen assertion that pointed to the actual issue.", "libsystem_malloc.dylib seems wrong; if you're using the pip package TensorFlow should be using jemalloc. Maybe [this symbol](https://github.com/rwth-i6/returnn/blob/148d103e13ebecb83c0d40539ae84eadabc14873/NativeOp.cpp#L499) is leaking into TensorFlow since you [load the op with RTLD_GLOBAL](https://github.com/rwth-i6/returnn/blob/b53a0a5c1c6acd0235fde2adcfeb8fb20b213fa8/TFNativeOp.py#L88)? If so, you could just link the op to libtensorflow_framework.so and load with RTLD_LOCAL.", "Thanks for the answers. @allenlavoie  The symbol you refer to is inside `struct Context` (a macro trick to make `OpKernelContext* context` from the outer scope available inside the functions there), so I don't think that symbol causes confusion.\r\n\r\nHowever, it's good that you mention libsystem_malloc.dylib. So, you assume the `malloc` from libsystem_malloc overwrites the `malloc` from libtensorflow_framework?\r\nShould it also use jemalloc for `operator new`?\r\nAre you sure it should use jemalloc on MacOSX? When looking into libtensorflow_framework, I see lots of references to `_malloc`, which I think should resolve to the one in libsystem_malloc.\r\n\r\nThe `RTLD_GLOBAL` loaded lib is from `tensorflow.contrib.rnn.python.ops` and not my own. I wanted to access some symbols from that op (`LSTMBlock` utilities) inside my own op. Do you think that will cause the problem? But why exactly? Maybe I should link instead to it? `otool -L contrib/rnn/python/ops/_lstm_ops.so` gives me:\r\n```\r\n\t@rpath/_lstm_ops.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libtensorflow_framework.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)\r\n```\r\nThe same for `libtensorflow_framework.so` gives me:\r\n```\r\n\t@rpath/libtensorflow_framework.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)\r\n\t/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)\r\n```\r\nAnd `libSystem.B.dylib` links to `/usr/lib/system/libsystem_malloc.dylib`.\r\n**Edit: I just removed that `RTLD_GLOBAL` lib loading as I didn't need it anyway for the CPU-only version of my op. It made no difference. So that is not related to the problem.**\r\n**Edit 2: Also note, this whole workaround is there basically to solve [this](https://stackoverflow.com/questions/41428756/own-tensorflow-op-with-cublassgemm). Related is [this issue](https://github.com/tensorflow/tensorflow/issues/6602).**\r\n**Edit 3: The problem also happens when I remove any of the custom `sgemm_` calls, which might have been related. But the crash still happens.**\r\n\r\nMy own custom op, I load via `tf.load_op_library` ([here](https://github.com/rwth-i6/returnn/blob/e06729e54131d4122d550bd0d576750b966a3f38/TFUtil.py#L2171)), so I guess it will use `RTLD_LOCAL`.\r\n", "The commit I just did seems to fix it. It was actually not a bug in TensorFlow but in my code. I kept using the pointer `tensor->shape().dim_sizes().data()` which becomes invalid after the statement evaluation is finished. I wonder if that was valid before or not. Anyway, this seemed to be the problem and it doesn't crash anymore. So I'm closing this issue for now, unless it will happen again."]}, {"number": 14284, "title": "Estimator from Keras by model_to_estimator cannot export", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.4 or Master \r\n\r\n\r\n### Describe the problem\r\nwhen calling model_to_estimator, the model_fn create by _create_keras_model_fn didn't set ```export_outputs``` in the returned ```EstimatorSpec```, see [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/estimator.py#L182). That make it unable to export to SavedModel, the following error raised when call ```estimator.export_savedmodel()```\r\n\r\n```\r\npackages/tensorflow/python/estimator/export/export.py\", line 193, in build_all_signature_defs\r\n    raise ValueError('export_outputs must be a dict.')\r\nValueError: export_outputs must be a dict.\r\n```\r\n\r\nI would like contrib a PR for fixing this if is OK\r\n\r\n", "comments": ["@yifeif , can you please take a look at this issue? Thanks,\r\n", "Thanks @yjmade. A PR is more than welcomed! Feel free to request a review from me whenever you have it ready. cc: @fchollet.", "PR Merged, close now"]}, {"number": 14283, "title": "Can't use estimator + dataset and train for less than one epoch", "body": "TensorFlow 1.4 moves TF Dataset to core (tf.data.Dataset) and doc/tutorial suggest to use tf.estimator to train models.\r\n\r\nHowever, as recommended at the end of this [page](https://www.tensorflow.org/programmers_guide/datasets), the Dataset object and its iterator must be instantiated inside the input_fn function. This means the iterations through the dataset will start over for each call to estimator.train(input_fn, steps). Thus, calling is with steps < number of samples in epoch, will lead to train the model on a subset of the dataset.\r\n\r\nThus my question. Is it possible to implement something like this with Estimator + Dataset:\r\n\r\n```\r\nfor i in range(num_epochs):\r\n    # Train for some steps\r\n    estimator.train(input_fn=train_input_fn, steps=valid_freq)\r\n\r\n   validation_iterator.\r\n    # Evaluate on the validation set (steps=None, we evaluate on the full validation set)\r\n   estimator.evaluate(input_fn=valid_input_fn)\r\n```\r\n\r\nwithout starting training samples iterations from scratch at each call to `estimator.train(input_fn=train_input_fn, steps=valid_freq)\r\n`\r\n\r\nFor example, unlike [here](https://www.tensorflow.org/programmers_guide/datasets), instantiate the Dataset and its iterator outside input_fn. I tried it but it does not work because then the input (from the dataset iterator) and the model (from the estimator model_fn) are not part of the same graph.\r\n\r\nThanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Isn't it a feature request? As, as far as I understand, Dataset + input_fn are designed in a way that does not seem to allow to train for a few steps, evaluate, then train again without restarting the dataset iterations from scratch.\r\nThanks", "I misread the issue as a is-this-possible-if-so-how question. Sorry about that.\r\n\r\n@davidsoergel, can you take a look at this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "We have the same issue: We have a large dataset which trains several hours per epoch, but wish to evaluate in between epochs. Unfortunately there is no resource sharing for local sessions, so iterators cannot be shared between sessions.\r\n\r\nSo these are the possibilities I see:\r\n- As far as I understand, dataset.skip() always skips the first lines, thus it is not usable for this case. Also the global step is not available during graph creation in the Estimator, so it cannot be used since it is stateful.\r\n- Allow local sessions to share resources, and keep the dataset iterator alive.\r\n- Prevent the graph and session from being destroyed, thus keeping the iterator. This might be problematic with memory usage.\r\n- Allow a way to use the main graph for both, evaluation and training (i.e. evaluation only uses a subgraph of training, there might be some `tf.cond`s involved with a placeholder for switching the mode). The problem here is that saving the graph will result in a lot of unnececssary ops. (<- this is still my favorite, because it seems feasable)\r\n- Allow to skip entries on first training step after the session was created (e.g. `iterator.skip_initial_steps`), thus continuing training at about the i-th step (the shuffle buffer is not exactly the same then, but at least close to). This should be optimized, such that the dataset does not actually read the data, but only skip it.\r\n- Add a simple option to evaluate after exactly one epoch, thus ensure each sample was read. This unfortunately does not allow for evaluations in between, but is rather a workaround.\r\n\r\nAre there any other suggestions?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 72 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 103 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 118 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 133 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 148 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 163 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mrry sorry to be throwing a lot of things your way. This seems related to datasets. ", "This is possible in the raw `tf.data` API, but the session management in estimators makes it tricky. I believe the recommended API to avoid restarting the session at evaluation time is [`tf.contrib.estimator.InMemoryEvaluatorHook`](https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/InMemoryEvaluatorHook). Assigning to @ispirmustafa for confirmation.", "Every Estimator.train call will re-create whole graph (including input-fn) by design. If you want to evaluate periodically without impacting training, as Derek mentioned please give `tf.contrib.estimator.InMemoryEvaluatorHook` a shot."]}, {"number": 14282, "title": "Neural Net regression in Tensorflow", "body": "### Describe the problem\r\nHi, I am trying to perform a Neural Network in tensorflow for The Bike-Sharing-Dataset with a regresion model. I checked out the tensorflow tutorials but I couldn't find  a solution. Something's wrong with the tf.matmul() and the input and  targets matrix. \r\n\r\n### Source code / logs\r\nHere is my [repository](https://github.com/deepSalva/bikes_in_tensorflow.git) and the dataset.\r\n\r\nThanks so much!\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thanks for your answer. Anyway I think there is a bug in there since I can' t train the model because of the tensor shapes. Also it will be interesting if someone in the tensorflow community can show me the way to make this regression model with a hidden layer in tensorflow. I already write in StackOverflow and I got no answer... I am really stuck since 2 days"]}, {"number": 14281, "title": "better visualization name", "body": "", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?"]}, {"number": 14280, "title": "Failed to load the native TensorFlow runtime.", "body": "when I import tensorflow-gpu,\r\n\r\n>>import tensorflow as tf\r\n\r\n\r\nthis message occurs\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/wonjinlee/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14279, "title": "Cannot interpret feed_dict key as tensor: tensor tensor(import/input:0, shape=(1, 299, 299, 3), dtype=float32) is not an element of this graph.", "body": "I'm running `label_image.py` code given at [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py](url) for inference on an image using InceptionV3 model trained on my own dataset. But it's giving the following error\r\n\r\n> Cannot interpret feed_dict key as tensor: tensor tensor(import/input:0, shape=(1, 299, 299, 3), dtype=float32) is not an element of this graph.\r\n\r\nCould anyone please help me with this issue ? Thanks in advance.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14278, "title": "depthwise_ops build incorrect on MSVC debug mode", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n- **Python version**: \r\n3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\nVS 2017\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\nbazel --output_base C:\\os\\t build --config=monolithic --incompatible_disallow_set_constructor=false --action_env=USE_DYNAMIC_CRT=1  --color=no --compilation_mode fastbuild --verbose_failures --experimental_ui  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nCompile tensorflow in fastbuild mode( release mode,but without any optimization) failed.\r\n\r\n### Source code / logs\r\n848123e61c95b030a5034b2cdaa504870ebf7da6 is a dirty hack for this problem.  First,  you shouldn't simply remove the double type kernel in Windows Debug build, it will make some unitests fail.   Second, this fix doesn't fit for the \"fastbuild\" mode, as \"_DEBUG\" is not defined.\r\n\r\nThe root cause is VC cannot eliminate the dead code which is introduced by std::is_same. This problem also exists in tensorflow/core/kernels/depthtospace_op.cc and tensorflow/core/kernels/spacetodepth_op.cc.\r\n\r\n", "comments": ["@chsigg could you take a look at this feedback on a change you made recently?", "I agree with @snnn's assessment. This seems incorrect. Should be fixed. cc: @mrry.", "Nagging Assignee @chsigg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @chsigg: It has been 106 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 14277, "title": "How to realize model parallelism", "body": "I cannot find the model parallelism in the tensorflow, there are many examples about data parallelism. I write a programm to realize the model parallelism, but it cannot work.\r\n\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\n\r\ntf.train.ClusterSpec({\r\n    \"worker\": [\r\n        \"node1:2222\"\r\n    ],\r\n    \"ps\": [\r\n        \"node2:2222\"\r\n    ]})\r\n\r\nwith tf.device(\"/job:ps/task:0\"):\r\n    weights = tf.Variable(tf.random_normal([1]),dtype=tf.float32,name='weights')\r\n    bias = tf.Variable(tf.ones([1])+0.5,dtype=tf.float32, name='bias')\r\n\r\nwith tf.device(\"/job:worker/task:0\"):\r\n    X = np.random.normal(0, 100, 100000)\r\n    y = X * 0.5 + 10 + np.random.normal(0, 0.1, len(X))\r\n    results = weights * X + bias\r\n    loss = tf.reduce_mean(tf.square(y-results))\r\n    optimizer = tf.train.GradientDescentOptimizer(0.1)\r\n    train_op = optimizer.minimize(loss)\r\n    init = tf.global_variables_initializer()\r\n\r\nwith tf.Session(\"grpc://node2:2222\") as sess:\r\n    sess.run(init)\r\n    for _ in range(10000):\r\n        Loss, T = sess.run([loss, train_op])\r\n        print(Loss)", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14276, "title": "Cannot interpret feed_dict key as tensor: tensor tensor(import/input:0, shape=(1, 299, 299, 3), dtype=float32) is not an element of this graph.", "body": "I'm running `label_image.py` given at [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py](url) on `inception_resnet_v2` frozen graph. When I run this code it's giving me the following error\r\n`cannot interpret feed_dict key as tensor: tensor tensor(import/input:0, shape=(1, 299, 299, 3), dtype=float32) is not an element of this graph.`\r\n\r\nBut when I print the tensors, there exists the above tensor in the graph. Could anyone please help me with this ? Thanks in advance.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14275, "title": "changes in  ops_to_register.h file does not effect resulting compilation ", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (https://github.com/tensorflow/tensorflow/issues/14215)**:\r\n- **OS Platform and Distribution (Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source )**:\r\n- **TensorFlow version (1.4)**:\r\n- **Python version  2.7: \r\n- **Bazel version (0.7.0)**:\r\n\r\n\r\n\r\n### Describe the problem\r\nI have been running print_selective_registration_header to produce libtensorflow_inference.so\r\nfor android as described in : tensorflow/tensorflow/python/tools/print_selective_registration_header.py\r\ni saw that ops_to_register.h was created correctly but the resulting    libtensorflow_inference.so did not work in android and there were always missing kernel ops errors,\r\ni repeated the process with a few different graphs and i have noticed that the ops_to_register.h is being updated but there were no changes in the resulting libtensorflow_inference.so file.\r\ni have tried all the possible bazel clean option and bazel dump thinking it is a bazel caching issue.\r\nnone helped ,\r\ni opened this issue thinking it is an android bug :https://github.com/tensorflow/tensorflow/issues/14215\r\nand closed it once i understood that the issue is that the result of the bazel build is worng\r\n\r\ncan anyone check this out?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it looks like a configuration-related build issue. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14274, "title": "how can I print predictions  in TF-Slim's eval_image_classifier.py?", "body": "I want to print prediction results into txt files  in TF-Slim [eval_image_classifier.py](https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py) .  I have tried many times, but failed to do it.  \r\nThank you in advance.\r\n\r\n\r\n\r\n===============================================================\r\n`from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport math\r\nimport tensorflow as tf\r\n\r\nfrom datasets import dataset_factory\r\nfrom nets import nets_factory\r\nfrom preprocessing import preprocessing_factory\r\n\r\nslim = tf.contrib.slim\r\n\r\ntf.app.flags.DEFINE_integer(\r\n    'batch_size', 100, 'The number of samples in each batch.')\r\n\r\ntf.app.flags.DEFINE_integer(\r\n    'max_num_batches', None,\r\n    'Max number of batches to evaluate by default use all.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'master', '', 'The address of the TensorFlow master to use.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'checkpoint_path', '/tmp/tfmodel/',\r\n    'The directory where the model was written to or an absolute path to a '\r\n    'checkpoint file.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'eval_dir', '/tmp/tfmodel/', 'Directory where the results are saved to.')\r\n\r\ntf.app.flags.DEFINE_integer(\r\n    'num_preprocessing_threads', 4,\r\n    'The number of threads used to create the batches.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'dataset_name', 'imagenet', 'The name of the dataset to load.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'dataset_split_name', 'test', 'The name of the train/test split.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'dataset_dir', None, 'The directory where the dataset files are stored.')\r\n\r\ntf.app.flags.DEFINE_integer(\r\n    'labels_offset', 0,\r\n    'An offset for the labels in the dataset. This flag is primarily used to '\r\n    'evaluate the VGG and ResNet architectures which do not use a background '\r\n    'class for the ImageNet dataset.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'model_name', 'inception_v3', 'The name of the architecture to evaluate.')\r\n\r\ntf.app.flags.DEFINE_string(\r\n    'preprocessing_name', None, 'The name of the preprocessing to use. If left '\r\n    'as `None`, then the model_name flag is used.')\r\n\r\ntf.app.flags.DEFINE_float(\r\n    'moving_average_decay', None,\r\n    'The decay to use for the moving average.'\r\n    'If left as None, then moving averages are not used.')\r\n\r\ntf.app.flags.DEFINE_integer(\r\n    'eval_image_size', None, 'Eval image size')\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\n\r\ndef main(_):\r\n  if not FLAGS.dataset_dir:\r\n    raise ValueError('You must supply the dataset directory with --dataset_dir')\r\n\r\n  tf.logging.set_verbosity(tf.logging.INFO)\r\n  with tf.Graph().as_default():\r\n    tf_global_step = slim.get_or_create_global_step()\r\n\r\n    ######################\r\n    # Select the dataset #\r\n    ######################\r\n    dataset = dataset_factory.get_dataset(\r\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\r\n\r\n    ####################\r\n    # Select the model #\r\n    ####################\r\n    network_fn = nets_factory.get_network_fn(\r\n        FLAGS.model_name,\r\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\r\n        is_training=False)\r\n\r\n    ##############################################################\r\n    # Create a dataset provider that loads data from the dataset #\r\n    ##############################################################\r\n    provider = slim.dataset_data_provider.DatasetDataProvider(\r\n        dataset,\r\n        shuffle=False,\r\n        common_queue_capacity=2 * FLAGS.batch_size,\r\n        common_queue_min=FLAGS.batch_size)\r\n    [image, label] = provider.get(['image', 'label'])\r\n    label -= FLAGS.labels_offset\r\n\r\n    #####################################\r\n    # Select the preprocessing function #\r\n    #####################################\r\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\r\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n        preprocessing_name,\r\n        is_training=False)\r\n\r\n    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\r\n\r\n    image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\r\n\r\n    images, labels = tf.train.batch(\r\n        [image, label],\r\n        batch_size=FLAGS.batch_size,\r\n        num_threads=FLAGS.num_preprocessing_threads,\r\n        capacity=5 * FLAGS.batch_size)\r\n\r\n    ####################\r\n    # Define the model #\r\n    ####################\r\n    logits, _ = network_fn(images)\r\n\r\n    if FLAGS.moving_average_decay:\r\n      variable_averages = tf.train.ExponentialMovingAverage(\r\n          FLAGS.moving_average_decay, tf_global_step)\r\n      variables_to_restore = variable_averages.variables_to_restore(\r\n          slim.get_model_variables())\r\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\r\n    else:\r\n      variables_to_restore = slim.get_variables_to_restore()\r\n\r\n    predictions = tf.argmax(logits, 1)\r\n    labels = tf.squeeze(labels)\r\n\r\n    # Define the metrics:\r\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n        'Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\r\n        'Recall_5': slim.metrics.streaming_recall_at_k(\r\n            logits, labels, 5),\r\n    })\r\n\r\n    # Print the summaries to screen.\r\n    for name, value in names_to_values.items():\r\n      summary_name = 'eval/%s' % name\r\n      op = tf.summary.scalar(summary_name, value, collections=[])\r\n      op = tf.Print(op, [value], summary_name)\r\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\r\n\r\n    # TODO(sguada) use num_epochs=1\r\n    if FLAGS.max_num_batches:\r\n      num_batches = FLAGS.max_num_batches\r\n    else:\r\n      # This ensures that we make a single pass over all of the data.\r\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\r\n\r\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\r\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\r\n    else:\r\n      checkpoint_path = FLAGS.checkpoint_path\r\n\r\n    tf.logging.info('Evaluating %s' % checkpoint_path)\r\n\r\n    slim.evaluation.evaluate_once(\r\n        master=FLAGS.master,\r\n        checkpoint_path=checkpoint_path,\r\n        logdir=FLAGS.eval_dir,\r\n        num_evals=num_batches,\r\n        eval_op=list(names_to_updates.values()),\r\n        variables_to_restore=variables_to_restore)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()`", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14273, "title": "RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS High Sierra 10.13.1\r\n- **TensorFlow installed from (source or binary)**:\r\npip3 install tensor flow\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC: stable 7.2.0 \r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\npython3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nError after install Tensorflow:\r\n\r\npython3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\nv1.4.0-rc1-11-g130a514 1.4.0\r\n\r\ngetting this and ONE MORE error when running:\r\npython3 -c \"import keras; print (keras.__version__)\"\r\nUsing TensorFlow backend.\r\n/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n2017-11-06 15:12:09.361728: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2.0.9\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing this as a duplicate of #14182 ", "HI, am having the same issue....  any one with a solution?", "I am also having the same problem:\r\n\r\nUsing TensorFlow backend.\r\n/Users/Create/.pyenv/versions/3.6.3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)", "https://github.com/lakshayg/tensorflow-build\r\nthis guy can help.", "Thank you! @SteffenNa ", "I have met the same issue with python3 in virtualenv", "Thank you @SteffenNa ", "Thanks @SteffenNa \r\n\r\nFor anyone else, the \"macOS Sierra\" build worked on my macOS High Sierra 10.13.1", "+1 to @drewszurko and @SteffenNa \r\n\r\nBetter to use Sierra (1.4.0) on High Sierra than what is marked High Sierra (1.4.1):\r\n\r\n`pip install --ignore-installed --upgrade \"https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.0-cp36-cp36m-macosx_10_12_x86_64.whl\"`", "Thank you! @SteffenNa", "@sshadmand I installed the High Sierra build on High Sierra with no problems (running macOS version 10.13.2)", "@SteffenNa suggestion is the solution (at least for me). Just go to https://github.com/lakshayg/tensorflow-build click on the on wheel you want and then on the download button right click and copy link address and paste into your pip line... e.g.\r\n\r\n    $  pip install --upgrade https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.1-cp36-cp36m-macosx_10_13_x86_64.whl\r\n\r\nWorked great and fixed the warning about mismatched version.", "worked perfectl for me on : OSX High Sierra\n\nOn Wed, Dec 27, 2017 at 8:48 PM, Brian Wylie <notifications@github.com>\nwrote:\n\n> @SteffenNa <https://github.com/steffenna> suggestion is the solution (at\n> least for me). Just go to https://github.com/lakshayg/tensorflow-build\n> click on the on wheel you want and then on the download button right click\n> and copy link address and paste into your pip line... e.g.\n>\n> $  pip install --upgrade https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.1-cp36-cp36m-macosx_10_13_x86_64.whl\n>\n> Worked great and fixed the warning about mismatched version.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14273#issuecomment-354167593>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALQBzZjyWU3o1J2vGWINnZsJYjrh1PtYks5tEp75gaJpZM4QSzht>\n> .\n>\n", "https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n(ubuntu version) works for current Opensuse Tumbleweed ", "how about for RHEL7?", "Worked perfect , Thanks.", "@KyleTheNewbie I've installed with\r\n\r\n`pip install https://github.com/mind/wheels/releases/download/tf1.3-cpu/tensorflow-1.3.0-cp36-cp36m-linux_x86_64.whl`\r\n\r\non Fedora and it seems good", "@SteffenNa correct. It's work. Thanks", "Using   \r\nmacOS 10.13.2 (17C88)\r\nKernel-Version:\tDarwin 17.3.0\r\n\r\nWhy do I get the following error? What to do?\r\n\r\nLast login: Fri Jan 26 17:25:23 on ttys000\r\n\r\n$ pip install --ignore-installed --upgrade \"https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.0-cp36-cp36m-macosx_10_12_x86_64.whl\"\r\nCollecting tensorflow==1.4.0 from https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.0-cp36-cp36m-macosx_10_12_x86_64.whl\r\n  Using cached https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.0-cp36-cp36m-macosx_10_12_x86_64.whl\r\nCollecting six>=1.10.0 (from tensorflow==1.4.0)\r\n  Using cached six-1.11.0-py2.py3-none-any.whl\r\nCollecting enum34>=1.1.6 (from tensorflow==1.4.0)\r\n  Using cached enum34-1.1.6-py3-none-any.whl\r\nCollecting wheel>=0.26 (from tensorflow==1.4.0)\r\n  Using cached wheel-0.30.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.3.0 (from tensorflow==1.4.0)\r\n  Using cached protobuf-3.5.1-py2.py3-none-any.whl\r\nCollecting numpy>=1.12.1 (from tensorflow==1.4.0)\r\n  Using cached numpy-1.14.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\r\nCollecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.0)\r\n  Using cached tensorflow_tensorboard-0.4.0-py3-none-any.whl\r\nCollecting setuptools (from protobuf>=3.3.0->tensorflow==1.4.0)\r\n  Using cached setuptools-38.4.0-py2.py3-none-any.whl\r\nCollecting html5lib==0.9999999 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\r\nCollecting markdown>=2.6.8 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\r\n  Using cached Markdown-2.6.11-py2.py3-none-any.whl\r\nCollecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\r\n  Using cached Werkzeug-0.14.1-py2.py3-none-any.whl\r\nCollecting bleach==1.5.0 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\r\n  Using cached bleach-1.5.0-py2.py3-none-any.whl\r\nInstalling collected packages: six, enum34, wheel, setuptools, protobuf, numpy, html5lib, markdown, werkzeug, bleach, tensorflow-tensorboard, tensorflow\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python3.6/site-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/usr/local/lib/python3.6/site-packages/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/usr/local/lib/python3.6/site-packages/pip/wheel.py\", line 323, in clobber\r\n    shutil.copyfile(srcfile, destfile)\r\n  File \"/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/shutil.py\", line 121, in copyfile\r\n    with open(dst, 'wb') as fdst:\r\nPermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.6/site-packages/six.py'", "@informixx you don't have permission to write to the python installation. You should run `sudo pip install ...`", "@erickrf Thx, for your quick answer!\r\n\r\nOf course that's the solution and I've learned something :-)\r\n\r\nBut, poorly that fix does not help, I still got this error:\r\n\r\n/Users/hamburg/PycharmProjects/Linear Regression Example/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n/Users/hamburg/PycharmProjects/Linear Regression Example/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/Users/hamburg/Library/Preferences/PyCharmCE2017.3/scratches/Linear_Regression_Example.py\", line 6, in <module>\r\n    import matplotlib.pyplot as plt\r\n  File \"/Users/hamburg/PycharmProjects/Linear Regression Example/venv/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 116, in <module>\r\n    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()\r\n  File \"/Users/hamburg/PycharmProjects/Linear Regression Example/venv/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 60, in pylab_setup\r\n    [backend_name], 0)\r\n  File \"/Users/hamburg/PycharmProjects/Linear Regression Example/venv/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py\", line 17, in <module>\r\n    from matplotlib.backends import _macosx\r\nRuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.\r\n\r\n", "What if I have: Python 3.6.4 :: Anaconda custom (64-bit), does Anaconda custom affect any upgrade installation?\r\n\r\nwelp, I'll try it now.\r\n", "NVM it works.", "Any suggestions on what to try with this environment? I can't seem to find a TF build on that link above ^ for an exact match to my system, but upgrading some of this stuff seems fairly complex. \r\n\r\n**macOS Sierra**\r\nApple LLVM version 9.0.0 (clang-900.0.37)\r\nPython 3.6.1 :: Anaconda custom (64-bit)\r\nTensorFlow v1.4.0-rc1-11-g130a514 1.4.0\r\n\r\nThanks for your help. ", "It works for pip with:\r\n`pip install --upgrade pip`\r\n\r\nOr maybe you can execute if you use conda:\r\n`conda install tensorflow`", "on Mac OS High Seirra the following worked for me \r\n\r\n`pip install --ignore-installed --upgrade \"https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.5.0-cp36-cp36m-macosx_10_13_x86_64.whl\"`\r\n  ", "@SteffenNa Thank you :)", "tensorflow-1.5.0-cp36-cp36m-macosx_10_13_x86_64.whl is not a supported wheel on this platform.\r\n", "@benedictchen I'm getting the same error\r\n", "@benedictchen @animaleja32 Are you using a wheel that matches your Python version?", "take the right version of this @SteffenNa ,and i resolve it. thx ", "@zhilevan thanks for your suggestion, works one my Mac", "Facing the same issue on Mojave. \r\nAny solution ?\r\npython -c \"import tensorflow as tf; print(tf.__version__)\"\r\n/Users/rakshitsareen/venv/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7", "When using pyenv, omit the \"--user\", so in my case: pip install --ignore-installed --upgrade \"Download URL\"\r\n\r\nIf you already ran the command, you can \"rm -rf ~/.local\" to cleanup your mess on OSX :)\r\n\"pip --user\" reference: https://docs.python.org/3.7/library/site.html\r\n\r\n@rakshitsareen That's the whole purpose of this issue, so read carefully from the top."]}, {"number": 14272, "title": "typo fixed : did't -> didn't", "body": "typo fixed ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", " I signed it!\r\n", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 14271, "title": "tf.gradients return NaN when batch_norm is in inference mode (is_training=False)?", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: Python 2.7.11 |Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:/usr/local/cuda-8.0 & libcudnn.so.6.0.21\r\n- **GPU model and memory**:GeForce GTX 1080\r\n- **Exact command to reproduce**:\r\n```\r\ngit clone https://github.com/wenwei202/models.git\r\ncd models\r\ngit checkout bug-fixing\r\ncd research/slim\r\npython eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\r\n--dataset_name imagenet \\\r\n--dataset_split_name validation \\\r\n--model_name resnet_v1_152 \\\r\n--batch_size 5 --eval_dir /tmp/bn_grad \\\r\n--labels_offset=1  --max_num_batches 1\r\n```\r\n\r\n### Describe the problem, Source code / logs\r\nI need to compute the gradients of dy/dx, where y is the max logit and x is input image. I just add a few lines to the standard [research/slim/eval_image_classifier.py](https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py):\r\n```\r\n    max_logits = tf.reduce_max(logits, axis=1)\r\n    themaps = tf.gradients(max_logits, images)\r\n    themaps = themaps[0]\r\n    with tf.control_dependencies([tf.Print(themaps,[themaps],first_n=3)]):\r\n      themaps = tf.abs(themaps)\r\n```\r\nwhich is [here](https://github.com/wenwei202/models/blob/bug-fixing/research/slim/eval_image_classifier_bn_grad.py#L155-L159).\r\nWhen I run `eval_image_classifier_bn_grad.py` to evaluate models [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models), vgg and lenet work well, but `tf.gradients` always return `nan` when I evaluate inception and resnet models.\r\nCommand:\r\n```\r\npython eval_image_classifier_bn_grad.py -alsologtostderr --checkpoint_path /tmp/resnet_v1_152.ckpt  --dataset_dir /tmp/imagenet-data \\\r\n--dataset_name imagenet \\\r\n--dataset_split_name validation \\\r\n--model_name resnet_v1_152 \\\r\n--batch_size 5 --eval_dir /tmp/bn_grad \\\r\n--labels_offset=1  --max_num_batches 1\r\n```\r\nOutput:\r\n```\r\n...\r\n2017-11-05 23:25:19.751859: I tensorflow/core/kernels/logging_ops.cc:79] [[[[nan nan nan]]]...]\r\n...\r\n```\r\n\r\nI guess the issue comes from the`slim.batch_norm` when it is in inference mode, and somehow, `tf.gradients` does not work anymore. Why?\r\n  1. vgg and lenet without batch_norm layers work well \r\n  2. inception and resnet with batch_norm layers return `nan`\r\n  3. inception and resnet work after I enforce the `is_training=True` of `slim.batch_norm`\r\n  4. it seems \"reasonable\" to ignore the possibility of the usage of `tf.gradients` when `batch_norm` is in inference mode.\r\n", "comments": ["Should be fixed by #10857 in this commit: https://github.com/tensorflow/tensorflow/commit/ed259c237425230b77a6fbcede714282b69b753a#diff-09d0717441656db7d7338b7febc2251a\r\n\r\nPlease upgrade to TF 1.4.", "@ppwwyyxx Issue solved by upgrading to tf 1.4 (thumb up)"]}, {"number": 14270, "title": "Keep the document consistent", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14269, "title": "Fix typo", "body": "Fix typo", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 14268, "title": "minor eager notebook examples cleanup", "body": "Minor eager notebook examples cleanup.  See https://github.com/tensorflow/early-eager/pull/37/files", "comments": ["Can one of the admins verify this patch?", "@asimshankar has context on this.", "Thanks for the PR. Looks good, but would you mind changing the remaining references to `tf.contrib.data` to `tf.data` as well?\r\n\r\nThanks!", "Sorry, I'd overlooked the comments.  I think we're set now.  PTAL.", "Jenkins, test this please"]}, {"number": 14267, "title": "opencv cannot read any image with tensorflow", "body": "It is the same issue as #1924, since the bug is closed, I open a new one, because this bug haven't been solved yet.\r\n\r\nFrom subashp\r\n\r\n> I am using the TF 1.4 and linking against C++ code. Below code always says it failed to read the file. I have incorporated above suggestions and it doesnt make any difference. Thoughts/suggestions?\r\n\r\n\r\n```\r\ncv::String pathImg = argv[1];\r\ncv::Mat img = cv::imread(pathImg, CV_LOAD_IMAGE_COLOR | CV_LOAD_IMAGE_ANYDEPTH);\r\nif (img.empty()) {\r\nerror(\"Failed to read the file {}\", argv[1]);\r\nreturn -1;\r\n}\r\n\r\n```\r\n\r\nFrom rmmal\r\n\r\n> we cloned the latest version of tensorflow and still there is a problem of reading images using opencv , everytime i initiate a TENSOR object the opencv doesn't work.\r\n\r\nFrom me, using tensor1.4, build from source\r\n\r\n```\r\n#include <opencv2/core.hpp>\r\n#include <opencv2/highgui.hpp>\r\n\r\n//unable to read any image if I include this header, no matter\r\n//it before or after opencv\r\n#include <tensorflow/cc/ops/const_op.h>\r\n\r\n#include <iostream>\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    cv::Mat input_mat = cv::imread(argv[1], cv::IMREAD_COLOR);\r\n    std::cout<<argv[1]<<\", size:\"<<input_mat.size()<<std::endl;    \r\n}\r\n\r\n```", "comments": ["This sounds like the same issue as https://github.com/tensorflow/tensorflow/issues/13278. Could you add a comment there describing how you're building (Bazel? Linking against libtensorflow_framework.so? Linking against the C++ API .so?)? TensorFlow needs to use image libraries, but it's possible we can hide the symbols in some cases.", "@allenlavoie \r\n\r\n>how you're building (Bazel? Linking against libtensorflow_framework.so? Linking against the C++ API .so?)?\r\n\r\nI build tensorflow libs by Bazel\r\n\r\n1. Clone tensorflow from https://github.com/tensorflow\r\n2. Install require dependencies\r\n3. bazel build //tensorflow:libtensorflow.so\r\n4. Link to cuda, -L/usr/local/cuda-8.0/lib64 -lcuda -lcudart -lcurand -lcublas -lcudnn -lcusolver -lcufft\r\n5. Link to protof and tensorflow, -L/usr/local -lprotobuf -ltensorflow_framework -ltensorflow_cc\r\n6. Link to opencv, -L/usr/local/lib -lopencv_core -lopencv_imgproc -lopencv_highgui -lopencv_imgcodecs -lopencv_video -lopencv_videoio\r\n6. Compile and run, images always empty\r\n\r\n> TensorFlow needs to use image libraries, but it's possible we can hide the symbols in some cases.\r\n\r\nCould we separate images loading part as another module, make it as an independent lib? Let the users select they want to build it or not. ", "If you're using the C++ API, you shouldn't need to link against libtensorflow_framework.so directly. Does it work if you take \"-ltensorflow_framework\" out?", "@allenlavoie \r\n\r\n>Does it work if you take \"-ltensorflow_framework\" out?\r\n\r\nIt will pop out a lot of undefined symbol when compile", "@stereomatchingkiss which symbols? We'd like the C/C++ APIs (libtensorflow.so/libtensorflow_cc.so) to be usable on their own, and those have a linker script so that they don't expose extra symbols (preventing issues like the one you've reported).", "@allenlavoie \r\n\r\n> Which symbols\r\n\r\nMany of them(1270), I put the records at [pastebin](https://pastebin.com/jmLBncx8)\r\n\r\n", "This seems to be the main problem:\r\n```usr/bin/ld: warning: libtensorflow_framework.so, needed by /home/ramsus/Qt/3rdLibs/tf_test/lib/libtensorflow_cc.so, not found (try using -rpath or -rpath-link)```\r\n\r\nlibtensorflow_framework.so should be accessible from libtensorflow_cc.so (in the same directory). As long as that's true and you stick to the C++ API it should work.", "@allenlavoie \r\n\r\nI use rpath to specify the folder, this time all of the compile time error gone(if I do not call tensorflow::graph_def), but the images read by cv::imread remain empty.\r\n\r\nHowever, create tensorflow::graph_def still got error\r\n\r\n> /usr/bin/ld: tensorflow_obj_detect_example.o: undefined reference to symbol '_ZN10tensorflow8GraphDefD1Ev'\r\n> /home/ramsus/Qt/3rdLibs/tf_test/lib/libtensorflow_framework.so: error adding symbols: DSO missing from command line\r\n\r\nI guess a quick fix is split up the image reading part into an independent lib.", "@skye might know whether GraphDef should be in the C++ API.\r\n\r\nBut could you verify that cv::imread works if you don't link in the TF C++ API, and doesn't work if you do link it in (making sure you're only linking to that and no other TF libraries)? This would be very surprising, since there's a linker script to control the symbols it exports. Basically we've already split the image reading part into an independent lib (libtensorflow_framework.so) that you don't need to link against.", "@allenlavoie \r\n\r\n>But could you verify that cv::imread works if you don't link in the TF C++ API, and doesn't work if you do link it in (making sure you're only linking to that and no other TF libraries)? \r\n\r\nAs long as I do not link to tensorflow library, cv::imread can read the image without any issue.\r\nIf I link to tensorflow library but do not include any tensorflow header, cv::imread still able to read the image. I do not know if it is cause by macro magic or symbol conflict.\r\n", "Ah, so it's not a symbol conflict (and splitting up our shared objects wouldn't help). Interesting. Maybe a ```#define```? I'd think the compiler would complain about most other header issues. ", "@allenlavoie \r\n\r\nI set the option -Wall, most of the worning are related to unused parameter. The header \"#include <tensorflow/cc/ops/const_op.h>\" will cause imread always return empty image\r\n\r\n[Complete warning message](https://pastebin.com/r0arZUt8)", "Yeah, those warnings all look fine. The next step would be to narrow down and figure out which transitively included header from const_op.h is causing problems; any interest?", "@allenlavoie  \r\n\r\n>The next step would be to narrow down and figure out which transitively included header from const_op.h is causing problems;\r\n\r\nMy tracking history\r\n\r\n1. <tensorflow/cc/ops/const_op.h>\r\n2. <tensorflow/cc/framework/ops.h>\r\n3. <tensorflow/core/framework/tensor.h>\r\n4. <tensorflow/core/lib/core/refcount.h>\r\n\r\nAs long as I include recount.h, the image return by imread will become empty. However, include <tensorflow/core/platform/logging.h> do not effect the resutls, this is weird. Do not looks like it is caused by #define. ", "@allenlavoie \r\nHi,\r\n\r\nI have a similar problem as well, although its surprising. I am not using cv in my project. I just trained a network on python and am trying to load the graph on a C++ project. I am trying to link libtensorflow_cc to my c++ project (building with cmake). It fails during linking - undefined reference to symbol '_ZN10tensorflow8GraphDefD1Ev'\r\n/usr/local/lib/libtensorflow_framework.so: error adding symbols: DSO missing from command line.\r\n\r\n\r\n\r\nAny help would be highly appreciated.\r\n\r\n", "Link with libtensorflow_framework.so should solve the issue\n\n<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>\nVirus-free.\nwww.avast.com\n<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>\n<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>\n\n2017-11-25 18:46 GMT+08:00 kikushah <notifications@github.com>:\n\n> @allenlavoie <https://github.com/allenlavoie>\n> Hi,\n>\n> I have a similar problem as well, although its surprising. I am not using\n> cv in my project. I just trained a network on python and am trying to load\n> the graph on a C++ project. I am trying to link libtensorflow_cc to my c++\n> project (building with cmake). It fails during linking - undefined\n> reference to symbol '_ZN10tensorflow8GraphDefD1Ev'\n> /usr/local/lib/libtensorflow_framework.so: error adding symbols: DSO\n> missing from command line.\n>\n> Any help would be highly appreciated.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14267#issuecomment-346932821>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABt-ulyybo_3buDu-BYlEUGX91aPFjXzks5s5-_0gaJpZM4QSpkr>\n> .\n>\n", "@allenlavoie\r\nHi, I encountered the same problem. Is there any solution? \r\nI am using tensorflow 1.4 & opencv 3.2.0 in Ubuntu 16.04.\r\nThe bazel version is 0.6.1, and the tensorflow compile command is: ` bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda tensorflow:libtensorflow_cc.so`\r\n", "Hi, I meet the same problem. Is there any solution?\r\nI am using tensorflow r1.4 and opencv3.1 in ubuntu14.04.\r\nAs far as I include #include <tensorflow/core/public/session.h> or #include \"tensorflow/cc/ops/standard_ops.h\" I cannot read image. When I didn include these tensorflow files, I can read frame successfully. Anyone could help me? Thanks a lot!!!\r\nmy cpp file:\r\n#include <tensorflow/core/platform/env.h>\r\n//#include <tensorflow/core/public/session.h>\r\n//#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include <opencv2/opencv.hpp>\r\n#include <iostream>\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\nint main()\r\n{\r\n    cv::VideoCapture cap;\r\n    if(!cap.open(\"/home/kx/project/RM-dataset/01.avi\")){\r\n        std::cout<<\"cannot open video \"<<std::endl;\r\n    }\r\n    cv::Mat frame;\r\n    while(1){\r\n        cap>>frame;\r\n        if(frame.empty()){\r\n            std::cout<<\"no frame\"<<std::endl;\r\n            continue;\r\n        }\r\n        cv::imshow(\"frame\",frame);\r\n        cv::waitKey(0);\r\n    }\r\n    return 0;\r\n} \r\n\r\nmy cmake file:\r\n\r\n cmake_minimum_required (VERSION 2.8)\r\n project (tf_example)\r\n\r\n set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -g -std=c++11 -W\")\r\n find_package(OpenCV 3.1.0 REQUIRED)\r\n include_directories(\r\n         /home/kx/something/tensorflow-r1.4\r\n         /home/kx/something/tensorflow-r1.4/tensorflow/bazel-genfiles\r\n         /home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/protobuf/include\r\n         /home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/host_obj\r\n         /home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/gen/proto\r\n         /home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/downloads/nsync/public\r\n         /home/kx/something/tensorflow-r1.4/tensorflow/contrib/makefile/downloads/eigen\r\n         /home/kx/something/tensorflow-r1.4/bazel-out/local-py3-opt/genfiles\r\n         ${OPENCV_INCLUDE_DIRS}\r\n         )\r\n\r\n add_executable(tf_test  tf_test.cpp)\r\n target_link_libraries(tf_test\r\n         /home/kx/something/tensorflow-r1.4/bazel-bin/tensorflow/libtensorflow_cc.so\r\n         /home/kx/something/tensorflow-r1.4/bazel-bin/tensorflow/libtensorflow_framework.so\r\n         ${OpenCV_LIBS}\r\n         )\r\n", "@stereomatchingkiss sorry, got a bit distracted. Is the claim that `#include <atomic>` is causing the issue? Everything else in that file is in a namespace, so I don't see how it could cause an issue.", "@allenlavoie Me neither. Whatever, we prefer another tool for our jobs.\r\n\r\nTensorflow is the most popular deep learning library, got rich features, huge community, support by the most brilliant engineers hired by google, amazing tensorboard, tensorflow have a lot of pros, but it got obvious drawback too(no perfect library, just pick the one suit your need)\r\n\r\n1. Very complicated dependencies and build system, I do not know why google prefer bazel but not cmake, cmake is mature and seldom break backward compatibility, but bazel do, it do not looks like a mature tool compare with cmake\r\n\r\n2. Api of tensroflow are far too complicated compared with keras and pytorch.Especially pytorch, it is much easier to debug and describe complicated network architectures by pytorch, everything are modularize and works just like numpy and the good old python way, you do not need to learn tons of new concepts when using pytorch, just some basic and you are good to go.\r\n\r\n3. Deployment steps are non-trivial and not yet mature, like the bugs we encountered in this post\r\n\r\nAlmost everything in tensorflow need us spend lots of times to pick up new concepts, tons of times to study, figure out how to use it properly. Tensorflow keep telling us \"deep learning is super hard\", while keras and pytorch show us how easy deep learning could be.\r\n\r\nI do not know what the other thinks, these are the major drawback I found in tensorflow.", "I'm guessing this is only an issue on later versions of OpenCV? I tried with 2.4 (since that was what the package manager had) and it seems to work fine including standard_ops.h.\r\n\r\nCan someone check for me whether it's just `#include <atomic>` causing the issue in OpenCV 3.1/3.2? In which case this seems like it's an OpenCV issue rather than something TensorFlow can address.", "@allenlavoie \r\nIt's wired!\r\nI'm using opencv 2.4.13.2. I cann't use `imread` to load image if I add the tensorflow's include file path & link lib to the compile command. \r\n\r\nBut when I'm using opencv 3.2.0. I can add the tensorflow's include path & link lib to the compile command and load file normally except the circumstance that I defined ` SessionOptions sessOptions;` variable.\r\n\r\n# opencv 3.2.0\r\n- link option:`-lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_imgcodecs -ltensorflow_cc -ltensorflow_framework`;  ==> connot imread image;\r\n- link option: `-lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_imgcodecs -ltensorflow_cc`; ==>can imread image; but `SessionOptions sessOptions; ` included in my source code, the compile gets error:\r\n/usr/bin/ld: /tmp/ccCMSFkT.o: undefined reference to symbol '_ZN10tensorflow14SessionOptionsC1Ev'\r\n//usr/lib/libtensorflow_framework.so: error adding symbols: DSO missing from command line.\r\n\r\n# opencv 2.4.13.2\r\n- link option: `-lopencv_core -lopencv_highgui -lopencv_imgproc  -ltensorflow_cc -ltensorflow_framework`; ==> connot imread image;\r\n- link option: `-lopencv_core -lopencv_highgui -lopencv_imgproc -ltensorflow_cc` ==>the same as opencv3.2.0 link without tensorflow_framework.\r\n\r\nSo maybe the problem is tensorflow_framework.so.", "@bitzy Right, that's a symbol conflict. My original comments in this thread were incorrect; you do need libtensorflow_framework.so for protocol buffers right now, and that's the one with symbols which conflict with OpenCV. The eventual solution is going to be for us to split out a third shared object with just the protocol buffer symbols for use with the C++ API (which I believe @gunan is considering anyway).\r\n\r\nIn the meantime, as long as you're not using any custom ops you can build libtensorflow_cc.so with `bazel build --config=monolithic`, which will condense everything together into one shared object (no libtensorflow_framework dependence) and seal off non-TensorFlow symbols. That shared object will have protocol buffer symbols.", "@allenlavoie \r\n\r\nNow I can load image with opencv. Thanks!", "Looks to be resolved?\r\nI will close the issue now, but please reopen if there are still things to address.", "Hi! I meet the same problem.\r\nI'm using opencv3.1 tensorflow1.4 in ubuntu14.04. When I included tensorflow headers like #include <tensorflow/core/public/session.h> or #include \"tensorflow/cc/ops/standard_ops.h\", cv::imread can not read images encoded by JPEG, but can read other encoding images like Uncompressed 8-bit RGB. When I commented TF headers, I can read any images by cv::imread. Is there any solution? Thanks a lot!!!  @allenlavoie @bitzy \r\n\r\nupdate:\r\nI rebuild libtensorflow_cc.so using command  `bazel build --config=monolithic :libtensorflow_cc.so ` , and can read any images successfully. Thank you @allenlavoie . \r\nThough the problem appear to have been solved, I have some questions below\uff1a\r\n1.What is the root cause of this problem?(libjpeg version in opencv and tensorflow conficts? or other reason? ) \r\n2.Since there is no libtensorflow_framework, are there some functions that I can't use?\r\n3.Could we get a new TF version in the future, which can solve this problem? \r\n", "@kxhit \r\nNeat, so headers aren't causing issues on their own for you?\r\n\r\n1. That's a reasonable guess; I've never tracked down exactly which image library in TF conflicts. \r\n2. Custom ops are the main limitation (since they need symbols which are sealed off from the C++ API; typically they'd link against libtensorflow_framework directly, but if that's not built then there's no good way to support them); you can use everything else in the C++ API. libtensorflow_framework does have lots of other undocumented/internal symbols which get sealed off from the C++ API, but most likely aren't super useful.\r\n3. Yes, we'd like to split off our protocol buffers into another shared object. C++ API users could then link against libtensorflow_cc and e.g. libtensorflow_protobufs rather than relying on the metaphorical junk drawer which is libtensorflow_framework for protocol buffer symbols. I think this should be feasible, since we already have header-only protobuf rules (they need to live in exactly one .so, so we already need to exclude them from language bindings).", "@allenlavoie \r\nYes! I can read any images with headers `#include <tensorflow/core/public/session.h>  #include \"tensorflow/cc/ops/standard_ops.h\"`. Thank you for your help!", "Hi, is this also a problem in tensorflow 1.5.0? upgrading takes a long time, so I don't want to do it if it also has the same problem :-(", "@RoboEvangelist I am having the same issue in tensorflow r1.5 as well.", "@allenlavoie is there any workaround to build custom ops, which using opencv? cv::resize seems to work, but cv::imread/cv::imdecode always return empty matrix, if it used inside custom op.", "@svetlov for sure, you can link your custom op .so against a second .so which does not link against libtensorflow_framework.so (and doesn't include TensorFlow headers, since the symbols won't be defined). There's [one contrib op doing basically this](https://github.com/tensorflow/tensorflow/blob/f610284f878b341423bde42afc90f917c337138c/tensorflow/contrib/tensor_forest/BUILD#L408). The second .so won't see any TensorFlow symbols and so won't have a problem calling OpenCV.", "I hit the same issue. I can't use \"monolithic\" workaround since it breaks TensorRT integration. The conflicts are jpeg_CreateDecompress and jpeg_CreateCompress. libopencv_imgcodec expects to import from libjpect.so.8, but it somehow picked up from libtensorflow_framework. \r\n\r\nI solved it by compiling tensorflow jpeg with \"-fvisibility\" flag. The change is around line 41 in thirdparty/jpeg/jpeg.BUILD. \"-fPIC\" is needed as the result of \"-fvisibility\".\r\n    \"//conditions:default\": [\r\n        \"-fvisibility=hidden -fPIC\"\r\n    ],  \r\n\r\nThere are some linking warnings for undefined dynamic symbol in tensorflow/contrib/lite/toco/toco, which I don't use.", "Ran into the same problem while I'm trying to load a cv::Mat data to a tensor following this [page](https://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), imread() can not load any image. I even can't find out what caused the problem. It solved fine following @kxhit 's solution.", "Same for me too, Unfortunately \"Videcapture\" function for camera does not work with tensorflow libraries.I use Python for programming and using tegra k1 embedded Nvdia controller card.Tensor flow version is TF 0.8 and opencv version is 2.4.13 .These versions are correctly working on this embedded platform but unfortunately does not work together.Could you please help me about this problem.I didn't use any bazel implementation for TF\r\n\r\n**Solution for Python,TF0.8, Opencv 2.4.13:**\r\n\r\nI solved this problem implementing opencv libraries before tensorflow libraries like;\r\n\r\nFirst -->` import cv2`\r\nSecond --> `import tensorflow as tf`\r\n\r\nIt was totally strange but worked :)", "@allenlavoie hello, I build as follow:\r\nbazel build --config=opt --config=cuda --config=monolithic -c opt --copt=-msse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --verbose_failures //tensorflow:libtensorflow_cc.so\r\n\r\nBut when link libtensorflow_cc.so, opencv imread still not work\uff0cload image is empty", "@zgsxwsdxg which platform, which OpenCV version, which TF commit hash, and are you linking to anything besides libtensorflow_cc.so? Those symbols should be locked down pretty well (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tf_exported_symbols.lds or https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tf_version_script.lds depending on the platform).", "@allenlavoie  Thanks for your reply. \r\n\r\nplatform: ubuntu16.04 LTS\r\nopencv: 3.1\r\ntensorflow: rc1.3\r\ncuda\uff1a8.0\r\ncudnn\uff1a6.0\r\n\r\nI only link opencv\u2019libs besides libtensorflow_cc.so for my project.\r\n\r\nIn addition when I build libtensorfow_cc.so with \u201c--config=monolithic\u201d option\uff0cone warning occurs as follow\uff1a\"WARNING: Config values are not defined in any .rc file: monolithic\"\r\n\r\nWhat should I do with the warning\uff1f\r\n\r\nDoes this have an impact on the question between tensorflow and opencv\uff1f\r\n\r\nLook forward to your reply. Best wishes.", "TensorFlow 1.3 was the release before we removed RTLD_GLOBAL, so that's always monolithic and won't know about the option. Looks like [the C++ API .so had no linker script for that version](https://github.com/tensorflow/tensorflow/blob/408fd454d7d2a16269576ea12bcd516e25a6b0c5/tensorflow/BUILD#L446). So you could either backport the linker script in https://github.com/drpngx/tensorflow/commit/3438981ca7b659e57fb1e15152a1f9fd99b5d6bc or use a more recent version.", "opencv3.4.6 tensorflow1.4 on MacOS is ok, the same libs on unbuntu16.04 is not ok . so I try this code \"bazel build --config=monolithic :libtensorflow_cc.so\""]}]