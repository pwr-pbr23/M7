[{"number": 34238, "title": "Tensorflow failed to find 'TRTEngineOp' when building pip-package and libtensorflow_cc.so", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Build from source\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc 7.4.0\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory: 2080ti 11GB\r\n\r\n**Describe the problem**\r\n\r\nIn brief, I built tensorflow for both python (pip_package) and c++ (libtensorflow_cc.so), I want to train a model, convert it to tf-trt in python, then run it on c++. And it failed when running it on c++.\r\n\r\nHere are the details:\r\n\r\nI'm trying to build tensorflow python version by bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package and build c++ version by bazel build --config=opt --config=monolithic --config=cuda //tensorflow:libtensorflow_cc.so. And I want them both with tensorrt support. After building it sequentially (configure first, build_pip_package and then libtensorflow_cc.so), I find that the python version could load tensorrt successfully, but when I'm trying to run session run on c++, it gives me the error saying:\r\n\r\n```\r\nCheck failed: status.ok() Loading error: Op type not registered 'TRTEngineOp' in binary running on my_dev_docker. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you\r\nare loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\nSince I'm using tf 1.14 where the tensorrt has been promoted to first-class citizen from tf.contrib. So I believe that I don't need to dynatically load it in my c++ code using c api \"TF_LoadLibrary\". So I started to suspect that maybe the tensorrt support is successfully installed but not on the c++ one and I didn't find any good reference on the internet on how to enable tf-trt for tensorflow_cc.so.\r\n\r\nThe success of python tensorflow with tensorrt support is tested by running \"from tensorflow.python.compiler.tensorrt import trt_convert as trt\". Although I have to run \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/TensorRT-5.1.2.2/lib\" first otherwise it will say \"ImportError: libnvinfer.so.5: cannot open shared object file: No such file or directory\".\r\n\r\nThe following is the exact command:\r\n\r\n PYTHON_BIN_PATH=/usr/bin/python3.6 \\                                                                                                                                                                                                                                              \r\n PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \\                                                                                                                                                                                                                          \r\n TF_ENABLE_XLA=1 \\                                                                                                                                                                                                                                                                 \r\n TF_NEED_OPENCL_SYCL=0 \\                                                                                                                                                                                                                                                           \r\n TF_NEED_ROCM=0 \\                                                                                                                                                                                                                                                                  \r\n TF_NEED_CUDA=1 \\                                                                                                                                                                                                                                                                  \r\n TF_CUDA_VERSION=10 \\                                                                                                                                                                                                                                                              \r\n TF_CUDA_PATHS=/usr/local/cuda,/usr/lib/x86_64-linux-gnu,/usr/include \\                                                                                                                                                                                                            \r\n CUDA_TOOLKIT_PATH=/usr/local/cuda \\                                                                                                                                                                                                                                               \r\n CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \\                                                                                                                                                                                                                                    \r\n TF_CUDNN_VERSION=7 \\                                                                                                                                                                                                                                                              \r\n TF_NEED_TENSORRT=1 \\                                                                                                                                                                                                                                                              \r\n TF_TENSORRT_VERSION=5 \\                                                                                                                                                                                                                                                           \r\n TENSORRT_INSTALL_PATH=/usr/local/TensorRT-5.1.2.2 \\                                                                                                                                                                                                                               \r\n TF_NCCL_VERSION=2 \\                                                                                                                                                                                                                                                               \r\n TF_CUDA_COMPUTE_CAPABILITIES=\"6.1,7.5\" \\                                                                                                                                                                                                                                          \r\n TF_CUDA_CLANG=0 \\                                                                                                                                                                                                                                                                 \r\n TF_DOWNLOAD_CLANG=0 \\                                                                                                                                                                                                                                                             \r\n GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\                                                                                                                                                                                                                                             \r\n CLANG_CUDA_COMPILER_PATH=/usr/local/clang_8.0.0/bin/clang \\                                                                                                                                                                                                                       \r\n TF_NEED_MPI=0 \\                                                                                                                                                                                                                                                                   \r\n CC_OPT_FLAGS=\"-mavx -Wno-sign-compare\" \\                                                                                                                                                                                                                                          \r\n TF_SET_ANDROID_WORKSPACE=0 \\                                                                                                                                                                                                                                                      \r\n ./configure                                                                                                                                                                                                                                                                       \r\n                                                                                                                                                                                                                                                                                   \r\n bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package                                                                                                                                                                                           \r\n ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg                                                                                                                                                                                                    \r\n pip3 install /tmp/tensorflow_pkg/tensorflow-1.14.0-cp36-cp36m-linux_x86_64.whl                                                                                                                                                                                                    \r\n                                                                                                                                                                                                                                                                                   \r\n bazel build --config=opt --config=monolithic --config=cuda //tensorflow:libtensorflow_cc.so\r\n\r\n**Any other info / logs**\r\n\r\nI run the conversion in python by:\r\n\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nconverter = trt.TrtGraphConverter(input_saved_model_dir=input_saved_model_dir)\r\nconverter.convert()\r\nconverter.save(output_saved_model_dir)\r\n\r\nI run the converted model in c++ by:\r\n\r\nconst auto status =                                                                                                                                                                                                                                                             \r\n       LoadSavedModel(session_options, run_options, net_param_.model_path(),                                                                                                                                                                                                       \r\n                      {tensorflow::kSavedModelTagServe}, &bundle_, gpu_id);\r\n\r\nAnd I get error message from status.error_message();\r\n\r\nCould you kindly provide some hints on what I might be doing wrong?", "comments": ["Hi @qcraftai, I can see a dependency chain based on:\r\n```\r\nbazel query 'somepath(tensorflow:libtensorflow_cc.so, tensorflow/compiler/tf2tensorrt:trt_conversion)'\r\n```\r\n\r\n. When you run bazel build on tensorflow:libtensorflow_cc.so, can you try adding --config=tensorrt? Or make sure TensorRT is configured during the build by checking that environment variables like TF_NEED_TENSORRT and TENSORRT_INSTALL_PATH are set? ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34238\">No</a>\n"]}, {"number": 34237, "title": "Failed on runtime - image not found", "body": "**System information**\r\n- macOS Mojave 10.14.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2\r\n- Python version: 3.7\r\n- Installed using virtualenv? no\r\n- Bazel version (if compiling from source): bazel release 0.29.1\r\n- GCC/Compiler version (if compiling from source):g++ -v = Apple clang version 11.0.0 (clang-1100.0.33.12)\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: no gpu\r\n\r\n**problem**\r\nI installed Tensorflow from source and used below command to compile my cpp file, It compiled successfully\r\n`/usr/bin/g++  /var/www/cpp/main.cpp -std=c++17 -L /Users/raza/development/tensorflow/bazel-bin/tensorflow/ -l tensorflow_framework -l tensorflow_cc -l protobuf  -I /Users/raza/development/tensorflow -I /Users/raza/development/tensorflow/bazel-bin/tensorflow -I /Users/raza/development/tensorflow/bazel-genfiles -I /Users/raza/development/tensorflow/bazel-tensorflow/external/com_google_protobuf/src -I /Users/raza/development/tensorflow/bazel-tensorflow/external/com_google_absl -I /Users/raza/development/tensorflow/bazel-tensorflow/external/eigen_archive -o /var/www/cpp/main --debug -v`\r\n\r\nBut when I run `./main` I see below error:\r\n```\r\ndyld: Library not loaded: @rpath/libtensorflow_framework.2.dylib\r\n  Referenced from: /private/var/www/cpp/./main\r\n  Reason: image not found\r\n```\r\n\r\n**Any other info / logs**\r\nI see under my path I have this library ~/development/tensorflow/bazel-bin/tensorflow and also other files with similiar names: \r\n```\r\n$ls\r\n__init__.py                         core                                libtensorflow_cc.so.2.0.0           stream_executor\r\n__init__.py.original                libtensorflow.so                    libtensorflow_framework.2.0.0.dylib tools\r\n_api                                libtensorflow.so.2                  libtensorflow_framework.2.dylib     virtual_root.__init__.py\r\nc                                   libtensorflow.so.2.0.0              libtensorflow_framework.dylib\r\ncc                                  libtensorflow_cc.so                 lite\r\ncompiler                            libtensorflow_cc.so.2               python\r\n```\r\nand output of `otool -L libtensorflow_cc.so`\r\n```\r\n$ otool -L libtensorflow_cc.so\r\nlibtensorflow_cc.so:\r\n        @rpath/libtensorflow_cc.so.2 (compatibility version 0.0.0, current version 0.0.0)\r\n        /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 800.7.0)\r\n        @rpath/libtensorflow_framework.2.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1281.0.0)\r\n        /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation (compatibility version 150.0.0, current version 1673.126.0)\r\n        /System/Library/Frameworks/Security.framework/Versions/A/Security (compatibility version 1.0.0, current version 59306.41.2)\r\n        /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)\r\n        /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation (compatibility version 300.0.0, current version 1673.126.0)\r\n        /usr/lib/libobjc.A.dylib (compatibility version 1.0.0, current version 228.0.0)\r\n```\r\n\r\nand\r\n```\r\n$otool -L libtensorflow_framework.2.dylib\r\nlibtensorflow_framework.2.dylib:\r\n        @rpath/libtensorflow_framework.2.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n        /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 800.7.0)\r\n        /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation (compatibility version 150.0.0, current version 1673.126.0)\r\n        /System/Library/Frameworks/Security.framework/Versions/A/Security (compatibility version 1.0.0, current version 59306.41.2)\r\n        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1281.0.0)\r\n        /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)\r\n        /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation (compatibility version 300.0.0, current version 1673.126.0)\r\n        /usr/lib/libobjc.A.dylib (compatibility version 1.0.0, current version 228.0.0)\r\n```", "comments": ["I was able to solve this issue by copying `libtensorflow_framework.2.dylib` and `libtensorflow_cc.so` from my TensorFlow build directory to the same directory as my executable.\r\n\r\nI must also add that C/C++ integration docs are missing or incomplete, so for a new user, it takes a huge time to understand how to build and link properly all the libraries. I hope it gets easier in the future. ", "This problem is still happening. This is not fixed, please do not close not fixed bugs. Also, the solution provided does not work."]}, {"number": 34236, "title": "Add `all` support for autograph with dataset", "body": "This PR is a follow up to PR #34089. In PR #34089 `any`\r\nsupport for autograph with dataset has been added.\r\n\r\nThis PR add `all` in a similar way.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 34235, "title": "tf.keras.backend.gradients error in eager mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): n/a\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130\r\n- GPU model and memory: Tesla P100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUsage of tf.keras.backend.gradients in tensorflow 2.0 gives the following error :\r\n\r\nRuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n\r\nThe entire error is as below : \r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-55-a2847ee8d0c4> in <module>()\r\n     69         ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\r\n     70 \r\n---> 71 vis_img_in_filter()\r\n\r\n3 frames\r\n<ipython-input-55-a2847ee8d0c4> in vis_img_in_filter(img, layer_name)\r\n     30 \r\n     31         # compute the gradient of the input picture wrt this loss\r\n---> 32         grads = K.gradients(loss, model.input)[0]\r\n     33 \r\n     34         # normalization trick: we normalize the gradient\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in gradients(loss, variables)\r\n   3795   \"\"\"\r\n   3796   return gradients_module.gradients(\r\n-> 3797       loss, variables, colocate_gradients_with_ops=True)\r\n   3798 \r\n   3799 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\r\n    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n    157         gate_gradients, aggregation_method, stop_gradients,\r\n--> 158         unconnected_gradients)\r\n    159   # pylint: enable=protected-access\r\n    160 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    503   \"\"\"Implementation of gradients().\"\"\"\r\n    504   if context.executing_eagerly():\r\n--> 505     raise RuntimeError(\"tf.gradients is not supported when eager execution \"\r\n    506                        \"is enabled. Use tf.GradientTape instead.\")\r\n    507   if src_graph is None:\r\n\r\nRuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ksasi ,\r\nThank you for reporting the issue, Can you please provide  code to reproduce the issue reported here from our side?\r\nAlso,Tensorflow 2.0 by default uses Eager-Execution.please deactivate the eager execution and try running the code :\r\n`tf.compat.v1.disable_eager_execution()`\r\nFind this SO [link](https://stackoverflow.com/questions/56478454/in-tensorflow-2-0-with-eager-execution-how-to-compute-the-gradients-of-a-networ) of similar issue and let us know if its was helpful.\r\n", "Hi,\r\n\r\ntf.compat.v1.disable_eager_execution() seems to be helping with the error. This can be a workaround. However , shouldn't \"tf.keras.backend.gradients\" automatically detect the eager vs graph mode and then obtain gradients?\r\n\r\nBelow is the code to reproduce the issue. The \"vis_img_in_filter()\" fails with the error, as eager is enabled by default in tf.keras 2.0\r\n\r\nimport\u00a0numpy\u00a0as\u00a0np\r\nimport\u00a0tensorflow\u00a0as\u00a0tf\r\n\r\nfrom\u00a0tensorflow.keras.models\u00a0import\u00a0Sequential\r\nfrom\u00a0tensorflow.keras.layers\u00a0import\u00a0Dense,\u00a0Dropout,\u00a0Activation,\u00a0Flatten,\u00a0Add\r\nfrom\u00a0tensorflow.keras.layers\u00a0import\u00a0Conv2D,\u00a0MaxPool2D\r\nfrom\u00a0tensorflow.keras\u00a0import\u00a0utils\r\n\r\nfrom\u00a0tensorflow.keras.datasets\u00a0import\u00a0mnist\r\n\r\nprint(tf.version.VERSION)\r\nprint(tf.keras.__version__)\r\n\r\n\r\n(X_train,\u00a0y_train),\u00a0(X_test,\u00a0y_test)\u00a0=\u00a0mnist.load_data()\r\n\r\nX_train\u00a0=\u00a0X_train.astype('float32')\r\nX_test\u00a0=\u00a0X_test.astype('float32')\r\nX_train\u00a0/=\u00a0255\r\nX_test\u00a0/=\u00a0255\r\n\r\n\r\nY_train\u00a0=\u00a0utils.to_categorical(y_train,\u00a010)\r\nY_test\u00a0=\u00a0utils.to_categorical(y_test,\u00a010)\r\n\r\n\r\nfrom\u00a0keras.layers\u00a0import\u00a0Activation\r\nmodel\u00a0=\u00a0Sequential()\r\n\r\nmodel.add(Conv2D(filters\u00a0=\u00a032,\u00a0kernel_size\u00a0=\u00a0(3,\u00a03),\u00a0activation='relu',\u00a0input_shape=(28,28,1),\u00a0use_bias\u00a0=\u00a0False))\r\nmodel.add(Conv2D(filters\u00a0=\u00a010,\u00a0kernel_size\u00a0=\u00a0(1,\u00a01)\u00a0,\u00a0activation='relu',\u00a0use_bias\u00a0=\u00a0False))\r\nmodel.add(Conv2D(filters\u00a0=\u00a010,\u00a0kernel_size\u00a0=\u00a0(26,\u00a026),\u00a0use_bias\u00a0=\u00a0False))\r\nmodel.add(Flatten())\r\nmodel.add(Activation('softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0optimizer='adam',\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0metrics=['accuracy'])\r\n\r\nmodel.fit(X_train,\u00a0Y_train,\u00a0batch_size=128,\u00a0epochs=10,\u00a0validation_data\u00a0=\u00a0(X_test,\u00a0Y_test),\u00a0verbose=1)\r\n\r\n\r\nlayer_dict\u00a0=\u00a0dict([(layer.name,\u00a0layer)\u00a0for\u00a0layer\u00a0in\u00a0model.layers])\r\n\r\n\r\nimport\u00a0numpy\u00a0as\u00a0np\r\nfrom\u00a0matplotlib\u00a0import\u00a0pyplot\u00a0as\u00a0plt\r\nfrom\u00a0tensorflow.keras\u00a0import\u00a0backend\u00a0as\u00a0K\r\n%matplotlib\u00a0inline\r\n#\u00a0util\u00a0function\u00a0to\u00a0convert\u00a0a\u00a0tensor\u00a0into\u00a0a\u00a0valid\u00a0image\r\ndef\u00a0deprocess_image(x):\r\n\u00a0\u00a0\u00a0\u00a0#\u00a0normalize\u00a0tensor:\u00a0center\u00a0on\u00a00.,\u00a0ensure\u00a0std\u00a0is\u00a00.1\r\n\u00a0\u00a0\u00a0\u00a0x\u00a0-=\u00a0x.mean()\r\n\u00a0\u00a0\u00a0\u00a0x\u00a0/=\u00a0(x.std()\u00a0+\u00a01e-5)\r\n\u00a0\u00a0\u00a0\u00a0x\u00a0*=\u00a00.1\r\n\r\n\u00a0\u00a0\u00a0\u00a0#\u00a0clip\u00a0to\u00a0[0,\u00a01]\r\n\u00a0\u00a0\u00a0\u00a0x\u00a0+=\u00a00.5\r\n\u00a0\u00a0\u00a0\u00a0x\u00a0=\u00a0np.clip(x,\u00a00,\u00a01)\r\n\r\n\u00a0\u00a0\u00a0\u00a0#\u00a0convert\u00a0to\u00a0RGB\u00a0array\r\n\u00a0\u00a0\u00a0\u00a0x\u00a0*=\u00a0255\r\n\u00a0\u00a0\u00a0\u00a0#x\u00a0=\u00a0x.transpose((1,\u00a02,\u00a00))\r\n\u00a0\u00a0\u00a0\u00a0x\u00a0=\u00a0np.clip(x,\u00a00,\u00a0255).astype('uint8')\r\n\u00a0\u00a0\u00a0\u00a0return\u00a0x\r\n\r\ndef\u00a0vis_img_in_filter(img\u00a0=\u00a0np.array(X_train[2]).reshape((1,\u00a028,\u00a028,\u00a01)).astype(np.float64),\u00a0\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0layer_name\u00a0=\u00a0'conv2d_1'):\r\n\u00a0\u00a0\u00a0\u00a0layer_output\u00a0=\u00a0layer_dict[layer_name].output\r\n\u00a0\u00a0\u00a0\u00a0img_ascs\u00a0=\u00a0list()\r\n\u00a0\u00a0\u00a0\u00a0for\u00a0filter_index\u00a0in\u00a0range(layer_output.shape[3]):\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0build\u00a0a\u00a0loss\u00a0function\u00a0that\u00a0maximizes\u00a0the\u00a0activation\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0of\u00a0the\u00a0nth\u00a0filter\u00a0of\u00a0the\u00a0layer\u00a0considered\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0loss\u00a0=\u00a0K.mean(layer_output[:,\u00a0:,\u00a0:,\u00a0filter_index])\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0compute\u00a0the\u00a0gradient\u00a0of\u00a0the\u00a0input\u00a0picture\u00a0wrt\u00a0this\u00a0loss\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0grads\u00a0=\u00a0K.gradients(loss,\u00a0model.input)[0]\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0normalization\u00a0trick:\u00a0we\u00a0normalize\u00a0the\u00a0gradient\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0grads\u00a0/=\u00a0(K.sqrt(K.mean(K.square(grads)))\u00a0+\u00a01e-5)\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0this\u00a0function\u00a0returns\u00a0the\u00a0loss\u00a0and\u00a0grads\u00a0given\u00a0the\u00a0input\u00a0picture\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0iterate\u00a0=\u00a0K.function([model.input],\u00a0[loss,\u00a0grads])\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0step\u00a0size\u00a0for\u00a0gradient\u00a0ascent\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0step\u00a0=\u00a05.\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0img_asc\u00a0=\u00a0np.array(img)\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0run\u00a0gradient\u00a0ascent\u00a0for\u00a020\u00a0steps\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for\u00a0i\u00a0in\u00a0range(20):\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0loss_value,\u00a0grads_value\u00a0=\u00a0iterate([img_asc])\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0img_asc\u00a0+=\u00a0grads_value\u00a0*\u00a0step\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0img_asc\u00a0=\u00a0img_asc[0]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0img_ascs.append(deprocess_image(img_asc).reshape((28,\u00a028)))\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\r\n\u00a0\u00a0\u00a0\u00a0if\u00a0layer_output.shape[3]\u00a0>=\u00a035:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plot_x,\u00a0plot_y\u00a0=\u00a06,\u00a06\r\n\u00a0\u00a0\u00a0\u00a0elif\u00a0layer_output.shape[3]\u00a0>=\u00a023:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plot_x,\u00a0plot_y\u00a0=\u00a04,\u00a06\r\n\u00a0\u00a0\u00a0\u00a0elif\u00a0layer_output.shape[3]\u00a0>=\u00a011:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plot_x,\u00a0plot_y\u00a0=\u00a02,\u00a06\r\n\u00a0\u00a0\u00a0\u00a0else:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0plot_x,\u00a0plot_y\u00a0=\u00a01,\u00a02\r\n\u00a0\u00a0\u00a0\u00a0fig,\u00a0ax\u00a0=\u00a0plt.subplots(plot_x,\u00a0plot_y,\u00a0figsize\u00a0=\u00a0(12,\u00a012))\r\n\u00a0\u00a0\u00a0\u00a0ax[0,\u00a00].imshow(img.reshape((28,\u00a028)),\u00a0cmap\u00a0=\u00a0'gray')\r\n\u00a0\u00a0\u00a0\u00a0ax[0,\u00a00].set_title('Input\u00a0image')\r\n\u00a0\u00a0\u00a0\u00a0fig.suptitle('Input\u00a0image\u00a0and\u00a0%s\u00a0filters'\u00a0%\u00a0(layer_name,))\r\n\u00a0\u00a0\u00a0\u00a0fig.tight_layout(pad\u00a0=\u00a00.3,\u00a0rect\u00a0=\u00a0[0,\u00a00,\u00a00.9,\u00a00.9])\r\n\u00a0\u00a0\u00a0\u00a0for\u00a0(x,\u00a0y)\u00a0in\u00a0[(i,\u00a0j)\u00a0for\u00a0i\u00a0in\u00a0range(plot_x)\u00a0for\u00a0j\u00a0in\u00a0range(plot_y)]:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if\u00a0x\u00a0==\u00a00\u00a0and\u00a0y\u00a0==\u00a00:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0continue\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ax[x,\u00a0y].imshow(img_ascs[x\u00a0*\u00a0plot_y\u00a0+\u00a0y\u00a0-\u00a01],\u00a0cmap\u00a0=\u00a0'gray')\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ax[x,\u00a0y].set_title('filter\u00a0%d'\u00a0%\u00a0(x\u00a0*\u00a0plot_y\u00a0+\u00a0y\u00a0-\u00a01))\r\n\r\nvis_img_in_filter()\r\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34235\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34235\">No</a>\n", "@ymodak , I am not sure if this is a usable question. But rather the intended behaviour. Can you confirm if this behaviour is as per the intended design and this is not a bug?"]}, {"number": 34234, "title": "Checkpoint managers overwrite not owned checkpoints", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro linux testing\r\n- TensorFlow installed from: pypi binary\r\n- TensorFlow version (use command below): v1.12.1-16854-g6778662 2.1.0-dev20191028\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nA `tf.train.CheckpointManager` erases checkpoints in its directory path that were not written by it.\r\nThis is a different behaviour from `tf.train.Saver` in TF1.\r\n\r\n**Describe the expected behavior**\r\nDo not remove checkpoints that are not owned by a CheckpointManager instance.\r\nIn the code below, I expected to have checkpoints 1, 2, 4, 5 at the end. Instead, only 4 and 5 survived.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nvar = tf.Variable(initial_value=12)\r\n\r\ncheckpoint = tf.train.Checkpoint(var=var)\r\nmanager = tf.train.CheckpointManager(\r\n    checkpoint=checkpoint,\r\n    directory='./delete_me/',\r\n    max_to_keep=2)\r\n\r\nmanager.save(0)\r\nmanager.save(1)\r\nmanager.save(2)\r\n\r\nmanager2 = tf.train.CheckpointManager(\r\n    checkpoint=checkpoint,\r\n    directory='./delete_me/',\r\n    max_to_keep=2)\r\n\r\nmanager2.save(3)\r\nmanager2.save(4)\r\nmanager2.save(5)\r\n#  Why were 1 and 2 deleted by manager2 ?\r\n```\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["@georgesterpu, In the `tf.train.CheckpointManager()`, change the value of `max_to_keep=5`.\r\nmax_to_keep shows the number of checkpoints to keep. Please refer this [link](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager) for more information. Thanks!", "Hi @gadagashwini \r\nThanks for the reply.\r\nMy issue is slightly different. I think that `manager2` should not erase checkpoints written by `manager`, or in general by any other checkpoint manager.\r\n\r\nThe aim is to keep up to N checkpoints from a set of M managers writing K checkpoints each, where K >> N. You cannot control this with the `max_to_keep` parameter alone, unless it is set to a very large value of the order of M*K.\r\n\r\nI expected each checkpoint manager to keep a list of all the checkpoints it has written so far, and to do the sweep operation only on this list. Instead, it appears that a checkpoint manager inspects its working directory (which can be shared by other managers) and is able to sweep any checkpoint inside, leaving up to N checkpoints there. With `tf.train.Saver` in TensorFlow 1, that directory would contain N * M checkpoints.\r\n\r\nPlease let me know if this explanation is more clear.", "Issue is replicating on colab with TF 2.1.0.dev20191028.\r\nPlease see gist [here](https://colab.sandbox.google.com/gist/gadagashwini/458b64b41d927e2e18318900b306d959/untitled258.ipynb). Thanks!", "@georgesterpu,\r\nAs per @gadagashwini's comment, by using `max_to_keep=10`, all the Checkpoints are retained. Please find the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/729f03581c84f22332f4eb2ff29b47f5/untitled258.ipynb#scrollTo=m9Ca1XPakRK9). Please let me know if you need more information. Thanks!", "Thanks, @rmothukuru,\r\nSorry, I think that I haven't explained this clear enough, please have a look at another snippet below.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nvar = tf.Variable(initial_value=12)\r\n\r\ncheckpoint = tf.train.Checkpoint(var=var)\r\nmanager = tf.train.CheckpointManager(\r\n    checkpoint=checkpoint,\r\n    directory='./delete_me/',\r\n    max_to_keep=2)\r\n\r\nfor i in range(1000):\r\n    manager.save(i)\r\n\r\nmanager2 = tf.train.CheckpointManager(\r\n    checkpoint=checkpoint,\r\n    directory='./delete_me/',\r\n    max_to_keep=2)\r\n\r\nfor i in range(1000, 2000):\r\n    manager2.save(i)\r\n```\r\n\r\nWhat I would like to find in the checkpoint directory is the following list of checkpoints:\r\n`ckpt-998, ckpt-999, ckpt-1998, ckpt-1999`, or the last two checkpoints saved by each manager.\r\nInstead, there are only `ckpt-1998, ckpt-1999`. You are suggesting that the solution to this is to change max to keep to 1000, which will save an unnecessary amount of checkpoints.", "Could reproduce the issue with TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/c350687aa6389346a38ac2e7595c2c7b/untitled262.ipynb). Thanks!", "The checkpoint directory should be viewed as a single unit of work, and in general, we would not advise using the same checkpoint dir for multiple checkpoint managers, for this reason and others. In this case, please use separate directories for each Manager, or adjust params to match the expectations of the checkpoint dir rather than each manager alone.", "Thanks, @karmel, I will update my code in that case.\r\nI have an use case where a new Manager resumes training from the latest checkpoint of a previous Manager, but also need to store the last checkpoint of each manager. Some checkpoint copying script will do it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34234\">No</a>\n"]}, {"number": 34233, "title": "Bad performance in Keras: It diverges if sample_weight is not added to train_in_batch but sample_weight_mode=\"temporal\" is in Model.compile. ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab, with the latest packages.\r\n- TensorFlow installed from (source or binary): Provided by Google Colab.\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.x\r\n\r\n**Describe the current behavior**\r\nThis might not be a bug, but a change has happened in the recent days in the Keras API that has led my models to either divergence to NaNs or reduced performance. Let me explain (long explanation):\r\n\r\nFor the last 3 months, I have been training a Dense U-net to solve the problem from 'Digest Challenge- Conference MICCAI 2019':  **https://digestpath2019.grand-challenge.org/Home/**\r\nI have done many experiments, using Google Colab and Tensorflow 2.0 with integrated Keras. I never specify the version of the packages, so I use whatever Google Colab loads by default. And for months that was perfectly fine. But one week ago, on 7th November 2019, something was changed in the Keras API that all my models went crazy.\r\n\r\nTo give some context, this dataset contains histopathology images, i.e. images of the human colon where cancer cells are present. The dataset is divided into two parts: Negative images, where all tissue is healthy, and Positive images, where cancer tissue is present. Thus, there are two classes of pixels: cancer and non-cancer. The goal is to do segmentation of the cancer tissue. The graph below show the accuracy in the training set (in blue) and in the validation set, where the Positive images (in red) and Negative images (in green) are displayed separately. Usually, the 12 non-stop hours that Google Colab provides allows for 40 iterations. So the following day I load the weights and keep training.\r\n\r\nSince these images are huge, I take patches, build my own batch (trying to balance the classes), and use Keras function _train_on_batch_ for training. Sometimes, I have used the option of _sample_weight_ from _train_on_batch_ to weight the classes even further or simply to play with the weights ...\r\n\r\n```   tLoss = model.train_on_batch(batch_data, batch_label, sample_weight=batch_weight)```\r\nadding the option sample_weight_mode=\"temporal\" in compile() ...\r\n```    \r\nmodel.compile(optimizer=nadam, \r\n             sample_weight_mode=\"temporal\",\r\n             loss= 'binary_crossentropy', \r\n             metrics=['binary_accuracy'])\r\n```\r\nSo far so good!\r\nNow, in the past, I have sometimes forgotten to remove the line _sample_weight_mode=\"temporal\"_ when I was **NOT** using _sample_weight_ in _train_on_batch_, yet there were NO changes in the performance (this is, either removing or not that line in _compile()_ gave the same performance if no actual weighting was introduced in _train_on_batch_ ... or at least I do not remember observing any difference, but I cannot ensure this statement). \r\n\r\nThis was like that until a week ago! Suddenly, if I am NOT using _sample_weight_ but I have the line _sample_weight_mode=\"temporal\"_ in compile(), the network diverges to a fixed loss-value and an accuracy of 0.5 to both classes (sometimes it is not straight forward... maybe after some iterations).\r\n\r\nThen I discovered that I had to remove that line to avoid this. And that makes sense, so it would not be an issue. The problem is that, suddenly, the performance has been reduced a lot (in the Positive images, red line). In the following graph, you could see how the accuracy was going quite well, reaching a plateau in both types of images (Positives and Negatives), until one day that I load the model to keep training and this situation happens (on iteration 175). Since then, the network cannot reach the performance that was obtained before.\r\n\r\n![Accuracy_Validation2019 11 09](https://user-images.githubusercontent.com/52449827/68773542-0b87d380-062c-11ea-8c61-8d468eca45b7.png)\r\n\r\nSo, to summarize, I was training using _train_on_batch_ with NO input argument _sample_weight_ but WITH the line _sample_weight_mode=\"temporal\"_ in compile() (from iteration 0 until iteration 175 in the graph). It went well. At one point in time, that setting made the network to diverge (and I highlight that I did NOT change my code whatsoever, simply load the network and keep training). Once I removed the line _sample_weight_mode=\"temporal\"_ in compile(), the network did not diverge but performance decreased (from iteration 175 forward in the graph).\r\nI have replicated this in other old networks (and in new networks trained from scratch) and the same situation happens.\r\n\r\nIs this a bug? I am not sure!\r\nMy hypotheses: \r\n- Some kind of weighting was internally done before, which was corrected recently, leading to a decrease in my current performance. In this case, I would be interested in knowing what was changed.\r\n- It is a new bug introduced recently.\r\n\r\n\r\n**Code to reproduce the issue**\r\nSee Jupyter Notebook: https://drive.google.com/open?id=1gMnrIe3WBRyEpGumisMMP8ZqqTm5XtX1\r\nThe link to the dataset is within the Notebook.\r\n", "comments": ["@jpviguerasguillen Can you please provide a colab notebook reproducing this issue. Also please try it with TF-nightly and see if it resolves the issue. Thanks!", "The Jupyter notebook to replicate the issue is:\r\nhttps://drive.google.com/open?id=1gMnrIe3WBRyEpGumisMMP8ZqqTm5XtX1\r\n\r\nI have edited the comment before, as I noted that the network does NOT give NaNs but it diverges..", "**UPDATE:**\r\nI am trying tf_nightly (tensorflow version 2.1.0) and the divergence issue seems to be solved (the training metrics look very stable, as it used to be).\r\nTherefore, I am closing this thread.\r\nThanks!"]}, {"number": 34232, "title": "use tensorflow inference on CPU and get error \"undefined symbol: _ZN10tensorflow10DEVICE_CPUE\"", "body": "I use tensorflow to inference lstm crf model on CPU, and I compile a lib use tensorflow C++ API, depend on tensorflow source code.\r\nmy compile environment is GCC 4.8.5, tensorflow 1.12.0, bazel 0.15.0\r\n\r\nwhen I load the so lib, it report an error: undefined symbol: _ZN10tensorflow10DEVICE_CPUE, anybody knows why? or if I must provide more information?", "comments": ["@psnbbgy, Please provide more information about the issue.\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@psnbbgy, Please provide the information asked in the template. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@psnbbgy did you ever solve this? I'm facing it now."]}, {"number": 34231, "title": "@tf.function slower the GPU performance?", "body": "    import time\r\n    import tensorflow as tf\r\n    @tf.function #Commenting out this line makes a huge difference in GPU performance\r\n    def measure(x, steps):\r\n      tf.matmul(x, x)\r\n      start = time.time()\r\n      for i in range(steps):\r\n        x = tf.matmul(x, x)\r\n      end = time.time()\r\n      return end - start\r\n    \r\n    shape = (1000, 1000)\r\n    steps = 2000\r\n    #print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\r\n    \r\n    # Run on CPU:\r\n    with tf.device(\"/cpu:0\"):\r\n      print(\"CPU: {} secs\".format(measure(tf.random.normal(shape), steps)))\r\n    \r\n    # Run on GPU, if available:\r\n    if tf.test.is_gpu_available():\r\n      with tf.device(\"/gpu:0\"):\r\n        print(\"GPU: {} secs\".format(measure(tf.random.normal(shape), steps)))\r\n\r\nGPU:1080ti\r\nCPU:2990wx\r\n\r\nwhen use @tf.function\uff0cresults below\uff1a\r\nCPU: 1.223414659500122 secs\r\nGPU: 1.223414659500122 secs\r\n\r\nwhen don't use @tf.function\r\nCPU: 6.363093614578247 secs\r\nGPU: 0.25109028816223145 secs\r\n\r\nwhy 4x slower ?\r\nwhen should we use  \"@tf.function\" instead of  \"with tf.device(\"/gpu:0\"):\"", "comments": ["I have tried on colab with TF version 2.0  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9513fe1081ff6b3149d6ed6ba9c0e25c/untitled366.ipynb). Thanks!", "```@tf.function``` does graph representation of your python code to deploy your model across various devices. The speed up can be observed on executing computationally intensive tasks and may not happen in simple cases such as matrix multiplication."]}, {"number": 34230, "title": "Android - Posenet is slow/laggy", "body": "**System information**\r\n- I'm using the posenet example\r\n- Samsung J7 Pro running Android 9\r\n\r\n**Describe the current behavior**\r\nThe preview is slow/laggy. The interpreter takes on avarage 300ms when using GPU and 400ms when using CPU\r\n\r\n**Describe the expected behavior**\r\nThe preview should display the frames smoothly\r\n\r\n**Code to reproduce the issue**\r\nI'm running the posenet example application\r\n\r\n**Other info / logs**\r\nThis issue is not present in the Tensorflow Detect example. \r\nI tried reducing the preview size, but the issue remains\r\n\r\n    org.tensorflow.lite.examples.posenet I/posenet: Interpreter took 305,41 ms\r\n    org.tensorflow.lite.examples.posenet I/posenet: Scaling to [-1,1] took 104,03 ms\r\n\r\nPlease let me know if you need more info.\r\n", "comments": ["Currently, this is done intentionally to sync the frame with the estimated pose displayed on screen. We could add an option to toggle this behavior.", "@yyoon That would be great.", "@HBiSoft @yyoon \r\nhi, i'm running posenet example of tensorflow lite.\r\ni got same issue. how can you solve that problem? did you sync the frame with estimated pose displayed on screen? can you share your modified code?\r\ni appreciate for your help\r\n", "@eQueue \r\nI have not yet found a way around this.\r\n\r\n@yyoon \r\n>Currently, this is done intentionally to sync the frame with the estimated pose displayed on screen.\r\n\r\nDoes this mean that we will not be able to detect a pose in real time? Any updates on this issue will be appreciated.", "I thought about two ways to improve.\r\n1. Edit lines 446, 447 and 449 of PosenetActivity.kt\r\n\r\n```\r\n    // Process an image for analysis in every 3 frames.\r\n      frameCounter = (frameCounter + 1) % 3          // delete or comment this line\r\n      if (frameCounter == 0) {                       // delete or comment this line\r\n        processImage(rotatedBitmap)\r\n      }                                             // delete or comment this line\r\n```\r\n\r\n\r\n2.  Eddit PREVIEW_WIDTH and  PREVIEW_HEIGHT of  PosenetActivity.kt\r\nThe lower these two values, the faster. However, it is not recommended due to the loss of accuracy.", "@Nishikoh \r\nI tried both of your suggestions and neither of them made a difference.", "We fixed this app to run with 4-threads CPU now, and it runs at about 10fps on Pixel 3. Should be close to the realtime.", "@lintian06 \r\nI tested the changes and the preview is still \"choppy\". With CPU, I now get around ->\r\n\r\n```\r\nScaling to [-1,1] took 103,96 ms\r\nInterpreter took 176,59 ms\r\n```", "the same problem\uff0cthe preview delay\uff0c why readme gif is affluent", "It may depend on phone models. The test on Pixel 3 is like:\r\n\r\nCPU: 4 threads\r\nScaling to [-1,1] took 46.63 ms\r\nInterpreter took 43.72 ms\r\n~10 fps\r\n\r\nGPU\r\nScaling to [-1,1] took 46.67 ms\r\nInterpreter took 15.28 ms\r\n\r\nIt depends on hardware. You may adjust according to your phone.", "@lintian06 \r\n>It depends on hardware.\r\n\r\nThis is a big issue since there are 4000+ android devices, all with different chipsets.\r\n\r\n>You may adjust according to your phone.\r\n\r\nWhat do you suggest we adjust?\r\n\r\nI have tried:\r\n- Increasing the threads\r\n- Reducing the preview dimensions\r\n- Feeding frames continuously (instead of every third frame)\r\n- Increase/decrease the frame rate\r\n- Running tensorflow/posenet on its own (without drawing anything on the canvas except the preview frame)\r\n\r\nCPU comparison between the Pixel 3 and my device:\r\n---\r\n\r\nGoogle Pixel 3:\r\n`Octa-core (4x2.5 GHz Kryo 385 Gold & 4x1.6 GHz Kryo 385 Silver)`\r\n\r\nSamsung J7 Pro:\r\n`Octa-core 1.6 GHz Cortex-A53`\r\n", "cc/ @terryheo ", "https://github.com/tensorflow/tensorflow/issues/34230#issuecomment-597255383 the change looks good to me. I'll handle it.\r\n\r\n@HBiSoft I think there are two area to improve.\r\n1. Quantization\r\nhttps://www.tensorflow.org/lite/performance/model_optimization\r\nIf you quantized model, you'll get better performance.\r\n2. Optimize scaling\r\nIn your log, scaling takes 100ms. I can see some inefficiency on converting YUV to ARGB and resizing. I think we could get resized ARGB image directly from the camera.\r\nAlso there is a logic creating input tensor in Java layer. We might be able to handle this natively.", "https://github.com/tensorflow/examples/commit/f61466134db544776a1819cbe7af8673c7626216 is merged.\r\n\r\nhttps://github.com/tensorflow/examples/commit/a50d45351c16c990a62beb65cf99fa066eb5cbd9 resolves the scaling issue.\r\nThere is still a room to improve with the quantization but we don't have a concrete plan yet. Let's close this.", "@terryheo \r\nI tested the changes and I could barely see any improvement in performance. It is still \"laggy\".\r\n\r\n    Interpreter took 201,77 ms\r\n    Scaling to [-1,1] took 108,88 ms\r\n\r\nI'm not sure how much quantization will increase performance.", "@HBiSoft Please check if you build & install correctly.\r\nI have a low end device which took 350ms in scaling. Now it only takes 50ms.", "@terryheo \r\n>Please check if you build & install correctly.\r\n\r\nI downloaded [tensorflow examples](https://github.com/tensorflow/examples) then opened the [posenet for android example app](https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android). After that, I made the changes you mentioned above - tensorflow/examples@f614661 and tensorflow/examples@a50d453 - Then ran the app on my device.\r\n\r\nAm I missing something?", "> I downloaded tensorflow examples then opened the posenet for android example app. After that, I made the changes you mentioned above - tensorflow/examples@f614661 and tensorflow/examples@a50d453 - Then ran the app on my device.\r\n\r\nYou don't need to apply changes manually. Plz just use master branch that contains everything you need.\r\n\r\n```\r\na50d453 (HEAD, origin/master, origin/HEAD) Posenet: Use getPixels() to read pixels efficiently\r\nf614661 Posenet: Handle every coming frame\r\n89fa240 Upgrade permission requests.\r\nf026043 ModelMaker Hparams should extend Hub's make_image_classifier_lib Hparams\r\ncfddb05 Migrate to use dataset.cardinality.\r\n36c0ad2 Refactor model_maker utils and TensorFlow 1 tests.\r\n6c3c9b6 Posenet: Use TFLite 2.2.0\r\n```\r\nIf you have these already, how about changing log \"Scaling to [-1,1] took xxx ms\" to a different string to verify if you're running correct version?\r\n", "@terryheo \r\nThank you for your reply. I ran the master branch and I get the following:\r\n\r\n    Interpreter took 204,42 ms\r\n    Scaling to [-1,1] took 26,47 ms\r\n\r\nThe scaling is faster, but the preview is still not as it should be. I guess quantization is the only option to improve this, It looks kinda complicated though. I will have a look into it."]}, {"number": 34229, "title": "after graph_transforms with pb file , something went error ", "body": "## Enviroment\r\n**GPU Type**: Tesla T4\r\n**Nvidia Driver Version**: 418.87.01\r\n**CUDA Version**: 10.1.243\r\n**CUDNN Version**: 7.6.3\r\n**Python Version**: 3.7.4\r\n**TensorFlow Version**: 1.14.1 \r\n**Bazel Version**: 0.24.1\r\n**Operating System Version**: Ubuntu 16.04.6 LTS (GNU/Linux 4.4.0-142-generic x86_64)\r\n\r\n\r\n## Tools\r\n\r\n**graph_transforms**\r\nIt's a toolkit in tensorflow original code (tensorflow/tools/graph_transforms)\r\n\r\n## Step\r\n\r\n### 1. build\r\n\r\n```bash\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\n```\r\n\r\n### 2. transform pb \r\n\r\n```bash\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=my.pb \\\r\n--out_graph=out.pb \\\r\n--inputs='inputs' \\\r\n--outputs='BiasAdd' \\\r\n--transforms='\r\nadd_default_attributes\r\nstrip_unused_nodes(type=float)\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms\r\nround_weights(num_steps=256)\r\nquantize_weights\r\nquantize_nodes\r\nstrip_unused_nodes\r\nsort_by_execution_order'\r\n```\r\n\r\n### 3. test origin pb file i.e. **my.pb**, it's OK\r\n\r\n### 4. test output pb file i.e. **out.pb** , it went error:\r\n\r\n```bash\r\n(0) Invalid argument: requested_output_max must be >= requested_output_min, but got -nan and 0\r\n\t [[{{node Tacotron-2/inference/decoder/while/CustomDecoderStep/mul/eightbit/requantize}}]]\r\n\t [[Tacotron-2/inference/decoder/while/CustomDecoderStep/decoder_LSTM/decoder_LSTM/multi_rnn_cell/cell_0/decoder_LSTM_1/add/_545]]\r\n\r\n(1) Invalid argument: requested_output_max must be >= requested_output_min, but got -nan and 0\r\n\t [[{{node Tacotron-2/inference/decoder/while/CustomDecoderStep/mul/eightbit/requantize}}]]\r\n``` \r\n\r\n## Additional \r\n\r\nI used the following code to test if pb file worked:\r\n```python\r\ndef pb2inference(args):\r\n    tf.reset_default_graph()\r\n    my_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(args.pb, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        my_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(my_graph_def, name = '')\r\n\r\n    my_graph = tf.get_default_graph()\r\n\r\n    inputs = my_graph.get_tensor_by_name('inputs:0')\r\n    out = my_graph.get_tensor_by_name('BiasAdd:0')\r\n\r\n    with tf.Session(graph=my_graph) as sess:\r\n        feed_dict = {inputs:seq}\r\n        out= sess.run(out, feed_dict = feed_dict)\r\n```\r\n\r\nIt worked in my.pb, but got error in out.pb. In addition, the transform process has no warning or error.\r\n", "comments": ["Please attach input pbfile to repro the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34229\">No</a>\n"]}, {"number": 34228, "title": "TFLite_Detection_PostProcess", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jianhuaxiao \r\n\r\nCan you please elaborate about the issue & the context.Will it be possible to provide related code.Thanks!", "@jianhuaxiao \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34227, "title": "Updated docs for resize_images_v2", "body": "Corrected grammatical errors", "comments": ["Since merging PRs is an expensive process due to multiple CI jobs needing to be run, we will not accept PRs that have a delta of just a small number of characters.\r\n\r\nPlease fix an entire file/directory instead of just one typo."]}, {"number": 34226, "title": "Updated docs of resize_images_v2", "body": "Corrected grammatical error", "comments": []}, {"number": 34225, "title": "Corrected grammatical error in CONTRIBUTING.md", "body": "Added missing hyphen for how-to", "comments": ["Since merging PRs is an expensive process due to multiple CI jobs needing to be run, we will not accept PRs that have a delta of just a small number of characters.\r\n\r\nPlease fix an entire file/directory instead of just one typo. Combine this with #34223 and fix the entirety of both files."]}, {"number": 34224, "title": "Rejection resampling with multi-label output", "body": "### Description\r\n\r\nI was reading the about the experimental rejection sampling method and I was wondering. What if I have multi-label binary classes? E.g. [1, 0, ,0, 1, 0, 0, 0, 0, 0, 0]. If I want to even the distribution of each label with `target_dist=[0.1] * 10 ` what is the expected behaviour?\r\n\r\n#### URL(s) with the issue: [Rejection resampling Docs](https://www.tensorflow.org/api_docs/python/tf/data/experimental/rejection_resample)\r\n\r\nThanks in advance!\r\n\r\n\r\n", "comments": ["@qbeer As you see from the doc,\r\n\r\n> target_dist: A floating point type tensor, shaped [num_classes]\r\n\r\nBy the definition of rejection_resampling() , the way to apply multilabel binary classification is you have to check the number of multi label binary classes individually and assign probability based on their occurrences in dataset. \r\n\r\nFor example lets consider there were four pieces of candy (red, yellow).\r\nThe possible combinations are (1,0),(0,1) and (1,1). So, in this case lets say you have 100 samples and the number of occurances of (1,0), (0,1) and (1,1) are 30,30,and 40 then you have 3 classes here with target_dist as [0.3,0.3,0.4]. This is how it works.\r\n\r\nHope this clarifies your doubt. ", "@qbeer Can I close this issue. Let me know if you have any doubts?", "Yes you can close this, thank you for the answer. I understand know that I had to implement the `class_fn` which used an `argmax` operation therefore the resampler only works for exclusive labels. It is not straigthforward to implement it in a fashion I would have liked to but the doubt is answered now."]}, {"number": 34223, "title": "Corrected grammatical error in README.md", "body": "Added missing hyphen for backwards-compatible", "comments": ["Since merging PRs is an expensive process due to multiple CI jobs needing to be run, we will not accept PRs that have a delta of just a small number of characters.\r\n\r\nPlease fix an entire file/directory instead of just one typo. Combine this with #34225 and fix the entirety of both files."]}, {"number": 34222, "title": "ubuntu  cuda10.1 build error. Linking of rule '//tensorflow/python:gen_logging_ops_py_wrappers_cc' failed (Exit 1)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:  v2.0.0-rc2\r\n- Python version: 3.7\r\n- Installed using conda\r\n- Bazel version (if compiling from source):0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: cuda 10.1  cuDNN 7.6.5\r\n- GPU model and memory:  nvida RTX2060\uff0c memory 6GB\r\n\r\ncompile config\uff1a\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/ubuntu/.conda/envs/tensorflow-gpu/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/ubuntu/.conda/envs/tensorflow-gpu/lib/python3.7/site-packages\"\r\nbuild --python_path=\"/home/ubuntu/.conda/envs/tensorflow-gpu/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/home/ubuntu/ThirdpartyLibrary/ode/ode-build/lib:/usr/local/lib:/usr/local/lib/boost:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-7\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\nuse build opt\uff1a\r\nbazel build --config=opt --config=v2 --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\nerror info\uff1a\r\nexternal/com_google_absl/absl/types/optional.h(425): warning: expression has no effect\r\n          detected during instantiation of \"const T &absl::optional<T>::operator*() const & [with T=stream_executor::dnn::AlgorithmDesc]\" \r\n./tensorflow/stream_executor/dnn.h(804): here\r\n\r\nexternal/com_google_absl/absl/types/optional.h(425): warning: expression has no effect\r\n          detected during instantiation of \"const T &absl::optional<T>::operator*() const & [with T=size_t]\" \r\n./tensorflow/stream_executor/dnn.h(858): here\r\n\r\nINFO: From Compiling tensorflow/core/kernels/slice_op_gpu.cu.cc:\r\nexternal/com_google_absl/absl/strings/string_view.h(495): warning: expression has no effect\r\n\r\nexternal/com_google_absl/absl/strings/string_view.h(495): warning: expression has no effect\r\n\r\nERROR: /home/ubuntu/ThirdpartyLibrary/tensorflow/tensorflow/tensorflow/python/BUILD:2444:1: Linking of rule '//tensorflow/python:gen_logging_ops_py_wrappers_cc' failed (Exit 1)\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `google::protobuf::internal::ArenaStringPtr::CreateInstance(google::protobuf::Arena*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const*)':\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0x36): undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAlignedAndAddCleanup(unsigned long, void (*)(void*))'\r\nop_gen_lib.cc:(.text._ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN6google8protobuf8internal14ArenaStringPtr14CreateInstanceEPNS0_5ArenaEPKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0xc0): undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const*, unsigned long) const'\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `tensorflow::(anonymous namespace)::MergeArg(tensorflow::ApiDef_Arg*, tensorflow::ApiDef_Arg const&)':\r\nop_gen_lib.cc:(.text._ZN10tensorflow12_GLOBAL__N_18MergeArgEPNS_10ApiDef_ArgERKS1_+0x4c): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'\r\nop_gen_lib.cc:(.text._ZN10tensorflow12_GLOBAL__N_18MergeArgEPNS_10ApiDef_ArgERKS1_+0x77): undefined reference to `google::protobuf::internal::fixed_address_empty_string[abi:cxx11]'\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `tensorflow::WordWrap[abi:cxx11](absl::string_view, absl::string_view, int)':\r\nop_gen_lib.cc:(.text._ZN10tensorflow8WordWrapB5cxx11EN4absl11string_viewES1_i+0x10a): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&)'\r\nop_gen_lib.cc:(.text._ZN10tensorflow8WordWrapB5cxx11EN4absl11string_viewES1_i+0x20f): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&)'\r\nop_gen_lib.cc:(.text._ZN10tensorflow8WordWrapB5cxx11EN4absl11string_viewES1_i+0x299): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&)'\r\nop_gen_lib.cc:(.text._ZN10tensorflow8WordWrapB5cxx11EN4absl11string_viewES1_i+0x2c6): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `tensorflow::PBTxtFromMultiline[abi:cxx11](absl::string_view)':\r\nop_gen_lib.cc:(.text._ZN10tensorflow18PBTxtFromMultilineB5cxx11EN4absl11string_viewE+0x199): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nop_gen_lib.cc:(.text._ZN10tensorflow18PBTxtFromMultilineB5cxx11EN4absl11string_viewE+0x229): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&)'\r\nop_gen_lib.cc:(.text._ZN10tensorflow18PBTxtFromMultilineB5cxx11EN4absl11string_viewE+0x350): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&)'\r\nop_gen_lib.cc:(.text._ZN10tensorflow18PBTxtFromMultilineB5cxx11EN4absl11string_viewE+0x4a4): undefined reference to `tensorflow::strings::internal::AppendPieces(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::initializer_list<absl::string_view>)'\r\nop_gen_lib.cc:(.text._ZN10tensorflow18PBTxtFromMultilineB5cxx11EN4absl11string_viewE+0x540): undefined reference to `tensorflow::strings::StrAppend(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/host/bin/tensorflow/core/libop_gen_lib.a(op_gen_lib.o): In function `tensorflow::Status tensorflow::errors::FailedPrecondition<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)':\r\nop_gen_lib.cc:\r\n....\r\n...\r\n....\r\n`tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\nlogging_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.69+0x1217): undefined reference to `tensorflow::OpDefBuilder::SetIsStateful()'\r\nlogging_ops.cc:(.text.startup._Z41__static_initialization_and_destruction_0ii.constprop.69+0x1235): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 7445.735s, Critical Path: 194.55s\r\nINFO: 10220 processes: 10220 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n", "comments": ["@ydxt \r\nHow have you resolved this?", "> @ydxt\r\n> How have you resolved this?\r\n\r\nthe problem still exists. I don't have an answer right now.", "Same problem in ubuntu 19.10, same configuration as ydxt", "Seems we're observing something similar after recent Bazel version upgrade.", "Possibly related issue https://github.com/tensorflow/tensorflow/issues/34117", "I confirm that with bazel 0.24.1 does work and the linking error disappears.", "@ydxt \r\n\r\nCan you please confirm if @marcojerome's workaround is working for you.Thanks!", "> I confirm that with bazel 0.24.1 does work and the linking error disappears.\r\n\r\n@marcojerome  @ravikyram Thanks, I will try it with bazel 0.24.1", "JFYI\r\nFor our similar case the problem is not reproducible with TensorFlow master since yesterday (looks like there were some fixes). Bazel version: 0.29.1.\r\nFor example, this TensorFlow commit (95051b912c0b00f9cde62aada17687151944eb0d) is built successfully.", "tensorflow r2.0 with  bazel 0.24.1 successed builded, thank you for your reply. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34222\">No</a>\n"]}, {"number": 34221, "title": "Got different result if train Keras model eagerly", "body": "**Problem**\r\n\r\nI got unexpected result while trying to train a simple Keras model in eagerly mode (so I can debug).  The problem is reproducible on local machine and Colab.  \r\n\r\n**Describe the current behavior**\r\n   Same keras model trained in eager mode, but got different result compared to non-eager mode.\r\n   The model didn't convergent.\r\n\r\n**Describe the expected behavior**\r\n   Result should not depend on eagerly mode On or Off.\r\n\r\n**Code to reproduce the issue**\r\n  [Colab notebook that reproduce problem ](https://colab.research.google.com/drive/1kzCj9vzrOuUnl90eo8pRoJvoeNwqVPP_) \r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n## model don't work if uncomment following line\r\n# tf.config.experimental_run_functions_eagerly(True)\r\n\r\nTRAIN_DATASET = tfds.load(name=\"cifar10\")['train']\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(32,32, 3)),\r\n    tf.keras.layers.Dense(10, activation='softmax'),\r\n])\r\n\r\nBATCH_SIZE = 50\r\n\r\nmodel.compile(loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\ntrain_set = TRAIN_DATASET.map(lambda item: (item['image'], item['label'])).batch(BATCH_SIZE)\r\nmodel.fit(train_set, epochs = 5)\r\n ```\r\n\r\n**Other info / logs**\r\nCorrect Output:\r\n``` \r\nEpoch 1/5\r\n1000/1000 [==============================] - 19s 19ms/step - loss: 330.7991 - accuracy: 0.1935\r\nEpoch 2/5\r\n1000/1000 [==============================] - 15s 15ms/step - loss: 299.7854 - accuracy: 0.2237\r\nEpoch 3/5\r\n1000/1000 [==============================] - 15s 15ms/step - loss: 292.7084 - accuracy: 0.2317\r\n```\r\n\r\nOutput in eagerly mode\r\n```\r\nEpoch 1/5\r\n1000/1000 [==============================] - 30s 30ms/step - loss: 14.5070 - accuracy: 0.0999\r\nEpoch 2/5\r\n1000/1000 [==============================] - 28s 28ms/step - loss: 14.5060 - accuracy: 0.1000\r\nEpoch 3/5\r\n1000/1000 [==============================] - 28s 28ms/step - loss: 14.5060 - accuracy: 0.1000\r\n```\r\n", "comments": ["@xgzeng \r\nCan you try running the code in latest -`tf-nightly 2.1.0.dev20191111` version? Issue seemed to be fixed, kindly find the [gist ](https://colab.sandbox.google.com/gist/ravikyram/c69588901a7fb90c7e0c22f61ff74eb4/untitled367.ipynb)for the same.Thanks!", "@ravikyram  Thanks!\r\nBut tf-nightly 2.1.0.dev20191111 don't work. I tried on local machine and [colab](https://colab.research.google.com/drive/1kzCj9vzrOuUnl90eo8pRoJvoeNwqVPP_).\r\n\r\nThe issue is: \r\nWhen tf.config.experimental_run_functions_eagerly(False) , we can get accuracy around 0.23.\r\nWhen tf.config.experimental_run_functions_eagerly(True) , accuracy stuck around 0.1.\r\n\r\n\r\n\r\n\r\n", "@xgzeng Thanks for reporting this issue. I could reproduce the issue with `tf-nightly`. However, when I ran [your code](https://colab.sandbox.google.com/gist/jvishnuvardhan/cbb0b6512150bf612441fb84aa9ded0f/untitled649.ipynb) with `tf-nightly` (eager mode is default in TF2.x) without `tf.config.experimental_run_functions_eagerly(True)`, then the results matches well with the code where `tf.config.experimental_run_functions_eagerly(False)` is enabled (Graph mode). \r\n\r\nI think this issue may be more related to `Eager` than `keras`. The `tf.config.experimental_run_functions_eagerly` is defined [here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/def_function.py#L254-L271)\r\n\r\n@alextp adding you as it may be of interest to you. Thanks!", "This looks like a bug in the keras training loop. @jvishnuvardhan am I correct in reading your comment that tf-nightly shows no difference in behavior? If so then it's likely this bug has already been fixed.", "@alextp The discrepancy in the results appear only when we use `tf.config.experimental_run_functions_eagerly(True)` to run the model Eagerly. \r\n\r\nIf we don't use `tf.config.experimental_run_functions_eagerly(True)`, then the model runs in Eager mode and there is no discrepancy in the results.\r\n\r\nBased on [docs](`https://www.tensorflow.org/guide/effective_tf2`), `tf.config.experimental_run_functions_eagerly(True)` is used for debugging the code the code. Thanks!", "Ah so there is indeed a bug in the eager version of the keras training loop.\n\nOn Fri, Nov 15, 2019 at 12:50 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> The discrepancy in the results appear\n> only when we use tf.config.experimental_run_functions_eagerly(True) to\n> run the model Eagerly.\n>\n> If we don't use tf.config.experimental_run_functions_eagerly(True), then\n> the model runs in Eager mode and there is no discrepancy in the results.\n>\n> Based on docs, tf.config.experimental_run_functions_eagerly(True) is used\n> for debugging the code the code. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34221?email_source=notifications&email_token=AAABHRNMAELE3U4QQJS3CDTQT4DS5A5CNFSM4JMZJABKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEGVF4Y#issuecomment-554521331>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKMOZJCPUKTLDIU74DQT4DS5ANCNFSM4JMZJABA>\n> .\n>\n\n\n-- \n - Alex\n", "The `lost` during training is totally different for two modes. Maybe the model is not evaluated in same way.", "I wrote some [code](https://colab.research.google.com/drive/1d5d5XpeQ7fW0b7K34HntkD57o4jKLLZ_) to calculate model lost for a batch of data without any training loop.\r\n\r\nThe result is similar to training result. Model lost value is totally different in eager/graph mode.\r\n \r\nSo I think the problem is not in training loop.\r\n", "@pavithrasv @alextp  I though the problem lies in `sparse_categorical_crossentropy` loss function.\r\n\r\nFollowing code in `sparse_categorical_crossentropy` function ('keras/backend.py' file), cause different behavior in eager/graph mode.\r\n\r\n```\r\n  if not from_logits:\r\n    if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\r\n        output.op.type != 'Softmax'):\r\n      epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\r\n      output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\r\n      output = math_ops.log(output)\r\n    else:\r\n      # When softmax activation function is used for output operation, we\r\n      # use logits from the softmax function directly to compute loss in order\r\n      # to prevent collapsing zero when training.\r\n      # See b/117284466\r\n      assert len(output.op.inputs) == 1\r\n      output = output.op.inputs[0]\r\n```\r\n\r\nIf we rewrite model code to skip the optimization, the problem is gone.\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nTRAIN_DATASET = tfds.load(name=\"cifar10\")['train']\r\n\r\ntf.config.experimental_run_functions_eagerly(True) # result won't be affected by eager/graph mode\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(32,32, 3)),\r\n    tf.keras.layers.Dense(10), # no softmax in the layer\r\n])\r\n\r\nBATCH_SIZE = 50\r\n\r\nmodel.compile(loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True), # do softmax in loss function\r\n              metrics=['accuracy'])\r\n\r\ntrain_set = TRAIN_DATASET.map(lambda item: (item['image'], item['label'])).batch(BATCH_SIZE)\r\nmodel.fit(train_set, epochs = 5)\r\n```\r\n\r\n", "Hi, \r\n\r\nThere are a few things going on here, that combine top make this happen:\r\n\r\n------------------------------------------------------------\r\n\r\n1. `crossentropy(y,softmax(x))` will overflow and return NANs easily.\r\n\r\n2. The root of the difference is that, to help beginners, Keras tries to patching this to use the safe \"from_logits=True\" form when it is running in graph mode (inside a `tf.function`). If it's not in graph mode is cannot patch the calculation and falls back to including an `epsilon` to prevent the calculation from overflowing.\r\n\r\n3. We recommend that people people [never use `activation=softmax` of `activation=sigmoid` for classification outputs](https://github.com/tensorflow/docs/commit/0775cca21a59d7f4c8f0a61a9baca1a8f5eb38e1), To avoid exactly this problem. \r\n\r\n4. Often the difference is tiny, as it's only extremely badly classified examples that can trigger this behavior, like assigning a probability of `1e-7` to the true class. And usually only shows up  later in training (or in models with huge numbers of class outputs) because a well initialized classification model should be uncertain about the classifications, and to trigger this you need to be **sure of the wrong answer**.\r\n\r\n    A well initialized model should have an initial crossentropy error around `log(num_classes)` or in this case: `log(10) == 2.3`. \r\n\r\n    Your error starts at ~300. The only way it's possible is if the model is too sure of the wrong answer from the start. it's assigning a probability of ~`0` to the true class in many cases. It is **very sure of the wrong answer**. Why?\r\n\r\n5. Usually the default layer initializations are fine. But these typically assume the inputs have a small mean and variance, typically like `mean=0, var=1` or a uniform distribution on [-1,1] or [0,1]. With small inputs they return small outputs.\r\n\r\n    You're passing the image data without normalizing it. **This has an input range of [0, 255]**.\r\n\r\n    So after running the model the inputs to the softmax function, instead of being similar `mean=0, var=1`,  they're huge  `mean=0, std=150`.  And the `softmax` of that is very certain of a random class.\r\n\r\n----------------------------------------------------\r\nFor your case, to fix this:\r\n\r\n1. Don't include softmax in the oputput layer.\r\n2. Normalize your input data.\r\n\r\n----------------------------------------------------\r\n\r\nI'm not sure what we can do to fix this without breaking a lot of existing code. \r\n\r\nHaving the keras losses print some warnings to explain what they're doing and why might be a good start.", ">For your case, to fix this:\r\n>   Don't include softmax in the oputput layer.\r\n\r\nActually, what I meant to say was:  \"nobody should ever use softmax in the output layer during training. ever.\".\r\nBut other than that advice, everything is working as intended, so I'm closing this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34221\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34221\">No</a>\n"]}, {"number": 34220, "title": "Added Micro Speech support for ESP", "body": "(Work In Progress)\r\nThis PR adds support for micro speech example on ESP. To execute the example you would require an ESP board with an inbuilt microphone  ( preferably ESP-EYE ) .( or you can connect mic externally to any esp board but that will require to make respective changes in audio_provider.cc)\r\n\r\n- [x] To fix some build system issues\r\nmicrofrontend/lib srcs and kissfft are added in CMakeLists.txt of main directory , they need to be removed from srcs makefile variable of main dir (  for now the esp/Makefile.inc adds these srcs in  makefile variable of component/tfmicro/CMakeLists.txt where they should be added ) \r\n- [x] Fix errors in headers for recognize_commands.h\r\na change of path in header micro_model_settings.h which causes error\r\n- [x] A way to copy `sdkconfig.defaults\u2019 and \u2018Kconfig.projbuild\u2019 ( espressif specific files) from `examples/micro_speech/esp/\u2018 to their respective location \r\ndest. for \u2018sdkconfig.defaults\u2019 -> \u2018micro_speech/esp-idf\u2019 ( the project folder)\r\ndest. for \u2018Kconfig.projbuild\u2019 -> \u2018micro_speech/esp-idf/main/\u2018 (`main` folder of the project). ( Note:  Kconfig.projbuild is not required for this project in particular, but it is required for [Person detection](https://github.com/tensorflow/tensorflow/pull/34751)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34220) for more info**.\n\n<!-- need_sender_cla -->", "@fredrec ", "@flyingraijin9 please sign CLA when you are done with changes ", "@fredrec please have a look at the build issues listed in PR description , as I believe you wrote the initial buid templates and helper_functions for esp  in tensorflow.", "@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34220) for more info**.\n\n<!-- ok -->", "Hi Aditya, thank you for this PR. \r\n\r\nAbout the build issue, I will send a patch that will allow to do this more cleanly.\r\n\r\nIn the end I could compile and flash to an ESP-EYE board, but unfortunately could not have it recognize voice commands. In particular, it never recognized `yes`. Did you have more success ?\r\n", "@fredrec , Yes I had it correctly recognising my command most of the times , ( provided the environment was quiet one and I said yes,no in a specific tone ).  ", "@fredrec @rthadur , PTAL , I have updated the `CMakeLists.txt.tpl` fixes from my side, it adds the `O3` flag which was not previously used while compiling , I have also uploaded a video for you to see at this [link](https://drive.google.com/file/d/1AYC7LsKUb1JOaByf1JbYwU1xwvUL7HDD/view) ,  Also Please provide the patch to fix the build issues and if there are no further issues remaining , and Please help merge this MR ASAP , Thank You\r\n\r\n**Please Checkout `esp-idf/master` branch to test the examples**\r\nWhile testing  Please copy `sdkconfig.defaults` from `micro_speech/esp` into `esp-idf` folder and then build it ( provided any sdkconfig or sdkconfig.old in same folder is removed first )\r\n_Note: While testing the example at start it takes roughly about 5-10 seconds to give first output , after that everything is instantaneous_", "> Thanks for your patience and sorry for the long time this PR got unattended.\r\n> \r\n> After some minor modifications I could get the code to build and successfully flash an ESP-EYE board.\r\n> Here is the fix I had to apply: [AdityaHPatwardhan#1](https://github.com/AdityaHPatwardhan/tensorflow/pull/1) Please have a look.\r\n\r\nThank You @fredrec for the build fixes.I have cherry-picked your fixes in my PR, They work fine. \r\nI am afraid There is one more thing required here i.e. a way to copy `sdkconfig.defaults` from `examples/micro_speech/esp` to the home folder of the project (`micro_speech/esp-idf/` ). , I am sorry for not adding this in the checklist of the description above as I had added it in [person detection PR ](https://github.com/tensorflow/tensorflow/pull/34751) ( and I thought these issues would be fixed at same time.).\r\n ", "@fredrec , I have made the necessary changes,The example now builds successfully without any build issue.\r\nPTAL. ", "Everything seems to be working fine.\r\n@petewarden can you please merge this PR ?", "Fixed some Compiler warnings , Example is working properly ( less surrounding noise is preferable)"]}, {"number": 34218, "title": "Add empty linker_bin_path", "body": "Fixes regression introduced in #34202", "comments": ["Possibly unrelated failure:\r\n\r\n> WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/aws/aws-sdk-cpp/archive/1.5.8.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n\r\nfollowed by\r\n> gcc: error: unrecognized command line option '-std=c++14'", "@Flamefire can you please resolve conflicts ?", "Someone already reverted the commit that introduced the bug fixed here. I updated the commit message to the original one now: See #34202", "@Flamefire can you please fix the build failures ?", "This seems to be (again) the error that it can't download  https://storage.googleapis.com/mirror.tensorflow.org/github.com/aws/aws-sdk-cpp/archive/1.5.8.tar.gz. I don't know where this is coming from but certainly not from this PR. I suspect something in the CI setup of TensorFlow has changed? Any ideas?\r\n\r\nThe other is \r\n\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/compiler/xla/BUILD:\r\n1c1\r\n< load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\", \"cc_header_only_library\")\r\n---\r\n> load(\"//tensorflow:tensorflow.bzl\", \"cc_header_only_library\", \"tf_cc_test\")\r\ntensorflow/compiler/jit/BUILD:\r\n1c1\r\n< load(\"//tensorflow:tensorflow.bzl\", \"if_mlir\", \"tf_cc_test\", \"cc_header_only_library\")\r\n---\r\n> load(\"//tensorflow:tensorflow.bzl\", \"cc_header_only_library\", \"if_mlir\", \"tf_cc_test\")\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```\r\n\r\nAgain nothing touched by this PR. Shall I make the suggested changes? I don't understand them though.", "These should be fixed soon, sorry for the delay"]}, {"number": 34217, "title": "Incorrect predictions occur when using a custom quantized model", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: STM32F746 Discovery kit\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:Quadro P4000 8GB\r\n\r\nThis is a issue with TensorFlow Lite for Microcontrollers. Incorrect predictions occur when my ClassifyHeartbeats ( https://github.com/on-device-ai/ClassifyHeartbeats/tree/quantized_model/ ) project uses the quantized model; however, if I use a no quantized model, it is normal predictions.", "comments": ["On running the jupyter notebook [here](https://github.com/on-device-ai/ClassifyHeartbeats/blob/quantized_model/tensorflow/classify_heartbeats_cnn.ipynb), the predictions are the same for the two tflite models (tflite and quantized_tflite). \r\n\r\nIs the issue only occurring when you run it on the STM32F746 Discovery kit?\r\n\r\nThank you providing a well documented repository. Is it also possible to provide instructions you've used for running your C model file on the microcontroller?", "Yes, the issue happens the TensorFlow Lite for Microcontrollers is run on the STM32F746 discovery kit. About the C model running, used the Hello World example to generate the ARM Mbed  (please see [NOTE.txt](https://github.com/on-device-ai/ClassifyHeartbeats/blob/quantized_model/NOTE.txt)) and \"manually\" modify it to the ClassifyHeartbeats project. The modified files are at the \"[mbed](https://github.com/on-device-ai/ClassifyHeartbeats/tree/quantized_model/mbed)\" folder. Please see [RESULT.txt](https://github.com/on-device-ai/ClassifyHeartbeats/blob/quantized_model/RESULT.txt) get the running result (including no quantized and quantized. No quantized version \"//#define USE_QUANTIZED\" result is correct).", "Hello, could you confirm if you are still facing this issue? I am yet to reproduce the issue on the micro-controller but it seems to be working on the jupyter notebook.", "I used the float point model with the last TensorFlow source code doing the demo because the quantized model has the same issue for prediction. I just joined the tinyML Summit 2020, and I will take the time to confirm it again.  \r\n![200218](https://user-images.githubusercontent.com/44540872/74725841-69563800-5279-11ea-828d-4e0461c4e6d3.png)", "I got Daniel Situnayake's suggestion at tensorflow/sig-micro-Gitter, and set a parameter (need to provide a representative dataset) when converting to the tflite model, which solved the problem. The source code at https://github.com/on-device-ai/ClassifyHeartbeats/tree/quantized_model . This Issue can be closed, thank you.", "We're updating the examples to reflect this change. Glad that you were able to fix this issue! "]}, {"number": 34216, "title": "Cmake issue with cuDNN; Docker container doesn't seem to have cuDNN?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Docker, `docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu-py3`\r\n\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.8 \r\n- Installed using virtualenv? pip? conda?: Docker\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n\r\ncudNN: does not seem to come with Docker image. I am trying to use\r\n\r\n`sudo docker cp libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb [container ID]:/ && sudo docker cp libcudnn7-dev_7.6.5.32-1+cuda10.0_amd64.deb [container ID]:/ && sudo docker cp libcudnn7-doc_7.6.5.32-1+cuda10.0_amd64.deb [container ID]:/` on the host then\r\n\r\n`dpkg -i libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb && dpkg -i libcudnn7-dev_7.6.5.32-1+cuda10.0_amd64.deb && dpkg -i libcudnn7-doc_7.6.5.32-1+cuda10.0_amd64.deb`\r\n\r\n- GPU model and memory: RTX 2060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nIf I don't install cuDNN manually, dlib compiles \"successfully\" but will not use CUDA. If I install cudNN via dpkg, I get this error during the build:\r\n\r\n-- Using CMake version: 3.10.2\r\n-- Compiling dlib version: 19.18.99\r\n-- Found system copy of libpng: /usr/lib/x86_64-linux-gnu/libpng.so;/usr/lib/x86_64-linux-gnu/libz.so\r\n-- Found system copy of libjpeg: /usr/lib/x86_64-linux-gnu/libjpeg.so\r\n-- Looking for cuDNN install...\r\n-- Found cuDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so\r\n-- Enabling CUDA support for dlib.  DLIB WILL USE CUDA\r\n-- C++11 activated.\r\nCMake Error: The following variables are used in this project, but they are set to NOTFOUND.\r\nPlease set them or make sure they are set and tested correctly in the CMake files:\r\nCUDA_cublas_LIBRARY (ADVANCED)\r\n    linked by target \"dlib\" in directory /home/project/dlib/dlib\r\nCUDA_curand_LIBRARY (ADVANCED)\r\n    linked by target \"dlib\" in directory /home/project/dlib/dlib\r\nCUDA_cusolver_LIBRARY (ADVANCED)\r\n    linked by target \"dlib\" in directory /home/project/dlib/dlib\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/home/project/dlib/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/home/project/dlib/build/CMakeFiles/CMakeError.log\".\r\nroot@c7f1e218bc6f:/home/project/dlib/build# \r\n\r\nI also tried\r\n\r\n`sudo docker cp cudnn-10.0-linux-x64-v7.6.5.32.tgz [c7f1e218bc6f]:/ `\r\n\r\n`tar -xzvf cudnn-10.0-linux-x64-v7.6.5.32.tgz && cp cuda/include/cudnn.h /usr/local/cuda/include && cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 && chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*`\r\n\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/include\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`sudo docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu-py3 #run container`\r\n\r\n`python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\" #hello world`\r\nnote: this^ runs just fine.\r\n\r\n`sudo docker cp libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb [container ID]:/ && sudo docker cp libcudnn7-dev_7.6.5.32-1+cuda10.0_amd64.deb [container ID]:/ && sudo docker cp libcudnn7-doc_7.6.5.32-1+cuda10.0_amd64.deb [container ID]:/`\r\n\r\n`dpkg -i libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb && dpkg -i libcudnn7-dev_7.6.5.32-1+cuda10.0_amd64.deb && dpkg -i libcudnn7-doc_7.6.5.32-1+cuda10.0_amd64.deb`\r\n\r\n`apt update && apt upgrade && apt install git cmake libx11-dev nano\r\ncd home && mkdir project && cd project \r\ngit clone https://github.com/davisking/dlib.git #setup dlib`\r\n\r\n`cd dlib && mkdir build; cd build; cmake ..; cmake --build .`\r\n\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nCMakeOutput.log: https://pastebin.com/QC1kdejw\r\nCMakeError.log: https://pastebin.com/JfQc02Qb\r\n", "comments": ["Can you try using bazel instead of cmake and let us know how it progresses. Thanks!", "@IntegralWorks \r\n\r\nAny update on this issue please. Thanks!", "@ravikyram\nSorry, new to GitHub and I'm busy this week with examinations. I hope this\nmessage reaches you; I'm replying on email.\nUnfortunately I decided using Docker for my project was not worth it and I\nended up successfully installing dlib with CUDA support on bare\nmetal/regular install.\nI noticed that the key difference between my regular install and the Docker\ncontainer is that the container seems to refuse multiple versions of cuDNN.\nIf you're interested in how this issue turns out I highly suggest finding\nsomeone who needs cuDNN in their project and wants to use the Tensorflow\nGPU container.\n\nEven stranger is that nvcc works out the box for the Tensorflow container,\nwhereas for a manual install the user-developer must meticulously follow\nnVidia's documentation to the letter, such as adding things to .bashrc and\nthe such. But cuDNN seems broken even upon installation either through the\ntar file or the deb package. With a properly, meticulously installed\nregular CUDA installation cuDNN works seamlessly--the point I'm trying to\nmake is I might so bold as to suggest the Tensorflow GPU container might\nhave a semi-broken CUDA installation. Of course, it's more likely that I\njust don't know how to install cuDNN on the Tensorflow GPU container\nproperly--but there doesn't seem to be any documentation(!) on this edge\ncase(?) I don't see how it could be an edge case if dlib isn't some obscure\nlibrary.\n\ntl;dr\nI have no idea how to use Bazel, I've been trained to use gcc, clang, cmake\netc. All compilation methods I know of cause dlib to *detect *CUDA, only to\nsay \"cuDNN is broken, sorry\". Therefore I want to emphasize that right now*\ndlib does **not compile with GPU support on the Tensorflow GPU Docker\nContainer, *despite the fact that *intuitively* you'd expect it to just be\nplug-and-play, at least after installing cuDNN via the *documented\ninstructions on nVidia's websites.*\n\nOn Wed, Nov 20, 2019 at 12:03 AM ravikyram <notifications@github.com> wrote:\n\n> @IntegralWorks <https://github.com/IntegralWorks>\n>\n> Any update on this issue please. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34216?email_source=notifications&email_token=AGOWRXBB73ZPW3VJ6DYPP4LQUTHJRA5CNFSM4JMTBNM2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEQZ4HA#issuecomment-555851292>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGOWRXDE2CIGFGYNR43XHW3QUTHJRANCNFSM4JMTBNMQ>\n> .\n>\n", "@IntegralWorks \r\n\r\nCUDA and cuDNN can be installed by following these [instructions](https://www.tensorflow.org/install/gpu#install_cuda_with_apt). Thanks!", "@IntegralWorks \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34216\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34216\">No</a>\n"]}, {"number": 34215, "title": "build tf2.0 with gcc4.8.5", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n```\r\n[tensorflow]$ gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n[tensorflow]$ g++ -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-redhat-linux/4.8.5/lto-wrapper\r\nTarget: x86_64-redhat-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,objc,obj-c++,java,fortran,ada,go,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-redhat-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n```\r\n\r\nI change the std++14 to std++11 in tensorflow/.bazelrc, and compile with bazel build -s -c opt //tensorflow:libtensorflow_cc.so\r\n\r\n**Other info / logs**\r\n```\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/aws/_objs/aws/DeleteBucketMetricsConfigurationRequest.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/aws/_objs/aws/DeleteBucketMetricsConfigurationRequest.pic.o' -fPIC -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote external/aws -iquote bazel-out/k8-opt/bin/external/aws -iquote external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include '-std=c++11' '-DAWS_SDK_VERSION_MAJOR=1' '-DAWS_SDK_VERSION_MINOR=5' '-DAWS_SDK_VERSION_PATCH=8' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/aws/aws-cpp-sdk-s3/source/model/DeleteBucketMetricsConfigurationRequest.cpp -o bazel-out/k8-opt/bin/external/aws/_objs/aws/DeleteBucketMetricsConfigurationRequest.pic.o)\r\nERROR: /home/odin/ethanzhanghui/.cache/bazel/_bazel_ethanzhanghui/6fbeba4478bcebc2af8a7fbe84821865/external/llvm/BUILD.bazel:3399:1: C++ compilation of rule '@llvm//:support' failed (Exit 1)\r\nIn file included from external/llvm/include/llvm/ADT/ArrayRef.h:15:0,\r\n                 from external/llvm/include/llvm/ADT/DenseMapInfo.h:16,\r\n                 from external/llvm/include/llvm/ADT/DenseMap.h:16,\r\n                 from external/llvm/include/llvm/ADT/DenseSet.h:16,\r\n                 from external/llvm/lib/Support/DynamicLibrary.cpp:15:\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:555:49: error: 'std::index_sequence' has not been declared\r\n   template <size_t... Ns> value_type deref(std::index_sequence<Ns...>) const {\r\n                                                 ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:555:63: error: expected ',' or '...' before '<' token\r\n   template <size_t... Ns> value_type deref(std::index_sequence<Ns...>) const {\r\n                                                               ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:560:36: error: 'std::index_sequence' has not been declared\r\n   decltype(iterators) tup_inc(std::index_sequence<Ns...>) const {\r\n                                    ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:560:50: error: expected ',' or '...' before '<' token\r\n   decltype(iterators) tup_inc(std::index_sequence<Ns...>) const {\r\n                                                  ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:565:36: error: 'std::index_sequence' has not been declared\r\n   decltype(iterators) tup_dec(std::index_sequence<Ns...>) const {\r\n                                    ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:565:50: error: expected ',' or '...' before '<' token\r\n   decltype(iterators) tup_dec(std::index_sequence<Ns...>) const {\r\n                                                  ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: In member function 'llvm::detail::zip_common<ZipType, Iters>::value_type llvm::detail::zip_common<ZipType, Iters>::operator*()':\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:572:41: error: 'index_sequence_for' is not a member of 'std'\r\n   ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: In member function 'llvm::concat_iterator<ValueT, IterTs>& llvm::concat_iterator<ValueT, IterTs>::operator++()':\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:897:15: error: 'index_sequence_for' is not a member of 'std'\r\n     increment(std::index_sequence_for<IterTs...>());\r\n               ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:897:45: error: expected primary-expression before '...' token\r\n     increment(std::index_sequence_for<IterTs...>());\r\n                                             ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: In member function 'ValueT& llvm::concat_iterator<ValueT, IterTs>::operator*() const':\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:902:16: error: 'index_sequence_for' is not a member of 'std'\r\n     return get(std::index_sequence_for<IterTs...>());\r\n                ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:902:46: error: expected primary-expression before '...' token\r\n     return get(std::index_sequence_for<IterTs...>());\r\n                                              ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:902:52: error: there are no arguments to 'get' that depend on a template parameter, so a declaration of 'get' must be available [-fpermissive]\r\n     return get(std::index_sequence_for<IterTs...>());\r\n                                                    ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:902:52: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: At global scope:\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:926:52: error: 'std::index_sequence' has not been declared\r\n   template <size_t... Ns> iterator begin_impl(std::index_sequence<Ns...>) {\r\n                                                    ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:926:66: error: expected ',' or '...' before '<' token\r\n   template <size_t... Ns> iterator begin_impl(std::index_sequence<Ns...>) {\r\n                                                                  ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:929:50: error: 'std::index_sequence' has not been declared\r\n   template <size_t... Ns> iterator end_impl(std::index_sequence<Ns...>) {\r\n                                                  ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:929:64: error: expected ',' or '...' before '<' token\r\n   template <size_t... Ns> iterator end_impl(std::index_sequence<Ns...>) {\r\n                                                                ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: In member function 'llvm::detail::concat_range<ValueT, RangeTs>::iterator llvm::detail::concat_range<ValueT, RangeTs>::begin()':\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:938:40: error: 'index_sequence_for' is not a member of 'std'\r\n   iterator begin() { return begin_impl(std::index_sequence_for<RangeTs...>{}); }\r\n                                        ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:938:71: error: expected primary-expression before '...' token\r\n   iterator begin() { return begin_impl(std::index_sequence_for<RangeTs...>{}); }\r\n                                                                       ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: In member function 'llvm::detail::concat_range<ValueT, RangeTs>::iterator llvm::detail::concat_range<ValueT, RangeTs>::end()':\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:939:36: error: 'index_sequence_for' is not a member of 'std'\r\n   iterator end() { return end_impl(std::index_sequence_for<RangeTs...>{}); }\r\n                                    ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:939:67: error: expected primary-expression before '...' token\r\n   iterator end() { return end_impl(std::index_sequence_for<RangeTs...>{}); }\r\n                                                                   ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: At global scope:\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1507:46: error: 'std::index_sequence' has not been declared\r\n auto apply_tuple_impl(F &&f, Tuple &&t, std::index_sequence<I...>)\r\n                                              ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1507:60: error: expected ',' or '...' before '<' token\r\n auto apply_tuple_impl(F &&f, Tuple &&t, std::index_sequence<I...>)\r\n                                                            ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1520:5: error: 'make_index_sequence' is not a member of 'std'\r\n     std::make_index_sequence<\r\n     ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1520:5: error: 'make_index_sequence' is not a member of 'std'\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1521:66: error: expected primary-expression before '{' token\r\n         std::tuple_size<typename std::decay<Tuple>::type>::value>{})) {\r\n                                                                  ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1521:66: error: expected ')' before '{' token\r\nexternal/llvm/include/llvm/ADT/STLExtras.h: In function 'decltype (llvm::detail::apply_tuple_impl(forward<F>(f), forward<Tuple>(t), ((<expression error> < std::tuple_size<typename std::decay<_Tp2>::type>::value) > <expression error>))) llvm::apply_tuple(F&&, Tuple&&)':\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1522:19: error: expected type-specifier\r\n   using Indices = std::make_index_sequence<\r\n                   ^\r\nexternal/llvm/include/llvm/ADT/STLExtras.h:1526:35: error: 'Indices' was not declared in this scope\r\n                                   Indices{});\r\n                                   ^\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 7.772s, Critical Path: 5.70s\r\nINFO: 138 processes: 138 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["I'm getting this too under similar conditions. Massive flow of errors.", "have the same error under ubuntu 1804 and gcc 7.5.0", "@zh794390558,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34215\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34215\">No</a>\n"]}, {"number": 34214, "title": "Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution ( Linux Ubuntu 16.04):\r\n- TensorFlow installed from (pip):\r\n- TensorFlow version: 1.14.0 gpu\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? also have anaconda\r\n- CUDA/cuDNN version: cuda 10.0 ,cudnn how to know the version ?\r\n- GPU model and memory:\r\nGPU RTX 2080 10989MiB\r\n\r\n```\r\nTrain on 15285 samples, validate on 3822 samples\r\nEpoch 1/100\r\n2019-11-13 11:58:28.507273: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-13 11:58:28.790550: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-13 11:58:28.791219: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2019-11-13 11:58:28.791275: E tensorflow/stream_executor/cuda/cuda_dnn.cc:337] Possibly insufficient driver version: 410.48.0\r\n2019-11-13 11:58:28.791290: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2019-11-13 11:58:28.791312: E tensorflow/stream_executor/cuda/cuda_dnn.cc:337] Possibly insufficient driver version: 410.48.0\r\nTraceback (most recent call last):\r\n  File \"main_ResNet.py\", line 229, in <module>\r\n    shuffle=True)\r\n  File \"/./anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 1239, in fit\r\n    validation_freq=validation_freq)\r\n  File \"/./anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\", line 196, in fit_loop\r\n    outs = fit_function(ins_batch)\r\n  File \"/./anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/./anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}}]]\r\n\t [[Mean/_417]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_1/convolution}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\n```\r\n\r\nany advice or suggestion will be appriciated.\r\nThx\r\n", "comments": ["@ucasiggcas ,\r\nCan you please go through this [link](https://stackoverflow.com/questions/43147983/could-not-create-cudnn-handle-cudnn-status-internal-error) and let us know if it was useful?Thanks!", "`sudo rm -rf ~/.nv/`\r\nthis is no use", "@ucasiggcas ,\r\nCan you share a simple and standalone code to reproduce the issue? thanks!", "First thing will be to check if compatible CUDA, cuDNN drivers are installed correctly. Then you may try gpu memory resources management by allowing gpu memory growth.  \r\n\r\n**allow_growth** option, attempts to allocate only as much GPU memory based on runtime allocations: it starts out allocating very little memory, and as Sessions get run and more GPU memory is needed, it extend the GPU memory region needed by the TensorFlow process.\r\n  \r\nTo know more see https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth   \r\nYou can try Allowing GPU memory growth with:\r\n```python\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n```", "@ymodak \r\nThanks\uff0c\r\nI have tried this,but not success\r\nof course ,I checked the Version of the CUDA and cuDNN, they are compatible,\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0  \r\n```\r\nand I install cuDNN by\r\n`dpkg -i libcudnn7_7.6.5.32-1+cuda10.0_amd64.deb`\r\nIf you could read chinese, see this [ blog](https://blog.csdn.net/SPESEG/article/details/103042752) for details.\r\n\r\nthanks for your kind help\r\n\r\n", "@oanush \r\nSorry, this problem is about **Install tf-gpu** ,not the code \r\n", "Today  I have tried again,\r\ninstalled tf-gpu==1.14 by conda in new environment\r\nbut the error makes me confused,\r\n\r\nbut when I use tf-gpu==1.15, I got success, so I close the issue now", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34214\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34214\">No</a>\n"]}, {"number": 34213, "title": "MultiWorkerMirroredStrategy distribution error BaseCollectiveExecutor::StartAbort Invalid argument: Lower bound check fail for input 1 from node Mkl2Tf/_30 to node scoped_allocator_concat_1_1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no-ish, [keras multi-worker mirrored example](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora Server 31\r\n- TensorFlow installed from (source or binary): source (master branch, commit 73a34133f6a414a03e54971f4975584c3d6251cc), identical on both machines\r\n- TensorFlow version (use command below): v1.12.1-17924-g73a34133f6 2.0.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): GCC 9.2.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nCrashes at model.fit\r\n\r\n**Describe the expected behavior**\r\nNot, crash?\r\n\r\n**Code to reproduce the issue**\r\nExact code used\r\n[Node0](https://pastebin.com/CtTjs146)\r\n[Node1](https://pastebin.com/5LtGA6Hd)\r\n\r\n**Other info / logs**\r\nLogs from run\r\n[Node0](https://pastebin.com/HQAQwtBg)\r\n[Node1](https://pastebin.com/ZFUjWkzf)\r\n\r\nTo compile on Fedora 31 I did need to use a grpc version patch, this patch can be found [here](https://github.com/tensorflow/tensorflow/issues/33758#issuecomment-547867642) in issue #33758. It consists of running this command before compiling tensorflow.\r\n```\r\ncurl -L https://github.com/tensorflow/tensorflow/compare/master...hi-ogawa:grpc-backport-pr-18950.patch | git apply\r\n```\r\n\r\nbazel configured with Python 3 directory, and default otherwise.\r\nbazel build command used is as follows\r\n```\r\nbazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```", "comments": ["This seems to be a MKL related issue, as I got it working by recompiling without --config=mkl in the build, though with some seeming unrelated dataset overrun issues there", "@TensorFlow-MKL Could you please help take a look? Thank you very much!", "we are looking into this issue. will keep you posted on our findings shortly", "@preethivenkatesh Any updates?", "We have an engineer looking into this issue now. I'll keep you posted", "@preethivenkatesh Thank you very much!", "any updates?", "We looked at the issue. The issue is coming form the fact that Intel optimization does not support Scope allocator optimization. Scope allocator optimization is applied by default to Tensorflow's own distributed strategy. We can temporarily disable this optimization (scope allocator optimization) from multi-worker distribution strategy when used Intel optimization. The example should still run correctly. Will this be ok? Moreover, we are planning to support Scope allocator optimization for Intel Optimized Tensorflow in few months, and then we will again enable Scope allocator optimization. ", "I'm not in need of the fix right now, I just ran into this issue, so do whatever you believe is best.", "@RogueLogix Is it possible that you close this issue?", "@RogueLogix Could you close this issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34213\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34213\">No</a>\n", "@RogueLogix  Thank you very much! ", "@ashraf-bhuiyan seems TF 2.4 MKL build still has the issue. I can reproduce using https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\r\nAs a quick workaround, how can I disable scope allocator optimization from multi-worker distribution strategy? Any macro I can set? Or any patch I can apply to re-compile TF 2.4. Thank you!\r\n\r\n File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Upper bound check fail for input 5 from node Mkl2Tf/_30 to node scoped_allocator_concat_1_4 input bounds = [0x7fd53c18ff00, 0x7fd53c18ff80] backing_tensor bounds = [0x7fd4af3dc740, 0x7fd4afe6e280]\r\n\t [[{{node scoped_allocator_concat_1_4}}]] [Op:__inference_train_function_740]\r\n\r\n"]}, {"number": 34212, "title": "[INTEL MKL] parallel implementation of the resize nearest neighbor op", "body": "The origin resize_nearest_neighbor_op is running the resize operation in serial and here is the benchmark test result:\r\n```\r\nExecuting tests from //tensorflow/core/kernels:resize_benchmark_test\r\n-----------------------------------------------------------------------------\r\n2019-11-12 11:50:12.127157: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nRunning main() from test_main.cc\r\nBenchmark                                        Time(ns) Iterations\r\n--------------------------------------------------------------------\r\nBM_Resize_ResizeNearestNeighbor_cpu_10_499_499   48277810        100     154.7M items/s\r\nBM_Resize_ResizeBilinear_cpu_10_499_499          39131850        100     190.9M items/s\r\n```\r\n\r\nHere we modified this operation into parallel implementation and here is the new benchmark test result:\r\n```\r\nExecuting tests from //tensorflow/core/kernels:resize_benchmark_test\r\n-----------------------------------------------------------------------------\r\n2019-11-12 11:54:53.781602: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nRunning main() from test_main.cc\r\nBenchmark                                        Time(ns) Iterations\r\n--------------------------------------------------------------------\r\nBM_Resize_ResizeNearestNeighbor_cpu_10_499_499    5076820        100     1471.4M items/s\r\nBM_Resize_ResizeBilinear_cpu_10_499_499          38758780        100     192.7M items/s\r\n```\r\n\r\nHere we see almost **10x** performance improvement with this PR.", "comments": []}, {"number": 34211, "title": "No gradient defined error when training Sequential model restored from SavedModel with `tf.keras.models.load_model`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS  10.15.1**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.0.0**\r\n- Python version: **3.7.3**\r\n- CUDA/cuDNN version: **none**\r\n- GPU model and memory: **none**, **(MacBook pro iCore i7, 16 GB)**\r\n\r\n\r\n**Describe the current behavior**\r\nWhen training a Sequential model (Embedding + LSTM + Dense layer) restored from SavedModel with load_model(`model = tf.keras.models.load_model('imodel_saved')`, I am getting  the following error : `LookupError: No gradient defined for operation 'while' (op type: While)`.\r\n\r\nI am able to train the same model, if the model was previously saved in `h5` format (`tf.keras.models.save_model(model, 'imdb_model', include_optimizer=True, save_format='h5` ). However it should work with `save_format='tf`` as well. It works for other models (e.g. convolutional model) but not for this specific model:\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(Embedding(20000, 128))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nWe should be able to train the model when restored from SavedModel.\r\n\r\n**Code to reproduce the issue**\r\n\r\nFirst run this script to define and save the Sequential model:\r\n```\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense,Embedding\r\nfrom tensorflow.keras.layers import LSTM\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(20000, 128))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\ntf.keras.models.save_model(model, \r\n                          'imdb_model', \r\n                           include_optimizer=True, \r\n                           save_format='tf')\r\n```\r\n\r\nThen run the following script to restore the model and train it on the IMDB dataset:\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing import sequence\r\n\r\n(x_train, y_train), _ = imdb.load_data(num_words=20000)\r\nx_train = sequence.pad_sequences(x_train, maxlen=80)\r\n\r\nmodel = tf.keras.models.load_model('imdb_model')\r\n\r\nmodel.fit(x_train, y_train, epochs=1)\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 582, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 629, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 619, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 350, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 715, in _registered_grad_fn\r\n    return self._rewrite_forward_and_call_backward(op, *doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 661, in _rewrite_forward_and_call_backward\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 582, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 629, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 619, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 637, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'while' (op type: While)\r\n yanndupis@yanns-MBP \ue0b0 ~/Documents/dropoutlabs/project-cowbay/example/medical-symptoms \ue0b0 \ue0a0 model-training \u25cf ? \u235f1 \ue0b0\r\n yanndupis@yanns-MBP \ue0b0 ~/Documents/dropoutlabs/project-cowbay/example/medical-symptoms \ue0b0 \ue0a0 model-training \u25cf ? \u235f1 \ue0b0 clear\r\n yanndupis@yanns-MBP \ue0b0 ~/Documents/dropoutlabs/project-cowbay/example/medical-symptoms \ue0b0 \ue0a0 model-training \u25cf ? \u235f1 \ue0b0 python train_model.py\r\n\r\n2019-11-12 17:36:48.074011: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-12 17:36:48.086305: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff129c9cae0 executing computations on platform Host. Devices:\r\n2019-11-12 17:36:48.086321: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nTrain on 25000 samples\r\n   32/25000 [..............................] - ETA: 1:28Traceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2383, in get_attr\r\n    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 345, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2387, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2383, in get_attr\r\n    c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 345, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2387, in get_attr\r\n    raise ValueError(str(e))\r\nValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 621, in _GradientsHelper\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2541, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/registry.py\", line 97, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: While\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 11, in <module>\r\n    model.fit(x_train, y_train, epochs=1)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 73, in distributed_function\r\n    per_replica_function, args=(model, x, y, sample_weights))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 760, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1787, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2132, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 311, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 268, in _process_single_batch\r\n    grads = tape.gradient(scaled_total_loss, trainable_weights)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1014, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 738, in _backward_function\r\n    return self._rewrite_forward_and_call_backward(call_op, *args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 661, in _rewrite_forward_and_call_backward\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 582, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 629, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 619, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 350, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 715, in _registered_grad_fn\r\n    return self._rewrite_forward_and_call_backward(op, *doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 661, in _rewrite_forward_and_call_backward\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 582, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 629, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 619, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 350, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 679, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 715, in _registered_grad_fn\r\n    return self._rewrite_forward_and_call_backward(op, *doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 661, in _rewrite_forward_and_call_backward\r\n    forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 582, in forward_backward\r\n    forward, backward = self._construct_forward_backward(num_doutputs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 629, in _construct_forward_backward\r\n    func_graph=backwards_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 619, in _backprop_function\r\n    src_graph=self._func_graph)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 637, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'while' (op type: While)\r\n\r\n", "comments": ["@yanndupis \r\n\r\nI have tried on colab with TF version 2.0 and i am not seeing any error message. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d301ea4c8d4f311e7047e2161df2576d/untitled360.ipynb).Thanks!", "Hello @ravikyram , thank you very much for your quick response. \r\n\r\nI should have been more precise. You will get the error, if you load the model in a different script (where the model hasn't been explicitly defined) then start training it. In the gist, the entire code is in the same Colab. You should be able to reproduce if you have two different Colabs. \r\n\r\nIn the first Colab you run:\r\n```\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense,Embedding\r\nfrom tensorflow.keras.layers import LSTM\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(20000, 128))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\ntf.keras.models.save_model(model, \r\n                          'imdb_model', \r\n                           include_optimizer=True, \r\n                           save_format='tf')\r\n```\r\n\r\nand in a different Colab you run:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing import sequence\r\n\r\n(x_train, y_train), _ = imdb.load_data(num_words=20000)\r\nx_train = sequence.pad_sequences(x_train, maxlen=80)\r\n\r\nmodel = tf.keras.models.load_model('imdb_model')\r\n\r\nmodel.fit(x_train, y_train, epochs=1)\r\n```\r\n\r\nLet me know if you have any question. Thank you!", "I have tried on jupyter notebook with TF version 2.0 and was able to reproduce the issue.Please, find the files in attachment.Thanks!\r\n[error.tar.gz](https://github.com/tensorflow/tensorflow/files/3844965/error.tar.gz)\r\n", "Hi @yanndupis, @ravikyram ,\r\nIt looks like we have the same problem, with a similar error, when saving a model, and then loading it again, e.g. for retrain.\r\n\r\nWe save the model using **tf.saved_model.save()** and not Keras.\r\n\r\nWe are running on Ubuntu 16.04, Python 3.6, TF2.0\r\nWe have just tried and the problem still exists with TF2.1.0-rc1\r\n\r\n", "Is there any update on this ???", "Hitting the same error with a bidirectional GRU layer", "hitting a similar error with an LSTM model", "Hitting similar issue with simple LSTM. ", "@yanndupis I cannot reproduce the issue with recent `tf-nightly`. Can you please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/f5204a88549ae0746a4b9d2287de8017/untitled848.ipynb). I ran the first cell to save the model, then restarted colab's runtime, then ran second cell to load the model and execute `model.fit`.\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "@yanndupis Closing this as it was resolved in `tf-nightly`. Please feel free to reopen if this was not resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34211\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34211\">No</a>\n", "Error still present in `tf-nightly`. Ran the example given by @jvishnuvardhan in two separate colab runtimes (saving in one, and loading in the other) and the error is still present:\r\n\r\n```StagingError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\r\n        self.trainable_variables)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2722 _minimize\r\n        gradients = tape.gradient(loss, trainable_variables)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py:1073 gradient\r\n        unconnected_gradients=unconnected_gradients)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py:77 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:797 _backward_function\r\n        return self._rewrite_forward_and_call_backward(call_op, *args)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:712 _rewrite_forward_and_call_backward\r\n        forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:621 forward_backward\r\n        forward, backward = self._construct_forward_backward(num_doutputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:669 _construct_forward_backward\r\n        func_graph=backwards_graph)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:659 _backprop_function\r\n        src_graph=self._func_graph)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:669 _GradientsHelper\r\n        lambda: grad_fn(op, *out_grads))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:336 _MaybeCompile\r\n        return grad_fn()  # Exit early\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:669 <lambda>\r\n        lambda: grad_fn(op, *out_grads))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:712 _rewrite_forward_and_call_backward\r\n        forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:621 forward_backward\r\n        forward, backward = self._construct_forward_backward(num_doutputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:669 _construct_forward_backward\r\n        func_graph=backwards_graph)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:659 _backprop_function\r\n        src_graph=self._func_graph)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:669 _GradientsHelper\r\n        lambda: grad_fn(op, *out_grads))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:336 _MaybeCompile\r\n        return grad_fn()  # Exit early\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:669 <lambda>\r\n        lambda: grad_fn(op, *out_grads))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:712 _rewrite_forward_and_call_backward\r\n        forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:621 forward_backward\r\n        forward, backward = self._construct_forward_backward(num_doutputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:669 _construct_forward_backward\r\n        func_graph=backwards_graph)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:659 _backprop_function\r\n        src_graph=self._func_graph)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:623 _GradientsHelper\r\n        (op.name, op.type))\r\n\r\n    LookupError: No gradient defined for operation 'while' (op type: StatelessWhile)```", "@Gentatsu I ran the first part and then restarted the `runtime` and then ran the second part (loading). I don't see any issue. Please note that I ran both the code in one colab but two runtimes. \r\n\r\nAs this is an old issue, Can you please open a new issue with standalone code to reproduce the issue. Thanks!", "Facing the same error with tensorflow 2.3.1 and python 3.8. The model includes an LSTM layer. And I want to load the model and retrain it. I have been using the same script. I use tf.save_model.save and tf.save_model.load.\r\n\r\n\r\n<ipython-input-7-59ea268a147c> in compute_loss(data, model, y_truth, inverse_transform)\r\n      1 def compute_loss(data,model,y_truth,inverse_transform):\r\n----> 2     predictions,_,_,_ = model(data)\r\n      3     predictions=tf.matmul(predictions,inverse_transform)\r\n      4     loss = tf.math.reduce_mean((predictions-y_truth)**2)\r\n      5     return loss\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)\r\n    507 \r\n    508 def _call_attribute(instance, *args, **kwargs):\r\n--> 509   return instance.__call__(*args, **kwargs)\r\n    510 \r\n    511 \r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    812       # In this case we have not created variables on the first call. So we can\r\n    813       # run the first trace but we should fail if variables are created.\r\n--> 814       results = self._stateful_fn(*args, **kwds)\r\n    815       if self._created_variables:\r\n    816         raise ValueError(\"Creating variables on a non-first call to a function\"\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2827     with self._lock:\r\n   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2830 \r\n   2831   @property\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1841       `args` and `kwargs`.\r\n   1842     \"\"\"\r\n-> 1843     return self._call_flat(\r\n   1844         [t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n   1845          if isinstance(t, (ops.Tensor,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1927         possible_gradient_type,\r\n   1928         executing_eagerly)\r\n-> 1929     forward_function, args_with_tangents = forward_backward.forward()\r\n   1930     if executing_eagerly:\r\n   1931       flat_outputs = forward_function.call(\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in forward(self)\r\n   1430   def forward(self):\r\n   1431     \"\"\"Builds or retrieves a forward function for this call.\"\"\"\r\n-> 1432     forward_function = self._functions.forward(\r\n   1433         self._inference_args, self._input_tangents)\r\n   1434     return forward_function, self._inference_args + self._input_tangents\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in forward(self, inference_args, input_tangents)\r\n   1187       (self._forward, self._forward_graph, self._backward,\r\n   1188        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (\r\n-> 1189            self._forward_and_backward_functions(inference_args, input_tangents))\r\n   1190     return self._forward\r\n   1191 \r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)\r\n   1386     while len(outputs) < len(self._func_graph.outputs):\r\n   1387       outputs = list(self._func_graph.outputs)\r\n-> 1388       self._build_functions_for_outputs(\r\n   1389           outputs, inference_args, input_tangents)\r\n   1390     (forward_function, forward_graph,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)\r\n    893             graph_placeholder(gradient_dtype, gradient_shape))\r\n    894       with ops.device(None):\r\n--> 895         gradients_wrt_inputs = gradients_util._GradientsHelper(  # pylint: disable=protected-access\r\n    896             trainable_outputs,\r\n    897             self._func_graph.inputs,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    666                 # If grad_fn was found, do not use SymbolicGradient even for\r\n    667                 # functions.\r\n--> 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n    669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)\r\n    710   def _rewrite_forward_and_call_backward(self, op, *doutputs):\r\n    711     \"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\r\n--> 712     forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    713     if not backwards_function.outputs:\r\n    714       return backwards_function.structured_outputs\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)\r\n    619     if forward_backward is not None:\r\n    620       return forward_backward\r\n--> 621     forward, backward = self._construct_forward_backward(num_doutputs)\r\n    622     self._cached_function_pairs[num_doutputs] = (forward, backward)\r\n    623     return forward, backward\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)\r\n    662       backwards_graph = func_graph_module.FuncGraph(\r\n    663           _backward_name(self._func_graph.name))\r\n--> 664       func_graph_module.func_graph_from_py_func(\r\n    665           name=backwards_graph.name,\r\n    666           python_func=_backprop_function,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)\r\n    653     def _backprop_function(*grad_ys):\r\n    654       with ops.device(None):\r\n--> 655         return gradients_util._GradientsHelper(  # pylint: disable=protected-access\r\n    656             trainable_outputs,\r\n    657             self._func_graph.inputs,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    666                 # If grad_fn was found, do not use SymbolicGradient even for\r\n    667                 # functions.\r\n--> 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n    669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)\r\n    710   def _rewrite_forward_and_call_backward(self, op, *doutputs):\r\n    711     \"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\r\n--> 712     forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    713     if not backwards_function.outputs:\r\n    714       return backwards_function.structured_outputs\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)\r\n    619     if forward_backward is not None:\r\n    620       return forward_backward\r\n--> 621     forward, backward = self._construct_forward_backward(num_doutputs)\r\n    622     self._cached_function_pairs[num_doutputs] = (forward, backward)\r\n    623     return forward, backward\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)\r\n    662       backwards_graph = func_graph_module.FuncGraph(\r\n    663           _backward_name(self._func_graph.name))\r\n--> 664       func_graph_module.func_graph_from_py_func(\r\n    665           name=backwards_graph.name,\r\n    666           python_func=_backprop_function,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)\r\n    653     def _backprop_function(*grad_ys):\r\n    654       with ops.device(None):\r\n--> 655         return gradients_util._GradientsHelper(  # pylint: disable=protected-access\r\n    656             trainable_outputs,\r\n    657             self._func_graph.inputs,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    666                 # If grad_fn was found, do not use SymbolicGradient even for\r\n    667                 # functions.\r\n--> 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n    669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)\r\n    710   def _rewrite_forward_and_call_backward(self, op, *doutputs):\r\n    711     \"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\r\n--> 712     forward_function, backwards_function = self.forward_backward(len(doutputs))\r\n    713     if not backwards_function.outputs:\r\n    714       return backwards_function.structured_outputs\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in forward_backward(self, num_doutputs)\r\n    619     if forward_backward is not None:\r\n    620       return forward_backward\r\n--> 621     forward, backward = self._construct_forward_backward(num_doutputs)\r\n    622     self._cached_function_pairs[num_doutputs] = (forward, backward)\r\n    623     return forward, backward\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _construct_forward_backward(self, num_doutputs)\r\n    662       backwards_graph = func_graph_module.FuncGraph(\r\n    663           _backward_name(self._func_graph.name))\r\n--> 664       func_graph_module.func_graph_from_py_func(\r\n    665           name=backwards_graph.name,\r\n    666           python_func=_backprop_function,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _backprop_function(*grad_ys)\r\n    653     def _backprop_function(*grad_ys):\r\n    654       with ops.device(None):\r\n--> 655         return gradients_util._GradientsHelper(  # pylint: disable=protected-access\r\n    656             trainable_outputs,\r\n    657             self._func_graph.inputs,\r\n\r\n~/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    619               grad_fn = func_call.python_grad_func\r\n    620             else:\r\n--> 621               raise LookupError(\r\n    622                   \"No gradient defined for operation '%s' (op type: %s)\" %\r\n    623                   (op.name, op.type))\r\n\r\nLookupError: No gradient defined for operation 'while' (op type: While)"]}, {"number": 34210, "title": "TFLiteConverter - GetOpWithOutput error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): `pip install tf-nightly`\r\n- TensorFlow version (use command below): 2.1.0-dev20191111\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nI'm trying to convert a BERT keras model imported from the `transformers` library. Using the `convert()` method from the `TFLiteConverter`, the following `GetOpWithOutput` error happens, no matter the values used passed to `allow_custom_ops` or `target_spec.supported_ops`:\r\n\r\n`tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"Identity\" is not produced by any op in this graph`\r\n\r\n**Describe the expected behavior**\r\nThe TFLiteConverter should convert the model and return the tflite version\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/16kWs2ji8xrgjSuLftXknDqEh2VixDR4R\r\n\r\n**Other info / logs**\r\nThe same `bert-large-uncased-whole-word-masking-finetuned-squad` model imported from the `transformers` library works perfectly when used directly on a QA task without TFLite conversion.\r\n\r\nAlso, the conversion of the DistilBERT model from the same library and using the same steps works correctly with `converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]`.", "comments": ["Hi,\r\n\r\nIt's possible that the model doesn't have an output node named \"Identity\". Can you use netron (https://lutzroeder.github.io/netron/) to visualize this graph and see what's the output?\r\n\r\n(I'm guessing this is just a tensor naming issue, if not I can take a further look)", "Thank you for your answer. I tried to visualize the graph using Netron without success, either with a `h5` or `SavedModel` version.\r\n\r\nHowever, as you can see in [this cell of the notebook](https://colab.research.google.com/drive/16kWs2ji8xrgjSuLftXknDqEh2VixDR4R#scrollTo=94UoamBP17VO&line=4&uniqifier=1), it seems there are two outputs `Identity` and `Identity_1`. Also visible in this Tensorboard graph:\r\n<img width=\"751\" alt=\"Screen Shot 2019-11-13 at 2 34 35 PM\" src=\"https://user-images.githubusercontent.com/5020707/68797525-c1e6bb00-0622-11ea-99ef-b5c1e8454ccc.png\">\r\n\r\nDoes it answer your question @haozha111?", "That seems like a bug in our old converter.\r\n\r\nWe have developed a new MLIR-based converter, you can try it out by downloading the tf-nightly pip package, and then set converter.experimental_new_converter = True.\r\n\r\nCould you try this out and see if the problem is resolved?", "I get this error when trying with `converter.experimental_new_converter = True`, whatever the value of other options `target_spec.supported_ops` and `allow_custom_ops`:\r\n\r\n```\r\n2019-11-15 15:18:32.489426: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\r\n2019-11-15 15:18:34.411116: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:106] Ignored output_format.\r\n2019-11-15 15:18:34.411167: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:112] Ignored drop_control_dependency.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Graph does not contain node: input_1\r\n```\r\n\r\nBut it seems there is an `input_1` in the model, as visible in my previous TensorBoard graph and in [this cell of the notebook](https://colab.research.google.com/drive/16kWs2ji8xrgjSuLftXknDqEh2VixDR4R#scrollTo=94UoamBP17VO&line=4&uniqifier=1).", "I could reproduce the issue now. It seems like a bug in our converter, will take a look and try to fix it soon.", "I changed `transformers>=2` in the pip command. https://colab.research.google.com/drive/1MrPEJWqReZGKWBHAkoYYCrDiDTccmLUb\r\nDoes this help?", "I tried again with the latest TF nightly version and it works now using the experimental converter and partial TF operators:\r\n\r\n```python\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops  = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\nSpecifying `transformers>=2` doesn't affect the result since it defaults to the last version available which is `2.3.0` @ucalyptus.\r\n\r\nClosing this, thanks to the TF team for all the continuous improvements! \ud83d\udc4f", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34210\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34210\">No</a>\n", "Using these options did great! Thanks\r\n**converter.experimental_new_converter = True\r\nconverter.allow_custom_ops = True**\r\n"]}, {"number": 34209, "title": "Update README.md", "body": "Removed grammatical errors.", "comments": []}, {"number": 34208, "title": "Add XLA context to the function cache key", "body": "", "comments": []}]