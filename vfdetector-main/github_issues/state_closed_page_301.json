[{"number": 45244, "title": "tflite was crashed randomly on Android 9 arm64 device.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Android 9 arm64 device\r\n- TensorFlow installed from: source\r\n- TensorFlow version (use command below): v1.14.0\r\n- GCC/Compiler version: android-ndk-r20\r\n\r\n**Describe the current behavior**\r\nRandom crash\r\n\r\n**Other info / logs**\r\n```\r\n11-25 19:26:28.025 22053 22146 E CRASH   : signal 4 (SIGILL), code 1 (ILL_ILLOPC), fault addr 0x70ef20b4e4 (*pc=0x4e8296d5)\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x0  000000710159f888  x1  000000710159d000  x2  00000070ec7dc750  x3  00000070ec7dca50\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x4  00000070ec7dc6d0  x5  0000000000000004  x6  0000000000000002  x7  00000070ec7de6b8\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x8  0000000000000001  x9  0000000000000080  x10 0000000000000008  x11 0000000000000020\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x12 0000000000000010  x13 000000710159fb08  x14 0000000000000001  x15 0000000000000040\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x16 00000070ef370700  x17 00000070ef20b478  x18 0000000000000004  x19 0000000000000040\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x20 0000000000000001  x21 0000000000000000  x22 0000000000000005  x23 0000000000000000\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x24 0000000000000000  x25 000000000000000a  x26 000000000000000d  x27 0000000000000a00\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     x28 0000000000000001  x29 00000070ec7de4d0\r\n11-25 19:26:28.025 22053 22146 E CRASH   :     sp  00000070ec7dc520  lr  00000070ef2082c8  pc  00000070ef20b4e4\r\n11-25 19:26:28.025 22053 22146 E CRASH   : \r\n11-25 19:26:28.025 22053 22146 E CRASH   : backtrace:\r\n11-25 19:26:28.025 22053 22146 E CRASH   :       #00 pc 00000000000c84e4  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (tflite::optimized_ops::depthwise_conv::ProcessPerDepth<(tflite::DepthwiseConvImplementation)3>::Run(unsigned char const*, int const*, signed char*, int*, tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProdParams const*)+108)\r\n11-25 19:26:28.026 22053 22146 E CRASH   :       #01 pc 00000000000c52c4  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (void tflite::optimized_ops::depthwise_conv::DepthwiseConvDotProduct3x3<(tflite::DepthwiseConvImplementation)3>(tflite::DepthwiseParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*)+2116)\r\n11-25 19:26:28.026 22053 22146 E CRASH   :       #02 pc 00000000000d0b98  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (tflite::optimized_ops::DepthwiseConvWorkerTask<unsigned char, int>::Run()+60)\r\n11-25 19:26:28.026 22053 22146 E CRASH   :       #03 pc 00000000000d0ea8  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (void gemmlowp::WorkersPool::Execute<tflite::optimized_ops::DepthwiseConvWorkerTask<unsigned char, int> >(int, tflite::optimized_ops::DepthwiseConvWorkerTask<unsigned char, int>*)+208)\r\n11-25 19:26:28.026 22053 22146 E CRASH   :       #04 pc 00000000000c43ec  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (void tflite::optimized_ops::DepthwiseConv<unsigned char, int>(tflite::DepthwiseParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*, tflite::CpuBackendContext*)+980)\r\n11-25 19:26:28.027 22053 22146 E CRASH   :       #05 pc 00000000000d7cc4  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (void tflite::ops::builtin::depthwise_conv::EvalQuantized<(tflite::ops::builtin::depthwise_conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteDepthwiseConvParams*, tflite::ops::builtin::depthwise_conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*)+556)\r\n11-25 19:26:28.027 22053 22146 E CRASH   :       #06 pc 00000000000bd9e8  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (TfLiteStatus tflite::ops::builtin::depthwise_conv::Eval<(tflite::ops::builtin::depthwise_conv::KernelType)2>(TfLiteContext*, TfLiteNode*)+136)\r\n11-25 19:26:28.027 22053 22146 E CRASH   :       #07 pc 00000000001aae40  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (tflite::Subgraph::Invoke()+504)\r\n11-25 19:26:28.028 22053 22146 E CRASH   :       #08 pc 00000000001ae508  /data/app/fun.gostudy.android.phygital-QHoCDLpSJXY1hPYpBkyg_A==/lib/arm64/libtensorflowlite.so (tflite::Interpreter::Invoke()+32)\r\n```\r\n", "comments": ["When I set the thread num to 1, there is no crash any more.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45244\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45244\">No</a>\n"]}, {"number": 45243, "title": "tf.keras.applications.mobilenet_v3.preprocess_input documentation not according source code ", "body": "\r\nIn https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input\r\n[Said ](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input#returns) that:\r\n\r\nPreprocessed numpy.array or a tf.Tensor with type float32.\r\nThe inputs pixel values are scaled between -1 and 1, sample-wise.\r\n\r\nHowever in source code in GitHub you find this:\r\n\r\n@keras_export('keras.applications.mobilenet_v3.preprocess_input')\r\ndef preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\r\n  return x\r\n\r\nAnd if you check it in a notebook you see that as source code state it is doing nothing. Do not scale and do not change dtype to float32 of the input.\r\n\r\n", "comments": ["Thanks for reporting the issue.\r\n\r\nTo give more context, the preprocess_input() was a standard method for all models to normalize the input data. However, it is quite common that user forget to call this method before they feed the data to model, which result into poor result. When we realized this problem, we move the preprocessing logic from this method to the model itself for some new models (like mobilenet_v3). In order to keep the alignment between old and new model, we still keep this method for new model, but as an empty shell, so that user can easily switching between different version of models without changing much code.\r\n\r\nI will update the docstring for this method to make it align with the implementation detail.", "Hi, the docs at https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input are still misleading (specifically, the `Returns` section)\r\n\r\nFor example, when trying to replicate the preprocessing based on the documentation to run it on mobile, this might lead to scaling the input values twice.", "I think the doc change wasn't included in the 2.4 release, but will be reflect into 2.5 release (coming soon).", "The nightly view shows what the current state is.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input?version=nightly"]}, {"number": 45242, "title": "Fix linking failure of Flex delegate in Archive mode", "body": "Without this change, users will not be able to push the app with Flex delegate to App store.\r\nPlease see https://github.com/tensorflow/tensorflow/issues/44879 for more context.\r\n\r\nPiperOrigin-RevId: 344578122\r\nChange-Id: Ibce63196691dcec2f958064e72f6ffec5f9a617a", "comments": ["@mihaimaruseac Can I merge this PR now?", "No, please don't merge manually. Please let the release owner do the merging."]}, {"number": 45241, "title": " InvalidArgumentError:  assertion failed: [predictions must be <= 1] [Condition x <= y did not hold element-wise:] [x (model/dense_3/Relu:0) = ] [[0.179184318 1.62250423 0...]...] [y (metrics/auc/Cast_2/x:0) = ] [1] \t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/else/_11/Assert}}]] [Op:__inference_distributed_function_8508]", "body": "Updated version of issue https://github.com/tensorflow/tensorflow/issues/45219\r\nI'm trying a new model involving word embedding for labels as well as Image classification in order to increase accuracy of the Image classifier. \r\nI'm using tensorflow Version 2.\r\nHere is the code of the model and summary.\r\n```\r\ndef build_mobilenet_embedd_model(img_input ):\r\n    input_x = Input(shape = (img_input.shape[1:]), name='feature')\r\n    base_model = MobileNet(include_top= False, input_shape=img_input.shape[1:], weights='imagenet')(input_x)\r\n    avgpool_x = GlobalAveragePooling2D()(base_model)\r\n    dense1_x = Dense(2048, activation='relu')(avgpool_x)\r\n    batch1_x = BatchNormalization()(dense1_x)\r\n    dropout1_x = Dropout(0.2)(batch1_x)\r\n    dense2_x = Dense(512, activation='relu')(dropout1_x)\r\n    batch2_x = BatchNormalization()(dense2_x)\r\n    dropout2_x = Dropout(0.2)(batch2_x)\r\n    predictions = Dense(len(all_labels), activation='relu')(dropout2_x)\r\n    embedd_layer = Embedding(13, 100, weights=[embedding_matrix], trainable=False,name = \"embedding_1\")(predictions)\r\n    flatten = Flatten(name=\"flatten\")(embedd_layer)\r\n    final_predictions = Dense(13, activation='relu')(flatten)\r\n    model = keras.models.Model(inputs= input_x, outputs = final_predictions)\r\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics= METRICS\r\n    model.summary()\r\n    return model\r\n```\r\nand here is the summary:-\r\n```\r\nModel: \"model_4\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nfeature (InputLayer)         [(None, 128, 128, 3)]     0         \r\n_________________________________________________________________\r\nmobilenet_1.00_128 (Model)   (None, 4, 4, 1024)        3228864   \r\n_________________________________________________________________\r\nglobal_average_pooling2d_5 ( (None, 1024)              0         \r\n_________________________________________________________________\r\ndense_16 (Dense)             (None, 2048)              2099200   \r\n_________________________________________________________________\r\nbatch_normalization_10 (Batc (None, 2048)              8192      \r\n_________________________________________________________________\r\ndropout_10 (Dropout)         (None, 2048)              0         \r\n_________________________________________________________________\r\ndense_17 (Dense)             (None, 512)               1049088   \r\n_________________________________________________________________\r\nbatch_normalization_11 (Batc (None, 512)               2048      \r\n_________________________________________________________________\r\ndropout_11 (Dropout)         (None, 512)               0         \r\n_________________________________________________________________\r\ndense_18 (Dense)             (None, 13)                6669      \r\n_________________________________________________________________\r\nembedding_1 (Embedding)      (None, 13, 100)           1300      \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1300)              0         \r\n_________________________________________________________________\r\ndense_19 (Dense)             (None, 13)                16913     \r\n=================================================================\r\nTotal params: 6,412,274\r\nTrainable params: 6,383,966\r\nNon-trainable params: 28,308\r\n_________________________________________________________________\r\n```\r\nAdding `embedd_layer ` and `flatten layer` after the layer `prediction` resulted in this error to pop-up,\r\nwhen I run the model fit I'll get this warning  & error message.\r\n```\r\nWARNING:tensorflow:From <ipython-input-23-65f5e4d4075d>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nWARNING:tensorflow:sample_weight modes were coerced from\r\n  ...\r\n    to  \r\n  ['...']\r\nTrain for 660 steps, validate on 1024 samples\r\nEpoch 1/30\r\nWARNING:tensorflow:Gradients do not exist for variables ['conv1/kernel:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0', 'conv_dw_1/depthwise_kernel:0', 'conv_dw_1_bn/gamma:0', 'conv_dw_1_bn/beta:0', 'conv_pw_1/kernel:0', 'conv_pw_1_bn/gamma:0', 'conv_pw_1_bn/beta:0', 'conv_dw_2/depthwise_kernel:0', 'conv_dw_2_bn/gamma:0', 'conv_dw_2_bn/beta:0', 'conv_pw_2/kernel:0', 'conv_pw_2_bn/gamma:0', 'conv_pw_2_bn/beta:0', 'conv_dw_3/depthwise_kernel:0', 'conv_dw_3_bn/gamma:0', 'conv_dw_3_bn/beta:0', 'conv_pw_3/kernel:0', 'conv_pw_3_bn/gamma:0', 'conv_pw_3_bn/beta:0', 'conv_dw_4/depthwise_kernel:0', 'conv_dw_4_bn/gamma:0', 'conv_dw_4_bn/beta:0', 'conv_pw_4/kernel:0', 'conv_pw_4_bn/gamma:0', 'conv_pw_4_bn/beta:0', 'conv_dw_5/depthwise_kernel:0', 'conv_dw_5_bn/gamma:0', 'conv_dw_5_bn/beta:0', 'conv_pw_5/kernel:0', 'conv_pw_5_bn/gamma:0', 'conv_pw_5_bn/beta:0', 'conv_dw_6/depthwise_kernel:0', 'conv_dw_6_bn/gamma:0', 'conv_dw_6_bn/beta:0', 'conv_pw_6/kernel:0', 'conv_pw_6_bn/gamma:0', 'conv_pw_6_bn/beta:0', 'conv_dw_7/depthwise_kernel:0', 'conv_dw_7_bn/gamma:0', 'conv_dw_7_bn/beta:0', 'conv_pw_7/kernel:0', 'conv_pw_7_bn/gamma:0', 'conv_pw_7_bn/beta:0', 'conv_dw_8/depthwise_kernel:0', 'conv_dw_8_bn/gamma:0', 'conv_dw_8_bn/beta:0', 'conv_pw_8/kernel:0', 'conv_pw_8_bn/gamma:0', 'conv_pw_8_bn/beta:0', 'conv_dw_9/depthwise_kernel:0', 'conv_dw_9_bn/gamma:0', 'conv_dw_9_bn/beta:0', 'conv_pw_9/kernel:0', 'conv_pw_9_bn/gamma:0', 'conv_pw_9_bn/beta:0', 'conv_dw_10/depthwise_kernel:0', 'conv_dw_10_bn/gamma:0', 'conv_dw_10_bn/beta:0', 'conv_pw_10/kernel:0', 'conv_pw_10_bn/gamma:0', 'conv_pw_10_bn/beta:0', 'conv_dw_11/depthwise_kernel:0', 'conv_dw_11_bn/gamma:0', 'conv_dw_11_bn/beta:0', 'conv_pw_11/kernel:0', 'conv_pw_11_bn/gamma:0', 'conv_pw_11_bn/beta:0', 'conv_dw_12/depthwise_kernel:0', 'conv_dw_12_bn/gamma:0', 'conv_dw_12_bn/beta:0', 'conv_pw_12/kernel:0', 'conv_pw_12_bn/gamma:0', 'conv_pw_12_bn/beta:0', 'conv_dw_13/depthwise_kernel:0', 'conv_dw_13_bn/gamma:0', 'conv_dw_13_bn/beta:0', 'conv_pw_13/kernel:0', 'conv_pw_13_bn/gamma:0', 'conv_pw_13_bn/beta:0', 'dense/kernel:0', 'dense/bias:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['conv1/kernel:0', 'conv1_bn/gamma:0', 'conv1_bn/beta:0', 'conv_dw_1/depthwise_kernel:0', 'conv_dw_1_bn/gamma:0', 'conv_dw_1_bn/beta:0', 'conv_pw_1/kernel:0', 'conv_pw_1_bn/gamma:0', 'conv_pw_1_bn/beta:0', 'conv_dw_2/depthwise_kernel:0', 'conv_dw_2_bn/gamma:0', 'conv_dw_2_bn/beta:0', 'conv_pw_2/kernel:0', 'conv_pw_2_bn/gamma:0', 'conv_pw_2_bn/beta:0', 'conv_dw_3/depthwise_kernel:0', 'conv_dw_3_bn/gamma:0', 'conv_dw_3_bn/beta:0', 'conv_pw_3/kernel:0', 'conv_pw_3_bn/gamma:0', 'conv_pw_3_bn/beta:0', 'conv_dw_4/depthwise_kernel:0', 'conv_dw_4_bn/gamma:0', 'conv_dw_4_bn/beta:0', 'conv_pw_4/kernel:0', 'conv_pw_4_bn/gamma:0', 'conv_pw_4_bn/beta:0', 'conv_dw_5/depthwise_kernel:0', 'conv_dw_5_bn/gamma:0', 'conv_dw_5_bn/beta:0', 'conv_pw_5/kernel:0', 'conv_pw_5_bn/gamma:0', 'conv_pw_5_bn/beta:0', 'conv_dw_6/depthwise_kernel:0', 'conv_dw_6_bn/gamma:0', 'conv_dw_6_bn/beta:0', 'conv_pw_6/kernel:0', 'conv_pw_6_bn/gamma:0', 'conv_pw_6_bn/beta:0', 'conv_dw_7/depthwise_kernel:0', 'conv_dw_7_bn/gamma:0', 'conv_dw_7_bn/beta:0', 'conv_pw_7/kernel:0', 'conv_pw_7_bn/gamma:0', 'conv_pw_7_bn/beta:0', 'conv_dw_8/depthwise_kernel:0', 'conv_dw_8_bn/gamma:0', 'conv_dw_8_bn/beta:0', 'conv_pw_8/kernel:0', 'conv_pw_8_bn/gamma:0', 'conv_pw_8_bn/beta:0', 'conv_dw_9/depthwise_kernel:0', 'conv_dw_9_bn/gamma:0', 'conv_dw_9_bn/beta:0', 'conv_pw_9/kernel:0', 'conv_pw_9_bn/gamma:0', 'conv_pw_9_bn/beta:0', 'conv_dw_10/depthwise_kernel:0', 'conv_dw_10_bn/gamma:0', 'conv_dw_10_bn/beta:0', 'conv_pw_10/kernel:0', 'conv_pw_10_bn/gamma:0', 'conv_pw_10_bn/beta:0', 'conv_dw_11/depthwise_kernel:0', 'conv_dw_11_bn/gamma:0', 'conv_dw_11_bn/beta:0', 'conv_pw_11/kernel:0', 'conv_pw_11_bn/gamma:0', 'conv_pw_11_bn/beta:0', 'conv_dw_12/depthwise_kernel:0', 'conv_dw_12_bn/gamma:0', 'conv_dw_12_bn/beta:0', 'conv_pw_12/kernel:0', 'conv_pw_12_bn/gamma:0', 'conv_pw_12_bn/beta:0', 'conv_dw_13/depthwise_kernel:0', 'conv_dw_13_bn/gamma:0', 'conv_dw_13_bn/beta:0', 'conv_pw_13/kernel:0', 'conv_pw_13_bn/gamma:0', 'conv_pw_13_bn/beta:0', 'dense/kernel:0', 'dense/bias:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n  1/660 [..............................] - ETA: 8:05:51WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\r\nWARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: lr\r\n```\r\nError:\r\n```\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1304         use_multiprocessing=use_multiprocessing,\r\n   1305         shuffle=shuffle,\r\n-> 1306         initial_epoch=initial_epoch)\r\n   1307 \r\n   1308   @deprecation.deprecated(\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\nE:\\anaconda\\envs\\gputest\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError:  assertion failed: [predictions must be <= 1] [Condition x <= y did not hold element-wise:] [x (model/dense_3/Relu:0) = ] [[0.179184318 1.62250423 0...]...] [y (metrics/auc/Cast_2/x:0) = ] [1]\r\n\t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/else/_11/Assert}}]] [Op:__inference_distributed_function_8508]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```", "comments": ["@WrathofBhuvan11 \r\nCode shared has syntax errors, can you please share a colab gist with the issue faced.", "> @WrathofBhuvan11 \n> Code shared has syntax errors, can you please share a colab gist with the issue faced.\n\nCan you please say what u r referring to as syntax error ?", "@WrathofBhuvan11 \r\nCan you please share your code with the issue reported in a colab gist.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45241\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45241\">No</a>\n", "InvalidArgumentError:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential/dense_2/Sigmoid:0) = ] [[-nan][-nan][-nan]...] [y (Cast_6/x:0) = ] [0]\r\n\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]] [Op:__inference_train_function_1772]\r\n\r\nFunction call stack:\r\ntrain_function", "If you continue marking all the open problems as stale and close them they will never be resolved. These issues persist throughout the whole platform and nobody seems to care", "@hanneswiedenhofer \r\nPlease create a new issue as this issue was closed as there was no response from user.", "has this issue been resolved? I am getting same error after Epoch 5"]}, {"number": 45240, "title": "InvalidArgumentError: Input to DecodeRaw has length 1080022 that is not a multiple of 4, the size of int32 [Op:DecodeRaw]", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10 x64\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0rc3\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11, 8.0.2\r\n- GPU model and memory: RTX  3080 10 GB\r\n\r\n\r\n**Code:**\r\n```\r\nimport tensorflow as tf # version = 2.4.0rc3\r\nfrom tensorflow.train import BytesList, FloatList, Int64List\r\nfrom tensorflow.train import Feature, Features, Example\r\n\r\nim = tf.random.uniform((300, 300, 3), minval=0, maxval=256, dtype=tf.int32)\r\nmask_1 = tf.random.uniform((300, 300, 3), minval=0, maxval=2, dtype=tf.int32)\r\n\r\nif1 = Feature(bytes_list = BytesList(value = [tf.io.serialize_tensor(im).numpy()]))\r\nmf1 = Feature(bytes_list = BytesList(value = [tf.io.serialize_tensor(mask_1).numpy()]))\r\nsf1 = Feature(bytes_list = BytesList(value = [\"eisp12ie\".encode()]))\r\n\r\ndata_example = Example(features = Features(feature = {\"image\": if1, \"mask\":mf1, \"sample_id\": sf1}))\r\n\r\nwith tf.io.TFRecordWriter(\"data_example_1.tfrecord\") as f:\r\n    f.write(data_example.SerializeToString())\r\n\r\nfeature_description = {\r\n    \"image\" : tf.io.FixedLenFeature([], tf.string),\r\n    \"mask\" : tf.io.FixedLenFeature([], tf.string),\r\n    \"sample_id\" : tf.io.FixedLenFeature([], tf.string)\r\n}\r\n\r\nfor serialized_example in tf.data.TFRecordDataset([\"data_example_1.tfrecord\"]):\r\n    parsed_example = tf.io.parse_single_example(serialized_example,\r\n                                                feature_description)\r\n\r\ntf.io.decode_raw(parsed_example[\"image\"], out_type = tf.int32)\r\n```\r\n\r\n**Error:**\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-81-1950f51902b7> in <module>\r\n----> 1 tf.io.decode_raw(parsed_example[\"image\"], out_type = tf.int32)\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\ops\\parsing_ops.py in decode_raw(input_bytes, out_type, little_endian, fixed_length, name)\r\n    881         name=name)\r\n    882   else:\r\n--> 883     return gen_parsing_ops.decode_raw(\r\n    884         input_bytes, out_type, little_endian=little_endian, name=name)\r\n    885 \r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py in decode_raw(bytes, out_type, little_endian, name)\r\n    416       return _result\r\n    417     except _core._NotOkStatusException as e:\r\n--> 418       _ops.raise_from_not_ok_status(e, name)\r\n    419     except _core._FallbackException:\r\n    420       pass\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in raise_from_not_ok_status(e, name)\r\n   6860   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6861   # pylint: disable=protected-access\r\n-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6863   # pylint: enable=protected-access\r\n   6864 \r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Input to DecodeRaw has length 1080022 that is not a multiple of 4, the size of int32 [Op:DecodeRaw]\r\n```\r\n\r\n\r\n\r\nI set the dtype of im variable to tf.int32, and decode_raw with dtype = tf.int32.\r\nBut it raised an error.\r\nWhen i set the out_type = tf.uint8, it was okay.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45240\">No</a>\n"]}, {"number": 45239, "title": "tf.image.extract_patches argument description", "body": "https://www.tensorflow.org/api_docs/python/tf/image/extract_patches\r\n\r\nThere is only one argument description for this function.\r\n\r\nMaybe typo? some tags like `<tr>` are in the description.", "comments": ["This is now fixed with TF 2.4.1 api docs. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/extract_patches#args"]}, {"number": 45237, "title": "Enabling XNNPACK changes the output.", "body": "**System information**\r\n- OS Platform and Distribution: Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: All android\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.3\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nRunning a computer vision model for emotion detection. The output is as expected when XNN is disabled but if I enable **tfliteOptions.setUseXNNPACK(true)** then the output is incorrect and totally different.\r\n\r\n**Describe the expected behavior**\r\nOutput should be same for both XNN enabled and non-XNN option.\r\n\r\nI have attached inference class files and model files below.\r\n\r\n[ClassFiles.zip](https://github.com/tensorflow/tensorflow/files/5609851/Archive.zip)\r\n[emotion_fp16t.tflite.zip](https://github.com/tensorflow/tensorflow/files/5609852/emotion_fp16t.tflite.zip)\r\n", "comments": ["Having the same issue with XNNPACK. If running delegate with XNNPACK the result is totally different from the disabled one. (TF2 version 2.3.1)", "@Maratyszcza could you take a look at this issue?", "@kafan1986 Does it reproduce if you build TFLite from the master branch? There was a bug with asymetric padding a while ago that was fixed in google/XNNPACK@facecc526bf35d09ac3319f44072d640101b2b70", "@Maratyszcza I have tested with 0.0.0-nightly build of tflite and the output is same for both version. But the performance seems to be remain same for both with XNN module true and false, whereas earlier the XNN true would significantly run the inference faster although the output was wrong.", "AFAIK the nightly build of TFLite enables XNNPACK by default (@multiverse-tf to confirm), that is why you don't see any difference from explicitly enabling it. I checked that your model does get fully delegated to XNNPACK.", "@Maratyszcza But I was explicitly disabling the XNNPACK by passing false to setXNNPACK.", "It is possible that this is not sufficient to disable XNNPACK. You may profile the binary, and if it spends much time in `xnn_*` functions, XNNPACK is being used.", "Works OK with tensorflow lite 2.4.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45237\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45237\">No</a>\n"]}, {"number": 45236, "title": "WARNING: function Model.make_predict_function.<locals>.predict_function", "body": "I'm facing the following warning message in `tf 2.3` in my local machine. This error probably doesn't impact on the model performance (or do). \r\n\r\n> WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002424DCD6678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has an experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n\r\nThe issue is, I can't reproduce it in **Colab** or **Kaggle** either with the same version of tensorflow `tf 2.3`. What makes it prevent to show this warning message logs on colab or kaggle? I tried to disable the warning on my local machine but didn't work. Below is the reproducible code. In short, I've tried to train a subclassed model and saved a few checkpoints, and ended up with inference iteratively. This is actually a mimic version of my original code. \r\n\r\n```python\r\nimport tensorflow as tf \r\nimport numpy as np\r\nfrom glob import glob\r\nimport gc\r\n\r\nclass Classifier(tf.keras.Model):\r\n    def __init__(self, dim):\r\n        super(Classifier, self).__init__()\r\n        # define all layers in init\r\n        # Layer of Block 1\r\n        self.Base  = tf.keras.layers.Conv2D(32, 3, strides=2, activation=\"relu\")\r\n\r\n        self.GAP   = tf.keras.layers.GlobalAveragePooling2D()\r\n        self.OUT   = tf.keras.layers.Dense(10, activation='softmax')\r\n    \r\n    def call(self, input_tensor, training=False):\r\n        x  = self.Base(input_tensor)\r\n        x  = self.GAP(x)\r\n        return self.OUT(x)\r\n\r\n# train set / data\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_train = np.expand_dims(x_train, axis=-1)\r\nx_train = np.repeat(x_train, 3, axis=-1)\r\nx_train = x_train.astype('float32') / 255\r\n# train set / target \r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\r\n\r\n# compile \r\nprint('Model Sub-Classing API')\r\nsub_classing_model = Classifier((28,28,3))\r\nsub_classing_model.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\r\n              metrics = tf.keras.metrics.CategoricalAccuracy(),\r\n              optimizer = tf.keras.optimizers.Adam())\r\n# fit \r\nsub_classing_model.fit(x_train, y_train,\r\n         batch_size=128, epochs=1);\r\n\r\n# few checkpoints\r\nsub_classing_model.save_weights('sub1.h5')\r\nsub_classing_model.save_weights('sub2.h5')\r\nsub_classing_model.save_weights('sub3.h5')\r\n\r\nfor i in sorted(glob('./sub*.h5')):\r\n    sub_classing_model = Classifier((28,28,3))\r\n    sub_classing_model.build((None, *(28,28,3)))\r\n    sub_classing_model.load_weights(i)\r\n    z = sub_classing_model.predict(x_train) # <------ Perhaps something makes it happen \r\n    del sub_classing_model\r\n    gc.collect()\r\n```", "comments": ["@innat \r\nI ran the code shared and face a different error, please refer to [gist here](https://colab.research.google.com/gist/Saduf2019/8e931005bb045dc727180f1eab514aee/untitled477.ipynb). if possible please a colab gist with the issue reported.", "@Saduf2019 Find it [here](https://colab.research.google.com/gist/Saduf2019/8e931005bb045dc727180f1eab514aee/untitled477.ipynb). \r\n\r\nHowever, As I told you earlier, it **can't be reproduced** in colab or kaggle, it's happening in the local machine, having the same version of packages. I just want to know, if this is a normal warning message, what makes a clean output log in those virtual environments?", "@innat \r\nCould you please check if you are facing the same issue in a virtual environment?\r\n\r\nAlternatively, you can also try suppressing the warnings by running the below code before importing TensorFlow.\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\nThanks!"]}, {"number": 45235, "title": "[TF-1.14] Batch Cholesky Decomposition No Parallelism ? ", "body": "Hi, I am profiling the batch Cholesky decompositions of tensorflow. \r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.4 LTS (Bionic Beaver)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda-10.0; cudnn-10.0-v7.6.3.30\r\n- GPU model and memory: Tesla P100-PCIE-12GB\r\n\r\nI noticed that there seems no parallelism speedups for the batch operations. The running code is as in the following. When **B=1**, the printing time is **0.036327**, when **B=10**, the printing time is **0.2908551**. It seems there are basically no parallelisms for the batch operations. Is there a way to fix this issue?\r\n\r\n```\r\nimport time\r\nimport tensorflow as tf\r\n\r\nB, N, N = 1, 1024, 1024\r\n\r\na = tf.random_normal(shape=[B, N, N], dtype=tf.float64)\r\na = tf.matmul(a, a, transpose_b=True) + 1e-8 * tf.eye(N, dtype=a.dtype)\r\nb = tf.linalg.cholesky(a)\r\n\r\nsess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=10,\r\n                                 inter_op_parallelism_threads=10))\r\n\r\nfor _ in range(20):\r\n    sess.run(b)\r\n\r\nstart = time.time()\r\nfor _ in range(21, 30):\r\n    sess.run(b)\r\nprint(time.time() - start)\r\n```", "comments": ["@ssydasheng \r\nThe tf version used 1.14 is not supported, could you please upgrade to 2.x and let us know of the issue persist.", "Thanks for your suggestion. I tried using TF-2.3 + CUDA-10.1.  It seems to be faster, \r\n\r\n**B=1, N=1024: 0.02647** vs **B=10, N=1024: 0.117697**;\r\n**B=1, N=128: 0.0054** vs **B=10, N=128: 0.0054**.\r\n\r\nFor small matrices, the parallelism is awesome. For larger matrices, there are still parallelisms to certain degree. Seems pretty good. Is this in accord to your expectation as well, or you expected more parallelisms for N=1024 ?  \r\n\r\n```\r\nimport time\r\nimport tensorflow as tf\r\n\r\nB, N, N = 1, 1024, 1024\r\n\r\n@tf.function\r\ndef run():\r\n    a = tf.random.normal(shape=[B, N, N], dtype=tf.float64)\r\n    a = tf.matmul(a, a, transpose_b=True) + 1e-8 * tf.eye(N, dtype=a.dtype)\r\n    b = tf.linalg.cholesky(a)\r\n    return tf.reduce_mean(b)\r\n\r\nfor _ in range(20):\r\n    run()\r\n\r\nstart = time.time()\r\nfor _ in range(21, 30):\r\n    run()\r\nprint(time.time() - start)\r\n\r\n```"]}, {"number": 45234, "title": "[TF 2.3] Hessian of vector field in eager mode", "body": "**System information**\r\n- TensorFlow version: 2.3.0\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCalculate hessians in eager mode.\r\n\r\n**Will this change the current api? How?**\r\nThere should be `tf.batch_hessian()`, or something similar for eager mode; `tf.hessians()` does not work in eager mode.\r\n\r\n**Additional info**\r\nI am trying to compute the hessian of an embedding. Currently calling `tf.batch_jacobian()` twice, but this results in a tensor filled with 0s. \r\n\r\n**Code to reproduce issue.**\r\n```python\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(x)\r\n    y = model(x)\r\n  grads = tape.batch_jacobian(y, x)\r\n  hessian = tape.batch_jacobian(grads, x)\r\n  hessian.numpy().any() > 0\r\n\r\nOutput: False\r\n\r\n```", "comments": ["@Abdul-Moeed \r\n\r\nAre you interested to submit a PR that supports your feature. If so please submit a PR. Thanks!", "For now I'm just looking for a workaround. Is there a hacky way to calculate hessian of a vector function in eager mode?", "Hi @Abdul-Moeed, I think this is a duplicate of #29781. Have you seen that thread?", "Yes, but I couldn't find a workaround for Hessian of a vector field in that thread. Instead, the following works for me:\r\n\r\n```python\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(x)\r\n    with tf.GradientTape(persistent=True) as tape2:\r\n        tape2.watch(x)\r\n        y = model(x)\r\n    grads = tape2.batch_jacobian(y, x)\r\nhessian = tape.batch_jacobian(grads, x)\r\n```", "Got it. Given that other thread is still open I think it makes sense to close this one so we can track progress on Hessians in one place. Your solution will probably be helpful to other users as well searching for similar functionality. It seems from the other thread that the main confusion resulting in errors is that you need to call jacobian in the tape context to be able to call jacobian again on that.\r\n\r\nClosing this issue now so we can track progress in #29781.\r\n"]}, {"number": 45232, "title": "added subprocess.Popen to view pdf extension file", "body": "plot_model saves image as pdf extension as well but there is no option to directly open it as soon as the file is created to check the file as IPython only supports image format types.\r\nSo I have added subprocess library to open the pdf extension files as well so that they are opened as soon as it is created by the function plot_model.", "comments": ["Thanks for the PR. The current  behavior is intended."]}, {"number": 45231, "title": "Unable to restore a layer of class TextVectorization - Text Classification ", "body": "\r\n**System information**\r\nGoogle Colab\r\n\r\n\r\n**When I run the example provided by official tensorflow Basic text classification, everything runs fine until model  save. But when I load the model it gives me this error. \r\n\r\nRuntimeError: Unable to restore a layer of class TextVectorization. Layers of class TextVectorization require that the class be provided to the model loading code, either by registering the class using @keras.utils.register_keras_serializable on the class def and including that file in your program, or by passing the class in a keras.utils.CustomObjectScope that wraps this load call.\r\n**\r\n\r\n**Model should be loaded successfully and process raw input**\r\n\r\n**https://colab.research.google.com/gist/amahendrakar/8b65a688dc87ce9ca07ffb0ce50b84c7/44199.ipynb#scrollTo=fEjmSrKIqiiM**\r\n\r\n**Example Link: https://tensorflow.google.cn/tutorials/keras/text_classification** \r\n\r\nThere is another issue [(#44199)](https://github.com/tensorflow/tensorflow/issues/44199) like this but it was closed abruptly without any solution so that's why I have to create a new issue as I did not receive any response. Thank you\r\n", "comments": ["I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/24dd76c7a8f04e54dd5bd8ad6e937c6d/untitled472.ipynb).", "I was able to execute the script successfully using the hack mentioned in tagged github issue above. Find the [gist](https://colab.research.google.com/gist/ymodak/afb4b5015dd5b2abf017d1a79da6ee60/44199.ipynb) here.\r\n```python\r\n@tf.keras.utils.register_keras_serializable()\r\ndef custom_standardization(input_data):\r\n ...\r\n```", "> I was able to execute the script successfully using the hack mentioned in tagged github issue above. Find the [gist](https://colab.research.google.com/gist/ymodak/afb4b5015dd5b2abf017d1a79da6ee60/44199.ipynb) here.\r\n> \r\n> ```python\r\n> @tf.keras.utils.register_keras_serializable()\r\n> def custom_standardization(input_data):\r\n>  ...\r\n> ```\r\n\r\nThank you!\r\nLet me try it again, if all good, I will close the issue.", "I just tested it, it does work only if you save the model and load back in the same notebook like you did. As soon as you download the whole model folder and try to load it in new notebook, it starts giving same error.\r\n\r\n![image](https://user-images.githubusercontent.com/23615976/100745057-020c0d80-33df-11eb-9e1c-c289a5775342.png)\r\n\r\nFind [gist](https://colab.research.google.com/drive/1zNt8yz_mdW9JhZ0cHULr_lgYb_Qj9g_8?usp=sharing) here.\r\n", "@ymodak  Do we have any update here?", "Hello, Everyone!\r\nThere is no response on this issue in two weeks. Could someone kindly update the status here? Thank you", "Push +1", "l have the same problem./(\u3112o\u3112)/~~", "In your new notebook, you can try the following:\r\n\"\"\"\r\nimport re\r\nimport string\r\n\r\n@tf.keras.utils.register_keras_serializable()\r\ndef custom_standardization(input_data):\r\n  lowercase = tf.strings.lower(input_data)\r\n  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\r\n  return tf.strings.regex_replace(stripped_html,\r\n                                  '[%s]' % re.escape(string.punctuation), '')\r\n\r\n\"\"\"\r\nThen l reload the model successfully. Hope it can help you,", "But why do we need to preprocess the incoming text again? Input for the prediction should be raw!", "I have run into the same issue following this example, I agree that the work around is not a true solution. Documentation examples should run without errors. \r\n\r\nEven with the workaround I received this warning when having the model make predictions on raw text:\r\nWARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000285CEFBCEE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n\r\nWhich I suspect is due to inappropriate use of \r\n@tf.keras.utils.register_keras_serializable()\r\nin the workaround ", "> I have run into the same issue following this example, I agree that the work around is not a true solution. Documentation examples should run without errors.\r\n> \r\n> Even with the workaround I received this warning when having the model make predictions on raw text:\r\n> WARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function..predict_function at 0x00000285CEFBCEE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n> \r\n> Which I suspect is due to inappropriate use of\r\n> @tf.keras.utils.register_keras_serializable()\r\n> in the workaround\r\n\r\nCould you link the notebook where you were able to run the example and load model etc? Because for me, those example did not work at all (for loading part) when you try to load model into new notebook. Thank you", "In truth my code is quite different from the example, but I did use it as guidance for writing a custom standardization function as part of a TextVectorization layer. \r\nI assure you that my model saved and loaded perfectly before changing from lower_and_strip_punctuation to my own custom standardization in the TextVectorization layer. After switching I had the same RuntimeError you encountered in the head of this page. ", "I also ran into this error message (`RuntimeError: Unable to restore a layer of class TextVectorization. [...]`) when I implemented (and customized) the code from the \"Basic Text Classification\" tutorial.\r\n\r\nInstead of running the code in a notebook, I have two scripts, one for building, training and saving the model and the other one for loading it and making predictions.\r\n\r\nImplementing the comments above solved the issue for me. Because it was confusing to me, just to sum up what I had to do in order to get the script with the predictions running:\r\n\r\n1. I added this first line in the first script before the function definition and built, trained and saved the model again\r\n\r\n```\r\n@tf.keras.utils.register_keras_serializable()\r\ndef custom_standardization(input_data):\r\n[...]\r\n```\r\n\r\n2. I also had to add the same line and the whole function definition in the second script to make sure that it works if I restart(!) ipython (where I currently run the scripts) and only run the second script.\r\n\r\nIf I run the second script without restarting ipython after running the first script, I get this error:\r\n\r\n`ValueError: Custom>custom_standardization has already been registered [...]`\r\n", "Alternatively, you can just use the default standardization method in the vectorizer layer when building the model:\r\n\r\n```\r\nvectorize_layer = TextVectorization(\r\n        standardize='lower_and_strip_punctuation',\r\n        max_tokens=max_features,\r\n        output_mode='int',\r\n        output_sequence_length=sequence_length)\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45231\">No</a>\n", "The issue still persists. Can this be reopened?", "@statneoproteek , you can solve the issue by following the steps as suggested in the error. `RuntimeError: Unable to restore a layer of class TextVectorization. Layers of class TextVectorization require that the class be provided to the model loading code, either by registering the class using @keras.utils.register_keras_serializable on the class def and including that file in your program, or by passing the class in a keras.utils.CustomObjectScope that wraps this load call.`\r\n\r\nAttaching the working [code](https://colab.research.google.com/gist/sachinprasadhs/9471d05206aa1c206584c33a9d5d598f/44199.ipynb) with the above changes as reference. Thanks!", "Deserialization of the TextVectorization layer shouldn't  be dependent on the class to be present in the model loading code.  It definitely does not facilitate sharing models in a clean way. \r\nPlease correct me if I am wrong: Wouldn't passing the class in a keras.utils.CustomObjectScope would also need the class to be present in the model loading code?\r\n\r\nTL;DR: I think since the layer does not serialize completely, it warrants a fix.\r\nI am not sure if this is the github issue to handle this. Please feel free to point it out if that's the case.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@statneoproteek, This issue has been moved to keras-team/keras repo, you can track the further details using above linked issue request. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45231\">No</a>\n"]}, {"number": 45230, "title": "added subprocess.Popen to view pdf extension file", "body": "plot_model saves image as pdf extension as well but there is no option to directly open it as soon as the file is created to check the file as IPython only supports image format types.\r\nSo I have added subprocess library to open the pdf extension files as well so that they are opened as soon as it is created by the function plot_model.", "comments": []}, {"number": 45229, "title": "Problems for accessing CLA ", "body": "It seems there is no response for cla-submissions@google.com,\r\n\r\nPer for [Contributor License Agreements FAQs](https://cla.developers.google.com/about)\r\n \r\nWhat if I lose access to my contributor group or don't know the group for my company?\r\nIf you need help with your corporate contributor group, email cla-submissions@google.com.\r\n\r\nNo response (yet).", "comments": ["US has been in Thanksgiving holiday. Please send a ping by Wednesday if you still don't get a response.", "Hello,\r\n\r\nwe're still waiting for the response from cla-submissions@google.com ...\r\n\r\ncan you please help?", "@ewilderj , @theadactyl can you help here please?", "Looking into it. @peter197321 feel free to forward that email to me at ewj at google dot com so I can chase it down internally. Thanks.", "Resolved out-of-band in email.\r\n"]}, {"number": 45228, "title": "Building tests fails on POWER", "body": "Very short: There is an issue in the currently used abseil (as of TF 2.4-rc3 and master) which prevents abseil being built but which is required for running the tests (using `bazel test`)\r\n\r\nThis is happening on POWER only and fixed in https://github.com/abseil/abseil-cpp/commit/ea8a689cf5e71f31f96af78859eccc11161fa36a, see the error message there.\r\n\r\nSo I'd like TF to increase the currently used abseil version or add that commit as a patch please.", "comments": ["@Flamefire,\r\n\r\nAs per the latest stable version of Tensorflow which is `2.6.0`, I see that abseil 0.13.0 is used, which is the recent python implementation of [absl-py](https://pypi.org/project/absl-py/#history). Can you confirm if this resolves your issue? Thanks!", "I can confirm this has been fixed in TF 2.5+", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45228\">No</a>\n"]}, {"number": 45227, "title": "TFLite Conv.h inner loop optimizations", "body": "This PR contains a number of inner loop optimizations for TFLite's conv.h kernel, to avoid calling `Offset()` multiple times in the loop, and to do boundary checking once when possible. For a typical image model on TensorFlow Lite Micro running on a Cortex-M4F MCU (without CMSIS-NN kernels) we see a decrease in inferencing time from 388ms. to 302ms. \r\n\r\nI see that there are plenty of regression tests for this kernel, so I hope that CI runs them on this PR, as I have no idea how to do so :-) It has passed our internal regression suite, but we have only a small variety of models and we are branched off of TF 2.3 tag.\r\n\r\nThis work was done by @bitbank2 but sponsored by Edge Impulse - which is why I'm opening the PR.", "comments": ["Thanks for the PR. There is a tension here: this is reference code so one goal of the code is to provide a clear demonstration of the implementation without significant modification for performance optimization. However, it appears that this code is what is called for TFL Micro in a production setting. Over to @advaitjain to see if this is desirable on the Micro side.", "Echoing what @talumbau said: The reference code isn't really aimed at being performant (the goal being generality, readability and accuracy). As such, we are hesitant to make changes here for improving performance. For production use-cases both Lite and Micro rely on target-specific optimized kernel implementations.\r\n\r\nIt would be really useful to profile the [cmsis-nn kernels](https://github.com/tensorflow/tensorflow/tree/ebc70b7a592420d3d2f359e4b1694c236b82c7ae/tensorflow/lite/micro/kernels/cmsis-nn) and make any performance improvements in [upstream CMSIS-NN](https://github.com/ARM-software/CMSIS_5/tree/develop/CMSIS/NN).\r\n\r\nTensorflowLite Micro collaborates closely with the CMSIS-NN team and we are open to regularly updating the CMSIS versions to get additional performance improvements. \r\n\r\nI'm going to close the current PR but I'm also looping in @freddan80 from the CMSIS-NN team in case there are performance improvements in CMSIS-NN that you see an opportunity for.", "@advaitjain You might want to re-open this because at least 2 MCUs fall through the cracks - Cortex-M0/M3 and ESP32. Both end up running reference code because there is no optimized code for them, even in the CMSIS-NN functions. The CMSIS lib has optimized code for the M4/M7, but other MCUs will fall through to using the reference code indicated in this PR. This is the reason for the PR.", "I second @bitbank2's comment here. We use CMSIS-NN and the ARC kernels where possible, but there are *many* devices out there which use the reference kernels. This usecase comes directly from us trying to make inferencing faster on an M0+. We could swap out the conv kernel ourselves, but I'd like to keep close to mainline TF.", "Thanks for the additional context.\r\n\r\nWhat's missing for me is why we would want to use the reference kernels (even for M0/M3). My assumption was that CMSIS has fallback paths for all cortex M targets. If those can be improved, I would lean towards making changes in CMSIS-NN, but I'll let @freddan80 weigh in on this.\r\n\r\nI do understand the value of having portable TensorflowLite Micro kernels that are both efficient and applicable to multiple targets. For now, however, we have made the explicit choice to not go down that path and instead rely on target-specific optimized implementations, even at the cost of duplicated code.\r\n\r\nThis is to allow the maintainers of the optimized implementations freedom to decide the technical direction for their specific target, how they want to balance readability, code size and speed etc.\r\n\r\nAdding in @vikramdattu for ESP32 optimizations. I'd be happy to have a micro/kernels/esp32 if there is a desire to optimize kernels for that target but will let @vikramdattu decide when the right time for that might be.\r\n\r\nSome additional PRs for context on similar discussions:\r\n * https://github.com/tensorflow/tensorflow/pull/42715\r\n * https://github.com/tensorflow/tensorflow/pull/42477#issuecomment-676610773\r\n\r\n\r\nWhile I don't want to rule out portable efficient kernels for Tensorflow Lite Micro completely,  it is not a direction that we want to pursue right now. Especially for code that is shared between Lite and Micro.\r\n\r\nI recognize that these are subjective decisions that the TensorflowLite Micro team has made so we're happy to hear arguments that suggest a different path so that we can consider how we might proceed in the future.\r\n\r\nThe near to medium term path for these optimizations is to work with @freddan80 for CMSIS-NN and @vikramdattu on creating a directory for optimized ESP32 kernels.", "@advaitjain thanks for the mention.\r\nI have tested CMSIS reference versions some time back, also tested by just removing some redundant sanity checks + removing some macros(viz., offset) manually. It does make a difference. Was getting significant performance benefit with simple restructure.\r\n\r\nAs far as adding it as a ESP32 version, this is definitely something under consideration.", "@bitbank2 Optimizations are done in cmsis-nn for Arm Cortex-M processors without the DSP or MVE extensions(Likes of Cortex-M0, Cortex-M3). But, because of a bug in the TFLu cmsis-nn wrapper, the reference TFLu code was used even when specifying 'cmsis-nn' as a build option until a couple of months back. That was fixed in this PR that was merged on Sept 29 https://github.com/tensorflow/tensorflow/pull/43484.\r\n   The goal is to provide optimizations across all Cortex-M processors. \r\n "]}, {"number": 45226, "title": "Slower prediction of tflite model then expected", "body": "Hi,\r\n\r\nWhy trained object detection model quantized via tf.lite converter works more then 2 seconds per 1 sample on 1 CPU instance,\r\nwhile expected time ~20 ms mentioned in tf2 detection zoo? Is it normal? What can be wrong?\r\n\r\nI completed all necessary steps:\r\n\r\n1) Fine tuned model according to tensorflow-object-detection manual \r\n[https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#exporting-a-trained-model](url)\r\n\r\n   finetuned model: ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\r\n   [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md](url)\r\n\r\n2) Then froze tf graph using export_tflite_graph_tf2.py from [https://github.com/tensorflow/models/tree/master/research/object_detection](url)\r\n\r\n3) Converted model via tf.lite.TFLiteConverter. \r\nIt doesn't work for tf.2.3 version  (the same problems as in #(42065), #(41877)) , therefore I used tf-nightly==2.5.0.dev20201126\r\n\r\n4) Made a prediction from tflite model in Jupyter Notebook on 1 CPU instance\r\n\r\n`interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file), num_threads=1)`\r\n`interpreter.allocate_tensors()`\r\n`input_details = interpreter.get_input_details()`\r\n`output_details = interpreter.get_output_details()`\r\n\r\n`interpreter.set_tensor(input_details[0][\"index\"], features)`\r\n`interpreter.invoke()`\r\n`res = interpreter.get_tensor(output_details[0][\"index\"])`\r\n`interpreter.reset_all_variables()`\r\n\r\nI expect to get ~20 ms per one sample, but got ~2 seconds:\r\nCPU times: user 2.1 s, sys: 2.78 ms, total: 2.11 s\r\nWall time: 2.11 s\r\n\r\nAnd for ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8 - the latency is more then 2 **minutes**.\r\n\r\n**System information**\r\n- OS Platform and Distribution: RHEL 7.7\r\n- TensorFlow installed from (source or binary): from binary\r\n- TensorFlow version (or github SHA if from source): tensorflow-gpu==2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=SAVED_MODEL_PATH)`\r\n`converter.optimizations = [tf.lite.Optimize.DEFAULT]`\r\n`tflite_model = converter.convert()`\r\n\r\n**Failure details**\r\n- Producing correct results, but the model is slower than expected\r\n\r\n**Any other info / logs**\r\n\r\nRunning export_tflite_graph_tf2.py i got many info messages like this:\r\n```\r\nINFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf30107b8>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010748>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], True), {}).\r\nI1127 11:49:05.573841 139763811198784 def_function.py:1038] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf30107b8>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010748>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], True), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4400>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4470>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], False), {}).\r\nI1127 11:49:05.574001 139763811198784 def_function.py:1038] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4400>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4470>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], False), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2f95b70>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2f95be0>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2f95ef0>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], True), {}).\r\nI1127 11:49:10.198460 139763811198784 def_function.py:1038] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2f95b70>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2f95be0>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2f95ef0>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], True), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fac898>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fac908>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2facc18>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], False), {}).\r\nI1127 11:49:10.198984 139763811198784 def_function.py:1038] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fac898>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fac908>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2facc18>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], False), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf30107b8>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010748>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], True), {}).\r\nI1127 11:49:10.199471 139763811198784 def_function.py:1038] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf30107b8>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010748>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf3010780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], True), {}).\r\nINFO:tensorflow:Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4400>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4470>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], False), {}).\r\nI1127 11:49:10.199697 139763811198784 def_function.py:1038] Unsupported signature for serialization: (([(<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4400>, TensorSpec(shape=(None, 40, 40, 32), dtype=tf.float32, name='image_features/0/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4470>, TensorSpec(shape=(None, 20, 20, 96), dtype=tf.float32, name='image_features/1/1')), (<tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f1bf2fd4780>, TensorSpec(shape=(None, 10, 10, 1280), dtype=tf.float32, name='image_features/2/1'))], False), {}).\r\nINFO:tensorflow:Assets written to: exported-model/saved_model/assets\r\nI1127 11:49:11.159580 139763811198784 builder_impl.py:775] Assets written to: exported-model/saved_model/assets\r\nINFO:tensorflow:Writing pipeline config file to exported-model/pipeline.config\r\n```\r\n\r\nQuantization logs:\r\n```\r\n2020-11-27 15:22:43.446327: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags {serve}; Status: success: OK. Took 1257095 microseconds.                                                    \r\n2020-11-27 15:22:45.713440: I tensorflow/lite/tools/optimize/quantize_weights.cc:224] Skipping quantization of tensor ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/Conv1/Conv2D because it has fewer than 1024 elements (864).                                                                          \r\n2020-11-27 15:22:45.713525: I tensorflow/lite/tools/optimize/quantize_weights.cc:224] Skipping quantization of tensor ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/expanded_conv_depthwise_BN/FusedBatchNormV3;ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/expanded_conv_depthwise/depthwise;ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/block_5_project/Conv2D because it has fewer than 1024 elements (288).                                                                            \r\n2020-11-27 15:22:45.713543: I tensorflow/lite/tools/optimize/quantize_weights.cc:224] Skipping quantization of tensor ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/expanded_conv_project/Conv2D because it has fewer than 1024 elements (512).                                                          \r\n2020-11-27 15:22:45.713561: I tensorflow/lite/tools/optimize/quantize_weights.cc:224] Skipping quantization of tensor ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/block_1_depthwise_BN/FusedBatchNormV3;ssd_mobile_net_v2fpn_keras_feature_extractor/functional_1/block_1_depthwise/depthwise;ssd_mobil\r\ne_net_v2fpn_keras_feature_extractor/functional_1/block_12_project/Conv2D because it has fewer than 1024 elements (864).\r\n```\r\n", "comments": ["@valery-kustov \r\nPlease share a simple stand alone code to replicate the issue faced or if possible share a colab gist with the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45225, "title": "A hierarchical model for device placement.", "body": "Whether the technology mentioned in the paper\u2014A hierarchical model for device placement is open source\uff1fand what are the related APIs", "comments": ["@wjykl22 \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "hello, i am interested in this paper  too. do you have any info about the source code of this paper ? thanks for your help\r\n"]}, {"number": 45224, "title": "Unable to deserialize custom RNN cell decorated via register_keras_serializable when used inside tf.keras.layers.RNN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Ubuntu 18.04:\r\n- TensorFlow 2.3.1\r\n- Python 3.6:\r\n- CUDA 10.2, cuDNN 7.6.5.32\r\n- GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nCustom RNNCell layer decorated by ```tf.keras.utils.register_keras_serializable``` are not found when deserializing a ```tf.keras.layers.RNN``` object via ```tf.keras.models.load_model```\r\n\r\n**Describe the expected behavior**\r\nCustom layers decorated by ```register_keras_serializable``` should be able to be deserialized when used inside ```tf.keras.layers.RNN```. \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndata_x = np.random.default_rng().normal(size=(100, 16, 10))\r\ndata_y = np.random.default_rng().normal(size=(100, 16, 1))\r\n\r\n@tf.keras.utils.register_keras_serializable(package=\"Custom\")\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self, units, outputs, **kwargs):\r\n        self.units = units\r\n        self.outputs = outputs\r\n        super(CustomLayer, self).__init__(**kwargs)\r\n\r\n    @property\r\n    def state_size(self):\r\n        return self.units\r\n\r\n    def build(self, input_shape):\r\n        self.w = self.add_weight(\"w\", shape=(input_shape[-1], self.units))\r\n        self.o = self.add_weight(\"o\", shape=(self.units, self.outputs))\r\n        self.r = self.add_weight(\"r\", shape=(self.units, self.units))\r\n        self.built = True\r\n\r\n    def call(self, inputs, states):\r\n        next_hidden = tf.nn.tanh(\r\n            tf.matmul(inputs, self.w) + tf.matmul(states[0], self.r)\r\n        )\r\n        output = tf.matmul(next_hidden, self.o)\r\n        return output, [next_hidden]\r\n\r\n    def get_config(self):\r\n        return {\"units\": self.units, \"outputs\": self.outputs}\r\n\r\nmodel = tf.keras.models.Sequential(\r\n    [\r\n        tf.keras.Input((16, 10,)),\r\n        tf.keras.layers.RNN(CustomLayer(10, 1), return_sequences=True),\r\n    ]\r\n)\r\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\r\n\r\nmodel.fit(x=data_x, y=data_y, batch_size=25, epochs=1)\r\nmodel.evaluate(x=data_x, y=data_y)\r\nmodel.save(\"test_2.h5\")\r\n\r\nmodel = tf.keras.models.load_model(\"test_2.h5\")  ## Crashes here\r\nmodel.evaluate(x=data_x, y=data_y)\r\n```", "comments": ["@mlech26l,\r\nOn adding the argument `custom_objects={\"CustomLayer\": CustomLayer}` to the `load_model` function I was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0b9c130c321af03470c7f234ce4fd068/45224.ipynb). \r\n\r\nFor more information, please go through [this guide](https://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects) for saving models with custom objects and [this similar](https://stackoverflow.com/a/50838031) StackOverflow query. Thanks!", "@amahendrakar thanks. I was already aware of the ```custom_objects``` argument workaround, however, it does not provide a clean solution for third-party libraries that implement several RNN cell classes. \r\n\r\nThus, will this feature be added in the future or do third-party libraries have to provide a ```dict``` storing all serializable RNN cell?\r\nThanks!", "@mlech26l Can you please share the code where `load_model` fails with third-party libraries? Thanks!", "Hi @jvishnuvardhan \r\nThe [keras-ncp](https://github.com/mlech26l/keras-ncp) define RNNCells similar to the snippet above and crashes. \r\nCurrently, the only option is to pass the ```custom_objects``` parameter to the ```load_model``` function, which is a sub-optimal solution. \r\nI highly expect that the Keras-API decorator is intended to cover the use inside an RNN object as well.\r\n\r\nThanks. ", "Hi, \r\nThis issue has been fixed with the 2.4 release\r\n\r\nThank you"]}, {"number": 45223, "title": "DataLossError: Unable to parse tensor from stored proto when loading saved large tf dataset.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 x64\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0rc3\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11, 8.0.2\r\n- GPU model and memory: RTX 3080 10GB\r\n\r\n\r\n\r\n**Code:**\r\nhttps://drive.google.com/file/d/1oIjXa-gH-g8SB4jGisu8nPxMd5TJgoDa/view?usp=sharing\r\n\r\n\r\n\r\n**Full error log:**\r\n```\r\n---------------------------------------------------------------------------\r\nDataLossError                             Traceback (most recent call last)\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in execution_mode(mode)\r\n   2112       ctx.executor = executor_new\r\n-> 2113       yield\r\n   2114     finally:\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py in _next_internal(self)\r\n    729     with context.execution_mode(context.SYNC):\r\n--> 730       ret = gen_dataset_ops.iterator_get_next(\r\n    731           self._iterator_resource,\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)\r\n   2577     except _core._NotOkStatusException as e:\r\n-> 2578       _ops.raise_from_not_ok_status(e, name)\r\n   2579     except _core._FallbackException:\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in raise_from_not_ok_status(e, name)\r\n   6861   # pylint: disable=protected-access\r\n-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6863   # pylint: enable=protected-access\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nDataLossError: Unable to parse tensor from stored proto. [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataLossError                             Traceback (most recent call last)\r\n<ipython-input-6-a2ce188c35b6> in <module>\r\n----> 1 tds1_1 = next(iter(tds1_loaded.take(1)))\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py in __next__(self)\r\n    745   def __next__(self):\r\n    746     try:\r\n--> 747       return self._next_internal()\r\n    748     except errors.OutOfRangeError:\r\n    749       raise StopIteration\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py in _next_internal(self)\r\n    737         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    738       except AttributeError:\r\n--> 739         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    740 \r\n    741   @property\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n    129                 value = type()\r\n    130             try:\r\n--> 131                 self.gen.throw(type, value, traceback)\r\n    132             except StopIteration as exc:\r\n    133                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in execution_mode(mode)\r\n   2114     finally:\r\n   2115       ctx.executor = executor_old\r\n-> 2116       executor_new.wait()\r\n   2117 \r\n   2118 \r\n\r\n~\\anaconda3\\envs\\hk2\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py in wait(self)\r\n     67   def wait(self):\r\n     68     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 69     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     70 \r\n     71   def clear_error(self):\r\n\r\nDataLossError: Unable to parse tensor from stored proto.\r\n```", "comments": ["@Crispy13 \r\nUnable to acess the code shared, please share colab gist with error reported and access to view it.", "@Saduf2019 \r\nI'm sorry. I uploaded it to colab.\r\n\r\nhttps://colab.research.google.com/drive/1LhN2VjBKkxev0T3hIbgvJq0nrkGJRY0A?usp=sharing", "@ymodak \r\nI ran the code shared  and colab crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4bf1a8103e65b1cd1282a2971fbd8247/untitled472.ipynb).", "I see that the colab crashes due OOM  and works if we reduce tensor size.", "@Crispy13 Was able to reproduce the issue in TF 2.5 and the system crashes after utilizing entire memory. As suggested above Please try reducing the size of the tensor.Thanks!", "@Crispy13  Please let us know the update on this.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45223\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45223\">No</a>\n"]}, {"number": 45222, "title": "Integrate Renode tests with TFLu makefile", "body": "This adds Renode tests to the Makefile.\r\nPlease note, that this is based on #44729, and should be merged after it.\r\n\r\nAddresses http://b/172939049", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "I'm going to have to wait for #44729 to be merged to better see the changes specific to this PR."]}, {"number": 45221, "title": "[Old Link to NVIDIA Driver] Ubuntu 16.04 (CUDA 10.1)", "body": "## URL(s) with the issue\r\n- https://www.tensorflow.org/install/gpu\r\n\r\n## Description of the issue:\r\nIn the section `Ubuntu 16.04 (CUDA 10.1)`, the following command is used to download the driver\r\n\r\n```bash\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\r\n```\r\n\r\nBut the link specified is actually outdated and invalid so that it has to be replaced as follows\r\n- Old: http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\r\n- New: https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb", "comments": ["Hii, I would like to update this.", "@Rowing0914 \r\n\r\nThis issue got merged in the [PR#1755](https://github.com/tensorflow/docs/pull/1755). Please, verify once and close the issue. Thanks!", "Thank you!!"]}, {"number": 45220, "title": "Addition for resources ", "body": "Added Coursera course on Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45220) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 45219, "title": "ValueError: Variable <tf.Variable 'conv1_5/kernel:0' shape=(3, 3, 3, 32) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.", "body": "I'm trying a new model involving word embedding for labels as well as Image classification in order to increase accuracy of the Image classifier. \r\nI'm using tensorflow 1.15.0 Version.\r\nHere is the code of the model and summary.\r\n```\r\ndef build_mobilenet_embedd_model(img_input ):\r\n    input_x = Input(shape = (img_input.shape[1:]), name='feature')\r\n    base_model = MobileNet(include_top= False, input_shape=img_input.shape[1:], weights='imagenet')(input_x)\r\n    avgpool_x = GlobalAveragePooling2D()(base_model)\r\n    dense1_x = Dense(2048, activation='relu')(avgpool_x)\r\n    batch1_x = BatchNormalization()(dense1_x)\r\n    dropout1_x = Dropout(0.2)(batch1_x)\r\n    dense2_x = Dense(512, activation='relu')(dropout1_x)\r\n    batch2_x = BatchNormalization()(dense2_x)\r\n    dropout2_x = Dropout(0.2)(batch2_x)\r\n    predictions = Dense(len(all_labels), activation='relu')(dropout2_x)\r\n    embedd_layer = Embedding(13, 100, weights=[embedding_matrix], trainable=False,name = \"embedding_1\")(predictions)\r\n    flatten = Flatten(name=\"flatten\")(embedd_layer)\r\n    final_predictions = Dense(13, activation='relu')(flatten)\r\n    model = keras.models.Model(inputs= input_x, outputs = final_predictions)\r\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics= METRICS\r\n    model.summary()\r\n    return model\r\n```\r\nand here is the summary:-\r\n```\r\nModel: \"model_4\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nfeature (InputLayer)         [(None, 128, 128, 3)]     0         \r\n_________________________________________________________________\r\nmobilenet_1.00_128 (Model)   (None, 4, 4, 1024)        3228864   \r\n_________________________________________________________________\r\nglobal_average_pooling2d_5 ( (None, 1024)              0         \r\n_________________________________________________________________\r\ndense_16 (Dense)             (None, 2048)              2099200   \r\n_________________________________________________________________\r\nbatch_normalization_10 (Batc (None, 2048)              8192      \r\n_________________________________________________________________\r\ndropout_10 (Dropout)         (None, 2048)              0         \r\n_________________________________________________________________\r\ndense_17 (Dense)             (None, 512)               1049088   \r\n_________________________________________________________________\r\nbatch_normalization_11 (Batc (None, 512)               2048      \r\n_________________________________________________________________\r\ndropout_11 (Dropout)         (None, 512)               0         \r\n_________________________________________________________________\r\ndense_18 (Dense)             (None, 13)                6669      \r\n_________________________________________________________________\r\nembedding_1 (Embedding)      (None, 13, 100)           1300      \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1300)              0         \r\n_________________________________________________________________\r\ndense_19 (Dense)             (None, 13)                16913     \r\n=================================================================\r\nTotal params: 6,412,274\r\nTrainable params: 6,383,966\r\nNon-trainable params: 28,308\r\n_________________________________________________________________\r\n```\r\nAdding `embedd_layer ` and `flatten layer` after the layer `prediction` resulted in this error to pop-up,\r\nwhen I run the model fit I'll get this error message.\r\n```\r\nValueError: Variable <tf.Variable 'conv1_5/kernel:0' shape=(3, 3, 3, 32) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.\r\n\r\n```", "comments": ["@WrathofBhuvan11,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks!", "> @WrathofBhuvan11,\r\n> TensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks!\r\n\r\nI tried in tf Version 2\r\nI'm getting this error,\r\n```\r\nInvalidArgumentError:  assertion failed: [predictions must be <= 1] [Condition x <= y did not hold element-wise:] [x (model/dense_3/Relu:0) = ] [[0.179184318 1.62250423 0...]...] [y (metrics/auc/Cast_2/x:0) = ] [1]\r\n\t [[{{node metrics/auc/assert_less_equal/Assert/AssertGuard/else/_11/Assert}}]] [Op:__inference_distributed_function_8508]\r\n```\r\nthis issue which I have posted in detail (https://github.com/tensorflow/tensorflow/issues/45241)\r\nFunction call stack:\r\ndistributed_function", "> @WrathofBhuvan11,\r\n> TensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks!\r\nHello sir I have given complete detail of this new error which I'm getting in tf version 2 in this link\r\nhttps://github.com/tensorflow/tensorflow/issues/45241", "@WrathofBhuvan11,\r\nCan you please close this issue as it is already being tracked in [#45241](https://github.com/tensorflow/tensorflow/issues/45241). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45219\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45219\">No</a>\n"]}, {"number": 45218, "title": "need a  quantized model for posenet tensorflow lite", "body": "System information\r\n\r\nTensorFlow version (you are using): 2.3.1\r\nAre you willing to contribute it (Yes/No): Yes\r\nDescribe the feature and the current behavior/state.\r\nThere is no quantized model for posenet\r\n\r\nWill this change the current api? How?\r\nNo\r\n\r\nWho will benefit with this feature?\r\nPeople who need to run pose detection on edge IOT NPUs.\r\n\r\nAny Other info.\r\ntensorflow js has a quantized model already, https://github.com/tensorflow/tfjs-models/tree/master/posenet , can we get it in tensorflow lite?", "comments": ["@yangcheng \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "Thanks, should I mark it as a feature reqeust? ", "@yangcheng\r\nplease explain in details the issue faced and an example code if any.", "@Saduf2019 the request is providing a new quantized model for posenet. there is no example code. What kind of detail should I provide here?\r\n", "Hi @yangcheng !Have you checked INT8 and FP16 quantized models from [MoveNet](https://www.tensorflow.org/lite/examples/pose_estimation/overview#performance_benchmarks) for pose estimation? ", "I was using MLKit for now. \r\nThanks for the information. I'll take a look at movenet now!", "@yangcheng ! Shall we move this feature request to closed status then?"]}, {"number": 45217, "title": "tokenizer = tfds.features.text.Tokenizer(),error is has no attribute 'text'.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windowns10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):tf.2.3\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):none\r\n- GCC/Compiler version (if compiling from source):none\r\n- CUDA/cuDNN version:none\r\n- GPU model and memory:CPU ,16G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nAttributeError: module 'tensorflow_datasets.core.features' has no attribute 'text'\r\n**Describe the expected behavior**\r\nrun program to use.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**the code is that, run it in the google colab, Can you tell me the reason?**\r\n\r\ntokenizer = tfds.features.text.Tokenizer()\r\n\r\nvocabulary_set = set()\r\nfor text_tensor, _ in all_labeled_data:\r\n  some_tokens = tokenizer.tokenize(text_tensor.numpy())\r\n  vocabulary_set.update(some_tokens)\r\n\r\nvocab_size = len(vocabulary_set)\r\nvocab_size\r\n", "comments": ["@funny000 \r\n\r\nPlease, refer the [link](https://github.com/tensorflow/datasets/issues/2647) and see if it helps you.\r\nIf the issue still persists please share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Yes, I met the same issue too! ", "The method has been deprecated, instead of using tfds.features.text.Tokenizer(), you can try tfds.deprecated.text.Tokenizer()", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45217\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45217\">No</a>\n"]}, {"number": 45215, "title": "Making label smoothing documentation more helpful", "body": "label_smoothing documentation is vague for both Categorical and BinaryCrossentropy losses.\r\n\r\nThe CategoricalCrossentropy class' documentation is especially confusing.\r\nFirstly, it appears to be referring only to the case where there are 2 output classes (as if it were BinaryCrossentropy), therefore the example doesn't generalise.\r\nSecondly, the smoothing is implemented in a non-intuitive way, adding part of the smoothing on to the reduced label. This isn't explained properly by the documentation, requiring users to view source code.\r\n\r\nThe new documentation states both loss functions' label_smoothing implementation more generally and completely, and includes a better spelled-out example for CategoricalCrossentropy.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45215) for more info**.\n\n<!-- need_sender_cla -->", "@j-bernardi  Can you please sign CLA. Thanks!", "@googlebot I signed it!\n\nOn Fri, Nov 27, 2020 at 12:25 AM google-cla[bot] <notifications@github.com>\nwrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here with @googlebot\n> I signed it! and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2139\ufe0f *Googlers: Go here\n> <https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45215>\n> for more info*.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/45215#issuecomment-734515266>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIA5EHYHGO7UDLHJH4PZZQLSR3WWDANCNFSM4UEI5NXA>\n> .\n>\n"]}, {"number": 45214, "title": "disable \"Successfully opened dynamic library libcudart.so\" logger", "body": "**Describe the current behavior**\r\n\r\n\r\nPlease kindly turn off this logger:\r\n```\r\n$ python -c \"import tensorflow\"\r\n2020-11-26 12:14:41.249276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nShould not log by default unless requested to. \r\n\r\nIf this log line is important in some situation please make it configurable with the default being silent.\r\n\r\nThank you!\r\n\r\nUsing tf-nightly-gpu-2.5.0.dev20201120", "comments": ["Thanks, this is a good feature request and we'll keep it in mind for TF 2.5.\r\n\r\nAny chance you're willing to make a PR?", "I don't know/use tf - it just gets loaded in `transformers` whether we want it or not along with pytorch. So please guide me if you'd like me to make a PR.\r\n\r\nUsing grep it seems to be coming from:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/1feb592358c3ab4dfe89ea7848993c8201b860c8/tensorflow/stream_executor/platform/default/dso_loader.cc#L49\r\n\r\nBut most likely the issue is that by default the log-level shouldn't be INFO, but rather WARNING or higher. But I don't know where you have that defined.\r\n\r\nThanks.", "I agree with @stas00, the default log level should not be `INFO`.", "I also agree with @stas00, INFO logs should be able to disable", "Yeah, it's really annoying to work with. ", "This is fixed after https://github.com/tensorflow/tensorflow/commit/73f56e99958893bbe536e8282688cfc6080d4034."]}, {"number": 45213, "title": "Tensorflow lite int8 quantisation for depth_to_space() op", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No(t now)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe operation depth_to_space() currently seems to not be supported for quantisation to 8-bit in tensorflow lite.\r\n\r\n**Who will benefit with this feature?**\r\ndepth_to_space() is frequently used in super-resolution models and sometimes in models that do upsampling. \r\nThese models are usually more computationally expensive than traditional classifiers because of their massive intermediate feature maps; hence, they would greatly benefit from int8 quantisation when deployed on mobile hardware, but that's not possible without int8 support for depth_to_space().\r\nIs this in the works? Has it been planned for a release?\r\n\r\nBest\r\n", "comments": ["Hello?", "Should be supported now. You can try with later nightly builds.\r\n\r\nThanks", "Hey, thank you for your help.\r\n\r\nI installed the latest nightly version of tensorflow (2.5.0.dev20201210), converted a TensorFlow model using the depth_to_space() op to TFLite with int8 quantisation (no float fallback), and tried running it on an android device. \r\n\r\nWhen using NNAPI, the following error appears:\r\n```\r\nI/tflite: Created TensorFlow Lite delegate for NNAPI.\r\nE/org.tensorflow.benchmarking.MainActivity: Error initialising tflite interpreter\r\n    java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'DEPTH_TO_SPACE' version '2'\r\n```\r\nAm I supposed to compile tensorflow lite myself, or is there something else I'm missing? \r\n\r\nThe build.gradle file in the app I wrote imports tensorflow lite as below:\r\n```\r\ndependencies {\r\n    implementation 'androidx.appcompat:appcompat:1.1.0'\r\n    // Build off of nightly TensorFlow Lite\r\n    implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly') { changing = true }\r\n    implementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly') { changing = true }\r\n    implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly') { changing = true }\r\n}\r\n```\r\n\r\nI'd be grateful if you could point me in the right direction, and let me know if you need any more information.\r\n", "That means the TFLite runtime you're using is not the latest.\r\nThere is an issue with Jcenter not up to date which we are looking into it (might be the reason).\r\nCan you access this file and use it\r\n https://bintray.com/google/tensorflow/download_file?file_path=org%2Ftensorflow%2Ftensorflow-lite%2F0.0.0-nightly%2Ftensorflow-lite-0.0.0-nightly.aar", "Thanks for the file.\r\n\r\nIn addition to that, I also had to fetch the .aar archive for tensorflow-lite-support from Jcenter manually, otherwise an error popped up, which I'm chalking up to some confict between the local module for tensorflow-lite and the remote module for tensorflow-lite-support: \r\n```\r\nMore than one file was found with OS independent path 'lib/arm64-v8a/libtensorflowlite_jni.so'\r\n```\r\nWith both modules installed locally, I was able to build and run the app.\r\n\r\nSomething interesting I noticed, the int8 quantised version of depth_to_space() doesn't seem to provide any speed advantage over the float version (in a model quantised with float fallback). Is there any way to check in Android what's happening inside the model, e.g. whether it uses only int8 operations?", "@Cy-r0 Thanks for confirming.\r\nYou can visualize the model and check the ops using https://netron.app/"]}, {"number": 45212, "title": "Very poor results from GPU implementation using CNN Tutorial", "body": "**System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I have used the stock code provided by [The CNN Tutorial](https://www.tensorflow.org/tutorials/images/cnn).\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Windows 10 Pro, Version 20H2\r\n- **TensorFlow installed from (source or binary):** TensorFlow installed via pip command `pip install tensorflow`\r\n- **TensorFlow version (use command below):** v2.3.0-54-gfcc4b966f1 2.3.1\r\n- **Python version:** 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- **CUDA/cuDNN version:** CUDA v10.1, cuDNN v7.6.0.64\r\n- **GPU model and memory:** NVIDIA GeForce RTX 3080, 10GB\r\n\r\n\r\n**Describe the current behaviour**\r\nI followed the CNN tutorial linked above, using both GPU and CPU implementations. Whilst the CPU implementation produced very similar results to the graphs displayed on the tutorial, the GPU implementation never surpassed 0.1 accuracy in training.\r\n\r\nI enabled the CPU implementation by the following line of code:\r\n```python\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n```\r\n\r\nI discovered this bug by testing and training my own implementation which, similarly, had good results using CPU and terrible results using GPU.\r\n\r\n**Describe the expected behaviour**\r\nI expect the GPU implementation results to closely match that of the CPU implementation results.\r\n\r\n**Standalone code to reproduce the issue**\r\nLink to the jupyter notebook can be found [here](https://drive.google.com/file/d/1C6bnXhBqHCv-rNqS1QWWf4yZx-6x-qte/view?usp=sharing).\r\n\r\nWith minimal code below:\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_images, train_labels, epochs=10, \r\n                    validation_data=(test_images, test_labels))\r\n```\r\n\r\n**Other info / logs**\r\nBelow is the output from the `model.fit` step in the code provided above.\r\n\r\n> Epoch 1/10\r\n1563/1563 [==============================] - 4s 3ms/step - loss: 2.3028 - accuracy: 0.0960 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 2/10\r\n1563/1563 [==============================] - 4s 2ms/step - loss: 2.3028 - accuracy: 0.0975 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 3/10\r\n1563/1563 [==============================] - 4s 2ms/step - loss: 2.3028 - accuracy: 0.0968 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 4/10\r\n1563/1563 [==============================] - 4s 2ms/step - loss: 2.3028 - accuracy: 0.0967 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 5/10\r\n1563/1563 [==============================] - 4s 3ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 6/10\r\n1563/1563 [==============================] - 4s 3ms/step - loss: 2.3028 - accuracy: 0.0970 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 7/10\r\n1563/1563 [==============================] - 4s 3ms/step - loss: 2.3028 - accuracy: 0.0972 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 8/10\r\n1563/1563 [==============================] - 4s 2ms/step - loss: 2.3028 - accuracy: 0.0980 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 9/10\r\n1563/1563 [==============================] - 4s 2ms/step - loss: 2.3028 - accuracy: 0.1002 - val_loss: 2.3026 - val_accuracy: 0.1000\r\nEpoch 10/10\r\n1563/1563 [==============================] - 4s 2ms/step - loss: 2.3028 - accuracy: 0.1007 - val_loss: 2.3026 - val_accuracy: 0.1000\r\n\r\nThe python process that was running the jupyter notebook displayed this warning:\r\n> 2020-11-26 17:44:27.891143: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\nlibrary cudnn64_7.dll\r\nation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n\r\n", "comments": ["@mattlyon93,\r\nI did not observe much difference in the accuracy while running the code on [CPU](https://colab.research.google.com/gist/amahendrakar/eadf44fb38c5d44b715cfe7ea28ed5f1/45212-cpu.ipynb) and [GPU](https://colab.research.google.com/gist/amahendrakar/a77a13624c25bed71b3bd47c5d4286db/45212-gpu.ipynb). Please check the attached gist for reference. Thanks!", "It turns out the issue was due to the version of CUDA. Reading through comments in issue [#43588](https://github.com/tensorflow/tensorflow/issues/43588), it seems that the RTX 3080 is only compatible with CUDA 11.1+, therefore I upgraded to the latest cuDNN and CUDA versions, and installing `tf-nightly`.\r\n\r\nNow on my Windows machine, the GPU is not used and presents this error:\r\n\r\n> 2020-11-27 12:32:42.294382: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1764] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\n\r\nI have double checked my installation to ensure that cuDNN, CUDA, & CUPTA have been installed and added to the environment `PATH`.", "I have fixed this issue by installing the following: `CUDA v11.0`, `cuDNN 8.0.2`, `tensorflow v2.4.0rc3`."]}]