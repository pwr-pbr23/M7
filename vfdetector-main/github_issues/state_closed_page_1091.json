[{"number": 20539, "title": "fix: slot and primary can be different shape", "body": "See github issue #19457 ", "comments": ["@martinwicke please review my PR or assign to someone else\r\nthanks", "@protoget ", "@martinwicke  Can you assign to someone else to review? @protoget didn't response on this.", "Nagging Assignee @martinwicke: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@allenlavoie you had commented on the issue, you're probably in a good position to properly review this. ", "FYI this was just rolled back because (we think) of MetaGraph import issues with variable->constant conversion that it triggered. I have requested a snippet to reproduce the issue and will follow up when I have one with a plan to get this resubmitted."]}, {"number": 20538, "title": "[Bazel/MSVC] Enable png SIMD for MSVC", "body": "", "comments": ["Windows Bazel builds all failed at link step: `LINK : fatal error LNK1000: unknown error at 000000013FE44241; consult documentation for technical support options`. Probably just flake.\r\n\r\nMacOS Contrib also failed with mysterious \"Internal CI infrastructure error\".\r\n\r\nRebased against master, please trigger CI test again.\r\n", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20537, "title": "[Bazel/MSVC] Enable jpeg SIMD for MSVC", "body": "", "comments": ["Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I cannot tell how, but the failures look legit.\r\nThis is what we see on windows:\r\n```\r\nERROR: T:/tmp/bigvaudl/external/nasm/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@nasm//:nasm':\r\nthis rule is missing dependency declarations for the following files included by 'external/nasm/stdlib/strlcpy.c':\r\n  'external/nasm/config/msvc.h'\r\n```\r\n@rongjiecomputer could you conditionally add msvc.h  to this list, maybe what will fix it?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/nasm.BUILD#L45\r\nI think with the new headers, somehow different code paths are activated in nasm by some macros.", "@gunan https://github.com/tensorflow/tensorflow/pull/20537/commits/f22edf35083dff5f54636fab2ecac3672a3d2d7d should fix the build error for nasm. Please trigger CI build again.", "#21993 is going to break this PR again. Can we get this or that PR merged soon?", "@rongjiecomputer the update to 2.0.0 needs to go in quickly cuz it fixes some CVEs in the current version. That version passes all the tests but I don't really know how to do the SIMD stuff on windows. It looks like 2.0.0 adds quite a bit of support tho so ideally could you rebase this on top of the 2.0.0 one to enable the windows SIMD stuff?", "@perfinion I will rebase and update this PR when your PR is merged. It is just that all my Windows PRs tend to drag very long before it gets imported for some reason. Having to keep updating PR due to newer changes breaking my PR is pretty frustrating.", "@rongjiecomputer yep I hear you. Ping me when it's rebased and I'll help push it through. Thanks for the patience :-)", "Can I get a CI test? Thanks!", "@rongjiecomputer Looks like some windows test failures. can you take a look?\r\nAlso can you squash those commits together when you update it? `git rebase -i origin/master` then change the pick to squash. The early commits are just confusing with the new 2.0 stuff so better have it all in one so there are no issues when merging in later.", "@perfinion Windows builds failed because symbols from `simd/x86_64/jsimd.c` was missing. Added them back.\r\n\r\nI have squashed the commits as requested. I did not do it initially as I thought it will make it difficult for reviewer to see what I have changed since last review."]}, {"number": 20536, "title": "[Bazel/MSVC] Cleanup flags and config_settings for MSVC", "body": "", "comments": ["PTAL @mrry ", "@rongjiecomputer Thank you so much for doing those clean up work!\r\nCould you also remove all usage of `//tensorflow:windows_msvc` config_settting? Because that's a legacy config_setting which is now identical to `//tensorflow:windows`.", "> Could you also remove all usage of //tensorflow:windows_msvc config_settting? Because that's a legacy config_setting which is now identical to //tensorflow:windows.\r\n\r\nDone. I have removed all references to `:windows_msvc` I can find with Github search. Let's see if this break CI build.", "@meteorcloudy After our last conversation, one more `windows_msvc` slipped into the codebase https://github.com/tensorflow/tensorflow/commit/158cd6220231fcf758a45c2dcd40d93cd0aec9e0#diff-15609f78c0cd35bfdc31f9d8bd9b1e91R2171.\r\n\r\nI have rebased and removed the `windows_msvc`.\r\n\r\nPlease help to spread awareness to internal team to not use `windows_msvc` anymore.", "@meteorcloudy maybe we can use a METADATA warning to prevent new occurrences.", "Thanks @martinwicke , will do it!", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like my change is pushed from internal already. Closing this now. \ud83d\ude04 "]}, {"number": 20535, "title": "r1.9-cherry-pick-request: Docs updates for tutorials, guides, and notebooks", "body": "This documentation update brings 1.9 up-to-date with master for tutorials, guides, and the eager notebooks.\r\nThis is a straight checkout from master---no commit cherrypicks, no merges. Only the doc files are updated. This loses some history for these late commits on the 1.9 branch, but that's not very important. Commit history is maintained on master.\r\n\r\n", "comments": ["I realize this is an annoying cherrypick, but documentation cat needs it to publish tensorflow.org ...\r\n![cat-on-computer](https://i.imgur.com/au632bU.jpg)", "Thank you!"]}, {"number": 20534, "title": "Reapplying #20285 for the fix to convert_graph.cc", "body": "The fix was lost due to some (unknown?) merging issues, similar to the  one mentioned in #20460.", "comments": ["Thanks @aaroey. Are there any other files from https://github.com/tensorflow/tensorflow/commit/1e7b0e4ad6d0f57f3241fe0b80a65f2c2a7f11b0#diff-d4d68c9604eef9b135422b83ac451c3b look suspicious?", "@yifeif they look good to me. It seems to me that the problem only affects changes committed before 1e7b0e4 to files listed in that commit.", "Thanks for checking @aaroey. Looks like we had a bad merge."]}, {"number": 20533, "title": "Branch 203192045", "body": "", "comments": []}, {"number": 20532, "title": "freeze_graph doesn't work", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu \r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9/7.1\r\n- **GPU model and memory**: dual gtx 1080ti , 11gb\r\n- **Exact command to reproduce**: \r\n\r\n`python freeze_graph.py \\\r\n    --input_checkpoint=full-path-to-logs/mnist-9999 \\\r\n    --output_graph=model.pb \\\r\n    --output_node_names=fc2/Relu`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 382, in <module>\r\n    run_main()\r\n  File \"freeze_graph.py\", line 379, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"freeze_graph.py\", line 378, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"freeze_graph.py\", line 272, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"freeze_graph.py\", line 128, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\r\n    self.build()\r\n  File \"/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/dhingratul/.virtualenvs/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1372, in _build\r\n    raise ValueError(\"No variables to save\")\r\nValueError: No variables to save\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@dhingratul I would recommend you to use the Estimator API if possible and export a saved model. It generates a frozen model (.pb file) and it can be easily deployed.\r\n\r\nIf you do need to freeze the model in an old fashion way take a look on the script below I have been using until TensorFlow 1.6. It is a little different from what is provided by TF official repo\r\n\r\n\r\n```python\r\n\r\nimport os\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\n\r\ndef freeze_graph(model_dir, output_tensors, output_pb):\r\n    \"\"\" Freeze graph model into **.pb** file\r\n    \r\n    Extract the sub graph defined by the output nodes and convert all its variables to constants\r\n\r\n    Args:    \r\n        ``model_dir`` (str): directory that contains all checkpoint files (.ckpt, .meta, .info)\r\n        \r\n        ``output_tensors`` (str):  comma separated list of all the output node's names\r\n\r\n        ``output_pb`` (str): output **.pb** frozen graph file      \r\n        \r\n    \"\"\"\r\n    if not tf.gfile.Exists(model_dir):\r\n        raise AssertionError(\r\n            \"Export directory doesn't exists. Please specify an export \"\r\n            \"directory: %s\" % model_dir)\r\n\r\n    if not output_tensors:\r\n        tf.loogin.error(\"You need to supply the name of a node to --output_tensors.\")\r\n        return -1\r\n\r\n    # Retrieve checkpoint fullpath\r\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\r\n    input_checkpoint = checkpoint.model_checkpoint_path\r\n    \r\n    # Clear original devices from graph\r\n    clear_devices = True\r\n\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        # Import graph from .meta file\r\n        saver = tf.train.import_meta_graph(\r\n            input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n        input_graph_def = tf.get_default_graph().as_graph_def()\r\n\r\n        # and restore weights\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        # Convert variables to constants\r\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess,\r\n            input_graph_def,            \r\n            output_tensors.split(\",\"),\r\n            variable_names_blacklist=['global_step']\r\n        )\r\n\r\n        # Dump frozen model\r\n        with tf.gfile.GFile(output_pb, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n        tf.logging.info(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n    #return output_graph_def\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_dir\", type=str, default=\"\",\r\n                        help=\"Model folder to export\")\r\n    parser.add_argument(\"--output_tensors\", type=str, default=\"\",\r\n                        help=\"The name of the output nodes, comma separated.\")\r\n    parser.add_argument(\"--output_pb\", type=str,\r\n                        default=\"frozen_model.pb\", help=\"Output pb file\")\r\n    args = parser.parse_args()\r\n\r\n    freeze_graph(args.model_dir, args.output_tensors, args.output_pb)\r\n```", "Thanks. I have a working script that works for me, but I read in some documentation for tf quantization, I need to use freeze_graph from tensorflow. If I use a script(like the one above), tf quantization fails.", "@dhingratul can you provide the exact code or script needed to repro your problem?", "The script is the one on tensorflow's page (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py)\r\n", "From looking at `freeze_graph.py`, this means that either the GraphDef doesn't match the checkpoint (all of the `tensor = sess.graph.get_tensor_by_name(key + \":0\")` lines throw KeyError) or the checkpoint is empty. I'm guessing the first is more likely.\r\n\r\nWhere did the GraphDef and checkpoint come from?", "I trained a very simple MNIST Model, see the first half of the code from here,\r\nhttps://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/workflows/tf_to_tensorrt.html", "> @dhingratul I would recommend you to use the Estimator API if possible and export a saved model. It generates a frozen model (.pb file) and it can be easily deployed.\r\n> \r\n> If you do need to freeze the model in an old fashion way take a look on the script below I have been using until TensorFlow 1.6. It is a little different from what is provided by TF official repo\r\n> \r\n> ```python\r\n> import os\r\n> import argparse\r\n> import tensorflow as tf\r\n> \r\n> \r\n> def freeze_graph(model_dir, output_tensors, output_pb):\r\n>     \"\"\" Freeze graph model into **.pb** file\r\n>     \r\n>     Extract the sub graph defined by the output nodes and convert all its variables to constants\r\n> \r\n>     Args:    \r\n>         ``model_dir`` (str): directory that contains all checkpoint files (.ckpt, .meta, .info)\r\n>         \r\n>         ``output_tensors`` (str):  comma separated list of all the output node's names\r\n> \r\n>         ``output_pb`` (str): output **.pb** frozen graph file      \r\n>         \r\n>     \"\"\"\r\n>     if not tf.gfile.Exists(model_dir):\r\n>         raise AssertionError(\r\n>             \"Export directory doesn't exists. Please specify an export \"\r\n>             \"directory: %s\" % model_dir)\r\n> \r\n>     if not output_tensors:\r\n>         tf.loogin.error(\"You need to supply the name of a node to --output_tensors.\")\r\n>         return -1\r\n> \r\n>     # Retrieve checkpoint fullpath\r\n>     checkpoint = tf.train.get_checkpoint_state(model_dir)\r\n>     input_checkpoint = checkpoint.model_checkpoint_path\r\n>     \r\n>     # Clear original devices from graph\r\n>     clear_devices = True\r\n> \r\n>     with tf.Session(graph=tf.Graph()) as sess:\r\n>         # Import graph from .meta file\r\n>         saver = tf.train.import_meta_graph(\r\n>             input_checkpoint + '.meta', clear_devices=clear_devices)\r\n> \r\n>         input_graph_def = tf.get_default_graph().as_graph_def()\r\n> \r\n>         # and restore weights\r\n>         saver.restore(sess, input_checkpoint)\r\n> \r\n>         # Convert variables to constants\r\n>         output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n>             sess,\r\n>             input_graph_def,            \r\n>             output_tensors.split(\",\"),\r\n>             variable_names_blacklist=['global_step']\r\n>         )\r\n> \r\n>         # Dump frozen model\r\n>         with tf.gfile.GFile(output_pb, \"wb\") as f:\r\n>             f.write(output_graph_def.SerializeToString())\r\n>         tf.logging.info(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n> \r\n>     #return output_graph_def\r\n> \r\n> \r\n> if __name__ == '__main__':\r\n>     parser = argparse.ArgumentParser()\r\n>     parser.add_argument(\"--model_dir\", type=str, default=\"\",\r\n>                         help=\"Model folder to export\")\r\n>     parser.add_argument(\"--output_tensors\", type=str, default=\"\",\r\n>                         help=\"The name of the output nodes, comma separated.\")\r\n>     parser.add_argument(\"--output_pb\", type=str,\r\n>                         default=\"frozen_model.pb\", help=\"Output pb file\")\r\n>     args = parser.parse_args()\r\n> \r\n>     freeze_graph(args.model_dir, args.output_tensors, args.output_pb)\r\n> ```\r\n\r\nDoes the .pb file contain gragh and variables'value?", "Hey !\r\nI trained a speech recognition model and I get all the checkpoints file (.meta, .pbtxt, .index) but I need the pb file to test my model.\r\nI noticed that all method specified here use the name of the output_tensors.\r\nHow can we do if we don't know the output_tensors ?\r\nIs there an other python script that helps us to get the pb files without knowing the output_tensors ?", "+1", "any updates?", "After migrating to Estimator API and now to Keras, this is no longer an issue for me. Exporting a saved model through a serving function works pretty well. In my view this is an old issue and it doesn't make sense to keep it open as there are [official way](https://www.tensorflow.org/guide/saved_model#using_savedmodel_with_estimators) to do it.", "Hello, \r\n\r\nActually i applied this file i sent you and got as a result the screenshot attached. My point is to get a .pb file so i can after convert it into mlmodel. \r\nAm i going in the right direction ?\r\n![Screen Shot 2019-08-06 at 2 24 15 PM](https://user-images.githubusercontent.com/5887468/62536139-e5e17600-b855-11e9-97b3-0649ea4dc7bf.png)\r\n\r\n[Uploading 03_implementing_lstm.py.zip\u2026]()\r\n", "@dhingratul,\r\nCan you please confirm if the issue is resolved, as per [Tauranis comment](https://github.com/tensorflow/tensorflow/issues/20532#issuecomment-518623533)? Thanks! ", "I was able to hack around it back then, I am sure it works now with Keras APIs, as i have been using it in another context.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20532\">No</a>\n"]}, {"number": 20531, "title": "Problem with callbacks and LSTM : is failing", "body": "I try to use callbacks and it is failing with following example, if I use GRU with callbacks is ok but with LSTM is failing:\r\n\r\n\r\nThis is system info :\r\n\r\n== cat /etc/issue ===============================================\r\nLinux gpuMachine 4.13.0-45-generic #50~16.04.1-Ubuntu SMP Wed May 30 11:18:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux strategy.ca.alcatel-lucent.com 4.13.0-45-generic #50~16.04.1-Ubuntu SMP Wed May 30 11:18:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                             1.14.5   \r\nprotobuf                          3.6.0    \r\ntensorflow                        1.8.0    \r\ntensorflow-gpu                    1.8.0    \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-9.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Jul  3 16:05:54 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 107...  Off  | 00000000:02:00.0  On |                  N/A |\r\n|  0%   43C    P2    41W / 180W |   1141MiB /  8118MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     28189      C   ...user1/.conda/envs/tensorflow/bin/python  1007MiB |\r\n|    0     28294      G   /usr/lib/xorg/Xorg                           122MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n\r\n\r\n\r\nwith this example I have problem:\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Embedding, LSTM, Dense\r\nfrom keras.callbacks import TensorBoard\r\nimport numpy as np\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.1\r\nsession = tf.Session(config=config)\r\npath_checkpoint = 'test.keras'\r\nSEQUENCES = 5\r\nTIME_STEPS = 10\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(100, 4))\r\nmodel.add(LSTM(32))\r\nmodel.add(Dense(1))\r\nmodel.compile(optimizer='rmsprop', loss='mse')\r\n\r\ntensor_board = TensorBoard(log_dir='log', batch_size=2, write_graph=False,\r\nwrite_grads=True, histogram_freq=4)\r\ncallback_early_stopping = EarlyStopping(monitor='val_loss',\r\npatience=2, verbose=2)\r\ncallback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\r\nmonitor='val_loss',\r\nverbose=2,\r\nsave_weights_only=True,\r\nsave_best_only=True)\r\ncallback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\r\nfactor=0.1,\r\nmin_lr=1e-4,\r\npatience=0,\r\nverbose=2)\r\nx_train = np.random.randint(100, size=(SEQUENCES, TIME_STEPS), dtype='int8')\r\n\r\ny_train = np.random.rand(SEQUENCES)\r\n\r\nmodel.fit(x_train, y_train, batch_size=2, epochs=4, shuffle=True, callbacks=[callback_reduce_lr,tensor_board,callback_early_stopping,callback_checkpoint])\r\n```\r\nand I got this :+1: \r\nError +1\r\nEpoch 1/4\r\n\r\nFailedPreconditionError Traceback (most recent call last)\r\nin ()\r\n36 y_train = np.random.rand(SEQUENCES)\r\n37\r\n---> 38 model.fit(x_train, y_train, batch_size=2, epochs=4, shuffle=True, callbacks=[callback_reduce_lr,tensor_board,callback_early_stopping,callback_checkpoint])\r\n\r\n~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n1040 initial_epoch=initial_epoch,\r\n1041 steps_per_epoch=steps_per_epoch,\r\n-> 1042 validation_steps=validation_steps)\r\n1043\r\n1044 def evaluate(self, x=None, y=None,\r\n\r\n~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py in fit_loop(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n197 ins_batch[i] = ins_batch[i].toarray()\r\n198\r\n--> 199 outs = f(ins_batch)\r\n200 if not isinstance(outs, list):\r\n201 outs = [outs]\r\n\r\n~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in call(self, inputs)\r\n2659 return self._legacy_call(inputs)\r\n2660\r\n-> 2661 return self._call(inputs)\r\n2662 else:\r\n2663 if py_any(is_tensor(x) for x in inputs):\r\n\r\n~/.conda/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in _call(self, inputs)\r\n2629 symbol_vals,\r\n2630 session)\r\n-> 2631 fetched = self._callable_fn(*array_vals)\r\n2632 return fetched[:len(self.outputs)]\r\n2633\r\n\r\n~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py in call(self, *args)\r\n1452 else:\r\n1453 return tf_session.TF_DeprecatedSessionRunCallable(\r\n-> 1454 self._session._session, self._handle, args, status, None)\r\n1455\r\n1456 def del(self):\r\n\r\n~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in exit(self, type_arg, value_arg, traceback_arg)\r\n517 None, None,\r\n518 compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 519 c_api.TF_GetCode(self.status.status))\r\n520 # Delete the underlying status object from memory otherwise it stays alive\r\n521 # as there is a reference to status from this from the traceback due to\r\n\r\nFailedPreconditionError: Attempting to use uninitialized value RMSprop_16/lr\r\n[[Node: RMSprop_16/lr/read = IdentityT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\r\n[[Node: loss_16/mul/_621 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1571_loss_16/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I put output from tools which was in template:\r\nHave I written custom code : none\r\nOS Platform Ubuntu: \"16.04.4 LTS (Xenial Xerus)\"\r\nTensorflow : 1.8.0 GIT VERSION = v1.8.0-0-g93bc2e2072\r\nTensorFlow installed from : anaconda3\r\nBazel version: none\r\nCUDA/cuDNN version : cuda_9.0.176_384.81 and cuDNN : 7.1.4.18+cuda9.0\r\nGPU model and memory: GPU\r\nExact command to reproduce:\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Embedding, LSTM, Dense\r\nfrom keras.callbacks import TensorBoard\r\nimport numpy as np\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.1\r\nsession = tf.Session(config=config)\r\npath_checkpoint = 'test.keras'\r\nSEQUENCES = 5\r\nTIME_STEPS = 10\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(100, 4))\r\nmodel.add(LSTM(32))\r\nmodel.add(Dense(1))\r\nmodel.compile(optimizer='rmsprop', loss='mse')\r\n\r\ntensor_board = TensorBoard(log_dir='log', batch_size=2, write_graph=False,\r\nwrite_grads=True, histogram_freq=4)\r\ncallback_early_stopping = EarlyStopping(monitor='val_loss',\r\npatience=2, verbose=2)\r\ncallback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\r\nmonitor='val_loss',\r\nverbose=2,\r\nsave_weights_only=True,\r\nsave_best_only=True)\r\ncallback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\r\nfactor=0.1,\r\nmin_lr=1e-4,\r\npatience=0,\r\nverbose=2)\r\nx_train = np.random.randint(100, size=(SEQUENCES, TIME_STEPS), dtype='int8')\r\n\r\ny_train = np.random.rand(SEQUENCES)\r\n\r\nmodel.fit(x_train, y_train, batch_size=2, epochs=4, shuffle=True, callbacks=[callback_reduce_lr,tensor_board,callback_early_stopping,callback_checkpoint])\r\n", "Can you create a minimal example that reproduces the problem? I see you have four callbacks, but can you reproduce with only one callback, or even zero callbacks? It's easier to debug issues if the examples are as small and simple as possible.\r\n\r\nAlso, it's easier to read the post if you put source code in [code blocks](https://help.github.com/articles/creating-and-highlighting-code-blocks/). I edited your original post to use a code block.", "there is no problem with no callbacks : but even with one callback is failing:\r\n ```\r\nimport tensorflow as tf\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Embedding, LSTM, Dense\r\nfrom keras.callbacks import TensorBoard\r\nimport numpy as np\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.1\r\nsession = tf.Session(config=config)\r\npath_checkpoint = 'test.keras'\r\nSEQUENCES = 5\r\nTIME_STEPS = 10\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(100, 4))\r\nmodel.add(LSTM(32))\r\nmodel.add(Dense(1))\r\nmodel.compile(optimizer='adam', loss='mse')\r\n\r\n\r\ncallback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\r\n                                       factor=0.1,\r\n                                       min_lr=1e-4,\r\n                                       patience=0,\r\n                                       verbose=2)\r\nx_train = np.random.randint(100, size=(SEQUENCES, TIME_STEPS), dtype='int8')\r\n\r\ny_train = np.random.rand(SEQUENCES)\r\n\r\nmodel.fit(x_train, y_train, batch_size=2, epochs=4, shuffle=True, callbacks=[callback_reduce_lr])\r\n\r\n ```\r\n\r\n ", "This works if you either use just `tensorflow.python.keras` or `keras` in your imports, but in your current code, you mix them. Try changing \r\n```\r\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\r\n``` \r\nto\r\n\r\n```\r\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\r\n```\r\n/CC @fchollet, is mixing `tf.keras` and `keras` supposed to work? If not, perhaps we should detect that case and throw an error.", "thanks a lot . ", "Closing this since the original issue has been solved."]}, {"number": 20530, "title": "Add tflite performance page", "body": "", "comments": ["b/111062899"]}, {"number": 20529, "title": "[tf.keras] Stateful Metrics assorted errors.", "body": "I will break this issue down into several code snippets each displaying a different error. @fchollet. In total 3 issues. All of these issues are only relevant to ``tf.keras`` implementation. The ``keras`` implementation works as intended.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary.\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Problem 1\r\nIssues with multi-input/multi-output and batch averaging. This happens for both train and validation metrics.\r\n\r\n\r\n### Source code/logs\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.keras.datasets import mnist\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\r\n\r\n\r\nclass BatchCounter(tf.keras.layers.Layer):\r\n\r\n        def __init__(self, name='batch_counter', **kwargs):\r\n            super(BatchCounter, self).__init__(name=name, **kwargs)\r\n            self.stateful = True\r\n            self.batches = tf.keras.backend.variable(value=0, dtype='int32')\r\n\r\n        def reset_states(self):\r\n            tf.keras.backend.set_value(self.batches, 0)\r\n\r\n        def __call__(self, y_true, y_pred):\r\n            updates = [tf.keras.backend.update_add(self.batches, tf.keras.backend.variable(value=1, dtype='int32'))]\r\n            self.add_update(updates)\r\n            return self.batches\r\n\r\n\r\nbatch_size = 100\r\nnum_classes = 10\r\nepochs = 1\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# Data\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1).astype('float32') / 255\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1).astype('float32') / 255\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\n# Convolutional Encoder\r\ninput_img = Input(shape=(img_rows, img_cols, 1))\r\nconv_1 = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\r\npool_1 = MaxPooling2D((2, 2), padding='same')(conv_1)\r\nconv_2 = Conv2D(8, (3, 3), activation='relu', padding='same')(pool_1)\r\npool_2 = MaxPooling2D((2, 2), padding='same')(conv_2)\r\nconv_3 = Conv2D(8, (3, 3), activation='relu', padding='same')(pool_2)\r\nencoded= MaxPooling2D((2, 2), padding='same')(conv_3)\r\n\r\n# Classification\r\nflatten = Flatten()(encoded)\r\nfc = Dense(128, activation='relu')(flatten)\r\nsoftmax = Dense(num_classes, activation='softmax', name='classification')(fc)\r\n\r\n# Decoder\r\nconv_4 = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\r\nup_1 = UpSampling2D((2, 2))(conv_4)\r\nconv_5 = Conv2D(8, (3, 3), activation='relu', padding='same')(up_1)\r\nup_2 = UpSampling2D((2, 2))(conv_5)\r\nconv_6 = Conv2D(16, (3, 3), activation='relu')(up_2)\r\nup_3 = UpSampling2D((2, 2))(conv_6)\r\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='autoencoder')(up_3)\r\n\r\nmodel = Model(inputs=input_img, outputs=[softmax, decoded])\r\n\r\nmodel.compile(loss={'classification': 'categorical_crossentropy',\r\n                    'autoencoder': 'binary_crossentropy'},\r\n              loss_weights={'classification': 1.0,\r\n                            'autoencoder': 0.5},\r\n              optimizer='adam',\r\n              metrics={'classification': 'accuracy', 'autoencoder': BatchCounter()})\r\n\r\nhistory = model.fit(x_train,\r\n          {'classification': y_train, 'autoencoder': x_train},\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          validation_data= (x_test, {'classification': y_test, 'autoencoder': x_test}),\r\n          verbose=1)\r\n```\r\n\r\n```\r\nEpoch 1/1\r\n60000/60000 [==============================] - 41s 677us/step - loss: 0.5086 - classification_loss: 0.4051 - autoencoder_loss: 0.2069 - classification_acc: 0.8755 - autoencoder_batch_counter: 299.7983 - val_loss: 0.2001 - val_classification_loss: 0.1242 - val_autoencoder_loss: 0.1518 - val_classification_acc: 0.9596 - val_autoencoder_batch_counter: 50.1000\r\n```\r\n\r\n``autoencoder_batch_counter`` & ``val_autoencoder_batch_counter`` should always be (600, 100) respectively.  These metrics are batch averaged. This does not happen in the Keras implementation.", "comments": ["### Problem 2\r\n\r\nBatchwise averaging with ``fit_generator``\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Model\r\n\r\nimport numpy as np\r\n\r\nclass BatchCounter(tf.keras.layers.Layer):\r\n\r\n        def __init__(self, name=\"batch_counter\", **kwargs):\r\n            super(BatchCounter, self).__init__(name=name, **kwargs)\r\n            self.stateful = True\r\n            self.batches = tf.keras.backend.variable(value=0, dtype=\"int32\")\r\n\r\n        def reset_states(self):\r\n            tf.keras.backend.set_value(self.batches, 0)\r\n\r\n        def __call__(self, y_true, y_pred):\r\n            updates = [\r\n                tf.keras.backend.update_add(\r\n                    self.batches, \r\n                    tf.keras.backend.variable(value=1, dtype=\"int32\"))]\r\n            self.add_update(updates)\r\n            return self.batches\r\n\r\nclass DummyGenerator(object):\r\n    \"\"\" Dummy data generator. \"\"\"\r\n\r\n    def run(self):\r\n        while True:\r\n            yield np.ones((10, 1)), np.zeros((10, 1))\r\n\r\ntrain_gen = DummyGenerator()\r\nval_gen = DummyGenerator()\r\n\r\n# Dummy model\r\ninputs = Input(shape=(1,))\r\noutputs = Dense(1)(inputs)\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[BatchCounter()])\r\n\r\nmodel.fit_generator(\r\n    train_gen.run(), \r\n    steps_per_epoch=5, \r\n    epochs=10, \r\n    validation_data=val_gen.run(), \r\n    validation_steps=5)\r\n```\r\n\r\n```\r\n5/5 [==============================] - 0s 58ms/step - loss: 2.5346 - batch_counter: 2.2000 - val_loss: 2.5155 - val_batch_counter: 4.0000\r\nEpoch 2/10\r\n5/5 [==============================] - 0s 3ms/step - loss: 2.5029 - batch_counter: 2.2000 - val_loss: 2.4839 - val_batch_counter: 5.0000\r\nEpoch 3/10\r\n5/5 [==============================] - 0s 3ms/step - loss: 2.4713 - batch_counter: 2.6000 - val_loss: 2.4525 - val_batch_counter: 4.0000\r\nEpoch 4/10\r\n5/5 [==============================] - 0s 4ms/step - loss: 2.4400 - batch_counter: 2.4000 - val_loss: 2.4214 - val_batch_counter: 5.0000\r\nEpoch 5/10\r\n5/5 [==============================] - 0s 4ms/step - loss: 2.4090 - batch_counter: 2.2000 - val_loss: 2.3905 - val_batch_counter: 4.0000\r\nEpoch 6/10\r\n5/5 [==============================] - 0s 3ms/step - loss: 2.3782 - batch_counter: 2.4000 - val_loss: 2.3598 - val_batch_counter: 4.0000\r\nEpoch 7/10\r\n5/5 [==============================] - 0s 3ms/step - loss: 2.3476 - batch_counter: 2.8000 - val_loss: 2.3293 - val_batch_counter: 4.0000\r\nEpoch 8/10\r\n5/5 [==============================] - 0s 4ms/step - loss: 2.3173 - batch_counter: 2.4000 - val_loss: 2.2993 - val_batch_counter: 4.0000\r\nEpoch 9/10\r\n5/5 [==============================] - 0s 4ms/step - loss: 2.2873 - batch_counter: 2.8000 - val_loss: 2.2695 - val_batch_counter: 4.0000\r\nEpoch 10/10\r\n5/5 [==============================] - 0s 4ms/step - loss: 2.2577 - batch_counter: 2.0000 - val_loss: 2.2400 - val_batch_counter: 4.0000\r\n```\r\n\r\nThe training batch counter is a non-integer -> batch averaging.", "### Problem 3\r\n\r\nIt appears batches are sometimes missing from stateful metrics. An example is attached below, we define a simple stateful metric: a batch counter that increments each batch. At the end of the epoch the displayed value is either **n** (the total number of batches) or **n-1**.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.models import Model\r\n\r\nimport numpy as np\r\n\r\nclass BatchCounter(tf.keras.layers.Layer):\r\n\r\n        def __init__(self, name=\"batch_counter\", **kwargs):\r\n            super(BatchCounter, self).__init__(name=name, **kwargs)\r\n            self.stateful = True\r\n            self.batches = tf.keras.backend.variable(value=0, dtype=\"int32\")\r\n\r\n        def reset_states(self):\r\n            tf.keras.backend.set_value(self.batches, 0)\r\n\r\n        def __call__(self, y_true, y_pred):\r\n            updates = [\r\n                tf.keras.backend.update_add(\r\n                    self.batches, \r\n                    tf.keras.backend.variable(value=1, dtype=\"int32\"))]\r\n            self.add_update(updates)\r\n            return self.batches\r\n\r\n# Dummy dataset\r\nX = np.ones((50, 1))\r\ny = np.zeros((50, 1))\r\n\r\n# Dummy model\r\ninputs = Input(shape=(1,))\r\noutputs = Dense(1)(inputs)\r\nmodel = Model(inputs=inputs, outputs=outputs)\r\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[BatchCounter()])\r\n\r\nmodel.fit(X, y, batch_size=10, epochs=10, validation_data = (X, y))\r\n```\r\n\r\n```\r\nEpoch 1/10\r\n50/50 [==============================] - 0s 2ms/step - loss: 3.8098e-04 - batch_counter: 5.0000 - val_loss: 1.8327e-04 - val_batch_counter: 4.0000\r\nEpoch 2/10\r\n50/50 [==============================] - 0s 93us/step - loss: 1.0319e-04 - batch_counter: 5.0000 - val_loss: 2.0676e-05 - val_batch_counter: 4.0000\r\nEpoch 3/10\r\n50/50 [==============================] - 0s 80us/step - loss: 6.5885e-06 - batch_counter: 5.0000 - val_loss: 4.7464e-06 - val_batch_counter: 4.0000\r\nEpoch 4/10\r\n50/50 [==============================] - 0s 76us/step - loss: 1.5232e-05 - batch_counter: 5.0000 - val_loss: 2.8963e-05 - val_batch_counter: 5.0000\r\nEpoch 5/10\r\n50/50 [==============================] - 0s 76us/step - loss: 3.0441e-05 - batch_counter: 5.0000 - val_loss: 2.6422e-05 - val_batch_counter: 5.0000\r\nEpoch 6/10\r\n50/50 [==============================] - 0s 76us/step - loss: 1.9016e-05 - batch_counter: 5.0000 - val_loss: 8.0616e-06 - val_batch_counter: 5.0000\r\nEpoch 7/10\r\n50/50 [==============================] - 0s 86us/step - loss: 3.6174e-06 - batch_counter: 4.0000 - val_loss: 2.7267e-08 - val_batch_counter: 4.0000\r\nEpoch 8/10\r\n50/50 [==============================] - 0s 91us/step - loss: 7.0071e-07 - batch_counter: 5.0000 - val_loss: 2.4935e-06 - val_batch_counter: 4.0000\r\nEpoch 9/10\r\n50/50 [==============================] - 0s 77us/step - loss: 3.3024e-06 - batch_counter: 5.0000 - val_loss: 3.5519e-06 - val_batch_counter: 4.0000\r\nEpoch 10/10\r\n50/50 [==============================] - 0s 76us/step - loss: 2.7035e-06 - batch_counter: 4.0000 - val_loss: 1.2286e-06 - val_batch_counter: 4.0000\r\n```\r\n\r\n``batch_counter`` & ``val_batch_counter`` should always be 5 in the above logs, (50 samples, batch size = 10). They are occasionally 4 or 5. This does not happen in the Keras implementation.\r\n", "Related to this issue (was not fixed). https://github.com/tensorflow/tensorflow/issues/19186", "@fchollet, could you please take a look?", "@pavithrasv could you also take a look at https://github.com/keras-team/keras/pull/10673\r\n\r\nThe issue is also apparent in tf.keras.", "Thank you @briannemsick, as @fchollet started reviewing the change I think we should wait for approval from him?", "@pavithrasv Do you want some help?\r\n\r\nI'm very keen on using Stateful Metrics for production.", "Adding @raymond-yuan who is looking into this issue.", "The batch averaging issue has been resolved, looking into the occasional metric not updating issue ", "Thanks for the response @raymond-yuan. Assuming it is still an internal google branch.\r\n\r\nShould I make a new issue for: https://github.com/tensorflow/tensorflow/pull/21071\r\n\r\nIt is the ``TensorBoard`` issue with Stateful Metrics. It was merged into Keras https://github.com/keras-team/keras/pull/10673 and needs the equivalent ``tf.keras`` fix.", "@raymond-yuan I think the progress bar is still busted for ``fit_generator``:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/20650/files\r\n\r\nLooks like ``keras`` and ``tf.keras`` are out sync.\r\n\r\nThanks again for fixing that issue :)", "Hey @brge17 sorry for the late response\r\n\r\n> Should I make a new issue for: #21071\r\n\r\nYes please make a new issue for #21071\r\n\r\nIf there's nothing directly related to this issue, I'll go ahead and close it? ", "Is the metric not updating issue fixed?", "@pavithrasv will be looking into that issue!", "@pavithrasv any update on the missing/overcounting batches issue?", "Closing as stale.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20529\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20529\">No</a>\n"]}, {"number": 20528, "title": "[Feature Request] random_poisson GPU Kernel", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.8\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX 1050 4GB\r\n- **Exact command to reproduce**: random_poisson\r\n\r\n### Describe the problem\r\n[`random_poisson` currently only has a CPU kernel and not a GPU kernel.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_poisson_op.cc#L322) [See also this Stack Overflow post for a specific manifestation of the issue.](https://stackoverflow.com/questions/50858572/tensorflows-random-poisson-only-runs-on-cpu/51159962)\r\n", "comments": ["@eanschuetz,\r\n[tf.random.poisson](https://www.tensorflow.org/api_docs/python/tf/random/poisson) seems to utilize GPU Kernel in the **`Latest Version of Tensorflow (2.4.1)`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/e24b4cdee5d809bbdf303d5e12e28c44/gh_20528.ipynb) of the working code. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20527, "title": "tf.layers.conv3d with \"channels_first\" does not accept dynamic shapes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-0-g93bc2e2072\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\ncuDNN 7.0 CUDA 9.1\r\n- **GPU model and memory**:\r\nGeforce 1080 Ti 12Gb\r\n- **Exact command to reproduce**:\r\nSee given code snippets\r\n\r\n\r\n### Describe the problem\r\nCalling tf.layers.conv3d with \"channels_first\" on a tf.placeholder with dynamic shapes produces an error, whereas it does not for \"channels_last\" (see source code below). Cause is usage of reshape operation [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L203-L207) as outlined in a similar [issue](https://github.com/tensorflow/tensorflow/issues/15655). Unfortunately, no general solution supporting dynamic shapes was found in aforementioned issue, and shape inference does not allow for >1 dynamic shape.\r\n\r\n### Source code / logs\r\nthis produces error:\r\n```\r\nimport tensorflow as tf\r\nx = tf.placeholder(dtype=tf.float32, shape=[None, 1, None, None, None])\r\ny = tf.layers.conv3d(x, 32, 9, data_format='channels_first')\r\n```\r\n\r\nthis works fine:\r\n```\r\nimport tensorflow as tf\r\nx = tf.placeholder(dtype=tf.float32, shape=[None, None, None, None, 1])\r\ny = tf.layers.conv3d(x, 32, 9, data_format='channels_last')\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "@tensorflowbutler done", "If `bias_add` supports 5-d tensor for NCHW, we could remove the reshape operator which seems error-prone with dynamic shape.", "According to [documentation,](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/nn_ops.py#L1485-L1488) nn.bias_add supports N-d tensors due to broadcasting.\r\n\r\nHowever, the underlying [C implementation/documentation](https://github.com/tensorflow/tensorflow/blob/86f5ab7474825da756838b34e1b4eac93f5fc68a/tensorflow/core/ops/nn_ops.cc#L430-L455)  states explicitly that for NCHW, bias would be added to third-to-the-last dimension, which forces the reshape operations.\r\n\r\nI would therefore suggest a change here, such that bias is added to first dimension instead of third-to-the-last dimension, as channels will always be first dim in NCHW format. This would make reshaping operations obsolete.\r\n\r\nUnfortunately, I am unfamiliar with the tensorflow C backend, and therefore do not know where the change would need to be made.", "Source code is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/bias_op.cc). I think you can improve it if you'd like :-)", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Added a PR #22127 for 3D/4D/5D support of bias_add with NCHW data format.", "Create a PR #23004 to fix the issue.", "Close the issue as PRs have been merged."]}, {"number": 20526, "title": "Move get_started to tutorials and rearrange", "body": "", "comments": ["There are surprisingly not many links in the code pointing to the get_started/ material (maybe because this directory was added later).\r\nThe ones that I found use the full URL, which our redirects will catch. We should still update those files, but I'll wait until after this all settles.\r\n\r\nI've been building the docs regularly and staging. The doc generator did catch a few instances of broken ${} links, but they were all in docs_src/"]}, {"number": 20525, "title": "Using shallow clones from git repos in CMake build", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "The Windows CMake build fails with: \r\n\r\n```\r\nCustomBuild:\r\n         fatal: reference is not a tree: d184fa229d75d336aedea0041bd59cb93e7e267f\r\n         CMake Error at T:/src/github/tensorflow/cmake_build/grpc/tmp/grpc-gitclone.cmake:65 (message):\r\n           Failed to checkout tag: 'd184fa229d75d336aedea0041bd59cb93e7e267f'\r\n```\r\n\r\nIs this related?", "CMake is going away in TF 1.11. See https://github.com/tensorflow/tensorflow/releases/tag/v1.10.0. Since this PR has been stale and has not been update for a while. Closing this for now, and please reopen if you feel otherwise.\r\n\r\nThanks.", ">CMake is going away in TF 1.11\r\n\r\nIt'd be better if bazel (the one which is a build tool, not a city) gone away."]}, {"number": 20524, "title": "Explanation for why tf.gradients() no longer propagates through integer tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.13.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0-dev20180628 (nightly)\r\n- **Python version**: 2.7.15\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nFor performance reasons, we sometimes work with integer tensors like `tf.uint8`.  We have custom ops that operate on these tensors, and in our unit tests we verify the correctness of the gradients using `tf.gradients()`.  However, it seems as of version 1.9, the behavior of `tf.gradients()` has been changed so that it no longer backpropagates through integer tensors.\r\n\r\nThis was causing `tf.gradients()` to always return `None`.\r\n\r\nThis seems like a significant change that introduces inconsistent behavior (or at least special cases that need to be accounted for).  I assume there's a good rationale, but could additional documentation be added to explain why this change was necessary (or deemed desirable)?  The only documntation I'm currently aware of is this line from the release notes: \r\n\r\n> Prevent `tf.gradients()` from backpropagating through integer tensors.\r\n\r\nAnd the following comment in `gradients_impl.py`:\r\n\r\n> All integer tensors are considered constant with respect to all `xs`, as if they were included in `stop_gradients`.\r\n\r\nThanks.\r\n\r\n### Source code / logs\r\n\r\n        with self.test_session(config=self.config) as session:\r\n            t = tf.ones(5, dtype=tf.float32) * 2\r\n            t2 = t**2\r\n\r\n            grad_ys = tf.ones(5)\r\n            grad = tf.gradients(t2, t, grad_ys)[0]\r\n            grad_out = session.run(grad)\r\n            print(\"grad_out: \", grad_out)\r\n\r\nPrints `[4. 4. 4. 4. 4.]` as expected\r\n\r\n        with self.test_session(config=self.config) as session:\r\n            t = tf.ones(5, dtype=tf.int32) * 2\r\n            t2 = t**2\r\n\r\n            grad_ys = tf.ones(5)\r\n            grad = tf.gradients(t2, t, grad_ys)[0]\r\n            grad_out = session.run(grad)\r\n            print(\"grad_out: \", grad_out)\r\n\r\nRaises `TypeError: Fetch argument None has invalid type <type 'NoneType'>` ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This change stems from https://github.com/tensorflow/tensorflow/issues/16504\r\n\r\nIn effect, allowing gradients on integer tensors was causing incorrectness in tf.while_loop, and there was no satisfactory way to resolve them without this change.", "Thanks for the explanation!", "To add a little more to this explanation, it would have been possible (but difficult) to fix the tf.while_loop bug with a more localized change that would have affected only tf.while_loop. But we considered the change to disallow integer gradients to be more principled. So in addition to fixing the tf.while_loop bug, this makes TensorFlow's treatment of gradients more consistent.\r\n\r\nFirst of all, before this change, integer gradients were inconsistently implemented. For example, before this change:\r\n```\r\nk = tf.constant(3)\r\ngrad_1, = tf.gradients(k * k, k)\r\ngrad_2, = tf.gradients(tf.square(k), k)\r\n\r\ngrad_1 is a tf.int32 Tensor that evaluates to 6.\r\nThe tf.gradients call for grad_2 raises an exception: [TypeError: Expected int32 passed to parameter 'y' of op 'Mul', got 2.0 of type 'float' instead.]\r\n```\r\n\r\nBut secondly, and more importantly, it is not clear in general how integer gradients should work. In the example above, what should `grad_1` be? If f(k)=k*k is interpreted as a real-valued function then the gradient at k=3 is 6. But f(k) is an integer-valued function, so perhaps it is more correct for the gradient to be f(4)-f(3) = 7. Or should it be f(3)-f(2) = 5? Or should it be the average of the last two options? Ultimately, it is not clear if there is a sensible way to support gradients of integer-valued functions. This is why we made this change.\r\n\r\nProbably we should have explained this further in the release notes."]}, {"number": 20523, "title": "ImportError: cannot import name gen_collective_ops", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:centos-release-7-3.1611.el7.centos.x86_64\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**:1.7\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**:0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**:No (CPU only)\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**: \r\n\r\n\r\n### Describe the problem\r\nI am working on distributed tensorflow using horovod in CPU cluster mode.\r\nBelow is the run command used:\r\nmpirun -x LD_LIBRARY_PATH -x OMP_NUM_THREADS -cpus-per-proc 20 --map-by socket --overscribe --report-bindings  -n 2 python  $python_script      --mkl=True --forward_only=False --num_batches=200 --kmp_blocktime=0 --num_warmup_batches=50 --num_inter_threads=$inter_op --distortions=False --optimizer=sgd --batch_size=$batch_size --num_intra_threads=$intra_op --data_format=NCHW --model=$MODEL --variable_update horovod --horovod_device cpu --data_dir=XXX\r\n\r\n\r\nError Stack:\r\n Traceback (most recent call last):\r\n  File \"/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\", line 27, in <module>\r\n    import benchmark_cnn\r\n  File \"/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py\", line 53, in <module>\r\n    import variable_mgr\r\n  File \"/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/variable_mgr.py\", line 25, in <module>\r\n    import allreduce\r\n  File \"/tensorflow/benchmarks/scripts/tf_cnn_benchmarks/allreduce.py\", line 28, in <module>\r\n    from tensorflow.python.ops import collective_ops\r\n  File \"/.local/lib/python2.7/site-packages/tensorflow/python/ops/collective_ops.py\", line 21, in <module>\r\n    from tensorflow.python.ops import gen_collective_ops\r\nImportError: cannot import name gen_collective_ops\r\n\r\n\r\n", "comments": ["Hi @Krishnaprasad-T -- tf_cnn_benchmarks expects the nightly release of TensorFlow when run from master. Can you try with the nightly version of TensorFlow (`pip install tf-nightly` or `pip install tf-nightly-gpu`)?", "Same problem here. Here is the traceback when I import tensorflow.contrib with code in master:\r\n\r\n```\r\nimport tensorflow.contrib\r\nTraceback (most recent call last):\r\n  Python Shell, prompt 11, line 1\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 39, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.distribute.python.collective_all_reduce_strategy import CollectiveAllReduceStrategy\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\collective_all_reduce_strategy.py\", line 24, in <module>\r\n    from tensorflow.contrib.distribute.python import cross_tower_ops as cross_tower_ops_lib\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_ops.py\", line 24, in <module>\r\n    from tensorflow.contrib.distribute.python import cross_tower_utils\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_utils.py\", line 31, in <module>\r\n    from tensorflow.python.ops import collective_ops\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\python\\ops\\collective_ops.py\", line 21, in <module>\r\n    from tensorflow.python.ops import gen_collective_ops\r\nbuiltins.ImportError: cannot import name 'gen_collective_ops'\r\n```\r\nDoes the module gen_collective_ops never exist?", "@guileryu01 , if you are seeing the same issue, please provide the details requested in the issue template for yourself as well. As I note above, a possible cause is an older version of TF than the benchmark expects.", "@karmel I wasn't running the benchmark code. All I did was import tensorflow.contrib, although it hit the same error as above. Here is system information:\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: master (as of 8/4/2018)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: Visual Studio 2017\r\n- **CUDA/cuDNN version**: 9.2/7.1\r\n- **GPU model and memory**: GTX 1080\r\n- **Exact command to reproduce**: import tensorflow.contrib\r\n\r\n\r\n### Describe the problem\r\nImportError when importing tensorflow.contrib\r\n### Source code / logs\r\n```\r\nimport tensorflow.contrib\r\nTraceback (most recent call last):\r\n  Python Shell, prompt 11, line 1\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 39, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.distribute.python.collective_all_reduce_strategy import CollectiveAllReduceStrategy\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\collective_all_reduce_strategy.py\", line 24, in <module>\r\n    from tensorflow.contrib.distribute.python import cross_tower_ops as cross_tower_ops_lib\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_ops.py\", line 24, in <module>\r\n    from tensorflow.contrib.distribute.python import cross_tower_utils\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_utils.py\", line 31, in <module>\r\n    from tensorflow.python.ops import collective_ops\r\n  File \"C:\\Anaconda3\\envs\\python36\\Lib\\site-packages\\tensorflow\\python\\ops\\collective_ops.py\", line 21, in <module>\r\n    from tensorflow.python.ops import gen_collective_ops\r\nbuiltins.ImportError: cannot import name 'gen_collective_ops'\r\n```", "@guptapriya -- this seems to be related to dependencies introduced for distribution; is this something you've seen? My guess is there's some version mismatch or missing flag (cc @gunan )-- any ideas?", "hey @karmel - thanks for bringing to attention. collective ops was added to tf_cnn_benchmarks by @poxvoculi and @yuefengz added them to the distribution strategy - will let them address why this is failing imports", "Looks like some kind of build dependency issue.  gen_collective_ops is supposed to be generated when the files in python/ops are correctly compiled.  That's the limit of my understanding of that part of the system.", "@martinwicke @allenlavoie \r\nNot sure where the issue is, for me things seem to be working.\r\nIs it possible this is happening because you did not run \"import tensorflow\" first?\r\nOr are you running this as:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib as tfc\r\n```\r\n", "@Krishnaprasad-T -- is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to staleness. Please use the latest version for TensorFlow and build again. Feel free to open a new issue if it still persists. Thanks!"]}, {"number": 20522, "title": "Creating variables folder for hosting model on server", "body": "I'm using [this](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py) image_retraining script for image classification and I got a .pb file. For hosting this model on a server, I need to have a `variables` folder as mentioned [here](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/serving_basic.md). Can I generate `variables` folder from .pb file or I need to modify [retrain script](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/image_retraining/retrain.py) and do training again? There's some discussion [here](https://github.com/tensorflow/serving/issues/467).", "comments": ["Have I written custom code  **`No`**\r\nOS Platform and Distribution  **`MacOS High Sierra 10.13.5`**\r\nTensorFlow installed from   **`binary`**\r\nTensorFlow version  **`v1.8.0-0-g93bc2e2072 1.8.0`**\r\nBazel version  **`0.13.1`**\r\nCUDA/cuDNN version  **`N/A`**\r\nGPU model and memory  **`Intel Iris Pro 1536 MB`**\r\nExact command to reproduce  **`N/A`**", "Nagging Assignee @tatatodd: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@duplex143 Is this still an issue?", "@ymodak It's not exactly an issue. I needed some help with this thing. I still didn't figure this out and I've stopped working on this long time back."]}, {"number": 20521, "title": "prefetch_to_device not working with SparseTensor when fetching to GPU", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.9\r\n- **Python version**: 2.7\r\n- **Bazel version**: 0.12.0\r\n- **GCC/Compiler version**: 4.8.4\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: GTX 1080, 8114MiB\r\n- **Exact command to reproduce**:\r\n(Any sparse tensor being prefetched to GPU will work here, this is just one short example)\r\n```\r\nimport tensorflow as tf\r\n\r\n# Set up simple sparse tensor\r\nds = tf.data.Dataset.from_tensors(tf.contrib.layers.dense_to_sparse([1], 0))\r\n\r\n# prefetch to GPU\r\ngpu_ds = ds.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))\r\n\r\n# Get a value\r\ngpu_iter = gpu_ds.make_one_shot_iterator()\r\nval = gpu_iter.get_next()\r\n\r\n# Barf\r\nwith tf.Session() as sess:\r\n    sess.run(val)\r\n```\r\n### Describe the problem\r\nBug resulting from prefetch_to_device with the following conditions: \r\n* device is GPU\r\n* dataset element contains a sparse tensor\r\n\r\nIterator functions appropriately, only when you try to _use_ the data do you run into the error. (see log for error)\r\n\r\nAlso, as a side note which may or may not be relevant, I've found that even a single sparse tensor in an element will spoil any other nice, kind, and dense tensors in the pack. So, if, in the previous example, I were to do the following, I would still run into an error:\r\n```\r\nimport tensorflow as tf\r\n# Make a sparse tensor\r\nds = tf.data.Dataset.from_tensors(tf.contrib.layers.dense_to_sparse([1], 0))\r\n\r\n# Add in a non-sparse component\r\nds = ds.map(lambda x: (x,1))\r\n\r\n# Prefetch to device\r\ngpu_ds = ds.apply(tf.contrib.data.prefetch_to_device('/gpu:0'))\r\n\r\n# Pull out vals\r\ngpu_iter = gpu_ds.make_one_shot_iterator()\r\nval_sparse, val_normal = gpu_iter.get_next()\r\n\r\n# Uh oh!\r\nwith tf.Session() as sess:\r\n    sess.run(val_normal)\r\n```\r\n\r\n### Log\r\nTraceback (most recent call last):\r\n  File \"sparse_bunk.py\", line 17, in <module>\r\n    sess.run(val_normal)\r\n  File \"/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: No unary variant device copy function found for direction: 1 and Variant type_name: tensorflow::Tensor\r\n     [[Node: FunctionBufferingResourceGetNext = FunctionBufferingResourceGetNext[output_types=[DT_VARIANT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n\r\nThank you", "comments": ["Have I written custom code: yes\r\nOS Platform and Distribution: Linux Ubuntu 14.04\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.9\r\nPython version: 2.7\r\nBazel version: 0.12.0\r\nGCC/Compiler version: 4.8.4\r\nCUDA/cuDNN version: 9.1\r\nGPU model and memory: Tesla K40, K80\r\n\r\nI have been getting a similar error when I use MirroredStrategy() for training across multiple devices. Here is the stack trace for it:\r\n\r\n**Stacktrace**\r\n`  File \"train.py\", line 113, in <module>\r\n    tf.app.run()\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 110, in main\r\n    classifier.train(input_fn=lambda: _get_input_stream())\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1141, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1366, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1449, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 583, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1059, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1150, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1135, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1207, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 987, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: No unary variant device copy function found for direction: 1 and Variant type_name: tensorflow::Tensor\r\n         [[Node: FunctionBufferingResourceGetNext = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_STRING, DT_INT32, DT_VARIANT], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n         [[Node: GroupCrossDeviceControlEdges_0/train/OptimizeLoss/global_norm/gradient_norm/tags/_766 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1235_..._norm/tags\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op u'FunctionBufferingResourceGetNext', defined at:\r\n  File \"train.py\", line 113, in <module>\r\n    tf.app.run()\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 110, in main\r\n    classifier.train(input_fn=lambda: _get_input_stream())\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 376, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1141, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1241, in _train_model_distributed\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1011, in _get_features_and_labels_from_input_fn\r\n    return estimator_util.parse_input_fn_result(result)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/estimator/util.py\", line 111, in parse_input_fn_result\r\n    result = iterator.get_next()\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/values.py\", line 619, in get_next\r\n    data_list = self._iterator.get_next(name=name)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py\", line 124, in get_next\r\n    self.output_types, self.output_classes)), name=name)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/gen_dataset_ops.py\", line 351, in function_buffering_resource_get_next\r\n    output_types=output_types, name=name)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3212, in create_op\r\n    op_def=op_def)\r\n  File \"/home/weinman/virtualenv/tf-b2fe2a874/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1702, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): No unary variant device copy function found for direction: 1 and Variant type_name: tensorflow::Tensor\r\n         [[Node: FunctionBufferingResourceGetNext = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_STRING, DT_INT32, DT_VARIANT], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n         [[Node: GroupCrossDeviceControlEdges_0/train/OptimizeLoss/global_norm/gradient_norm/tags/_766 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1235_..._norm/tags\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]`\r\n**End of Stacktrace**\r\n\r\nMy guess is that the underlying issue should be the same. I do not make any explicit calls to prefetch_to_device(*Insert GPU here*) with sparse tensors anywhere in my code. Hence, the issue probably lies with references to prefetching ops under the hood. \r\n", "I think I'm experiencing the same error here.\r\n\r\nAfter staring at it for a while, I realized that the `direction` (HOST_TO_DEVICE = 1), the `recv_device` (`\"/job:localhost/replica:0/task:0/device:CPU:0\"`), and the `send_device` (`\"/job:localhost/replica:0/task:0/device:GPU:0\"`) don't seem to be in agreement.\r\n\r\nPerhaps I'm misunderstanding what's going on here, but maybe it's a simple issue related to getting the direction of the copy correct?", "@rohan100jain Any thoughts on the above?", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So @mrry added some support recently that makes this go away. I ran the same piece of code with tf-nightly-gpu and seems to work.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/02ae1e2e781b8e049d1fc1ab7b52f6ee7edb4423#diff-3d32c23cf21d0adac13fdd28576dd6f9\r\n\r\nPlease test it and let me know if there is still a problem.", "Everything I've tested appears to work. Thank you!"]}, {"number": 20520, "title": "Update calling of expand_dims with axis", "body": "This fix updates calling of `expand_dims` with `dim -> axis`\r\nas the `dim=` in `tf.expand_dims` has been deprecated and\r\nwas generating unnecessary warnings.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20518, "title": "how to use tensorboard debugger with tf.slim?", "body": "I am using tf.slim to train a network, I hope to debug the net and check out the inner variables with tensorboard debugger. how can I do that? the session  wraped by slim API.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I find it's simple", "@changkaizhao \r\nHi, have you figured out how to use tensorboard with tf.slim? My phone number is 18117129206. Can I consult with you?"]}, {"number": 20517, "title": "Python 3.7 compatibility", "body": "I'm sure developers are working hard to catch up with Python 3.7.\r\nIs there any timeline?\r\n\r\npip3 install tensorflow - apparently does not work, building from source:\r\n\r\nOS Platform and Distribution: Mac OS X 10.13.5\r\nPython: Python 3.7.0 (Homebrew)\r\nTensorFlow installed from: source (https://github.com/tensorflow/tensorflow.git)\r\nTensorFlow version: TensorFlow 1.9.0-rc2\r\nBazel version:\r\n```\r\nBuild label: 0.15.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 26 12:42:27 2018 (1530016947)\r\nBuild timestamp: 1530016947\r\nBuild timestamp as int: 1530016947\r\n```\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce:\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n```\r\nStarting local Bazel server and connecting to it...\r\n...........................\r\nWARNING: /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/kfac/python/ops/BUILD:80:1: in py_library rule //tensorflow/contrib/kfac/python/ops:loss_functions: target '//tensorflow/contrib/kfac/python/ops:loss_functions' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /Users/zardoz/Projects/tensorflow/tensorflow/contrib/BUILD:14:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Linking external/grpc/libgrpc_base_c.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(endpoint_pair_uv.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(endpoint_pair_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(ev_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(fork_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(gethostname_fallback.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(gethostname_host_name_max.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(iocp_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(iomgr_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_set_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_uv.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(pollset_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(resolve_address_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_utils_linux.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_utils_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(socket_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_client_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_server_utils_posix_noifaddrs.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_server_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_uv.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(tcp_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(timer_uv.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(unix_sockets_posix_noop.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc_base_c.a(wakeup_fd_eventfd.o) has no symbols\r\nINFO: From Linking external/grpc/libalts_util.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libalts_util.a(check_gcp_environment_linux.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libalts_util.a(check_gcp_environment_windows.o) has no symbols\r\nINFO: From Linking external/grpc/libtsi.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libtsi.a(ssl_session_openssl.o) has no symbols\r\nINFO: From Linking external/grpc/libgrpc++_base.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgrpc++_base.a(rpc_method.o) has no symbols\r\nINFO: From Linking external/grpc/libgpr_base.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_iphone.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_linux.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(cpu_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(env_linux.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(env_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(log_android.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(log_linux.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(log_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(string_util_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(string_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(sync_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(time_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(tls_pthread.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(tmpfile_msys.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(tmpfile_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(wrap_memcpy.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(thd_windows.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/libgpr_base.a(stap_timers.o) has no symbols\r\nINFO: From Linking external/grpc/third_party/address_sorting/libaddress_sorting.a [for host]:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bazel-out/host/bin/external/grpc/third_party/address_sorting/libaddress_sorting.a(address_sorting_windows.o) has no symbols\r\nERROR: /Users/zardoz/Projects/tensorflow/tensorflow/python/BUILD:5315:1: Executing genrule //tensorflow/python:framework/fast_tensor_util.pyx_cython_translation failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/execroot/org_tensorflow/bazel-out/host/bin/external/cython/cython_binary.runfiles/cython/cython.py\", line 17, in <module>\r\n    main(command_line = 1)\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py\", line 720, in main\r\n    result = compile(sources, options)\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py\", line 695, in compile\r\n    return compile_multiple(source, options)\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py\", line 666, in compile_multiple\r\n    context = options.create_context()\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py\", line 590, in create_context\r\n    self.cplus, self.language_level, options=self)\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Main.py\", line 75, in __init__\r\n    from . import Builtin, CythonScope\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/CythonScope.py\", line 5, in <module>\r\n    from .UtilityCode import CythonUtilityCode\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/UtilityCode.py\", line 3, in <module>\r\n    from .TreeFragment import parse_from_strings, StringParseContext\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/TreeFragment.py\", line 17, in <module>\r\n    from .Visitor import VisitorTransform\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/Visitor.py\", line 15, in <module>\r\n    from . import ExprNodes\r\n  File \"/private/var/tmp/_bazel_zardoz/5e080a8a46c0e2b2146c013eb1079337/external/cython/Cython/Compiler/ExprNodes.py\", line 2875\r\n    await = None\r\n          ^\r\nSyntaxError: invalid syntax\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 179.318s, Critical Path: 6.38s\r\nINFO: 413 processes: 413 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Updated the original post as requested.", "@homofortis You could use this meanwhile with Homebrew to downgrade your Python version.\r\n\r\n```\r\nbrew install https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb\r\n```\r\n\r\n`async` and `await` are now keywords and need to be replaced in the TF codebase. See https://docs.python.org/3/whatsnew/3.7.html#summary-release-highlights", "#17022", "@activatedgeek Pardon me, but I fail to see how downgrading answers the the OP question regarding the timeline of making Tensorflow compatible with Python 3.7 (released almost a month ago).", "@homofortis Apologies. I probably missed a few words in there and thought that your main objective was to compile from source. A lot of searches were leading to this issue, I thought it would be good for everyone who's looking to just run Tensorflow.", "As I see from description diagnostics is not related to tensorflow compatibility with Python-3.7 but with usage of too old Cython and currently exactly this issue is not reproducing because Cython mentioned in  Bazel workspace is new enough. On the other hand there are at least 2 Python-3.7 compatibility issues:\r\n- usage of ``async`` keyword as variable name in ``pywrap_tensorflow_internal.py`` generated from ``tensorflow/c/eager/c_api.{h,cc}`` - #20690\r\n- and breakage caused by change ``PyUnicode_AsUTF8AndSize()``'s return type from ``char *`` to ``const char *``\r\nMaybe it would be better to put the latter to separate issue.", "FWIW, I just built (not tested yet) tensorflow 1.9 with MKL on Windows for Python 3.7 using VS2017 and the following patch:\r\n```diff\r\ndiff --git a/tensorflow/c/eager/c_api.h b/tensorflow/c/eager/c_api.h\r\nindex 1862af3ce2..093b97110f 100644\r\n--- a/tensorflow/c/eager/c_api.h\r\n+++ b/tensorflow/c/eager/c_api.h\r\n@@ -76,7 +76,7 @@ typedef enum TFE_ContextDevicePlacementPolicy {\r\n // Sets the default execution mode (sync/async). Note that this can be\r\n // overridden per thread using TFE_ContextSetAsyncForThread.\r\n TF_CAPI_EXPORT extern void TFE_ContextOptionsSetAsync(TFE_ContextOptions*,\r\n-                                                      unsigned char async);\r\n+                                                      unsigned char is_async);\r\n \r\n TF_CAPI_EXPORT extern void TFE_ContextOptionsSetDevicePlacementPolicy(\r\n     TFE_ContextOptions*, TFE_ContextDevicePlacementPolicy);\r\n@@ -125,7 +125,7 @@ TFE_ContextGetDevicePlacementPolicy(TFE_Context*);\r\n \r\n // Overrides the execution mode (sync/async) for the current thread.\r\n TF_CAPI_EXPORT extern void TFE_ContextSetAsyncForThread(TFE_Context*,\r\n-                                                        unsigned char async,\r\n+                                                        unsigned char is_async,\r\n                                                         TF_Status* status);\r\n \r\n // Causes the calling thread to block till all ops dispatched in async mode\r\ndiff --git a/tensorflow/core/platform/windows/port.cc b/tensorflow/core/platform/windows/port.cc\r\nindex 174f41a993..b06434620e 100644\r\n--- a/tensorflow/core/platform/windows/port.cc\r\n+++ b/tensorflow/core/platform/windows/port.cc\r\n@@ -57,6 +57,11 @@ int NumSchedulableCPUs() {\r\n   return system_info.dwNumberOfProcessors;\r\n }\r\n \r\n+int NumHyperthreadsPerCore() {\r\n+  static const int ht_per_core = tensorflow::port::CPUIDNumSMT();\r\n+  return (ht_per_core > 0) ? ht_per_core : 1;\r\n+}\r\n+\r\n void* AlignedMalloc(size_t size, int minimum_alignment) {\r\n #ifdef TENSORFLOW_USE_JEMALLOC\r\n   void* ptr = NULL;\r\ndiff --git a/tensorflow/python/eager/pywrap_tfe_src.cc b/tensorflow/python/eager/pywrap_tfe_src.cc\r\nindex 6c9481c3af..13edbb07db 100644\r\n--- a/tensorflow/python/eager/pywrap_tfe_src.cc\r\n+++ b/tensorflow/python/eager/pywrap_tfe_src.cc\r\n@@ -813,7 +813,7 @@ char* TFE_GetPythonString(PyObject* o) {\r\n   }\r\n #if PY_MAJOR_VERSION >= 3\r\n   if (PyUnicode_Check(o)) {\r\n-    return PyUnicode_AsUTF8(o);\r\n+    return (char *)PyUnicode_AsUTF8(o);\r\n   }\r\n #endif\r\n   return nullptr;\r\ndiff --git a/tensorflow/python/lib/core/ndarray_tensor.cc b/tensorflow/python/lib/core/ndarray_tensor.cc\r\nindex 9df38d464c..4150fbfdd4 100644\r\n--- a/tensorflow/python/lib/core/ndarray_tensor.cc\r\n+++ b/tensorflow/python/lib/core/ndarray_tensor.cc\r\n@@ -154,7 +154,7 @@ Status PyBytesArrayMap(PyArrayObject* array, F f) {\r\n     if (PyUnicode_Check(item.get())) {\r\n #if PY_VERSION_HEX >= 0x03030000\r\n       // Accept unicode by converting to UTF-8 bytes.\r\n-      ptr = PyUnicode_AsUTF8AndSize(item.get(), &len);\r\n+      ptr = (char *)PyUnicode_AsUTF8AndSize(item.get(), &len);\r\n       if (!ptr) {\r\n         return errors::Internal(\"Unable to get element as UTF-8.\");\r\n       }\r\ndiff --git a/tensorflow/python/lib/core/py_func.cc b/tensorflow/python/lib/core/py_func.cc\r\nindex 30c1a9c759..231a66de59 100644\r\n--- a/tensorflow/python/lib/core/py_func.cc\r\n+++ b/tensorflow/python/lib/core/py_func.cc\r\n@@ -322,7 +322,7 @@ Status ConvertNdarrayToTensor(PyObject* obj, Tensor* ret) {\r\n         Py_ssize_t el_size;\r\n         if (PyBytes_AsStringAndSize(input_data[i], &el, &el_size) == -1) {\r\n #if PY_MAJOR_VERSION >= 3\r\n-          el = PyUnicode_AsUTF8AndSize(input_data[i], &el_size);\r\n+          el = (char *)PyUnicode_AsUTF8AndSize(input_data[i], &el_size);\r\n #else\r\n           el = nullptr;\r\n           if (PyUnicode_Check(input_data[i])) {\r\n```", "I prefer to add the `const` qualifier to the target instead of removing it from from the result of `PyUnicode_AsUTF8AndSize()`. This is a constant string, and it should not be modified.", "@asimshankar says that he's been reviewing and sending PRs on this topic. I'll assign him. ", "I'm facing similar issues when building from source too, using python 3.7 in Arch Linux.\r\n```\r\nERROR: /home/rharish/.cache/bazel/_bazel_rharish/5d4d7b1255c710f6c814ab2f3f084405/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/rharish/.cache/bazel/_bazel_rharish/5d4d7b1255c710f6c814ab2f3f084405/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=:/usr/local/lib:/opt/cuda/lib64 \\\r\n    PATH=/home/rharish/bin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/opt/cuda/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.pic.d '-frandom-seed=bazel-out/host/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.pic.o' -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/local_config_python -iquote bazel-out/host/genfiles/external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -isystem external/protobuf_archive/python -isystem bazel-out/host/genfiles/external/protobuf_archive/python -isystem bazel-out/host/bin/external/protobuf_archive/python -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/local_config_python/python_include -isystem bazel-out/host/genfiles/external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-writable-strings '-DGOOGLE_PROTOBUF_HAS_ONEOF=1' '-DPROTOBUF_PYTHON_ALLOW_OVERSIZE_PROTOS=1' -c external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc -o bazel-out/host/bin/external/protobuf_archive/_objs/python/google/protobuf/pyext/_message.so/descriptor_containers.pic.o)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc: In function 'bool google::protobuf::python::descriptor::_GetItemByKey(google::protobuf::python::PyContainer*, PyObject*, const void**)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:172:13: note: in expansion of macro 'PyString_AsStringAndSize'\r\n         if (PyString_AsStringAndSize(key, &name, &name_size) < 0) {\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:189:13: note: in expansion of macro 'PyString_AsStringAndSize'\r\n         if (PyString_AsStringAndSize(key, &camelcase_name, &name_size) < 0) {\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-writable-strings'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 63.634s, Critical Path: 9.08s\r\nINFO: 464 processes: 464 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nMy system configuration is:\r\nOS Platform and Distribution: Arch Linux\r\nPython: Python 3.7.0\r\nTensorFlow installed from: source (https://github.com/tensorflow/tensorflow.git), master branch\r\nTensorFlow version: TensorFlow 1.9.0\r\nBazel version: 0.16.0\r\nCUDA/cuDNN version: CUDA 9.2\r\nGPU model and memory: NVIDIA GeForce GTX 960M, 4GB", "@bstriner has volunteered to update #21202 which will take us one step further. But it seems that we'll need to wait for a protobuf release with support for Python 3.7 and then update TensorFlow dependencies to use that new protobuf version.", "@rharish101 \r\nIf you are using Arch Linux, you can install Tensorflow with `pacman-S python-tensorflow`.", "@rharish101 \r\nIf you need CUDA support, you can install `pacman-S python-tensorflow-cuda`", "@hzxie Yup, it works well now! How did Arch's guys get it to work?", "No protobuf release yet supports 3.7 but if you're willing to use a snapshot from master then you can build TF for 3.7", "@bstriner \r\nI still cannot compile the latest master. The build exits with errors.\r\n\r\nOS Platform and Distribution: Mac OS X 10.13.5\r\nPython: Python 3.7.0\r\nTensorFlow installed from: source (https://github.com/tensorflow/tensorflow.git)\r\nTensorFlow version: TensorFlow 1.10\r\nBazel version: 0.15.2-homebrew\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce:\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nERROR: /Users/zardoz/Projects/tensorflow/tensorflow/python/eager/BUILD:10:1: C++ compilation of rule '//tensorflow/python/eager:pywrap_tfe_lib' failed (Exit 1)\r\ntensorflow/python/eager/pywrap_tfe_src.cc:219:11: error: cannot initialize a variable of type 'char *' with an rvalue of type 'const char *'\r\n    char* buf = PyUnicode_AsUTF8AndSize(py_value, &size);\r\n          ^     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/python/eager/pywrap_tfe_src.cc:834:12: error: cannot initialize return object of type 'char *' with an rvalue of type 'const char *'\r\n    return PyUnicode_AsUTF8(o);\r\n           ^~~~~~~~~~~~~~~~~~~\r\n2 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 5896.213s, Critical Path: 267.71s\r\nINFO: 2883 processes: 2883 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@cgohlke How to build tensorflow-gpu for python 3.7?", "@homofortis that's because this pr isn't merged yet. You need to pull it in yourself. https://github.com/tensorflow/tensorflow/pull/21202\r\n\r\nTo pull py37 fixes into your current repo, something like this:\r\n```\r\ngit remote add bstriner https://github.com/bstriner/tensorflow.git\r\ngit fetch bstriner\r\ngit merge bstriner/py37\r\n```", "Basically, if you see issues with `const char *` being cast to `char *` or something similar, that is the changes in py37. Fix and discussion in the linked PR.", "@bstriner I need a wheel file of tensorflow gpu for python 3.7. I cant build it by myself", "When is this issue fixed??", "Thanks to @bstriner 's contribution (PR #21202), we should be able to build for Python 3.7 now. However, as he's alluded to in the PR, the eigen and protobuf libraries still need a fix to compile on Windows.\r\n\r\n@gunan @angersson would know about whether the official release of TensorFlow 1.11 will support Python 3.7, or if you'll have to compile from source.", "We still have a problem on the infra side we need to figure out.\r\nAll of our build infra uses ubuntu 14, due to community requests (to be compatible with ubuntu 14).\r\nHowever, python 3.7 is not straightforward to build on ubuntu 14, since it needs updated versions of a few libraries that come with ubuntu.\r\n\r\nWhile we wait for eigen and protobuf fixes on windows, we will resolve those.\r\nBut this means 1.11 wont have a prebuilt package for python 3.7\r\n", "@SukeshP1995 You may try this url https://www.lfd.uci.edu/~gohlke/pythonlibs/#tensorflow", "Are the any Ubuntu 16.04 TensorFlow gpu wheels for python 3.7 available? Or, if not, could I please get a short guide for building it myself? I don't have the experience to just go outand do it without anyone holding my hand :)", "@bjtho08 have you seen https://www.tensorflow.org/install/install_linux#python_36 ?", "@morenoh149 that is only for python 3.6. I have 3.7 because I just did a fresh install. ", "https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-418442189 still applies.\r\nIn addition to infra issues (python 3.7 on ubuntu 14) not all of our dependencies have python 3.7 support yet. So we are still working with them to move to python 3.7.", "ah no, that's the fundamental problem @bjtho08 tensorflow needs to stop using the new python keywords in 3.7. That's what this thread is about. In the meantime you can use the pending forks or use 3.6", "@morenoh149 so my options are to build an older version of python or building tensorflow from git/master? ", "as of today yes. This PR https://github.com/tensorflow/tensorflow/pull/21202 is on master but a release hasn't been cut yet. FWIW you can use [pyenv](https://github.com/pyenv/pyenv) to install many python versions on a system. A professional software engineer should be able to pin their dependencies and tools depending on the project.", "@morenoh149 @bjtho08 building tensorflow from master branch isn't enough. Some dependecies still don't support 3.7 yet.", "@adrianodennanni the dependencies support 3.7, but not in released versions. You need to change the libraries in the workspace to the current masters. Can't change the dependencies in tensorflow until those libraries have stable releases.\r\n\r\nSo, for current protobuf, use this:\r\n```\r\n    PROTOBUF_URLS = [\r\n        \"https://mirror.bazel.build/github.com/google/protobuf/archive/a6e1cc7e328c45a0cb9856c530c8f6cd23314163.tar.gz\",\r\n        \"https://github.com/google/protobuf/archive/a6e1cc7e328c45a0cb9856c530c8f6cd23314163.tar.gz\",\r\n    ]\r\n    PROTOBUF_SHA256 = \"f785d2009ea7c8484cb0443d9db8fe55f73cfdb6e112bfa659a8a5cdaf664ccd\"\r\n    PROTOBUF_STRIP_PREFIX = \"protobuf-a6e1cc7e328c45a0cb9856c530c8f6cd23314163\"\r\n```\r\n\r\nYou might also need latest eigen.", "> @rharish101\r\n> If you need CUDA support, you can install `pacman-S python-tensorflow-cuda`\r\n\r\nwell done  thanks ", "> as of today yes. This PR #21202 is on master but a release hasn't been cut yet. FWIW you can use [pyenv](https://github.com/pyenv/pyenv) to install many python versions on a system. A professional software engineer should be able to pin their dependencies and tools depending on the project.\r\n\r\nThanks for the tip, @morenoh149 ! I made it work by starting over using pyenv and CUDA 9.0 :)", "@gunan \r\n> However, python 3.7 is not straightforward to build on ubuntu 14, since it needs updated versions of a few libraries that come with ubuntu.\r\n\r\nI'd like to share my experience in building Python-3.7 on Ubuntu 12, and I hope it will help if you haven't solved that issue yet. I build using following configure flags:\r\n```\r\n    --prefix=... \\\r\n    --enable-ipv6 \\\r\n    --with-dbmliborder=gdbm \\\r\n    --with-system-expat \\\r\n    --with-computed-gotos \\\r\n    --with-system-ffi \\\r\n    --with-ensurepip=no\r\n```\r\nAnd the only system library too outdated for Python was OpenSSL, thus ``ssl`` module cannot be built. To solve the issue I decided to build OpenSSL from source and link it statically using ``cryptography`` recipe (https://cryptography.io/en/latest/installation/#static-wheels):\r\n- build properly configured OpenSSL with only static libs:\r\n```\r\nOPENSSL_VERSION=1.0.2p\r\nwget https://www.openssl.org/source/openssl-${OPENSSL_VERSION}.tar.gz\r\ntar xf openssl-${OPENSSL_VERSION}.tar.gz\r\ncd openssl-${OPENSSL_VERSION}\r\n./config no-shared no-ssl2 no-ssl3 -fPIC --prefix=$(pwd)/_openssl\r\nmake && make install\r\n```\r\n- build Python using that OpenSSL by passing flag to ``configure`` script: ``--with-openssl=$(pwd)/openssl-${OPENSSL_VERSION}/_openssl``, so ``configure`` invocation looks like:\r\n```\r\n./configure\r\n    --prefix=... \\\r\n    --enable-ipv6 \\\r\n    --with-dbmliborder=gdbm \\\r\n    --with-system-expat \\\r\n    --with-computed-gotos \\\r\n    --with-system-ffi \\\r\n    --with-ensurepip=no \\\r\n    --with-openssl=$(pwd)/openssl-${OPENSSL_VERSION}/_openssl\r\n```\r\n\r\nBesides outdated OpenSSL issue I haven't met problems building Python-3.7 on old Ubuntu, I tested my build and it seems to work properly, tests from Python source pass.", "Nagging Assignees @gunan, @angersson: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@bstriner you suggested changing PROTOBUF_URLS, PROTOBUF_SHA256 and PROTOBUF_STRIP_PREFIX. I assume you do this in tensorflow/workspace.bzl? Anywhere else? Thanks.", "Actually, after trying this, bazel build of tensorflow 1.11 results in:\r\n\r\nERROR: tensorflow/tensorflow/tools/pip_package/BUILD:216:1: error loading package 'tensorflow': Extension file not found. Unable to load package for '@bazel_skylib//:lib.bzl': The repository could not be resolved and referenced by '//tensorflow/tools/pip_package:build_pip_package'\r\n", "@jeffcbecker I also got this problem. It seems that the URL `https://mirror.bazel.build/github.com/google/protobuf/archive/a6e1cc7e328c45a0cb9856c530c8f6cd23314163.tar.gz` is not avaliable. Anyone got a workaround for this?", "I was able to work around by using Python 3.6.\nCheers\nJeff\nSent from my T-Mobile 4G LTE Device\n-------- Original message --------From: Adriano Dennanni <notifications@github.com> Date: 10/21/18  1:40 PM  (GMT-08:00) To: tensorflow/tensorflow <tensorflow@noreply.github.com> Cc: jeffcbecker <jeff.c.becker@gmail.com>, Mention <mention@noreply.github.com> Subject: Re: [tensorflow/tensorflow] Python 3.7 compatibility (#20517) \n@jeffcbecker I also got this problem. It seems that the URL https://mirror.bazel.build/github.com/google/protobuf/archive/a6e1cc7e328c45a0cb9856c530c8f6cd23314163.tar.gz is not avaliable. Anyone got a workaround for this?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://assets-cdn.github.com/images/email/message_cards/header.png\",\"avatar_image_url\":\"https://assets-cdn.github.com/images/email/message_cards/avatar.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@adrianodennanni in #20517: @jeffcbecker I also got this problem. It seems that the URL `https://mirror.bazel.build/github.com/google/protobuf/archive/a6e1cc7e328c45a0cb9856c530c8f6cd23314163.tar.gz` is not avaliable. Anyone got a workaround for this?\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-431701713\"}}}\n[\n{\n\"@context\": \"http://schema.org\",\n\"@type\": \"EmailMessage\",\n\"potentialAction\": {\n\"@type\": \"ViewAction\",\n\"target\": \"https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-431701713\",\n\"url\": \"https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-431701713\",\n\"name\": \"View Issue\"\n},\n\"description\": \"View this Issue on GitHub\",\n\"publisher\": {\n\"@type\": \"Organization\",\n\"name\": \"GitHub\",\n\"url\": \"https://github.com\"\n}\n},\n{\n\"@type\": \"MessageCard\",\n\"@context\": \"http://schema.org/extensions\",\n\"hideOriginalBody\": \"false\",\n\"originator\": \"AF6C5A86-E920-430C-9C59-A73278B5EFEB\",\n\"title\": \"Re: [tensorflow/tensorflow] Python 3.7 compatibility (#20517)\",\n\"sections\": [\n{\n\"text\": \"\",\n\"activityTitle\": \"**Adriano Dennanni**\",\n\"activityImage\": \"https://assets-cdn.github.com/images/email/message_cards/avatar.png\",\n\"activitySubtitle\": \"@adrianodennanni\",\n\"facts\": [\n\n]\n}\n],\n\"potentialAction\": [\n{\n\"name\": \"Add a comment\",\n\"@type\": \"ActionCard\",\n\"inputs\": [\n{\n\"isMultiLine\": true,\n\"@type\": \"TextInput\",\n\"id\": \"IssueComment\",\n\"isRequired\": false\n}\n],\n\"actions\": [\n{\n\"name\": \"Comment\",\n\"@type\": \"HttpPOST\",\n\"target\": \"https://api.github.com\",\n\"body\": \"{\\n\\\"commandName\\\": \\\"IssueComment\\\",\\n\\\"repositoryFullName\\\": \\\"tensorflow/tensorflow\\\",\\n\\\"issueId\\\": 20517,\\n\\\"IssueComment\\\": \\\"{{IssueComment.value}}\\\"\\n}\"\n}\n]\n},\n{\n\"name\": \"Close issue\",\n\"@type\": \"HttpPOST\",\n\"target\": \"https://api.github.com\",\n\"body\": \"{\\n\\\"commandName\\\": \\\"IssueClose\\\",\\n\\\"repositoryFullName\\\": \\\"tensorflow/tensorflow\\\",\\n\\\"issueId\\\": 20517\\n}\"\n},\n{\n\"targets\": [\n{\n\"os\": \"default\",\n\"uri\": \"https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-431701713\"\n}\n],\n\"@type\": \"OpenUri\",\n\"name\": \"View on GitHub\"\n},\n{\n\"name\": \"Unsubscribe\",\n\"@type\": \"HttpPOST\",\n\"target\": \"https://api.github.com\",\n\"body\": \"{\\n\\\"commandName\\\": \\\"MuteNotification\\\",\\n\\\"threadId\\\": 352548653\\n}\"\n}\n],\n\"themeColor\": \"26292E\"\n}\n]", "protobuf wheel have [been upgraded](https://pypi.org/project/protobuf/#files) to support 3.7. \r\n\r\nNow it is time to release tensorflow for 3.7 support.  ", "Great news!\r\nWould you like to send a PR to bump TF protobuf dependencies in workspace and setup.py?", "Tensorflow is apparently the last missing piece of Python-3.7.1. It may have an impact on some Cloud Electric bill as Python-3.7 is more efficient than Python-3.6.", "https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.12.0/py37\r\n\r\nFor anyone who wants to test tensorflow 1.12.0 with python 3.7 on windows.\r\n\r\ndepands on protobuf v3.6.0 + cherry-pick https://github.com/protocolbuffers/protobuf/commit/0a59054c30e4f0ba10f10acfc1d7f3814c63e1a7", "Managed to compile it for Linux too!\r\n\r\nhttps://github.com/adrianodennanni/tensorflow-1.12.0-cp37-cp37m-linux_x86_64/", "It's been months since 3.7 release, updates for TF are still rolling out, so what is the word/ETA on compatibility?", "So the year of 2018 is coming to an end, and the official python 3.7 supported release is still a bubble? :)", "The version posted by @adrianodennanni works for me. I'm using python3.7 features in my code and it's a real hustle to switch environments all the time. ", "For what it's worth, I was able to compile tf and build a Python3.7 wheel on OSX using the current master.", "I've submitted https://github.com/tensorflow/tensorflow/commit/b0d7d8a477d3041e2d0ebd0cb1d35e4a7fa09663 which should allow you to build for 3.7. tf-nightly now has a CPU (Ubuntu only) version that only works on Ubuntu16.04+. GPU version for Ubuntu coming shortly. ", "@av8ramit \r\n\r\nthanks to you, I've just successfully built tf with python 3.7.1, cuda 10, and cudnn 7.4\r\n\r\nnow only bazel 0.20 support is left", "tf-nightly-gpu builds with CUDA 10 are also on pypi now. I'll keep this bug open until we have an official Python3.7 release build.", "@alanpurple can you elaborate? I cant build tf 1.12 to save my life on Ubuntu 18 and python 3.7", "https://drive.google.com/open?id=16vXTOJHXCLDKMTqtxMLDugUsRhiZPriQ\r\n1.12 Python 3.7 without AVX extensions.", "https://drive.google.com/open?id=1ni7ExGVb6-c6gvb4J0hohpT4Jj4Z4xxO\r\n1.12 Python 3.7 wheel with SSE, XLA.", "On windows:\r\nsuccessfully builded 1.12-cpu, and imported in py3.7.\r\nbut gpu:\r\nps: using bazel 0.21\r\n```\r\nINFO: From Linking tensorflow/contrib/tpu/python/ops/_tpu_ops.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/tpu/python/ops/python/ops/lib_tpu_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/tpu/python/ops/python/ops/lib_tpu_ops.so.exp\r\nINFO: From Linking tensorflow/contrib/tensor_forest/python/ops/_stats_ops.so:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/tensor_forest/python/ops/python/ops/lib_stats_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/tensor_forest/python/ops/python/ops/lib_stats_ops.so.exp\r\nERROR: C:/tensorflow/tensorflow/python/keras/api/BUILD:28:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/USER/_bazel_USER/xv6zejqw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Python37/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.exe  --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api_v1/ --apiname=keras --apiversion=1  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api._v1 bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/python/keras/api/_v1/keras/wrappers/scikit_learn/__init__.py\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\USER\\AppData\\Local\\Temp\\Bazel.runfiles_ms8gr8rl\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2602.108s, Critical Path: 565.09s\r\nINFO: 4691 processes: 4691 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Let me tell you what issue I faced and how I resolved them. I am using Mac OS\r\n\r\nTensorflow is not compatible with python 3.7 its only available for i think upto python 3.6 as of now..\r\n\r\nI was using python 3.7 so I download python 3.6 from its official website.. a installer package and install it. And pin the python 3.6 app to dock because as soon as python 3.7 is in the launcher. Python 3.6 will not show.. even though it will show on application tab as a secondary folder\r\n\r\nNow open terminal and type: nano .bash_profile\r\nThen a nano editor will open uncomment the path for python 3.6 and comment the path for 3.7. then press control+X then y for yes and then Enter\r\n\r\nAfterdat restart the terminal and type: echo $PATH\r\nMake sure the first link is of python 3.6\r\n\r\nNow type: python3 and hit enter and check which version is opening just to double sure that is python 3.6\r\n\r\nNow type: python3 -m pip install tensorflow\r\nYou can download other modules in the same way.\r\n\r\nNow when you want to work with python 3.6 open. Python from the dock. And work.. and if you want to work on 3.7 you can open the python 3.7 and it will also work fluently\r\n\r\nTo install modules in python 3.7,. Just type: python3.7 -m pip install package namr", "Got python3.7.2 very recent compiled for windows with CUDA 10, links at this repo:\r\n[https://github.com/PlatinumLyfe/tf-windows-gpu/](https://github.com/PlatinumLyfe/tf-windows-gpu/)", "Hi, any progress on this compatibility issue?\r\n\r\nbbhattmaclap:~ bbhatt$ pip3 install --upgrade tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\nbbhattmaclap:~ bbhatt$", "@BhuvaneshBhatt the latest tensorflow official package does not have Python3.7 support. You'll have to use your tf-nightly-gpu package. We are trying to have it for 1.13.", "@PlatinumLyfe unable to install.\r\nyour link only have `xxx-cp36-cp36m-...`.\r\nno `cp37` and no `-gpu-` version.", "Please stop asking for other people to provide compiled binaries.  In addition to spamming everyone interested in when official Python 3.7 support lands, asking on a publicly commentable page for someone to send you a binary to run is not particularly secure.", "Tensorflow 1.13-rc0 has been released (https://github.com/tensorflow/tensorflow/releases/tag/v1.13.0-rc0), however there is no Python 3.7 build on PyPI (https://pypi.org/project/tensorflow/1.13.0rc0/#files). Will Tensorflow 1.13 be released for Python 3.7 officially?", "We are aiming to try and have Windows and Ubuntu python binaries by rc2 or the official.", "Any news about Mac support? I'm stuck on Mac for now.\nOn Jan 24, 2019, 6:55 AM -0600, Amit Patankar <notifications@github.com>, wrote:\n> We are aiming to try and have Windows and Ubuntu python binaries by rc2 or the official.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "We now have a `tf-nightly` [CPU package](https://files.pythonhosted.org/packages/47/a7/913eabde0a8c8ea1f1cc7e3a274cf9d2b91b89b654df0c9a9556c770fd15/tf_nightly-1.13.0.dev20190206-cp37-cp37m-macosx_10_9_x86_64.whl) for Mac. Will try and have that for rc2 as well. ", "To anyone else coming to this thread, the above mentioned tf-nightly packages are good. Install them with `pip3 install tf-nightly` for the CPU version. Tested, working on Python 3.7.2 on MacOS Mojave 10.14.2", "The 1.13.0rc1 release includes Python3.7 binaries for all OS-es for [cpu](https://pypi.org/project/tensorflow/1.13.0rc1/#files) and [gpu](https://pypi.org/project/tensorflow-gpu/1.13.0rc1/#files). ", "I can't seem to be able to install it. Any simple troubleshooting steps ?\r\nEDIT : Issue was the 32-bit version of CPython was used on a 64-bit system.", "@MagixInTheAir I'm closing this issue since it's just about general Python 3.7 support. Please reopen a new issue with logs and more information about your setup if you're still having issues. ", "[Tensorflow 1.13.1](https://github.com/tensorflow/tensorflow/releases/tag/v1.13.1) now supports Python 3.7.", "https://pypi.org/project/tensorflow/#files There are cp37 releases for tensorflow==1.13.1. There may be something wrong in your environment.", "I am using 3.7.2 and I have the same issue, the version/tags reported for 1.31.1 are the following:\r\n\r\n```python\r\n{('cp37', 'cp37m', 'manylinux1_x86_64')}\r\n```\r\n\r\nwhile my 3.7.2 supports the following:\r\n\r\n```python\r\n[('cp37', 'cp37m', 'linux_x86_64'), ('cp37', 'abi3', 'linux_x86_64'), ('cp37', 'none', 'linux_x86_64'), ('cp36', 'abi3', 'linux_x86_64'), ('cp35', 'abi3', 'linux_x86_64'), ('cp34', 'abi3', 'linux_x86_64'), ('cp33', 'abi3', 'linux_x86_64'), ('cp32', 'abi3', 'linux_x86_64'), ('py3', 'none', 'linux_x86_64'), ('cp37', 'none', 'any'), ('cp3', 'none', 'any'), ('py37', 'none', 'any'), ('py3', 'none', 'any'), ('py36', 'none', 'any'), ('py35', 'none', 'any'), ('py34', 'none', 'any'), ('py33', 'none', 'any'), ('py32', 'none', 'any'), ('py31', 'none', 'any'), ('py30', 'none', 'any')]\r\n```\r\n\r\n(from pep425tags.get_supported())\r\n\r\nso I believe the issue is just that it should be ```linux``` not ```manylinux1``` in the wheel name.\r\n\r\nI have a clean python build from source with almost default parameters.", "@dellelce  you may be using a very old version of pip?\r\n```\r\nimport pip._internal; print(pip._internal.pep425tags.get_supported())\r\n```\r\ngives me manylinux tags.", "I just checked and the problem is with the alpine build, my build (dellelce/py-base) and official docker alpine image (python:alpine) have the issue while an image built on debian (python:latest)  works fine.\r\nIt must be related to the libc library used (musl vs glibc) ? So other dists that do not used glibc may have the issue... \r\n\r\nAll have latest pip and  3.7.3 or 3.7.2... ", " I\u2019m pretty sure that Alpine is not included in the many Linux that manylinux supports.", "@ppwwyyxx should we have another ticket for supporting non-glibc linux distributions?\r\n\r\nPEP 571/PEP 513 (\"manylinux\") support only glibc.", "@dellelce Supporting alpine linux could be an interesting idea but may not be trivial. At the moment most of the tensorflow binaries are compiled with Ubuntu 14.04. Even some other commonly used platforms  (e.g. CentOS) occasionally encountered some issues. I think alpine support is unlikely to be a priority in the short term.\r\n\r\nOpening an issue is always a good thing, as it could help gauge the need and interest from the community, and it could always be labeled as \"contributions welcome\"."]}, {"number": 20516, "title": "Cannot restore variables with Checkpoint because keys do not align", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tf-nightly==1.10.0.dev20180609\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**:\r\n\r\nI get the error that is thrown here:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/util.py#L633\r\n\r\nI cannot provide code to reproduce it. But basically what happens is I have a class that inherits from `Checkpointable`. It assigns all variables to itself to make them checkpointable. It also assigns an optimizer to itself. I then save the model and restore it. When calling `.assert_consumed()` on the `load_status` object of `restore(dir, session)` it throws an error because some key does not match. The variable it tries to restore is actually in the saved checkpoint it just has a different key then the one it gets from `enumerate(self._checkpoint.object_graph_proto.nodes)`.\r\n\r\nThis describes it as good as I can. Sorry for not being able to share the code. I tried to reproduce it but so far I cannot. I believe it is a bug, because I call `ckpt.save()` and immediately after it `ckpt.restore()` and I get the exception.\r\n\r\nOutput of `self._checkpoint.object_by_proto_id.keys()` at line 631.\r\nYou can see that some keys are missing (idk why) but theses are the ones I need to restore.\r\n```\r\n[0, 1, 2, 3, 4, 5, 6, 7, 14, 19, 10, 17, 22, 11, 18, 23, 12, 13]\r\n```\r\n\r\nOutput of `util._serialize_object_graph(self.checkpoint, None)` after `restore` before `assert_consumed`\r\n```json\r\nnodes {\r\n  children {\r\n    node_id: 1\r\n    local_name: \"model\"\r\n  }\r\n  children {\r\n    node_id: 2\r\n    local_name: \"save_counter\"\r\n  }\r\n}\r\nnodes {\r\n  children {\r\n    node_id: 3\r\n    local_name: \"_global_step_pretrain\"\r\n  }\r\n  children {\r\n    node_id: 4\r\n    local_name: \"embedding\"\r\n  }\r\n  children {\r\n    node_id: 5\r\n    local_name: \"cell\"\r\n  }\r\n  children {\r\n    node_id: 6\r\n    local_name: \"dense\"\r\n  }\r\n  children {\r\n    node_id: 7\r\n    local_name: \"_optimizer\"\r\n  }\r\n  children {\r\n    local_name: \"_checkpoint\"\r\n  }\r\n}\r\nnodes {\r\n  attributes {\r\n    name: \"VARIABLE_VALUE\"\r\n    full_name: \"initialize_or_restore/save_counter\"\r\n    checkpoint_key: \"save_counter/.ATTRIBUTES/VARIABLE_VALUE\"\r\n  }\r\n}\r\nnodes {\r\n  attributes {\r\n    name: \"VARIABLE_VALUE\"\r\n    full_name: \"ConditionedLSTMGenerator2/CONDITIONED_LSTM/pretrain/global_step\"\r\n    checkpoint_key: \"model/_global_step_pretrain/.ATTRIBUTES/VARIABLE_VALUE\"\r\n  }\r\n}\r\nnodes {\r\n  attributes {\r\n    name: \"VARIABLE_VALUE\"\r\n    full_name: \"ConditionedLSTMGenerator2/CONDITIONED_LSTM/embedding\"\r\n    checkpoint_key: \"model/embedding/.ATTRIBUTES/VARIABLE_VALUE\"\r\n  }\r\n}\r\nnodes {\r\n  children {\r\n    node_id: 8\r\n    local_name: \"kernel\"\r\n  }\r\n  children {\r\n    node_id: 9\r\n    local_name: \"bias\"\r\n  }\r\n  attributes {\r\n    name: \"OBJECT_CONFIG_JSON\"\r\n    checkpoint_key: \"model/cell/.ATTRIBUTES/OBJECT_CONFIG_JSON\"\r\n  }\r\n}\r\nnodes {\r\n  children {\r\n    node_id: 10\r\n    local_name: \"kernel\"\r\n  }\r\n  children {\r\n    node_id: 11\r\n    local_name: \"bias\"\r\n  }\r\n  attributes {\r\n    name: \"OBJECT_CONFIG_JSON\"\r\n    checkpoint_key: \"model/dense/.ATTRIBUTES/OBJECT_CONFIG_JSON\"\r\n  }\r\n}\r\nnodes {\r\n  children {\r\n    node_id: 12\r\n    local_name: \"beta1_power\"\r\n  }\r\n  children {\r\n    node_id: 13\r\n    local_name: \"beta2_power\"\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 4\r\n    slot_name: \"m\"\r\n    slot_variable_node_id: 14\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 8\r\n    slot_name: \"m\"\r\n    slot_variable_node_id: 15\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 9\r\n    slot_name: \"m\"\r\n    slot_variable_node_id: 16\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 10\r\n    slot_name: \"m\"\r\n    slot_variable_node_id: 17\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 11\r\n    slot_name: \"m\"\r\n    slot_variable_node_id: 18\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 4\r\n    slot_name: \"v\"\r\n    slot_variable_node_id: 19\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 8\r\n    slot_name: \"v\"\r\n    slot_variable_node_id: 20\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 9\r\n    slot_name: \"v\"\r\n    slot_variable_node_id: 21\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 10\r\n    slot_name: \"v\"\r\n    slot_variable_node_id: 22\r\n  }\r\n  slot_variables {\r\n    original_variable_node_id: 11\r\n    slot_name: \"v\"\r\n    slot_variable_node_id: 23\r\n  }\r\n}\r\n```\r\n", "comments": ["I assume this is something for @allenlavoie ?", "I found the culprit. Double assigning a variable. Here `BasicLSTMCell`.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.training.checkpointable import base as checkpointable\r\nfrom tensorflow.python.training.checkpointable import util\r\n\r\nclass Model(checkpointable.Checkpointable):\r\n\r\n  def __init__(self):\r\n    self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)\r\n    self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)\r\n    # self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)\r\n    out = self.cell(tf.constant([[1.]]), self.cell.zero_state(1, tf.float32))\r\n    self.optimizer = tf.train.AdamOptimizer()\r\n    self.optimizer.minimize(tf.reduce_sum(out[0]))\r\n    self.session = tf.Session()\r\n    self.checkpoint = tf.train.Checkpoint(model=self)\r\n\r\n  def init(self):\r\n    print('Init')\r\n    self.session.run(tf.global_variables_initializer())\r\n\r\n  def save(self):\r\n    print('Save')\r\n    self.checkpoint.save('./tmp/', self.session)\r\n\r\n  def restore(self):\r\n    print('Restore')\r\n    latest = tf.train.latest_checkpoint('./tmp/')\r\n    load_status = self.checkpoint.restore(latest)\r\n    print(util._serialize_object_graph(self.checkpoint, None))\r\n    load_status.assert_consumed().run_restore_ops(self.session)\r\n\r\n  def print(self):\r\n    print(self.session.run(self.cell._kernel))\r\n\r\n\r\nm = Model()\r\nm.init()\r\nm.print()\r\nm.save()\r\nm.restore()\r\nm.print()\r\n```", "It works if I double assign a `tf.get_variable` though. Seems like this https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/base.py#L593 does not override/clear all previous state.\r\n\r\nEdit: Also does not work for `tf.layers.Dense` and probably others.", "Huh, good point, looks like the by-name lookup isn't being updated when the dependency is replaced. Thank you for the report!", "Happy to help"]}, {"number": 20515, "title": " TFLite Demo mobilenet_ssd ", "body": "How are mobilenet_ssd.tflite and [detect.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip)  converted  in the newest [TFLite Demo ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android)?\r\n\r\nPreviously, I converted that model by following [this instructions](https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630),  but now,  I see that there are 4 outputs in TFLiteObjectDetectionAPIModel in the source code [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java), \r\n\r\n`    d.tfLite.setNumThreads(NUM_THREADS);\r\n    d.outputLocations = new float[1][NUM_DETECTIONS][4];\r\n    d.outputClasses = new float[1][NUM_DETECTIONS];\r\n    d.outputScores = new float[1][NUM_DETECTIONS];\r\n    d.numDetections = new float[1];`\r\n\r\nthe previously converted model can't be used.\r\n\r\nI tried the following instructions, but the error occurred.\r\n\r\n`bazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=${MODEL_PATH}/frozen_inference_graph_stripped.pb \\\r\n  --output_file=${MODEL_PATH}/ssd_mobilenet_v1_float.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shapes=1,300,300,3 \\\r\n  --input_arrays=Preprocessor/sub \\\r\n  --output_arrays=detection_boxes,detection_scores,detection_classes,num_detections \\\r\n  --dump_graphviz=${MODEL_PATH} `\r\n\r\n.......\r\n2018-07-03 19:35:14.798049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: TensorArrayGatherV3\r\n2018-07-03 19:35:14.798066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: TensorArrayGatherV3\r\n2018-07-03 19:35:14.798080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1090] Converting unsupported operation: TensorArrayGatherV3\r\n2018-07-03 19:35:15.248275: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 4096 operators, 6814 arrays (0 quantized)\r\n2018-07-03 19:35:15.563598: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4053 operators, 6719 arrays (0 quantized)\r\n2018-07-03 19:35:15.977287: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4053 operators, 6719 arrays (0 quantized)\r\n2018-07-03 19:35:16.225122: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kMerge \r\nAborted (core dumped)\r\n\r\nWho can help ?\r\n\r\nThanks!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi. I actually have the prev model in quantized format as given in the instructions you just linked. Can you please tell me how you used it bcos I am trying to run it in the TFLITE Interpreter but it doesnt seem to work :sweat_smile: ", "@abhi-rf,  @andrewharp, @karmel , The prev model by following this[ instructions](https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630)  can not work in the latest TFLite Demo, Whether it is quantified or float,  because the tflite model in the latest TFLite demo  requires 4 outputs , but prev model only have 2 outputs( concat, concat1).\r\n\r\nPlease help , Thanks!\r\n", "@WenguoLi  Hi. Yea I got that. However, I wanted to compare the models in terms of their results for my internship purposes. I am using the python API but I do not know how can I interpret the results from the concat and concat_1. I need help for that. I am using something similar to this code. \r\n```\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ninterpreter = tf.contrib.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n\r\ninput_shape = input_details[0]['shape']\r\ninput_data = my_image_array\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)`\r\n\r\n```\r\n\r\nAnd the new detect.tflite, what output corresponds to what actually ? \r\n\r\n\r\nAny help would be greatly appreciated. Thanks!", "Hi \r\nPlease specify how you obtain detect.tflite model from Tensorflow graph. We will be able to help with subsequent instructions on this in the following week (coming soon)", "Downloaded it from here\r\n[Link](https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip)\r\n\r\nI found this link in the repo only \r\n[Here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/app/download-models.gradle)", "Hi Abhishek\r\n\r\nThe downloaded model on the link (detect.tflite) you provided should work in the app - it is a quantized model that gives the four outputs as detected_boxes, detected_classes, detected_scores, num_boxes.\r\nThe earlier model mobilenet_ssd did have two outputs - if you specifically need earlier version of the app. Please sync to the github code in history.\r\nThe instructions to generate the new tflite file with four outputs are coming soon (within next week)", "Thanks @achowdhery \r\n", "Here is the blog post showing Tensorflow Lite detailed instructions\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n", "@achowdhery , Thanks!", "@achowdhery \r\nThanks, but I can't open your web", "> Here is the blog post showing Tensorflow Lite detailed instructions\r\n> https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n\r\nThis is helpful but it doesnt explain how to use the tflite model on pc, i want to test on raspberrypi but i have no idea how to interprete the **print(output_data)`**"]}, {"number": 20514, "title": "No OpKernel was registered to support Op 'QuantizedReluX' with these attrs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary using conda\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: 8.0/7.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **GPU model and memory**: Quadro M2000M/ 4GB\r\n- **Exact command to reproduce**:\r\nTried to use quantized_relu_x and it doesn't work\r\nI have already used quantized_conv2d and quantized_max_pool, both of them works fine, but have only problem when I use quantized_relu_x\r\n\r\n### Describe the problem\r\nOther two quantized kernels QuantizedRelu and QuantizedRelu6 seem to have defined but QuantizedReluX appears to be missing, although three of them are registered together in core/ops/nn_ops.cc file\r\n\r\n### Source code / logs\r\nError message that I am getting is as follows:\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'QuantizedReluX' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  >no registered kernels\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "I have already added it 3 days ago.. Please try to have a look at it", "Added a PR #20700 for QuantizedReluX kernel."]}, {"number": 20513, "title": "Update grpc to v1.13.0", "body": "~~This fix is an update to grpc v1.12.1.~~\r\n\r\nThe updates of v1.12.1 was in #20245 but was not picked up (See https://github.com/tensorflow/tensorflow/pull/20245#issuecomment-402054059)\r\n\r\nThis fix updates grpc to v1.13.0 directly.\r\n\r\n/cc @gunan @perfinion \r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Thanks for this so quick!\r\nGRPC-1.13.0 is out as of a couple days ago, lets just jump straight to that.\r\nhttps://github.com/grpc/grpc/releases/tag/v1.13.0\r\n\r\nHere so you can just copy-paste:\r\n```\r\n      name = \"grpc\",\r\n      urls = [\r\n          \"https://mirror.bazel.build/github.com/grpc/grpc/archive/v1.13.0.tar.gz\",\r\n          \"https://github.com/grpc/grpc/archive/v1.13.0.tar.gz\",\r\n      ],\r\n      sha256 = \"50db9cf2221354485eb7c3bd55a4c27190caef7048a2a1a15fbe60a498f98b44\",\r\n      strip_prefix = \"grpc-1.13.0\",\r\n```", "@perfinion Thanks! Updated the PR."]}, {"number": 20512, "title": "Update .gitignore for cmake generated file", "body": "While running cmake in Linux:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh\r\n```\r\n\r\nthe following file is generated and left out:\r\n```\r\nubuntu@ubuntu:~/tensorflow$ git status\r\nOn branch master\r\nYour branch is up-to-date with 'origin/master'.\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n        estimator_api_init_files_list.txt\r\n\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\n```\r\n\r\nThis fix add `/estimator_api_init_files_list.txt`\r\nin gitignore so that it will not be picked by `git add -A`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20511, "title": "How to use pre-train cnn model with tensorflow-go", "body": "Hi all,\r\n\r\nI had train a cnn model with python, it has 57 inputs, one hidden layer with 12nodes, 3 output, and save the model as follow:\r\n\r\n```py\r\n        builder = tf.saved_model.builder.SavedModelBuilder('shield')\r\n        builder.add_meta_graph_and_variables(self.sess, ['login3'])\r\n        builder.save()\r\n```\r\n\r\nthen load it:\r\n\r\n```go\r\n\tmodel, err := tf.LoadSavedModel(\"shield\", []string{\"login3\"}, nil)\r\n```\r\n\r\nbut I got error like:\r\n\r\n```sh\r\n2018-07-03 18:23:14.476427: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { login3 }; from: shield\r\n2018-07-03 18:23:14.481530: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n2018-07-03 18:23:14.486676: I tensorflow/cc/saved_model/loader.cc:161] Restoring SavedModel bundle.\r\n2018-07-03 18:23:14.500925: I tensorflow/cc/saved_model/loader.cc:196] Running LegacyInitOp on SavedModel bundle.\r\n2018-07-03 18:23:14.501650: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { login3 }; Status: success. Took 25441 microseconds.\r\n2 In[0] is not a matrix\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, _output_shapes=[[?,12]], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_0_0, Variable/read)]]\r\n```\r\n\r\nHow to make it work?\r\n\r\nhere is my all code:\r\n\r\n```py\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nclass NeturalNetwork:\r\n\r\n    def __init__(self, model_path, input_nodes, hidden_layer_nodes, output_nodes, learning_rate):\r\n        self.input_nodes = input_nodes\r\n        self.hidden_layer_nodes = hidden_layer_nodes\r\n        self.output_nodes = output_nodes\r\n        self.learning_rate = learning_rate\r\n        self.model_path = model_path\r\n        self.x_data = tf.placeholder(shape=[None, input_nodes], dtype=tf.float32, name=\"input\")\r\n        self.y_target = tf.placeholder(shape=[None, output_nodes], dtype=tf.float32, name=\"target\")\r\n\r\n        l1 = add_layer(self.x_data, input_nodes, self.hidden_layer_nodes, activation_function=tf.nn.relu)\r\n        self.prediction = add_layer(l1, self.hidden_layer_nodes, output_nodes, activation_function=tf.nn.softmax)\r\n\r\n        y_clipped = tf.clip_by_value(self.prediction, 1e-10, 1)\r\n        self.loss = -tf.reduce_mean(tf.reduce_sum(self.y_target * tf.log(y_clipped), axis=1))\r\n\r\n        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\r\n    \r\n        # GO\r\n        infer = tf.argmax(self.prediction, 1, name=\"infer\")\r\n        correct_prediction = tf.equal(infer, tf.argmax(self.y_target, 1))\r\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n        self.saver = tf.train.Saver()\r\n\r\n        self.sess = tf.Session()\r\n        init = tf.global_variables_initializer()\r\n        self.sess.run(init)\r\n\r\n        meta_path = model_path + '/shield.nn.meta'\r\n        if os.path.exists(meta_path):\r\n            nn_path = model_path + '/shield.nn'\r\n            self.saver.restore(self.sess, nn_path)\r\n        else:\r\n            print 'ERROR: no model or restore model fail'\r\n\r\n    def Train(self, epoch, filenames):\r\n        interval = 5\r\n\r\n        x, y = load_dataset(filenames, 'ShieldResult')\r\n        x_values, y_values = shuffle(x, y)\r\n\r\n        x_train = x_values\r\n        y_train = y_values\r\n\r\n        test_size  = 10000\r\n        x_test = x_values[-test_size:]\r\n        y_test = y_values[-test_size:]\r\n\r\n        print('Training the model...', len(x_train))\r\n        for i in range(1, (epoch + 1)):\r\n            self.sess.run(self.optimizer, feed_dict={self.x_data: x_train, self.y_target: y_train})\r\n            if i % interval == 0:\r\n                print('Epoch', i, '|', 'Loss:', self.sess.run(self.loss, feed_dict={self.x_data: x_train, self.y_target: y_train}))\r\n\r\n        print(self.sess.run(self.accuracy, feed_dict={self.x_data: x_test, self.y_target: y_test}))\r\n\r\n        builder = tf.saved_model.builder.SavedModelBuilder('shield')\r\n        builder.add_meta_graph_and_variables(self.sess, ['login3'])\r\n        builder.save()\r\n\r\n        self.saver.save(self.sess, self.model_path + '/shield.nn')\r\n\r\n    def Predict(self, filenames):\r\n        x, y = load_dataset(filenames, 'ShieldResult')\r\n        x_values, y_values = shuffle(x, y)\r\n\r\n        test_size  = 10000\r\n        x_test = x_values[-test_size:]\r\n        y_test = y_values[-test_size:]\r\n\r\n        yes = 0 \r\n        for i in range(len(x_test)):\r\n            pred = np.rint(self.sess.run(self.prediction, feed_dict={self.x_data: [x_test[i]]}))\r\n            if (y_test[i] == pred[0]).all():\r\n                print('=== Actual:', y_test[i], 'Predicted:', pred[0])\r\n                yes = yes + 1\r\n            else:\r\n                print('!!! Actual:', y_test[i], 'Predicted:', pred[0])\r\n\r\n        print(yes)\r\n        print(yes / (len(x_test) * 1.0))\r\n\r\ndef random_seed():\r\n    seed = 1234\r\n    np.random.seed(seed)\r\n    tf.set_random_seed(seed)\r\n\r\ndef load_dataset(filenames, resultField):\r\n    dataset = pd.read_csv(filenames[0])\r\n    dataset = pd.get_dummies(dataset, columns=[resultField]) # One Hot Encoding\r\n    values = list(dataset.columns.values)\r\n\r\n    for i in range(len(filenames)):\r\n        if i == 0:\r\n            continue\r\n\r\n        tmpdataset = pd.read_csv(filenames[i])\r\n        tmpdataset = pd.get_dummies(tmpdataset, columns=[resultField]) # One Hot Encoding\r\n        dataset = dataset.append(tmpdataset)\r\n\r\n    y = dataset[values[-3:]]\r\n    x = dataset[values[0:-3]]\r\n\r\n    return np.array(x, dtype='float32'), np.array(y, dtype='float32')\r\n\r\ndef shuffle(x, y):\r\n    indices = np.random.choice(len(x), len(x), replace=False)\r\n    \r\n    return x[indices], y[indices]\r\n\r\ndef add_layer(inputs, in_size, out_size, activation_function=None):\r\n    weights = tf.Variable(tf.random_normal(shape = [in_size, out_size]))\r\n    biases = tf.Variable(tf.random_normal(shape = [out_size]))\r\n\r\n    w_plus_b = tf.matmul(inputs, weights) + biases\r\n\r\n    if activation_function is None:\r\n        outputs = w_plus_b\r\n    else:\r\n        outputs = activation_function(w_plus_b)\r\n\r\n    return outputs\r\n\r\nif __name__ == \"__main__\":\r\n    epoch = int(sys.argv[1])\r\n    node = int(sys.argv[2])\r\n    rate = float(sys.argv[3])\r\n    csv_num = sys.argv[4]\r\n\r\n    print(epoch)\r\n    print(node)\r\n    print(rate)\r\n    print(csv_num)\r\n\r\n    random_seed()\r\n\r\n    ann = NeturalNetwork('./shield-ai-engine', 57, node, 3, rate)\r\n    csvs = [\r\n        '../data/deal/11_' + csv_num + '.csv', \r\n        '../data/deal/12_' + csv_num + '.csv',\r\n    ]\r\n\r\n    ann.Train(epoch, csvs)\r\n\r\n```\r\n\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n)\r\n\r\nfunc main() {\r\n\tmodel, err := tf.LoadSavedModel(\"shield\", []string{\"login3\"}, nil)\r\n\tif err != nil {\r\n\t\tfmt.Println(1, err)\r\n\t\treturn\r\n\t}\r\n\r\n\tdefer model.Session.Close()\r\n\r\n\toutput := tf.Output{\r\n\t\tOp:    model.Graph.Operation(\"input\"),\r\n\t\tIndex: 0,\r\n\t}\r\n\r\n\ttensor, err := tf.NewTensor([]float32{\r\n\t\t12, 41, 41, 41, 41, 41, 70, 137, 259, 305, 344, 81, 148, 271, 370, 397, 81, 148, 271, 370, 397, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 122, 233, 295, 391, 1, 58, 195, 1, 6, 4, 195, 1, 6, 4, 165, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\r\n\t})\r\n\r\n\ttarget, err := tf.NewTensor([]float32{})\r\n\r\n\tif err != nil {\r\n\t\tfmt.Println(3, err)\r\n\t}\r\n\r\n\tfeeds := map[tf.Output]*tf.Tensor{\r\n\t\toutput: tensor,\r\n\t\tmodel.Graph.Operation(\"target\").Output(0): target,\r\n\t}\r\n\r\n\tfetches := []tf.Output{\r\n\t\t{\r\n\t\t\tOp:    model.Graph.Operation(\"infer\"),\r\n\t\t\tIndex: 0,\r\n\t\t},\r\n\t}\r\n\r\n\tresult, err := model.Session.Run(\r\n\t\tfeeds,\r\n\t\tfetches,\r\n\t\tnil,\r\n\t)\r\n\r\n\tif err != nil {\r\n\t\tfmt.Println(2, err)\r\n\t\treturn\r\n\t}\r\n\r\n\tfmt.Println(result[0].Value())\r\n}\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler  OK, thanks for  your reply.\r\n\r\nmy env:\r\n\r\nOS: mac 10.11.6\r\npython version: python2.7\r\ntensorflow install base on https://www.tensorflow.org/install/install_go\r\ntensorflow-go version: v1.9.0-rc2\r\ngo version: go1.9.2 darwin/amd64\r\n\r\ncommand to reproduce:\r\n\r\n```sh\r\npython ann.py 500 12 0.02 12\r\n```\r\n\r\nto train and save model\r\n\r\n```sh \r\ngo run main.go\r\n```\r\n\r\nto use save in go.\r\n\r\n\r\nmy dataset like:\r\n\r\n```\r\nEvent,DeviceIDPer5Min,DeviceIDPer10Min,DeviceIDPer20Min,DeviceIDPerHour,DeviceIDPerDay,IPPer5Min,IPPer10Min,IPPer20Min,IPPerHour,IPPerDay,AccountPer5Min,AccountPer10Min,AccountPer20Min,AccountPerHour,AccountPerDay,UserIDPer5Min,UserIDPer10Min,UserIDPer20Min,UserIDPerHour,UserIDPerDay,MobilePer5Min,MobilePer10Min,MobilePer20Min,MobilePerHour,MobilePerDay,EmailPer5Min,EmailPer10Min,EmailPer20Min,EmailPerHour,EmailPerDay,FingerprintPer5Min,FingerprintPer10Min,FingerprintPer20Min,FingerprintPerHour,FingerprintPerDay,DeviceIDIPs,IPDeviceIDs,AccountDeviceIDs,DeviceIDAccounts,AccountIPs,IPAccounts,UserIDDeviceIDs,DeviceIDUserID,UserIDIPs,IPUserIDs,FingerprintExistSeconds,MobileRiskScore,ReviewPassed,IsCommonDevice,IsDeviceIDLocked,IsIPLocked,IsLeakedAccount,IsUserIDLocked,IsForeignIP,IsMobileLocked,IsRiskAccount,ShieldResult\r\n12,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n12,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0\r\n12,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,5,5,5,5,5,1,1,2,1,2,1,2,1,2,1,323008,0,1,0,0,0,1,0,0,0,0,0\r\n12,1,1,1,3,10,1,1,1,3,11,1,1,1,1,2,1,1,1,1,2,0,0,0,0,0,0,0,0,0,0,1,1,1,3,13,1,3,2,5,1,5,2,5,1,5,173913,0,0,0,0,0,0,0,0,0,0,0\r\n12,1,1,1,1,1,7,45,45,45,45,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,27,2,1,2,27,3,1,3,27,3,0,0,0,0,0,0,0,0,0,0,0\r\n12,1,1,1,1,1,1,1,1,1,135,1,1,1,1,8,1,1,1,1,8,0,0,0,0,0,0,0,0,0,0,3,4,11,32,1040,1,225,31,1,3,16,31,1,3,16,20,0,0,0,0,0,0,0,0,0,0,0\r\n12,11,11,11,11,11,13,41,96,514,5507,13,41,96,514,3112,13,41,96,514,3112,0,0,0,0,0,0,0,0,0,0,11,33,73,401,4922,1,620,295,1,1,9,295,1,1,8,32,0,0,0,0,0,0,0,1,0,0,2\r\n12,12,12,12,12,12,14,42,97,515,5508,14,42,97,515,3113,14,42,97,515,3113,0,0,0,0,0,0,0,0,0,0,12,34,74,402,4923,1,620,295,1,1,9,295,1,1,8,32,0,0,0,0,0,0,0,1,0,0,2\r\n12,1,1,1,1,1,8,46,46,46,46,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,28,1,1,1,28,1,1,1,28,3,0,0,0,0,0,1,0,0,0,0,1\r\n12,1,1,1,1,1,9,47,47,47,47,3,3,3,3,3,3,3,3,3,3,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,29,2,1,2,29,2,1,2,29,4,0,0,0,0,0,1,0,0,0,0,1\r\n```\r\n\r\nis there any detail doc for go using pre-train model?  ", "The general idea you have (of creating a `SavedModel` in Python and then using that in Go) seems right.\r\n\r\nThe error message is trying to say that it expected a matrix (a 2D tensor) to be provided to the `MatMul` operation, but it received something else.\r\n\r\nFrom a cursory look at the provided code, the Python code is written so that `input` is a 2-D tensor (with shape `[None, input_nodes]`), but the Go code is feeding a 1D tensor (`[]float32`, which can be seen also by `fmt.Println(len(tensor.Shape())`).\r\n\r\nI suspect things will work out fine if `tensor` in Go is a 2D tensor (a `[][input_nodes]float32`)\r\n\r\nLet us know if that works. Thanks.\r\n", "@asimshankar Thanks!\r\n\r\nI had change:\r\n\r\n```golang\r\n\ttensor, err := tf.NewTensor([]float32{\r\n\t\t12, 41, 41, 41, 41, 41, 70, 137, 259, 305, 344, 81, 148, 271, 370, 397, 81, 148, 271, 370, 397, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 122, 233, 295, 391, 1, 58, 195, 1, 6, 4, 195, 1, 6, 4, 165, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\r\n\t})\r\n\r\n\ttarget, err := tf.NewTensor([]float32{})\r\n```\r\n\r\nto \r\n\r\n```golang\r\n\ttensor, err := tf.NewTensor([][]float32{{\r\n\t\t12, 41, 41, 41, 41, 41, 70, 137, 259, 305, 344, 81, 148, 271, 370, 397, 81, 148, 271, 370, 397, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 122, 233, 295, 391, 1, 58, 195, 1, 6, 4, 195, 1, 6, 4, 165, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\r\n\t}})\r\n\r\n\ttarget, err := tf.NewTensor([][]float32{})\r\n```\r\n\r\nit works.", "@qianlnk : Great, closing this out then."]}, {"number": 20510, "title": "Memory Leak", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7\r\n- **TensorFlow installed from (source or binary)**:\r\nAnaconda\r\n- **TensorFlow version (use command below)**:\r\nb'unknown' 1.8.0\r\nCuDNN - None\r\nGPU-Integrated\r\n### Describe the problem\r\nMemory Leak when running my code \r\n\r\n### Source code / logs\r\n`import numpy as np`\r\n`import os`\r\n`import tensorflow as tf`\r\n`tf.enable_eager_execution()`\r\n`import tensorflow.contrib.eager as tfe`\r\n`import csv as csv`\r\n\r\n`import matplotlib.pyplot as plt`\r\n`from tensorflow.contrib.opt.python.training.elastic_average_optimizer import *`\r\n`import keras as keras`\r\n\r\n\r\n\r\n`def parse_csv(line):`\r\n`  example_defaults = [[0.], [0.], [0.], [0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.],[0.], [0]] # sets field types`\r\n  \r\n`  parsed_line = tf.decode_csv(line, example_defaults)`\r\n  \r\n`  # First 4 fields are features, combine into single tensor`\r\n`  features = tf.reshape(parsed_line[:-1], shape=(50,))`\r\n`  # Last field is the label`\r\n`  label = tf.reshape(parsed_line[-1], shape=())`\r\n`  return features, label`\r\n   \r\n`train_dataset = tf.data.TextLineDataset(fileid + \"train\")`\r\n`train_dataset = train_dataset.map(parse_csv)`\r\n`train_dataset = train_dataset.shuffle(buffer_size=1000)`\r\n`train_dataset = train_dataset.batch(16)`\r\n\r\n`model = tf.keras.Sequential([`\r\n ` tf.keras.layers.Dense(100, input_shape=(50,)),  # input shape required`\r\n ` tf.keras.layers.Dense(2000),`\r\n`  tf.keras.layers.Dense(2000),`\r\n`  tf.keras.layers.Dense(200,activation = 'sigmoid'),`\r\n`  tf.keras.layers.Dense(200),`\r\n ` tf.keras.layers.Dense(2000),`\r\n  `tf.keras.layers.Dense(200),`\r\n ` tf.keras.layers.Dense(2)`\r\n  \r\n`])`\r\n`print (model.summary)`\r\n\r\n\r\n`def loss(model, x, y):`\r\n`  y_ = model(x)`\r\n`  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)`\r\n\r\n`def grad(model, inputs, targets):`\r\n\r\n`    with tf.GradientTape() as tape:`\r\n    \r\n `       loss_value = loss(model, inputs, targets)`\r\n  \r\n `   return tape.gradient(loss_value, model.variables)    `\r\n\r\n`optimizer = tf.train.AdamOptimizer(learning_rate = 0.0010)`\r\n`train_loss_results = []`\r\n`train_accuracy_results = []`\r\n`test_loss_results = []`\r\n`test_accuracy_results = []`\r\n`num_epochs = 2000`\r\n\r\n`for epoch in range(num_epochs):`\r\n`  epoch_loss_avg = tfe.metrics.Mean()`\r\n`  epoch_accuracy = tfe.metrics.Accuracy()`\r\n  \r\n\r\n`  train_dataset = train_dataset.shuffle(16)`\r\n` #  Training loop - using batches of 32`\r\n` for x, y in train_dataset:`\r\n`  #   Optimize the model`\r\n  `  grads = grad(model, x, y)`\r\n  `  optimizer.apply_gradients(zip(grads, model.variables),`\r\n           `                   global_step=tf.train.get_or_create_global_step())`\r\n\r\n`  #   Track progress`\r\n`    epoch_loss_avg(loss(model, x, y))  # add current batch loss`\r\n`     #compare predicted label to actual label`\r\n`    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)`\r\n \r\n`  # end epoch`\r\n`  train_loss_results.append(epoch_loss_avg.result())`\r\n`  train_accuracy_results.append(epoch_accuracy.result())`\r\n  \r\n` if epoch % 1 == 0:\r\n    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\r\n                                                                epoch_loss_avg.result(),\r\n                                                                epoch_accuracy.result()))`\r\n  \r\n`  test_dataset = tf.data.TextLineDataset(fileid + \"test\")\r\n  test_dataset = test_dataset.map(parse_csv)\r\n  test_dataset = test_dataset.shuffle(buffer_size=1000)\r\n  test_dataset = test_dataset.batch(32)`\r\n  \r\n ` test_accuracy = tfe.metrics.Accuracy()`\r\n  \r\n ` for (x, y) in test_dataset:\r\n      prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\r\n      test_accuracy(prediction, y)`\r\n      \r\n`  print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\r\n  train_dataset = tf.data.TextLineDataset(fileid + \"train\")\r\n  train_dataset = train_dataset.map(parse_csv)\r\n  train_dataset = train_dataset.batch(32)`\r\n  \r\n\r\n\r\n`del optimizer`\r\n\r\n`test_dataset = tf.data.TextLineDataset(fileid + \"test\")`\r\n`test_dataset = test_dataset.map(parse_csv)`\r\n`test_dataset = test_dataset.shuffle(buffer_size=1000)`\r\n`test_dataset = test_dataset.batch(32)`\r\n\r\n`test_accuracy = tfe.metrics.Accuracy()`\r\n\r\n`for (x, y) in test_dataset:`\r\n ` prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)`\r\n`  test_accuracy(prediction, y)`\r\n\r\n`print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))`\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Done", "Can you fix the formatting of your code? It's very hard to read right now... you can create code blocks using ```, see https://help.github.com/articles/creating-and-highlighting-code-blocks/", "done", "How do you know there's a memory leak? Can you try to narrow down where in your code the leak occurs?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20509, "title": "Cannot use tf.metrics with MirroredStrategy", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: using nvidia containter: https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.06.html#rel_18.06 \r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0.176\r\n- **GPU model and memory**: nvidia tesla v100\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n\r\n### Describe the problem\r\nI'm training a network using a custom tf.estimator. To monitor its training i used several metrics from tf. metrics: accuracy, auc, true positives, ... When running this on a single GPU, This works as expected, however when using `tf.contrib.distribute.MirroredStrategy` i get an exception. Apparently all metrics suffer from the same bug (tested by iteratively leaving the metric out).\r\n\r\n### Source code / logs\r\nI tried to extract the relevant part from my code:\r\n```\r\ndef configure_trainer(self):\r\n        if self.configuration.get(\"multi_gpu\"):\r\n            distribute = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\n        else:\r\n            distribute = None\r\n\r\n        run_config = tf.estimator.RunConfig(model_dir=out_dir, tf_random_seed=self.configuration.get(\"random_seed\"),\r\n                                                save_summary_steps=400,\r\n                                                save_checkpoints_steps=1000,\r\n                                                log_step_count_stepss400,\r\n                                                train_distribute=distribute)\r\n\r\n        self.trainer = tf.estimator.Estimator(\r\n            model_fn=self.get_model_fn,  \r\n            config=run_config\r\n        )\r\n\r\ndef get_model_fn(self, mode, features, labels, params):\r\n        self.configure_network(input_tensor=features, output_tensor=labels, mode=mode)\r\n        eval_metrics = self.get_accuracy(self.network.get_output_tensor(),labels)\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=self.predictions,\r\n                                              loss=self.loss, train_op=self.trainer_conf.get_train_op_fn(self.loss,mode),\r\n                                              eval_metric_ops=eval_metrics\r\n                                              )\r\n...\r\ndef get_accuracy(self, output_tensor,  ground_truth, name=\"\"):\r\n        metric_name = 'Accuracy_' + name\r\n        accuracy = tf.metrics.accuracy(\r\n                labels=ground_truth,\r\n                predictions=tf.argmax(output_tensor, axis=-1),\r\n                name=metric_name)\r\n        tf.summary.scalar(metric_name , accuracy[1])\r\n        return {\r\n            metric_name : accuracy\r\n        }\r\n```\r\n\r\nThis results in the following error trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 27, in <module>\r\n    experimenter.run_training_experiment(config)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 39, in run_training_experiment\r\n    tf.estimator.train_and_evaluate(self.trainer, self.training_specs, self.eval_specs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 439, in train_and_evaluate\r\n    executor.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 518, in run\r\n    self.run_local()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 650, in run_local\r\n    hooks=train_hooks)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 977, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5265, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5060, in get_controller\r\n    yield default\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5265, in get_controller\r\n    yield g\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 977, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 304, in __exit__\r\n    self._var_creator_scope.__exit__(exception_type, exception_value, traceback)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2283, in variable_creator_scope\r\n    yield\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2939, in _variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2283, in variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 756, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 466, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 295, in _mode\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 466, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 514, in device_policy\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 466, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5265, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5060, in get_controller\r\n    yield default\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5265, in get_controller\r\n    yield g\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 466, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4338, in device\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 466, in run\r\n    self.done = True\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5991, in __exit__\r\n    self._name_scope.__exit__(type_arg, value_arg, traceback_arg)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4115, in name_scope\r\n    yield \"\" if new_stack is None else new_stack + \"/\"\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 466, in run\r\n    self.done = True\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2097, in __exit__\r\n    self._graph_context_manager.__exit__(type_arg, value_arg, traceback_arg)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5265, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5060, in get_controller\r\n    yield default\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5265, in get_controller\r\n    yield g\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 466, in run\r\n    self.done = True\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2283, in variable_creator_scope\r\n    yield\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2939, in _variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2283, in variable_creator_scope\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 465, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 831, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 116, in get_model_fn\r\n    eval_metrics = self.logger.get_eval_metrics(self.predictions, labels)\r\n  File \"/media/local/myfiles/Mytraininglogger.py\", line 26, in get_eval_metrics\r\n    metrics.update(self.get_accuracy(output_tensor, ground_truth, name=name))\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/traininglogger.py\", line 60, in get_accuracy\r\n    name=metric_name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py\", line 409, in accuracy\r\n    name or 'accuracy')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py\", line 345, in mean\r\n    return mean_t, update_op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2095, in __exit__\r\n    self._current_name_scope.__exit__(type_arg, value_arg, traceback_arg)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5991, in __exit__\r\n    self._name_scope.__exit__(type_arg, value_arg, traceback_arg)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4115, in name_scope\r\n    yield \"\" if new_stack is None else new_stack + \"/\"\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/metrics_impl.py\", line 332, in mean\r\n    update_total_op = state_ops.assign_add(total, math_ops.reduce_sum(values))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 251, in assign_add\r\n    return ref.assign_add(value)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 311, in assign_add\r\n    return self.get(device=_get_update_device()).assign_add(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 283, in _get_update_device\r\n    \"Use DistributionStrategy.update() to modify a MirroredVariable.\")\r\nRuntimeError: Use DistributionStrategy.update() to modify a MirroredVariable.\r\nException ignored in: <generator object get_controller at 0x7f2dec1a2fc0>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5267, in get_controller\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 136, in pop\r\nIndexError: pop from empty list\r\nException ignored in: <generator object get_controller at 0x7f2dec1f0830>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5267, in get_controller\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 136, in pop\r\nIndexError: pop from empty list\r\nException ignored in: <generator object get_controller at 0x7f2dec17d9e8>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 5267, in get_controller\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/context.py\", line 136, in pop\r\nIndexError: pop from empty list\r\nMakefile:19: recipe for target 'train_gpu' failed\r\nmake: *** [train_gpu] Error 1\r\n```", "comments": ["@jdvylder - Metrics support in distribution strategies was added about a month ago and was not part of the tensorflow 1.8 release. It should be part of the nightly build though. Would it possible for you to try your code with tensorflow nightly build? (https://pypi.org/project/tf-nightly-gpu/) ", "Hi @guptapriya \r\nThanks for the reply. I've tried the nightly build and the training does seem to get past the point where it originally threw the exception. Since I've run into a new [bug](https://github.com/tensorflow/tensorflow/issues/20874), I can't test if the metrics work, but will close this issue since the initial exception seems to be solved.\r\n\r\nThanks\r\n\r\nJonas", "This is still an issue. Can this be reopened?\r\n\r\n```\r\ntensorflow/python/ops/metrics_impl.py in mean(values, weights, metrics_collections, updates_collections, name)\r\n    374       return mean_t\r\n    375 \r\n--> 376     mean_t = distribute_lib.get_tower_context().merge_call(\r\n    377         aggregate_across_towers, total, count)\r\n    378     update_op = _safe_div(update_total_op, update_count_op, 'update_op')\r\n\r\nAttributeError: 'NoneType' object has no attribute 'merge_call'\r\n```", "@carlthome - it seems like your might be seeing this error if you're trying to update metrics in cross tower mode. Typically, we would expect metrics to be computed in tower mode, as that's where your labels and logits etc are available. Could you provide a complete stack trace + your code that triggers this so we can debug further?  \r\n\r\n\r\n", "This seems to reproduce the error consistently:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef input_fn():\r\n    dataset = (\r\n        tf.data.Dataset\r\n        .range(100)\r\n        .map(lambda x: tf.random_normal([100]))\r\n        .batch(32)\r\n        .map(lambda x: ({'input': x}, {'output': x}))\r\n        .repeat()\r\n    )\r\n    return dataset\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    units = features['input'].shape[-1]\r\n    predictions = tf.layers.dense(features['input'], units)\r\n    labels = labels['output']\r\n\r\n    loss = tf.losses.mean_squared_error(labels, predictions)\r\n    tf.losses.add_loss(loss)\r\n    loss = tf.losses.get_total_loss()\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        metrics = {'mse': tf.metrics.mean_squared_error(labels, predictions)}\r\n\r\n        return tf.estimator.EstimatorSpec(\r\n            mode,\r\n            loss=loss,\r\n            eval_metric_ops=metrics,\r\n        )\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        step = tf.train.get_or_create_global_step()\r\n        optimizer = tf.train.AdamOptimizer()\r\n        train_op = optimizer.minimize(loss, step)\r\n\r\n        return tf.estimator.EstimatorSpec(\r\n            mode,\r\n            loss=loss,\r\n            train_op=train_op,\r\n        )\r\n\r\n\r\nestimator = tf.estimator.Estimator(\r\n    model_fn,\r\n    config=tf.estimator.RunConfig(\r\n        save_checkpoints_steps=100,\r\n        train_distribute=tf.contrib.distribute.MirroredStrategy()),\r\n)\r\n\r\ntf.estimator.train_and_evaluate(\r\n    estimator,\r\n    train_spec=tf.estimator.TrainSpec(input_fn),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn),\r\n)\r\n```\r\n\r\nAm I doing something wrong? All GPUs are utilized during training but this stack trace pops up upon evaluation:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b27091873f02> in <module>()\r\n     56     estimator,\r\n     57     train_spec=tf.estimator.TrainSpec(input_fn),\r\n---> 58     eval_spec=tf.estimator.EvalSpec(input_fn),\r\n     59 )\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    449         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    450 \r\n--> 451   return executor.run()\r\n    452 \r\n    453 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run(self)\r\n    588         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    589       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 590       return self.run_local()\r\n    591 \r\n    592     # Distributed case.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run_local(self)\r\n    689         max_steps=self._train_spec.max_steps,\r\n    690         hooks=train_hooks,\r\n--> 691         saving_listeners=saving_listeners)\r\n    692 \r\n    693     eval_result = listener_for_eval.eval_result or _EvalResult(\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    374 \r\n    375       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 376       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    377       logging.info('Loss for final step: %s.', loss)\r\n    378       return self\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1141   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1142     if self._distribution:\r\n-> 1143       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1144     else:\r\n   1145       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1366         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1367                                                hooks, global_step_tensor,\r\n-> 1368                                                saving_listeners)\r\n   1369 \r\n   1370   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n   1449       loss = None\r\n   1450       while not mon_sess.should_stop():\r\n-> 1451         _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n   1452     return loss\r\n   1453 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    581                           feed_dict=feed_dict,\r\n    582                           options=options,\r\n--> 583                           run_metadata=run_metadata)\r\n    584 \r\n    585   def run_step_fn(self, step_fn):\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1057                               feed_dict=feed_dict,\r\n   1058                               options=options,\r\n-> 1059                               run_metadata=run_metadata)\r\n   1060       except _PREEMPTION_ERRORS as e:\r\n   1061         logging.info('An error was raised. This may be due to a preemption in '\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1148         raise six.reraise(*original_exc_info)\r\n   1149       else:\r\n-> 1150         raise six.reraise(*original_exc_info)\r\n   1151 \r\n   1152 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1133   def run(self, *args, **kwargs):\r\n   1134     try:\r\n-> 1135       return self._sess.run(*args, **kwargs)\r\n   1136     except _PREEMPTION_ERRORS:\r\n   1137       raise\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1213               results=outputs[hook] if hook in outputs else None,\r\n   1214               options=options,\r\n-> 1215               run_metadata=run_metadata))\r\n   1216     self._should_stop = self._should_stop or run_context.stop_requested\r\n   1217 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in after_run(self, run_context, run_values)\r\n    462       if self._timer.should_trigger_for_step(global_step):\r\n    463         self._timer.update_last_triggered_step(global_step)\r\n--> 464         if self._save(run_context.session, global_step):\r\n    465           run_context.request_stop()\r\n    466 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in _save(self, session, step)\r\n    487     should_stop = False\r\n    488     for l in self._listeners:\r\n--> 489       if l.after_save(session, step):\r\n    490         logging.info(\r\n    491             \"A CheckpointSaverListener requested that training be stopped. \"\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in after_save(***failed resolving arguments***)\r\n    495       return True\r\n    496     if self._timer.should_trigger_for_step(global_step_value):\r\n--> 497       self._evaluate(global_step_value)  # updates self.eval_result\r\n    498       if not self._continuous_eval_listener.after_eval(self.eval_result):\r\n    499         logging.info('Exiting evaluation, as requested by '\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in _evaluate(self, global_step_value)\r\n    515     self._timer.update_last_triggered_step(global_step_value)\r\n    516     self.eval_result, self.export_results = (\r\n--> 517         self._evaluator.evaluate_and_export())\r\n    518     if self.eval_result.status != _EvalStatus.EVALUATED:\r\n    519       #  This is unexpected; should never happen.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in evaluate_and_export(self)\r\n    882           name=self._eval_spec.name,\r\n    883           checkpoint_path=latest_ckpt_path,\r\n--> 884           hooks=self._eval_spec.hooks)\r\n    885 \r\n    886       # _EvalResult validates the metrics.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in evaluate(self, input_fn, steps, hooks, checkpoint_path, name)\r\n    461         (scaffold, update_op,\r\n    462          eval_dict, all_hooks) = self._evaluate_build_graph(\r\n--> 463              input_fn, hooks, checkpoint_path)\r\n    464         return self._evaluate_run(\r\n    465             checkpoint_path=checkpoint_path,\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _evaluate_build_graph(self, input_fn, hooks, checkpoint_path)\r\n   1461                                                     model_fn_lib.ModeKeys.EVAL))\r\n   1462     estimator_spec = self._call_model_fn(\r\n-> 1463         features, labels, model_fn_lib.ModeKeys.EVAL, self.config)\r\n   1464 \r\n   1465     # Call to warm_start has to be after model_fn is called.\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1131 \r\n   1132     logging.info('Calling model_fn.')\r\n-> 1133     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1134     logging.info('Done calling model_fn.')\r\n   1135 \r\n\r\n<ipython-input-2-b27091873f02> in model_fn(features, labels, mode, params)\r\n     26 \r\n     27     if mode == tf.estimator.ModeKeys.EVAL:\r\n---> 28         metrics = {'mse': tf.metrics.mean_squared_error(labels, predictions)}\r\n     29 \r\n     30         return tf.estimator.EstimatorSpec(\r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in mean_squared_error(labels, predictions, weights, metrics_collections, updates_collections, name)\r\n   1297   squared_error = math_ops.square(labels - predictions)\r\n   1298   return mean(squared_error, weights, metrics_collections, updates_collections,\r\n-> 1299               name or 'mean_squared_error')\r\n   1300 \r\n   1301 \r\n\r\n~/.miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in mean(values, weights, metrics_collections, updates_collections, name)\r\n    374       return mean_t\r\n    375 \r\n--> 376     mean_t = distribute_lib.get_tower_context().merge_call(\r\n    377         aggregate_across_towers, total, count)\r\n    378     update_op = _safe_div(update_total_op, update_count_op, 'update_op')\r\n\r\nAttributeError: 'NoneType' object has no attribute 'merge_call'\r\n```", "@carlthome thanks for the code and stack trace. I think what's happening is that as part of `train_and_evaluate`, evaluate gets called in a hook (_NewCheckpointListenerForEvaluate) as part of the training call. This means that the distribution strategy scope that we open in estimator.train (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L1194)  is still open when we execute this hook.  \r\nDue to this, the metrics that you create in evaluate are created inside the scope in a cross tower context - and this leads to the error above. \r\nThis setup to use _NewCheckpointListenerForEvaluate is new and hence you're probably one of the first people to run into this. \r\n\r\nI see 2 options to move forward:\r\n1. Can you use Estimator.train and Estimator.evaluate directly instead of train_and_evaluate? We've definitely tested those to work as expected in presence of MirroredStrategy (even though evaluate itself is not distributed). The main benefit of using train_and_evaluate (from my understanding) is to run on multiple machines with Parameter Server distribution. Since you are not looking for that, directly using train and evaluate might be better and get around this error. \r\n2. As I mentioned above, Estimator.evaluate itself in general is not yet distributed under MirroredStrategy so currently it would just run on the first GPU (but should not throw errors). This support is actually in progress right now and should be available in 1-2 weeks. I suspect this might address the issue while using train_and_evaluate as well but I haven't tested that (will do). \r\n\r\nHope this helps! \r\n\r\n\r\n\r\n", "Thanks for the clarification @guptapriya!\r\n\r\nI'm using `train_and_evaluate` because I'd like to do [early stopping](https://en.wikipedia.org/wiki/Early_stopping) (checking loss on a validation set every now and then during training, and stopping the training loop when the model stops improving on the validation data). I'm currently doing this with `tf.contrib.estimator.stop_if_no_decrease_hook`. This also crashes with the same stack trace:\r\n```py\r\neval_hook = tf.contrib.estimator.InMemoryEvaluatorHook(estimator, validation_input_fn)\r\nestimator.train(training_input_fn, hooks=[early_stopping, eval_hook])\r\nmetrics = estimator.evaluate(test_input_fn)\r\n```\r\n\r\nI guess this is a workaround at the moment for local training:\r\n```py\r\nbest = float('inf')\r\nfor epoch in range(100):\r\n    estimator.train(training_input_fn)\r\n    metrics = estimator.evaluate(validation_input_fn)\r\n    if metrics['loss'] < best:\r\n        best = metrics['loss']\r\n    else:\r\n        tf.logging.info(f'Early stopping after {epoch} epochs.')\r\n        break\r\n```\r\nbut it would be nice to have asynchronous training and validation loops via some high-level construct like `train_and_evaluate` eventually! The dream would be to write high-level TensorFlow code once, and have that working in any hardware configuration (multi-machine, single-machine, multi-GPU, CPU-only, etc.).", "@carlthome absolutely, i am looking into fixing this issue, and also implementing distributed eval directly, both of which should address your issue. Your workaround in the meantime looks right.", "Also, please feel free to open a new issue for this with your code and stack trace. Thanks! ", "@guptapriya done! https://github.com/tensorflow/tensorflow/issues/21180"]}]