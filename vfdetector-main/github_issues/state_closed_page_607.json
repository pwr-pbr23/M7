[{"number": 35455, "title": "committing updated docs", "body": "", "comments": ["Also, I think the `id` of the Tensor should be removed:\r\n\r\n```\r\nExpected:\r\n    <tf.Tensor: id=345, shape=(), dtype=float32, numpy=1.55>\r\nGot:\r\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.55>\r\n```", "I noticed a file caused the check to fail so I fixed it. Please approve again if needed.", "I fixed the other failed doc tests that I did not see last time. Please approve again.", "@William-Yin123 Can you please resolve conflicts? Thanks!", "I resolved the conflicts. Can you approve again.", "@William-Yin123  Can you please resolve conflicts? Thanks!", "I do not know what is causing these errors. I would appreciate it if someone could help me.", "Can you rebase on master please?", "@William-Yin123 Can you please check mihaimaruseac's comments and keep us posted? Thanks!"]}, {"number": 35454, "title": "HashTableV2, LookupTableFindV2 are not supported", "body": "**System information**\r\n- Linux CentOs 7\r\n- TensorFlow installed from pip:\r\n- TensorFlow version 1.15.0:\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, FULLY_CONNECTED, LOGISTIC, MUL, ONE_HOT, PACK, RESHAPE, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, SUB, SUM. Here is a list of operators for which you will need custom implementations: HashTableV2, LookupTableFindV2.```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nlooks like the above two operators are not supported. We are using Estimator APIs and I think the above ops are used in categorical_column where we need to lookup the feature from a vocabulary list.\r\n", "comments": ["@wenyun ,\r\nHello, as per the [official documentation](https://www.tensorflow.org/lite/guide/ops_compatibility#unsupported_operations) TensorFlow Lite operations HashTableV2, LookupTableFindV2  are present but not ready for custom models yet.", "@wenyun,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35453, "title": "Metrics with Eager Execution", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda package\r\n- TensorFlow version (use command below): TF2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.6.4\r\n- GPU model and memory: Tesla V100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\ntf.compat.v1.metrics does not support eager execution, i.e. there is no way to log metrics with TF2.0 at this point. Is there an alternate I should be looking at or is the feature underway?", "comments": []}, {"number": 35452, "title": "which is the correct type for debugging.assert_shape?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes?version=stable\r\n\r\n## Description of issue (what needs changing):\r\n\r\nUpdate Documentation or implementation of `tf.debugging.assert_shapes`.     \r\n`Shapes` in its argument requires LIST OF TUPLE, not DICTIONARIES.\r\n\r\n### Clear description\r\n\r\nYour Documentation (Python 4's syntax?)\r\n```python\r\nx = tf.random.normal([128, 32, 32, 1])\r\ntf.debugging.assert_shapes(\r\n    [(x: (128, 32, 32, 1))]\r\n)\r\n# => \r\n#     [(x: (128, 32, 32, 1))]\r\n#       ^\r\n# SyntaxError: invalid syntax\r\n```\r\n\r\nWith dictionary (Python 3's syntax)\r\n```python\r\ntf.debugging.assert_shapes(\r\n    {x: (128, 32, 32, 1)}\r\n)\r\n# =>\r\n# ... ... Traceback (most recent call last):\r\n#   File \"<stdin>\", line 2, in <module>\r\n#  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 713, in __hash__\r\n#     raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\n# TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n```\r\nWith list of tuple (python 3's syntax)(NOT DICTIONARIES)\r\n```python\r\ntf.debugging.assert_shapes([\r\n    (x, (128, 32, 32, 1))\r\n]\r\n)\r\n# => nothing (correct)\r\n```\r\n", "comments": ["I have tried on colab with TF version 1.15 and i am not seeing any issue with dictionary  and with list of tuple (python 3's syntax) . I have tried in colab with TF 2.0 and i am able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/d75c8e3a0d3121bace78789f88b03b6a/untitled512.ipynb) Thanks!", "OK, I found we can use LIST OF TUPLE and also can use DICTIONARY at tf_1.15 (but its document is for tf_2.0 ...).\r\nIt's the problem of the document's syntax error.\r\nThanks!"]}, {"number": 35451, "title": "Error while trying to serilize Image Captioning keras model '_UserObject' object is not callable'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nexample from https://www.tensorflow.org/tutorials/text/image_captioning\r\n\r\n- OS Platform and Distribution:Linux Ubuntu 18\r\n- TensorFlow installed from (source or binary): bin\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0 \r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n> tf.saved_model.save(encoder, \"./models/1/encoder\")\r\n\r\nmodel get serialized with errors \r\n\r\n**Describe the expected behavior**\r\nencoder can be loaded without errors\r\n\r\n**Code to reproduce the issue**\r\n- train the model from https://www.tensorflow.org/tutorials/text/image_captioning example \r\n\r\n```\r\ntf.saved_model.save(encoder, \"./models/1/encoder\")\r\nencoder = tf.saved_model.load(\"./models/1/encoder\")\r\nencoder(img_tensor_val)\r\n\r\n```\r\nraises the error > \r\n'_UserObject' object is not callable\r\n\r\n\r\n`encoder.fc(img_tensor_val) \r\n`\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"inputs:0\", shape=(1, 120, 64), dtype=float32)\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 0 option(s):\r\n\r\n**Other info / logs**\r\nW1227 00:41:32.831883 139767989909312 save_impl.py:77] Skipping full serialization of Keras model <__main__.CNN_Encoder object at 0x7f1daf5354e0>, because its inputs are not defined.\r\n", "comments": ["model can be loaded from the checkpoint at the same time", "@veonua ,\r\nCould you please provide complete code to reproduce the issue? Thanks!", "should I just copy the example code to collab ?\n\nOn Mon, Dec 30, 2019, 09:06 oanush <notifications@github.com> wrote:\n\n> @veonua <https://github.com/veonua> ,\n> Could you please provide complete code to reproduce the issue? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35451?email_source=notifications&email_token=AALYALWCLRRB3KEBGIYTW63Q3GTYVA5CNFSM4KACUQMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHZZAUI#issuecomment-569610321>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALYALX3TMIXVNBXSQ4DUA3Q3GTYVANCNFSM4KACUQMA>\n> .\n>\n", "@veonua ,\r\nHi,yes please also try executing and share the error being faced by you. Thank you!", "https://colab.research.google.com/drive/1Ch3w8-75YBS3uvkeNxN36heAnznWGJdf\r\n\r\nit looks like it is possible to serialize encoder in TF 2.1.\r\nbut I am having an issue with the decoder still\r\n\r\ntf.saved_model.save(decoder, \"/tmp/model\")\r\n> call() missing 2 required positional arguments: 'features' and 'hidden'\r\n", "@veonua In the line\r\n`tf.saved_model.save(decoder, \"/tmp/model\",signatures=dncoder.call)`\r\n\r\nIn the `signatures=dncoder.call`, pass `signatures=decoder.call`\r\n\r\nAlso if you have any more question as this question is purely a support question and not related to  bug/performance, feature request, build/install or docs related issues, please post it in stack overflow.", "@gowthamkpr have you tried your answer or just wanted to close the issue?\r\nI would appreciate it if you spend another minute and open the collab link I have created.\r\n\r\nthe issue is related to the official TF example. So it would be strange to ask help on Stack Overflow. ", "@veonua As mentioned in my comment in the 2nd line, there us a **spelling mistake** in your code i.e., insted of using `signatures=decoder.call` you are using `signatures=dncoder.call` Please correct that and let me know if the example works.", "have you opened the collab link I have provided?\n\nmisspelling was corrected months ago\n\nOn Tue, Feb 4, 2020, 23:03 gowthamkpr <notifications@github.com> wrote:\n\n> @veonua <https://github.com/veonua> As mentioned in my comment in the 2nd\n> line, there us a *spelling mistake* in your code i.e., insted of using\n> signatures=decoder.call you are using signatures=dncoder.call Please\n> correct that and let me know if the example works.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35451?email_source=notifications&email_token=AALYALRODOAU77XJOATGWDDRBHQ23A5CNFSM4KACUQMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKZLO3I#issuecomment-582137709>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALYALVXOXARLXTPATIKC6DRBHQ23ANCNFSM4KACUQMA>\n> .\n>\n", "yes , could you please look into the issue, not just set/remove the label. thank you", "This should be fixed in the latest version of tensorflow -- can you update and check?", "@veonua \r\n\r\nIs this still an issue. Can you check with TF version 2.2 and see if the issue still persists?. Thanks!", "@ravikyram same issue here, \r\ncan you please point out the commit that supposes to fix this issue", "@k-w-w ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "NO! BOT STOP!", "@veonua,\r\nSorry for the delayed response. I am trying to reproduce your code but `inception_filename = '/content/drive/My Drive/ocr/invoice_no.inception.tfrecord'` is missing. Can you provide the `tfrecord file` so that we can help you. Thanks!", "@rmothukuru it was not an important part, the important thing- it is (was) not possible to store the model from example\r\n\r\n", "@rmothukuru here is the copy of the Image Captioning example where I am trying to save the decoder model\r\n\r\nhttps://colab.research.google.com/drive/1pdlFshsXNEfMM0ml8Z9jaA4a7P0UKogY?usp=sharing", "```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-37-67d6adcc380b> in <module>()\r\n----> 1 tf.saved_model.save(decoder, \"/tmp/model\")\r\n\r\n21 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    300   def wrapper(*args, **kwargs):\r\n    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 302       return func(*args, **kwargs)\r\n    303 \r\n    304   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\r\n```", "> @rmothukuru here is the copy of the Image Captioning example where I am trying to save the decoder model\r\n> \r\n> https://colab.research.google.com/drive/1pdlFshsXNEfMM0ml8Z9jaA4a7P0UKogY?usp=sharing\r\n\r\nI can't access the `Colab`. Can you please share the `Github Gist of that Colab` using **`File -> Save a Copy as Github Gist`**. Thanks!", "https://colab.research.google.com/gist/veonua/186420c02e34b3474bc95294ff01cbfa/copy-of-image_captioning.ipynb\n\nhope it helps\n\nor you can just run all of\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\n\nthen try to save models to the disk\n\ntf.saved_model.save(decoder, \"/tmp/model\")\n\nOn Sun, Nov 22, 2020 at 7:40 AM rmothukuru <notifications@github.com> wrote:\n\n> @rmothukuru <https://github.com/rmothukuru> here is the copy of the Image\n> Captioning example where I am trying to save the decoder model\n>\n>\n> https://colab.research.google.com/drive/1pdlFshsXNEfMM0ml8Z9jaA4a7P0UKogY?usp=sharing\n>\n> I can't access the Colab. Can you please share the Github Gist of that\n> Colab using *File -> Save a Copy as Github Gist*. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35451#issuecomment-731706396>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALYALSKTTAV4AL67AIKUXLSRCW4VANCNFSM4KACUQMA>\n> .\n>\n", "@veonua,\r\nCan you please refer this [Stack Overflow Answer](https://stackoverflow.com/questions/62250441/saving-a-tensorflow-keras-model-encoder-decoder-to-savedmodel-format) where similar error has been resolved. Thanks!", "so can you please make these changes in the example?\n\nit would be good experience to have example model that can be saved .\n(without using 3rd party services)\n\nOn Tue, 24 Nov 2020 at 12:38, rmothukuru <notifications@github.com> wrote:\n\n> @veonua <https://github.com/veonua>,\n> Can you please refer this Stack Overflow Answer\n> <https://stackoverflow.com/questions/62250441/saving-a-tensorflow-keras-model-encoder-decoder-to-savedmodel-format>\n> where similar error has been resolved. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35451#issuecomment-732881229>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALYALQYLKSDEH5OFUMS753SROLMJANCNFSM4KACUQMA>\n> .\n>\n", "@veonua It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "@sushreebarsa \r\n> It looks like you are using an older Version of Tensorflow .\r\n\r\nI am running an official (Google) example using Google Colab. The best integration possible, but it does not go smoothly.\r\n\r\nplease see tf.__version__ in the notebook attached to this issue \r\n\r\n> @oanush commented on Dec 30, 2019\r\n> @veonua ,\r\n> Could you please provide complete code to reproduce the issue? Thanks!\r\n\r\nwhy do you ask for the code if you never look into it? it's just a colab notebook,", "@veonua \r\nWe are checking to see if you still need help on this issue.Could you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "It is not for me, the official TF example is broken.\nCould you please test it first.\n\nOn Sun, 12 Sep 2021 at 20:39, Saduf2019 ***@***.***> wrote:\n\n> @veonua <https://github.com/veonua>\n> We are checking to see if you still need help on this issue.Could you\n> please try on latest stable version of tf and let us know if this is still\n> an issue.Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35451#issuecomment-917688021>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALYALWTQTZF5GTCYLTLTOLUBTXVFANCNFSM4KACUQMA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35451\">No</a>\n", "bad bot", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "it\u2019s not stale, just nobody give any attention to fix it.", "@veonua have you solve the problem? I meet the same bug", "unfortunately no, ended up with switched to another platform\n\nOn Thu, 23 Dec 2021 at 13:18, wuqiang ***@***.***> wrote:\n\n> @veonua <https://github.com/veonua> have you solve the problem? I meet\n> the same bug\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35451#issuecomment-1000266224>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AALYALVHBRBBEBXJY5K5OI3USMHRFANCNFSM4KACUQMA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "Hi @veonua ! I did not have access to tfrecords in above [comment ](https://github.com/tensorflow/tensorflow/issues/35451#issuecomment-570073048). But I did not find any issue in saving the encoder model from the image captioning example. Attaching [gist ](https://colab.sandbox.google.com/gist/mohantym/0e222185efa5771c740cf24041466e05/image_captioning.ipynb#scrollTo=Vrdtlh_-tdv-)for reference.  Please create a new ticket in Keras repo with the dataset if you are looking for further assistance.Thank you!", "Hi @mohantym. Yes it seems like it works in tf 2.8", "@veonua ! Thanks for confirmation. Can we move this issue to closed status then?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35451\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35451\">No</a>\n"]}, {"number": 35450, "title": "All values come out as 0 while converting int32 to float16 using `tf.image.convert_image_dtype` ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Version 1903\r\n- TensorFlow installed from (source or binary): Source (pip install is source I guess?)\r\n- TensorFlow version (use command below): v1.12.1-21327-gd3457b26a0 2.1.0-dev20191226\r\n- Python version: 3.7.3\r\n- GPU model and memory: GeForce 1050Ti / 16GB of RAM\r\n\r\n**Describe the current behavior**\r\nWhen I convert `int32` type of data to `float16`, all values come out as 0.\r\n\r\n**Describe the expected behavior**\r\nThe values should be like \"1.0\" I believe.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n>>> x = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], dtype=tf.int32)\r\n>>> x\r\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\r\narray([[[ 1,  2,  3],\r\n        [ 4,  5,  6]],\r\n\r\n       [[ 7,  8,  9],\r\n        [10, 11, 12]]])>\r\n>>> tf.image.convert_image_dtype(x, dtype=tf.float16, saturate=False) \r\n<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=\r\narray([[[0., 0., 0.],\r\n        [0., 0., 0.]],\r\n\r\n       [[0., 0., 0.],\r\n        [0., 0., 0.]]], dtype=float16)>\r\n```\r\n", "comments": ["@mihaimaruseac Hello, I created the issue but looks like I can't assign you so I'm just tagging you here.", "In the code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/image_ops_impl.py#L1809\r\n\r\nusing scale as 1/image.dtype.max which is 1/(max Int32) and multiplying leads to very small values of order 1e-10, float16 cannot handle such large precisions, if you use dtype=float32 in convert_image_dtype the value is not 0 but the actual small value.   ", "Hello, as your comment explains, I think that is some expected thing? But I'm not too sure about this so I'll leave decide to whether to close/not close this issue to @mihaimaruseac ", "Closing then as it is working as intended."]}, {"number": 35449, "title": "Error converting my model trained with TF 2.1rc1 to TFLite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary):  conda\r\n- TensorFlow version (or github SHA if from source): 2.1.0-rc1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n(dl4cv) D:\\development\\tensorflow\\ANPR\\output\\rmsprop>tflite_convert --keras_model_file=clpr-model.h5 --enable_select_tf_ops --output_file=model.tflite\r\n2019-12-27 14:09:27.630637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.summary API due to missing TensorBoard installation.\r\n2019-12-27 14:09:29.765186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-27 14:09:29.788663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2019-12-27 14:09:29.797947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-27 14:09:29.807131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-27 14:09:29.815002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-27 14:09:29.823010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-27 14:09:29.830529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-27 14:09:29.838455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-27 14:09:29.850408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-27 14:09:29.855640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-27 14:09:29.859642: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-27 14:09:29.865715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2019-12-27 14:09:29.874362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-27 14:09:29.878508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-27 14:09:29.882420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-27 14:09:29.886489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-27 14:09:29.890738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-27 14:09:29.894738: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-27 14:09:29.898962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-27 14:09:29.903241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-27 14:09:30.584878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-27 14:09:30.589782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2019-12-27 14:09:30.592385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2019-12-27 14:09:30.595864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6271 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nW1227 14:09:33.236038 28348 hdf5_format.py:177] No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n2019-12-27 14:09:33.443668: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-12-27 14:09:33.450829: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-12-27 14:09:33.467462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2019-12-27 14:09:33.478633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-27 14:09:33.483411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-27 14:09:33.487932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-27 14:09:33.492763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-27 14:09:33.498057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-27 14:09:33.502602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-27 14:09:33.508437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-27 14:09:33.513114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-27 14:09:33.516838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-27 14:09:33.521102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2019-12-27 14:09:33.524027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2019-12-27 14:09:33.529383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6271 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-12-27 14:09:33.597411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2019-12-27 14:09:33.603519: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 380 nodes (0), 484 edges (0), time = 17.869ms.\r\n2019-12-27 14:09:33.610268: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 380 nodes (0), 484 edges (0), time = 4.152ms.\r\n2019-12-27 14:09:33.615867: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_cond_2105\r\n2019-12-27 14:09:33.621215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-27 14:09:33.626682: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-27 14:09:33.631716: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_body_1602\r\n2019-12-27 14:09:33.637805: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-27 14:09:33.643095: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.647759: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_cond_1941\r\n2019-12-27 14:09:33.653473: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-27 14:09:33.658111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.663563: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_body_2106\r\n2019-12-27 14:09:33.668843: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-27 14:09:33.674032: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.679432: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_body_1766\r\n2019-12-27 14:09:33.685071: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.690795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.695366: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_cond_1601\r\n2019-12-27 14:09:33.701130: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.705783: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.710792: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_body_1942\r\n2019-12-27 14:09:33.717048: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-27 14:09:33.721844: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.726940: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_cond_1765\r\n2019-12-27 14:09:33.732340: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:33.737188: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-12-27 14:09:34.924139: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-12-27 14:09:34.931432: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-12-27 14:09:34.941771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2019-12-27 14:09:34.952985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-27 14:09:34.957078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-27 14:09:34.962751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-27 14:09:34.967762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-27 14:09:34.973494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-27 14:09:34.977944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-27 14:09:34.983289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-27 14:09:34.987946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-27 14:09:34.992126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-27 14:09:34.996303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2019-12-27 14:09:34.999665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2019-12-27 14:09:35.004366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6271 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-12-27 14:09:35.521865: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2019-12-27 14:09:35.526582: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 226 nodes (-16), 276 edges (-28), time = 227.782ms.\r\n2019-12-27 14:09:35.532999: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 226 nodes (0), 276 edges (0), time = 111.153ms.\r\n2019-12-27 14:09:35.538097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_body_2106_frozen\r\n2019-12-27 14:09:35.545175: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 2.335ms.\r\n2019-12-27 14:09:35.550989: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.737ms.\r\n2019-12-27 14:09:35.556165: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_cond_2105_frozen\r\n2019-12-27 14:09:35.562428: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.306ms.\r\n2019-12-27 14:09:35.567480: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.176ms.\r\n2019-12-27 14:09:35.573582: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_cond_1941_frozen\r\n2019-12-27 14:09:35.579766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.262ms.\r\n2019-12-27 14:09:35.584544: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.175ms.\r\n2019-12-27 14:09:35.590845: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_body_1942_frozen\r\n2019-12-27 14:09:35.597525: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 1.396ms.\r\n2019-12-27 14:09:35.602989: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.698ms.\r\n2019-12-27 14:09:35.609327: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_cond_1765_frozen\r\n2019-12-27 14:09:35.614684: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.274ms.\r\n2019-12-27 14:09:35.621170: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.216ms.\r\n2019-12-27 14:09:35.626793: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_backward_gru_while_body_1766_frozen\r\n2019-12-27 14:09:35.632917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 1.417ms.\r\n2019-12-27 14:09:35.639119: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.711ms.\r\n2019-12-27 14:09:35.645066: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_body_1602_frozen\r\n2019-12-27 14:09:35.652559: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (-1), 79 edges (0), time = 1.349ms.\r\n2019-12-27 14:09:35.658901: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 60 nodes (0), 79 edges (0), time = 0.749ms.\r\n2019-12-27 14:09:35.665589: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_bidirectional_forward_gru_while_cond_1601_frozen\r\n2019-12-27 14:09:35.673058: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.295ms.\r\n2019-12-27 14:09:35.680300: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.138ms.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Andreas\\Anaconda3\\envs\\dl4cv\\Scripts\\tflite_convert.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\python\\tflite_convert.py\", line 594, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\python\\tflite_convert.py\", line 577, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\python\\tflite_convert.py\", line 235, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-12-27 14:09:36.318110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nWARNING:root:Limited tf.summary API due to missing TensorBoard installation.\r\n2019-12-27 14:09:39.097723: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-27 14:09:39.107524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-27 14:09:39.163607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2019-12-27 14:09:39.164452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-27 14:09:39.206090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-27 14:09:39.249003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-27 14:09:39.256358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-27 14:09:39.307626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-27 14:09:39.326748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-27 14:09:39.414022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-27 14:09:39.414730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-27 14:09:42.459316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-27 14:09:42.459514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2019-12-27 14:09:42.459628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2019-12-27 14:09:42.460800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-12-27 14:09:42.536530: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-12-27 14:09:42.536727: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.536928: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-12-27 14:09:42.537124: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.537289: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-12-27 14:09:42.537455: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.537616: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-12-27 14:09:42.537804: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.537974: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2019-12-27 14:09:42.538136: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.538282: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.538438: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2019-12-27 14:09:42.538586: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.538734: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.538884: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-12-27 14:09:42.539041: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-12-27 14:09:42.539238: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-12-27 14:09:42.539398: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.539579: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2019-12-27 14:09:42.539737: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.539879: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-12-27 14:09:42.540034: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.540195: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2019-12-27 14:09:42.540357: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.540515: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2019-12-27 14:09:42.540692: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.540861: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.541036: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2019-12-27 14:09:42.541193: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.541338: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2019-12-27 14:09:42.541488: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-12-27 14:09:42.541642: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2019-12-27 14:09:42.550428: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 98 operators, 264 arrays (0 quantized)\r\n2019-12-27 14:09:42.555345: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 98 operators, 264 arrays (0 quantized)\r\n2019-12-27 14:09:42.562862: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 62 operators, 194 arrays (0 quantized)\r\n2019-12-27 14:09:42.564023: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 60 operators, 192 arrays (0 quantized)\r\n2019-12-27 14:09:42.565092: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 58 operators, 188 arrays (0 quantized)\r\n2019-12-27 14:09:42.566092: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 58 operators, 188 arrays (0 quantized)\r\n2019-12-27 14:09:42.566900: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 58 operators, 188 arrays (0 quantized)\r\n2019-12-27 14:09:42.567646: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Identify nearest upsample.: 58 operators, 188 arrays (0 quantized)\r\n2019-12-27 14:09:42.569453: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 655360 bytes, theoretical optimal value: 655360 bytes.\r\n2019-12-27 14:09:42.569852: I tensorflow/lite/toco/toco_tooling.cc:471] Number of parameters: 4121949\r\n2019-12-27 14:09:42.575404: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FILL, FULLY_CONNECTED, MAX_POOL_2D, PACK, RESHAPE, REVERSE_V2, SHAPE, SOFTMAX, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Andreas\\Anaconda3\\envs\\dl4cv\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\andreas\\anaconda3\\envs\\dl4cv\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FILL, FULLY_CONNECTED, MAX_POOL_2D, PACK, RESHAPE, REVERSE_V2, SHAPE, SOFTMAX, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n[download model](https://drive.google.com/file/d/18kaawo6lfOd98lH76U47H0jkYap3WjWU/view?usp=sharing)\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[GitHub project](https://github.com/aboerzel/ALPR-keras)\r\n\r\nModel architecture:\r\n```\r\nclass OCR:\r\n    @staticmethod\r\n    def build(input_size, pool_size, output_size):\r\n        conv_filters = 16\r\n        kernel_size = (3, 3)\r\n        time_dense_size = 32\r\n        rnn_size = 512\r\n\r\n        input_data = Input(name=\"input\", shape=input_size)\r\n\r\n        cnn = Conv2D(conv_filters, kernel_size, padding='same', kernel_initializer='he_normal')(input_data)\r\n        cnn = BatchNormalization()(cnn)\r\n        cnn = Activation('relu')(cnn)\r\n        cnn = MaxPooling2D(pool_size=(pool_size, pool_size))(cnn)\r\n\r\n        cnn = Conv2D(conv_filters, kernel_size, padding='same', kernel_initializer='he_normal')(cnn)\r\n        cnn = BatchNormalization()(cnn)\r\n        cnn = Activation('relu')(cnn)\r\n        cnn = MaxPooling2D(pool_size=(pool_size, pool_size))(cnn)\r\n\r\n        # CNN to RNN\r\n        shape = cnn.get_shape()\r\n        bgru = Reshape((shape[1], shape[2] * shape[3]))(cnn)\r\n\r\n        bgru = Bidirectional(GRU(units=rnn_size, return_sequences=True, dropout=0.5))(bgru)\r\n        bgru = TimeDistributed(Dense(units=time_dense_size))(bgru)\r\n\r\n        bgru = Bidirectional(GRU(units=rnn_size, return_sequences=True, dropout=0.5))(bgru)\r\n        output_data = TimeDistributed(Dense(units=output_size, activation=\"softmax\"))(bgru)\r\n\r\n        return input_data, output_data\r\n```\r\n\r\n\r\n", "comments": ["The new experimental converter flag helps to convert models containing control flow ops in tf lite.\r\nSee https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter?version=nightly\r\nCan you try with tf-nightly version and use experimental converter flag?\r\n```python\r\n# `experimental_new_converter` flag.\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```", "Thanks for the quick help, the problem seems to be solved in the tf-nightly version. I could now convert my Keras model to a TFLite model, see console output:\r\n\r\n```\r\n\r\n(tf-n) D:\\development\\tensorflow\\saved-model>python\r\nPython 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\n\r\n>>> import tensorflow as tf\r\n2020-01-03 11:53:31.382855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nWARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\n>>> print (tf.__version__)\r\n2.1.0-dev20200101\r\n>>> from tensorflow_core.lite.python.lite import TFLiteConverter\r\n>>> converter = TFLiteConverter.from_keras_model_file(\"clpr-model.h5\")\r\n2020-01-03 11:54:13.567519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-03 11:54:13.597923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-01-03 11:54:13.608151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-03 11:54:13.617085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-03 11:54:13.624166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-03 11:54:13.631583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-03 11:54:13.641627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-03 11:54:13.654835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-03 11:54:13.697939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-03 11:54:13.705541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-03 11:54:13.711667: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-03 11:54:13.718275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-01-03 11:54:13.727735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-03 11:54:13.732743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-03 11:54:13.737100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-03 11:54:13.741393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-03 11:54:13.745770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-03 11:54:13.750174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-03 11:54:13.754456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-03 11:54:13.758860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-03 11:54:16.071893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-03 11:54:16.078369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2020-01-03 11:54:16.082532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2020-01-03 11:54:16.087348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6304 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n2020-01-03 11:54:22.032670: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-01-03 11:54:22.041300: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-03 11:54:22.073419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-01-03 11:54:22.086076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-03 11:54:22.091592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-03 11:54:22.096409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-03 11:54:22.100644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-03 11:54:22.106354: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-03 11:54:22.111275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-03 11:54:22.115978: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-03 11:54:22.122779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-03 11:54:22.126064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-03 11:54:22.131080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2020-01-03 11:54:22.133956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2020-01-03 11:54:22.142183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6304 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-03 11:54:22.292642: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize\r\n2020-01-03 11:54:22.299483: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 380 nodes (0), 484 edges (0), time = 23.395ms.\r\n2020-01-03 11:54:22.305784: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 380 nodes (0), 484 edges (0), time = 4.831ms.\r\n2020-01-03 11:54:22.311102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_cond_1537\r\n2020-01-03 11:54:22.317628: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 2.427ms.\r\n2020-01-03 11:54:22.322884: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.328516: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_forward_gru_while_cond_1197\r\n2020-01-03 11:54:22.334748: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.340884: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.345895: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_forward_gru_while_body_1198\r\n2020-01-03 11:54:22.351964: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-03 11:54:22.356766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.362160: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_backward_gru_while_body_1362\r\n2020-01-03 11:54:22.368032: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.372883: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.377991: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_cond_1701\r\n2020-01-03 11:54:22.383364: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.388489: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.393606: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_1_backward_gru_1_while_body_1702\r\n2020-01-03 11:54:22.398977: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.404466: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-03 11:54:22.409196: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_backward_gru_while_cond_1361\r\n2020-01-03 11:54:22.415716: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-03 11:54:22.421434: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-03 11:54:22.427168: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_bidirectional_1_forward_gru_1_while_body_1538\r\n2020-01-03 11:54:22.433035: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-03 11:54:22.437814: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n>>> converter.experimental_new_converter = True\r\n>>> tflite_model = converter.convert()\r\n>>> open(\"clpr-model.tflite\", \"wb\").write(tflite_model)\r\n16612140\r\n>>>  \r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35449\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35449\">No</a>\n"]}, {"number": 35448, "title": "tf.keras.layers.Reshape not working as expected", "body": "**System information**\r\n- Ubuntu 18.043 LTS\r\n- TensorFlow installed using pip\r\n- TensorFlow version '2.0.0' (CPU version)\r\n- Python version 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nThe sample code show below give following (partial) output and exception:\r\n\r\n```\r\n(400, 100)\r\n(2, 200, 100)\r\nLambda reshape worked!\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-54-e2fee5681321> in <module>\r\n     12 print(out_1.shape)\r\n     13 print('Lambda reshape worked!')\r\n---> 14 reshape_layer(inp)\r\n\r\n~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    889           with base_layer_utils.autocast_context_manager(\r\n    890               self._compute_dtype):\r\n--> 891             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    892           self._handle_activity_regularization(inputs, outputs)\r\n    893           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs)\r\n    469   def call(self, inputs):\r\n    470     return array_ops.reshape(inputs,\r\n--> 471                              (array_ops.shape(inputs)[0],) + self.target_shape)\r\n    472 \r\n    473   def get_config(self):\r\n\r\n~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py in reshape(tensor, shape, name)\r\n    129     A `Tensor`. Has the same type as `tensor`.\r\n    130   \"\"\"\r\n--> 131   result = gen_array_ops.reshape(tensor, shape, name)\r\n    132   tensor_util.maybe_set_static_shape(result, shape)\r\n    133   return result\r\n\r\n~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   8104       try:\r\n   8105         return reshape_eager_fallback(\r\n-> 8106             tensor, shape, name=name, ctx=_ctx)\r\n   8107       except _core._SymbolicException:\r\n   8108         pass  # Add nodes to the TensorFlow graph.\r\n\r\n~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)\r\n   8142   _attrs =\r\n (\"T\", _attr_T, \"Tshape\", _attr_Tshape)\r\n   8143   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n-> 8144                              ctx=_ctx, name=name)\r\n   8145   _execute.record_gradient(\r\n   8146       \"Reshape\", _inputs_flat, _attrs, _result, name)\r\n\r\n~/code/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~/code/venv/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Input to reshape is a tensor with 40000 values, but the requested shape has 8000000 [Op:Reshape]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nDefined Reshape layer should reshape the input of shape (400, 100) to a tensor of shape (2, 200, 100)\r\nWrapping tf.reshape in a Lambda layer is working as an alternative solution\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\r\nfrom tensorflow.keras.layers import Reshape, Lambda\r\nimport tensorflow as tf\r\n\r\nmax_len = 200\r\nchar_hidden_dim = 50\r\nreshape_layer = Reshape((max_len, 2 * char_hidden_dim,))\r\nlambda_reshape_layer = Lambda(lambda x: tf.reshape(x, shape=[-1, max_len, 2 * char_hidden_dim]))\r\n\r\nn_samples = 400\r\ndim = 100\r\ninp = np.zeros((n_samples, dim), dtype=np.float32)\r\nprint(inp.shape)\r\n\r\nout_1 = lambda_reshape_layer(inp)\r\nprint(out_1.shape)\r\nprint('Lambda reshape worked!')\r\nreshape_layer(inp)\r\n```\r\n\r\nDo I have a flawed understanding of the Reshape layer or can someone confirm that it is indeed a bug? \r\nPlease help :)", "comments": ["@dhakrasp This is not a bug.\r\n\r\nFirst, the Reshape layer would not change your \"batch size\",  which is 400 in your example.\r\nSecond, the total element number in each sample should keep the same.  Your \"inp\" has too few element. See the modified example.\r\n\r\n```\r\nmax_len = 200\r\nchar_hidden_dim = 50\r\nreshape_layer = Reshape((max_len, 4 * char_hidden_dim))\r\nlambda_reshape_layer = Lambda(\r\n  lambda x: tf.reshape(x, shape=[-1, max_len, 2 * char_hidden_dim])\r\n)\r\n\r\nn_samples = 400\r\ndim = 100\r\ninp = np.zeros((1, n_samples, dim), dtype=np.float32)\r\nprint(inp.shape)\r\n\r\nout_1 = lambda_reshape_layer(inp)\r\nprint(out_1.shape)\r\nprint('Lambda reshape worked!')\r\nprint(reshape_layer(inp).shape)\r\n```\r\n\r\n", "@SummerRainET2008 , \r\nUnderstood the design and functionality of Reshape. I assumed it would change the batch_size too.\r\n\r\nThe reason to change the batch_size was that while feeding the character level inputs I was appending the character level inputs of multiple words (the actual i/p to the model).\r\n\r\ne.g.\r\n1 batch = 2 sentences\r\n1 sentence = 200 words\r\n1 word = 50 characters\r\n1 char = 50 dim vector\r\nInitial input -->\r\n2 x 200 x 50 x 50 \r\n~=400 x 50 x 50\r\nThis is the input to a Char-BiLSTM\r\nOutput of Char-BiLSTM -->\r\n400 x 100\r\n~= 2 x 200 x 100\r\n\r\nIs my Lambda layer the correct way implement this? If not, could you suggest a solution?", "Could able to reproduce the issue with Tf 2.0. Please find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/9e3195f0cf0bf39c5a0b225f798248a5/untitled334.ipynb). Thanks!", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35448\">No</a>\n"]}, {"number": 35447, "title": "Chain multiple estimators to create a single SavedModel ", "body": "Chain multiple estimators to create a single SavedModel with a single serving file/output.\r\n\r\nSuppose I have 3 estimators with me, first is BoostedTrees, who's output I want to use as input into DNNClassifier, and the output of which I want to use in my custom Estimator.\r\n\r\nIs there a way to chain output/input of each other to create a **mega** estimator of sorts.\r\n\r\nPlease help me out here.", "comments": ["@gadagashwini @ymodak @omalleyt12 \ud83d\udc46 ", "Hi @ravikyram @ymodak ,\r\n\r\nAny update on the above would be appreciated.", "Hi @ymodak @ravikyram @gadagashwini \r\n\r\nAny update on the above?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 35446, "title": "Failed to load model with scalar inputs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n> Both Win10 and Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n> from binary using pip\r\n- TensorFlow version (use command below):\r\n> 2.0.0\r\n- Python version:\r\n> 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n> cudnn-10.2-windows10-x64-v7.6.5.32\r\n- GPU model and memory:\r\n> 24G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n\r\nI defined a model with two inputs: one of shape `(seq_len, fea_size)`, and the other of shape `()`.\r\nIt is fine while training and saving the model, but it gets failed when loading the model from the export directory, with error show like this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/Work/Python/track_by_classification/src/test/test_model.py\", line 101, in <module>\r\n    test_model_with_scalar_input()\r\n  File \"D:/Work/Python/track_by_classification/src/test/test_model.py\", line 90, in test_model_with_scalar_input\r\n    new_model = tf.keras.models.load_model(model_home)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 86, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\load.py\", line 541, in load_internal\r\n    export_dir)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 103, in __init__\r\n    self._finalize()\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\load.py\", line 132, in _finalize\r\n    node._set_inputs(inputs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2709, in _set_inputs\r\n    outputs = self(inputs, **kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 842, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py\", line 57, in return_outputs_and_add_losses\r\n    outputs, losses = fn(inputs, *args, **kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py\", line 111, in wrap_with_training_arg\r\n    lambda: replace_training_and_call(False))\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py\", line 59, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\framework\\smart_cond.py\", line 59, in smart_cond\r\n    name=name)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py\", line 1174, in cond\r\n    return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\", line 84, in cond_v2\r\n    op_return_value=pred)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py\", line 110, in <lambda>\r\n    lambda: replace_training_and_call(True),\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\utils.py\", line 106, in replace_training_and_call\r\n    return wrapped_call(*args, **kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"D:\\WorkPrograms\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (3 total):\r\n    * [<tf.Tensor 'inputs:0' shape=(None, 10, 128) dtype=float32>, <tf.Tensor 'inputs_1:0' shape=(None, 1) dtype=float32>]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/1')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None,), dtype=tf.float32, name='input_2')]\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None,), dtype=tf.float32, name='inputs/1')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * [TensorSpec(shape=(None, 10, 128), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None,), dtype=tf.float32, name='input_2')]\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\n```\r\n\r\nAnd as I have hacked the tensorflow source code, I found the bug comes from here [training_utils.ModelInputs.get_symbolic_inputs](https://github.com/tensorflow/tensorflow/blob/03a2b3f1fb35373c415444f974e4d627cadb1d33/tensorflow/python/keras/engine/training_utils.py#L1852)\r\n\r\n```python\r\n  def get_symbolic_inputs(self, return_single_as_list=False):\r\n    \"\"\"Returns inputs to be set as self.inputs for a model.\"\"\"\r\n    # TODO(karmel): There is a side-effect here where what you get\r\n    # with as_list and as_dict depends on whether you have called this\r\n    # method first, since it modifies in place.\r\n    for i, (k, v) in enumerate(zip(self._input_names, self._flattened_inputs)):\r\n      if isinstance(v, (list, float, int)):\r\n        v = np.asarray(v)\r\n        if v.ndim == 1:\r\n          v = np.expand_dims(v, 1)\r\n\r\n      if isinstance(v, (np.ndarray, ops.EagerTensor)):\r\n        # We fix the placeholder shape except the batch size.\r\n        # This is suboptimal, but it is the best we can do with the info\r\n        # we have. The user should call `model._set_inputs(placeholders)`\r\n        # to specify custom placeholders if the need arises.\r\n        shape = (None,) + tuple(v.shape[1:])\r\n        if shape == (None,):\r\n          shape = (None, 1)\r\n        dtype = dtypes.as_dtype(v.dtype)\r\n        if dtype.is_floating:\r\n          dtype = K.floatx()\r\n        v = K.placeholder(shape=shape, name=k, dtype=dtype)\r\n      elif isinstance(v, tensor_spec.TensorSpec):\r\n        shape = (None,) + tuple(v.shape.as_list()[1:])\r\n        if shape == (None,):\r\n        >>>>>  shape = (None, 1) <<<< here is where the errors comes from\r\n        v = K.placeholder(shape=shape, name=k, dtype=v.dtype)\r\n\r\n      self._flattened_inputs[i] = v\r\n\r\n    if self._is_dict:\r\n      return dict(zip(self._input_names, self._flattened_inputs))\r\n    if self._is_single_input and not return_single_as_list:\r\n      return self._flattened_inputs[0]\r\n    return self._flattened_inputs\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\ndef test_model_with_scalar_input():\r\n    model_home = \"models\"\r\n    input_shape = 5, 10, 128\r\n    n_class = 5\r\n\r\n    batch_size, seq_len, fea_size = input_shape\r\n\r\n    input_fea = tf.keras.layers.Input(shape=input_shape[1:])\r\n    input_seq = tf.keras.layers.Input(shape=())\r\n\r\n    name = \"test-model\"\r\n    forward_layer = tf.keras.layers.LSTM(units=fea_size, return_sequences=True, name=name + \"-forward\")\r\n\r\n    backward_layer = tf.keras.layers.LSTM(units=fea_size, return_sequences=True, go_backwards=True,\r\n                                          name=name + \"-backward\")\r\n\r\n    bi_lstm = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer, name=name + \"-bi-lstm\")\r\n\r\n    fc = tf.keras.layers.Dense(units=n_class, activation=\"softmax\", name=\"fc-pred\")\r\n\r\n    mask = tf.sequence_mask(input_seq, seq_len)\r\n\r\n    y = bi_lstm(input_fea, mask=mask)\r\n    y = fc(y)\r\n\r\n    model = tf.keras.models.Model(inputs=[input_fea, input_seq], outputs=y, name=name)\r\n\r\n    features = np.random.normal(size=[batch_size * 100, seq_len, fea_size])\r\n    labels = np.random.randint(0, n_class, size=[batch_size * 100, seq_len])\r\n    # seq = np.random.randint(0, seq_len, size=[batch_size * 100])\r\n    seq = [seq_len] * (batch_size * 100)\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(((features, seq), labels))\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\r\n\r\n    saver = tf.keras.callbacks.ModelCheckpoint(model_home, monitor=\"loss\", verbose=1, save_best_only=True)\r\n    model.fit(dataset, epochs=5, callbacks=[saver], steps_per_epoch=10)\r\n\r\n    new_model = tf.keras.models.load_model(model_home)\r\n\r\n    # print(new_model.layers)\r\n\r\n    pred = new_model.predict(dataset)\r\n    pred = np.argmax(pred, axis=-1)\r\n    print(pred)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[The code showed above performs as expected on 2.1.0-rc1](https://colab.research.google.com/drive/1gQqC-vZPFSKwIof4LapCocs98PezC8GL#scrollTo=a2b8z-1QZRX0)", "comments": ["@loveychen ,\r\nHi,Can you please try using `tf-nightly 2.1.0.dev20191230 `version as it will be stable version going forward. Find the [gist](https://colab.sandbox.google.com/gist/oanush/fe07fc375bba5399c4e3f10920c051d6/35446.ipynb) of the colab for your reference. Thanks!", "Hi @oanush ,\r\n\r\nI run the code with `tf-nightly 2.1.0.dev20191230`, and yes, it works as expected.\r\n\r\nWhat's your time plan for the stable tf2.1?\r\n", "@loveychen ,\r\nTime plan will be soon announced, right now there is no official update. I will close the issue as it is resolved, please feel free to open if you face any issue.Thanks!"]}, {"number": 35445, "title": "TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input data should match weights width", "body": "**System information**\r\n- Have I written custom code :\r\n```java\r\nprivate List<Delegate> mDelegates = new ArrayList<>();\r\nmDelegates.add(new GpuDelegate());\r\n```\r\n\r\n```java\r\n            Interpreter.Options options = null;\r\n            if (interpreterOptions != null) {\r\n                options = new Interpreter.Options().setNumThreads(interpreterOptions.getNumThreads());\r\n            }\r\n            if (!mDelegates.isEmpty()) {\r\n                Iterator<Delegate> it = mDelegates.iterator();\r\n                while (it.hasNext()) {\r\n                    Delegate delegate = it.next();\r\n                    mLogger.debug(\"addDelegate: \" + delegate);\r\n                    options.addDelegate(delegate);\r\n                }\r\n            }\r\n            mInterpreter = new Interpreter(bb, options);\r\n```\r\n\r\n- Mobile device :\r\nMI 8 / MIUI 11.0.4\r\n\r\n- TensorFlow installed from (source or binary):\r\nimplementation 'org.tensorflow:tensorflow-lite:2.0.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.0.0'\r\n\r\n**Describe the current behavior**\r\ncrashed when load the attached file with 'GpuDelegate'\r\n\r\n**Describe the expected behavior**\r\nload the attached file correct\r\n\r\n**Code to reproduce the issue**\r\nN/A\r\n\r\n**Other info / logs**\r\n```text\r\n    java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input data should match weights width\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 3 (TfLiteGpuDelegateV2) failed to prepare.\r\n    \r\n    Restored previous execution plan after delegate application failure.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:162)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at com.didi.aoe.runtime.tensorflow.lite.TensorFlowMultipleInputsOutputsInterpreter.run(TensorFlowMultipleInputsOutputsInterpreter.java:159)\r\n        at com.didi.aoe.library.core.NativeProcessorWrapper.run(NativeProcessorWrapper.java:40)\r\n```\r\n\r\n[mnist_cnn_keras.tflite.zip](https://github.com/tensorflow/tensorflow/files/4004963/mnist_cnn_keras.tflite.zip)\r\n", "comments": ["what does the model look like", "@Kuloud is there workaround for this problem? I have same issue.", "> @Kuloud is there workaround for this problem? I have same issue.\r\n\r\nNone, we changed the model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35445\">No</a>\n"]}, {"number": 35444, "title": "Bug with Adam keras optimizer", "body": "I wanted to report a potential bug of Adam at line: https://github.com/tensorflow/tensorflow/blob/9bfa68666e16d6327cb5cd5e04d25f94248629b2/tensorflow/python/keras/optimizer_v2/adam.py#L164\r\n```\r\ndef _prepare_local(self, var_device, var_dtype, apply_state):\r\n    local_step = math_ops.cast(self.iterations + 1, var_dtype)\r\n```\r\n\r\nIn the beginning, self.iterations would return 1 instead of 0. With this, local_step has a value of 2, and not 1. From [Adam](https://arxiv.org/pdf/1412.6980.pdf) paper, I think time step t (which is local_step in our case) should start with 1, and definitly not 2.\r\n\r\nPlease correct me if I was wrong.", "comments": ["OK after some investigation I found it is not the case, and self.iterations return 0 in the beginning.\r\n\r\nI have another question, though: Why self.iterations and local_step always change between a batch_update. \r\nHere is a code to verify this, where I print local_step and self.iterations twice (on_batch_begin and on_batch_end)\r\n\r\n```\r\n'''Trains a simple convnet on the MNIST dataset.\r\nGets to 99.25% test accuracy after 12 epochs\r\n(there is still a lot of margin for parameter tuning).\r\n16 seconds per epoch on a GRID K520 GPU.\r\n'''\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom __future__ import print_function\r\nimport tensorflow.keras\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\nfrom tensorflow.keras import backend as K\r\nimport tensorflow as tf\r\n\r\n\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.keras import backend_config\r\nfrom tensorflow.python.keras.optimizer_v2 import optimizer_v2\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import control_flow_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import state_ops\r\nfrom tensorflow.python.training import training_ops\r\nfrom tensorflow.python.util.tf_export import keras_export\r\n\r\n\r\n@keras_export('keras.optimizers.Adam')\r\nclass Adam(optimizer_v2.OptimizerV2):\r\n  \"\"\"Optimizer that implements the Adam algorithm.\r\n  Adam optimization is a stochastic gradient descent method that is based on\r\n  adaptive estimation of first-order and second-order moments.\r\n  According to the paper\r\n  [Adam: A Method for Stochastic Optimization. Kingma et al.,\r\n  2014](http://arxiv.org/abs/1412.6980),\r\n   the method is \"*computationally efficient, has little memory\r\n  requirement, invariant to diagonal rescaling of gradients, and is well suited\r\n  for problems that are large in terms of data/parameters*\".\r\n  For AMSGrad see [On The Convergence Of Adam And Beyond.\r\n  Reddi et al., 5-8](https://openreview.net/pdf?id=ryQu7f-RZ).\r\n  \"\"\"\r\n\r\n  def __init__(self,\r\n               learning_rate=0.001,\r\n               beta_1=0.9,\r\n               beta_2=0.999,\r\n               epsilon=1e-7,\r\n               amsgrad=False,\r\n               name='Adam',\r\n               **kwargs):\r\n    r\"\"\"Construct a new Adam optimizer.\r\n    If amsgrad = False:\r\n      Initialization:\r\n      $$m_0 := 0 \\text{(Initialize initial 1st moment vector)}$$\r\n      $$v_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\r\n      $$t := 0 \\text{(Initialize timestep)}$$\r\n      The update rule for `variable` with gradient `g` uses an optimization\r\n      described at the end of section 2 of the paper:\r\n      $$t := t + 1$$\r\n      $$\\text{lr}_t := \\mathrm{learning_rate} *\r\n        \\sqrt{1 - \\beta_2^t} / (1 - \\beta_1^t)$$\r\n      $$m_t := \\beta_1 * m_{t-1} + (1 - \\beta_1) * g$$\r\n      $$v_t := \\beta_2 * v_{t-1} + (1 - \\beta_2) * g * g$$\r\n      $$\\text{variable} := \\text{variable} -\r\n        lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\r\n    If amsgrad = True:\r\n      Initialization:\r\n      $$m_0 := 0 \\text{(Initialize initial 1st moment vector)}$$\r\n      $$v_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\r\n      $$\\hat{v}_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\r\n      $$t := 0 \\text{(Initialize timestep)}$$\r\n      The update rule for `variable` with gradient `g` uses an optimization\r\n      described at the end of section 2 of the paper:\r\n      $$t := t + 1$$\r\n      $$\\text{lr}_t := \\mathrm{learning_rate} *\r\n        \\sqrt{1 - \\beta_2^t} / (1 - \\beta_1^t)$$\r\n      $$m_t := \\beta_1 * m_{t-1} + (1 - \\beta_1) * g$$\r\n      $$v_t := \\beta_2 * v_{t-1} + (1 - \\beta_2) * g * g$$\r\n      $$\\hat{v}_t := \\max(\\hat{v}_{t-1}, v_t)$$\r\n      $$\\text{variable} := \\text{variable} -\r\n        \\text{lr}_t * m_t / (\\sqrt{\\hat{v}_t} + \\epsilon)$$\r\n    The default value of 1e-7 for epsilon might not be a good default in\r\n    general. For example, when training an Inception network on ImageNet a\r\n    current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\r\n    formulation just before Section 2.1 of the Kingma and Ba paper rather than\r\n    the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\r\n    hat\" in the paper.\r\n    The sparse implementation of this algorithm (used when the gradient is an\r\n    IndexedSlices object, typically because of `tf.gather` or an embedding\r\n    lookup in the forward pass) does apply momentum to variable slices even if\r\n    they were not used in the forward pass (meaning they have a gradient equal\r\n    to zero). Momentum decay (beta1) is also applied to the entire momentum\r\n    accumulator. This means that the sparse behavior is equivalent to the dense\r\n    behavior (in contrast to some momentum implementations which ignore momentum\r\n    unless a variable slice was actually used).\r\n    Args:\r\n      learning_rate: A `Tensor`, floating point value, or a schedule that is a\r\n        `tf.keras.optimizers.schedules.LearningRateSchedule`. The learning rate.\r\n      beta_1: A float value or a constant float tensor. The exponential decay\r\n        rate for the 1st moment estimates.\r\n      beta_2: A float value or a constant float tensor. The exponential decay\r\n        rate for the 2nd moment estimates.\r\n      epsilon: A small constant for numerical stability. This epsilon is\r\n        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\r\n        Section 2.1), not the epsilon in Algorithm 1 of the paper.\r\n      amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\r\n        the paper \"On the Convergence of Adam and beyond\".\r\n      name: Optional name for the operations created when applying gradients.\r\n        Defaults to \"Adam\".\r\n      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\r\n        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\r\n        gradients by value, `decay` is included for backward compatibility to\r\n        allow time inverse decay of learning rate. `lr` is included for backward\r\n        compatibility, recommended to use `learning_rate` instead.\r\n    @compatibility(eager)\r\n    When eager execution is enabled, `learning_rate`, `beta_1`, `beta_2`,\r\n    and `epsilon` can each be a callable that takes no arguments and\r\n    returns the actual value to use. This can be useful for changing these\r\n    values across different invocations of optimizer functions.\r\n    @end_compatibility\r\n    \"\"\"\r\n\r\n    super(Adam, self).__init__(name, **kwargs)\r\n    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\r\n    self._set_hyper('decay', self._initial_decay)\r\n    self._set_hyper('beta_1', beta_1)\r\n    self._set_hyper('beta_2', beta_2)\r\n    self.epsilon = epsilon or backend_config.epsilon()\r\n    self.amsgrad = amsgrad\r\n    self.local_step = -float('inf')\r\n\r\n  def _create_slots(self, var_list):\r\n    # Create slots for the first and second moments.\r\n    # Separate for-loops to respect the ordering of slot variables from v1.\r\n    for var in var_list:\r\n      self.add_slot(var, 'm')\r\n    for var in var_list:\r\n      self.add_slot(var, 'v')\r\n    if self.amsgrad:\r\n      for var in var_list:\r\n        self.add_slot(var, 'vhat')\r\n\r\n  def _prepare_local(self, var_device, var_dtype, apply_state):\r\n    super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)\r\n\r\n    self.local_step = math_ops.cast(tf.identity(self.iterations) + 1, var_dtype)\r\n    beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))\r\n    beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))\r\n    beta_1_power = math_ops.pow(beta_1_t, self.local_step)\r\n    beta_2_power = math_ops.pow(beta_2_t, self.local_step)\r\n    lr = (apply_state[(var_device, var_dtype)]['lr_t'] *\r\n          (math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))\r\n    apply_state[(var_device, var_dtype)].update(dict(\r\n        lr=lr,\r\n        epsilon=ops.convert_to_tensor(self.epsilon, var_dtype),\r\n        beta_1_t=beta_1_t,\r\n        beta_1_power=beta_1_power,\r\n        one_minus_beta_1_t=1 - beta_1_t,\r\n        beta_2_t=beta_2_t,\r\n        beta_2_power=beta_2_power,\r\n        one_minus_beta_2_t=1 - beta_2_t\r\n    ))\r\n\r\n  def set_weights(self, weights):\r\n    params = self.weights\r\n    # If the weights are generated by Keras V1 optimizer, it includes vhats\r\n    # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\r\n    # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\r\n    num_vars = int((len(params) - 1) / 2)\r\n    if len(weights) == 3 * num_vars + 1:\r\n      weights = weights[:len(params)]\r\n    super(Adam, self).set_weights(weights)\r\n\r\n  def _resource_apply_dense(self, grad, var, apply_state=None):\r\n    var_device, var_dtype = var.device, var.dtype.base_dtype\r\n    coefficients = ((apply_state or {}).get((var_device, var_dtype))\r\n                    or self._fallback_apply_state(var_device, var_dtype))\r\n\r\n    m = self.get_slot(var, 'm')\r\n    v = self.get_slot(var, 'v')\r\n\r\n    if not self.amsgrad:\r\n      return training_ops.resource_apply_adam(\r\n          var.handle,\r\n          m.handle,\r\n          v.handle,\r\n          coefficients['beta_1_power'],\r\n          coefficients['beta_2_power'],\r\n          coefficients['lr_t'],\r\n          coefficients['beta_1_t'],\r\n          coefficients['beta_2_t'],\r\n          coefficients['epsilon'],\r\n          grad,\r\n          use_locking=self._use_locking)\r\n    else:\r\n      vhat = self.get_slot(var, 'vhat')\r\n      return training_ops.resource_apply_adam_with_amsgrad(\r\n          var.handle,\r\n          m.handle,\r\n          v.handle,\r\n          vhat.handle,\r\n          coefficients['beta_1_power'],\r\n          coefficients['beta_2_power'],\r\n          coefficients['lr_t'],\r\n          coefficients['beta_1_t'],\r\n          coefficients['beta_2_t'],\r\n          coefficients['epsilon'],\r\n          grad,\r\n          use_locking=self._use_locking)\r\n\r\n  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\r\n    var_device, var_dtype = var.device, var.dtype.base_dtype\r\n    coefficients = ((apply_state or {}).get((var_device, var_dtype))\r\n                    or self._fallback_apply_state(var_device, var_dtype))\r\n\r\n    # m_t = beta1 * m + (1 - beta1) * g_t\r\n    m = self.get_slot(var, 'm')\r\n    m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\r\n    m_t = state_ops.assign(m, m * coefficients['beta_1_t'],\r\n                           use_locking=self._use_locking)\r\n    with ops.control_dependencies([m_t]):\r\n      m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\r\n\r\n    # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\r\n    v = self.get_slot(var, 'v')\r\n    v_scaled_g_values = (grad * grad) * coefficients['one_minus_beta_2_t']\r\n    v_t = state_ops.assign(v, v * coefficients['beta_2_t'],\r\n                           use_locking=self._use_locking)\r\n    with ops.control_dependencies([v_t]):\r\n      v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\r\n\r\n    if not self.amsgrad:\r\n      v_sqrt = math_ops.sqrt(v_t)\r\n      var_update = state_ops.assign_sub(\r\n          var, coefficients['lr'] * m_t / (v_sqrt + coefficients['epsilon']),\r\n          use_locking=self._use_locking)\r\n      return control_flow_ops.group(*[var_update, m_t, v_t])\r\n    else:\r\n      v_hat = self.get_slot(var, 'vhat')\r\n      v_hat_t = math_ops.maximum(v_hat, v_t)\r\n      with ops.control_dependencies([v_hat_t]):\r\n        v_hat_t = state_ops.assign(\r\n            v_hat, v_hat_t, use_locking=self._use_locking)\r\n      v_hat_sqrt = math_ops.sqrt(v_hat_t)\r\n      var_update = state_ops.assign_sub(\r\n          var,\r\n          coefficients['lr'] * m_t / (v_hat_sqrt + coefficients['epsilon']),\r\n          use_locking=self._use_locking)\r\n      return control_flow_ops.group(*[var_update, m_t, v_t, v_hat_t])\r\n\r\n  def get_config(self):\r\n    config = super(Adam, self).get_config()\r\n    config.update({\r\n        'learning_rate': self._serialize_hyperparameter('learning_rate'),\r\n        'decay': self._serialize_hyperparameter('decay'),\r\n        'beta_1': self._serialize_hyperparameter('beta_1'),\r\n        'beta_2': self._serialize_hyperparameter('beta_2'),\r\n        'epsilon': self.epsilon,\r\n        'amsgrad': self.amsgrad,\r\n    })\r\n    return config\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\r\n              optimizer=Adam(),\r\n              metrics=['accuracy'])\r\n\r\n\r\n\r\nclass My_Callback(tensorflow.keras.callbacks.Callback):\r\n    def on_batch_end(self, batch, logs={}):\r\n        print(\"\\nDone iteration: \", K.eval(self.model.optimizer.iterations), K.eval(self.model.optimizer.local_step))\r\n        return\r\n    def on_batch_begin(self, batch, logs={}):\r\n        print(\"\\nStart iteration: \", K.eval(self.model.optimizer.iterations), K.eval(self.model.optimizer.local_step))\r\n        return\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          callbacks=[My_Callback()],\r\n          validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n```"]}, {"number": 35443, "title": "ValueError: Column dtype and SparseTensors dtype must be compatible. key: adt_sev_flag, column dtype: <dtype: 'int64'>, tensor dtype: <dtype: 'float32'>", "body": "I use TF1.14 ane keras 2.2.4 , code and error is :\r\n\r\nfrom future import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ndataframe.head()\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\ndataframe = dataframe.copy()\r\nlabels = dataframe.pop('target')\r\nds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\nif shuffle:\r\nds = ds.shuffle(buffer_size=len(dataframe))\r\nds = ds.batch(batch_size)\r\nreturn ds\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nage = feature_column.numeric_column(\"age\")\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\n\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\nfeature_columns.append(feature_column.numeric_column(header))\r\nfeature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\nfeature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\nbatch_size = 100 # \u5c0f\u6279\u91cf\u5927\u5c0f\u7528\u4e8e\u6f14\u793a\r\nepoch = 10\r\nsteps_per_epoch =  len(train)//(batch_size*epoch)\r\nval_steps = len(val)//batch_size\r\nmodel.fit(train_ds, epochs = epoch, steps_per_epoch=steps_per_epoch, validation_data=val_ds,validation_steps=10)\r\n\r\noutput the error:\r\n\r\n----> 2 model.fit(train_ds, epochs = epoch, steps_per_epoch=steps_per_epoch, validation_data=val_ds,validation_steps=10)\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    707         steps=steps_per_epoch,\r\n    708         validation_split=validation_split,\r\n--> 709         shuffle=shuffle)\r\n    710 \r\n    711     # Prepare validation data.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2556       else:\r\n   2557         cast_inputs = x_input\r\n-> 2558       self._set_inputs(cast_inputs)\r\n   2559     else:\r\n   2560       y_input = y\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)\r\n   2774       kwargs = {'training': training} if self._expects_training_arg else {}\r\n   2775       try:\r\n-> 2776         outputs = self(inputs, **kwargs)\r\n   2777       except NotImplementedError:\r\n   2778         # This Model or a submodel is dynamic and hasn't overridden\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    633                 else:\r\n--> 634                   outputs = call_fn(inputs, *args, **kwargs)\r\n    635 \r\n    636             except TypeError as e:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)\r\n    259         kwargs['training'] = training\r\n    260 \r\n--> 261       outputs = layer(inputs, **kwargs)\r\n    262 \r\n    263       # `outputs` will be the inputs to the next layer.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    632                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    633                 else:\r\n--> 634                   outputs = call_fn(inputs, *args, **kwargs)\r\n    635 \r\n    636             except TypeError as e:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    147       except Exception as e:  # pylint:disable=broad-except\r\n    148         if hasattr(e, 'ag_error_metadata'):\r\n--> 149           raise e.ag_error_metadata.to_exception(type(e))\r\n    150         else:\r\n    151           raise\r\n\r\nValueError: in converted code:\r\n    relative to /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column:\r\n\r\n    feature_column_v2.py:473 call *\r\n        tensor = column.get_dense_tensor(transformation_cache,\r\n    feature_column_v2.py:4291 get_dense_tensor\r\n        return transformation_cache.get(self, state_manager)\r\n    feature_column_v2.py:2562 get\r\n        transformed = column.transform_feature(self, state_manager)\r\n    feature_column_v2.py:4230 transform_feature\r\n        transformation_cache, state_manager)\r\n    feature_column_v2.py:3710 get_sparse_tensors\r\n        transformation_cache.get(self, state_manager), None)\r\n    feature_column_v2.py:2562 get\r\n        transformed = column.transform_feature(self, state_manager)\r\n    feature_column_v2.py:3688 transform_feature\r\n        return self._transform_input_tensor(input_tensor)\r\n    feature_column_v2.py:3664 _transform_input_tensor\r\n        self.key, self.dtype, input_tensor.dtype))\r\n\r\n    ValueError: Column dtype and SparseTensors dtype must be compatible. key: adt_sev_flag, column dtype: <dtype: 'int64'>, tensor dtype: <dtype: 'float32'>\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": ["@WakerKQ, Looks like provided code is incomplete, `model is not defined`. Could you provide the complete code to reproduce the reported issue. Thanks!", "> @WakerKQ, Looks like provided code is incomplete, `model is not defined`. Could you provide the complete code to reproduce the reported issue. Thanks!\r\nthanks for replying!  model is defined as :\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  tf.keras.layers.Dense(1024, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(1024, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(256, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(64, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(32, activation=tf.nn.relu),\r\n  tf.keras.layers.Dense(3, activation=tf.nn.softmax)\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.categorical_crossentropy,\r\n              metrics=['accuracy'])", "@WakerKQ, I tried with Tf 1.15 I am facing different error. Please find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/18482e7e8aeb3a573eceb05cb44e1845/untitled342.ipynb) and confirm. Thanks! ", "@WakerKQ, Did you get a chance to look at gist. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35442, "title": "[TF 2.0]ParameterServerStrategy and CentralStorageStrategy doesn't work with Keras when using GPU, even though it works well with CPU.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I used code in [MultiWorkerMirroredStrategy tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras). And i only changed MultiWorkerMirroredStrategy to ParameterServerStrategy and turned off the eager mode.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (use command below): tensorflow 2.0.0 / tensorflow-gpu 2.0.0\r\n- Python version: python 3.6.8\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: TITAN Xp\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy()\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\nNUM_WORKERS = 2\r\n\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n  return image, label\r\n\r\ndatasets, info = tfds.load(name='mnist',\r\n                           with_info=True,\r\n                           as_supervised=True)\r\n\r\ntrain_datasets_unbatched = datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\ntrain_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE).repeat()\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\nwith strategy.scope():\r\n  multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=938)\r\n```\r\n**Describe the current behavior**\r\nAbove code works well with CPU. But when using GPU it produces errors like below. \r\nMy TF_CONFIG variable was like this ( TF_CONFIG='{\"cluster\": {\"worker\": [\"localhost:7779\"], \"ps\": [\"localhost:7777\"]}, \"task\": {\"index\": 0, \"type\": \"ps\"}}' ).\r\nAnd it also produces same errors when I tried to apply CentralStorageStrategy.\r\n```\r\nTraceback (most recent call last):\r\n  File \"temp.py\", line 48, in <module>\r\n    multi_worker_model = build_and_compile_cnn_model()\r\n  File \"temp.py\", line 35, in build_and_compile_cnn_model\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 114, in __init__\r\n    self.add(layer)\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 178, in add\r\n    layer(x)\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 817, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2141, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 165, in build\r\n    dtype=self.dtype)\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2311, in __setattr__\r\n    if val.trainable:\r\n  File \"/home/elzino/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 477, in trainable\r\n    raise NotImplementedError\r\nNotImplementedError\r\n```\r\n\r\n**Describe the expected behavior**\r\nI should work well like when using CPU.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Could you provide more details on how you run with CPU and GPU? ParameterServerStrategy is only supported with Estimator API, not Keras.", "I used environment variable CUDA_VISIBLE_DEVICES to switch between CPU and GPU. Like ```CUDA_VISIBLE_DEVICES= python above_code.py``` or ```CUDA_VISIBLE_DEVICES=0,1,2,3,4 python above_code.py```.\r\nIs there any plan about when to support Keras? I think it will be easily done because it already works well with CPU. Recently i'm digging into tf distribute codes. Can i contribute to this part if there is any chance?", "I found out the reason. When using ParameterServerStrategy, it wraps the value with  AggregatingVariable but AggregatingVariable doesn't have the trainable property. I fixed this issue in above PR. Please check that.\r\n\r\n\r\n### More detail:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/parameter_server_strategy.py#L392\r\n\r\nWhen using CPU, self._num_replicas_in_sync was not more than one. So it just created value without the AggregatingVariable wrapper. But when using GPU, It wrapped the value with AggregatingVariable class, and the errors came up.", "also related to https://github.com/tensorflow/tensorflow/issues/35017", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35442\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35442\">No</a>\n", "Thanks for triage the issue. We're actively working on PS strategy and there're still some design discussion happening. Before that, you can use CentralStorageStrategy if you only have one worker.", "Thank you for letting me know. Actually, I'm working to make [parallax](https://github.com/snuspl/parallax) compatible with tf 2.0. So i need to use more than one worker. But still thanks for your comment and hope your work goes well!"]}, {"number": 35441, "title": "kears.layers.concatenate Does Not Work when Saving a Model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): A custom model\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version:  3.75\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0.1\r\n- GPU model and memory: Quadro K620M\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI built a custom model as follows\r\n\r\n```\r\n\tclass C3BR(tf.keras.Model):\r\n\t\t''' 3D Convolution + Batch Normalisation + Relu '''\r\n\t\tdef __init__(self, filterNum, kSize, strSize, padMode):\r\n\t\t\tsuper(C3BR, self).__init__()\r\n\t\t\tself.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')\r\n\t\t\tself.BN = layers.BatchNormalization(axis=1)\r\n\t\t\r\n\t\tdef call(self, inputs, ifTrain=True):\r\n\t\t\tx = self.conv(inputs)\r\n\t\t\tif ifTrain == True:\r\n\t\t\t\tx= self.BN(x)\r\n\t\t\treturn activations.relu(x)\r\n\r\n\t\tdef build_model(self, input_shape):\r\n\t\t\t''' A work-around to define dimensions of signals through the NN'''\r\n\t\t\tself.build(input_shape)\r\n\t\t\tinputs = tf.keras.Input(shape=input_shape[1:])\r\n\t\t\t_ = self.call(inputs) \r\n\r\n\tclass SimpleUNet1(tf.keras.Model):\r\n\t\t\"\"\"\r\n\t\tSerialise basic units so as to build up a double-layered encoder-decoder U-Net\r\n\t\tInput:\r\n\t\t\tinDim: (for initialisation) [modaility/channel, tensor dimensions]\r\n\t\t\tclassNum: background included\r\n\t\t\tname: name for the net\r\n\t\t\tinputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order\r\n\t\t\tinput_shape: a 1X5 tuple [mbSize, modaility/channel, tensor dimensions]\r\n\t\t\tifTrain: True for training, and False for validation and testing\r\n\t\tReturns:\r\n\t\t\toutputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]\r\n\t\t\"\"\"\r\n\t\tdef __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):\r\n\t\t\tsuper(SimpleUNet1, self).__init__(name=name, **kwarg)\r\n\t\t\tself.inDim = inDim\r\n\t\t\tself.classNum = classNum\r\n\t\t\tdimEnSt1End = np.array(inDim)[2:]-2-2\r\n\t\t\tdimEnSt2Ed = dimEnSt1End/2-2-2\r\n\t\t\tdimBridgeEnd = (dimEnSt2Ed/2-2-2)*2\r\n\t\t\tdimDEStd1End = (dimBridgeEnd-2-2)*2\r\n\t\t\tself.outDim = dimDEStd1End-2-2-2\r\n\t\t\ttemp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')\r\n\t\t\tcrop3d1 = tuple(np.tile(temp, (2, 1)).T)\r\n\t\t\ttemp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')\r\n\t\t\tcrop3d2 = tuple(np.tile(temp, (2, 1)).T)\r\n\r\n\t\t\tself.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')\r\n\t\t\tself.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')\r\n\t\t\tself.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n\t\t\tself.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n\t\t\tself.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')\r\n\t\t\tself.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n\t\t\tself.bridge_cbr1 = C3BR(128, 3, 1, 'valid')\r\n\t\t\tself.bridge_cbr2 = C3BR(256, 3, 1, 'valid')    \r\n\t\t\tself.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')\r\n\t\t\tself.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')\r\n\t\t\tself.de_st1_concat = layers.Concatenate(axis=1)\r\n\t\t\tself.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')\r\n\t\t\tself.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')    \r\n\t\t\tself.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')\r\n\t\t\tself.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')\r\n\t\t\tself.de_st2_concat = layers.Concatenate(axis=1)\r\n\t\t\tself.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n\t\t\tself.de_st2_cbr2 = C3BR(64, 3, 1, 'valid') \r\n\t\t\tself.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')\r\n\t\t\t\t\t\r\n\t\t#@tf.function\r\n\t\tdef call(self, inputs, ifTrain=True):\r\n\t\t\tx0 = self.en_st1_cbr1(inputs, ifTrain)\r\n\t\t\txEnSt1End = self.en_st1_cbr2(x0, ifTrain)\r\n\t\t\tx1 = self.en_st2_mp(xEnSt1End)\r\n\t\t\tx2 = self.en_st2_cbr1(x1, ifTrain)\r\n\t\t\txEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)\r\n\t\t\tx3 = self.bridge_mp(xEnSt2Ed)  \r\n\t\t\tx4 = self.bridge_cbr1(x3, ifTrain)\r\n\t\t\tx5 = self.bridge_cbr2(x4, ifTrain)    \r\n\t\t\txBridgeEnd = self.bridge_tconv1(x5)\r\n\t\t\txCrop1 = self.de_3dcrop1(xEnSt2Ed)\r\n\t\t\tprint(xBridgeEnd.shape)\r\n\t\t\tprint(xCrop1.shape)\r\n\t\t\tx6 = self.de_st1_concat([xBridgeEnd, xCrop1])\r\n\t\t\tprint(x6.shape)\r\n\t\t\tx7 = self.de_st1_cbr1(x6, ifTrain)\r\n\t\t\tx8 = self.de_st1_cbr2(x7, ifTrain)\r\n\t\t\txDeSt1End = self.de_st1_tconv1(x8)\r\n\t\t\txCrop2 = self.de_3dcrop2(xEnSt1End)\r\n\t\t\tx9 = self.de_st2_concat([xDeSt1End, xCrop2])\r\n\t\t\tx10 = self.de_st2_cbr1(x9, ifTrain)\r\n\t\t\tx11 = self.de_st2_cbr2(x10, ifTrain)\r\n\t\t\tx12 = self.final_conv3D(x11)\r\n\t\t\toutputs = activations.softmax(x12, axis=1)\r\n\t\t\t\r\n\t\t\treturn outputs\r\n\t\t\t\r\n\t\tdef build_model(self, input_shape):\r\n\t\t\t''' A work-around to define dimensions of signals through the NN'''\r\n\t\t\tself.build(input_shape)\r\n\t\t\tinputs = tf.keras.Input(shape=input_shape[1:])\r\n\r\n\t\t\t_ = self.call(inputs)\r\n\t\t\t\r\n\t\tdef compute_output_shape(self):\r\n\t\t\t# Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.\r\n\t\t\treturn tf.TensorShape(np.append(self.classNum, self.outDim))    \r\n\r\n```\r\n\r\nPlease pay attention to the following two definitions. If I use concatenate layer in this way (the C is a capitalised one), when I try and save the model, it works\r\n```\r\n\tself.de_st1_concat = layers.Concatenate(axis=1)\r\n\tself.de_st2_concat = layers.Concatenate(axis=1)\r\n```\r\nFor instance,\r\n```\r\n\tmodelInDim = (4, 64, 64, 64)\r\n\tclassNum = 2\r\n\tmbSize = 2\r\n\tTUNet = SimpleUNet1(modelInDim, classNum)\r\n\tTUNet.build_model(input_shape=(mbSize,)+modelInDim)\r\n\tx=tf.random.uniform((mbSize, 4, 64, 64, 64))\r\n\ty=TUNet(x)\r\n\tTUNet.summary()\r\n\tTUNet._set_inputs(x)\r\n\tTUNet.save(r'...\\TTweight', save_format='tf') \r\n```\r\n\r\nBut if, as per [this page](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate?hl=en&version=stable) I use layers.concatenate (small 'c') to generate signals x6 and x9, respectively, that is\r\n```\r\n\tx6 = layers.concatenate([xBridgeEnd, xCrop1], axis=1)\r\n\tx9 = layers.concatenate([xDeSt1End, xCrop2], axis=1)\r\n```\r\nThen save the model in the same way above, it raised an error\r\n```\r\n\t  ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 256, None, None, None), (None, 128, 18, 18, 18)]\r\n```\r\nThe detailed log is [here](https://github.com/tensorflow/tensorflow/issues/19241)\r\n\r\nIt takes me almost an entire day to figure out the root cause. In conclusion, I think the second one may have to be deprecated; otherwise users like me will be confused and misguided.", "comments": ["@gadagashwini @gowthamkpr \r\nI encountered a further issue when trying to save the model under the mixed precision version. It quite confusing. I followed instructions at [here](https://www.tensorflow.org/guide/keras/mixed_precision#training_the_model_with_a_custom_training_loop)\r\nSo I did the following\r\n```\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\ncurOpt = tf.keras.optimizers.Adam(learning_rate=lr)\r\ncurOpt = mixed_precision.LossScaleOptimizer(curOpt, loss_scale='dynamic')\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n# query if it has been well set\r\nprint('Compute dtype: %s' % policy.compute_dtype)\r\nprint('Variable dtype: %s' % policy.variable_dtype)\r\nfrom MedNN import SimpleUNet\r\nTUNet = SimpleUNet(modelInDim, classNum)\r\nTUNet.build_model(input_shape=(mbSize,)+modelInDim)\r\n```\r\nThen if I use a test signal x0 of tf.float32 to do the following:\r\n```\r\nx0 =tf.random.uniform((mbSize, 4, 64, 64, 64), dtype=tf.float32)\r\ny=TUNet(x0, True)\r\nTUNet.summary()\r\nTUNet._set_inputs(x0, True)\r\n# Method #1\r\ntf.saved_model.save(TUNet, r'...\\TUNet_Test')\r\n# Method #2\r\nTUNet.save(r'...\\TUNet_Test', save_format='tf') \r\n```\r\nWhether I ran method #1 or 2, an error was raised. It seems the dtype does not match\r\n```\r\nValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'conv3d/Cast:0' shape=(None, 4, 64, 64, 64) dtype=float16>,)), input_signature ((TensorSpec(shape=(None, 4, None, None, None), dtype=tf.float32, name=None),))\r\n```\r\nEven if I cast x0 into tf.float16, the same error occurs.\r\n\r\nSo my questions are:\r\n1. Is it a bug about tf's functionality of saving custom model or mixed precision?\r\n2. Can I use mixed precision for custom models safely?\r\n3. If it is neither, how can I fix the problem?\r\nI am aware that multiple issues are involved here, and I might need to report them in different issues, Yet they are tightly coupled as presented above, hence could they be scrutinised here in this issue altogether? Many thanks, indeed!\r\n", "At the same time, I would like to say that I surmise that loads of people tend to use custom layers, custom loops, custom models, and custom training, but tf's doc seems not quite to keep apace well. For example, in the mixed precision guide (the link above), it uses the toy example of digit recognition, then states the following:\r\n![\u6355\u83b7](https://user-images.githubusercontent.com/51472988/71657145-9e7bea80-2d0c-11ea-8a97-4c907a9d8fe2.JPG)\r\n\r\n1. Why can't the doc use a more explicit or articulating example to state such a delicate and important case?\r\n2. The underlined sentence is quite confusing. Will the model do it automatically, or shall the user do it, for example in the call() function of a custom model? ", "Sorry, I mistakenly closed the issue by pressing the wrong keys....\r\nAnother issue about loading custom models:\r\nAfter saving it in tf format as I mentioned in the first post, I load it by\r\n```\r\nnewModel= tf.keras.models.load_model(...path)\r\n```\r\nThe path contains the following\r\n![image](https://user-images.githubusercontent.com/51472988/71661707-105c3000-2d1d-11ea-8d66-37722cd4e8a5.png)\r\nPlease note that my model above can run and can be trained--working properly. Yet tf prompted\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"input_1_5:0\", shape=(None, 4, 64, 64, 64), dtype=float32)\r\n  Keyword arguments: {'training': <tf.Tensor 'keras_learning_phase:0' shape=() dtype=bool>}\r\n\r\nExpected these arguments to match one of the following 2 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 4, 64, 64, 64), dtype=tf.float32, name='input_1')\r\n    * False\r\n    * False\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 4, 64, 64, 64), dtype=tf.float32, name='input_1')\r\n    * False\r\n    * True\r\n  Keyword arguments: {}\r\n```\r\nMay you be kind to take a look? Many thanks.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441\">No</a>\n", "When I try to run the code in the original post, I get the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"issue35441.py\", line 118, in <module>\r\n    TUNet = SimpleUNet1(modelInDim, classNum)\r\n  File \"issue35441.py\", line 63, in __init__\r\n    self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')\r\n  File \"/usr/local/google/home/reedwm/venvs/tf2_0/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 2565, in __init__\r\n    'Found: ' + str(cropping))\r\nValueError: `cropping` should have 3 elements. Found: (array([4, 4], dtype=int32), array([4, 4], dtype=int32))\r\n```\r\n\r\nI am unfamiliar with UNet so I'm not sure what's wrong. Also, it would help if you post a single self-contained example to reproduce the issue. In your original post, you split the code over several code blocks, and you don't have the import statements like `import tensorflow as tf`.\r\n\r\nCan you post a self-contained example to reproduce the original issue?\r\n\r\nAs for your mixed-precision related questions:\r\n\r\n> 1. Is it a bug about tf's functionality of saving custom model or mixed precision?\r\n\r\nIt's probably a bug with the combination of the two features. But this may have been fixed by cd6184047e9e497955c473b88387b54818ff23a0 so try again with the latest nightly build. You can install a nightly build with `pip install tf-nightly`.\r\n\r\n> 2. Can I use mixed precision for custom models safely?\r\n\r\nYes, this should work.\r\n\r\n> Why can't the doc use a more explicit or articulating example to state such a delicate and important case?\r\n\r\nAgreed this point is not emphasized much. I did try to make this clear, as earlier in the doc, there is a sentence stating the same thing: \"They cast their inputs to float16 in order to do float16 computations, which causes their outputs to be float16 as a result.\" After that sentence, there is an example showing the output is float16.\r\n\r\nIt was tough to balance having a lot of explanation on details like that, and having the doc be too long. I will see if I can make this more clear but I also don't want to add another paragraph as the doc is already very long.\r\n\r\n> The underlined sentence is quite confusing. Will the model do it automatically, or shall the user do it, for example in the call() function of a custom model?\r\n\r\nThe model will automatically do it. I'll also see if I can add a sentence stating this to the doc. This is useful information, but it's tough to find a good place to mention this.\r\n\r\n", "@reedwm Sorry for the confusion, here is the complete code, which I have tested via Colab.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import layers, activations\r\n\r\nclass C3BR(tf.keras.Model):\r\n\t\t''' 3D Convolution + Batch Normalisation + Relu '''\r\n\t\tdef __init__(self, filterNum, kSize, strSize, padMode):\r\n\t\t\tsuper(C3BR, self).__init__()\r\n\t\t\tself.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')\r\n\t\t\tself.BN = layers.BatchNormalization(axis=1)\r\n\t\t\r\n\t\tdef call(self, inputs, ifTrain=True):\r\n\t\t\tx = self.conv(inputs)\r\n\t\t\tif ifTrain == True:\r\n\t\t\t\tx= self.BN(x)\r\n\t\t\treturn activations.relu(x)\r\n\r\n\t\tdef build_model(self, input_shape):\r\n\t\t\t''' A work-around to define dimensions of signals through the NN'''\r\n\t\t\tself.build(input_shape)\r\n\t\t\tinputs = tf.keras.Input(shape=input_shape[1:])\r\n\t\t\t_ = self.call(inputs) \r\n\r\nclass SimpleUNet1(tf.keras.Model):\r\n\t\"\"\"\r\n\tSerialise basic units so as to build up a double-layered encoder-decoder U-Net\r\n\tInput:\r\n\t\tinDim: (for initialisation) [modaility/channel, tensor dimensions]\r\n\t\tclassNum: background included\r\n\t\tname: name for the net\r\n\t\tinputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order\r\n\t\tinput_shape: a 1X5 tuple [mbSize, modaility/channel, tensor dimensions]\r\n\t\tifTrain: True for training, and False for validation and testing\r\n\tReturns:\r\n\t\toutputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]\r\n\t\"\"\"\r\n\tdef __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):\r\n\t\tsuper(SimpleUNet1, self).__init__(name=name, **kwarg)\r\n\t\tself.inDim = inDim\r\n\t\tself.classNum = classNum\r\n\t\tdimEnSt1End = np.array(inDim)[1:]-2-2\r\n\t\tdimEnSt2Ed = dimEnSt1End/2-2-2\r\n\t\tdimBridgeEnd = (dimEnSt2Ed/2-2-2)*2\r\n\t\tdimDEStd1End = (dimBridgeEnd-2-2)*2\r\n\t\tself.outDim = dimDEStd1End-2-2-2\r\n\t\ttemp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')\r\n\t\tcrop3d1 = tuple(np.tile(temp, (2, 1)).T)\r\n\t\ttemp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')\r\n\t\tcrop3d2 = tuple(np.tile(temp, (2, 1)).T)\r\n\r\n\t\tself.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')\r\n\t\tself.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')\r\n\t\tself.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n\t\tself.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n\t\tself.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')\r\n\t\tself.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n\t\tself.bridge_cbr1 = C3BR(128, 3, 1, 'valid')\r\n\t\tself.bridge_cbr2 = C3BR(256, 3, 1, 'valid')    \r\n\t\tself.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')\r\n\t\tself.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')\r\n\t\tself.de_st1_concat = layers.Concatenate(axis=1)\r\n\t\tself.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')\r\n\t\tself.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')    \r\n\t\tself.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')\r\n\t\tself.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')\r\n\t\tself.de_st2_concat = layers.Concatenate(axis=1)\r\n\t\tself.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n\t\tself.de_st2_cbr2 = C3BR(64, 3, 1, 'valid') \r\n\t\tself.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')\r\n\t\t\t\t\r\n\t#@tf.function\r\n\tdef call(self, inputs, ifTrain=True):\r\n\t\tx0 = self.en_st1_cbr1(inputs, ifTrain)\r\n\t\txEnSt1End = self.en_st1_cbr2(x0, ifTrain)\r\n\t\tx1 = self.en_st2_mp(xEnSt1End)\r\n\t\tx2 = self.en_st2_cbr1(x1, ifTrain)\r\n\t\txEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)\r\n\t\tx3 = self.bridge_mp(xEnSt2Ed)  \r\n\t\tx4 = self.bridge_cbr1(x3, ifTrain)\r\n\t\tx5 = self.bridge_cbr2(x4, ifTrain)    \r\n\t\txBridgeEnd = self.bridge_tconv1(x5)\r\n\t\txCrop1 = self.de_3dcrop1(xEnSt2Ed)\r\n\t\tprint(xBridgeEnd.shape)\r\n\t\tprint(xCrop1.shape)\r\n\t\tx6 = self.de_st1_concat([xBridgeEnd, xCrop1])\r\n\t\tprint(x6.shape)\r\n\t\tx7 = self.de_st1_cbr1(x6, ifTrain)\r\n\t\tx8 = self.de_st1_cbr2(x7, ifTrain)\r\n\t\txDeSt1End = self.de_st1_tconv1(x8)\r\n\t\txCrop2 = self.de_3dcrop2(xEnSt1End)\r\n\t\tx9 = self.de_st2_concat([xDeSt1End, xCrop2])\r\n\t\tx10 = self.de_st2_cbr1(x9, ifTrain)\r\n\t\tx11 = self.de_st2_cbr2(x10, ifTrain)\r\n\t\tx12 = self.final_conv3D(x11)\r\n\t\toutputs = activations.softmax(x12, axis=1)\r\n\t\t\r\n\t\treturn outputs\r\n\t\t\r\n\tdef build_model(self, input_shape):\r\n\t\t''' A work-around to define dimensions of signals through the NN'''\r\n\t\tself.build(input_shape)\r\n\t\tinputs = tf.keras.Input(shape=input_shape[1:])\r\n\r\n\t\t_ = self.call(inputs)\r\n\t\t\r\n\tdef compute_output_shape(self):\r\n\t\t# Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.\r\n\t\treturn tf.TensorShape(np.append(self.classNum, self.outDim))    \r\n\r\nmodelInDim = (4, 64, 64, 64)\r\nclassNum = 2\r\nmbSize = 2\r\nTUNet = SimpleUNet1(modelInDim, classNum)\r\nTUNet.build_model(input_shape=(mbSize,)+modelInDim)\r\nTUNet.summary()\r\nTUNet.save(r'e:\\TTweight', save_format='tf') \r\n```\r\nThe above works in saving the moel\r\nIf we change the code above into\r\n```\r\nx6 = layers.concatenate([xBridgeEnd, xCrop1], axis=1)\r\nx9 = layers.concatenate([xDeSt1End, xCrop2], axis=1)\r\n```\r\nAn error was raised as stated before.\r\n\r\nHas the solution for saving models under mixed precision been submitted to Tf2.1 as well?\r\n\r\nBy the way, at present I am saving neither model nor weight. I simply save everything to a checkpoint, and it can be loaded back. It is fine as long as I work in Python environment and have full access to original code for building up the model, but not export model to C++ etc.\r\n\r\nFinally, I would say as I proceed in the doc (basically I read all of the guide, the tutorial, as well as 2.0.0 API except the RNN and reinforcement learning part that are not my field), I have got a strong feeling that the writing style in different parts differ a lot. Sometimes semi-formal, sometimes informal, even colloquial. I, being a non-native English speaker, am afraid to say there are even some grammatical mistakes and pdigin English, which do not hinder engineers from understanding the stuff though, are flaws. Hope Google will pay some attentions to it.", "Thanks for the example to reproduce! However, when I try running the new example (without changing the x6 and x9 lines to the version), I get the error:\r\n\r\n```\r\nValueError: Model <__main__.SimpleUNet1 object at 0x7f7ea76ec690> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).\r\n```\r\n\r\nCan you double check the example? I got that above error in TF 2.0, TF 2.1, and the nightly TF.\r\n\r\n> Has the solution for saving models under mixed precision been submitted to Tf2.1 as well?\r\n\r\nUnfortunately no. The fix will only be in TF 2.2 and above.\r\n\r\n> Finally, I would say as I proceed in the doc (basically I read all of the guide, the tutorial, as well as 2.0.0 API except the RNN and reinforcement learning part that are not my field), I have got a strong feeling that the writing style in different parts differ a lot. Sometimes semi-formal, sometimes informal, even colloquial. I, being a non-native English speaker, am afraid to say there are even some grammatical mistakes and pdigin English, which do not hinder engineers from understanding the stuff though, are flaws. Hope Google will pay some attentions to it.\r\n\r\nThanks for the feedback. Ideally all documentation should conform to the [Google documentation style guide](https://developers.google.com/style). Feel free to point out different docs where you think the writing style differs a lot, or where there are grammatical mistakes. You can also file a new issue for this. The docs are written by different people so style may differ, but we try to keep the style consistent.", "@reedwm I see. Well, for some reason I do not know, we have to run a dummy signal through the net as follows before saving the model\r\n```\r\nx=tf.random.uniform((mbSize, 4, 64, 64, 64))\r\ny=TUNet(x)\r\n# use your directory in lieu of r'...\\TTweight'\r\nTUNet.save(r'...\\TTweight', save_format='tf') \r\n```\r\nor\r\n```\r\nx=tf.random.uniform((mbSize, 4, 64, 64, 64))\r\nTUNet._set_inputs(x)\r\nTUNet.save(r'...\\TTweight', save_format='tf') \r\n```", "With your second code sample, with `TUNet._set_inputs(x)`, I was able to get the example to work. Once I replace `x6` and `x9` though, I get a different error on the `TUNet.summary()` line. If I comment the summary line out, I can reproduce your original error in TF 2.0 (\"A `Concatenate` layer requires inputs with matching shapes...\"). However, I cannot reproduce in TF 2.1. Therefore, I'm assuming the original issue is fixed so I'm closing it.\r\n\r\nHowever, there still may be a bug, as the summary doesn't work. Feel free to file another bug about that if you think that is an issue, with a self-contained example to reproduce. If you do file such a bug, consider CCing me (add the line \"/CC @reedwm\") as I have some context and can therefore more easily triage to someone else.\r\n\r\nFor reference, here is the code sample I ran that works in TF 2.0 but not TF 2.1. It is the same as the code sample you provided, except i called `_set_inputs` as you suggested, I commented out `TUNet.summary()`, and I replaced `x6` and `x9` as you suggested.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import layers, activations\r\n\r\nclass C3BR(tf.keras.Model):\r\n  ''' 3D Convolution + Batch Normalisation + Relu '''\r\n  def __init__(self, filterNum, kSize, strSize, padMode):\r\n    super(C3BR, self).__init__()\r\n    self.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')\r\n    self.BN = layers.BatchNormalization(axis=1)\r\n\r\n  def call(self, inputs, ifTrain=True):\r\n    x = self.conv(inputs)\r\n    if ifTrain == True:\r\n      x= self.BN(x)\r\n    return activations.relu(x)\r\n\r\n  def build_model(self, input_shape):\r\n    ''' A work-around to define dimensions of signals through the NN'''\r\n    self.build(input_shape)\r\n    inputs = tf.keras.Input(shape=input_shape[1:])\r\n    _ = self.call(inputs)\r\n\r\nclass SimpleUNet1(tf.keras.Model):\r\n  \"\"\"\r\n  Serialise basic units so as to build up a double-layered encoder-decoder U-Net\r\n  Input:\r\n    inDim: (for initialisation) [modaility/channel, tensor dimensions]\r\n    classNum: background included\r\n    name: name for the net\r\n    inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order\r\n    input_shape: a 1X5 tuple [mbSize, modaility/channel, tensor dimensions]\r\n    ifTrain: True for training, and False for validation and testing\r\n  Returns:\r\n    outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]\r\n  \"\"\"\r\n  def __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):\r\n    super(SimpleUNet1, self).__init__(name=name, **kwarg)\r\n    self.inDim = inDim\r\n    self.classNum = classNum\r\n    dimEnSt1End = np.array(inDim)[1:]-2-2\r\n    dimEnSt2Ed = dimEnSt1End/2-2-2\r\n    dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2\r\n    dimDEStd1End = (dimBridgeEnd-2-2)*2\r\n    self.outDim = dimDEStd1End-2-2-2\r\n    temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')\r\n    crop3d1 = tuple(np.tile(temp, (2, 1)).T)\r\n    temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')\r\n    crop3d2 = tuple(np.tile(temp, (2, 1)).T)\r\n\r\n    self.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')\r\n    self.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')\r\n    self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n    self.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n    self.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')\r\n    self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n    self.bridge_cbr1 = C3BR(128, 3, 1, 'valid')\r\n    self.bridge_cbr2 = C3BR(256, 3, 1, 'valid')\r\n    self.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')\r\n    self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')\r\n    self.de_st1_concat = layers.Concatenate(axis=1)\r\n    self.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')\r\n    self.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')\r\n    self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')\r\n    self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')\r\n    self.de_st2_concat = layers.Concatenate(axis=1)\r\n    self.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n    self.de_st2_cbr2 = C3BR(64, 3, 1, 'valid')\r\n    self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')\r\n\r\n  #@tf.function\r\n  def call(self, inputs, ifTrain=True):\r\n    x0 = self.en_st1_cbr1(inputs, ifTrain)\r\n    xEnSt1End = self.en_st1_cbr2(x0, ifTrain)\r\n    x1 = self.en_st2_mp(xEnSt1End)\r\n    x2 = self.en_st2_cbr1(x1, ifTrain)\r\n    xEnSt2Ed = self.en_st2_cbr2(x2, ifTrain)\r\n    x3 = self.bridge_mp(xEnSt2Ed)\r\n    x4 = self.bridge_cbr1(x3, ifTrain)\r\n    x5 = self.bridge_cbr2(x4, ifTrain)\r\n    xBridgeEnd = self.bridge_tconv1(x5)\r\n    xCrop1 = self.de_3dcrop1(xEnSt2Ed)\r\n    print(xBridgeEnd.shape)\r\n    print(xCrop1.shape)\r\n    x6 = layers.concatenate([xBridgeEnd, xCrop1], axis=1)\r\n    print(x6.shape)\r\n    x7 = self.de_st1_cbr1(x6, ifTrain)\r\n    x8 = self.de_st1_cbr2(x7, ifTrain)\r\n    xDeSt1End = self.de_st1_tconv1(x8)\r\n    xCrop2 = self.de_3dcrop2(xEnSt1End)\r\n    x9 = layers.concatenate([xDeSt1End, xCrop2], axis=1)\r\n    x10 = self.de_st2_cbr1(x9, ifTrain)\r\n    x11 = self.de_st2_cbr2(x10, ifTrain)\r\n    x12 = self.final_conv3D(x11)\r\n    outputs = activations.softmax(x12, axis=1)\r\n\r\n    return outputs\r\n\r\n  def build_model(self, input_shape):\r\n    ''' A work-around to define dimensions of signals through the NN'''\r\n    self.build(input_shape)\r\n    inputs = tf.keras.Input(shape=input_shape[1:])\r\n\r\n    _ = self.call(inputs)\r\n\r\n  def compute_output_shape(self):\r\n    # Override this function if one expects to use the subclassed model in Kera's fit() method; Otherwise, it is optional.\r\n    return tf.TensorShape(np.append(self.classNum, self.outDim))\r\n\r\nmodelInDim = (4, 64, 64, 64)\r\nclassNum = 2\r\nmbSize = 2\r\nTUNet = SimpleUNet1(modelInDim, classNum)\r\nTUNet.build_model(input_shape=(mbSize,)+modelInDim)\r\n# TUNet.summary()\r\n\r\nx=tf.random.uniform((mbSize, 4, 64, 64, 64))\r\nTUNet._set_inputs(x)\r\n# use your directory in lieu of r'...\\TTweight'\r\nTUNet.save(r'TTweight', save_format='tf')\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35441\">No</a>\n"]}, {"number": 35440, "title": "could not find 'TrtGraphConverter", "body": " i want to create a TensorRT inference graph directly from my SavedModel. However i met a problem\r\n\"from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nModuleNotFoundError: No module named 'tensorflow.python.compiler\"\r\nwhen i change it into \"from tensorflow.contrib.tensorrt.python import trt_convert as trt\" i met another problem that \"AttributeError: module 'tensorflow.contrib.tensorrt.python.trt_convert' has no attribute 'TrtGraphConverter'\"\r\nmy tensorflow is 1.13.1", "comments": ["@MrCrazyCrab \r\n\r\nI tried in colab with TF 1.13.1 with `from tensorflow.contrib.tensorrt.python import trt_convert as trt`and i am not seeing any issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/729d680a9bdc44c92fb2edf62a72605f/untitled509.ipynb). Thanks!", "@MrCrazyCrab If am not mistaken , TrtGraphConverter did not exist yet in TF 1.13. You would need to update TF to 1.15 or 2.0 I think.\r\nWith TF 1.13, you can use the \"old\" way (which is no longer documented on Nvidia website):\r\n   ```\r\nfrom tensorflow.contrib import tensorrt as trt\r\ntrt_graph = trt.create_inference_graph(...)\r\n```\r\n\r\ncreate_inference_graph has options very similar to TrtGraphConverter", "@jnd77 thank you, i would try later.", "You need to install tensorflow-gpu version. Can you confirm this?\r\nAlso startting TF 1.14, TF-TRT was moved to core from contrib module.\r\nSo in your case you should try;\r\n`from tensorflow.contrib.tensorrt.python import trt_convert as trt`", "@MrCrazyCrab \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I still get the same  **TrtGraphConverter** issue\r\nUbuntu 18.04 \r\nTensorflow version 2.0 GPU supported \r\nand changed `from tensorflow.contrib.tensorrt.python import trt_convert as trt` to `from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\nCan anyone help me with this ?\r\n `", "@saikrishnadas try it on ubuntu not windows!", "@MrCrazyCrab I use Ubuntu 18.04 \r\nError Log:\r\n`AttributeError: module 'tensorflow.python.compiler.tensorrt.trt_convert' has noattribute 'TrtGraphConverterV2'`", "@saikrishnadas uh... maybe you should see if there is a file \"trt_convert.py\" in the /compile/tensorrt/. If not, you should consider whethere  to reinstall the tensorflow.", "@MrCrazyCrab Yep ! I just re-installed tensorflow and that worked. But is it still .pb format after it convert it to tensorRT with savedmodel ? I recently converted my yolo.weight model to onnx and to tensorrt which gave a extension of .trt . \r\n", "@saikrishnadas  yeah, and there is more files after converting.", "@MrCrazyCrab Thanks\r\n"]}, {"number": 35439, "title": "tf.metrics.Mean* metrics miscalculated", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ntf.metrics.MeanAbsoluteError and others compute the mean of means.\r\n\r\n**Describe the expected behavior**\r\nThey should compute the mean of all the data to support iterating over evaluation datasets.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport tensorflow as tf\r\nm=tf.metrics.MeanAbsoluteError()\r\nm.update_state(y_true=[0,0], y_pred=[1,1])\r\nm.update_state(y_true=[0], y_pred=[2])\r\nassert m.result() == 2\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe fix is simple, instead of the mean_squared_error function use a (new) sum_squared_error function in the metric and pass that to MeanMetricWrapper.", "comments": ["The behavior seems expected according to source code since `update_state` accumulates ([\\[1\\]](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/metrics.py#L213), [\\[2\\]](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError?version=stable#update_state)) the errors. You can verify this by checking the value of `m.total` variable after executing each `update_state` statement. I have done so by running the following\r\n```\r\nimport tensorflow as tf\r\nm=tf.metrics.MeanAbsoluteError()\r\nm.update_state(y_true=[0,0], y_pred=[1,1])\r\nprint(m.total.numpy())\r\nm.update_state(y_true=[0], y_pred=[2])\r\nprint(m.total.numpy())\r\nprint(m.result().numpy())\r\n```\r\nThen since state count is 2 and total is 3, the result returned is 1.5. Source code of `result()` is referred from [here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/metrics.py#L361-L371).", "Of course the result matches the source code :) However, I believe the source code does not match the intent.\r\n\r\nIf you do an eval loop like this where the data is sequential hence label is (batch,step,1)-shaped, it won't calculate the MAE/MSE properly:\r\n```\r\nfor feature, label in eval_dataset:\r\n   pred = model.predict(feature)\r\n   metric.update_state(pred, label)\r\n```\r\nThe reason is that \"mean of means of squares\" != \"mean of squares\" as long as in the first term the dimension varies.", "Oh now I got it. Sorry for misinterpreting your question the first time. @gsimko ", "To avoid metric accumulation you may try `reset_states()` method.\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean?version=nightly#reset_states\r\n```python\r\nimport tensorflow as tf\r\nm=tf.metrics.MeanAbsoluteError()\r\nm.update_state(y_true=[0,0], y_pred=[1,1])\r\nm.reset_states()\r\nm.update_state(y_true=[0], y_pred=[2])\r\nassert m.result() == 2\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439\">No</a>\n", "Resetting the state does not solve the problem - during evals one needs to accumulate the metrics. Why was this issue closed?", "@gsimko Thank you for the question. For metrics and losses, we expect that labels and predictions are at least 2D.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError?version=nightly#update_state\r\n```\r\ny_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\r\n        sparse loss functions such as sparse categorical crossentropy where\r\n        shape = `[batch_size, d0, .. dN-1]`\r\ny_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\r\n```\r\n\r\nFor example, let's say y_pred and y_true are of the shape [batch_size, d0], we compute the mean across the last axis in a sample -> this will give us one value per-sample -> `[batch_size]`.  The second mean is computed across all samples across all batches.\r\n\r\n", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@pavithrasv: thanks for the response, that answered it! I also just realized that my original assertion was off (it should have been 4/3 instead of 2), and by following what you described (adding an inner dimension) I get that answer.\r\n\r\nIf someone else finds this problem, this works as expected:\r\nimport tensorflow as tf\r\nm=tf.metrics.MeanAbsoluteError()\r\nm.update_state(y_true=[[0],[0]], y_pred=[[1],[1]])\r\nm.update_state(y_true=[[0]], y_pred=[[2]])\r\nassert (m.result() - 4/3)<1e-6", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35439\">No</a>\n"]}, {"number": 35438, "title": "Add wheel for python3.8 on pypi", "body": "Currently, there is no wheel on pypi for python 3.8\r\n\r\nI strongly hope that there will be support for latest python version.", "comments": ["@stevenleeS0ht ,\r\nHello, We can expect python 3.8 support with TF 2.2.\r\nPlease check  [#33374 ](https://github.com/tensorflow/tensorflow/issues/33374#issuecomment-562663993)Thanks!", "Closing as all information is in #33374", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35438\">No</a>\n"]}, {"number": 35437, "title": "Document undocumented tf.strings methods", "body": "Added API documentation for `tf.strings.lower` and `tf.strings.upper`, which were undocumented.", "comments": ["Yeah let's not do that\n\nOn Fri, Dec 27, 2019 at 10:24 AM Mar\u00e7al Comajoan Cara <\nnotifications@github.com> wrote:\n\n> *@salcc* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/api_def/base_api/api_def_StringLower.pbtxt\n> <https://github.com/tensorflow/tensorflow/pull/35437#discussion_r361720185>\n> :\n>\n> > @@ -1,3 +1,13 @@\n>  op {\n>    graph_op_name: \"StringLower\"\n> +  summary: \"Lowercases strings in `input`\"\n> +  description: <<END\n> +Converts each uppercase character of each string in the input Tensor to\n> +lowercase.\n> +\n> +>>> strings = ['Hello', 'TensorFlow']\n> +>>> tf.strings.lower(strings).numpy()\n>\n> By the way, I used .numpy() because in other places is used to do the\n> same thing, for example in\n> https://github.com/tensorflow/tensorflow/blob/1e65730120aafc413e8c3dcddcf19cd8d184fe1b/tensorflow/core/api_def/base_api/api_def_StringLength.pbtxt#L30\n> (\n> https://www.tensorflow.org/api_docs/python/tf/strings/length?version=nightly\n> ).\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35437?email_source=notifications&email_token=AAABHRL7KTWXGSTMJJJ2PCLQ2ZB53A5CNFSM4J7SMAZKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCQJ4KMQ#discussion_r361720185>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPBF6E3LF67VKJJEPLQ2ZB53ANCNFSM4J7SMAZA>\n> .\n>\n\n\n-- \n - Alex\n", "Similar changes have been merged in this commit `bd6b683` closing this PR , thank you for your contribution."]}, {"number": 35436, "title": "Pin libcublas10=10.2.1.243-1", "body": "There seems to be a regression (or some change) in cublas 10.2.2.89-1\r\n(the latest as of this commit) which prevents cublas from initializing\r\nproperly. This change pins the dockerfiles to a known-working version of libcublas10.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/9489#issuecomment-562394257.\r\nResolves https://github.com/tensorflow/tensorflow/issues/35364.", "comments": []}, {"number": 35435, "title": "tf.math.tanh can produce values outside of `[-1, 1]`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  https://colab.research.google.com/\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0-rc2\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.float32(-8.51089)\r\nprint(tf.math.tanh(x))  # => <tf.Tensor: shape=(), dtype=float32, numpy=-1.0000001>\r\nprint(tf.math.tanh(-x))  # => <tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe output of `tanh(x)` should always be `>= -1` and `<= 1`.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe above code is a minimal repro.\r\n\r\nYou can also reproduce the issue using the following colab: https://colab.research.google.com/drive/1BHGZoBgMzOlG0e3cYhGYEnLPp1_uSpIY .\r\n\r\n\r\n**Other info / logs**\r\n\r\nThis issue was originally filed as https://github.com/tensorflow/tensorflow/issues/34773 against TF 1.15.0 in Colab, but it is not clear if the issue could be reproduced in TF 1.15.0.\r\n\r\nI haven't done an exhaustive search, but it looks like this issue exists in TF 2.1.0-dev20191203 and TF 2.1.0-dev20191218, as well, and was fixed in TF 2.1.0-dev20191219.\r\n\r\n - TF 2.1.0-dev20191218 repro: https://colab.research.google.com/drive/1GfA45tP2Q1ZQDmRIusfz4mmfTA5TW1ne\r\n\r\n - Correct output in TF 2.1.0-dev20191219: https://colab.research.google.com/drive/104qs8RAzWmsHATfCZ-01NbnBDnhqRzv0\r\n\r\n\r\nIf the issue has been fixed in TF 2.1.0-dev20191219,  does this mean the issue will be fixed in TF 2.1.0?\r\n\r\n", "comments": ["@jburnim, Its fixed in latest Tf-nightly (2.1.0.dev20191226). It will be fixed in Future release 2.1.\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/3516941d5de628801730adeb5e632b56/untitled328.ipynb). Thanks!", "I just tested in the newly-released TF 2.1.0, and I'm still getting an incorrect result:\r\n```\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.1.0'\r\n>>> print(tf.math.tanh(x))\r\ntf.Tensor(-1.0000001, shape=(), dtype=float32)\r\n>>> print(tf.math.tanh(-x))\r\ntf.Tensor(1.0000001, shape=(), dtype=float32)", "@jburnim Thanks for reporting this issue. I was able to reproduce the issue in `TF2.1`. However, I cannot reproduce the issue with `tf-nightly`. I am sure this was resolved with `tf-nightly`.\r\n\r\nI am closing the issue as it was resolved in `tf-nightly`, but feel free to reopen the issue if it persists again. Thanks!"]}, {"number": 35434, "title": "Install extra TensorRT dependency", "body": "Resolves the warning noted in https://github.com/tensorflow/tensorflow/issues/35364", "comments": []}, {"number": 35432, "title": "TypeError: Expected Operation, Variable, or Tensor, got None while saving tensorflow model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: '2.0.0'\r\n- Python version: Python 3.7.5 /Conda \r\n- CUDA/cuDNN version: cuda10.0_0/cudnn-7.6.5\r\n- GPU model and memory: Tesla V100-PCIE / 32 GB memory\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n**I am getting TypeError: Expected Operation, Variable, or Tensor, got None while saving the model using model.save('../output/my_model')**\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-49-5ab71d0ebc23> in <module>\r\n----> 1 model.save('../output/my_model')\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    973     \"\"\"\r\n    974     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n--> 975                       signatures, options)\r\n    976 \r\n    977   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    113   else:\r\n    114     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n--> 115                           signatures, options)\r\n    116 \r\n    117 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     72   # default learning phase placeholder.\r\n     73   with K.learning_phase_scope(0):\r\n---> 74     save_lib.save(model, filepath, signatures, options)\r\n     75 \r\n     76   if not include_optimizer:\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    868   if signatures is None:\r\n    869     signatures = signature_serialization.find_function_to_export(\r\n--> 870         checkpoint_graph_view)\r\n    871 \r\n    872   signatures = signature_serialization.canonicalize_signatures(signatures)\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)\r\n     62   # If the user did not specify signatures, check the root object for a function\r\n     63   # that can be made into a signature.\r\n---> 64   functions = saveable_view.list_functions(saveable_view.root)\r\n     65   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)\r\n     66   if signature is not None:\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/saved_model/save.py in list_functions(self, obj)\r\n    139     if obj_functions is None:\r\n    140       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n--> 141           self._serialization_cache)\r\n    142       self._functions[obj] = obj_functions\r\n    143     return obj_functions\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)\r\n   2420   def _list_functions_for_serialization(self, serialization_cache):\r\n   2421     return (self._trackable_saved_model_saver\r\n-> 2422             .list_functions_for_serialization(serialization_cache))\r\n   2423 \r\n   2424 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)\r\n     89         `ConcreteFunction`.\r\n     90     \"\"\"\r\n---> 91     fns = self.functions_to_serialize(serialization_cache)\r\n     92 \r\n     93     # The parent AutoTrackable class saves all user-defined tf.functions, and\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)\r\n     77   def functions_to_serialize(self, serialization_cache):\r\n     78     return (self._get_serialized_attributes(\r\n---> 79         serialization_cache).functions_to_serialize)\r\n     80 \r\n     81   def _get_serialized_attributes(self, serialization_cache):\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)\r\n     92 \r\n     93     object_dict, function_dict = self._get_serialized_attributes_internal(\r\n---> 94         serialization_cache)\r\n     95 \r\n     96     serialized_attr.set_and_validate_objects(object_dict)\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n     45     # cache (i.e. this is the root level object).\r\n     46     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:\r\n---> 47       default_signature = save_impl.default_save_signature(self.obj)\r\n     48 \r\n     49     # Other than the default signature function, all other attributes match with\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)\r\n    204   original_losses = _reset_layer_losses(layer)\r\n    205   fn = saving_utils.trace_model_call(layer)\r\n--> 206   fn.get_concrete_function()\r\n    207   _restore_layer_losses(original_losses)\r\n    208   return fn\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n    774       if self._stateful_fn is None:\r\n    775         initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 776         self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n    777         self._initialize_uninitialized_variables(initializer_map)\r\n    778 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saving_utils.py in _wrapped_model(*args)\r\n    141     with base_layer_utils.call_context().enter(\r\n    142         model, inputs=inputs, build_graph=False, training=False, saving=True):\r\n--> 143       outputs_list = nest.flatten(model(inputs=inputs, training=False))\r\n    144 \r\n    145     try:\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    846                 else:\r\n--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    848 \r\n    849             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/transformers/modeling_tf_albert.py in call(self, inputs, **kwargs)\r\n    783 \r\n    784     def call(self, inputs, **kwargs):\r\n--> 785         outputs = self.albert(inputs, **kwargs)\r\n    786 \r\n    787         pooled_output = outputs[1]\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    846                 else:\r\n--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    848 \r\n    849             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/transformers/modeling_tf_albert.py in call(self, inputs, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, training)\r\n    680 \r\n    681         embedding_output = self.embeddings(\r\n--> 682             [input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\r\n    683         encoder_outputs = self.encoder(\r\n    684             [embedding_output, extended_attention_mask, head_mask], training=training)\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    889           with base_layer_utils.autocast_context_manager(\r\n    890               self._compute_dtype):\r\n--> 891             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    892           self._handle_activity_regularization(inputs, outputs)\r\n    893           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in return_outputs_and_add_losses(*args, **kwargs)\r\n     55     inputs = args[inputs_arg_index]\r\n     56     args = args[inputs_arg_index + 1:]\r\n---> 57     outputs, losses = fn(inputs, *args, **kwargs)\r\n     58     layer.add_loss(losses, inputs)\r\n     59     return outputs\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)\r\n    109         training,\r\n    110         lambda: replace_training_and_call(True),\r\n--> 111         lambda: replace_training_and_call(False))\r\n    112 \r\n    113   # Create arg spec for decorated function. If 'training' is not defined in the\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\r\n     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     58   return smart_module.smart_cond(\r\n---> 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     60 \r\n     61 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     54       return true_fn()\r\n     55     else:\r\n---> 56       return false_fn()\r\n     57   else:\r\n     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in <lambda>()\r\n    109         training,\r\n    110         lambda: replace_training_and_call(True),\r\n--> 111         lambda: replace_training_and_call(False))\r\n    112 \r\n    113   # Create arg spec for decorated function. If 'training' is not defined in the\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)\r\n    104     def replace_training_and_call(training):\r\n    105       set_training_arg(training, training_arg_index, args, kwargs)\r\n--> 106       return wrapped_call(*args, **kwargs)\r\n    107 \r\n    108     return tf_utils.smart_cond(\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in __call__(self, *args, **kwargs)\r\n    531     if not self.call_collection.tracing:\r\n    532       self.call_collection.add_trace(*args, **kwargs)\r\n--> 533     return super(LayerCall, self).__call__(*args, **kwargs)\r\n    534 \r\n    535   def get_concrete_function(self, *args, **kwargs):\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    492       # In this case we have not created variables on the first call. So we can\r\n    493       # run the first trace but we should fail if variables are created.\r\n--> 494       results = self._stateful_fn(*args, **kwds)\r\n    495       if self._created_variables:\r\n    496         raise ValueError(\"Creating variables on a non-first call to a function\"\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1820   def __call__(self, *args, **kwargs):\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n-> 1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)\r\n    513         layer, inputs=inputs, build_graph=False, training=training,\r\n    514         saving=True):\r\n--> 515       ret = method(*args, **kwargs)\r\n    516     _restore_layer_losses(original_losses)\r\n    517     return ret\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)\r\n    109         training,\r\n    110         lambda: replace_training_and_call(True),\r\n--> 111         lambda: replace_training_and_call(False))\r\n    112 \r\n    113   # Create arg spec for decorated function. If 'training' is not defined in the\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\r\n     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     58   return smart_module.smart_cond(\r\n---> 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     60 \r\n     61 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     54       return true_fn()\r\n     55     else:\r\n---> 56       return false_fn()\r\n     57   else:\r\n     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in <lambda>()\r\n    109         training,\r\n    110         lambda: replace_training_and_call(True),\r\n--> 111         lambda: replace_training_and_call(False))\r\n    112 \r\n    113   # Create arg spec for decorated function. If 'training' is not defined in the\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)\r\n    104     def replace_training_and_call(training):\r\n    105       set_training_arg(training, training_arg_index, args, kwargs)\r\n--> 106       return wrapped_call(*args, **kwargs)\r\n    107 \r\n    108     return tf_utils.smart_cond(\r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(inputs, *args, **kwargs)\r\n    555   layer_call = _get_layer_call_method(layer)\r\n    556   def call_and_return_conditional_losses(inputs, *args, **kwargs):\r\n--> 557     return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)\r\n    558   return _create_call_fn_decorator(layer, call_and_return_conditional_losses)\r\n    559 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in get_losses_for(self, inputs)\r\n   1382     losses = [l for l in self.losses if not l._unconditional_loss]\r\n   1383     inputs = nest.flatten(inputs)\r\n-> 1384     reachable = tf_utils.get_reachable_from_inputs(inputs, losses)\r\n   1385     return [l for l in losses if l in reachable]\r\n   1386 \r\n\r\n/app/AI_RD/conda/envs/cont_tag_sup/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in get_reachable_from_inputs(inputs, targets)\r\n    132       outputs = x.consumers()\r\n    133     else:\r\n--> 134       raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\r\n    135 \r\n    136     for y in outputs:\r\n\r\nTypeError: Expected Operation, Variable, or Tensor, got None\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n`import tensorflow as tf`\r\n`import pandas as pd`\r\n\r\n`from sklearn.model_selection import train_test_split`\r\n\r\n`import transformers`\r\n`from transformers import AlbertConfig, AlbertTokenizer, TFAlbertForSequenceClassification,glue_convert_examples_to_features `\r\n`!pip install transformers`\r\n\r\n`data_df = pd.read_excel(\"../input/test.xlsx\")`\r\n`model_dir = '../input/albert_xxlarge_v2/'`\r\n`EPOCHS = 3`\r\n`MAX_SEQ_LENGTH = 256`\r\n`label_list = [0,1]`\r\n\r\n`config = AlbertConfig.from_pretrained('albert-xxlarge-v2')`\r\n`tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2', cache_dir=model_dir)`\r\n`model = TFAlbertForSequenceClassification.from_pretrained('albert-xxlarge-v2', ``cache_dir=model_dir, config=config)`\r\n\r\n`train_df, test_df = train_test_split(data_df[['id','text1', 'text2', 'LABEL']], \r\n                                                    random_state=42, shuffle=True, \r\n                                                    test_size=0.20, stratify=data_df['LABEL'])`\r\n\r\n`train_InputExamples = train_df.apply(lambda x: InputExample(guid=x['id'],\r\n                                                                           text_a=x['text1'],\r\n                                                                           text_b=x['text2'],\r\n                                                                           label=x['LABEL']), axis=1)`\r\n\r\n`train_dataset = glue_convert_examples_to_features(examples=train_InputExamples, tokenizer=tokenizer,\r\n                                                  max_length=MAX_SEQ_LENGTH,\r\n                                                  label_list = label_list, output_mode=\"classification\")`\r\n\r\n`optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)`\r\n`loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\r\n`metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')`\r\n\r\n`input_ids_train = []`\r\n`attention_mask_train = []`\r\n`token_type_ids_train = []`\r\n`output_label_train = []`\r\n`for f in train_dataset:`\r\n    `input_ids_train.append(f.input_ids)`\r\n    `attention_mask_train.append(f.attention_mask)`\r\n    `token_type_ids_train.append(f.token_type_ids)`\r\n    `output_label_train.append(f.label)`\r\n`model.compile(optimizer=optimizer, loss=loss, metrics=[metric])`\r\n\r\n`input_ids_train = np.array(input_ids_train)`\r\n`attention_mask_train = np.array(attention_mask_train)`\r\n`token_type_ids_train = np.array(token_type_ids_train)`\r\n`output_label_train = np.array(output_label_train)`\r\n\r\n`model.fit([input_ids_train,attention_mask_train, token_type_ids_train], y=output_label_train,\r\n          epochs = EPOCHS, batch_size=4)`\r\n\r\n`model.save('../output/my_model')`", "comments": ["The training of the model is successful, but getting errors only while saving the model", "@jonanem \r\n\r\nCan you please help us with minimal standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Below is the minimal code to reproduce the issue\r\n\r\n`import tensorflow as tf`\r\n`import pandas as pd`\r\n\r\n`from sklearn.model_selection import train_test_split`\r\n\r\n`import transformers`\r\n`from transformers import AlbertConfig, AlbertTokenizer, TFAlbertForSequenceClassification,glue_convert_examples_to_features `\r\n`!pip install transformers`\r\n\r\n`data_df = pd.read_excel(\"../input/test.xlsx\")`\r\n`model_dir = '../input/albert_xxlarge_v2/'`\r\n`EPOCHS = 3`\r\n`MAX_SEQ_LENGTH = 256`\r\n`label_list = [0,1]`\r\n\r\n`config = AlbertConfig.from_pretrained('albert-xxlarge-v2')`\r\n`tokenizer = AlbertTokenizer.from_pretrained('albert-xxlarge-v2', cache_dir=model_dir)`\r\n`model = TFAlbertForSequenceClassification.from_pretrained('albert-xxlarge-v2', ``cache_dir=model_dir, config=config)`\r\n\r\n`train_df, test_df = train_test_split(data_df[['id','text1', 'text2', 'LABEL']], \r\n                                                    random_state=42, shuffle=True, \r\n                                                    test_size=0.20, stratify=data_df['LABEL'])`\r\n\r\n`train_InputExamples = train_df.apply(lambda x: InputExample(guid=x['id'],\r\n                                                                           text_a=x['text1'],\r\n                                                                           text_b=x['text2'],\r\n                                                                           label=x['LABEL']), axis=1)`\r\n\r\n`train_dataset = glue_convert_examples_to_features(examples=train_InputExamples, tokenizer=tokenizer,\r\n                                                  max_length=MAX_SEQ_LENGTH,\r\n                                                  label_list = label_list, output_mode=\"classification\")`\r\n\r\n`optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)`\r\n`loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\r\n`metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')`\r\n\r\n`input_ids_train = []`\r\n`attention_mask_train = []`\r\n`token_type_ids_train = []`\r\n`output_label_train = []`\r\n`for f in train_dataset:`\r\n    `input_ids_train.append(f.input_ids)`\r\n    `attention_mask_train.append(f.attention_mask)`\r\n    `token_type_ids_train.append(f.token_type_ids)`\r\n    `output_label_train.append(f.label)`\r\n`model.compile(optimizer=optimizer, loss=loss, metrics=[metric])`\r\n\r\n`input_ids_train = np.array(input_ids_train)`\r\n`attention_mask_train = np.array(attention_mask_train)`\r\n`token_type_ids_train = np.array(token_type_ids_train)`\r\n`output_label_train = np.array(output_label_train)`\r\n\r\n`model.fit([input_ids_train,attention_mask_train, token_type_ids_train], y=output_label_train,\r\n          epochs = EPOCHS, batch_size=4)`\r\n\r\n`model.save('../output/my_model')`", "@jonanem \r\n\r\nWill it be possible to provide supporting files and colab link to reproduce the issue in our environment.Thanks!", "@jonanem \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Have the same problem. The model is a transformer from hugging face module.", "Same issue, with [Electra model from HuggingFace](https://huggingface.co/transformers/model_doc/electra.html#tfelectramodel)\r\n\r\nMinimal example :\r\n```\r\n!pip install transformers\r\n\r\nfrom transformers import TFElectraModel\r\n\r\nmodel = TFElectraModel.from_pretrained(\"google/electra-base-discriminator\")\r\n!mkdir swag\r\nmodel.save(\"swag\")\r\n```", "Same problem when trying to save a model in SavedModel format @ravikyram, here is a colab to reproduce the issue: https://colab.research.google.com/drive/1AxAAUbtBV99j3WzO6wnuU7WJRQKE3a7F?usp=sharing", "> Same issue, with [Electra model from HuggingFace](https://huggingface.co/transformers/model_doc/electra.html#tfelectramodel)\r\n> \r\n> Minimal example :\r\n> \r\n> ```\r\n> !pip install transformers\r\n> \r\n> from transformers import TFElectraModel\r\n> \r\n> model = TFElectraModel.from_pretrained(\"google/electra-base-discriminator\")\r\n> !mkdir swag\r\n> model.save(\"swag\")\r\n> ```\r\n\r\nSame issue here. Changed the model to Albert and saved it without a problem. I think something might be wrong from HuggingFace side."]}, {"number": 35431, "title": "Unable to build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAfter about 20+ minutes of watching the code build the build stopped abruptly\r\n\r\n**Any other info / logs**\r\nHere is a link to an enviorment where you can run my exact dev-env in the cloud\r\nhttps://gitpod.io/#https://github.com/JesterOrNot/tensorflow/tree/JesterOrNot/gitpod-setup\r\n\r\n```\r\n./tensorflow/python/lib/core/pybind11_proto.h:40:44: warning: 'pybind11::str pybind11::detail::object_api<Derived>::str() const [wi\r\nth Derived = pybind11::handle]' is deprecated: Use py::str(obj) instead [-Wdeprecated-declarations]\r\n       std::string(py_object.get_type().str()), \" is not a valid proto.\"));\r\n                                            ^\r\nIn file included from external/pybind11/include/pybind11/cast.h:13,\r\n                 from external/pybind11/include/pybind11/attr.h:13,\r\n                 from external/pybind11/include/pybind11/pybind11.h:49,\r\n                 from tensorflow/python/client/device_lib_wrapper.cc:18:\r\nexternal/pybind11/include/pybind11/pytypes.h:147:19: note: declared here\r\n     pybind11::str str() const;\r\n                   ^~~\r\nINFO: From Compiling tensorflow/stream_executor/stream_executor_pimpl.cc [for host]:\r\ntensorflow/stream_executor/stream_executor_pimpl.cc: In member function 'stream_executor::DeviceMemoryBase stream_executor::StreamE\r\nxecutor::Allocate(tensorflow::uint64, tensorflow::int64)':\r\ntensorflow/stream_executor/stream_executor_pimpl.cc:462:31: warning: comparison of integer expressions of different signedness: 'lo\r\nng long unsigned int' and 'tensorflow::int64' {aka 'long long int'} [-Wsign-compare]\r\n       mem_alloc_bytes_ + size > memory_limit_bytes_) {\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:29,\r\n                 from ./tensorflow/core/platform/logging.h:27,\r\n                 from ./tensorflow/core/platform/status.h:24,\r\n                 from ./tensorflow/core/platform/errors.h:22,\r\n                 from ./tensorflow/core/lib/core/errors.h:19,\r\n                 from ./tensorflow/stream_executor/device_memory_allocator.h:23,\r\n                 from ./tensorflow/stream_executor/stream_executor_pimpl.h:28,\r\n                 from tensorflow/stream_executor/stream_executor_pimpl.cc:20:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = long long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\ntensorflow/stream_executor/stream_executor_pimpl.cc:700:3:   required from here\r\n./tensorflow/core/platform/default/logging.h:386:25: warning: comparison of integer expressions of different signedness: 'const int' and 'const long long unsigned int' [-Wsign-compare]\r\n                         ==)  // Compilation error with CHECK_EQ(NULL, x)?\r\n./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n                                                 ^\r\n./tensorflow/core/platform/default/logging.h:385:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n ^~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc [for host]:\r\ntensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc: In function 'tensorflow::Status tensorflow::{anonymous}::GetT\r\nPUDevices(tensorflow::Devices, llvm::ArrayRef<tensorflow::DeviceNameUtils::ParsedName>, llvm::SmallVectorImpl<llvm::SmallVector<ten\r\nsorflow::DeviceNameUtils::ParsedName, 8> >*)':\r\ntensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc:129:27: warning: comparison of integer expressions of differen\r\nt signedness: 'int' and 'size_t' {aka 'long unsigned int'} [-Wsign-compare]\r\n     if (num_tpus_per_host != host_tpu_devices.size())\r\n         ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/kernels/quantization_utils.cc [for host]:\r\nIn file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23,\r\n                 from external/gemmlowp/public/gemmlowp.h:19,\r\n                 from ./tensorflow/core/kernels/quantization_utils.h:37,\r\n                 from tensorflow/core/kernels/quantization_utils.cc:16:\r\nexternal/gemmlowp/public/../internal/multi_thread_gemm.h: In member function 'void gemmlowp::WorkersPool::LegacyExecuteAndDestroyTa\r\nsks(const std::vector<gemmlowp::Task*>&)':\r\nexternal/gemmlowp/public/../internal/multi_thread_gemm.h:405:23: warning: comparison of integer expressions of different signedness\r\n: 'int' and 'std::size_t' {aka 'long unsigned int'} [-Wsign-compare]\r\n     for (int i = 0; i < tasks_count - 1; i++) {\r\n                     ~~^~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/core/kernels/quantization_utils.cc:16:\r\n./tensorflow/core/kernels/quantization_utils.h: In function 'void tensorflow::RequantizeManyInNewRangeReference(const qint32*, tensorflow::int64, float, float, float, float, tensorflow::quint8*)':\r\n./tensorflow/core/kernels/quantization_utils.h:271:32: warning: comparison of integer expressions of different signedness: 'size_t' {aka 'long unsigned int'} and 'tensorflow::int64' {aka 'long long int'} [-Wsign-compare]\r\n   for (size_t index = 0; index < count; ++index) {\r\n                          ~~~~~~^~~~~~~\r\n[6,434 / 12,032] 16 actions running\r\n    Compiling tensorflow/python/tfe_wrapper.cc [for host]; 77s local\r\n    Compiling tensorflow/core/kernels/rnn/lstm_ops.cc [for host]; 44s local\r\n    Compiling tensorflow/core/kernels/rnn/gru_ops.cc [for host]; 43s local\r\n    Compiling tensorflow/stream_executor/stream.cc [for host]; 41s local\r\n    Compiling tensorflow/core/kernels/split_lib_cpu.cc [for host]; 28s local\r\n    Compiling .../core/kernels/serialize_sparse_op.cc [for host]; 24s local\r\n    //tensorflow/core/kernels:deserialize_sparse_string_op; 22s local\r\n    Compiling .../core/kernels/sparse_reorder_op.cc [for host]; 22s local ...\r\n\r\nServer terminated abruptly (error code: 14, error message: 'Socket closed', log file: '/home/gitpod/.cache/bazel/_bazel_gitpod/2c92b5569ddded7b3a6bd5e139451b60/server/jvm.out'\r\n```", "comments": ["Looks like your cloud instance closed connection, not an issue of TF itself.", "please try once with older versions of python...", "@JesterOrNot \r\nAny update on the issue please. Thanks!", "@Gubarev What python version do you mean?", "> @Gubarev What python version do you mean?\r\n\r\n@JesterOrNot Seems like I was mentioned by mistake and I think this question should be addressed to @Gaurav7004 :)", "@Gaurav7004 What python version do you mean?\r\nAlso sorry @Gubarev ", "Is this still an issue? Not sure if you are trying to build tf from prebuilt binary or from source.\r\nIn case of building from source can you please try by lowering your bazel version to 0.26.1\r\nhttps://github.com/tensorflow/tensorflow/blob/a641fa1c5c10f0a278c7671fd6f7df550a74935d/configure.py#L53", "I am building it from the source I will test with those modifications.", "![image](https://user-images.githubusercontent.com/50386709/77009552-1b932500-6936-11ea-96f8-c7a902aa4cab.png)\r\n", "Make sure you have the right bazel version. Please run `./configure`", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35431\">No</a>\n"]}, {"number": 35430, "title": "ReLU layer doesn't handle integer dtype", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 2.1.0-rc1\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAn exception during `tf.keras.layers.ReLU` construction with integer dtype and `max_value`\r\n\r\n**Describe the expected behavior**\r\nA layer is properly constructed and functional\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ninput = tf.keras.layers.Input(shape=(), name='x', dtype='int64')\r\ny = tf.keras.layers.ReLU(max_value=100, dtype='int64')(input)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in _AssertCompatible(values, dtype)\r\n    323   try:\r\n--> 324     fn(values)\r\n    325   except ValueError as e:\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in inner(values)\r\n    262   def inner(values):\r\n--> 263     _ = [_check_failed(v) for v in nest.flatten(values)\r\n    264          if not isinstance(v, expected_types)]\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in <listcomp>(.0)\r\n    263     _ = [_check_failed(v) for v in nest.flatten(values)\r\n--> 264          if not isinstance(v, expected_types)]\r\n    265   return inner\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in _check_failed(v)\r\n    247   # it is safe to use here.\r\n--> 248   raise ValueError(v)\r\n    249 \r\n\r\nValueError: 0.0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-2baabe6a9f04> in <module>\r\n      1 input = tf.keras.layers.Input(shape=(), name='x', dtype='int64')\r\n----> 2 y = tf.keras.layers.ReLU(max_value=100, dtype='int64')(input)\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    771                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    772                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    775                     # circular dependencies.\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/advanced_activations.py in call(self, inputs)\r\n    317                   alpha=self.negative_slope,\r\n    318                   max_value=self.max_value,\r\n--> 319                   threshold=self.threshold)\r\n    320 \r\n    321   def get_config(self):\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in relu(x, alpha, max_value, threshold)\r\n   4373   if clip_max:\r\n   4374     max_value = _constant_to_tensor(max_value, x.dtype.base_dtype)\r\n-> 4375     zero = _constant_to_tensor(0., x.dtype.base_dtype)\r\n   4376     x = clip_ops.clip_by_value(x, zero, max_value)\r\n   4377 \r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py in _constant_to_tensor(x, dtype)\r\n    676       A tensor.\r\n    677   \"\"\"\r\n--> 678   return constant_op.constant(x, dtype=dtype)\r\n    679 \r\n    680 \r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    256   \"\"\"\r\n    257   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 258                         allow_broadcast=True)\r\n    259 \r\n    260 \r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    294       tensor_util.make_tensor_proto(\r\n    295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 296           allow_broadcast=allow_broadcast))\r\n    297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    298   const_tensor = g._create_op_internal(  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    449       nparray = np.empty(shape, dtype=np_dt)\r\n    450     else:\r\n--> 451       _AssertCompatible(values, dtype)\r\n    452       nparray = np.array(values, dtype=np_dt)\r\n    453       # check to them.\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in _AssertCompatible(values, dtype)\r\n    329     else:\r\n    330       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\r\n--> 331                       (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n    332 \r\n    333 \r\n\r\nTypeError: Expected int64, got 0.0 of type 'float' instead.\r\n```\r\n\r\n\r\n```\r\ntf.constant(0., dtype='int64')\r\n```\r\nfails as well but with different backtrace.\r\n\r\ntf_nightly-2.1.0.dev20191226 is affected too.", "comments": ["@0x0badc0de ,\r\nI was able to replicate the issue for `dtype='int'` , works fine for` 'float'`, [gist](https://colab.sandbox.google.com/gist/oanush/2ae5d64fed93e8376bc70c37dea8e35f/35430.ipynb) of colab replicating the issue.", "Added PR #36037 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35430\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35430\">No</a>\n"]}, {"number": 35429, "title": "Add usage example for tf.math.sigmoid", "body": "Added a brief example showing how to use the sigmoid function in tf.math", "comments": ["Please follow the guideliness at https://www.tensorflow.org/community/contribute/docs_ref", "@mihaimaruseac  Does it look better now?\r\n", "@mihaimaruseac Does it look good now?", "Failure:\r\n\r\n```\r\nFailed example:\r\n    tf.sigmoid(x)\r\nExpected:\r\n    <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\r\n    array([0.0, .5, 1.0), dtype=float32)>\r\nGot:\r\n    <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0. , 0.5, 1. ], dtype=float32)>\r\n```\r\n\r\nPlease fix.", "#35541does the same thing", "```\r\nFAIL: Found 1 non-whitelisted pylint errors:\r\ntensorflow/python/ops/math_ops.py:3221: [C0301(line-too-long), ] Line too long (85/80)\r\n```\r\n\r\nPlease fix this and all other failures (click on details to the right of the failed builds)", "```\r\nFailed example:\r\n    tf.sigmoid(x)\r\nExpected:\r\n    <tf.Tensor: shape=(3,), dtype=float32, numpy=\r\n    array([0. , 0.5, 1. ], dtype=float32)>\r\nGot:\r\n    <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0. , 0.5, 1. ], dtype=float32)>\r\n```\r\n\r\nPlease consult https://www.tensorflow.org/community/contribute/docs_ref for a way to write this line so that it doesn't pass over the line length limit and it doesn't fail the test.\r\n\r\nIn particular, see section about testing on own machine. This line has been erroring for a few days.", "Where can I see the error messages you are sending? @mihaimaruseac ", "![t84238p3ZTJ](https://user-images.githubusercontent.com/323199/71858969-0716ee80-30a2-11ea-849b-1746315c6653.png)\r\n", "How do I interpret the errors it is throwing, for example \r\n```\r\nT:/src/github/tensorflow/tensorflow/core/profiler/utils/BUILD:107:1: C++ compilation of rule '//tensorflow/core/profiler/utils:xplane_builder' failed (Exit 2)\r\ntensorflow/core/profiler/utils/xplane_builder.cc(53): error C2429: language feature 'init-statements in if/switch' requires compiler flag '/std:c++17'\r\n```", "As you are only changing python code, C++ errors could be ignored if they are not on a \"Required\" build.", "Ah, I see"]}, {"number": 35428, "title": "Added usage example for tf.math.sigmoid", "body": "Added a brief example of how to use tf.math.sigmoid", "comments": []}, {"number": 35427, "title": "Added usage example for tf.math.sigmoid", "body": "Added a brief example of how to use tf.math.sigmoid", "comments": []}, {"number": 35426, "title": "Can't install tensorflow even with no cache dir \"pip3 --no-cache-dir install tensorflow\" ", "body": "ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\r\n    grpcio>=1.8.6 from https://www.piwheels.org/simple/grpcio/grpcio-1.26.0-cp37-cp37m-linux_armv7l.whl#sha256=e1478c82e0848fbdd790389c23fb94290da96447cc576679e9fdc16b7d4bc7e8 (from tensorflow):\r\n        Expected sha256 e1478c82e0848fbdd790389c23fb94290da96447cc576679e9fdc16b7d4bc7e8\r\n             Got        ec6268c7c1aab71ff12c2c97f87ba0bf148229d6d519d57db198c469c033b8b9\r\n\r\n", "comments": ["@dzilaan, Please fill the [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n"]}, {"number": 35425, "title": "Add usage example to math.poly_val", "body": "", "comments": []}]