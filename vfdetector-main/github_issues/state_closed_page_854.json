[{"number": 27896, "title": "Changing the tf_random_seed (RunConfig) doesn't change results/parameters for LinearClassifier", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nUsing different `tf_random_seed` values for `tf.estimator.RunConfig` produces the same model performance and parameters for `tf.estimator.LinearClassifier`.\r\n\r\n**Describe the expected behavior**\r\nDifferent seeds to produce different performance/parameters, specially if the dataset is small or if the optimizer only takes a step for a small batch size.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.metrics import roc_auc_score\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.logging import set_verbosity\r\nfrom tensorflow.estimator.inputs import pandas_input_fn\r\nfrom tensorflow.estimator import LinearClassifier\r\nfrom tensorflow.feature_column import numeric_column\r\n\r\n\r\nX, y = make_classification(n_samples=50000, n_classes=2, \r\n                           n_clusters_per_class=4, class_sep=1.0, \r\n                           random_state=10, weights=[0.8, 0.2], \r\n                           n_features=10, n_informative=5)\r\n\r\nX = pd.DataFrame(X, columns=[\"f{}\".format(i) for i in range(10)])\r\ny = pd.Series(y)\r\n\r\ninput_fn = pandas_input_fn(X, y=y, batch_size=5, num_epochs=1, shuffle=False)\r\n\r\nfeature_columns = [numeric_column(key=c) for c in X.columns]\r\n\r\nclassifier = LinearClassifier(feature_columns, config=tf.estimator.RunConfig(tf_random_seed=10))\r\n\r\nclassifier.train(input_fn, steps=1)\r\n\r\nprint('\\n\\n\\n')\r\n\r\nfor c in X.columns:\r\n    print(c, classifier.get_variable_value(\"linear/linear_model/{}/weights\".format(c))[0][0])\r\n\r\nfeature_columns = [numeric_column(key=c) for c in X.columns]\r\n\r\nclassifier = LinearClassifier(feature_columns, config=tf.estimator.RunConfig(tf_random_seed=31231))\r\n\r\nclassifier.train(input_fn, steps=1)\r\n\r\nprint('\\n\\n\\n')\r\n\r\nfor c in X.columns:\r\n    print(c, classifier.get_variable_value(\"linear/linear_model/{}/weights\".format(c))[0][0])\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nf0 -0.1945913\r\nf1 -0.19702944\r\nf2 0.1967695\r\nf3 0.19892408\r\nf4 -0.18479808\r\nf5 -0.19904605\r\nf6 0.1991331\r\nf7 -0.19970222\r\nf8 0.186639\r\nf9 -0.1857145\r\n\r\n\r\n\r\n\r\nf0 -0.1945913\r\nf1 -0.19702944\r\nf2 0.1967695\r\nf3 0.19892408\r\nf4 -0.18479808\r\nf5 -0.19904605\r\nf6 0.1991331\r\nf7 -0.19970222\r\nf8 0.186639\r\nf9 -0.1857145\r\n```", "comments": ["@echo66 I think as you set `shuffle=False`, the input data is same for both the cases and hence the model parameters are same. I tried 'shuffle=True' and removed 'tf_random_seed'. Now the parameters are different. Please let me know what you think. Thanks!\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.metrics import roc_auc_score\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.logging import set_verbosity\r\nfrom tensorflow.estimator.inputs import pandas_input_fn\r\nfrom tensorflow.estimator import LinearClassifier\r\nfrom tensorflow.feature_column import numeric_column\r\n\r\n\r\nX, y = make_classification(n_samples=50000, n_classes=2, \r\n                           n_clusters_per_class=4, class_sep=1.0, \r\n                           random_state=10, weights=[0.8, 0.2], \r\n                           n_features=10, n_informative=5)\r\n\r\nX = pd.DataFrame(X, columns=[\"f{}\".format(i) for i in range(10)])\r\ny = pd.Series(y)\r\n\r\ninput_fn = pandas_input_fn(X, y=y, batch_size=5, num_epochs=1, shuffle=True)\r\n\r\nfeature_columns = [numeric_column(key=c) for c in X.columns]\r\n\r\nclassifier = LinearClassifier(feature_columns, config=tf.estimator.RunConfig())\r\n\r\nclassifier.train(input_fn, steps=1)\r\n\r\nprint('\\n\\n\\n')\r\n\r\nfor c in X.columns:\r\n    print(c, classifier.get_variable_value(\"linear/linear_model/{}/weights\".format(c))[0][0])\r\n\r\nfeature_columns = [numeric_column(key=c) for c in X.columns]\r\n\r\nclassifier = LinearClassifier(feature_columns, config=tf.estimator.RunConfig())\r\n\r\nclassifier.train(input_fn, steps=1)\r\n\r\nprint('\\n\\n\\n')\r\n\r\nfor c in X.columns:\r\n    print(c, classifier.get_variable_value(\"linear/linear_model/{}/weights\".format(c))[0][0])\r\n```", "Hello @jvishnuvardhan !\r\n\r\nIn the code snippet, you have, at least, two processes that (should) require seeds for random numbers generators: (a) dataset shuffling and (b) linear model parameters initialization. If I don't shuffle the dataset and I change the seed, I still get the same parameters' values at the end of a single short optimization phase. This shouldn't be the case, unless (1) the parameters' initialization doesn't depend on the seed or (2) there is some kind of closed form solution for the model (AFAIK, FTRL optimizer is iterative).", "Yeah may be we have some basic concept missed about Lnear classifier\r\nI got the basic idea about Linear classifiers from one tutorial \r\nhere is the link for this\r\nURL: https://youtu.be/PKhojDaOi9Q\r\n", "Ok the issue is that in the `LinearClassifier` the inits are `tf.keras.initializers.zeros()` so we don't have any seed effects:\r\nhttps://github.com/tensorflow/estimator/blob/2248336960a2cc65d2b7d0b21aca76157ccd0e79/tensorflow_estimator/python/estimator/canned/linear.py#L1464-L1472 \r\n\r\n/cc @qlzh727 What we want to do?", "@echo66 Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27896\">No</a>\n"]}, {"number": 27895, "title": "[TF 2.0] documentation in guides for feature columns, in particular `numeric_column`", "body": "\r\n**System information**\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Doc Link: https://www.tensorflow.org/alpha\r\n\r\nGuides/examples that use feature columns follow a bad pattern for numeric features. In particular, the examples in the guides always create a list of scalar `numeric_column`s instead of one `numeric_column` with all numeric features. This results in really bad performance when training. See [this thread](https://groups.google.com/a/tensorflow.org/d/msg/discuss/Vt0JGKF_Bno/S5XXmdaoCQAJ).\r\nLocations in the docs (there may be others I missed): \r\n* https://www.tensorflow.org/alpha/tutorials/estimators/linear#base_feature_columns\r\n* https://www.tensorflow.org/guide/estimators#structure_of_a_pre-made_estimators_program\r\n* https://www.tensorflow.org/alpha/tutorials/keras/feature_columns#choose_which_columns_to_use\r\n* https://www.tensorflow.org/alpha/tutorials/estimators/boosted_trees#create_feature_columns_and_input_functions\r\n* https://www.tensorflow.org/alpha/tutorials/estimators/boosted_trees_model_understanding#create_feature_columns_input_fn_and_the_train_the_estimator\r\n\r\n\r\nThe '[Feature Columns guide](https://www.tensorflow.org/guide/feature_columns)' from TF 1 or anything analogous is not available in the [TF 2 docs](https://www.tensorflow.org/alpha). That guide does mention the shape parameter, but does not discuss performance at all, and also describes creating a separate `numeric_column` for each feature in the Iris dataset in [this section](https://www.tensorflow.org/guide/feature_columns#numeric_column).", "comments": ["@random-forests -- what is the status of that guide? Do we have someone working on it?\r\n\r\nRegarding the performance issues, @lendle , would you be interested in submitting a PR to one of the colabs to illustrate what you mean? Or creating a separate tutorial discussing the performance implications of decisions around feature columns?", "@karmel the thread I referenced in the issue links to a jupyter notebook gist that illustrates the performance issues. I won't have time to provide more than that for the time being.", "@lendle, thank you for this feedback! @karmel, we don't have anyone working on a FC guide from my side. We do have a contributor hacking on an updated \"Classify structured data\" tutorial to use the Pet Finder dataset https://www.kaggle.com/c/petfinder-adoption-prediction/data, which is a bit more fun / real-world. I'll share this thread with her so we can avoid these performance problems in the new tutorial at least.", ". @MarkDaoust added an example of a fix in https://github.com/tensorflow/docs/commit/05db4b0b1aa2e399898f10886585b19c611a85c5 if someone is available to apply it across the other notebooks that are listed here.", "Still open? would like to help", "@lendle please confirm if the issue still exist", "Moving this to closed status as there is no response.", "Some of these got fixed, we're moving away from feature-columns, but we are sure in newer docs avoid this sort of issue with equivalent keras layers."]}, {"number": 27894, "title": "TypeError: The `train_input_config` must be a input_reader_pb2.InputReader.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nno\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nlinux ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn\\a\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv1.12.0-10232-g9a43dfe 1.13.1\r\n- Python version:\r\nPython 3.7.1\r\n- Bazel version (if compiling from source):\r\nn/a\r\n- GCC/Compiler version (if compiling from source):\r\nn/a\r\n- CUDA/cuDNN version:\r\nn/a\r\n- GPU model and memory:\r\nn/a\r\n\r\n**Describe the current behavior**\r\nI've been following examples located here:\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md\r\n\r\nInstead of running it in the Google cloud, I wanted to run it locally. I will attach training configs files below. So when i run this command:\r\n`object_detection/model_main.py --pipeline_config_path=/home/konsof01/work/models/research/pipiline_config.proto\r\n--model_dir=/home/konsof01/tmp\r\n--alsologtostderr`\r\n\r\nI get the following error:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"object_detection/model_main.py\", line 109, in <module>\r\n>     tf.app.run()\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n>     _run_main(main, args)\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"object_detection/model_main.py\", line 105, in main\r\n>     tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 471, in train_and_evaluate\r\n>     return executor.run()\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 611, in run\r\n>     return self.run_local()\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 712, in run_local\r\n>     saving_listeners=saving_listeners)\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\r\n>     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\r\n>     return self._train_model_default(input_fn, hooks, saving_listeners)\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1151, in _train_model_default\r\n>     input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 992, in _get_features_and_labels_from_input_fn\r\n>     self._call_input_fn(input_fn, mode))\r\n>   File \"/home/konsof01/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1079, in _call_input_fn\r\n>     return input_fn(**kwargs)\r\n>   File \"/home/konsof01/work/models/research/object_detection/inputs.py\", line 486, in _train_input_fn\r\n>     raise TypeError('The `train_input_config` must be a '\r\n> TypeError: The `train_input_config` must be a input_reader_pb2.InputReader.\r\n\r\nThe problem occurs in this piece of code in `inputs.py`:\r\n\r\n`\r\n    if not isinstance(train_input_config, input_reader_pb2.InputReader):\r\n      raise TypeError('The `train_input_config` must be a '\r\n                      'input_reader_pb2.InputReader.')\r\n`\r\nHowever:\r\n\r\n> model_pb2.DetectionModel\r\n> <class 'model_pb2.DetectionModel'>\r\n> type(model_config)\r\n> <class 'model_pb2.DetectionModel'>\r\n> \r\n\r\nbut `isinstance(train_input_config, input_reader_pb2.InputReader) is False`\r\nPipeline config file: \r\n[pipiline_config.proto.zip](https://github.com/tensorflow/tensorflow/files/3085417/pipiline_config.proto.zip)\r\n\r\n\r\n\r\nWhy is that?\r\nThank you\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@sofeikov This issue is more related TF models repo [here](https://github.com/tensorflow/models/issues). I am closing it here. Please post it in models repo so that the issue has more visibility. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27894\">No</a>\n"]}, {"number": 27893, "title": "Transforming date values", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): No\u00a0\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHi.\r\n\r\nI have a use case where I want to use date features as input values for a predictive model. I need to transform the date features to be useful.\r\nFor example, I need to know the difference between two dates (for example, just the difference in days between 01-04-2019 and 16-04-2019, but the dates can also be months or years apart).\r\nOr just getting the day of the month, the month itself or the year (i.e. for 16-04-2019, getting 16, 4 and 2019 as seperate values).\r\n\r\nMy question is if it is possible to do this within TFX and if not, is this a feature that is coming up?\r\nIt would be important for my use case because the transform needs to be done in the graph format so that I can serve the model with the transformations inside the pipeline.\r\nOtherwise I would need to add something that can do this for me outside of TFX.\r\n\r\nThanks in advance!\r\n\r\nMartijn", "comments": ["Same issue here!", "@Malonl Could you post this in [transform](https://github.com/tensorflow/transform/issues) or [tfx](https://github.com/tensorflow/tfx/issues) repos as it is more related to them. Please post i t there and then close here. Thanks! ", "@jvishnuvardhan I will. Thank you."]}, {"number": 27892, "title": "Crash on TF 1.13 when custom `RNNCell which has `Template`. ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v1.13.0-rc2-5-g6612da8 / 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / 7.5.0\r\n- GPU model and memory: GTX 1080 / 8G\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nA code below works well on Python 3.6 with TF 1.12. On Python 3.7 with TF 1.13, however, the code crashes.\r\n\r\n**Describe the expected behavior**\r\nDo not crash\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass Cell(tf.nn.rnn_cell.RNNCell):\r\n  \r\n  def __init__(self, state_size, reuse=None):\r\n    super(Cell, self).__init__(_reuse=reuse)\r\n    self.__state_size = state_size\r\n    self.__encoder = tf.make_template(\"encoder\", self.encoder)\r\n  \r\n  @property\r\n  def state_size(self):\r\n    return self.__state_size\r\n  \r\n  @property\r\n  def output_size(self):\r\n    return self.state_size\r\n\r\n  def zero_state(self, batch_size, dtype):\r\n    return tf.zeros([batch_size, self.state_size])\r\n\r\n  def encoder(self, prev_state, obs):\r\n    inputs = tf.concat([prev_state, obs], -1)\r\n    return tf.layers.dense(inputs, self.state_size, None)\r\n\r\n  def call(self, inputs, prev_state):\r\n    state = self.__encoder(prev_state, inputs)\r\n    return state, state\r\n\r\n\r\ninputs = tf.placeholder(tf.float32, [32, 2, 20])\r\n\r\nrnn_cell = Cell(20)\r\noutputs, state = tf.nn.dynamic_rnn(rnn_cell, inputs, dtype=tf.float32)\r\n```\r\n\r\nTraceback\r\n\r\n```\r\nWARNING:tensorflow:From test.py:33: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\r\nWARNING:tensorflow:From [PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From test.py:24: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.dense instead.\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 33, in <module>\r\n    outputs, state = tf.nn.dynamic_rnn(rnn_cell, inputs, dtype=tf.float32)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py\", line 671, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py\", line 879, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3556, in while_loop\r\n    return_same_structure)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3087, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3022, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3525, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py\", line 847, in _time_step\r\n    (output, new_state) = call_cell()\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn.py\", line 833, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 234, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/layers/base.py\", line 534, in __call__\r\n    _add_elements_to_collection(self.updates, ops.GraphKeys.UPDATE_OPS)\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 653, in updates\r\n    return self._updates + self._gather_children_attribute('updates')\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1647, in _gather_children_attribute\r\n    getattr(layer, attribute) for layer in self._layers))\r\n  File \"[PYTHONDIRECTORY]/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1647, in <genexpr>\r\n    getattr(layer, attribute) for layer in self._layers))\r\nAttributeError: 'Template' object has no attribute 'updates'\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["Sorry for the breakage, seems that the issue has already been fixed by https://github.com/tensorflow/tensorflow/commit/eb741cedf304b411af6cae8ec2d60578cd3980a1. Which hasn't reach the release yet. I tested the same code in nightly release and the error is mitigated.\r\n\r\nFor now, I think you can update your code with following line, since the template does not have any updates.\r\n\r\n```python\r\nself.__encoder.updates = []\r\n```", "Thanks, I should have checked this on master version. Since the problem already has been fixed, I'll close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27892\">No</a>\n"]}, {"number": 27891, "title": "Uncomment device binding option, which allows Go application to bind graph processing to specified GPU device(s)", "body": "Since r1.13 is already out, uncomment device binding option, which allows Go application to bind graph processing to specified GPU device(s)\r\n\r\nFor discussion about this patch (and its C-counterpart), please see https://github.com/tensorflow/tensorflow/pull/20412", "comments": ["@yifeif can you please help merge this PR", "Hi, any progress on this?", "Still no luck? Should I generate pull request against master?", "Ping?", "@rthadur what happened here?", "> @rthadur what happened here?\r\n\r\n@alextp, I don't have merge access here , @yifeif can you please help merge this PR.", "Thanks @bioothod for the patch! I don't think we are making any patch releases for 1.13 unless we are doing security patches @mihaimaruseac? Also @gunan, what's our policy for accepting patches without doing releases?", "There is going to be a 1.13.2, hopefully this week or the next one", "It is somewhat unfair that you do not accept pull request because of the missed deadline for the patch which had been submitted a month ago and being subsequently notified on a weekly basis.\r\n\r\nWhat tree should I rebase it against?", "Hi @bioothod, unfortunately, after a branch is cut, we only accept critical bugfixes or security patches to the branch. So even if this patch was ready before 1.13 release was out.\r\n\r\nI believe in the other pull request discussion meant we will reenable this on master?\r\n@alextp may be able to comment.\r\n\r\nI am happy to accept this on master, but for release branches our policy is only critical bugfixes.", "Yes, reenabling this on master would be great as 1.14 is about to be cut.\n\n*From: *Gunhan Gulsoy <notifications@github.com>\n*Date: *Tue, May 14, 2019 at 5:19 PM\n*To: *tensorflow/tensorflow\n*Cc: *Alexandre Passos, Mention\n\nClosed #27891 <https://github.com/tensorflow/tensorflow/pull/27891>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27891?email_source=notifications&email_token=AAABHRN55H35NFF5LD6UVHLPVNJHLA5CNFSM4HGI5JGKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOROKITLY#event-2341767599>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMLTDTI5ICPUJZW773PVNJHLANCNFSM4HGI5JGA>\n> .\n>\n\n\n-- \n - Alex\n", "Pull request against master tree https://github.com/tensorflow/tensorflow/pull/28768"]}, {"number": 27890, "title": "DLL load failed while importing tensorflow cpu 2.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 x64\r\n- TensorFlow installed from (source or binary): \r\ncmd cd C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python37\\Scripts\r\nThen  pip install tensorflow==2.0.0-alpha0 \r\nNp here\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nI have an error while import tensorflow (cf screen)\r\n\r\n**Describe the expected behavior**\r\ntensorflow should import properly\r\n\r\n**Code to reproduce the issue**\r\nI install tensorflow as I tell and I try :\r\nimport tensorflow\r\n\r\n**Other info / logs**\r\nI tried w/ conda, same thing\r\n![clean](https://user-images.githubusercontent.com/30508173/56212750-f4174680-605a-11e9-9b89-a622371760e5.png)\r\n\r\n", "comments": ["You need to install Microsoft Visual C++ Build Tools 2015. Please take a look instructions given on https://www.tensorflow.org/install/pip. Select Windows in steps 1 and 2.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27889, "title": "Error: Can't build Tensorflow Lite for Raspberry Pi 3b+.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Raspbian GNU/Linux 9.8 (stretch)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):N/A\r\n- TensorFlow version:**the latest**\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):**gcc version 6.3.0 20170516 (Raspbian 6.3.0-18+rpi1+deb9u1)**\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\ntrying to build Tensorflow Lite for  Raspberry Pi 3b+\r\nfollowed the instructions on https://tensorflow.google.cn/lite/guide/build_rpi \r\n\r\n\r\n **at the last step got lots of errors such as:** \r\n\r\n`In file included from ./tensorflow/lite/experimental/c/c_api.h:24:0,\r\n                 from ./tensorflow/lite/experimental/c/c_api_experimental.h:19,\r\n                 from tensorflow/lite/experimental/c/c_api_experimental.cc:16:\r\n./tensorflow/lite/experimental/c/c_api_types.h:1:1: error: expected unqualified-id before \u2018.\u2019 token\r\n ../../c/c_api_internal.h\r\n ^\r\nIn file included from tensorflow/lite/experimental/c/c_api_experimental.cc:16:0:\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:28:24: error: \u2018TFL_Status\u2019 does not name a type\r\n TFL_CAPI_EXPORT extern TFL_Status TFL_InterpreterResetVariableTensors(\r\n                        ^~~~~~~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:38:5: error: variable or field \u2018TFL_InterpreterOptionsAddBuiltinOp\u2019 declared void\r\n     TFL_InterpreterOptions* options, TFL_BuiltinOperator op,\r\n     ^~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:38:5: error: \u2018TFL_InterpreterOptions\u2019 was not declared in this scope\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:38:29: error: \u2018options\u2019 was not declared in this scope\r\n     TFL_InterpreterOptions* options, TFL_BuiltinOperator op,\r\n                             ^~~~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:38:58: error: expected primary-expression before \u2018op\u2019\r\n     TFL_InterpreterOptions* options, TFL_BuiltinOperator op,\r\n                                                          ^~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:39:5: error: expected primary-expression before \u2018const\u2019\r\n     const TFL_Registration* registration, int min_version, int max_version);\r\n     ^~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:39:43: error: expected primary-expression before \u2018int\u2019\r\n     const TFL_Registration* registration, int min_version, int max_version);\r\n                                           ^~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:39:60: error: expected primary-expression before \u2018int\u2019\r\n     const TFL_Registration* registration, int min_version, int max_version);\r\n                                                            ^~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:48:5: error: variable or field \u2018TFL_InterpreterOptionsAddCustomOp\u2019 declared void\r\n     TFL_InterpreterOptions* options, const char* name,\r\n     ^~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:48:5: error: \u2018TFL_InterpreterOptions\u2019 was not declared in this scope\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:48:29: error: \u2018options\u2019 was not declared in this scope\r\n     TFL_InterpreterOptions* options, const char* name,\r\n                             ^~~~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:48:38: error: expected primary-expression before \u2018const\u2019\r\n     TFL_InterpreterOptions* options, const char* name,\r\n                                      ^~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:49:5: error: expected primary-expression before \u2018const\u2019\r\n     const TFL_Registration* registration, int min_version, int max_version);\r\n     ^~~~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:49:43: error: expected primary-expression before \u2018int\u2019\r\n     const TFL_Registration* registration, int min_version, int max_version);\r\n                                           ^~~\r\n./tensorflow/lite/experimental/c/c_api_experimental.h:49:60: error: expected primary-expression before \u2018int\u2019\r\n     const TFL_Registration* registration, int min_version, int max_version);\r\n                                                            ^~~\r\nIn file included from tensorflow/lite/experimental/c/c_api.cc:21:0:\r\n./tensorflow/lite/experimental/c/c_api_types.h:1:1: error: expected unqualified-id before \u2018.\u2019 token\r\n ../../c/c_api_internal.h\r\n\r\n`\r\n\r\n\r\n\r\n", "comments": ["does anyone know the solution? I got the same error too.", "@petewarden could you advise in resolving the error above? I have got the same issue, compiling and building lite for raspberry pi. @jinyige\r\n", "> does anyone know the solution? I got the same error too.\r\nI change the Tensorflow version. Choose the stable version, not the master"]}, {"number": 27888, "title": "tensorflow.multinomial performance ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\ntensorflow.multinomial and pytorch.multinomial,the performance gap is huge.In particular I think tensorflow.multinomial is very problematic\r\n\r\nI wrote the following code to compare the two functions,I set the first number to 10 and the rest to 0.0001, followed by softmax as the probability of selection.\r\n\r\n**Describe the expected behavior**\r\nI think tensorflow's performance is very unusual\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n[https://stackoverflow.com/questions/55705625/how-to-get-the-same-effect-and-what-makes-a-differencei-between-tensorflow-multi](url)\r\n\r\n\r\nimport torch\r\n\r\nimport numpy as np\r\n\r\nimport  tensorflow as tf\r\n\r\nsingle_data = np.array([10,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001])\r\n\r\n\r\nsingle_data = np.expand_dims(single_data, axis=0)\r\n\r\ndata =np.repeat(single_data, 50, axis=0)\r\n\r\ndata = tf.nn.softmax(data,axis=-1)\r\n\r\nsampled_word = tf.multinomial(data,1)\r\n\r\nsample = tf.reshape(sampled_word,[-1])\r\n\r\nwith tf.Session() as sess:\r\n\r\n    a_data=sess.run(data)\r\n\r\n    print(\"prob:\",a_data)\r\n\r\n    print(\"tensorflow.multinomial\",sess.run(sample))\r\n\r\n    a_data = torch.from_numpy(a_data)\r\n\r\n    idx = torch.multinomial(a_data, num_samples=1)\r\n\r\n    print(\"tensorflow.multinomial\",np.reshape(idx.data.numpy(),[-1]))\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nbut I get the result \r\nprob: [[9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05\r\n  4.51278541e-05 4.51278541e-05]\r\n [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05\r\n  4.51278541e-05 4.51278541e-05]\r\n [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05\r\n  4.51278541e-05 4.51278541e-05]\r\n ...\r\n [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05\r\n  4.51278541e-05 4.51278541e-05]\r\n [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05\r\n  4.51278541e-05 4.51278541e-05]\r\n [9.93907740e-01 4.51278541e-05 4.51278541e-05 ... 4.51278541e-05\r\n  4.51278541e-05 4.51278541e-05]]\r\ntensorflow.multinomial [121 129  35 104   4 133  60  92 104 129   4  49  35  99 109 111  62  87\r\n  23   5 109  63 103  61  78  43 101  85   2 127   0  36  53   0  74  44\r\n  64  55  51  59 108   0 112  32  36  24  68 135  72  22]\r\ntensorflow.multinomial [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\n 0 0 0 0 0 0 0 0 0 0 0 0 0]", "comments": []}, {"number": 27887, "title": "Saving model and loading model on a separate session gives wrong predictions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: using pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:1.12.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: - \r\n- **GCC/Compiler version (if compiling from source)**: - \r\n- **CUDA/cuDNN version**: 9\r\n- **GPU model and memory**: GeForce 940MX\r\n- **Exact command to reproduce**: As described below\r\n\r\n### Describe the problem\r\n\r\nAs soon as training is finished, generating predictions on the testing dataset gives an accuracy > 70.\r\n\r\nThen I save the model using model.save(...) method.\r\n\r\nWhen Trying to load the model in a separate session, the predictions are around 20s.\r\n\r\nI can confirm that i'm using the same testing dataset and same preprocessing steps that I've done when predicting after training.\r\n\r\nTo get over with the doubt on the different tokenizers, I've saved the tokenizer to a tokenizer.pickle file which will be loaded before conducting texts_to_sequences and pad_sequences on the testing text data (this didn't make any difference).\r\n\r\nI've tried compiling the model after loading before making predictions, this didn't work as well.\r\n\r\n### Source code / logs\r\n\r\n`model.save('model.h5') #for saving the model`\r\n\r\n---------------------------------------------------------------------------\r\n\r\n`#Predicting after saving on a different session`\r\n\r\n`trained_model = load_model('model.h5') #for loading the model in a different session.`\r\n\r\n`# load tokenizer`\r\n`tokenizer = Tokenizer()`\r\n`with open('trained_model/tokenizer.pickle', 'rb') as handle:`\r\n`  tokenizer = pickle.load(handle)`\r\n\r\n`test_revs = pd.read_csv('test_dataset.csv')`\r\n\r\n`test_revs.loc[:, 'rating'] = test_revs['rating'].apply(points_to_class) #converting decimal ratings to integer classes `\r\n\r\n`actual_texts = test_revs['text']`\r\n`actual_ratings = test_revs['rating']`\r\n\r\n`final_Y_test = to_categorical(actual_ratings, 5)`\r\n\r\n`actual_text_tokens = add_doc_to_vocab(actual_texts.tolist()) # preprocessing method`\r\n\r\n`sequences_test = tokenizer.texts_to_sequences(actual_text_tokens)`\r\n\r\n`X_test = pad_sequences(sequences_test, maxlen=1939, padding='post')`\r\n\r\n`# Predictions`\r\n`pred_test = trained_model.predict(X_test)`\r\n`pred_test = [np.argmax(x) for x in pred_test]`\r\n\r\n`# Actual`\r\n`true_test = final_Y_test`\r\n`true_test = [np.argmax(x) for x in true_test]`\r\n\r\n`# Find accuracies`\r\n`accuracy = accuracy_score(true_test, pred_test)`\r\n\r\n`print(\"The total accuracy is : \", accuracy)`\r\n\r\nI'm new to TensorFlow, if anyone has any suggestions, please raise, cheers !", "comments": ["@AchiraFernando It is strange. The saved model should give same performance as original. \r\nCould you please check the tutorial listed [here](https://github.com/tensorflow/docs/blob/r1.12/site/en/r2/tutorials/keras/save_and_restore_models.ipynb) on saving and restoring models. Please let me know how it progresses. Thanks!", "@jvishnuvardhan Thanks for checking!\r\n\r\n================================\r\nI think I found the issue.\r\nThe model saving and loading works alright as expected.\r\nBut the problem is with the tokenizing of the text data and converting the labels to categorical.\r\n\r\n1) I'm using the same tokenizer instance from the training setup works, but I\u2019ve tried using pickle to save the tokenizer instance but that didn\u2019t do any change. Code is as below for saving and loading. Or do you have any suggestions for saving the tokenizer ?\r\n\r\n```\r\n# saving\r\nwith open(\u2018tokenizer.pickle\u2019, \u2018wb\u2019) as handle:\r\npickle.dump(tokenizer_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\r\n\r\n# load tokenizer\r\ntokenizer = Tokenizer()\r\nwith open(\u2018tokenizer.pickle\u2019, \u2018rb\u2019) as handle:\r\ntokenizer = pickle.load(handle)\r\n```\r\n\r\n2) I\u2019m using to_categorical to convert the labels (0 to 5) to one hot vectors. I\u2019m using the below piece of code in the training session and testing session. But when testing, doing this gives me false accuracy but using the already converted values from the training session gives me the correct accuracy. Do you think we need to save this by any means as well ?\r\n\r\n```\r\ntest_revs = pd.read_csv(\u2018hotel_reviews_test.csv\u2019) # fetching the reviews from csv to a dataframe\r\n\r\ntest_revs.loc[:, \u2018rating\u2019] = test_revs[\u2018rating\u2019].apply(points_to_class)\r\n\r\nY_test = to_categorical(test_revs[\u2018rating\u2019], 5) # converting to one hot vectors with 5 as number of classes\r\n```", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 27886, "title": "[TF2.0] TypeError: Cannot convert provided value to EagerTensor when applying constraint on variable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0 alpha\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: V100, 16GB\r\n\r\n**Describe the current behavior**\r\nCannot convert provided value to EagerTensor when applying keras constraint on variable in TF2.0 eager mode.\r\n\r\n**Describe the expected behavior**\r\nVariable should be converted to EagerTensor, operation should return constrained variable.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nv = tf.Variable([[1, 2], [3, 4]])   \r\ntf.keras.constraints.UnitNorm(axis=1)(v)\r\n```\r\n\r\n**Other info / logs**\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-35-760885333ab7> in <module>\r\n> ----> 1 tf.keras.constraints.UnitNorm(axis=1)(v)\r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/constraints.py in __call__(self, w)\r\n>     110         K.epsilon() + K.sqrt(\r\n>     111             math_ops.reduce_sum(\r\n> --> 112                 math_ops.square(w), axis=self.axis, keepdims=True)))\r\n>     113 \r\n>     114   def get_config(self):\r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in sqrt(x)\r\n>    1878       A tensor.\r\n>    1879   \"\"\"\r\n> -> 1880   zero = _to_tensor(0., x.dtype.base_dtype)\r\n>    1881   inf = _to_tensor(np.inf, x.dtype.base_dtype)\r\n>    1882   x = clip_ops.clip_by_value(x, zero, inf)\r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in _to_tensor(x, dtype)\r\n>     612       A tensor.\r\n>     613   \"\"\"\r\n> --> 614   return ops.convert_to_tensor(x, dtype=dtype)\r\n>     615 \r\n>     616 \r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)\r\n>    1048   preferred_dtype = deprecation.deprecated_argument_lookup(\r\n>    1049       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\r\n> -> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n>    1051 \r\n>    1052 \r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n>    1106       name=name,\r\n>    1107       preferred_dtype=dtype_hint,\r\n> -> 1108       as_ref=False)\r\n>    1109 \r\n>    1110 \r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\r\n>    1184 \r\n>    1185     if ret is None:\r\n> -> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>    1187 \r\n>    1188     if ret is NotImplemented:\r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n>     302                                          as_ref=False):\r\n>     303   _ = as_ref\r\n> --> 304   return constant(v, dtype=dtype, name=name)\r\n>     305 \r\n>     306 \r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n>     243   \"\"\"\r\n>     244   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n> --> 245                         allow_broadcast=True)\r\n>     246 \r\n>     247 \r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n>     251   ctx = context.context()\r\n>     252   if ctx.executing_eagerly():\r\n> --> 253     t = convert_to_eager_tensor(value, ctx, dtype)\r\n>     254     if shape is None:\r\n>     255       return t\r\n> \r\n> ~/Apps/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n>     108       return ops.EagerTensor(\r\n>     109           value, handle, device, dtype, tensor)\r\n> --> 110     t = ops.EagerTensor(value, handle, device, dtype)\r\n>     111     scalar_cache[cache_key] = t\r\n>     112     return t\r\n> \r\n> TypeError: Cannot convert provided value to EagerTensor. Provided value: 0.0 Requested dtype: int32", "comments": ["This also happens on 1.13.1. `keras.constraints` is broken on eager mode.", "Converting your tensor to float data type can help in this case.\r\n```python\r\nimport tensorflow as tf \r\nv = tf.Variable([[1, 2], [3, 4]])   \r\ntf.keras.constraints.UnitNorm(axis=1)(tf.cast(v, tf.float32))\r\n```", "@ymodak I am not sure what is the cause and what need to be fixed. Should keras cast automatically or should EagerTensor do it? Anyway this should not be done by users themselves, for the sake of clean code.", "@pavithrasv Can you please take a look? Thanks!", "Closing this issue since casting tensor to float value fixes the problem. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27886\">No</a>\n", "lmodel, cnn, graph = init_model()\r\n \r\nTypeError: Cannot convert -0.05 to EagerTensor of dtype int32\r\n how to solve this error"]}, {"number": 27885, "title": "[Bug]Error when use ExponentialMovingAverage with distribute.Strategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MS Windows10 X64 1809 build17763\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):binary(use pip)\r\n- TensorFlow version (use command below):v1.13.1-0-g6612da8951\r\n- Python version:3.6.7\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:10.0/7.5.1\r\n- GPU model and memory:NVIDIA Geforce RTX2080TI 11GB x2\r\n\r\n**Describe the current behavior**\r\n```python\r\ndef _model_fn(...):\r\n  logits, _ = _models.build_model(\r\n          features,\r\n          model_name=FLAGS.model_name,\r\n          training=is_training,\r\n          override_params=override_params)\r\n......\r\n  global_step = tf.train.get_global_step()\r\n  if has_moving_average_decay:\r\n    ema = tf.train.ExponentialMovingAverage(\r\n        decay=FLAGS.moving_average_decay, num_updates=global_step)\r\n    ema_vars = tf.trainable_variables() + tf.get_collection('moving_vars')\r\n    for v in tf.global_variables():\r\n      if 'moving_mean' in v.name or 'moving_variance' in v.name:\r\n        ema_vars.append(v)\r\n    ema_vars = list(set(ema_vars))\r\n......\r\n  if has_moving_average_decay:\r\n      with tf.control_dependencies([train_op]):\r\n        train_op = ema.apply(ema_vars)\r\n......\r\ndef main():\r\n......\r\n  if FLAGS.num_gpus <= 1:\r\n    distribution_strategy = None\r\n  else:\r\n    distribution_strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=FLAGS.num_gpus,                                                                   cross_device_ops=tf.contrib.distribute.AllReduceCrossDeviceOps('hierarchical_copy', num_packs=2))\r\n  config = tf.estimator.RunConfig(\r\n      model_dir=FLAGS.model_dir,\r\n      train_distribute=distribution_strategy,\r\n      save_checkpoints_steps=save_checkpoints_steps,\r\n      log_step_count_steps=FLAGS.log_step_count_steps,\r\n      session_config=tf.ConfigProto(\r\n          allow_soft_placement=True,\r\n          graph_options=tf.GraphOptions(\r\n              rewrite_options=rewriter_config_pb2.RewriterConfig(\r\n                  disable_meta_optimizer=True))),)\r\n  model_est = tf.estimator.Estimator(\r\n                        model_fn=_model_fn,\r\n                        config=config,\r\n                        params=params\r\n  )\r\n......\r\n```\r\nI have find a bug when use ExponentialMovingAverage with distribute.Strategy.These codes work fine without using a distribution strategy.But when using a distribution strategy (such as MirroredStrategy), an error is reported:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\distribute\\shared_variable_creator.py\", line 90, in reuse_variable\r\n    v = shared_variable_store[canonical_name][variable_index]\r\nKeyError: 'mnasnet-a1/mnasnet/mnas_stem/conv2d/kernel/replica/ExponentialMovingAverage/'\r\n```\r\nBy debugging I can see that it works fine when ema.apply() is executed on the first GPU.But when executed on the second GPU, it will report an error.This is because MirroredVariable/ReplicaLocalVariable has been created when building the model in _model_fn().So on the second GPU, TF tries to create the variable 'conv2d/kernel/replica/ExponentialMovingAverage/'.But it doesn't exist at all, 'replica' should appear at the end of the variable name.\r\nI found a temporary solution by modifying the shared_variable_creator:\r\n```python\r\n  def reuse_variable(next_creator, *args, **kwargs):\r\n    \"\"\"Re-use existing variable from store with same name (in order).\"\"\"\r\n    del next_creator, args\r\n    name = kwargs.get(\"name\")\r\n    canonical_name = _canonicalize_variable_name(name)\r\n    replica_index = canonical_name.find('replica/')\r\n    if replica_index != -1:\r\n      canonical_name = canonical_name[0:replica_index] + canonical_name[replica_index+8:]\r\n    try:\r\n      variable_index = variable_scope_access_index.get(canonical_name, 0)\r\n      v = shared_variable_store[canonical_name][variable_index]\r\n      # TODO(priyag): Make this variable re-use more robust by adding checks\r\n      # that the requested shape and dtype match the existing variable.\r\n      variable_scope_access_index[canonical_name] = variable_index + 1\r\n      return v\r\n    except (KeyError, IndexError):\r\n      raise RuntimeError(\r\n          \"Tried to create variable {} with mismatching name on device {}\".\r\n          format(name, device_id))\r\n\r\n  if device_id == 0:\r\n    return create_new_variable\r\n  else:\r\n    return reuse_variable\r\n```\r\nBut this looks more like a patch than a solution.I hope someone can review this bug and fix it.\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Seems the same as https://github.com/tensorflow/tensorflow/issues/27392? can we keep just one of them? ", "> Seems the same as #27392? can we keep just one of them?\r\n\r\nYes,it's the same issue.", "Ok i will close this one then as it is a duplicate. thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27885\">No</a>\n"]}, {"number": 27884, "title": "Lite: Unpack Op additional memory fix", "body": "There is no need to allocate memory in Init.", "comments": ["@renjie-liu Hi, Could you PTAL and approve."]}, {"number": 27883, "title": "build error on latest master ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.4.1708\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit 0e4117f67192a140466ba66513035e24bbd1a474\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: source\r\n- Bazel version (if compiling from source):0.22.0\r\n- GCC/Compiler version (if compiling from source): gcc 6.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory:NA\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nERROR: /home/guizili/.cache/bazel/_bazel_guizili/fb08bc8aff974002b12a4e85119bd120/external/com_github_googlecloudplatform_google_cloud_cpp/google/cloud/BUILD:27:1: no such target '@bazel_tools//tools/cpp:cc_flags': target 'cc_flags' not declared in package 'tools/cpp' defined by /home/guizili/.cache/bazel/_bazel_guizili/fb08bc8aff974002b12a4e85119bd120/external/bazel_tools/tools/cpp/BUILD and referenced by '@com_github_googlecloudplatform_google_cloud_cpp//google/cloud:generate_build_info'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The same problem here (attempt to build from master) :\r\n\r\n- Ubuntu 18.04\r\n- Bazel version: 0.23.1\r\n- Python version: 2.7.15rc1 and 3.6.7", "This is because, we need bazel 0.24.0 or above to build the latest master. Try with bazel 0.24.0 or 0.24.1.", "updating Bazel  to 0.24.1 solves this build issue.", "Here is a tip for compiling the tensorflow master branch.In many cases only certain versions of bazel can be successfully compiled.Check the Continuous build status, read the compile log for the corresponding version, and find the available bazel version.", "Thanks all it works on 0.24.1.\r\nBasically, if the bazel version need upgrade, TF will increase the minimal supported version of bazel, that maybe a issue.\r\n", "The minimal version of bazel was updated yesterday in https://github.com/tensorflow/tensorflow/commit/407a4f3b9d39055a981b91c15f6911fdb38135d5\r\n\r\nI think this defect is resolved.", "I am closing the issue as it was resolved. But, please let me know if I'm mistaken. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27883\">No</a>\n", "Update 0.23.1 -> 0.25.2 helped."]}, {"number": 27882, "title": "Converting InceptionV3 to TfLite missing IdentityN", "body": "**System information**\r\nGoogle Colab\r\n- TensorFlow installed from (source or binary):\r\nInstalled from https://storage.googleapis.com/download.tensorflow.org/data/tensorflow_hub-0.4.0.dev0-py2.py3-none-any.whl\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, MEAN, RESHAPE, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nGoogle Colab Notebook: https://colab.research.google.com/drive/1idF4ZZysBnfcGwfoj0DovQb8C4NVA42w\r\n\r\nI updated the Google Colab Tutorial and updated it to use InceptionV3(based on the link included in the notebook and modified the file size to be [299,299].\r\n", "comments": ["@ngelotte : I tried executing your colab link, but it throws error at \"**Export your model**\" stage, would you please check and resolve it, so that i can debug the issue. Otherwise you can share the frozen pb file directly if you have. Thanks!", "@ANSHUMAN87 : Sorry I modified that same notebook to try and figure out how to freeze the graph. I copied over the network and took out what I had added - you can view it here. https://colab.research.google.com/drive/17e9E56uQpkdaD7Lz-csAOS2PL8Tzoz_1 \r\nHowever, It now gets a different error and I don't know what has changed. The new error is:\r\nInvalid argument: Input 1 of node keras_layer_1/StatefulPartitionedCall was passed float from Variable_1:0 incompatible with expected resource. ", "@ngelotte : Now i am able to run the notebook, for the error you faced(\"Invalid argument: Input 1 of node keras_layer_1/StatefulPartitionedCall was passed float from Variable_1:0 incompatible with expected resource.\"), is because of custom objects from Hub_Layer.\r\n\r\nAs tf.lite.TFLiteConverter.from_saved_model() does not support custom objects, i would suggest rather you save model in h5 file and use tf.lite.TFLiteConverter.from_keras_model_file(\"sample.h5\", custom_objects={'KerasLayer':hub.KerasLayer}).\r\n\r\nAs for your original issue: below i have provided snapshot for working code.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\"sample.h5\", custom_objects={'KerasLayer':hub.KerasLayer})\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\n\r\nNOTE: In COLAB Notebook some error is reported while saving to h5 file, so you can run offline. Or you can download the saved_model files and convert offline.", "@ngelotte Is this issue resolved by following @ANSHUMAN87 suggestion? Thanks! ", "I was not able to try it as I did not have a good local environment to run it in. As the goal of this conversion was to be able to use it on a EdgeTPU device I am able to follow the steps here https://github.com/tensorflow/tensorflow/issues/27880 to do the conversion. This area is still in active development it seems but I think it is the better route. Thanks for the help.", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "The original reported issue should be resolved by commit aca24307eba. \r\nCould you try it again with the next TF nightly build? Thanks!"]}, {"number": 27881, "title": "Lite: Unpack quantization support added", "body": "Quantization support added for Unpack Op", "comments": ["Add ref: #27760", "We should also bump the operator version when using quantized types. See also [this commit](https://github.com/tensorflow/tensorflow/commit/e476315d4484ed2b344983e1c5258b976927257c) for an example of how to bump version(s) when adding new features/types to an existing operator.", "> We should also bump the operator version when using quantized types. See also [this commit](https://github.com/tensorflow/tensorflow/commit/e476315d4484ed2b344983e1c5258b976927257c) for an example of how to bump version(s) when adding new features/types to an existing operator.\r\n\r\n@jdduke : Your comment is addressed now, please check, Thanks!", "Adding @suharshs and @jianlijianli for review.", "Looks good, thanks!", "@rthadur : Would you please help proceed with this PR, Thanks!", "@suharshs, @jianlijianli Could you PTAL and approve.", "@jdduke : Your comment is addressed now, Thanks!\r\n@jianlijianli : Thanks for approving the PR!", "@gbaned : I have resolved the merge conflict, please help proceed with this PR, thanks!", "@gbaned : Would you please help proceed with this PR, thanks!"]}, {"number": 27880, "title": "Quantization-Aware Training support in Keras", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13.1 (but willing to use 2.0.0-alpha0 if there is a good reason)\r\n- Are you willing to contribute it (Yes/No): Yes (given some pointers on how to best go about it)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently there is no obvious way to apply `tf.contrib.quantize.create_training_graph` to a keras model. The keras API only allows access to the graph after it has already created a session. Attempting to modify the graph at this point does not work:\r\nhttps://stackoverflow.com/questions/55123417/quantization-aware-retraining-a-keras-model\r\nhttps://stackoverflow.com/questions/52259343/quantize-a-keras-neural-network-model\r\n\r\nI have also tried to create a new session after rewriting the graph, without success:\r\n```\r\ntf.contrib.quantize.create_training_graph(input_graph=tf.keras.backend.get_session().graph, quant_delay=0)\r\n# create a new session after rewriting the graph\r\nnew_session = tf.Session()\r\ntf.keras.backend.set_session(new_session)\r\n```\r\n\r\nResults in this error when I try to fit the model:\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable dense_5/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/dense_5/bias/class tensorflow::Var does not exist.\r\n        [[{{node dense_5/BiasAdd/ReadVariableOp}}]]\r\n```\r\n\r\n**Will this change the current api? How?**\r\nProbably, but in a backwards-compatible way. I imagine some kind of graph rewriting hook would probably be necessary in the tf.keras API.\r\n\r\n**Who will benefit with this feature?** Users of TF Lite / Edge TPU wishing to easily train quantized models using the keras API (which is being pushed as the new \"one true API\" for tensorflow).\r\n\r\n**Any Other info.**\r\nRelated issue on the main keras project https://github.com/keras-team/keras/issues/11105", "comments": ["This is my code for quantization aware training in keras.\r\nhttps://gist.github.com/rocking5566/26637a1e969f0057811753050966a3a7\r\n\r\nYou can use  `tf.keras.backend.get_session()` to get the session, \r\nthen you can rewrite the graph created by keras.\r\nNote that you should call `sess.run(tf.global_variables_initializer())` after rewrite the graph.\r\n\r\nHowever, there is still another issue in this code.\r\nIf you save keras model and load again, fakequant layer disappear...QQ\r\nBecause keras does not know this layer.\r\nYou need to call `tf.contrib.quantize.create_training_graph()` again to get the training graph.\r\nHowever, you cannot initialized the variable... because the min max in fakequant may disappear.\r\nI still do not know how to solve this problem...", "How about support in Tensorflow 2.0?", "@suharshs for quantization + Keras", "Hi, we are actively working on a Keras replacement for contrib/quantize. We hare hoping to have it ready by the end of Q2. Thanks!", "@rocking5566 very interesting implementation! Did you also freeze the graph and convert it to a .tflite model?", "@rocking5566 so how can i use this quantization with fine-tuning and transfer learning? do you have any code for that?", "Following @rocking5566 code, you can recover a model with something like:\r\n \r\n\r\n    tf.keras.backend.clear_session()\r\n    g = tf.keras.backend.get_session().graph\r\n    with tf.Session(graph=g) as session:\r\n\r\n        model_clean = ...\r\n        tf.contrib.quantize.create_eval_graph(input_graph=g)\r\n        optimizer_not_used, loss = ...\r\n\r\n        # initialize automatically quantized variables\r\n        session.run(tf.global_variables_initializer())\r\n        # compile the model\r\n        model_clean.compile(optimizer=optimizer_not_used,\r\n                            loss=loss,\r\n                            metrics=['accuracy'])\r\n        # recover the model\r\n        saver = tf.train.Saver()\r\n        saver.restore(session, model_path)`\r\n\r\n@suharshs In https://www.tensorflow.org/model_optimization/guide/roadmap there is no specification of how the issue will be addressed. Is there any place where discussions can be followed?\r\n\r\nThanks", "> @rocking5566 very interesting implementation! Did you also freeze the graph and convert it to a .tflite model?\r\n\r\nDid anyone find a way to convert the quantized model to a tflite file? \r\n\r\nI tried to adapt the instructions from the github readme but with no success\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize", "@suharshs Is there any update for the \"Keras replacement for contrib/quantize\" or the timeline for implementation? I appreciate your assistance.", "I build simple keras model and convert it to tflite file for Edge TPU. It seems works well.\r\n\r\nhttps://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb", "@ohtaman Thank you very much for your help.\r\nUnfortunately, I run into problems when using less trivial models.\r\nI added a batchnorm layer in \r\n```\r\ndef build_keras_model():\r\n    return keras.Sequential([\r\n        keras.layers.Flatten(input_shape=(28, 28)),\r\n        keras.layers.Dense(128, activation=tf.nn.relu),\r\n        tf.keras.layers.BatchNormalization(),\r\n        keras.layers.Dense(10, activation=tf.nn.softmax)\r\n    ])\r\n```\r\n\r\nand now I get the following error complaining about the min value in the batchnorm being unitialized\r\n```\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min\r\n\t [[node training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min (defined at <ipython-input-4-13f3a7476e38>:17) ]]\r\n\t [[node save/control_dependency (defined at <ipython-input-4-13f3a7476e38>:20) ]]\r\n```\r\n\r\nsorry to bother with that but I don't understand the origin of the error, it looks like there is a fake layer missing but I don't get why nor how we could specify to add it", "> \r\n> \r\n> @ohtaman Thank you very much for your help.\r\n> Unfortunately, I run into problems when using less trivial models.\r\n> I added a batchnorm layer in\r\n> \r\n> ```\r\n> def build_keras_model():\r\n>     return keras.Sequential([\r\n>         keras.layers.Flatten(input_shape=(28, 28)),\r\n>         keras.layers.Dense(128, activation=tf.nn.relu),\r\n>         tf.keras.layers.BatchNormalization(),\r\n>         keras.layers.Dense(10, activation=tf.nn.softmax)\r\n>     ])\r\n> ```\r\n> \r\n> and now I get the following error complaining about the min value in the batchnorm being unitialized\r\n> \r\n> ```\r\n> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min\r\n> \t [[node training/Adam/gradients/batch_normalization_v1/batchnorm/mul_1_grad/Mul_1/activation_Mul_quant/min (defined at <ipython-input-4-13f3a7476e38>:17) ]]\r\n> \t [[node save/control_dependency (defined at <ipython-input-4-13f3a7476e38>:20) ]]\r\n> ```\r\n> \r\n> sorry to bother with that but I don't understand the origin of the error, it looks like there is a fake layer missing but I don't get why nor how we could specify to add it\r\n\r\nYou may be able to do the following:\r\n\r\n```\r\ndef build_keras_model():\r\n    return keras.Sequential([\r\n        keras.layers.Flatten(input_shape=(28, 28)),\r\n        keras.layers.Dense(128, activation=tf.nn.relu),\r\n        tf.keras.layers.BatchNormalization(),\r\n        keras.layers.Dense(10, activation=tf.nn.softmax)\r\n    ])\r\n\r\ntf.keras.backend.clear_session()\r\ng = tf.keras.backend.get_session().graph\r\nwith tf.Session(graph=g) as session:\r\n\r\n    model_clean = build_keras_model()\r\n    tf.contrib.quantize.create_eval_graph(input_graph=g)\r\n    optimizer_not_used, loss = ...\r\n\r\n    # initialize automatically quantized variables\r\n    session.run(tf.global_variables_initializer())\r\n    # compile the model\r\n    model_clean.compile(optimizer=optimizer_not_used,\r\n                        loss=loss,\r\n                        metrics=['accuracy'])\r\n    # recover the model\r\n    saver = tf.train.Saver()\r\n    saver.restore(session, model_path)\r\n```\r\n", "Thanks!\r\n\r\nUsing it without batchnorm works but when I add batchnorm I get:\r\n```\r\nValueError: Input 0 of node batch_normalization_v1/cond/ReadVariableOp/Switch was passed float from batch_normalization_v1/gamma:0 incompatible with expected resource\r\n```\r\n\r\ndo you have any idea why I don't get the same error? (I'm not well versed into tf.Session() and in general its APIs except for keras)", "@NatGr  Hi! I updated my sample notebook. This may help you.\r\n\r\n1. Add BatchNormalization layer.\r\n2. Remove create_eval_graph() after train_model.fit.\r\n    - It is the cause of the FailedPreconditionError and I found that it is not necessary.\r\n3. Add keras.backend.set_learning_phase(0)\r\n   - Then we can avoid the ValueError\r\n\r\nhttps://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb", "@ohtaman, great! thank you very much (again)!\r\nI still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture\r\n```\r\ndef build_keras_model():\r\n    return keras.Sequential([\r\n        tf.keras.layers.Conv2D(16, kernel_size=3, activation=\"relu\", padding=\"same\", use_bias=False, input_shape=(28, 28, 1)),\r\n        keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n        keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n        tf.keras.layers.AveragePooling2D(pool_size=7),\r\n        tf.keras.layers.Flatten(),\r\n        keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n```\r\n\r\n(you also need to change the inputs and command for the scipt to run):\r\n```\r\ntrain_images = np.reshape(train_images, [-1, 28, 28, 1])\r\ntest_images = np.reshape(test_images, [-1, 28, 28, 1])\r\n```\r\nand\r\n```\r\ntflite_convert \\\r\n    --output_file=model.tflite \\\r\n    --graph_def_file=frozen_model.pb \\\r\n    --inference_type=QUANTIZED_UINT8 \\\r\n    --input_arrays=conv2d_input \\\r\n    --output_arrays=dense/Softmax \\\r\n    --mean_values=0 \\\r\n    --std_dev_values=255\r\n```\r\n\r\nHere is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW\r\n\r\nI get an error related to the batchnorm layers\r\n```\r\n2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n```\r\nThis issue is also faced here but they were not answered:\r\nhttps://github.com/tensorflow/tensorflow/issues/27952\r\n\r\nDo you have any idea on how to solve it?", "> @ohtaman, great! thank you very much (again)!\r\n> I still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture\r\n> \r\n> ```\r\n> def build_keras_model():\r\n>     return keras.Sequential([\r\n>         tf.keras.layers.Conv2D(16, kernel_size=3, activation=\"relu\", padding=\"same\", use_bias=False, input_shape=(28, 28, 1)),\r\n>         keras.layers.BatchNormalization(),\r\n>         tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n>         keras.layers.BatchNormalization(),\r\n>         tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n>         tf.keras.layers.AveragePooling2D(pool_size=7),\r\n>         tf.keras.layers.Flatten(),\r\n>         keras.layers.Dense(10, activation='softmax')\r\n>     ])\r\n> ```\r\n> \r\n> (you also need to change the inputs and command for the scipt to run):\r\n> \r\n> ```\r\n> train_images = np.reshape(train_images, [-1, 28, 28, 1])\r\n> test_images = np.reshape(test_images, [-1, 28, 28, 1])\r\n> ```\r\n> \r\n> and\r\n> \r\n> ```\r\n> tflite_convert \\\r\n>     --output_file=model.tflite \\\r\n>     --graph_def_file=frozen_model.pb \\\r\n>     --inference_type=QUANTIZED_UINT8 \\\r\n>     --input_arrays=conv2d_input \\\r\n>     --output_arrays=dense/Softmax \\\r\n>     --mean_values=0 \\\r\n>     --std_dev_values=255\r\n> ```\r\n> \r\n> Here is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW\r\n> \r\n> I get an error related to the batchnorm layers\r\n> \r\n> ```\r\n> 2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n> ```\r\n> \r\n> This issue is also faced here but they were not answered:\r\n> #27952\r\n> \r\n> Do you have any idea on how to solve it?\r\n\r\nI'm having the exact same issue. Here's what my model looks like:\r\n\r\n```\r\ndef build_keras_model():\r\n  \r\n    return keras.Sequential([\r\n            keras.layers.Conv2D(32, (3, 3), padding=\"same\", input_shape=(28,28,1)),\r\n            keras.layers.Activation(\"relu\"),\r\n            keras.layers.BatchNormalization(axis=chanDim),\r\n            keras.layers.Conv2D(32, (3, 3), padding=\"same\"),\r\n            keras.layers.Activation(\"relu\"),\r\n            keras.layers.BatchNormalization(axis=chanDim),\r\n            keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n            keras.layers.Dropout(0.25),\r\n            keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\r\n            keras.layers.Activation(\"relu\"),\r\n            keras.layers.BatchNormalization(axis=chanDim),\r\n            keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\r\n            keras.layers.Activation(\"relu\"),\r\n            keras.layers.BatchNormalization(axis=chanDim),\r\n            keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n            keras.layers.Dropout(0.25),\r\n            keras.layers.Flatten(),\r\n            keras.layers.Dense(512),\r\n            keras.layers.Activation(\"relu\"),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.Dropout(0.5),\r\n            keras.layers.Dense(classes),\r\n            keras.layers.Activation(\"softmax\")\r\n    ])\r\n\r\n\r\n```\r\n\r\nERROR:\r\n`2019-06-08 15:09:20.754790: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n`\r\n\r\n\r\n[Link to modified ipynb](https://colab.research.google.com/gist/1n1n1t3/19a634f76bbfca9051efdfa865aba93e/edgetpu-with-keras.ipynb#scrollTo=gSfaOfSTnXUR)", "> @ohtaman, great! thank you very much (again)!\r\n> I still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture\r\n> \r\n> ```\r\n> def build_keras_model():\r\n>     return keras.Sequential([\r\n>         tf.keras.layers.Conv2D(16, kernel_size=3, activation=\"relu\", padding=\"same\", use_bias=False, input_shape=(28, 28, 1)),\r\n>         keras.layers.BatchNormalization(),\r\n>         tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n>         keras.layers.BatchNormalization(),\r\n>         tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n>         tf.keras.layers.AveragePooling2D(pool_size=7),\r\n>         tf.keras.layers.Flatten(),\r\n>         keras.layers.Dense(10, activation='softmax')\r\n>     ])\r\n> ```\r\n> \r\n> (you also need to change the inputs and command for the scipt to run):\r\n> \r\n> ```\r\n> train_images = np.reshape(train_images, [-1, 28, 28, 1])\r\n> test_images = np.reshape(test_images, [-1, 28, 28, 1])\r\n> ```\r\n> \r\n> and\r\n> \r\n> ```\r\n> tflite_convert \\\r\n>     --output_file=model.tflite \\\r\n>     --graph_def_file=frozen_model.pb \\\r\n>     --inference_type=QUANTIZED_UINT8 \\\r\n>     --input_arrays=conv2d_input \\\r\n>     --output_arrays=dense/Softmax \\\r\n>     --mean_values=0 \\\r\n>     --std_dev_values=255\r\n> ```\r\n> \r\n> Here is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW\r\n> \r\n> I get an error related to the batchnorm layers\r\n> \r\n> ```\r\n> 2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n> ```\r\n> \r\n> This issue is also faced here but they were not answered:\r\n> #27952\r\n> \r\n> Do you have any idea on how to solve it?\r\n\r\nadding parameter fused=False did the trick for me. I hope it's not messing much with performance. Accuracy seems to be unaffected. \r\n\r\n`keras.layers.BatchNormalization(fused=False),\r\n`", "> > @ohtaman, great! thank you very much (again)!\r\n> > I still have a problem though, I tried to used a convolutionnal model, and when adding convolutions to the picture\r\n> > ```\r\n> > def build_keras_model():\r\n> >     return keras.Sequential([\r\n> >         tf.keras.layers.Conv2D(16, kernel_size=3, activation=\"relu\", padding=\"same\", use_bias=False, input_shape=(28, 28, 1)),\r\n> >         keras.layers.BatchNormalization(),\r\n> >         tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n> >         keras.layers.BatchNormalization(),\r\n> >         tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\", use_bias=False),\r\n> >         tf.keras.layers.AveragePooling2D(pool_size=7),\r\n> >         tf.keras.layers.Flatten(),\r\n> >         keras.layers.Dense(10, activation='softmax')\r\n> >     ])\r\n> > ```\r\n> > \r\n> > \r\n> > (you also need to change the inputs and command for the scipt to run):\r\n> > ```\r\n> > train_images = np.reshape(train_images, [-1, 28, 28, 1])\r\n> > test_images = np.reshape(test_images, [-1, 28, 28, 1])\r\n> > ```\r\n> > \r\n> > \r\n> > and\r\n> > ```\r\n> > tflite_convert \\\r\n> >     --output_file=model.tflite \\\r\n> >     --graph_def_file=frozen_model.pb \\\r\n> >     --inference_type=QUANTIZED_UINT8 \\\r\n> >     --input_arrays=conv2d_input \\\r\n> >     --output_arrays=dense/Softmax \\\r\n> >     --mean_values=0 \\\r\n> >     --std_dev_values=255\r\n> > ```\r\n> > \r\n> > \r\n> > Here is a copy of the modified notebook https://colab.research.google.com/drive/1i8C0dwJGfZ5kpvmA9769RlLzoEyx8VIW\r\n> > I get an error related to the batchnorm layers\r\n> > ```\r\n> > 2019-06-07 20:59:30.238116: F tensorflow/lite/toco/tooling_util.cc:1702] Array batch_normalization_v1/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array batch_normalization_v1/FusedBatchNorm, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n> > ```\r\n> > \r\n> > \r\n> > This issue is also faced here but they were not answered:\r\n> > #27952\r\n> > Do you have any idea on how to solve it?\r\n> \r\n> adding parameter fused=False did the trick for me. I hope it's not messing much with performance. Accuracy seems to be unaffected.\r\n> \r\n> `keras.layers.BatchNormalization(fused=False), `\r\n\r\nAmazing, thank you!\r\nIt seems it finally works for me as well", "@1n1n1t3 did you test the runtime of your generated tf-lite files?\r\ntraining seems to go correctly, the input and output details of my .tflite file seem consistent\r\n```\r\n>>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')\r\n>>> interpreter.allocate_tensors()\r\n>>> input_details = interpreter.get_input_details()\r\n>>> output_details = interpreter.get_output_details()\r\n>>> print(input_details)\r\n[{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]\r\n>>> print(output_details)\r\n[{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\r\n```\r\n\r\nThe accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).\r\n\r\nHowever, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.\r\nUsing the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.\r\nThis should not be the case.", "> @1n1n1t3 did you test the runtime of your generated tf-lite files?\r\n> training seems to go correctly, the input and output details of my .tflite file seem consistent\r\n> \r\n> ```\r\n> >>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')\r\n> >>> interpreter.allocate_tensors()\r\n> >>> input_details = interpreter.get_input_details()\r\n> >>> output_details = interpreter.get_output_details()\r\n> >>> print(input_details)\r\n> [{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]\r\n> >>> print(output_details)\r\n> [{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\r\n> ```\r\n> \r\n> The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).\r\n> \r\n> However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.\r\n> Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.\r\n> This should not be the case.\r\n\r\nAre you using the Coral USB Accelerator ? I tried it on the Coral Dev Board and the model I posted above is very fast. It goes something like 1500 fps. I'm used the MNIST Fashion dataset (input: 28,28,1) though. Not sure if that is fast, normal or slow for that, tbh, but the MobilenetSSD models run at about 300 fps. \r\n\r\nFor sure you are not supposed to get slower performance on quantized models though. Maybe the problem is something specific to the Raspberry PI and the environment. ", "\r\n\r\n\r\n> > @1n1n1t3 did you test the runtime of your generated tf-lite files?\r\n> > training seems to go correctly, the input and output details of my .tflite file seem consistent\r\n> > ```\r\n> > >>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')\r\n> > >>> interpreter.allocate_tensors()\r\n> > >>> input_details = interpreter.get_input_details()\r\n> > >>> output_details = interpreter.get_output_details()\r\n> > >>> print(input_details)\r\n> > [{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]\r\n> > >>> print(output_details)\r\n> > [{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\r\n> > ```\r\n> > \r\n> > \r\n> > The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).\r\n> > However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.\r\n> > Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.\r\n> > This should not be the case.\r\n> \r\n> Are you using the Coral USB Accelerator ? I tried it on the Coral Dev Board and the model I posted above is very fast. It goes something like 1500 fps. I'm used the MNIST Fashion dataset (input: 28,28,1) though. Not sure if that is fast, normal or slow for that, tbh, but the MobilenetSSD models run at about 300 fps.\r\n> \r\n> For sure you are not supposed to get slower performance on quantized models though. Maybe the problem is something specific to the Raspberry PI and the environment.\r\n\r\nNope, I'm running on native Raspberry PI (it's for my master thesis, I just use the Raspberry PI as a benchmarck which is why I don't use USB accelerators). I did not precise it but I'm using a MobileNetv1 adapted to CIFAR-10's input size.\r\n\r\nYeah that extremely weird, at least it's not specific to the Raspberry Pi since it happens on my desktop computer as well. It might be worthwile to test the floating point 32 model in your use case as well (I'm not asking for anything, I'm already extremely grateful for your help with the BatchNorm).", "@ohtaman, perhaps you have guidance on resolving the problem I'm having.  I've been using [keras.applications MobileNet](https://keras.io/applications/#mobilenet) and run into missing quantization-aware training values when restoring the checkpoint after training.\r\n\r\n    NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\n    Key conv1/act_quant/max not found in checkpoint\r\n\t     [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]\r\n\t     [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]\r\n\r\nHere's a modified Colab that demonstrates the problem:\r\nhttps://colab.research.google.com/drive/15itdlIyLmXISK6SDAzAFGUgjatfVr0Yq\r\n\r\n", "> @ohtaman, perhaps you have guidance on resolving the problem I'm having. I've been using [keras.applications MobileNet](https://keras.io/applications/#mobilenet) and run into missing quantization-aware training values when restoring the checkpoint after training.\r\n> \r\n> ```\r\n> NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n> \r\n> Key conv1/act_quant/max not found in checkpoint\r\n>      [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]\r\n>      [[node save/RestoreV2 (defined at <ipython-input-7-995fccbe9e12>:11) ]]\r\n> ```\r\n> \r\n> Here's a modified Colab that demonstrates the problem:\r\n> https://colab.research.google.com/drive/15itdlIyLmXISK6SDAzAFGUgjatfVr0Yq\r\n\r\nI had the same use case and I ran into a lot of issues. I think the simplest is to modify the architecture a bit and code it yourself. Resizing to 224*224 is much more resource consuming than having a 28*28 base input. Personally I only managed to make it work when using Sequential models, you should set fused=False in you Batchnormalization layers and I also integrated the activations to the convolutions layers, I don't remember what problem I had when I did that and it might be useless when taking the other adaptations into account.\r\n\r\nThat being said, I still have the huge performance gap I mention earlier that nullifies the interest of quantizing the model.\r\n\r\nHere is the code I used (for my build_keras_model function, the depth is measured in number of blocks and not number of layers, it is not written anywhere but it build a MobileNetv1) https://github.com/NatGr/Master_Thesis/blob/master/training_from_scratch/train_quantize_save.py\r\n", "@NatGr, thanks for the alternative suggestion!  \r\n\r\nThe reason I'm using the Keras Applications MobileNet is to retain the pre-trained weights.  As far as performance, my input images are full scale images from a 12 MP CMOS camera, in which areas of interest are resized to 224x224 for inference.\r\n\r\nI am unaware on how to modify the network so that I can load pre-trained weights into the custom network with modified BNs, which I believe would also address my problem.", "@oursland @NatGr \r\n\r\nthe cause of the error is that  the result of the create_training_graph and the create_eval_graph wereinconsistent. I found that just we have to do is adding keras.backend.learning_phase(1) before the create_training_graph function. \r\nSee\r\n\r\nhttps://colab.research.google.com/gist/ohtaman/578bd54aaa5a44234d477cfc61b21531/edgetpu-with-keras-applications.ipynb\r\n\r\nThe code above works in this case but not for some other cases. The Quantization Aware Trainingprocess is complicated and I don't understand everything.\r\n\r\nIn TensorFlow2.0, we can use Post Training Integer Quantization to get EdgeTPU model. I haven't tried it yet, but I think this is a simpler way and will fit most of you.\r\n\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba", "Based on feedback that the contrib/quantize quantization-aware training tool is a bit brittle and hard to use on some model architectures, we have released a [post-training integer quantization tool](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba), that requires a small calibration dataset. Please take a look at the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb) and give it a try, it should work much better! And let us know if you run into any issues.\r\n\r\nWe are rethinking and working on an api to replace contrib/quantize quantization-aware-training (although post-training quantization above should be sufficient for the majority of use cases), but do try Post Training Integer Quantization as a much simpler approach :)\r\n\r\nThanks!\r\n-Suharsh\r\n", "@ohtaman @suharshs \r\n\r\nSorry, I got a bit confused right now. Can we use Post-training integer quantization tool for creating models that run on the EdgeTPU on Google Coral devices (Dev board/USB accelerator) ? ", "@1n1n1t3 Yes, it says it makes the weights and activations int8 which is what Coral needs. As far as I can find, at the moment Coral and EdgeTPU essentially mean the same thing as Google hasn't released any other products to the public using this chip.", "@suharshs  I tried the new post-training integer quantization tool on my keras model and got following error:\r\n```\r\n  File \"quant_convert.py\", line 65, in _main\r\n    tflite_quant_model = converter.convert()\r\n  File \"/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 923, in convert\r\n    inference_output_type)\r\n  File \"/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 201, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 75, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(calibration_sample)\r\n  File \"/root/.virtualenvs/py3tf2/local/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 112, in FeedTensor\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_FeedTensor(self, input_value)\r\nValueError: Cannot set tensor: Got tensor of type NOTYPE but expected type FLOAT32 for input 3, name: input_1\r\n```\r\nBut when checking the input info I do see it is a float32 tensor:\r\n```\r\n    model = load_model('test.h5')\r\n    print(model.input)   ###Tensor(\"input_1:0\", shape=(?, 416, 416, 3), dtype=float32)\r\n```\r\nAnything I missed?", "Can you share your code that constructs the representative_dataset and calls the TFLiteConverter?\r\n\r\nIt looks like the representative dataset is yielding an array that is not of type float.", "@suharshs  That's it. I yield a numpy array in the representative dataset generator with default numpy dtype, which is float64:\r\n```\r\n    def data_generator():\r\n        for i in range(sample_num):\r\n            image, _ = get_random_data(annotation_lines[i], model_input_shape, random=True)\r\n            image = np.array([image]) #image.dtype==float64\r\n            yield [image]\r\n\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = data_generator\r\n```\r\nNow it has been fixed with \"image = np.array([image], dtype=np.float32)\" and the convertion works. Thanks a lot for your help!", "@suharshs  when verifying the converted model, following error happens:\r\n```\r\n>>> interpreter = tf.lite.Interpreter(model_path='model_quant.tflite')\r\nINFO: Initialized TensorFlow Lite runtime.\r\n>>> interpreter.allocate_tensors()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/.virtualenvs/py3tf_nightly/lib/python3.5/site-packages/tensorflow_core/lite/python/interpreter.py\", line 184, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/root/.virtualenvs/py3tf_nightly/lib/python3.5/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:67 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteFloat16 was not true.Node number 64 (DEQUANTIZE) failed to prepare.\r\n```\r\nI checked the tflite model file in visualize tool Netron. The error node 64 is a Dequantize node with following properties:\r\n![2](https://user-images.githubusercontent.com/25219223/59481812-7627be00-8e98-11e9-9b4c-a7c3d8b149ee.jpg)\r\n", "I have a hunch on the issue (int32 intermediate tensors) will send a fix ASAP.\r\n\r\nedit: I have found and verified a fix, should be submitted soon. thx", "@suharshs\r\nI'm trying to get efficientnet-b3 to Google Coral Dev Board. \r\nI get this error when using the target ops parameter:\r\nRuntimeError: Quantization not yet supported for op: REDUCE_MAX\r\n\r\nCan I do something about it without modifying the model?\r\n\r\n\r\nUPDATE: Nevermind me. The problem was actually from my added top layers ....... the network converts successfully (yay). Too bad it doesn't want to go through the EdgeTPU compiler and it doesn't even tell why... :(  It's just \"Internal compiler error. Aborting! \" message.", "Hello\r\nI have tried to convert my keras model using post training integer quantization following the tutorial from this Medium [post](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba)\r\n\r\nI keep getting this error:\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n\r\nI have tried putting it putting the model into a tf.function to get a concrete function following [this](https://www.tensorflow.org/lite/r2/convert/concrete_function), but then at compilation I still get the error that the model is not quantized.\r\n\r\n\r\n", "@suharshs \r\n\r\nUPDATED: Sorry, at first I wrote 'quantization aware training' in mistake. I want to know about Post Training Integer Quantization.\r\n\r\n--\r\n\r\nHi, I tried post training  integer quantization but it looks like the generated tflite model doesn't support EdgeTPU. Is that correct? (post training \r\ninteger quantization  does not support EdgeTPU yet, right?)\r\n\r\nIn fact, I got `RuntimeError: Error in interpreter initialization` at initialization of BasicEngine.\r\nIf this behavior is not supposed, I will send a issue separately.", "Hi the expectation is that is should work, but currently EdgeTPU requires the inputs and outputs to be uint8. \r\n\r\nThis can be achieve by setting:\r\nconverter.inference_input_type and converter.inference_output_type to tf.uint8.\r\n\r\nAdditionally, by default Optimizations.DEFAULT with representative data set will leave ops that dont have quantized implementations in floating point. This does not work with EdgeTPU. To guarantee that all ops are quantized (and give you an error saying which ops aren't) you can set:\"\r\n\r\nconverter.target_ops = [tf.lite.OpSet.TFLITE_BUILTINS_INT8]\r\n\r\nHope that helps!", "@suharshs \r\nThanks,  great! I could build custom EdgeTPU model using post training integer quantization.\r\nI can share the example notebook (I implimented an small autoencoder):\r\nhttps://colab.research.google.com/gist/ohtaman/7101e90f1f71a1cb054364d179845866/tf2-0-edgetpu-keras-cnn-autoencoder.ipynb\r\n\r\nIn this notebook, I have to use `tf.compat.v1.lite.TFLiteConverter` instead of `tf.lite.TFLiteConverter`.\r\nIt seems that `tf.lite.TFLiteConverter` does not use inference_input/output_type property in quantization process.  \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L395", "Hi @suharshs , now I've been able to do the post-training integer quantization on my pretrained keras model and run it in TF-lite runtime. But I found the inference accuracy seems got some loss after the convertion. Since it is an object detection model for CV tasks, so what I saw is the inferenced object location and confidence score result got obvious deviation with the ground truth.\r\nI tried to feed the representative dataset generator with more calibration samples, but doesn't help. When I rollback the converter to \"post_training_quantize\" mode, the accuracy performance being fine, but the latency goes bad since it's a float32 model now.\r\nIs there any way for me to get a better accuracy on the integer quantized model? Should I retrain the model under the new tf-nightly build env?", "Hi @ohtaman, @suharshs.\r\nI have been able to compile the model to a tf-lite file (using @ohtaman method --thanks for that by the way -- and thus using \"tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(tmp_keras_file)\").\r\nPost-training quantization reduces model file size without affecting inference time too much, however, when trying to use tensorflow benchmark tool version 1.13.1 (a tf binary using the tf-ilte binaries); I get the following error:\r\n```\r\nDidn't find op for builtin opcode 'AVERAGE_POOL_2D' version '2'\r\n```\r\n\r\nShould it not be \"tf-lite 1.13.1\" compatible since I use tf.compat.v1.lite.TFLiteConverter?\r\n\r\nI also tried with a binary for tf-lite 2 (EDIT: tf-nightly) and get the same error but I might have a problem with that binary (I'm working on it).\r\n\r\nEDIT: managed to compile the binary of tf.nightly, it works now!", "Hi guys. Thanks for the useful thread!\r\nI'm trying to quantize trained model using `tf.compat.v1.lite.TFLiteConverter` and `tf.lite.TFLiteConverter` but both produce same error: `Invalid quantization params for op MAX_POOL_2D at index 2 in subgraph 0`. \r\nDo you know what could be the case? @suharshs \r\n \r\nCode for `tf.compat.v1`\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('ssrnet_model.h5')\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ntflite_model = converter.convert()\r\nwith open('ssrnet_model.tflite', 'wb') as o_:\r\n    o_.write(tflite_model)\r\n```\r\n\r\nError details:\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-27-4ab92887ea65> in <module>()\r\n      4 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n      5 \r\n----> 6 tflite_model = converter.convert()\r\n      7 \r\n      8 with open('ssrnet_model_a.tflite', 'wb') as o_:\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\n    113 \r\n    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):\r\n--> 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\n    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister\r\n    117 CalibrationWrapper_swigregister(CalibrationWrapper)\r\n\r\nRuntimeError: Invalid quantization params for op MAX_POOL_2D at index 3 in subgraph 0\r\n```", "Hello @suharshs \r\n\r\nHave you fixed/submitted the correction already? \r\n\r\nThanks!", "you might want to check into this\r\n\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba", "Hello,\r\n\r\nactually, I did everything that is explaining there. I get to the point of converting the tflite model. However, when I try to use the edgetpu compiler I receive an error in compilation time. As good as it is, the error does not give any information appart from a path and a number (google does not say anything about it).\r\n\r\nHave you encountered a similar problem?", "> Hello,\r\n> \r\n> actually, I did everything that is explaining there. I get to the point of converting the tflite model. However, when I try to use the edgetpu compiler I receive an error in compilation time. As good as it is, the error does not give any information appart from a path and a number (google does not say anything about it).\r\n> \r\n> Have you encountered a similar problem?\r\n\r\nHi, \r\n\r\nI have encountered the same problem trying to convert an EfficientNet model and deploy it t Coral Dev Board. I wrote email to Coral team and I was simply replied that \"We don't support converting EfficientNet to EdgeTPU yet. Check the site regularly for updates\"... I guess this is the cost of being an early adopter. I think they are having some problems with the compiler compiling more complex and newer models and are working on it, but no word on what and when.", "Hi,\r\n\r\nthe model I use is actually a stack of Dense Keras layers and I am getting that error. The problem is that we do not know, from the compiler, what is the trouble coming from. Is it the model? Is it the tflite compilation? Is it another thing?", "@1n1n1t3 to be honest, I ended up using the Intel Neural Compute Stick 2 just because I needed to get something working soon and the software stack is more mature. I still like the Coral and Google has been very upfront that it is still in beta, so no complaints there.\r\n\r\nThe NCS 2 is a very different product as it does everything in FP16 and the architecture at a high level sounds closer to a desktop GPU, which probably means it can't do as many matrix multiplications as the EdgeTPU, but also means there is much less hassle getting models to run as you can just round off the weights from FP32 to FP16 and the performance is generally OK.\r\n\r\nI think once the software stack matures and we can easily combine the high level Keras API with the raw power of the coral devices this will be an awesome product!", "> Hi,\r\n> \r\n> the model I use is actually a stack of Dense Keras layers and I am getting that error. The problem is that we do not know, from the compiler, what is the trouble coming from. Is it the model? Is it the tflite compilation? Is it another thing?\r\n\r\nI think you are doing something wrong then, because I managed to get other models to compile and run on the EdgeTPU. Did you convert the input/outputs in uint8 setting converter.inference_input_type and converter.inference_output_type to tf.uint8 ? \r\n\r\n@ed-alertedh Yes, but if we compare NCS 2 with EdgeTPU we get a game changing performance gain which is essential for ML devices on the edge. The loss of conferting to 8bit integers seems negligible in big models. Let's hope they polish things and get out of beta soon. \r\n\r\n", "Hello @1n1n1t3,\r\n\r\nI am using this piece of code to convert my model to tflite, I indeed use inference_types as tf.uint8. \r\n\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\r\nconverter.representative_dataset = self.representative_dataset_gen\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ntflite_model = converter.convert()\r\n\r\ndef representative_dataset_gen(self):\r\n    for i in range(len(self.samples)):\r\n        data = self.samples[i: i + 1]\r\n        yield [data]\r\n```\r\n\r\nActually, in the code I found they were using different pieces of code for the first two lines, but this did not work for me. Those original lines were:\r\n\r\n```python\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_model_path)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\n\r\nI save my keras models with a simple: `model.save(filename + '.h5')` and my model looks like:\r\n\r\n```python\r\nmodel = Sequential()\r\nmodel.add(Dense(units=int(64), input_shape=(layers[1],layers[2]), activation=activation))\r\nmodel.add(Dense(units=int(64), activation=activation))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(units=layers[3], activation='linear'))\r\n```\r\n\r\nDo you use some different code?\r\n\r\nCheers", "I was able to compile  keras mobilenet model for Edge-TPU using following steps:\r\n1. Train model using a quantization-aware training technique\r\n2. Convert model using  post-training quantization \r\nImportant: tf-nightly  and tf.compat.v1.lite.TFLiteConverter were used\r\nThanks, @ohtaman for the tip https://github.com/tensorflow/tensorflow/issues/27880#issuecomment-503400613\r\n\r\nAll of this quantization for Edge-TPU looks like black magic right now, to be honest. ", "> @oursland @NatGr\r\n> \r\n> the cause of the error is that the result of the create_training_graph and the create_eval_graph wereinconsistent. I found that just we have to do is adding keras.backend.learning_phase(1) before the create_training_graph function.\r\n> See\r\n> \r\n> https://colab.research.google.com/gist/ohtaman/578bd54aaa5a44234d477cfc61b21531/edgetpu-with-keras-applications.ipynb\r\n> \r\n> The code above works in this case but not for some other cases. The Quantization Aware Trainingprocess is complicated and I don't understand everything.\r\n> \r\n> In TensorFlow2.0, we can use Post Training Integer Quantization to get EdgeTPU model. I haven't tried it yet, but I think this is a simpler way and will fit most of you.\r\n> \r\n> https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba\r\n\r\nHi @ohtaman , I encountered the following error using your code:\r\n\r\n`Instructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0708 19:10:21.210556 140681547597568 deprecation.py:323] From /home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nTraceback (most recent call last):\r\n  File \"common/experimental/leoli/pspnet_train/pspnet_train.py\", line 192, in <module>\r\n    [eval_net.output.op.name]\r\n  File \"/home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 297, in convert_variables_to_constants\r\n    source_op_name = get_input_name(node)\r\n  File \"/home/leoli/.local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 254, in get_input_name\r\n    raise ValueError(\"Tensor name '{0}' is invalid.\".format(node.input[0]))\r\nValueError: Tensor name 'conv1_1_3x3_s2_bn/cond/ReadVariableOp/Switch:1' is invalid.\r\n` \r\n\r\nIt happened when I was trying to freeze the model. ", "> @suharshs\r\n> Thanks, great! I could build custom EdgeTPU model using post training integer quantization.\r\n> I can share the example notebook (I implimented an small autoencoder):\r\n> https://colab.research.google.com/gist/ohtaman/7101e90f1f71a1cb054364d179845866/tf2-0-edgetpu-keras-cnn-autoencoder.ipynb\r\n> \r\n> In this notebook, I have to use `tf.compat.v1.lite.TFLiteConverter` instead of `tf.lite.TFLiteConverter`.\r\n> It seems that `tf.lite.TFLiteConverter` does not use inference_input/output_type property in quantization process.\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py#L395\r\n\r\n@ohtaman Nice Work. It is working well. ", "@ohtaman I have been compiling my own network. I also got an issue with UpSampling2D and I also used a custom one made out of basic units in TF. \r\n\r\nWhen I save the model after training and load the keras model, the prediction works fine. \r\n\r\nBut when I convert it to a tflite model (which converts without giving any issues), when I use the tflite model to run the inference using the interpreter, it hangs in the line after the interpreter.invoke() and doesn't execute further. One other thing is the edgetpu_compiler also gives an unknown compilation issue when I used the generated tflite model. But the error is not clear. In my model, the only Non-quantized part is the Upsamling2D. But I replaced it. I am not sure what is wrong. Have any suggestions to debug this? ", "> I build simple keras model and convert it to tflite file for Edge TPU. It seems works well.\r\n> \r\n> https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb\r\n\r\n@ohtaman Thanks for sharing your code, but I run into errors. The error differs depending on which docker image I use. Could you share your setup, please?\r\n\r\nWith the nightly-gpu image (Python 3.6.8, tf 1.15.0-dev20190715), I get an error when trying to convert the model:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 503, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 499, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 437, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 155, in toco_convert_protos\r\n    fp_debug.write(debug_info_str)\r\n  File \"/usr/lib/python3.6/tempfile.py\", line 624, in func_wrapper\r\n    return func(*args, **kwargs)\r\nTypeError: a bytes-like object is required, not 'str'\r\n```\r\n\r\nOn the other hand, with the latest-gpu-py3 image (Python 3.6.8, tf 1.14.0), I get an error when trying to test the tflite model with the interpreter of the Python API. It happens when it tries to allocate memory with the `allocate_tensors()` call.\r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nAborted (core dumped)\r\n```\r\n\r\nHave you run into such issues and if you did, how did you resolve them?", "@DocDriven \r\nI think one of your image arrays maybe like ['1.0', '2.0', '3.0'] not [1.0,2.0,3.0]. The string types are not supported I guess. Could you check that? ", "@vibhatha \r\nI double checked it, but all entries are numpy arrays of type np.uint8. Also, the stable versions seems to have no problem with these inputs, so I assume something went wrong with the nightly build.", "@DocDriven \r\nI used Python 3.6.9 with the latest pip and tested this with\r\n```\r\nPython 3.6.9 (default, Jul  3 2019, 15:36:16) \r\n[GCC 5.4.0 20160609] on linux\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.0.0-beta1\r\n>>> \r\n```\r\n\r\nWhat is your setting? It feels like the 1.14 is the stable release for now. I assume you're using it. \r\nAnd also did you try using \r\n\r\n`converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)`\r\n\r\nI think you have used toco converter. What I did was saved the keras model and then load it to convert using the above tflite converter. \r\nI followed most of the steps in @ohtaman code and guide in the google coral page as well. \r\n\r\nCould you try this one? ", "> @ohtaman I have been compiling my own network. I also got an issue with UpSampling2D and I also used a custom one made out of basic units in TF.\r\n> \r\n> When I save the model after training and load the keras model, the prediction works fine.\r\n> \r\n> But when I convert it to a tflite model (which converts without giving any issues), when I use the tflite model to run the inference using the interpreter, it hangs in the line after the interpreter.invoke() and doesn't execute further. One other thing is the edgetpu_compiler also gives an unknown compilation issue when I used the generated tflite model. But the error is not clear. In my model, the only Non-quantized part is the Upsamling2D. But I replaced it. I am not sure what is wrong. Have any suggestions to debug this?\r\n\r\nI changed the image sizes from 1024x1024 to smaller image size, then it worked pretty well. \r\n\r\n@suharshs  Is there a constraint on the data size in the inference level?\r\n\r\nFor instance flat 28x28 which is lesser size compared to 1024 x 1024. The last layer is not softmax, I can remember there is a 16K limit on softmax. Any idea on this? \r\n", "> @DocDriven\r\n> I used Python 3.6.9 with the latest pip and tested this with\r\n> \r\n> ```\r\n> Python 3.6.9 (default, Jul  3 2019, 15:36:16) \r\n> [GCC 5.4.0 20160609] on linux\r\n> >>> import tensorflow as tf\r\n> >>> print(tf.__version__)\r\n> 2.0.0-beta1\r\n> >>> \r\n> ```\r\n> \r\n> What is your setting? It feels like the 1.14 is the stable release for now. I assume you're using it.\r\n> And also did you try using\r\n> \r\n> `converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)`\r\n> \r\n> I think you have used toco converter. What I did was saved the keras model and then load it to convert using the above tflite converter.\r\n> I followed most of the steps in @ohtaman code and guide in the google coral page as well.\r\n> \r\n> Could you try this one?\r\n\r\n@vibhatha \r\nYou are right, I am using the latest stable version of tf 1.14.0, however with Python 3.6.8. I also copy-pasted the code in combination with the official docker image latest-gpu-py3.\r\n\r\nI have also tried using the Python API of the converter, but it worked equally well as the CLI, e.g. it produces a file, but fails when allocating memory.\r\n\r\nI think the difference lies in the input format, as you are using a h5 file, not a protocol buffer. I guess this might be the reason why it is working out for you. Were you able to convert your tflite file with the edgetpu_compiler? My issue was that if I tried to do it this way, the compiler would not recognize that I have a quantized model.", "@ohtaman \r\nGood Custom Upsampling model \r\n\r\n[Custom Upsampling vs Original Upsampling](https://colab.research.google.com/drive/1oyWyijDUGwEP5sseaGDSBKiIqz8oiBPB)", "@DocDriven \r\nYes, I was able to compile it. \r\nI first took a look at what @ohtaman has done. Then read all the docs that I can find on quantization with TF. \r\n\r\nThen I created 3 or 4 models custom builts using previous examples and my own examples. \r\nI was able to get done 100% TPU conversion at the edgetpu_compiler and the inference also worked well. \r\n\r\nMy main issue is I am having troubles when handling larger image sizes. That's where I am currently stuck. \r\n\r\nDo you need a custom code? I can provide a simple one?\r\nMake sure at every time when you load the data astype(np.float32) or any data type okay for quantization is put there. \r\n\r\nAnd also the \r\n\r\n`converter.representative_dataset = representative_dataset_gen`\r\n\r\nthis sample data must have the same data type as the input data type. Just pay attention to those parts. \r\n\r\nThen put all the right quantization params for the tflite converter. Then before testing on edge, for the sake of the argument, just load the tflite converted file and test on CPU machine using interpreter API. \r\nIf it works well and fast, it may be okay. But there are certain issues shown in compiling due to Non-quantized model. What I used to get was a model is not quantized then I got an error when the model is too big (meaning the data size in the last layer is very big) saying compiling error. \r\n\r\nI guess this could help. ", "@vibhatha \r\nThank you for this walkthrough, I will investigate this the next few days. If you could provide a simple example, I would be very grateful. Maybe my bug is hiding in plain sight :/", "\r\nHere is what I did using an existing code by another user. The license part is from him. \r\n\r\nI added a comment on what I did change. \r\n[Google Collab Link](https://colab.research.google.com/drive/1hQbm3gzOqEycFYj8woAUaYGPDRcHKXKb)\r\n\r\nHope this would help. \r\n@ohtaman  Example is a very neat code which would help you. ", "@DocDriven \r\nThe above link has a quantized code. Please check and tell me if there is an issue. ", "Hello all, first of all, thanks for this helpful discussion I learned alot from the last two weeks. I wish that I was good enough to contribute to the community like you guys did. Honestly, I don't really understand the stuff behind the conversion. I am stuck at the last step: **convert frozen graph to tflite model** with the error:\r\n```\r\nF tensorflow/lite/toco/graph_transformations/quantize.cc:491] Unimplemented: this graph contains an operator of type Sqrt for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n```\r\nSo it complains about Sqrt opt, which has not implemented.\r\n\r\nBelow is the process I used:\r\n\r\n**Train with** \r\n\r\n```     \r\ntf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\r\ntrain_sess.run(tf.global_variables_initializer()) \r\n```\r\n\r\n**Save checkpoint after the training:**\r\n\r\n```\r\nsaver = tf.train.Saver()\r\nsaver.save(train_sess, path_checkpoint)`\r\n```\r\n\r\n**Evaluate phase or whatever to get a frozen graph** (I don't understand what it does):\r\n```\r\ntf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\r\neval_graph_def = eval_graph.as_graph_def()\r\nsaver = tf.train.Saver()\r\nsaver.restore(eval_sess, path_checkpoint)\r\n\r\nfrozen_graph_def = tf.graph_util.convert_variables_to_constants(\r\n    eval_sess,\r\n    eval_graph_def,\r\n    [model.output.op.name]\r\n)\r\n\r\nopen(path_frozen_graph, 'wb').write(frozen_graph_def.SerializeToString())     \r\n```\r\n\r\nEverything seems fine until this point,  `tflite_model = converter.convert()` gives me the mentioned error:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file=path_frozen_graph, \r\n    input_arrays=[x.input.name.split(':')[0] for x in model.layers[0:2] ], \r\n    output_arrays=[model.layers[-1].output.name.split(':')[0]]\r\n)\r\nconverter.representative_dataset = representative_dataset\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {\r\n    x : (0., 255) for x in input_arrays[0:2]\r\n} \r\n# converter.default_ranges_stats = (-100, +100)\r\ntflite_model = converter.convert()\r\nopen(path_lite_model, 'wb').write(tflite_model)\r\n```\r\nI know the problem because I am using lambda layer at the output, which is not in the compatibility table:\r\n```\r\ndef euclidean_distance(vects):\r\n    x, y = vects\r\n    return tf.norm(x-y, ord='euclidean',  axis=1, keepdims=True)\r\n#    I also replace those two line by below lines, but not work          \r\n#    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\r\n#    return K.sqrt(K.maximum(sum_square, K.epsilon()))\r\n...\r\nlayers.Lambda(euclidean_distance)([processed_a, processed_b])\r\n...\r\n```\r\nIf I replace the `euclidean_distance(vects)` function by something simpler like `return 2x - y` and add another layer: `layers.Dense(1)` to produce a same shape, my conversion process works just fine. \r\nOk I know most of the api here are currently in experiment phase. But I really want to convert my customized model to tflite model. It looks like tensorflow doesn't support the operations I want to use. I don't know if there is any other way or not. Please let me know if there is, I appreciate that.\r\n**Another interesting approach is to split the model**, here is the part output from my model summary:\r\n```\r\n...\r\nlambda (Lambda)                 (None, 1)            0           sequential[1][0] \r\n...\r\n```\r\nwhich shows it has zero parametter. If you look at my lambda layer, it is just basic math. I also tried to split my model at Keras level into 2 models. Lets say model.layers[0:1] and model.layers[2:3] (for lambda). When I use it, I can feed the output of the first one to the input a second one. It works just fine.  So I can somehow get rid of the complexity of the last layer (lambda). However, when I train the whole model, they have to come with each other. I don't know if it is possible to  save the checkpoint that contains just the information of model.layers[0:1]. And convert to tflite from that. At the end, I expect to have only model.layers[0:1]  quantized and converted to tflite. I belive this is possible in theory but still I don't know how to make it. Manual modify the checkpoint or frozengraph may work?!, but it really scary to look at those files. Again, if you know a way, please let me know. \r\nSorry for not sharing the code, it is not my properties. I sometimes feel bad when I am asking from the community but I can't contribute. But if you want to know anything, feel free to ask, I really want the solution for this problem", "@jimmyvo2410 \r\nI am also no expert on the topic. But I what I can see is the operator shown in the error message is not yet supported for quantization. \r\nIf you take a look at @ohtaman solution, the upsampling2D is not yet supported for quantization. So he has written a custom one using already quantization supported ops. This is something similar you might have to do. What I can see is this solution. But I am sure an expert may be able to give you a better answer. \r\nI also needed some of these ops quantized and I had to write custom ones from already existing components, for instance, I also needed Upsampling2D. Then I did a validation with a single layer of Upsamling2D and a custom model written by @ohtaman and seems like it is a very good match for the original op. \r\nSo you may want to double-check how good is your custom model with respect to the original one, just do it with a single layer of whatever the component you want to write. I am sure someone else might have the same problem and there can be a solution already in an existing thread. \r\n\r\nHope this helps. \r\n", "> Here is what I did using an existing code by another user. The license part is from him.\r\n> \r\n> I added a comment on what I did change.\r\n> [Google Collab Link](https://colab.research.google.com/drive/1hQbm3gzOqEycFYj8woAUaYGPDRcHKXKb)\r\n> \r\n> Hope this would help.\r\n> @ohtaman Example is a very neat code which would help you.\r\n\r\n@vibhatha \r\nThank you for your help. I have tested your code over the last few days, with mixed results.\r\n\r\n- When using the docker image with the TF 2.0.0-beta1, your code runs just fine. But as soon as I switch back to TF 1.14.0, I run into an error:\r\n\r\n```\r\n... (error-free output)\r\n\r\nTraceback (most recent call last):\r\n  File \"test_main.py\", line 205, in <module>\r\n    tflite_fname = gen_tflite()\r\n  File \"test_main.py\", line 164, in gen_tflite\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 783, in __getattribute__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'TFLiteConverter' object has no attribute 'target_spec'\r\n```\r\n\r\nIs there an equivalent of this attribute in v1? Also, you do not seem to prepare the graph for usage with quantized params as one had to do before. This seems to be new and happens under the hood somehow. The magic has to happen within these few lines of code:\r\n\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n```\r\n\r\nIt would be nice if you could tell me where you found the documentation for these options. Thanks again!", "@vibhatha thanks for the hint, I saw that code but never thougth it was about the same issue", "> > Here is what I did using an existing code by another user. The license part is from him.\r\n> > I added a comment on what I did change.\r\n> > [Google Collab Link](https://colab.research.google.com/drive/1hQbm3gzOqEycFYj8woAUaYGPDRcHKXKb)\r\n> > Hope this would help.\r\n> > @ohtaman Example is a very neat code which would help you.\r\n> \r\n> @vibhatha\r\n> Thank you for your help. I have tested your code over the last few days, with mixed results.\r\n> \r\n> * When using the docker image with the TF 2.0.0-beta1, your code runs just fine. But as soon as I switch back to TF 1.14.0, I run into an error:\r\n> \r\n> ```\r\n> ... (error-free output)\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"test_main.py\", line 205, in <module>\r\n>     tflite_fname = gen_tflite()\r\n>   File \"test_main.py\", line 164, in gen_tflite\r\n>     converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 783, in __getattribute__\r\n>     return object.__getattribute__(self, name)\r\n> AttributeError: 'TFLiteConverter' object has no attribute 'target_spec'\r\n> ```\r\n> \r\n> Is there an equivalent of this attribute in v1? Also, you do not seem to prepare the graph for usage with quantized params as one had to do before. This seems to be new and happens under the hood somehow. The magic has to happen within these few lines of code:\r\n> \r\n> ```\r\n> converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\n> converter.representative_dataset = representative_dataset_gen\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n> converter.inference_input_type = tf.uint8\r\n> converter.inference_output_type = tf.uint8\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> ```\r\n> \r\n> It would be nice if you could tell me where you found the documentation for these options. Thanks again!\r\n\r\n@DocDriven \r\n\r\nIn 1.14 there is no field called \r\n\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`\r\n\r\nInstead please use \r\n\r\n`converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`\r\n\r\nWhat I do is, I keep a generic converter which is written with a factory design pattern to support compiling for multiple TF versions. \r\nAlways take the support of the IntelliSense in the IDE, it will show you that that API extension is not in 1.14. \r\n\r\nThis should resolve the issue. \r\n", "@vibhatha \r\nThanks to your efforts, I was able to create a custom model in tf 2.0.0 and load it locally with an interpreter via the Python API. The effects of quantization is recognizable, but insignificant.\r\n\r\nI just wanted you to ask if you have any experience with deploying tflite models on the Coral Edge TPU stick. I tried to do this with my successfully generated model, but are getting Segmentation faults all the time. If you can contribute, take a look at my corresponding GitHub issue: #30714 (even though this is for tf 1.14, I get the same error with tf 2.0.0)\r\n\r\nThanks again!", "@DocDriven \r\nCan you provide me the log file created after edgetpu_compiler was used?\r\nIn that one, please check how much of your operators were successfully converted with TPU support. \r\nIt is a detailed report. \r\n\r\nPossible issues, first check the expected input from that loaded \"edgetpu compiled model\"? \r\nMake sure you provide input with a similar shape?\r\nWhen the model is partially compiled for TPU support? I also got a segmentation fault? But I am no expert to tell why is that?\r\n\r\nDebug Steps\r\n------------------\r\n\r\n1. After generating tflite model from your python tflite conerter api reload the model (tflite model) and do inference using the python API with the interpreter? (Examples are in plenty for this). \r\n\r\nIf this works, it could mean that your h5 -> tflite conversion is good. \r\n\r\n2. Then convert the tflite with edgetpu_compiler and look at the logs. \r\n\r\nIf everything is converted with TPU support, I think you're good. \r\n\r\n3. Then doing inference in TPU device, make sure the provided input array is correct?\r\n\r\nIf any of your ops seems to be not quantizable, edgetpu compiler will exit with an unknown error (they are continously working on this so there will be detailed info in future [My assumption]). \r\n\r\nCan you try these steps? I will also look at the code. \r\n", "Hi @ohtaman\r\n\r\nThanks for the great notebook.\r\n\r\nI followed your notebook but I am getting following error with the edgetpu compiler. Everything else works fine. Any idea?\r\n\r\nEdge TPU Compiler version 2.0.258810407\r\nInvalid model: converted_model_simple_1_2.0.0-beta1.tflite\r\nModel could not be parsed\r\nINFO: Initialized TensorFlow Lite runtime.\r\nERROR: quantized_dimension must be in range [0, 1). Was 3.\r\n\r\n", "@ankitmaurya001 \r\nOnly converting from frozen graph works for me with tf1.14.0. Try this:\r\n```\r\n# aware training\r\ntf.keras.backend.clear_session()\r\ntrain_graph = tf.Graph()\r\ntrain_sess = tf.Session(graph=train_graph)\r\ntf.keras.backend.set_session(train_sess)\r\n\r\nwith tf.device('/gpu:0'): \r\n    with train_graph.as_default():\r\n\r\n        model = build_keras_model()\r\n\r\n        tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\r\n        train_sess.run(tf.global_variables_initializer())    \r\n\r\n        model.compile(\r\n            optimizer='adam',\r\n            loss='sparse_categorical_crossentropy',\r\n            metrics=['accuracy']\r\n        )\r\n        model.fit(train_images, train_labels, epochs=2)\r\n        save_model(model, path_model)\r\n\r\n        # save graph and checkpoints\r\n        saver = tf.train.Saver()\r\n        saver.save(train_sess, path_checkpoint)    \r\n\r\n# evaluate ?! I am not sure\r\ntf.keras.backend.clear_session()\r\neval_graph = tf.Graph()\r\neval_sess = tf.Session(graph=eval_graph)\r\ntf.keras.backend.set_session(eval_sess)\r\n\r\nwith eval_graph.as_default():\r\n    tf.keras.backend.set_learning_phase(0)\r\n    model = build_keras_model()\r\n    tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\r\n    eval_graph_def = eval_graph.as_graph_def()\r\n    saver = tf.train.Saver()\r\n    saver.restore(eval_sess, path_checkpoint)\r\n\r\n    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        eval_sess,\r\n        eval_graph_def,\r\n        [model.output.op.name]\r\n    )\r\n\r\n    open(path_frozen_graph, 'wb').write(frozen_graph_def.SerializeToString())        \r\n\r\n# convert to tflite from frozen graph\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file=path_frozen_graph, \r\n    input_arrays=[model.layers[0].input.name.split(':')[0]], \r\n    output_arrays=[model.layers[-1].output.name.split(':')[0]]\r\n)\r\n\r\nconverter.representative_dataset = representative_dataset\r\nconverter.inference_type = tf.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0] : (0., 255)}  # mean, std_dev\r\nconverter.default_ranges_stats = (-100, +100)\r\ntflite_model = converter.convert()\r\nopen(path_lite_model, 'wb').write(tflite_model)\r\n\r\n# convert to tpu lite version\r\ncmd = \"edgetpu_compiler %s -o %s\" % (path_lite_model, path_export)\r\nprint(os.popen(cmd).read())\r\n\r\n```", "I am trying to train a neural network that is done with convolutional 1d layers. I can train it without problems but when it comes to tflite_convert I have the following issue:\r\n\r\n`2019-07-31 16:51:40.890861: F tensorflow/lite/toco/tooling_util.cc:1709] Array conv1d/Relu, which is an input to the Conv operator producing the output array conv1d_1/Relu, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.`\r\n\r\nNote that if I use the same identical code without the convolutional1d (only dense layers) I have no problem and I can convert to tflite and compile for TPU. What is the problem? In my model I absolutely need the convolutional1d", "Yes, the issue clearly mentions what you need to do. It is better to add a representative_dataset from your dataset. If you can follow @ohtaman notebook provided in this thread.\r\n\r\n`def representative_dataset_gen():\r\n    for i in range(1000):\r\n        yield [train_images[i: i + 1]]`\r\n\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\nadd this to your converter object which is an instance of \r\n\r\n`converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)`\r\n\r\nHope this would resolve your issue. \r\n\r\nOr you can check the default ranges min-max param in the converter API and set them. \r\nBut if you add this representative dataset, it might go away. ", "Actually I am not using the api converter but the command line program. Here you can inspect my code: https://colab.research.google.com/drive/1vY1Y-mxQBmrH-YzNgh3nLP2hkOPV8-1K\r\nI forgot to mention that I am using quantization aware training, not post training quantization. So how should I use representetive dataset gen? Isn't this for post training quantization?\r\nThanks\r\n", "`!tflite_convert --help`\r\n\r\ncheck the necessary param. \r\n\r\nIn your case, I think \r\n\r\n`--default_ranges_min DEFAULT_RANGES_MIN\r\n                        Default value for min bound of min/max range values\r\n                        used for all arrays without a specified range,\r\n                        Intended for experimenting with quantization via\r\n                        \"dummy quantization\". (default None)\r\n  --default_ranges_max DEFAULT_RANGES_MAX\r\n                        Default value for max bound of min/max range values\r\n                        used for all arrays without a specified range,\r\n                        Intended for experimenting with quantization via\r\n                        \"dummy quantization\". (default None)`\r\n\r\nTry to set these values. ", "@jimmyvo2410 \r\nUsing quantization aware training worked for me for edgeTPU. Thanks to @1n1n1t3 \r\n\r\nI am facing issues in using post training integer quantization.\r\n\r\nHi @vibhatha ,\r\nI tried your notebook . Was able to compile the model for EdgeTPU. \r\nBut when I added a CONV2d layer at the top. I am facing an issue with the edge TPU complier .\r\n\r\n```\r\nEdge TPU Compiler version 2.0.258810407\r\nInvalid model: converted_model_simple_1_2.0.0-beta1.tflite\r\nModel could not be parsed\r\nINFO: Initialized TensorFlow Lite runtime.\r\nERROR: quantized_dimension must be in range [0, 1). Was 3.\r\n```\r\n\r\nHere is the modified link.\r\nhttps://colab.research.google.com/drive/13wH9eyjQ_YkBBiQhEoo6lDxKIKfRaboA\r\n\r\nAny idea ?\r\n\r\n", "Where did you add the layer?\nI can\u2019t see the modification?\n\nOn Thu, Aug 1, 2019 at 6:07 AM truthSeeker <notifications@github.com> wrote:\n\n> @jimmyvo2410 <https://github.com/jimmyvo2410>\n> Using quantization aware training worked for me for edgeTPU. Thanks to\n> @1n1n1t3 <https://github.com/1n1n1t3>\n>\n> I am facing issues in using post integer quantization.\n>\n> Hi @vibhatha <https://github.com/vibhatha> ,\n> I tried your notebook . Was able to compile the model for EdgeTPU.\n> But when I added a CONV2d layer at the top. I am facing an issue with the\n> edge TPU complier .\n>\n> Edge TPU Compiler version 2.0.258810407\n> Invalid model: converted_model_simple_1_2.0.0-beta1.tflite\n> Model could not be parsed\n> INFO: Initialized TensorFlow Lite runtime.\n> ERROR: quantized_dimension must be in range [0, 1). Was 3.\n>\n> Here is the modified link.\n> https://colab.research.google.com/drive/13wH9eyjQ_YkBBiQhEoo6lDxKIKfRaboA\n>\n> Any idea ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27880?email_source=notifications&email_token=AC45OE3RSENQKKS55BDOEATQCK7YHA5CNFSM4HGE2DLKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3KGFZQ#issuecomment-517235430>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AC45OEYEEBVMKDVPMISBZSLQCK7YHANCNFSM4HGE2DLA>\n> .\n>\n-- \nVibhatha Abeykoon\n", "@vibhatha just at the bottom.\r\n\r\n\r\n```\r\n# Now change the input shape to (28,28,1)\r\n\r\n#train_images = train_images.astype(np.float32)\r\n#test_images = test_images.astype(np.float32)\r\n\r\ntrain_images = train_images[:, :, :, np.newaxis].astype(np.float32)\r\ntest_images = test_images[:, :, :, np.newaxis].astype(np.float32)\r\n\r\n\r\n\r\n#change the input to (28,28,1)\r\n#Add CONV2D and Activation layer\r\ndef build_model_2():\r\n    inputs = keras.layers.Input(shape=(28, 28,1))\r\n    x = keras.layers.Conv2D(16, (3, 3), padding='same')(inputs)\r\n    x = keras.layers.Activation('relu')(x)\r\n    x = keras.layers.Flatten()(x)\r\n    encoded = keras.layers.Dense(128, activation='relu')(x)\r\n    x = keras.layers.Dense(10)(encoded)\r\n    decoded = keras.layers.Dense(10, activation='softmax')(x)\r\n\r\n    model = keras.models.Model(inputs, decoded)\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    return model\r\n  \r\n  \r\nmodel = build_model_2()\r\ntrain_model()\r\npredictions = do_prediction()\r\n#plot_stuff1()\r\n#plot_stuff2()\r\n#plot_stuff3()\r\nsummary1()\r\ntflite_fname = gen_tflite()\r\ninference()\r\n\r\nprint(train_images.shape)\r\nprint(test_images.shape)\r\nprint(model.summary())\r\n\r\n```", "@ankitmaurya001 \r\nmake sure the representative data also provides the expected input layer's shape. Because I think 28,28 is the previous input shape. I will check this as well. \r\n\r\nBtw, which edge device are you using to test the tflite model after compiling it to edgetpu_compileable model? ", "@vibhatha \r\nI think representative data takes the correct input shape as train_images.shape is changed to (28,28,1). \r\n\r\nI am using google coral dev board.\r\n\r\nThanks for the help!!\r\n\r\n", "@ankitmaurya001 \r\nCan you compile the code you shared? I cannot compile it. \r\nAre you sure you used 2.0.0-beta?", "@vibhatha \r\nNo, with (28,28,1) shape and conv2d it does not even complie with edge TPU compiler .\r\n\r\nI am facing the same error with @ohtaman notebook.\r\nhttps://colab.research.google.com/gist/ohtaman/7101e90f1f71a1cb054364d179845866/tf2-0-edgetpu-keras-cnn-autoencoder.ipynb\r\n\r\nIt throws the same error. Only difference I can think of is the change in compiler version.\r\n\r\n**@ohtaman  notebook uses 1.0.249710469\r\nWhile my version is 2.0.258810407.** \r\n\r\nI am not sure where to get the old complier. Might be interesting to check.\r\n\r\nAny thoughts?", "No I mean the training part doesn't happen. Can you unit test your code after adding this new layer? \r\n\r\nStep 1, check model.train and then model.predict\r\n\r\nIf these two checks out, we can unit test the other components. I had an issue in running the training model. ", "@ankitmaurya001 \r\nI tested @ohtaman notebook previously and it did work fine. \r\nLet me check it again. I developed models in both 1.14 and 2.0.0-beta and they work fine. \r\nAlso, I recently updated my dev board with the July update with 2.11 runtime. \r\nBut I tested with the default runtime which was there (I guess it was a setup a while ago) and it worked just fine. I have both post quantized and qt aware models up and running in tf 1.14 but they are also working with tf 2.0.0-beta as well. I doubt runtime is the issue or tf vesion. Could you do the unit testing? ", "@vibhatha \r\n\r\nI cleaned up the code a little. Please check you should be able to run now. Yes I am using 2.0.0-beta .\r\n\r\nI am thinking it has to do with the edge TPU compiler version not the tf version.\r\n\r\n```\r\n%%bash\r\nedgetpu_compiler -v\r\nEdge TPU Compiler version 2.0.258810407\r\n```\r\n\r\nWhich compiler version you used ?", "Ah yes, when I got the latest updates on the dev board, it was updated with the latest. Now the runtime is v.12 and my compiler version is Edge TPU Compiler version 2.0.258810407 [same as yours]\r\n\r\n", "Are you able to compile with the latest version (with CONV2D + 3D input shape)?", "I cannot compile yours, but I definitely have a set of conv2d layers in the design, but I didn't get such issue. I also use 3D inputs. I will look into this. Till now I couldn't figure out why.\r\nThere is a stack-overflow question raising a similar concern. Did you check this with existing issues in TF Github issues? ", "Did you compile your design with the latest compiler without any issues?\r\n\r\nYeah, I saw the stack overflow concern, but still no answers :(.\r\nChecking the TF Github issues now !!\r\n\r\n", "Yes, I was able to get it compiled no issues. Yes, the main reason is the error message is not clear. If it points out to a specific cause. I think an API expert will definitely understand why this is happening. I am not sure why it is happening. It has to be something with the PQ related conversion params. Nothing to do with the model. Some param is missing or some param is inserted incorrectly. I am sorry, I cannot give a 100% correct answer. ", "@vibhatha \r\nNo issues, thanks for the help anyways. Will explore further !!", "I am using post training quantization with a function similar to the @ohtaman one, mine is:\r\n`def representative_dataset_gen():\r\n    for i in range(num_calibration_steps):\r\n        yield [x_train_scaled[i:i+1]]`\r\nbut I always get the following error: \r\n`ValueError: Cannot set tensor: Got tensor of type NOTYPE but expected type FLOAT32 for input 16, name: dense_input`\r\nThe variable `x_train_scaled` is a numpy array containing the dataset (each row is one example of the dataset). What's wrong?", "`diff = num_calibration_steps - x_trained_scaled.shape[0]  `\r\n\r\nWhat is this value?", "Its -20475, since the dimension of the `x_train_scaled` (is 20575) it's much bigger than the `num_calibration_step` (I used 100)", "Can u iterate through the yielding values can make sure all values are of type FLOAT32?\r\n", "Yes, this is a generic output of what is yielding: `[array([[ 0.04949207, -0.75925859, -0.30664617, ..., -0.38399959,\r\n         2.03520339, -0.77253633]])]`", "so all of the values are dtype FLOAT32. \r\nDo you have a code snippet? It is hard to tell otherwise. ", "Sure, the neural network is the following very simple:\r\n\r\n\r\ndef create_model():\r\n    model = Sequential()\r\n    model.add(Dense(500, input_shape=(time_periods,), activation=\"relu\"))\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(500, activation=\"relu\"))\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(500, activation=\"relu\"))\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(500, activation=\"relu\"))\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(1, activation=\"sigmoid\"))\r\n    return model\r\n\r\n\r\nThen it's just compiled with `model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])`. \r\nAt the end It's saved with `model.save(filepath=\"/content/drive/My Drive/post_quantum.h5\", include_optimizer=True)`. \r\n\r\nThe code of the generator is:\r\n``num_calibration_steps=100\r\ndef representative_dataset_gen():\r\n    for i in range(num_calibration_steps):\r\n        yield [x_train_scaled[i]]``\r\nwhere `x_train_scaled` has been used during training, and has shape (20575, 4492).\r\nThe code for quantization and conversion is:\r\n``converter = tf.lite.TFLiteConverter.from_keras_model_file(\"DenseModel.h5\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()``", "why haven't you set the input and output type?\r\n\r\n`converter.representative_dataset = representative_dataset_gen\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_model = converter.convert()`\r\n\r\nTry something like this, but make sure you correctly add the input and output types based on your model. ", "I get always the same error, nothing changes", "If you have a google collab link, please share if it is okay with you. It is hard to tell without looking at the code. ", "Sure I can, here's the link: https://colab.research.google.com/drive/1vFSEvu4mudfDb2vLMXtgGmQEMBqLwqnN\r\nLet me know if you need the dataset too.\r\nThank you very much!", "No need, just mention the shape and types of X and Y", "X has shape (20575, 4492); Y has shape 20575 (1D vector, is a classification problem with two classes, so elements of Y are or 1 or 0).", "@ulorentz \r\nI see that you are using \r\n`converter = tf.lite.TFLiteConverter.from_keras_model_file(\"/content/drive/My Drive/model_quant_aware.h5\")`\r\n\r\nCan you try this instead?\r\n\r\ntf version : 2.0.0-beta1\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"/content/drive/My Drive/model_quant_aware.h5\")\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n```", "> @ulorentz\r\n> I see that you are using\r\n> `converter = tf.lite.TFLiteConverter.from_keras_model_file(\"/content/drive/My Drive/model_quant_aware.h5\")`\r\n> \r\n> Can you try this instead?\r\n> \r\n> tf version : 2.0.0-beta1\r\n> \r\n> ```\r\n> converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"/content/drive/My Drive/model_quant_aware.h5\")\r\n> converter.representative_dataset = representative_dataset_gen\r\n> converter.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n> converter.inference_input_type = tf.uint8\r\n> converter.inference_output_type = tf.uint8\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> ```\r\n\r\nGood suggestion. If you look at the runtime warnings, it definitely shows that the depreciated APIs and suggest using this API. ", "> Yes, I was able to get it compiled no issues. Yes, the main reason is the error message is not clear. If it points out to a specific cause. I think an API expert will definitely understand why this is happening. I am not sure why it is happening. It has to be something with the PQ related conversion params. Nothing to do with the model. Some param is missing or some param is inserted incorrectly. I am sorry, I cannot give a 100% correct answer.\r\n\r\n@vibhatha \r\nIf it is not much of a problem, can you share a simple notebook with 3D input and CONV2D layers \r\nwhich you are able to compile with the latest edge TPU compiler using post-training quantization?\r\n\r\nWill be of great help!!", "@ankitmaurya001 \r\nI will try to compile one snippet. ", "Finally I am able to convert to tflite with post integer quantization, but now I am facing the same problem of @ankitmaurya001, that is `ERROR: quantized_dimension must be in range [0, 1). Was 3.`\r\n\r\nYou can inspect my code at https://colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7", "Actually I got the same error even with the @ohtaman code (I little modified it in order to work with tflite 2.0): https://colab.research.google.com/gist/ulorentz/4375fea7099c15a47204819b92a85fcd/tf2-0-edgetpu-keras-cnn-autoencoder.ipynb", "Hello,\r\nI am seeing the same problem using the code in:\r\n https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb \r\n\r\nas an example to get the tflite quantized models.  When using the compiler I get:\r\n\r\n...coral/compiler$ edgetpu_compiler mnist_model_quant.tflite\r\nEdge TPU Compiler version 2.0.258810407\r\nINFO: Initialized TensorFlow Lite runtime.\r\nERROR: quantized_dimension must be in range [0, 1). Was 3.\r\nInvalid model: mnist_model_quant.tflite\r\nModel could not be parsed\r\n\r\nI am wondering if anybody has found a way forward ? Thanks,\r\n", "> Hello,\r\n> I am seeing the same problem using the code in:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\r\n> \r\n> as an example to get the tflite quantized models. When using the compiler I get:\r\n> \r\n> ...coral/compiler$ edgetpu_compiler mnist_model_quant.tflite\r\n> Edge TPU Compiler version 2.0.258810407\r\n> INFO: Initialized TensorFlow Lite runtime.\r\n> ERROR: quantized_dimension must be in range [0, 1). Was 3.\r\n> Invalid model: mnist_model_quant.tflite\r\n> Model could not be parsed\r\n> \r\n> I am wondering if anybody has found a way forward ? Thanks,\r\n\r\nI have the same problem here! Does anyone have a clue?", "I got a reply from Google saying that there was a problem with the edgetpu compiler and keras based models such as the mnist conv model used in the: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb \r\n\r\ntutorial.\r\n\r\nThey were looking into it but did not know about timing. \r\n\r\nAnybody has a post training quantization notebook example that is compatible with the edge tpu compiler and is willing to share ?  Please.  I just need something simple to get started. \r\n\r\n", "The same here: \r\n\r\n```\r\nEdge TPU Compiler version 2.0.258810407\r\nINFO: Initialized TensorFlow Lite runtime.\r\nERROR: quantized_dimension must be in range [0, 1). Was 3.\r\nInvalid model: /data/model.tflite\r\nModel could not be parsed\r\n```\r\n\r\nTF2 Beta1 Keras model with post-training quantization:\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_data_gen\r\n```\r\n\r\nTFLite model inference in notebook works without issues. Problem with `edge_compiler` being cryptic. \r\n\r\nA side note: I have my simple image classification model. Nothing weird.", "Same here...\r\n\"ERROR: quantized_dimension must be in range [0, 1). Was 3.\"\r\n\r\nNoticeably, the batch normalization layers are conveniently missing from the EdgeTPU sample model here: https://coral.withgoogle.com/docs/edgetpu/retrain-classification/\r\n\r\nand other examples that use EdgeTPU quantization like:\r\nhttps://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html\r\nalso use their own form of batch normalization layers, apparently to get around this problem.\r\nNot sure if it's just a problem when fuse=True or if it's a more general problem with batch normalization layers.", "Apparently (as stated in their website - they just added it-) using TensorFlow 1.15 \"nightly\" works. I can confirm that trying to compile a Resnet50 works with that build (with plain tensorflow I had the \"quantized dimension range error\").", "@ulorentz Grazzi mille! Despite trying a lot of workarounds such as using EfficientNet or changing batchnormalization layer's `fuse` parameter to `False`, I spent a couple weeks without getting a single model to compile on the EdgeTPU. I can confirm that only by using the unreleased TF nightly 1.15, that I was finally able to get models to consistently compile on the EdgeTPU device. Any models that include Keras BatchNormalization layers such as transfer learned models built off of MobileNetV2 or ResNet50 seem to fail automatically when compiled with any other version of TensorFlow.", "Hello, @ulorentz, @Alekxos \r\nI have the same problem with TF2.0rc1 (use MobileNet v2). As a result of visualizing the model with [visualize.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/visualize.py), Input / Output is Float, not Full intger quantization model. I think this is the reason why EdgeTPUCompiler fails.\r\n\r\nI have found that using \"**from_keras_model_file**\" instead of \"**from_keras_model**\" works well for model conversion.\r\n(When \"from_keras_model_file\" is used, Input / Output becomes UINT8 and becomes Full intger quantization model. )\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(path_to_model_file)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_full_integer_quant_model = converter.convert()\r\ntflite_full_integer_model_quant_file = models_dir/'mobilenet_v2_full_integer_quant.tflite'\r\ntflite_full_integer_model_quant_file.write_bytes(tflite_full_integer_quant_model)\r\n```", "I am able to do post training full integer quantization, with mobilenet V1. Compiled with edge TPU , worked fine.\r\n\r\nThe conversion from floating point to INT needs to be done only with tensorflow nightly 1.15, other wise this \"ERROR: quantized_dimension must be in range [0, 1). Was 3.\" error occurs. Regards", "@ohtaman I just want to know how do you decide the output layer name as : dense_1/Softmax?\r\nWhat is the rule of the naming? How can I check the name of last layer?\r\nAlso, can I name it with my customized name for it? thanks.", "@jk78346, you can open your model with netron, to check the name of the last layer.", "> Finally I am able to convert to tflite with post integer quantization, but now I am facing the same problem of @ankitmaurya001, that is `ERROR: quantized_dimension must be in range [0, 1). Was 3.`\r\n> \r\n> You can inspect my code at https://colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7\r\n\r\nCan you share a dummy_data so that we can run and reproduce the issue? Thanks!", "> > > @1n1n1t3 did you test the runtime of your generated tf-lite files?\r\n> > > training seems to go correctly, the input and output details of my .tflite file seem consistent\r\n> > > ```\r\n> > > >>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')\r\n> > > >>> interpreter.allocate_tensors()\r\n> > > >>> input_details = interpreter.get_input_details()\r\n> > > >>> output_details = interpreter.get_output_details()\r\n> > > >>> print(input_details)\r\n> > > [{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]\r\n> > > >>> print(output_details)\r\n> > > [{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).\r\n> > > However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.\r\n> > > Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.\r\n> > > This should not be the case.\r\n> > \r\n> > \r\n> > Are you using the Coral USB Accelerator ? I tried it on the Coral Dev Board and the model I posted above is very fast. It goes something like 1500 fps. I'm used the MNIST Fashion dataset (input: 28,28,1) though. Not sure if that is fast, normal or slow for that, tbh, but the MobilenetSSD models run at about 300 fps.\r\n> > For sure you are not supposed to get slower performance on quantized models though. Maybe the problem is something specific to the Raspberry PI and the environment.\r\n> \r\n> Nope, I'm running on native Raspberry PI (it's for my master thesis, I just use the Raspberry PI as a benchmarck which is why I don't use USB accelerators). I did not precise it but I'm using a MobileNetv1 adapted to CIFAR-10's input size.\r\n> \r\n> Yeah that extremely weird, at least it's not specific to the Raspberry Pi since it happens on my desktop computer as well. It might be worthwile to test the floating point 32 model in your use case as well (I'm not asking for anything, I'm already extremely grateful for your help with the BatchNorm).\r\n\r\nIs this problem solved now?", "> > Finally I am able to convert to tflite with post integer quantization, but now I am facing the same problem of @ankitmaurya001, that is `ERROR: quantized_dimension must be in range [0, 1). Was 3.`\r\n> > You can inspect my code at [colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7](https://colab.research.google.com/drive/1h184XpvfGk5d-w6XZdiYORtV7ztbHCm7)\r\n> \r\n> Can you share a dummy_data so that we can run and reproduce the issue? Thanks!\r\n\r\nI am experiencing the same problem. My code is here: https://pastebin.com/LfmPANwV. This is my error: https://pastebin.com/pW1rchLj. Here is all the data you should need to reproduce and debug locally: [train_and_convert_for_coral.zip](https://github.com/tensorflow/tensorflow/files/3913277/train_and_convert_for_coral.zip). You should also know that I run the notebook (in the zip I attached) in a Docker container here: https://hub.docker.com/r/wpilib/axon-playground (just run it and it will start the notebook server).", "@tensorflowbutler Yes, I guess it still is", "This is still an issue, no way to do quantization-aware training, even in TF2.1.\r\n\r\nWas expected Q2 2019, now it's Q1 2020.", "Hi there @gilescoope. I'm one of the individuals developing the Keras quantization-aware training API.\r\n\r\nStay tuned on https://www.tensorflow.org/model_optimization and its roadmap. We're currently in the midst of creating user-facing documentation and tutorials, together with addressing some outstanding usability issues (e.g. removing the need for quantized_input_stats, as previously needed) and doing some final testing. With our successful training experiments, we are also in a good shape. There is a decent chance that initial usage will have to depend on tf-nightly, after which we'll move onto a stable TF release - we'll see what happens there.\r\n\r\nI'll update this issue once it has been launched. I apologize for prior miscommunication on timelines. Following the Keras quantization API launch, we'll be making the progress (or non-progress) on major features a bit more transparent.", "An update since my last comment:\r\n\r\nWe did a launch review and one of the suggestions (to the benefit of most users) is to get people to dogfood the tool. This is currently in the process and we have a number of teams actively testing it. As developers, we will also be training on additional models ourselves (while expanding model coverage) to improve upon things. \r\n\r\nGiven this and the work I previously mentioned, things will take longer (but be more polished at launch). Expect it take around a month (end of March) (and hopefully not longer) for the initial release with docs, a blog post, and the pip package. ", "For expectations on what would be available at launch, see what we have for [pruning](https://www.tensorflow.org/model_optimization/guide/pruning) on the [API Compatibility Matrix](https://www.tensorflow.org/model_optimization/guide/pruning#api_compatibility_matrix) as a reference point. Notably in contrast to pruning, we would only be supporting TF 2.X to start at launch and tf.distribute integration may not quite be there yet. For fundamental reasons on the way quantization works, subclassed models will have even less support.\r\n\r\nPost launch, we'd work on tf.distribute and increased model coverage, prioritizing essentially based on what users request. If we don't get enough user feedback, we'd prioritize based on our intuition and past experience.", "Also new quantization infra will come in MLIR https://llvm.discourse.group/t/rfc-a-proposal-for-implementing-quantization-transformations-in-mlir/655", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "> It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nyes we are awaiting updates slated for end March. Thanks to everyone working on this!", "Update from my last comment:\r\n\r\n- Tutorials and examples are in-review but almost complete with the general structure approved.\r\n- Initial LGTM to move through launch process based on good experiments results and feedback.\r\n- Making some final minor tweaks to improve usability (e.g. error messages, pydocs, etc.) and clarifying what's available at launch\r\n\r\nEnd of March is reasonable and if not ~ a week later given some processes. \r\n\r\n", "I found this page indicating how to use tfmot.quantization to do quantization-aware training: https://www.tensorflow.org/model_optimization/guide/quantization/api_usage\r\nBut it seems this api is not released yet?", "Update given that it's past the end of March. We've just gotten all approvals and are just finishing up the final steps. \r\n\r\nA few documents need to be submitted and a couple more PRs. Then we'll do the release at some scheduled date. \r\n\r\n@kazenokizi : the examples in that document don't fully work and the API is not released yet. We were incrementally working on the documents. ", "Hi everyone. The Keras quantization aware training API has now launched!\r\n\r\nEdit: please reference the below. I will no longer respond to comments on this thread. \r\n\r\nDocumentation on what versions of TensorFlow and models are supported, as well as general API \r\ncompatibility and support matrices: https://www.tensorflow.org/model_optimization/guide/quantization/training. This page also links to other end-to-end tutorials.\r\n\r\nFile feature requests and bugs: after reading the above, you can go to https://github.com/tensorflow/model-optimization for filing anything. In general,\r\nif it's an error you run into from using \"import tensorflow_model_optimization\", file it there.\r\nIf it's an error you run into from using \"import tensorflow as tf\" after the correct versions of TensorFlow, file it in the main https://github.com/tensorflow/tensorflow.\r\n\r\nBlog post: https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html\r\n\r\nTwitter: https://twitter.com/TensorFlow/status/1247933575183233024\r\n\r\nEdit:added content from below up here for ease \r\n", "Thanks for the update @alanchiao ! Which version of TF are the changes in the video in?  I didn't see the changes mentioned in these release notes: https://github.com/tensorflow/tensorflow/releases", "For general documentation, refer to the TFMOT documentation and Github since the tool is installed via the TFMOT pip package, not the TensorFlow pip package.\r\n\r\nSee the [overview page](https://www.tensorflow.org/model_optimization/guide/quantization/training#api_compatibility) for what versions of TF can be used. This information is also mentioned in the [release notes for TFMOT](https://github.com/tensorflow/model-optimization/releases).\r\n\r\nGenerally, it's best to reference the documentation and code examples on tensorflow.org/model_optimization, since that will stay updated, as opposed to the blog post and videos.\r\n\r\n\r\n\r\n", "In general for documentation, if you think certain aspects could be more visible/easier to find, we welcome feedback. You'll see from the documentation that there are different types of users, and we tried to make the documentation accommodate all of them.", "Hi @alanchiao,\r\ni built an NN with BatchNormalization layer  and i have tried to quantize the whole model for EdgeTPU application. But it doesn't work. I'm using tfmot 0.3.0 and tensorflow 2.1.\r\n\r\nThis is the error\r\n\r\n```\r\nRuntimeError: Layer batch_normalization:<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'> is not supported. You can quantize \r\nthis layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.\r\n```\r\nand the code\r\n\r\n```\r\nmodel = create_model()\r\nquant_aware_model = tfmot.quantization.keras.quantize_model(model)\r\nquant_aware_model.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nquantized_tflite_model = converter.convert()\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@lravano and others.\r\n\r\nPlease reference the [overview page](https://www.tensorflow.org/model_optimization/guide/quantization/training#api_compatibility) to see what is supported currently.\r\n\r\nThen if something is supported but you're facing an issue, file a bug request. If it's not supported, file a feature request, ideally with the motivation (e.g. what model you are trying to apply quantization to, which may be relevant to others in the community also). You can do that at https://github.com/tensorflow/model-optimization.\r\n\r\nFor the case of BatchNormalization, you can see in the overview page that currently BatchNormalization is currently just supported only when it's after a convolutional or Dense layer. You should then file a feature request. ", "Hello! \r\nFor example, I made this command:\r\n\r\nmodel = create_model()\r\nquant_aware_model =tfmot.quantization.keras.quantize_model(model)\r\nquant_aware_model.summary()\r\n\r\nBut how will I able to convert this model with uint8 format?\r\nThis look as float format:\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nquantized_tflite_model = converter.convert()\r\n\r\nThese commands do not work:\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)", "@alex283h\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nquantized_tflite_model = converter.convert()\r\n```\r\nWhen I tried this example code with `tf2.1`, it returns all weights with float32. \r\nHowever, when I tried this with `tf-nightly`, all the weights are converted to int8 except input and output layers.\r\n\r\nI still wonder whether there is an option to convert the input/output layers to `uint8`.\r\nAlso for me, the below code doesn't work.\r\n\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\r\n```\r\n\r\n", "Please now look at https://github.com/tensorflow/tensorflow/issues/27880#issuecomment-611079359. I'll now stop commenting on this thread and only comment on other support channels.\r\n\r\n@alex283h, @hyungui: Thanks Hyungui for helping Alex. Officially, TF 2.1 is not supported, only tf-nightly. See our docs [here](https://www.tensorflow.org/model_optimization/guide/quantization/training#api_compatibility). When it works on a stable TF release, we'll update this page.\r\n\r\n@hyungui : regarding your question, see [here](https://www.tensorflow.org/model_optimization/guide/quantization/training#general_support_matrix), which points to a Github issue. It's a known issue that what you're trying doesn't work. ", "**@hyungui**, thank you for explain! I get the same results:\r\n>>tf2.1 returns all weights with float32;\r\n>>tf-nightly, all the weights are converted to int8 except input and output layers. \r\n\r\nIn fact it would be greate to have some way to get uint8 for the next converting in edge_tpu model.\r\n\r\n**@alanchiao** Thank you. I will study your links, maybe I will found the solution about uint8 there. If somebody know how to do it after learning model with quantization.keras.quantize_mode I will ve much appresiate.", "is tf.quantization.fake_quant_with_min_max_vars still useful now that quantization aware training is out?\r\n\r\nIs quantization aware training solely for getting TFLite models? and we would still have to fake_quant_with_min_max_vars to get the best INT8 TensorRT models? Or is there something I am missing out?", "> Hi @alanchiao,\r\n> i built an NN with BatchNormalization layer and i have tried to quantize the whole model for EdgeTPU application. But it doesn't work. I'm using tfmot 0.3.0 and tensorflow 2.1.\r\n> \r\n> This is the error\r\n> \r\n> ```\r\n> RuntimeError: Layer batch_normalization:<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'> is not supported. You can quantize \r\n> this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.\r\n> ```\r\n> \r\n> and the code\r\n> \r\n> ```\r\n> model = create_model()\r\n> quant_aware_model = tfmot.quantization.keras.quantize_model(model)\r\n> quant_aware_model.summary()\r\n> \r\n> converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> \r\n> quantized_tflite_model = converter.convert()\r\n> ```\r\nDid you find a solution to this problem?\r\nI am also stuck at exactly the same place. There a large number of image segmentation models that use batch_normalization and running them on the edge TPU would be awesome.", "> @1n1n1t3 did you test the runtime of your generated tf-lite files?\r\n> training seems to go correctly, the input and output details of my .tflite file seem consistent\r\n> \r\n> ```\r\n> >>> interpreter = tf.lite.Interpreter(model_path=f'quantized_mobv1_d15_w100_1.tflite')\r\n> >>> interpreter.allocate_tensors()\r\n> >>> input_details = interpreter.get_input_details()\r\n> >>> output_details = interpreter.get_output_details()\r\n> >>> print(input_details)\r\n> [{'name': 'conv2d_input', 'index': 169, 'shape': array([ 1, 32, 32,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.015625, 128)}]\r\n> >>> print(output_details)\r\n> [{'name': 'dense/Softmax', 'index': 172, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\r\n> ```\r\n> \r\n> The accuracy of the floating-point model is of 94.68%, while the quantized model one is 94.18% (CIFAR-10).\r\n> \r\n> However, when running the model using tf-lite binaries (tf 1.13.1 as well). The quantized model is 3 times slower than the floating-point one (0.5s vs 0.15s) on a Raspberry PI 3B.\r\n> Using the tf-lite interpreter within python (on a desktop computer) results in the unquantized version being 16* faster.\r\n> This should not be the case.\r\n\r\nwhen i use tf.keras quantized,TypeError: unsupported operand type(s) for +: 'NoneType' and 'float"]}, {"number": 27879, "title": "Which latest python is supported by the latest version?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Tensorflow 1.13.1 now supports Python 3.7.", "Closing this issue since @Dayananda-V has correctly answered the question. Thanks!", "ok but now it prompt me ImportError: DLL load failed: what should i do "]}, {"number": 27878, "title": "Fix windows build for CPU too", "body": "", "comments": []}, {"number": 27877, "title": "tf.keras with dataset operates much slower without persistence mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1 (from Anaconda)\r\n- Python version: 3.6.8 (Anaconda)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Titan Xp 12 Gb\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI am training a straightforward Keras stacked LSTM model, using model.compile and model.fit. When I train using nvidia-smi with persistence mode off, GPU utilization is at ~5-10% and training ETA goes up 3x. When I turn on persistence mode, GPU utilization goes to 60% and stays there.\r\n**Describe the expected behavior**\r\nI would like this code to work without needing to enable persistence mode.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nI can if deemed necessary. \r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}, {"number": 27876, "title": "Tests for BatchDatasetOp", "body": "This PR enables AsGraphDefInternal to work with two versions of BatchDatasetOp and adds tests for `BatchDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": ["Got it. I rewrite this PR:\r\n\r\n- https://github.com/tensorflow/tensorflow/pull/27876/commits/dac2bf1ba9361da4c416776fece91564f7bc174f: Minor updates of the comments in dataset.h file\r\n\r\n- https://github.com/tensorflow/tensorflow/pull/27876/commits/bed70239456192e2aa7da32cc34c966c40914af4: add the tests for BatchDatasetOp (remove the parts for op_version 1)", "The failure log for the test `Ubuntu CC` can't be accessed. Could you please help paste the log here?", "The `Ubuntu CC` test runs successfully on my laptop. @gbaned Could you please help re-trigger the test?", "Thanks @gbaned for re-triggering the tests. \r\n\r\nRecently, some changes (https://github.com/tensorflow/tensorflow/commit/0c23c60a1a013901c85e340c60afdfd9ec6e9dce#diff-79a93a3e76fded5f3522ad99b27976fd) were added to `BatchDatasetV2`, which causes the test failures. Will revise this PR accordingly. ", "@feihugis Any update please?  Thanks!", "@gbaned Sorry for the delay as a conference these two days. Will fix the issue today.", "> Recently, some changes ([0c23c60#diff-79a93a3e76fded5f3522ad99b27976fd](https://github.com/tensorflow/tensorflow/commit/0c23c60a1a013901c85e340c60afdfd9ec6e9dce#diff-79a93a3e76fded5f3522ad99b27976fd)) were added to `BatchDatasetV2`, which causes the test failures. Will revise this PR accordingly.\r\n\r\n@jsimsa @gbaned The new attr `parallel_copy` is added to `BatchDatasetOpTest` via https://github.com/tensorflow/tensorflow/pull/27876/commits/6e7399a4ca8df3b7d55f981605d32a00aaf51890. Could you please have a look at the change when you get a chance?\r\n\r\n", "> Recently, some changes ([0c23c60#diff-79a93a3e76fded5f3522ad99b27976fd](https://github.com/tensorflow/tensorflow/commit/0c23c60a1a013901c85e340c60afdfd9ec6e9dce#diff-79a93a3e76fded5f3522ad99b27976fd)) were added to `BatchDatasetV2`, which causes the test failures. Will revise this PR accordingly.\r\n> \r\n@jsimsa `BatchDatasetV2` got a new attribute (`parallel_copy`), so I revise this PR accordingly by adding `parallel_copy` to `BatchDatasetOpTest` via [6e7399a](https://github.com/tensorflow/tensorflow/commit/6e7399a4ca8df3b7d55f981605d32a00aaf51890). Could you please review the change when you have time?\r\n\r\n"]}, {"number": 27875, "title": "Second Order derivative of sparse_softmax_cross_entropy_with_logits", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Non\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): PyPI\r\n- TensorFlow version (use command below): 2.0-alpha\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behaviour**\r\nCurrently trying to take second order derivatives of `sparse_softmax_cross_entropy_with_logits` leads to the error:\r\n```\r\nLookupError: Gradient explicitly disabled. Reason: b\"Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()\"\r\n```\r\n\r\n**Describe the expected behavior**\r\nThis should work.\r\n", "comments": ["Just a quick show for a workaround (and potentially how this might be handled internally)\r\n```\r\n@tf.custom_gradient\r\ndef sparse_softmax_cross_entropy_with_logits_with_gradients(labels, logits):\r\n    \"\"\" Copied from tf.nn.sparse_softmax_cross_entropy_with_logits_with_gradients \"\"\"\r\n    with tf.name_scope(\"SparseSoftmaxCrossEntropyWithLogits\"):\r\n        labels = tf.convert_to_tensor(labels)\r\n        logits = tf.convert_to_tensor(logits)\r\n        precise_logits = tf.cast(logits, tf.float32) if (tf.as_dtype(logits.dtype) == tf.float16) else logits\r\n\r\n        # Store label shape for result later.\r\n        labels_static_shape = labels.get_shape()\r\n        labels_shape = tf.shape(labels)\r\n        static_shapes_fully_defined = (\r\n                labels_static_shape.is_fully_defined() and\r\n                logits.get_shape()[:-1].is_fully_defined())\r\n        if logits.get_shape().ndims is not None and logits.get_shape().ndims == 0:\r\n            raise ValueError(\r\n                \"Logits cannot be scalars - received shape %s.\" % logits.get_shape())\r\n        if logits.get_shape().ndims is not None and (\r\n                labels_static_shape.ndims is not None and\r\n                labels_static_shape.ndims != logits.get_shape().ndims - 1):\r\n            raise ValueError(\"Rank mismatch: Rank of labels (received %s) should \"\r\n                             \"equal rank of logits minus 1 (received %s).\" %\r\n                             (labels_static_shape.ndims, logits.get_shape().ndims))\r\n        if (static_shapes_fully_defined and\r\n                labels_static_shape != logits.get_shape()[:-1]):\r\n            raise ValueError(\"Shape mismatch: The shape of labels (received %s) \"\r\n                             \"should equal the shape of logits except for the last \"\r\n                             \"dimension (received %s).\" % (labels_static_shape,\r\n                                                           logits.get_shape()))\r\n        # Check if no reshapes are required.\r\n        if logits.get_shape().ndims == 2:\r\n            cost, dcost_dlogits = _tf_sotfmax_with_grads(precise_logits, labels)\r\n            if logits.dtype == tf.float16:\r\n                cost = tf.cast(cost, tf.float16)\r\n                dcost_dlogits = tf.cast(dcost_dlogits, tf.float16)\r\n\r\n            def grad(dcost, d2cost_dlogits):\r\n                return None, grad_of_sparse_softmax_cross_entropy_with_logits(logits, dcost_dlogits, dcost)\r\n\r\n            return (cost, dcost_dlogits), grad\r\n\r\n        # Perform a check of the dynamic shapes if the static shapes are not fully\r\n        # defined.\r\n        shape_checks = []\r\n        if not static_shapes_fully_defined:\r\n            shape_checks.append(\r\n                tf.assert_equal(\r\n                    tf.shape(labels),\r\n                    tf.shape(logits)[:-1]))\r\n        with tf.control_dependencies(shape_checks):\r\n            # Reshape logits to 2 dim, labels to 1 dim.\r\n            num_classes = tf.shape(logits)[tf.rank(logits) - 1]\r\n            precise_logits = tf.reshape(precise_logits, [-1, num_classes])\r\n            labels = tf.reshape(labels, [-1])\r\n            # The second output tensor contains the gradients.  We use it in\r\n            # _CrossEntropyGrad() in nn_grad but not here.\r\n            cost, dcost_dlogits = _tf_sotfmax_with_grads(precise_logits, labels)\r\n            cost = tf.reshape(cost, labels_shape)\r\n            cost.set_shape(labels_static_shape)\r\n            dcost_dlogits = tf.reshape(dcost_dlogits, logits.shape)\r\n            dcost_dlogits.set_shape(logits.shape)\r\n            if logits.dtype == tf.float16:\r\n                cost = tf.cast(cost, tf.float16)\r\n                dcost_dlogits = tf.cast(dcost_dlogits, tf.float16)\r\n\r\n            def grad(dcost, d2cost_dlogits):\r\n                return None, grad_of_sparse_softmax_cross_entropy_with_logits(logits, dcost_dlogits, dcost)\r\n\r\n            return (cost, dcost_dlogits), grad\r\n\r\n\r\n@tf.custom_gradient\r\ndef grad_of_sparse_softmax_cross_entropy_with_logits(logits, dcost_dlogits, dcost):\r\n    dcost = tf.expand_dims(dcost, axis=-1)\r\n\r\n    def grad(dy):\r\n        p = tf.nn.softmax(logits)\r\n        d2logits = p * dy - p * tf.reduce_sum(p * dy, axis=-1, keepdims=True)\r\n        return d2logits * dcost, None, dcost_dlogits * dy\r\n    return dcost_dlogits * dcost, grad\r\n\r\n\r\ndef sparse_softmax_cross_entropy_with_logits(labels, logits):\r\n    return sparse_softmax_cross_entropy_with_logits_with_gradients(labels, logits)[0]\r\n```", "Yes, I agree that this is a problem. Are you interested in contributing a version of this as a pull request?\r\n\r\nOne issue we had is that because the fused op emits both the cross entropy and the linearization if we naively used RegisterGradient we'd always compute the second derivative, which is expensive, but we can do the same trick we do in https://github.com/tensorflow/tensorflow/blob/2f3eb7b5c2fd927ec2b21ae972a39788cdce89c4/tensorflow/python/ops/nn_grad.py#L530 to avoid computation if not needed.", "Sorry was on holiday. I can try to cook up something. Should I attempt what you have linked of the gradient of the normal Softmax with Logits?", "@botev I think https://github.com/tensorflow/tensorflow/pull/22231 is already doing this", "Closing this issue since the associated PR has been merged. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27875\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27875\">No</a>\n", "I am getting this error in Python as well, similar to https://github.com/tensorflow/tensorflow/issues/27875#issue-433504286. Is there any update on this issue? Is there any short-term workaround for it?"]}, {"number": 27874, "title": "Having trouble running tensorflow with gpu", "body": "\r\n\r\n**System information**\r\n- Ubuntu 18.04\r\n- TensorFlow installed from source (not sure what this means)\r\n- TensorFlow version 1.12.0\r\n- Python version 3.6\r\n- Installed using pip\r\n- CUDA/cuDNN version 9.1\r\n- GPU model and memory: GV100 \r\n\r\n\r\nI'm new to tensorflow, and I'm trying to run it using a GPU. I've installed tensorflow in the past, but not the gpu version. Today, I uninstalled the old version, and installed the gpu version (1.12.0). I'm getting the following error when I try to run basic commands: \r\n\r\n\" ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\nFailed to load the native TensorFlow runtime. \"\r\n\r\nI've done some troubleshooting, including installing an older version of tensorflow, but nothing seems to work. Has anyone dealt with this problem before?\r\n\r\n\r\n", "comments": ["TensorFlow requires an exact version of cuda it was built with to be installed. It says above you have CUDA 9.1 installed, but TensorFlow only supported CUDA 9.0 and then with TensorFlow 1.13.1 they supported CUDA 10.0. \r\n\r\nYou need to uninstall CUDA 9.1 and install either CUDA 9.0 for TensorFlow 1.12, or CUDA 10.0 for TensorFlow 1.13.1", "Closing this issue since wdiron's comment solves the issue. Feel free to reopen if have any questions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27874\">No</a>\n"]}, {"number": 27873, "title": "Add --incompatible_disable_cc_toolchain_label_from_crosstool_proto", "body": "Hopefully this should be the last one", "comments": []}, {"number": 27872, "title": "examples/get_started/regression/dnn_regression.py failed with ValueError (nightly build)", "body": "The above example has not been working for awhile now. (I don't believe it worked with 1.13.1) the code looks valid, what is wrong with the example code?\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): x86 Linux Ubuntu 18.04 container\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-12503-g5e52b70188 1.14.1-dev20190415\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe example https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/get_started/regression/dnn_regression.py, fails with the following error:\r\nValueError: Tensor(\"buffer_size:0\", shape=(), dtype=int64, device=/device:CPU:0) must be from the same graph as Tensor(\"MapDataset_2:0\", shape=(), dtype=variant).\r\n\r\n(Actually all 3 examples in that folder fail with the same error)\r\n\r\n**Describe the expected behavior**\r\nProvided examples should run without failure\r\n\r\n**Code to reproduce the issue**\r\npip3 install tf-nightly\r\ngit clone http://github.com/tensorflow/tensorflow\r\ncd tensorflow/tensorflow/examples/get_started/regression/\r\npython3 dnn_regression.py\r\n\r\n**Other info / logs**\r\n```\r\nroot@c12a5b5766c5:~/tensorflow/tensorflow/examples/get_started/regression# python3 dnn_regression.py\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nDownloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\r\n32768/25936 [=====================================] - 0s 1us/step\r\nW0415 20:14:20.850975 140624720754496 deprecation.py:237] From /root/tensorflow/tensorflow/examples/get_started/regression/imports85.py:123: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\r\n\r\nI0415 20:14:20.987369 140624720754496 estimator.py:1761] Using default config.\r\nW0415 20:14:20.987863 140624720754496 estimator.py:1782] Using temporary folder as model directory: /tmp/tmpn4983vfs\r\nI0415 20:14:20.988089 140624720754496 estimator.py:203] Using config: {'_model_dir': '/tmp/tmpn4983vfs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe593747048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nW0415 20:14:20.993491 140624720754496 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nTraceback (most recent call last):\r\n  File \"dnn_regression.py\", line 105, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"dnn_regression.py\", line 85, in main\r\n    model.train(input_fn=input_train, steps=STEPS)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 360, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1152, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model_default\r\n    input_fn, ModeKeys.TRAIN))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1016, in _get_features_and_labels_from_input_fn\r\n    self._call_input_fn(input_fn, mode))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1107, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"dnn_regression.py\", line 46, in input_train\r\n    train.shuffle(1000).batch(128)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1634, in shuffle\r\n    buffer_size, seed, reshuffle_each_iteration))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 751, in shuffle\r\n    return ShuffleDataset(self, buffer_size, seed, reshuffle_each_iteration)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2743, in __init__\r\n    **flat_structure(self))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 4976, in shuffle_dataset\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 366, in _apply_op_helper\r\n    g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6098, in _get_graph_from_inputs\r\n    _assert_same_graph(original_graph_element, graph_element)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6034, in _assert_same_graph\r\n    original_item))\r\nValueError: Tensor(\"buffer_size:0\", shape=(), dtype=int64, device=/device:CPU:0) must be from the same graph as Tensor(\"MapDataset_2:0\", shape=(), dtype=variant).\r\nroot@c12a5b5766c5:~/tensorflow/tensorflow/examples/get_started/regression#\r\n\r\n```\r\n", "comments": ["In 1.13.1 this doesn't work either, it fails with the same error, but if you undo the last commit to the example (https://github.com/tensorflow/tensorflow/commit/e2b36c5026b856ca63041085c3183f0f7d490e71#diff-4cfd74ba277097d2966e281f8e8655b8) it works. \r\n\r\nThat fix however doesn't work with the nightly builds.", "This bug still exists in 1.14,  does anyone knows how to fix it?", "closing old issues I opened. I'm not sure if this test is still in 2.2.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27872\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27872\">No</a>\n", "Thanks for closing.\r\n\r\nSorry about the confusion. I've deleted these example as they were no longer relevant."]}, {"number": 27871, "title": "Fix incorrect frame numbers in tf_logging.warn", "body": "This fix tries to address the issue raised in #27848 where the frame numbers in tf_logging.warn is incorrect in python 3.\r\n\r\nThe reason was that in python 2, `warn = warning` but in python 3, an additional wrapper has been added to deprecate the warn. For that reason one additional frame has been added, thus causing\r\nincorrect output in the log.\r\n\r\nThis fix fixes #27848.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@caisq Hi, Could you PTAL and approve."]}, {"number": 27870, "title": "Add --incompatible_remove_native_http_archive=false to windows bazel calls", "body": "First two commits are to remove debug stuff I just now I realized I missed on #27801 Let me know if you'd rather have them on a different PR instead.\r\n\r\nThe last one is the reason for this PR: after updating bazel to 0.20.0 to make windows GPU display messages we need this flag otherwise we get\r\n\r\n```\r\nLoading: 0 packages loaded\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': The native http_archive rule is deprecated. load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\") for a drop-in replacement.\r\nUse --incompatible_remove_native_http_archive=false to temporarily continue using the native rule.\r\n```", "comments": []}, {"number": 27869, "title": "[Intel MKL] Disabling MatMul fusions in grappler for MKL backend", "body": "Temporarily disabling MatMul fusions in grappler for MKL backend,\r\nsince MKL-DNN does not support these fusions.", "comments": []}, {"number": 27868, "title": "Possible wrong assumption for how a dense flow filed is stored", "body": "In `dense_image_warp` function it is assumed that flow vectors are stored in source location of flow vectors while for dense optical flow datasets like **Middlebury**, **MPI Sinte**, **Flying Chairs**it is not the case.\r\n\r\nIn the following simple example a black dot moves from the location (2,6) in image one to (5,10) in image two. So, flow vector for that point is (3,4). In Tensorflow code it assumes that flow field is stored in **Format A**. That is why [here](https://github.com/tensorflow/tensorflow/blob/79d8779069a154a74786c28bf9d8af3d0cbd9905/tensorflow/contrib/image/python/ops/dense_image_warp.py#L172) it is explained:\r\n\r\n> ... where the warp is specified by a dense\r\n>   flow field of offset vectors that define the correspondences of pixel values\r\n>   in the output image back to locations in the  source image. Specifically, the\r\n>   pixel value at output[b, j, i, c] is\r\n>   images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c].\r\n\r\n\r\n![FlowExplained-tensorflow](https://user-images.githubusercontent.com/39167815/56152784-3ae13f80-5f7a-11e9-8cb0-a26d097f062d.png)\r\n\r\nCan anybody explain the reason to implement this function in this way? While this format is useful for some specific tasks, it is not consistent with standard datasets.\r\n\r\nI would suggest changing the assumption to **Format B** and modify the code to do backward warping (given image 2 and flow field, it will reconstruct image 1). Specifically, the pixel value at \r\noutput[b, j, i, c] would be  images[b, j + flow[b, j, i, 0], i + flow[b, j, i, 1], c]\r\n\r\nIn terms of implementation, line [210](https://github.com/tensorflow/tensorflow/blob/79d8779069a154a74786c28bf9d8af3d0cbd9905/tensorflow/contrib/image/python/ops/dense_image_warp.py#L210) can be changed to:\r\n\r\n```python\r\nquery_points_on_grid = batched_grid + flow\r\n```\r\n", "comments": ["So, what do you think guys? Let me know your thoughts, please.\r\nShould I go ahead and make a PR for this?", "Thanks for your investigation. Contrib is deprecated starting TF 2.0 . Probably this can be a good candidate for [tensorflow/addons](https://github.com/tensorflow/addons).\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/image/README.md#components"]}, {"number": 27867, "title": "Lite: Space_to_batch Op N-dim support", "body": "Arbitrary dimension support added", "comments": ["Add ref: #21526", "Is there a specific model you're using which requires this?", "> Is there a specific model you're using which requires this?\r\n\r\n@jdduke : I am using 4-D space-to-batch, but i have generalized the kernel, as it states N-Dim.", "So, in general I'm sympathetic to these changes. The problem is that, every time we change the TFLite operator semantics, we have to bump the operator version and that has non-trivial downstream implications for our accelerator library (i.e., every single one has to be updated, and that can take a year+ for NNAPI). Unless you have a model which requires this (and this applies for other operators that you're generalize), I would rather wait to modify the semantics until there's a clear and immediate need. Thanks for your patience.", "> So, in general I'm sympathetic to these changes. The problem is that, every time we change the TFLite operator semantics, we have to bump the operator version and that has non-trivial downstream implications for our accelerator library (i.e., every single one has to be updated, and that can take a year+ for NNAPI). Unless you have a model which requires this (and this applies for other operators that you're generalize), I would rather wait to modify the semantics until there's a clear and immediate need. Thanks for your patience.\r\n\r\n@jdduke: Thanks a lot for enlightening(Will keep in mind for my Future PRs). For your reference i am posting the issue link #21266, which sought for such generalization, which triggered my PR. I had also added a reference link(#21526) where @suharshs : added this feature as TBD state.", "Ahh, that's very helpful! There does appear to be a compelling model which requires this, though we should check that model (issue #21266) to see if your change would resolve the problem. Can you check, with your change?", "> Ahh, that's very helpful! There does appear to be a compelling model which requires this, though we should check that model (issue #21266) to see if your change would resolve the problem. Can you check, with your change?\r\n\r\n@jdduke : Sure, i will update once i get the result, Thanks!", "Sounds good, thanks for checking! Once you confirm that updating to support ND unblocks the referenced models, we can proceed with this and https://github.com/tensorflow/tensorflow/pull/28179.", "> Sounds good, thanks for checking! Once you confirm that updating to support ND unblocks the referenced models, we can proceed with this and #28179.\r\n\r\n@jdduke : I have verified with both my PRs #27867 & #28179 , it solves the blocking model execution.\r\nNote: i have used the model file attached in the issue(#22146), and the inference is successful.", "Thanks @ANSHUMAN87, I'm confirming internally about landing these changes, stay tuned.", "@jdduke : Any update on this & #28179 ?", "Thanks again for your contribution. Per previous directions, we're still reviewing proposals which have downstream impact on operator semantics. Anything that requires a version bump for the operator has a non-trivial impact on our accelerator ecosystem, and has to be considered carefully. Please be patient.", "Can one of the admins verify this patch?", "hoping to see the successful merge, I'm in great need of N-dim support of Conv1D operators for use in audio speech analysis applications.", "Would it simplify the implementation if we just supported the <=4D case?", "@ANSHUMAN87 Could you please check reviewer comments and keep us posted. Thanks!", "> Would it simplify the implementation if we just supported the <=4D case?\r\n\r\n@jdduke : Sorry for late response! Earlier only 4D was supported. So i generalized it. As of now i can not think of otherwise. Welcome for any suggestion. Thanks!", "> > Would it simplify the implementation if we just supported the <=4D case?\r\n> \r\n> @jdduke : Sorry for late response! Earlier only 4D was supported. So i generalized it. As of now i can not think of otherwise. Welcome for any suggestion. Thanks!\r\n\r\nIf we want to support only <=4D, may be we can have different kernel implementation based on Number of Dimension. It will be mostly like switch case. But i feel that may not be a good one. Waiting for your feedback!", "My main concern right now is the introduction of temporary tensor usage, which wasn't needed before. If we implement the <= 4D case, would we be able to get rid of that? Hopefully we could at least reuse some of the logic between dimensional cases?", "@ANSHUMAN87 Can you please check reviewer comments and keep us posted. Thanks!", "> My main concern right now is the introduction of temporary tensor usage, which wasn't needed before. If we implement the <= 4D case, would we be able to get rid of that? Hopefully we could at least reuse some of the logic between dimensional cases?\r\n\r\n@jdduke : In that case the operator cant not be N-Dim. However if you think limiting the kernel only to 4-Dim is best, i will check and confirm whether the existing logic can be reused.", "> However if you think limiting the kernel only to 4-Dim is best, i will check and confirm whether the existing logic can be reused.\r\n\r\nWorth checking, particularly if this will handle the Conv1D-related scenarios.", "> > However if you think limiting the kernel only to 4-Dim is best, i will check and confirm whether the existing logic can be reused.\r\n> \r\n> Worth checking, particularly if this will handle the Conv1D-related scenarios.\r\n\r\n@jdduke : Sorry for late reply! I checked regarding the point you suggested. I think it is possible to reuse the existing code to support (dim < 4), but it can not support (dim > 4), also in order to reuse, we have to create intermediate tensors to store the upgraded block_data & padding_data.\r\n\r\nPlease let me know your valuable opinion in this case. TIA!", "Looping in @thaink who is helping us improve our kernel ecosystem to support 5D shapes more generally, particularly for operators currently restricted to 4D.", "Thanks Jared. Let me take a look.", "Hi @ANSHUMAN87,\r\nAre you actively on this? we are having an internal user-case that need support 3D space_to_batch. So I think I'll add 3D support first, then let's discuss about N-D later.", "> Hi @ANSHUMAN87,\r\n> Are you actively on this? we are having an internal user-case that need support 3D space_to_batch. So I think I'll add 3D support first, then let's discuss about N-D later.\r\n\r\n@thaink : Thanks for your feedback! Yes i am active on the PRs. I believe adding for specific cases may not be the right approach. However i have put my analysis in my previous comment. We can reuse the existing code to support both 3-D & 4-D. But it will lack N-D support. If you have not started working on it. How about using the current PR for the internal user-case, and optimize it if any issue?\r\nJust a thought! What do you feel? \r\nHaving said that, still i would press for N-D kernels rather than specific kernels. \r\n", "@ANSHUMAN87: Implementation to support 3-D looks is simple enough to make it quick and it fills both your and our user-cases. So I prefer to support it first. Extending to ND is good but it will take sometime to review your PR.", "> @ANSHUMAN87: Implementation to support 3-D looks is simple enough to make it quick and it fills both your and our user-cases. So I prefer to support it first. Extending to ND is good but it will take sometime to review your PR.\r\n\r\n@thaink : Thanks! Will wait for your comments!", "space_to_batch and batch_to_space now supports 3D. You can sync to head to try it with your model.", "@ANSHUMAN87 Can you please check thaink's comments and resolve conflicts. Thanks!", "@gbaned : I will handle this PR possibly by Saturday. Thanks!", "> space_to_batch and batch_to_space now supports 3D. You can sync to head to try it with your model.\r\n\r\n@thaink : Thanks! I checked it works fine for 3D support.", "@gbaned : Thanks! The conflicts are resolved now.", "See also the latest comment on PR #28179. We're working on an overhaul for >4D kernel support, so I think it's best if we table this PR for now (and 1 or 2 of the related PRs) as we work toward that end. Thanks for your patience in iterating on this and pushing for generalization, and apologies we couldn't land this more promptly."]}]