[{"number": 43199, "title": "add suport for mips64 platform", "body": "tensorflow is not supported for mips64 platform. This patch can make it supported on mips64 platform on linux OS.\r\nconfigure pass, build pass!", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43199) for more info**.\n\n<!-- need_sender_cla -->", "@zhangqiang-hf  Thank you for your contribution. Can you please sign CLA? Thanks!", "Dear Sir\uff0c\r\n  I can not register a gmail for telephone not supported\uff0cso  I have no idea to sign the CLA\u3002\r\nPlease help me to reslove it\uff01\r\n\r\n\u53d1\u9001\u81ea Windows 10 \u7248\u90ae\u4ef6\u5e94\u7528\r\n\r\n\u53d1\u4ef6\u4eba: gbaned\r\n\u53d1\u9001\u65f6\u95f4: 2020\u5e749\u670814\u65e5 15:50\r\n\u6536\u4ef6\u4eba: tensorflow/tensorflow\r\n\u6284\u9001: zhangqiang-hf; Mention\r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] add suport for mips64 platform (#43199)\r\n\r\n@zhangqiang-hf Thank you for your contribution. Can you please sign CLA? Thanks!\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\n\r\n"]}, {"number": 43198, "title": "Tweaks to build tensorflow-lite-select-tf-ops", "body": "These were the minor updates I needed in order to successfully build the tensorflow-lite-select-tf-ops.aar dependency with a tflite input model.\r\n\r\nWithout the protobuf dependency, I could build my app but always got a runtime error when loading the tflite model:\r\n\r\n`java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZNK6google8protobuf7Message11GetTypeNameEv\" referenced by \"/data/app/org.auderenow.openrdt.reader-2/lib/arm64/libtensorflowlite_flex_jni.so\".`\r\n\r\nThe update to `delegate.cc` is taken directly from the quick fix described here: https://github.com/tensorflow/tensorflow/issues/42744. I'm sure there must be some other fix that would maintain functionality on Windows, but I have no idea what that would be.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43198) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43198) for more info**.\n\n<!-- ok -->", "cc @terryheo are there other ways to fix the protobuf issue?", "What's the build command you used to see the error?", "`sh tensorflow/lite/tools/build_aar.sh --input_models=<model.tflite>  --target_archs=arm64-v8a,armeabi-v7a`", "Or maybe I should be tagging @thaink ?", "@jennya likely.\r\nCould you confirm that the build command itself doesn't make an error but you'll see the error during runtime?", "@terryheo if I don't include the extra protobuf dependency (`clean_dep(\"@com_google_protobuf//:protobuf\"),`), the build succeeds but my android app crashes at runtime when loading the tflite model", "@jennya  As per update from @thaink since there is no reply from you to provide a way to reproduce, we are closing this PR. Thank you!"]}, {"number": 43197, "title": "Move read env from hdfsRead to the constructor of HDFSRandomAccessFile", "body": "This is a PR from TaiJi AI platform in Tencent.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/42860#issuecomment-691795258", "comments": ["@vnvo2409 @mihaimaruseac \r\n>> Fix a bug that the file copied by TF from HDFS to local may be wrong,\u2026 #42860\r\n\r\nAccording to the conclusion of the discussion, move `read env` from `hdfsRead` to `the constructor of HDFSRandomAccessFile`"]}, {"number": 43196, "title": "[Intel MKL] Fix in-place optimization in fusions involving Add", "body": "", "comments": []}, {"number": 43195, "title": "tf.keras SparseCategoricalCrossentropy with sample_weight on TPU: Error DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04(Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): TF-2.3.0\r\n- Colab TPU: Yes\r\n\r\nObjective: I am trying to using SparseCategoricalCrossentropy with *sample_weight* argument on **TPU** to train Mask Language Model.\r\n\r\nNote: Connecting TPU with tf.distribute.cluster_resolver.TPUClusterResolver()\r\n\r\nLoss function Looks like this:\r\n\r\n```\r\ndef masked_sparse_categorical_crossentropy(y_true, y_pred, sample_weight):\r\n\r\n  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\r\n    reduction=tf.keras.losses.Reduction.NONE,\r\n      from_logits=True\r\n  )\r\n  return loss_fn(y_true, y_pred, sample_weight=sample_weight)\r\n\r\n```\r\n\r\nWhen I'm running on CPU and GPU the code runs fine. But when I am running on TPU, I am getting an error **on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation'**\r\n\r\n```\r\n(0) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broadcastable_is_valid_shape_has_valid_nonscalar_shape_true_195924_const_0[] on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation' OpKernel for XLA_TPU_JIT devices compatible with node {{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}){{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}\r\n\t [[broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape]]\r\n\t [[broadcast_weights/assert_broadcastable/is_valid_shape]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_18335038187613310273/_6]]\r\n\t [[tpu_compile_succeeded_assert/_18335038187613310273/_6/_265]]\r\n  (1) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broadcastable_is_valid_shape_has_valid_nonscalar_shape_true_195924_const_0[] on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation' OpKernel for XLA_TPU_JIT devices compatible with node {{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}){{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}\r\n\t [[broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape]]\r\n\t [[broadcast_weights/assert_broadcastable/is_valid_shape]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_18335038187613310273/_6]]\r\n\t [[tpu_compile_succeeded_assert/_18335038187613310273/_6/_237]]\r\n  (2) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broadcastable_is_valid_shape_has_valid_nonscalar_shape_true_195924_const_0[] on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation' OpKernel for XLA_TPU_JIT devices compatible with node {{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}){{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}\r\n\t [[broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape]]\r\n\t [[broadcast_weights/assert_broadcastable/is_valid_shape]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_18335038187613310273/_6]]\r\n\t [[tpu_compile_succeeded_assert/_18335038187613310273/_6/_251]]\r\n  (3) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broa ... [truncated]\r\n```\r\n\r\nPlease help me to resolve this.\r\n\r\nRegards,\r\nAnkur ", "comments": ["@Ankur3107 \r\nI ran the code on colab tf 2.3 and nighly TPU and do not face any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/22f0831140dbd35a13137b2df06961ec/untitled410.ipynb).\r\nIs the code provided complete to replicate issue reported or please share complete code or colab link with error reported.", "@Saduf2019 \r\nThank you for your immediate reply. My complete code with an error is on https://colab.research.google.com/drive/1HYASbm7hqrL-bYZxXE7FPiWNNeOon48z?usp=sharing . \r\nPlease find the link.", "I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fb39c12e4c56d1b18f7ff894935d8a89/untitled413.ipynb)", "I also hit the same issue. My model has 2 outputs and each has its own sample_weights. I am using the sample_weights as masks (i.e. they are 0 or 1). The setup ran fine on GPU. \r\n\r\nDoes it look like there's a workaround?", "Seems that weights might not supported with TPUs this way. I am linking to two Kaggle threads that could help with finding a workaround. \r\n[Flower Classification with TPUs](https://www.kaggle.com/c/flower-classification-with-tpus/discussion/130272)\r\n[ALASKA2 Image Steganalysis](https://www.kaggle.com/c/alaska2-image-steganalysis/discussion/149689)", "It may be related to an issue @tf-marissaw is working on.\r\n\r\nMeanwhile, can you check if the problem goes away if you set `drop_remainder=True` when calling `dataset.batch`?", "I have merged a [fix](https://github.com/tensorflow/tensorflow/commit/e0d555fd3270570b8e7e143eba6017f7ff9ba987) into TensorFlow nightly and it should appear in the TensorFlow 2.5 release. \r\n\r\nCan you try running your model with TF nightly to see if the fix resolved this issue for you?", "@tf-marissaw,\r\nI ran the above model with `Tensorflow Nightly (2.5.0-dev20201211)` and it is resulting in the error, \r\n\r\n`NotFoundError: 'ComputeBatchSize' is neither a type of a primitive operation nor a name of a function registered in binary running on n-dda18766-w-0. Make sure the operation or function is registered in the binary running in this process.`\r\n\r\nPlease find the [Gist](https://colab.research.google.com/gist/rmothukuru/742e79d8ef58dd469c273324c8997938/untitled413.ipynb#scrollTo=I6mSleQBK1oh). Thanks!", "I'm able to run your code successfully in Tensorflow 2.7 and the latest transformers version without any error. \r\n[Here](https://colab.research.google.com/gist/sachinprasadhs/8c76f9e7d7293dcf6657d6627c984ee6/untitled413.ipynb) is the gist for reference.\r\nFeel free to reopen the issue if you still face an error. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43195\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43195\">No</a>\n"]}, {"number": 43194, "title": "tf.keras.applications.ResNet50 does not work properly ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \"Red Hat Enterprise Linux Server\" v7.6\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla v100 32G\r\n\r\nHello everyone,\r\nI have just tried to use pre-trained ResNet50 with tf.keras.applications.ResNet50(**args). The thing is happening for me is that after downloading weights, nothing else happens!! even after a couple of hours it does not go any further!! Any idea? what may be the casue of this? Any solution?\r\n\r\nMany thanks for your help in advance.\r\n\r\nI use as follow:\r\n\r\nENCODER_BASE = tf.keras.applications.ResNet50(include_top=False,input_shape=(None,None,3), weights='imagenet')\r\n\r\n![image](https://user-images.githubusercontent.com/54047164/93038849-62659b80-f689-11ea-8d8e-b634407469a7.png)\r\n\r\n", "comments": ["@nick-nikzad \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!", "Thanks for your reply.\r\n\r\nI get the issue just with running of below simple code:\r\n\r\nimport tensorflow as tf\r\nENCODER_BASE = tf.keras.applications.ResNet50(include_top=False,input_shape=(None,None,3),weights='imagenet')\r\n\r\nNote: once I run the above line on my laptop ( it is only cpu), it is fine and works. But as I run on our GPU cluster the issue raises. It also uses the almost full capacity of the GPU 32GB.\r\n\r\nThanks.\r\n\r\n", "@nick-nikzad \r\n\r\nI have tried in colab with TF-GPU version 2.3.0 and i am not seeing any issue. Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/7e9c0cd69f0b5527ed6f1717d4faa62c/untitled346.ipynb).Thanks!", "Thanks but once I tried for TF-GPU version 2.2.0 which is mine it fails.  Would you please take a look [here](https://colab.research.google.com/drive/1o6s4aWBtV3O4RzmD5i7fPWfUvgZid90v#scrollTo=u76PSye_TOdD). Thanks.", "@nick-nikzad \r\n\r\nPlease, grant me the access for the colab link you have provided.Also, please check you are facing the same issue with the latest version of Tensorflow ie. 2.3. Thanks!", "https://colab.research.google.com/drive/1o6s4aWBtV3O4RzmD5i7fPWfUvgZid90v?usp=sharing", "@nickdesaulniers Change your colab with `!pip install tensorflow==2.2.0`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43194\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43194\">No</a>\n"]}, {"number": 43193, "title": "W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found", "body": "I got this issue when I run import tensorflow as tf in command line\r\n\r\n>>> import tensorflow as tf\r\n2020-09-13 17:53:34.644899: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-13 17:53:34.649089: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nI use win10 and install the CUDA and cuDNN\r\nBut I don't understand why this still happened.\r\n![image](https://user-images.githubusercontent.com/68514251/93029699-06e4d080-f5eb-11ea-9948-a0133f0f8603.png)\r\n![image](https://user-images.githubusercontent.com/68514251/93029705-19f7a080-f5eb-11ea-9dc2-3087ff1379fe.png)\r\n", "comments": ["Please `pip install tf-nightly` for CUDA 11", "After I tried pip install tf-nightly, it still shows the same error I think\r\n![image](https://user-images.githubusercontent.com/68514251/93029816-22041000-f5ec-11ea-82c3-ecbc5dafbe05.png)\r\n", "Is not the same error cause now it is looking for `110`.\r\n\r\nAre you compilant with these steps?\r\n> Install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019. Starting with the TensorFlow 2.1.0 version, the msvcp140_1.dll file is required from this package (which may not be provided from older redistributable packages). The redistributable comes with Visual Studio 2019 but can be installed separately:\r\n\r\n> - Go to the Microsoft Visual C++ downloads,\r\n> - Scroll down the page to the Visual Studio 2015, 2017 and 2019 section.\r\n> - Download and install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 for your platform.\r\n> - Make sure long paths are enabled on Windows.\r\n> - Install the 64-bit Python 3 release for Windows (select pip as an optional feature).\r\n ", "You will find more on Windows tabs at https://www.tensorflow.org/install/pip#windows", "> Is not the same error cause now it is looking for `110`.\r\n> \r\n> Are you compilant with these steps?\r\n> \r\n> > Install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019. Starting with the TensorFlow 2.1.0 version, the msvcp140_1.dll file is required from this package (which may not be provided from older redistributable packages). The redistributable comes with Visual Studio 2019 but can be installed separately:\r\n> \r\n> > * Go to the Microsoft Visual C++ downloads,\r\n> > * Scroll down the page to the Visual Studio 2015, 2017 and 2019 section.\r\n> > * Download and install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 for your platform.\r\n> > * Make sure long paths are enabled on Windows.\r\n> > * Install the 64-bit Python 3 release for Windows (select pip as an optional feature).\r\n\r\nI think the problem is solved after I Download and install the Microsoft Visual C++ Redistributable.\r\nThank you so much !", "You need also to rely on `tf-nightly` for CUDA 11 untill TF 2.4 is released.", "I tried to use the example given by https://github.com/tensorflow/tensorflow\r\nafter I run the tf.add(1, 2).numpy()\r\nit still shows some errors but evenlty output the 3\r\nwill this be a problem?\r\n![image](https://user-images.githubusercontent.com/68514251/93030111-963fb300-f5ee-11ea-9da7-d99c22056cfc.png)\r\n", "Check that your software requirements are ok https://www.tensorflow.org/install/gpu#windows_setup", "> Check that your software requirements are ok https://www.tensorflow.org/install/gpu#windows_setup\r\n\r\nI think I have everything it required.\r\nAnd after I update my GPU driver, it shows the same error as the beginning.\r\n![image](https://user-images.githubusercontent.com/68514251/93030924-de61d400-f5f4-11ea-9b65-5c38f8f99066.png)\r\n\r\n", "Have you double checked the `%PATH%`?", "> Have you double checked the `%PATH%`?\r\n\r\nI run all four Path in https://www.tensorflow.org/install/gpu#windows_setup.\r\nBut it still missing for 110", "Have you changed the documentation PATH strings for your CUDA 11 (doc has CUDA 10)? \nHave you checked that these paths exist on your system?", "> Have you changed the documentation PATH strings for your CUDA 11 (doc has CUDA 10)?\r\n> Have you checked that these paths exist on your system?\r\n\r\nYes, I manually added it for the first three, but the last one \"C:\\tools\\cuda\\bin\", I don't think I have this folder\r\n\r\n![image](https://user-images.githubusercontent.com/68514251/93032144-ba56c080-f5fd-11ea-8f1d-a22e5f4be87c.png)\r\n![image](https://user-images.githubusercontent.com/68514251/93032149-bdea4780-f5fd-11ea-9a9b-765b82a64ce9.png)\r\n", "At the beginning, I copy the cuDNN's file to bin include and lib respectively, I tried to add a Path directly to cuDNN and it is still having this error", "Have you set in PATH\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin;%PATH%\n\nDo you have this dir in your system?", "> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin\r\n\r\nYes, and I already added to the path, but it still shows the error", "This is all the PATH I have, I think I already include all that required\r\n![image](https://user-images.githubusercontent.com/68514251/93033777-b169ed00-f605-11ea-8bfa-3f6206411067.png)\r\n![image](https://user-images.githubusercontent.com/68514251/93033781-b62ea100-f605-11ea-90b5-cc4ead6d14f6.png)\r\n", "Open `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin`.\nWhat file do you have there?\nCan you see the `cudart64_100.dll`?", "I think I only have cudart64_110.dll\r\n(The one that I highlighted)\r\n![image](https://user-images.githubusercontent.com/68514251/93034064-d0b54a00-f606-11ea-991e-429bb7713d58.png)\r\n", "Yes was a typo It is correct. \nOn the same terminal where you are trying to launch Tensorflow can you verify that you have the new path with `echo %PATH:;=&echo.%`?", "Yes, I believe I have this path in PATH\r\n![image](https://user-images.githubusercontent.com/68514251/93034880-84b7d480-f609-11ea-9b45-ca1f04f4dec3.png)\r\n", "I see a typo there:\r\n`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin\\bin`", "> I see a typo there:\r\n> `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin\\bin`\r\n\r\nYes, It is the path to cuDNN, After I download the cuDNN, I copy the bin in cuDNN to here. \r\nI have both:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin\r\n\r\nBefore I add the \r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11\\bin\\bin\r\nit still doesn't work, so I tired to add it\r\n", "And I delete it from PATH, it still has the same error", "Do you meant v11.0 or v11?", "> Do you meant v11.0 or v11?\r\n\r\nI mean v11.0, I delete the one ~/bin/bin\r\nbut it is still not working", "Is a python 64bit installation?", "Yes, I think I install the 64bit version?\r\nAnd the version I have is 3.8.5\r\n![image](https://user-images.githubusercontent.com/68514251/93036314-8edbd200-f60d-11ea-91c5-0f1b0081d37f.png)\r\n", "`python -c \"import sys; print(sys.maxsize > 2**32)\"`", "![image](https://user-images.githubusercontent.com/68514251/93036825-f0507080-f60e-11ea-9250-4f68b0acabf1.png)\r\n", "Ok It is 64bit.\n\nSo if you have `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin` in the PATH  and you have `cudart64_110.dll` there It is strange that It gives you not found on this file.", "> Ok It is 64bit.\r\n> \r\n> So if you have `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin` in the PATH and you have `cudart64_119.dll` there It is strange that It gives you not found on this file.\r\n\r\nyeahhh.....very strange, I am not sure what to do in this case....", "Can you try to close and open the terminal with python and try again?", "> Can you try to close and open the terminal with python and try again?\r\n\r\nSo I close the old terminal and reopen it, this is what I got after try to import it\r\n![image](https://user-images.githubusercontent.com/68514251/93037684-fd6e5f00-f610-11ea-8c3b-56f7357d0c31.png)\r\nI think it is still the same error", "Yes Is the same. \nIf the PATH Is currect I only see this old  case that required a reboot https://stackoverflow.com/questions/51111954/tensorflow-cant-find-cudart64-90-dll-even-though-it-is-installed-with-path-va/51112550#51112550", "> Yes Is the same.\r\n> If the PATH Is currect I only see this old case that required a reboot https://stackoverflow.com/questions/51111954/tensorflow-cant-find-cudart64-90-dll-even-though-it-is-installed-with-path-va/51112550#51112550\r\n\r\nJust come back from the reboot, it still shows not find the 110\r\n", "Have you installed python from the Microsoft store?", "I believe I install it from offcial website, I don't even know they have it in the Microsoft store", "Mhh.. Cause if It was the Microsoft one it Is sandboxed: https://github.com/tensorflow/tensorflow/issues/36111#issuecomment-614933000\r\n\r\nYou can try to reinstall tf-nightly in a fresh venv https://www.tensorflow.org/install/pip#windows_1", "Hello, \r\nI had the same problem but my tensorflow was asking for the 101 DLL which is found in version 10.1. Maybe try with version the version advised here : https://www.tensorflow.org/install/gpu#windows_setup ? ", "I have the exact same thing: tf-nightly, cudart64_110.dll on the path, yet the message is that it isn't found.\r\nThis is in a venv, if that matters.\r\n```python\r\nimport tensorflow as tf\r\n2020-09-21 18:02:24.223511: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2020-09-21 18:02:24.223673: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\ntf.__version__\r\n'2.4.0-dev20200917'\r\n```\r\nPath includes C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\r\n```\r\n(.venv) C:\\>dir \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\\cudart64_110.dll\"\r\n Volume in drive C is Windows\r\n Volume Serial Number is xxxxxxxxx\r\n\r\n Directory of C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\r\n\r\n07/23/2020  06:59 AM           401,408 cudart64_110.dll\r\n```\r\n```\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\extras\\demo_suite>deviceQuery\r\ndeviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce MX150\"\r\n  CUDA Driver Version / Runtime Version          11.0 / 11.0\r\n  CUDA Capability Major/Minor version number:    6.1\r\n  Total amount of global memory:                 2048 MBytes (2147483648 bytes)\r\n  ( 3) Multiprocessors, (128) CUDA Cores/MP:     384 CUDA Cores\r\n  GPU Max Clock rate:                            1532 MHz (1.53 GHz)\r\n  Memory Clock rate:                             3004 Mhz\r\n  Memory Bus Width:                              64-bit\r\n  L2 Cache Size:                                 524288 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               zu bytes\r\n  Total amount of shared memory per block:       zu bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          zu bytes\r\n  Texture alignment:                             zu bytes\r\n  Concurrent copy and kernel execution:          Yes with 5 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device supports Compute Preemption:            Yes\r\n  Supports Cooperative Kernel Launch:            Yes\r\n  Supports MultiDevice Co-op Kernel Launch:      No\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.0, CUDA Runtime Version = 11.0, NumDevs = 1, Device0 = GeForce MX150\r\nResult = PASS\r\n```\r\n\r\n", "Can you try to print from python `os.environ[\"PATH\"]` and check that you see the correct cuda paths?", "```python\r\n>>> os.environ[\"PATH\"]\r\n'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.0\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.0\\\\extras\\\\CPUTI\\\\lib64;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.0\\\\include;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.0\\\\libnvvp;C:\\\\Program Files (x86)\\\\RSA SecurID Token Common;C:\\\\Program Files\\\\RSA SecurID Token Common;C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\Program Files (x86)\\\\Microsoft SDKs\\\\Azure\\\\CLI2\\\\wbin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA NvDLISR;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\Program Files\\\\CrashPlan\\\\jre\\\\bin\\\\server\\\\;C:\\\\Program Files\\\\CrashPlan\\\\jre\\\\bin\\\\;C:\\\\Program Files\\\\Intel\\\\WiFi\\\\bin\\\\;C:\\\\Program Files\\\\Common Files\\\\Intel\\\\WirelessCommon\\\\;C:\\\\Program Files (x86)\\\\PuTTY\\\\;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2020.1.2\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\Tools\\\\Binn\\\\;C:\\\\Program Files\\\\Microsoft SQL Server\\\\Client SDK\\\\ODBC\\\\170\\\\Tools\\\\Binn\\\\;C:\\\\Program Files\\\\RSA SecurID Token Common\\\\;'\r\n```", "I should mention, I have the cudnn64_8.dll which seems to correspond to the cudart64_110.dll.  The docs only mention the cudnn64_7.dll", "@adamx97 It is ok for nightly.", "@CalendulaED,\r\nAny updates regarding? Is this still an issue? Thanks!", "> @CalendulaED,\r\n> Any updates regarding? Is this still an issue? Thanks!\r\n\r\nI believe I still have the problem of missing 110\r\n![image](https://user-images.githubusercontent.com/68514251/94458263-b68d7700-0183-11eb-944b-cbf2346bbe84.png)\r\n", "And I do have 110 in the folder I believe\r\n![image](https://user-images.githubusercontent.com/68514251/94458463-fa807c00-0183-11eb-82d9-04d134f7785e.png)\r\n", "@CalendulaED can you try with a fresh `venv` https://www.tensorflow.org/install/pip#2.-create-a-virtual-environment-recommended?", "> @CalendulaED can you try with a fresh `venv` https://www.tensorflow.org/install/pip#2.-create-a-virtual-environment-recommended?\r\n\r\nI try the venv, but It give me an error about environment error \r\n![image](https://user-images.githubusercontent.com/68514251/94460475-ca86a800-0186-11eb-8cda-af80ee68a6f5.png)\r\n", "![image](https://user-images.githubusercontent.com/68514251/94461223-d9ba2580-0187-11eb-8cbe-4a3096697e88.png)\r\n![image](https://user-images.githubusercontent.com/68514251/94461243-dfb00680-0187-11eb-9349-abf8f146fd41.png)\r\n![image](https://user-images.githubusercontent.com/68514251/94461251-e3438d80-0187-11eb-877a-44baebfd8224.png)\r\nAfter I add --user to it, I belive it still not working", "The idea was:\n > deactivate \u00a0# don't exit until you're done using TensorFlow` \n\nso to install and try to use TF inside the venv.", "![image](https://user-images.githubusercontent.com/68514251/94466288-38cf6880-018f-11eb-8c6e-0cf6643ee9cb.png)\r\nI think this is inside the venv, but I still show the same errors", "Now yes.\r\nCan you try to see if you can access to `cudart64_110.dll`\r\n```\r\nimport os\r\nos.access('<fill_with_your_path>cudart64_110.dll', os.R_OK)\r\n```", "I think import is not supported here\r\n![image](https://user-images.githubusercontent.com/68514251/94469065-7209d780-0193-11eb-835d-c1df00635210.png)\r\n", "It is python code", "Also <fill_with_your_path> it means \"you need to manually change it with your real cudart64 path\"", "It says false\r\n![image](https://user-images.githubusercontent.com/68514251/94470837-36244180-0196-11eb-8538-86f02538823d.png)\r\n", "Ok so with false it seems that you have read permission problem with that folder. Are you sure that you have not installed python from the Windows store? Like in https://installpython3.com/windows/", "I believe I download it from **https://www.python.org/downloads/**\r\nshould I download this again?", "Can you try from command line to run `type <fill_with_your_path>cudart64_110.dll`?", "I am sorry, it said True, it misses a symbol in the line\r\n![image](https://user-images.githubusercontent.com/68514251/94473414-2ad31500-019a-11eb-848c-96c5877de246.png)\r\n", "Ok can you run `python -c 'import tensorflow as tf; print(tf.__version__)'`", "I can't directly type it, so I run it in python, I think it said 2.4.0\r\n![image](https://user-images.githubusercontent.com/68514251/94474134-3541de80-019b-11eb-8440-130a06446b94.png)\r\n", "Ok.. the last thing.\r\nInside the venv can you execute:\r\n```\r\nimport os\r\nprint(os.environ['PATH'])\r\n```", "Yes.\r\n![image](https://user-images.githubusercontent.com/68514251/94482238-f7e34e00-01a6-11eb-88c9-1e69d3c76dae.png)\r\n", "The path seems ok to find cudart. Yes you can try to be sure to install and use python 64bit from https://www.python.org/downloads/", "![image](https://user-images.githubusercontent.com/68514251/94486076-e309b900-01ac-11eb-94ef-8283962be9fc.png)\r\nI reinstall the python, but I don't think it makes a difference.\r\n![image](https://user-images.githubusercontent.com/68514251/94486201-0d5b7680-01ad-11eb-9bec-5a2540b62dda.png)\r\n", "Can you execute:\r\n```python\r\nimport sys\r\nprint(sys.executable)\r\n```", "Yes,\r\n![image](https://user-images.githubusercontent.com/68514251/94487092-65df4380-01ae-11eb-87bb-cf989496b25e.png)\r\n", "Please re-run outside the venv", "![image](https://user-images.githubusercontent.com/68514251/94487722-6f1ce000-01af-11eb-9ede-716ef0417bcd.png)\r\nyeh, it saids Microsoft windows apps?\r\nBut I never download it from there, and I just download it from the official web site, why this happens...", "I understand but we cannot support you at this level cause it is not a Tensorflow bug. \r\nYou can try to find support for python installation at: https://www.python.org/about/help/\r\n\r\nOr you can use Tensorflow with our Docker images https://www.tensorflow.org/install/docker", "I think I have the correct path now\r\n![image](https://user-images.githubusercontent.com/68514251/94489999-8fe73480-01b3-11eb-8bfd-18d399dab17a.png)\r\n", "I think it give another different error when I tried to import tensorflow\r\n![image](https://user-images.githubusercontent.com/68514251/94490135-d9378400-01b3-11eb-9de6-b11fda3bac06.png)\r\n", "Prepare again venv with this version and install tensorflow with pip inside the venv", "Inside the venv it also shows the same error \r\n![image](https://user-images.githubusercontent.com/68514251/94490537-70044080-01b4-11eb-837c-ea479f9bfbe6.png)\r\n", "You need  `pip install --upgrade tensorflow` inside the venv", "after install it, it shows missing 101\r\n![image](https://user-images.githubusercontent.com/68514251/94492342-1867d400-01b8-11eb-886c-3daf09578ea0.png)\r\n", "if you are still with CUDA 11 you need nightly `pip install tf-nightly`", "O, I think I can import tensorflow, but why do I need to run it in venv? So I can't run it without venv?", "When I tried to import tensorflow outside the venv, it will missing module, and I tried to upgrade pip, it shows the error\r\n![image](https://user-images.githubusercontent.com/68514251/94493564-f0c63b00-01ba-11eb-8099-57549e853145.png)\r\n\r\n", "This are windows permission issues of your shell. Please ask support to stackoverflow or other python support channels\r\nhttps://stackoverflow.com/questions/51115744/access-is-denied-when-trying-to-pip-install-a-package-on-windows/55100713", "> This are windows permission issues of your shell. Please ask support to stackoverflow or other python support channels\r\n> https://stackoverflow.com/questions/51115744/access-is-denied-when-trying-to-pip-install-a-package-on-windows/55100713\r\n\r\nThank you, I think I can run it without the venv, but what is the difference?", "Nothing specific is to maintain a clean separate environment.", "> Nothing specific is to maintain a clean separate environment.\r\n\r\nOk, thank you so much!!!! really help me a lots", "I have to renamed these files in the bin folder to make it work:\r\ncudart64_101.dll -> cudart64_110.dll\r\ncusparse64_10.dll->cusparse64_11.dll\r\ncudnn64_7.dll->cudnn64_8.dll\r\ncublas64_10.dll->cublas64_11.dll\r\n", "The reality is that each version of Tensorflow uses an specific Cuda and Cudnn. In my case, for TF 2.3 it is Cuda 11.0. Installing this one solves the problem.", "> I have to renamed these files in the bin folder to make it work:\r\n> cudart64_101.dll -> cudart64_110.dll\r\n> cusparse64_10.dll->cusparse64_11.dll\r\n> cudnn64_7.dll->cudnn64_8.dll\r\n> cublas64_10.dll->cublas64_11.dll\r\n\r\nalso encountered cusolver64_10.dll not found. when I checked the bin folder, it is named as cusolver64_11.dll", "I was able to solve this by following the software requirements here: https://www.tensorflow.org/install/gpu#software_requirements\r\n\r\nHere's my system:\r\nNVIDIA Driver 460.89\r\nCUDA 11.0.3\r\nCuDNN 8.0.5.39\r\nPython 3.7.2\r\n\r\nI also made sure that the CUDA was compatible with the CuDNN version by checking it here: https://developer.nvidia.com/rdp/cudnn-download\r\n\r\n![image](https://user-images.githubusercontent.com/47623790/102877187-b30b4280-4489-11eb-9db0-5d5f75667511.png)\r\n\r\nFinally, don't forget to set the installation directories to the PATH environment variable. https://www.tensorflow.org/install/gpu#windows_setup\r\n", "The key is to install Python from python.org. All versions from Microsoft App Store don't work. Tried both 3.7, 3.8. Once installed the official version 3.8.7 downloaded from python.org, everything is fine now.\r\n\r\nThis is my current system:\r\n\r\nNVIDIA Driver 460.89\r\nCUDA 11.0.3\r\nCuDNN 8.0.5.39\r\nPython 3.8.7 (from python.org)\r\n\r\n\r\nI also tried with CUDA 11.1.1. It also worked after renaming cusolver64_11.dll to cusolver64_10.dll. It's a known issue tracked in issue #44291.", "I got a way to solve this problem, you can download cudart64_101.dll to C:\\Users\\hp and C:\\Windows\\System32  from \r\n https://www.ijinshan.com/filerepair/cudart64_101.dll.shtml.", "Hello, \r\nThanks @aparico for the tip ! It did work with this configuration and `pip install tensorflow`. However, the software versions in the software requirements section of the https://www.tensorflow.org/install/gpu#software_requirements are not correct as they recommand CUDA 10.1 and CuDNN 7.6 and the install works with CUDA 11.0 and CuDNN 8.0. \r\nMaybe this page needs update ? ", "**CUDA 11.2\r\ncuDNN 8.1\r\ntensorflow 2.5\r\npython 3.9.5**\r\nin virtual env write `\"where cudart64_110.dll\"` and get path.\r\nwhen taping  `\"import tensorflow as tf\"` get:\r\n`\"W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\"`\r\n\r\nTried to install `tf-nightly, tensorflow`\r\nI have been installing to main enviroment and to virtual enviroment, in both cases have got the same result.\r\n**VC_redist.x64** installed\r\n\r\n### Solution:\r\n```\r\nimport os\r\nos.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin\") \r\n```\r\nBefore importing tensorflow in virtualenv and still does not work in main env.\r\n", "Thank you for posting the os.add_dll_directory tip.  I believe the behavior is by design for newer versions of Python on Windows.  The Windows Path environment variable no longer works.  That's what I'm guessing is going on.\r\n\r\nhttps://docs.python.org/3/whatsnew/3.8.html#bpo-36085-whatsnew\r\n\r\n>  Specifically, PATH and the current working directory are no longer used, and modifications to these will no longer have any effect on normal DLL resolution.", "I do an easy way to deal with this problem like below :\r\n1. Install anaconda with python 3.8.8\r\n2. Install tensor flow : pip install tensorflow -> got an error like above \r\n\"W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\"\r\n3. Install package -> pip install tf-nightly\r\n4. Install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019 -> https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0\r\n5. Verify the system has a CUDA-capable GPU -> http://developer.nvidia.com/cuda-gpus. \r\n![image](https://user-images.githubusercontent.com/84208988/133694728-c11fa576-9a93-4aa3-bc41-7292960d33e8.png)\r\n6. Using Conda to Install the CUDA Software (https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/) -> conda install cuda -c nvidia\r\n7. Verify the Installation of CUDA toolkit ->  nvcc -v\r\n![image](https://user-images.githubusercontent.com/84208988/133694963-1c69d01e-9bf5-40b1-9bad-c7cc7c53a361.png)\r\n8. import tensorflow as tf is succeed, and you are ready to implement deep learning using jupyter notebook\r\nHope this information is helpful....", "I had this error, as well as:\r\nCould not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n\r\nHere's what worked for me, with Python 3.9.6 running on my Windows 10 laptop with a mobile RTX 3080.\r\n\r\nUpgrade to the latest NVidia drivers:\r\n-- https://www.nvidia.com/drivers\r\n\r\nInstall CUDA 11.4:\r\n-- https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_local\r\n\r\nInstall cuDNN 8.2.4 for CUDA 11.4:\r\n-- https://developer.nvidia.com/rdp/cudnn-download \r\n-- choose cuDNN Library for Windows (x64), extract to C:\\tools so you have C:\\tools\\cuda\\bin\\cudnn64_8.dll\r\n\r\nUpdate your Windows %PATH%, by going to Windows Advanced System Properties -> Environment Variables -> PATH -> New, adding these:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\extras\\CUPTI\\lib64\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\include\r\nC:\\tools\\cuda\\bin\r\n\r\nDownload and install the x64 C++ Redistributable:\r\n-- https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0\r\n\r\nDownload and install the x86 C++ Redistributable:\r\n-- https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0\r\n\r\npip install tensorflow\r\npip install tf-nightly\r\n\r\nReboot your computer.\r\n\r\nWorks perfectly now.", "What fixed it for us:\r\n\r\nimport os\r\nos.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin\") # (Where v11.2 is your version)", "> \r\n\r\nadd_dll_directory also works for me. This is due to since Python 3.8, %PATH% is not longer in the dll search path.\r\nhttps://docs.python.org/3/library/os.html#os.add_dll_directory\r\n", "> \r\n\r\nthis works like a gem :)"]}, {"number": 43192, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:2.2.0\r\n- Python version:3.8.5\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:nvidia940m, 4 gb\r\n\r\n\r\n\r\n**Describe the problem**  I am confident that I've installed Cuda and cudnn properly and added path in the environment variable, but still, I am getting this error\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am following their step: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tensorflow-object-detection-api-installation\r\nWhen I tried for \"python object_detection/builders/model_builder_tf2_test.py\" I am facing this error\r\n\r\n**Any other info / logs**  (tensorflow1) C:\\tensorflow1\\models\\research>python object_detection/builders/model_builder_tf2_test.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_tf2_test.py\", line 21, in <module>\r\n    import tensorflow.compat.v1 as tf\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "We don't support Anaconda installation.\r\nPlease try to install TF with pip/venv as https://www.tensorflow.org/install/pip\r\nIf you need Anaconda you could rely on third party support at https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\n\r\n/cc @karmel @av8ramit We could improve this new tensorflow-butler reply with a better text when there is an Anaconda string in the issue description. \r\nIt Is a quite recurrent case specially for Windows tickets. I've triaged many of these just the last few days.", "@bhack Sorry I've mistakenly written that used conda, I used pip for installation, Btw I fixed this by running setup.py, But I am getting another error now.\r\n(tensorflow1) C:\\tensorflow1\\models\\research>python object_detection/builders/model_builder_tf2_test.py\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_tf2_test.py\", line 21, in <module>\r\n    import tensorflow.compat.v1 as tf\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\nImportError: cannot import name 'descriptor' from 'google.protobuf' (unknown location)\r\n", "From you last log I still see that you are inside an `Anaconda3 envs`. \r\nUnder the [conda tab in the documentation](https://www.tensorflow.org/install/pip#conda): \r\n> While the TensorFlow provided pip package is recommended, a community-supported Anaconda package is available. To install, read the Anaconda TensorFlow guide. ", "We support python venv and pip. If you want you can follow the guidelines at https://www.tensorflow.org/install/pip#windows_1.\r\n\r\nInstead if you still need to use conda envs and Anaconda you need to find support at: https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\n\r\nPlease also respect our code of conduct (/cc @theadactyl) : https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md", "@bhack can you help me out:-\r\n\r\n(tensorflow1) C:\\tensorflow1\\models\\research>python object_detection/builders/model_builder_tf2_test.py\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_tf2_test.py\", line 21, in <module>\r\n    import tensorflow.compat.v1 as tf\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"C:\\Users\\Pranab\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\nImportError: cannot import name 'descriptor' from 'google.protobuf' (unknown location)\r\n\r\nI am not getting this error.\r\n\r\n", "We cannot support you on Anacoda sorry.", "@Pranab-pronay \r\nYou could be facing this issue because of the following reasons\r\n\r\n- You you running 32-bit Python or 32-bit OS\r\n- Please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n- Your CPU does not support AVX instructions, please provide the make and model of your CPU in this case.\r\n\r\nAlso, check these similar duplicate issues: #42058 #41596 #40459 #39007 #38916 #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 .\r\n\r\nIn case you still face issues after verifying the above, please create a issue on [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues)., and move this to closed status.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43192\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43192\">No</a>\n"]}, {"number": 43191, "title": "Use gcs partial reponse and absl strcat", "body": "- Most of the time, we only need a small portion of gcs response ( usually `size` ). I add `gcs::Fields` to fetch only the fields we need from gcs server. This technique is called [partial response](https://cloud.google.com/storage/docs/json_api/v1/how-tos/performance#partial-response) and it allows us to increase performance.\r\n\r\n- I mixed between `absl::StrCat` and `std::string` concatenation. This time, I change everything to `absl::StrCat`.", "comments": []}, {"number": 43190, "title": "Tensorflow hub USE model issue", "body": "This is worse. What documentation they have is also failing? Tensorflow, please help.\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\n# Create graph and finalize (finalizing optional but recommended).\r\ng = tf.Graph()\r\nwith g.as_default():\r\n  # We will be feeding 1D tensors of text into the graph.\r\n  text_input = tf.placeholder(dtype=tf.string, shape=[None])\r\n  embed = hub.Module(\"use_model\")\r\n  embedded_text = embed(text_input)\r\n  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\r\ng.finalize()\r\n\r\n# Create session and initialize.\r\nsession = tf.Session(graph=g)\r\nsession.run(init_op)\r\n\r\nresult = session.run(embedded_text, feed_dict={text_input: [\"Hello world\"]})\r\nprint(result.shape)\r\n\r\n\r\n\r\n2020-09-13 19:57:18.597868: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n2020-09-13 19:57:18.605416: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\native_module.py:55: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\saved_model_module.py:44: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\saved_model_module.py:45: The name tf.saved_model.constants.MAIN_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.MAIN_OP_KEY instead.\r\n\r\nWARNING:tensorflow:From endpoints.py:33: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\module.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\resolver.py:458: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\native_module.py:92: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\saved_model_lib.py:80: The name tf.saved_model.constants.SAVED_MODEL_FILENAME_PB is deprecated. Please use tf.saved_model.SAVED_MODEL_FILENAME_PB instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\saved_model_lib.py:221: The name tf.saved_model.constants.ASSETS_KEY is deprecated. Please use tf.saved_model.ASSETS_KEY instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\saved_model_lib.py:55: The name tf.saved_model.constants.VARIABLES_DIRECTORY is deprecated. Please use tf.saved_model.VARIABLES_DIRECTORY instead.\r\n\r\nWARNING:tensorflow:From C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\saved_model_lib.py:56: The name tf.saved_model.constants.VARIABLES_FILENAME is deprecated. Please use tf.saved_model.VARIABLES_FILENAME instead.\r\n\r\nTraceback (most recent call last):\r\n  File \"endpoints.py\", line 34, in <module>\r\n    embed = hub.Module(\"use_model\")\r\n  File \"C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\module.py\", line 144, in __init__\r\n    self._spec = as_module_spec(spec)\r\n  File \"C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\module.py\", line 33, in as_module_spec\r\n    return load_module_spec(spec)\r\n  File \"C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\module.py\", line 58, in load_module_spec\r\n    return registry.loader(path)\r\n  File \"C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\registry.py\", line 42, in __call__\r\n    return impl(*args, **kwargs)\r\n  File \"C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 115, in __call__\r\n    return _ModuleSpec(saved_model_handler, checkpoint_filename)\r\n  File \"C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 296, in __init__\r\n    saved_model_handler, _SUPPORTED_COLLECTIONS)\r\n  File \"C:\\Users\\jay.timbadia\\black\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 708, in check_collections_are_supported\r\n    \" as appropriate.\" % list(unsupported))\r\nValueError: Unsupported collections in graph: ['saved_model_main_op']\r\nUse hub.create_module_spec(..., drop_collections=[...]) as appropriate.", "comments": ["Please fill the ISSUE template.\r\nCan you share a very, very minimal but runnable standalone example or colab to reproduce this?\r\n", "This is the minimal application code @bhack \r\nI have tried with both version of tensorflow, still giving same issue.\r\n\r\nimport tensorflow_hub as hub\r\nimport tensorflow as tf\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n\r\n  text_input = tf.placeholder(dtype=tf.string, shape=[None])\r\n  embed = hub.Module(\"use_model\")\r\n  embedded_text = embed(text_input)\r\n  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\r\ng.finalize()\r\n\r\n\r\nsession = tf.Session(graph=g)\r\nsession.run(init_op)\r\n\r\nresult = session.run(embedded_text, feed_dict={text_input: [\"Hello world\"]})\r\nprint(result.shape)\r\n\r\n\r\n\r\nError:\r\nIssues is when hub.Module runs, \r\nI have downloaded https://tfhub.dev/google/universal-sentence-encoder-large/5 and extracted it in use_model folder.\r\n\r\nso hub.Module('use_model') should load it, but giving ValueError mentioned above.", "@jaytimbadia \r\n\r\nThis issue is more suitable for TensorFlow Hub repo. Please post it on hub repo from [here.](https://github.com/tensorflow/hub/issues) Thanks!", "I have raised a new issue at hub but will take time again, but @bhack might solve this issue, so will close this one as soon as I get something from him. Thanks.", " https://tfhub.dev/google/universal-sentence-encoder-large/5 is a [TF2 SavedModel](https://www.tensorflow.org/hub/tf2_saved_model) and cannot be loaded with the `hub.Module` API. Please follow the usage instructions on the model documentation page.", "You'll also need to update your version of tensorflow_hub (and maybe even tensorflow). The error message reads as if you're using 0.2; current is 0.9.", "Ok, so do they have to be loaded using URL in hub.load?\r\nIf we need to load it using URL, why they have provided download option?\r\nI mean how to load a folder downloaded model?\r\n\r\nAlso I tried using both version of hub 0.9 as well? gave same error.", "I have downloaded the model, and want to load it from there?> How should I do that? ", "You can see the logic at https://www.tensorflow.org/hub/tf2_saved_model?hl=en#using_a_savedmodel_in_low-level_tensorflow", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43190\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43190\">No</a>\n"]}, {"number": 43189, "title": "Tensorflow 2.3 not detecting gpu (cuda 10.0)", "body": "**System information**\r\n- OS Platform and Distribution:Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.3\r\n- Python version:3.6.9\r\n- Installed using:pip\r\n- CUDA/cuDNN version:10.0/ 7.6.5\r\n- GPU model and memory: gtx 1050 ti, 4GB\r\n\r\n\r\nI'm not able to figure out why tensorflow is not detecting gpu.\r\nAs soon as I import tf in cmd it shows this warning. \r\n\r\n_**>>> import tensorflow as tf**_\r\n2020-09-13 19:49:51.205462: W tensorflow/stream_executor/platform/default/dso_lo\r\nader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcuda\r\nrt.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRAR\r\nY_PATH: /usr/local/cuda-10.2/lib64\r\n2020-09-13 19:49:51.205488: I tensorflow/stream_executor/cuda/cudart_stub.cc:29]\r\n Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nI tried checking with \r\n**_>>> tf.test.gpu_device_name()_**\r\nbut it gives an empty string ' '\r\n\r\nIts not a dependency issue as tf 2 supports cuda 10\r\n\r\nAnd one more weird stuff, when I run **_'nvidia-smi'_** on cmd prompt it shows cuda version as 11 but when I run **_'cat /usr/local/cuda/version.txt'_** it shows version as 10.0.130\r\n", "comments": ["I think this more a question for Stackoverflow or Ubuntu support channels then a bug.\r\nIf you really have CUDA 11 try to install `pip install tf-nightly`", "@bhack  but I have cuda 10 installed, I'll try installing tf-nightly", "@offset-null1,\r\nTensorFlow v2.3 is compatible with CUDA 10.1 and cuDNN 7.6. For more information regarding this please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\n\r\nAnd the CUDA version mismatch query has been explained in [this](https://stackoverflow.com/a/53504578) StackOverflow comment. Thanks!", "@amahendrakar thanks, I have these cuda runtime api support in my system\r\n\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n\r\nBut still tf shows:\r\n\r\n>>> import tensorflow as tf\r\n2020-09-14 14:32:03.792992: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: **_libcudart.so.11.0_**: cannot open shared object file: No such file or directory; **_LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:_**\r\n2020-09-14 14:32:03.793013: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nStill tf searching for libcudart.so.11.0 version even though I have runtime version as 10.0 maybe because my driver version is 11.0 ?\r\n\r\n\r\n\r\n\r\n\r\n", "If `nvidia-smi` still gives you CUDA 11.0 and you have also CUDA 10 file It seems to me that your CUDA installation on your system is not in a good state.", "@bhack as per [this](https://stackoverflow.com/questions/53422407/different-cuda-versions-shown-by-nvcc-and-nvidia-smi/53504578#53504578) its not an issue and my cuda runtime api support (libcudart.so) is 10.0 which tensorflow asks for and supports too, but idkw is searching for libcudart.so.11.0 eventho my machine doesn't have it.", "Can you paste the output of `nvida-smi`?", " +-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105...  On   | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   66C    P0    N/A /  N/A |    632MiB /  4040MiB |     15%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2372      G   /usr/lib/xorg/Xorg                192MiB |\r\n|    0   N/A  N/A      3214      G   /usr/bin/kwin_x11                  66MiB |\r\n|    0   N/A  N/A      3220      G   /usr/bin/krunner                    1MiB |\r\n|    0   N/A  N/A      3222      G   /usr/bin/plasmashell               46MiB |\r\n|    0   N/A  N/A      3300      G   /usr/bin/latte-dock                10MiB |\r\n|    0   N/A  N/A      3878      G   ...AAAAAAAAA= --shared-files      216MiB |\r\n|    0   N/A  N/A     13705      G   ...oken=14238599874549303426       44MiB |\r\n|    0   N/A  N/A     13926      C   /usr/bin/python3                   43MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nIk it shows 11.0 but thats driver version.\r\n\r\nRuntime api version is:\r\n>  cat /usr/local/cuda/version.txt\r\nCUDA Version 10.0.130\r\n\r\n\r\n", "Please ask support to Ubuntu support channels on how to fix your CUDA setup. \nIt seems to me not clean cause you have mixed CUDA 11 and CUDA 10 elements.", "I am having the same issue on Jetson Nano SD image v42, Cuda 10.0, and Tensorflow 2.3.1 is trying to load libcudart.so.10.2", "On Ubuntu 18.04 you need to install NVidia repo from https://developer.nvidia.com/cuda-downloads (choose installer type \"deb (network)\"). Uninstall all nvidia and cuda packages and then install packages xserver-xorg-video-nvidia-460, nvidia-driver-460, cuda-toolkit-11-0, libcudnn8 from NVidia repo.\r\n\r\n```\r\n2020-12-24 16:15:43.617963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9823 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n```\r\n"]}, {"number": 43188, "title": "Adding HDFS connection cache on tensorflow side can improve performance by 20+ times #43187", "body": "This is a PR from TaiJi AI platform in Tencent.\r\n\r\nAdding HDFS connection cache on tensorflow side can improve performance by 20+ times #43187.", "comments": ["Any chance that your particular HDFS distribution does not cache the connection internally, or it is a general issue for most distributions out there?\r\n\r\nJust curious, not a blocker for this PR. Thanks for your contribution!", "> Any chance that your particular HDFS distribution does not cache the connection internally, or it is a general issue for most distributions out there?\r\n\r\n@byronyi \r\nThanks for your reminder. I found that this issue is based on Hadoop 2.7 and Hadoop 2.8. If necessary, I will later test whether the latest version of Hadoop 3.3 has this issue.\r\n", "Can you verify whether this is still an issue in recent versions of Hadoop before we merge? If it still is an issue, I don't mind merging this.", "> Can you verify whether this is still an issue in recent versions of Hadoop before we merge? If it still is an issue, I don't mind merging this.\r\n\r\n@jhseu \r\nThanks for your comment. I did the same test in Hadoop 3.3.0 and the performance issue still exists.\r\n\r\n If there is no cache, [HadoopFileSystem::Connect](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L160) will take more than 10ms. If the cache is added, the time consuming of [HadoopFileSystem::Connect](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L160) can be ignored. For details, please refer to the following content.\r\n\r\n![image](https://user-images.githubusercontent.com/70072713/93676649-19757300-fadf-11ea-9f0b-ae465766cab7.png)\r\n\r\n**test code**\r\n```\r\n#include <sys/time.h>\r\n#include <iostream>\r\n\r\nint64_t getCurrentTime() {\r\n  struct timeval tv;\r\n  gettimeofday(&tv, NULL);\r\n  return tv.tv_sec * 1000000 + tv.tv_usec;\r\n}\r\n\r\nStatus HadoopFileSystem::Connect(StringPiece fname, hdfsFS* fs) {\r\n  int64_t start = getCurrentTime();\r\n  TF_RETURN_IF_ERROR(libhdfs()->status());\r\n  StringPiece scheme, namenode, path;\r\n  .......................\r\n  if (*fs == nullptr) {\r\n    return errors::NotFound(strerror(errno));\r\n  }\r\n  int64_t end = getCurrentTime();\r\n  std::cout << \"HadoopFileSystem::Connect use time \" << (end - start) << \" \u00b5s\" << std::endl;\r\n  return Status::OK();\r\n}\r\n```\r\n\r\n**logs** \r\n[HadoopFileSystem::Connect](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L160) without connection cache\r\n![image](https://user-images.githubusercontent.com/70072713/93676370-cd2a3300-fade-11ea-9176-33ec67feae01.png)\r\n\r\n[HadoopFileSystem::Connect](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L160) with connection cache\r\n![image](https://user-images.githubusercontent.com/70072713/93677279-4a55a800-fadf-11ea-9565-78cd5b0f9aa3.png)\r\n", "@jhseu  \r\nI think it is necessary to temporarily suspend the merging of this patch until I further update this question. Because the cause of the performance problem is not very detailed.\r\n\r\nOut of my curiosity, I will further look for the cause of the problem.\r\nAdded debug code to [hdfsBuilderConnect](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/hdfs.c#L687) on the HDFS side, and found that hdfsBuilderConnect mainly takes time in `org.apache.hadoop.fs.FileSystem.get()`.\r\n\r\nFor further analysis, I will add later.\r\n", "> Added debug code to [hdfsBuilderConnect](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/hdfs.c#L687) on the HDFS side, and found that hdfsBuilderConnect mainly takes time in `org.apache.hadoop.fs.FileSystem.get()`.\r\n>\r\n> For further analysis, I will add later.\r\n\r\nFurthermore, by adding debug logs to [FileSystem.java getInternal](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3479) and [jni_helper.c invokeMethod](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c#L126), I found that it takes more than 10ms to call `org.apache.hadoop.fs.FileSystem.get()` through [CallStaticObjectMethodV](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c#L126), but the time consumption of `org.apache.hadoop.fs.FileSystem.get()` on the JVM side is negligible. JNI is the root cause of performance issue, Even if [FileSystem Connection Cache](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3431) is effective. In order to overcome the JNI shortcomings,  I think it is very necessary to reduce the number of calls to [hdfsBuilderConnect](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L192)  by adding the connection cache on the Tensorflow side.\r\n\r\n@jhseu  Do you agree with my analysis?\r\n", "So caching in this PR is primarily to reduce the number of JNI calls, is that correct?", "> So caching in this PR is primarily to reduce the number of JNI calls, is that correct?\r\n\r\n@byronyi  \r\nYes, it is very costly to obtain HDFS connection through JNI for every HDFS operation.\r\n", "/cc @mihaimaruseac who works on modular filesystems.", "Can you make these changes to the modular filesystem implementation, too, @yuanbopeng ? Otherwise, when we switch to them there will be a new regression.", "> Can you make these changes to the modular filesystem implementation, too, @yuanbopeng ? Otherwise, when we switch to them there will be a new regression.\r\n\r\ncc @mihaimaruseac @vnvo2409 https://github.com/tensorflow/tensorflow/pull/43525\r\n", "This has caused an issue of reading HDFS, using the official model garden ResNet50:\r\n\r\n```\r\n2020-09-30 16:49:16.731277: W tensorflow/core/nccl/nccl_manager.cc:859] NcclManager already aborted, ignoring subsequent StartAbort with Not found: Success\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[while/body/_1/while/IteratorGetNext]]\r\nTraceback (most recent call last):\r\n  File \"/home/byronyi/models/official/benchmark/models/resnet_imagenet_main.py\", line 304, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/byronyi/official/benchmark/models/resnet_imagenet_main.py\", line 297, in main\r\n    stats = run(flags.FLAGS)\r\n  File \"/home/byronyi/models/official/benchmark/models/resnet_imagenet_main.py\", line 260, in run\r\n    verbose=2)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 1076, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 789, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 849, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 2946, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1927, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 561, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found:  Success\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[while/body/_1/while/IteratorGetNext]]\r\n\t [[cluster_1_1/merge_oidx_10/_50]]\r\n  (1) Not found:  Success\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[while/body/_1/while/IteratorGetNext]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_29790]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n```", "@byronyi \r\nCan you provide error information about operating HDFS?", "It is likely to be a thread-safety issue. \r\n\r\nSimply calling `tf.io.gfile` APIs on an HDFS path is fine.", "@byronyi \r\nHow do you confirm that it is related to HDFS? Have you compared the situation of removing the `HDFS connection cache`? \r\n\r\nIn addition, you can try to merge https://github.com/tensorflow/tensorflow/pull/43647 into it", "> Simply calling `tf.io.gfile` APIs on an HDFS path is fine.\r\n\r\nhttps://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/LibHdfs.md#thread-safe", "The connection cache itself might need to be thread-safe as well.", "> The connection cache itself might need to be thread-safe as well.\r\n\r\n @byronyi\r\nDo you mean to add thread synchronization here?\r\n```c\r\n  if (connectionCache_.find(cacheKey) == connectionCache_.end()) {\r\n    hdfsFS cacheFs = libhdfs()->hdfsBuilderConnect(builder);\r\n    if (cacheFs == nullptr) {\r\n      return errors::NotFound(strerror(errno));\r\n    }\r\n    connectionCache_[cacheKey] = cacheFs;\r\n  }\r\n```\r\nDuring the HDFS connection init, multiple threads may obtain the **same** HDFS connection handle and insert it into `connectionCache_` concurrently. I am not sure if synchronization is required in this case.", "@yuanbopeng You need to add a mutex to guard the `connectionCache_`.\r\n\r\n`std::map` is not thread safe and cannot be accessed from multiple threads without synchronization.", "@byronyi Thanks for your suggestion, I will fix it now.\r\n ", "@yuanbopeng \r\nPlease leave the modular filesystem part for me. There is some other work to be done as well so I will make a PR to address this issue and those changes.", "@vnvo2409 ok"]}, {"number": 43187, "title": "Adding HDFS connection cache on tensorflow side can improve performance by 20+ times", "body": "This is a issue from TaiJi AI platform in Tencent.\r\n\r\n**System information**\r\n- OS Platform and Distribution : Linux version 4.14.105-1-tlinux3-0010\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\uff08we use\uff09and the latest version also has this problem\r\n- Python version: 3.6\r\n- C++ version: 11\r\n- hadoop version: 2.8\r\n\r\n**test conclusion**\r\nAfter testing, it is found that adding hdfs connection cache on the tensorflow side can improve performance by 20+ times.\r\n\r\nAll HDFS APIs that rely on [HDFS Connect](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L160) have this performance problem. Here is the test result of [the HDFS stat API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553). [The HDFS stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) with connection cache takes about 458 microseconds, but [The HDFS stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) without connection cache takes about 12114 microseconds, so it can be concluded that adding hdfs connection cache on the tensorflow side can improve performance by **20+ Times**. For details, please refer to the test results below.\r\n\r\n**solution**: [patch-1](https://github.com/tensorflow/tensorflow/pull/43188)\r\n\r\n**test code**\r\n```\r\n#include <sys/time.h>\r\n#include <iostream>\r\n\r\nint64_t getCurrentTime() {\r\n  struct timeval tv;\r\n  gettimeofday(&tv, NULL);\r\n  return tv.tv_sec * 1000000 + tv.tv_usec;\r\n}\r\n\r\nStatus HadoopFileSystem::Stat(const string& fname, TransactionToken* token,\r\n                              FileStatistics* stats) {\r\n  int64_t start = getCurrentTime();\r\n  .......................\r\n  int64_t end = getCurrentTime();\r\n  std::cout << \"HadoopFileSystem::Stat use time \" << (end - start) << \" \u00b5s\" << std::endl;\r\n  return Status::OK();\r\n}\r\n```\r\n\r\n**logs** \r\n[stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) without connection cache\r\n![image](https://user-images.githubusercontent.com/70072713/93019627-bed5a600-f60a-11ea-8dc6-1c9b83c4d18f.png)\r\n\r\n\r\n[stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) with connection cache\r\n![image](https://user-images.githubusercontent.com/70072713/93019596-98b00600-f60a-11ea-8e17-b919a250c386.png)\r\n", "comments": ["@yuanbopeng \r\nIs there any particular reason to be using tf 1.13, support is available from 1.15 and 2.x versions.\r\nWould you want to upgrade and let us know if you still face the issue.", "> @yuanbopeng\r\n> Is there any particular reason to be using tf 1.13, support is available from 1.15 and 2.x versions.\r\n> Would you want to upgrade and let us know if you still face the issue.\r\n\r\n@Saduf2019 1.15 and 2.x versions still have this performance  problem\r\n", "@vnvo2409\r\n\r\nI noticed that this performance problem also exists in [the new HDFS filesystem plugin](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/experimental/filesystem/plugins/hadoop/hadoop_filesystem.cc). You may need to pay attention to the progress of this issue.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/experimental/filesystem/plugins/hadoop/hadoop_filesystem.cc#L207", "Don't worry. I will keep track of the change in all 3 cloud filesystems and I will merge them into the plugins soon.", "This has been solved by https://github.com/tensorflow/tensorflow/pull/43188"]}, {"number": 43186, "title": "error when trying to modify code to train  on tpu's.", "body": "I am training model on colab. Already raised issue in googlecolab/colabtools .But, no answers.\r\n\r\nI am working on kaggle dataset [https://www.kaggle.com/c/severstal-steel-defect-detection/data]. Its a segmentation task. In which label(masks) are not given directly, they gave encoded pixels. we need to transform them back to image. So, Its a multilabel segmentation. for each class of the image from the encoded pixels I converted them to image and saved in four files(four classes) also has some images in which there are no labels for them I made simply np.zeros(). In the custom data generator my aim is to for every image get those mask images stack them and finally get mask as (x,y,4).\r\nbelow is the code of the custom data generator::\r\n```\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    def __init__(self,batch_size,dataframe,x_col,y_col,number_classes,dimensions=[256,1600,3],shuffle=True):\r\n        self.batch=batch_size\r\n        self.df=dataframe\r\n        self.x=x_col\r\n        self.y=y_col\r\n        self.classes=number_classes\r\n        self.dim=dimensions\r\n        self.indexes=self.df.index.tolist()\r\n        self.shuffle=shuffle\r\n        self.index_of_indexes=np.arange(len(self.indexes))\r\n        self.on_epoch_end()\r\n        self.n=0\r\n        self.max=self.__len__()\r\n    def __len__(self):\r\n        return int(np.floor(len(self.indexes)/self.batch))\r\n    def on_epoch_end(self):\r\n        if self.shuffle==True:\r\n            np.random.shuffle(self.index_of_indexes)\r\n    def __next__(self):\r\n      if self.n>=self.max:\r\n        self.n=0\r\n      result = self.__getitem__(self.n)\r\n      self.n += 1\r\n      return result\r\n    def __getitem__(self,index):\r\n        temp_index_of_indexes=self.index_of_indexes[index*self.batch:(index+1)*self.batch]\r\n        temp_indexes=[self.indexes[i] for i in temp_index_of_indexes]\r\n        X=np.empty((self.batch,self.dim[0],self.dim[1],self.dim[2]))\r\n        Y=np.empty((self.batch,self.dim[0],self.dim[1],self.classes))\r\n        for i,id_ in enumerate(temp_indexes):\r\n            image_name=str(self.df.loc[id_,self.x])\r\n            classes_list=np.array(self.df.loc[id_,self.y])\r\n            shape=[self.dim[0],self.dim[1]]\r\n            X[i,],Y[i,]=self.get_data(image_name,classes_list,shape)\r\n        return X,Y\r\n    def get_data(self,image_name,classes_list,shape):\r\n        for i,c in enumerate(classes_list):\r\n            if i==0 and c==1:\r\n                file=image_name.split('.')[0]+'.npy'\r\n                path='/content/severstal-steel-defect-detection/temp1/'+file\r\n                channel1=np.load(path)\r\n                channel1=channel1/255.0\r\n            elif i==0 and c==0:\r\n                channel1=np.zeros((shape[0],shape[1]))\r\n            elif i==1 and c==1:\r\n                file=image_name.split('.')[0]+'.npy'\r\n                path='/content/severstal-steel-defect-detection/temp2/'+file\r\n                channel2=np.load(path)\r\n                channel2=channel2/255.0\r\n            elif i==1 and c==0:\r\n                channel2=np.zeros((shape[0],shape[1]))\r\n            elif i==2 and c==1:\r\n                file=image_name.split('.')[0]+'.npy'\r\n                path='/content/severstal-steel-defect-detection/temp3/'+file\r\n                channel3=np.load(path)\r\n                channel3=channel3/255.0\r\n            elif i==2 and c==0:\r\n                channel3=np.zeros((shape[0],shape[1]))\r\n            elif i==3 and c==1:\r\n                file=image_name.split('.')[0]+'.npy'\r\n                path='/content/severstal-steel-defect-detection/temp4/'+file\r\n                channel4=np.load(path)\r\n                channel4=channel4/255.0\r\n            elif i==3 and c==0:\r\n                channel4=np.zeros((shape[0],shape[1]))\r\n        path='/content/severstal-steel-defect-detection/train_images/'+image_name\r\n        image=load_img(path,target_size=(shape[0],shape[1],3))\r\n        image=img_to_array(image)\r\n        image=image/255.0\r\n        mask=np.stack([channel1,channel2,channel3,channel4],axis=-1)\r\n        image=tf.cast(image,dtype=tf.float32)\r\n        mask=tf.cast(mask,dtype=tf.float32)  \r\n        return image,mask\r\n```\r\nI am using unet as my model. Also, when I am using gpu its working fine. But, taking a lot of time. So, I wanted to use TPU's.\r\nSo, the datagenerator which I created is working is fine. I also made my model as a function and my loss function amd metric I mentioned them in my tpu_strategy.scope() only. I am not understanding whats my error::\r\n```\r\nbatch1=4 * tpu_strategy.num_replicas_in_sync\r\nbatch2=2 * tpu_strategy.num_replicas_in_sync\r\n\r\n\r\nwith tpu_strategy.scope():\r\n  training_model=custom_model()\r\n  def soft_dice_loss(y_true,pred):\r\n    y_true=K.flatten(y_true)\r\n    pred=K.flatten(pred)\r\n    intersec=(2*K.sum(y_true*pred))+1e-9\r\n    deno=K.sum(y_true**2)+K.sum(pred**2)+1e-9\r\n    return 1-K.mean(intersec/deno)\r\n  def soft_dice_coeff(y_true,pred):\r\n    y_true=K.flatten(y_true)\r\n    pred=K.flatten(pred)\r\n    intersec=(2*K.sum(y_true*pred))+1e-9\r\n    deno=K.sum(K.abs(y_true))+K.sum(K.abs(pred))+1e-9\r\n    return K.mean(intersec/deno)\r\n  training_model.compile(\r\n      optimizer='Adam',\r\n      loss=soft_dice_loss,\r\n      metrics=[soft_dice_coeff])\r\n```\r\n```\r\nEpoch 1/100\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-57-1b0974d37f1f> in <module>()\r\n     13                               dimensions=[256, 1600, 3],\r\n     14                               shuffle=True)\r\n---> 15 training_model.fit(train_generator,epochs=100,steps_per_epoch=train_image_csv.shape[0]//4,validation_data=valid_generator,validation_steps=valid_image_csv.shape[0]//2)\r\n\r\n14 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1101               logs = tmp_logs  # No error, now safe to assign to logs.\r\n   1102               end_step = step + data_handler.step_increment\r\n-> 1103               callbacks.on_train_batch_end(end_step, logs)\r\n   1104         epoch_logs = copy.copy(logs)\r\n   1105 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n    438     \"\"\"\r\n    439     if self._should_call_train_batch_hooks:\r\n--> 440       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n    441 \r\n    442   def on_test_batch_begin(self, batch, logs=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    287       self._call_batch_begin_hook(mode, batch, logs)\r\n    288     elif hook == 'end':\r\n--> 289       self._call_batch_end_hook(mode, batch, logs)\r\n    290     else:\r\n    291       raise ValueError('Unrecognized hook: {}'.format(hook))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)\r\n    307       batch_time = time.time() - self._batch_start_time\r\n    308 \r\n--> 309     self._call_batch_hook_helper(hook_name, batch, logs)\r\n    310 \r\n    311     if self._check_timing:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)\r\n    340       hook = getattr(callback, hook_name)\r\n    341       if getattr(callback, '_supports_tf_logs', False):\r\n--> 342         hook(batch, logs)\r\n    343       else:\r\n    344         if numpy_logs is None:  # Only convert once.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n    959 \r\n    960   def on_train_batch_end(self, batch, logs=None):\r\n--> 961     self._batch_update_progbar(batch, logs)\r\n    962 \r\n    963   def on_test_batch_end(self, batch, logs=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)\r\n   1014     if self.verbose == 1:\r\n   1015       # Only block async when verbose = 1.\r\n-> 1016       logs = tf_utils.to_numpy_or_python_type(logs)\r\n   1017       self.progbar.update(self.seen, list(logs.items()), finalize=False)\r\n   1018 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)\r\n    535     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n    536 \r\n--> 537   return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n    538 \r\n    539 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)\r\n    531   def _to_single_numpy_or_python_type(t):\r\n    532     if isinstance(t, ops.Tensor):\r\n--> 533       x = t.numpy()\r\n    534       return x.item() if np.ndim(x) == 0 else x\r\n    535     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1061     \"\"\"\r\n   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1065 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1029       return self._numpy_internal()\r\n   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1032 \r\n   1033   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: 9 root error(s) found.\r\n  (0) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1599836582.583992584\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1599836582.583991302\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\t [[strided_slice_24/_190]]\r\n  (1) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1599836582.583992584\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1599836582.583991302\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\t [[Cast_1/_62]]\r\n  (2) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1599836582.583992584\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1599836582.583991302\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\t [[Cast_1/_32]]\r\n  (3) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1599836582.583992584\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1599836582.583991302\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional]]\r\n\t [[strided_slice_86/_250]]\r\n  (4) Unavailable: {{function_node __inference_train_function_14623}} failed to con ... [truncated]\r\n```", "comments": ["@RavitejaBadugu \r\n\r\nPlease, fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "dataset::[https://www.kaggle.com/c/severstal-steel-defect-detection/data](url)\r\ncolab link::[https://colab.research.google.com/drive/18fHifO1eXAZREJ8gdkB_XKyD3GlYuolx?usp=sharing](url)", "@ravikyram did u find the solution?", "I have tried in colab with TF version 2.3 and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/18aaae5b34fcab29324b27b2e7cb5ac1/untitled354.ipynb).Thanks!", "@jhseu \r\ndid you find whats wrong in my code?", "@rxsang  and @jhseu \r\nthis is another task which is nlp classification task. I am using TPU's and here also, I am getting the same error.\r\nrepositry_link::https://github.com/RavitejaBadugu/question-quality-rating/blob/master/stack-overflow.ipynb", "@rxsang and @jhseu                                                                                                                                                                                                      \r\ndid you guys found solution?", "Hi RavitejaBadugu,\r\n\r\nData generator (e.g. tf.keras.utils.Sequence) on TPUs is not supported as it uses py_function underlyingly. Is there a way to express your input pipeline using tf.data.Dataset?", "When building pipeline by tf.data can we apply pandas operations and calling data from files?", "> When building pipeline by tf.data can we apply pandas operations and calling data from files?\r\n\r\nYou can create tf.data pipeline using pandas DataFrame, follow the documentation [here](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe) for examples. \r\nAlso, you can load csv files to tf.data using `tf.data.Dataset.list_files(train_filepaths, seed=42)`\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43186\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43186\">No</a>\n", "I was recently experiencing similar issues when working with a custom generator function and batched data on TPUs.\r\n\r\nWhat I'm doing now is to pass the data batches individually to `model.fit()`.\r\nSo I'm calling consecutive `model.fit()` statements with the same model, to train on the separate batches one after the other.\r\n\r\nI imagine this probably has some tradeoffs relative to passing the different batches into one `model.fit()`, but I guess it's one possible workaround."]}, {"number": 43185, "title": "[README] git file upload", "body": "gif \ud30c\uc77c \uc5c5\ub85c\ub4dc\ud588\uc2b5\ub2c8\ub2e4!", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/43185\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "sorry, I made a mistake\ud83d\ude13", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43185) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 43184, "title": "Keras on_epoch_end() callback output happens BEFORE completion of epoch output with verbose=1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): modified TF example code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Win10\r\n- TensorFlow version (use command below): 2.3.0\r\n\r\n**Describe the current behavior**\r\nwhen training Model using `Model.fit()` with `verbose=1 `(for progress bar) and `keras.callbacks.Callback()` with `on_epoch_end() `function I observe that callback output happens BEFORE completion of the epoch progress output, like this:\r\n```\r\nEpoch 1/2\r\n 63/125 [==============>...............] - ETA: 0s - loss: 43.2549 - mean_absolute_error: 4.6003\r\nCALLBACK MESSAGE ON END EPOCH 0\r\n125/125 [==============================] - 0s 794us/step - loss: 37.6690 - mean_absolute_error: 4.6575\r\n```\r\n\r\n**Describe the expected behavior**\r\ncallback output should happen AFTER completion of the epoch progress output:\r\n```\r\nEpoch 1/2\r\n125/125 [==============================] - 0s 900us/step - loss: 36.9897 - mean_absolute_error: 4.8264\r\nCALLBACK MESSAGE ON END EPOCH 0\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n[see gist here](https://colab.research.google.com/gist/poedator/05d7c2f1ac4cc5de65706896113b8e4d/custom_callback.ipynb)", "comments": ["The print is before the end cause you have the weight update step. \r\nCheck this https://github.com/tensorflow/tensorflow/issues/29964#issuecomment-505498333 and follow-up comments.\r\n\r\nI suppose that if there is something missing or that you want to better notify to developers you can open a small documentation PR on these lines https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L858:L862", "So if you see it is after validation and before the training end https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/training.py#L1135-L1145\r\n\r\nP.s. And you still need to be in training mode for `on_epoch_end() ` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L411-L414", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@bhack , thank you for the replies. I looked at the code and figured out the reason for such behavior of the callbacks:\r\n\r\nin `tf.keras.Model.fit()` the standard progressbar is added the following way:  \r\n```\r\nif not isinstance(callbacks, callbacks_module.CallbackList):\r\n        callbacks = callbacks_module.CallbackList(\r\n            callbacks, ... )\r\n```\r\n`CallbackList.__init__()` calls `_add_default_callbacks()`\r\n\r\nAccording to [code](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/callbacks.py#L247-L264) in `tf.keras.CallbackList._add_default_callbacks()`, if no `ProgbarLogger` is provided (and `verbose !=0`), standard `ProgbarLogger` is **APPENDED AT THE END** of the callbacks list. This is why it gets executed last and allows `CustomCallback()` to print before epoch progress printout completion.\r\n\r\n\r\nIn the example below, I pass to `Model.fit()` a `CallbackList` and include there a standard ProgbarLogger FIRST. As you see - the problem goes away!\r\n[see gist here] (https://colab.research.google.com/gist/poedator/3630b1cdec32ff6ef6ccdf6b63c4957a/custom_callback.ipynb)", "@bhack, I'd like to propose a simple commit to make the standard ProgbarLogger come first in the list of callbacks. It requires changing [this line](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/callbacks.py#L260)  from `self.callbacks.append(self._progbar)` to `self.callbacks.insert(0, self._progbar)`\r\nWhich TF branch shall I commit to? Master?", "Yes PR are on master. I don't remeber the logic about why they are appended. You can try to collect feedbacks on the PR", "just made [pull request](https://github.com/tensorflow/tensorflow/pull/43477)", "@poedator,\r\nLooks like the issue has been fixed. \r\n\r\nOn running the code with the latest TF-nightly, I got the expected output. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0eef7549a3524760e55de49a56801224/43184.ipynb). Thanks!", "Thanks, guys! This is my first PR merged into a major project."]}, {"number": 43183, "title": "Learning Rate finder for optimizers", "body": "**System information**\r\n- TensorFlow version: 2.3\r\n- Are you willing to contribute it: Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n--\r\nI am currently reading the book \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio and Aaron Courville. In this book they propose a formula for finding the best learning rate  under certain constraints. The formula is: \r\n `e* = g.T*g / g.T*H*g`\r\nwhere g is the gradient and H is the Hessian matrix at X(0).\r\nOut of interest i implemented a short script for a random loss function with two variables x,y and a random data point X(0) = {x:1,y:1}. This gives me in this arbitrary example an optimal learning rate of `0.171232876712329`.\r\n[OptimalLR.pdf](https://github.com/tensorflow/tensorflow/files/5213736/OptimalLR.pdf)\r\n\r\nUnfortunately i do not have the in depth tensorflow knowledge to play with lets say the Adam Optimizer to try it out there. One would need to get the loss function, take a data point (i assume mini batches work as well) and apply the formula to it and see if it evaluates a valid learning rate. Then run test with the default 0.01 learning rate versus the calculated learning rate and see if the calculated one performs significantly better which would justify the extra calculation. \r\n\r\nI would apprechiate feedback on whether that is possible or whether i am missing something obvious as a reason why that is not possible. For example is this calculation even feasible for deep neural networks?\r\n\r\n\r\n\r\n\r\n**Who will benefit with this feature?**\r\n--\r\n\r\nIf this algorithm is possible one could replace the fixed default learning rate of 0.01 to a calculated one which potentially improves the performance of neural networks regarding the loss function.\r\n", "comments": ["Please open this support questions to stackoverflow.\r\nIn the meantime you can quickly control the learning rate with https://www.tensorflow.org/guide/keras/custom_callback?hl=en#learning_rate_scheduling", "@GereonFranken \r\nPlease update as per above comment.", "@Saduf2019 @bhack \r\nWill do thank you for the hint and the quick response. Should i leave this issue open or close it then?", "@GereonFranken \r\nThank you for your response, please move the issue to closed status as it will be tracked at stackoverflow."]}, {"number": 43182, "title": "How to check the tensor value(this is different from issue 27519)", "body": "I want to check the tensor value in a specific layer and I find that tensor.numpy represents the value in eager mode. However, there is tf.placeholder in my code and tf.placeholder is not compatible with eager mode so I have to disable eager mode. Then how to check the value in this case? \r\n\r\nIn brief, I want to see a data(eg. 1x32x32 image) going through the neural network and check all activation between every layer. It doesn't need to be training mode, inference mode is fine. Much appreciated!", "comments": ["Please ask this support question on [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow)", "@tu1258 As mentioned by @bhack Please post support related issues in Stackoverflow. GitHub is mainly for bugs and performance related issues.\r\n\r\nIn TF1.x, your model runs in graph mode whereas in TF2.x Eager is by default. So, in order to access a value in a graph, you need to run a `session` (`tf.Session()`). There are lot of examples in the internet. [Here](https://cs330.stanford.edu/material/Tensorflow%20(1.x)%20Review%20Session.pdf) are couple of examples provided in this resource. Thanks!\r\n\r\nI am closing this issue as this is resolved. Thanks! "]}, {"number": 43181, "title": "cuda", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 43180, "title": "Removed softmax Int16 dependency on std::vector", "body": "This addresses the issue raised in https://github.com/tensorflow/tensorflow/issues/43126\r\n\r\nThe reference implementation of int16 softmax uses std::vector, this PR removes that dependency. \r\nThe approach I took was rewriting the loop to in order to calculate the exponent twice instead of saving it after the first calculation - essentially paying with CPU time for memory. Slightly less efficient but embedded friendly, especially considering that most embedded platforms will just use it as reference and probably use an optimized implementation.  \r\n", "comments": ["Thanks for the PR.\r\n\r\nI'm ok with the change from the perspective of portability of the reference implementation for TF Micro, but adding in the folks involved in PR #35996 to get their opinion.\r\n\r\n@psunn, @jdduke, @jianlijianli : thoughts on this PR?\r\n\r\n@freddan80  and @sicong-li-arm  as other folks who might want to weigh in.", "Tagging @psunn who submitted the original implementation of int16 softmax", "Instead of calling `CalculateExp` two times for each `c` wouldn't it be possible to either:\r\n* Pass a buffer of size `c` in parameter to store the intermediate results.\r\n* Use the `output_data` as intermediate buffer to store the results of  `CalculateExp`. We'll only overwrite this result when we store the final output and thus when we don't need the result of `CalculateExp` anymore.", "@Tessil for second thoughts", "\r\nYes, possible. I considered using the output as scratch as it is ideal for it, however I thought that might be too much of a dirty trick for a reference implementation. What do you think?\r\n", "Personally I think that replacing `std::vector<int16_t> exp_result_Q015(depth);` with `int16_t* exp_result_Q015 = output_data + i * depth;` in the original code with a little comment regarding the `output_data` re-usage and the overriding would be alright but it'd be best to wait what the others think of it.", "> Personally I think that replacing `std::vector<int16_t> exp_result_Q015(depth);` with `int16_t* exp_result_Q015 = output_data + i * depth;` in the original code with a little comment regarding the `output_data` re-usage and the overriding would be alright but it'd be best to wait what the others think of it.\r\n\r\nSee commit from just a minute ago :)", "> Thanks for the PR.\r\n> \r\n> I'm ok with the change from the perspective of portability of the reference implementation for TF Micro, but adding in the folks involved in PR #35996 to get their opinion.\r\n> \r\n> @psunn, @jdduke, @jianlijianli : thoughts on this PR?\r\n> \r\n> @freddan80 and @sicong-li-arm as other folks who might want to weigh in.\r\n\r\nLooks good to me as well! Thanks @yair-ehrenwald for taking this issue :) "]}, {"number": 43179, "title": "Issue with Training CenterNet Resnet50 V2 Keypoints 512x512 ", "body": "Hello i am trying to train  CenterNet Resnet50 V2 Keypoints 512x512  with hand dataset 'https://www.robots.ox.ac.uk/~vgg/data/hands/downloads/hand_dataset.tar.gz'  but facing issue with label_map.pbxt and don't know how to resolve this issue. \r\n\r\n**Errors i got during training:**  \r\nFile \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 991, in _build_center_net_model\r\n    kp_params = keypoint_proto_to_params(task, keypoint_map_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 809, in keypoint_proto_to_params\r\n    label_map_item = keypoint_map_dict[kp_config.keypoint_class_name]\r\nKeyError: '/m/01g317'", "comments": ["Are you using Centernet from model garden?", "> Are you using Centernet from model garden?\r\n\r\nyes brother, i am using from model garden with these model files: \r\n![image](https://user-images.githubusercontent.com/26037495/93018021-70aeab80-f5e6-11ea-8fba-f38d96544a16.png)\r\n", "Ok, so please close this and open the ticket on https://github.com/tensorflow/models", "@pardeep-kesnani1234 \r\nPlease feel free to close this issue as, it will be tracked in the models repo.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Even I am getting a key error for centernet\r\n\r\nTraceback - \r\n```\r\nTraceback (most recent call last):\r\n  File \"/content/models/research/object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/content/models/research/object_detection/model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/model_lib_v2.py\", line 511, in train_loop\r\n    model_config=model_config, is_training=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 1089, in build\r\n    add_summaries)\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 996, in _build_center_net_model\r\n    kp_params = keypoint_proto_to_params(task, keypoint_map_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py\", line 814, in keypoint_proto_to_params\r\n    label_map_item = keypoint_map_dict[kp_config.keypoint_class_name]\r\nKeyError: '/m/01g317'\r\n```\r\n\r\nconfigs - \r\n```\r\n'centernet_hourglass_3': {\r\n        'model_name': 'centernet_hourglass104_512x512_kpts_coco17_tpu-32',\r\n        'base_pipeline_file': 'centernet_hourglass104_512x512_kpts_coco17_tpu-32.config',\r\n        'pretrained_checkpoint': 'centernet_hg104_512x512_kpts_coco17_tpu-32.tar.gz',\r\n        'batch_size': 8\r\n    },\r\n```"]}, {"number": 43178, "title": "Make fast builds work with MSVC", "body": "Allows for successful MSVC GPU enabled build with --compilation_mode fastbuild. The resulting DLL files are linked with /DEBUG:fastlink switch which is reasonably fast and generates usable .pdb files.\r\n", "comments": ["Yes. ", "@tomaszstrejczek can you please tell me success configs to build in windows with MSVC ?", "@tomaszstrejczek also I cannot build tf-master after your commit https://github.com/tensorflow/tensorflow/issues/43767\r\n\r\nbut before your commit, compiling kernels:concat_lib pass fine.", "You are right, @iperov Unfortunately, our post-submit CI failed to automatically revert this, so I'll do the revert now.", "@iperov The command line:\r\n`bazel build -c fastbuild  //tensorflow/tools/pip_package:build_pip_package`\r\n.tf_configure.bazelrc:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/work/tensorflow/venv/Scripts/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/work/tensorflow/venv/lib/site-packages\"\r\nbuild --python_path=\"C:/work/tensorflow/venv/Scripts/python.exe\"\r\nbuild --config=xla\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_CUDA_PATHS=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\"\r\nbuild --config=cuda\r\nbuild:opt --copt=/arch:AVX --copt=/FS\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\nbuild --define=override_eigen_strong_inline=true\r\n```", "@iperov Both\r\n` bazel build -c fastbuild  //tensorflow/core/kernels:concat_lib`\r\nand\r\n`bazel build -c opt  //tensorflow/core/kernels:concat_lib`\r\nbuild for me.\r\nCould you provide me with your .tf_configure.bazelrc, command line and the version of Visual C++ so I could try to reproduce your problem?\r\n", "After reverting this internally, all of our Windows builds started passing again."]}, {"number": 43177, "title": "custom BLAS", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No):Yes (WOULD LOVE IT!)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAfter a good amount of profiling on some of the codes [here](https://github.com/dragen1860/TensorFlow-2.x-Tutorials). I found out that TensorFlow uses 'mkldnn_sgemm' and Tensorflow with \"MKL-DNN support\" (--config=mkl during build time) uses 'mkldnn::impl::cpu::avx_gemm_f32::sgemm_nocopy_driver' for **MATRIX MULTIPLICATION**. What I wanted was that somehow every time Tensorflow does matrix multiplication we make TensorFlow do these matmul operations with user defined functions. Maybe with the help of .so file?  \r\n\r\n**Will this change the current api? How?**\r\nNO\r\n\r\n**Who will benefit with this feature?**\r\nresearchers and engineers who are working on TensorFlow from a systems point of view \r\n\r\n**Any Other info.**\r\n", "comments": ["@electricSamarth As I mentioned in your other issue you filed, we stopped supporting this in 2.4.  For 2.3 you can still do this by modifying https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/core/kernels/mkl_matmul_op.cc#L28  and just for this file undef ENABLE_MKLDNN_V1 .  This will make TF call to cblas gemm and you can provide own cblas.", "Yes I realised, the other way is to just rename my matmul function to dnnl_sgemm and pack it into a shared object and the LD_PRELOAD it, so no changes on the TF side will be required!", "Thanks @agramesh1 ", "@electricSamarth \r\nIf the issue is fixed, could you close it?", "yes, sure thing!"]}, {"number": 43176, "title": "Mobile \ud300 \ud504\ub85c\uc81d\ud2b8 \ucd94\uac00", "body": "\ubaa8\ubc14\uc77c \ud300 \ud504\ub85c\uc81d\ud2b8 \ucd94\uac00", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/43176\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43176) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 43175, "title": "While training fasterRCNN with Tensoflow Object Detection API 2.(using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.)", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.10\r\n- CUDA/cuDNN version: 10.1.243 / 7.6.5\r\n- GPU model and memory: 1080ti 11GB\r\n\r\nCommand used to run training:\r\n    python3 model_main_tf2.py     --pipeline_config_path=training/pipeline.config     --model_dir=training     --num_train_steps=10000     --sample_1_of_n_eval_examples=1     --alsologtostderr\r\n\r\nTraceback:\r\n2020-09-12 14:20:06.611317: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-12 14:20:07.902280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-12 14:20:07.932835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:07.933417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.721GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-12 14:20:07.933439: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-12 14:20:07.934581: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-12 14:20:07.935660: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-12 14:20:07.935852: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-12 14:20:07.937035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-12 14:20:07.937660: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-12 14:20:07.940093: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-12 14:20:07.940227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:07.940860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:07.941301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-12 14:20:07.941543: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-12 14:20:07.945801: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 4200000000 Hz\r\n2020-09-12 14:20:07.946049: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56548f8ff7c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-12 14:20:07.946060: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-12 14:20:08.017594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:08.028832: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565490ae2c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-12 14:20:08.028848: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-09-12 14:20:08.029003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:08.029293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.721GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-12 14:20:08.029314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-12 14:20:08.029341: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-12 14:20:08.029354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-12 14:20:08.029367: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-12 14:20:08.029379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-12 14:20:08.029392: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-12 14:20:08.029405: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-12 14:20:08.029445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:08.029749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:08.030020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-12 14:20:08.030039: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-12 14:20:08.314944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-12 14:20:08.314974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-12 14:20:08.314995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-12 14:20:08.315166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:08.315535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-12 14:20:08.315849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9866 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nI0912 14:20:08.317715 140151783593792 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: 10000\r\nI0912 14:20:08.320816 140151783593792 config_util.py:552] Maybe overwriting train_steps: 10000\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0912 14:20:08.320905 140151783593792 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0912 14:20:08.454587 140151783593792 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nW0912 14:20:08.457228 140151783593792 deprecation.py:323] From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nWARNING:tensorflow:From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0912 14:20:08.479154 140151783593792 deprecation.py:323] From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f775eae7940>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Constant'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nW0912 14:20:08.549343 140151783593792 ag_logging.py:146] AutoGraph could not transform <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f775eae7940>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Constant'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f775eab3400> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nW0912 14:20:08.632742 140151783593792 ag_logging.py:146] AutoGraph could not transform <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f775eab3400> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nW0912 14:20:08.635260 140151783593792 deprecation.py:323] From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nTraceback (most recent call last):\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 584, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 119, in convert\r\n    entity, program_ctx.options, program_ctx, custom_vars)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 412, in transform_function\r\n    extra_locals)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 373, in _transformed_factory\r\n    nodes, ctx = self._transform_function(fn, user_context)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 339, in _transform_function\r\n    node = self.transform_ast(node, context)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 70, in transform_ast\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 705, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 575, in visit_FunctionDef\r\n    node = self._visit_arg_annotations(node)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 551, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py\", line 556, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 526, in train_loop\r\n    train_dataset_fn)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1128, in experimental_distribute_datasets_from_function\r\n    dataset_fn, options)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 503, in _experimental_distribute_datasets_from_function\r\n    self._container_strategy())\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 136, in get_distributed_datasets_from_function\r\n    strategy)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1183, in __init__\r\n    dataset_fn))\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py\", line 1767, in _create_datasets_per_worker_with_input_context\r\n    dataset = dataset_fn(ctx)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/model_lib_v2.py\", line 521, in train_dataset_fn\r\n    input_context=input_context)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py\", line 838, in train_input\r\n    reduce_to_frame_fn=reduce_to_frame_fn)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py\", line 196, in build\r\n    batch_size, input_reader_config)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py\", line 175, in dataset_map_fn\r\n    fn_to_map, num_parallel_calls=num_parallel_calls)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2564, in map_with_legacy_function\r\n    use_legacy_function=True))\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4084, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3339, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 544, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 376, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 407, in _create_definition_if_needed_impl\r\n    capture_resource_var_by_value=self._capture_resource_var_by_value)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 969, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3331, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3299, in _wrapper_helper\r\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 255, in wrapper\r\n    return converted_call(f, args, kwargs, options=options)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 591, in converted_call\r\n    return _fall_back_unconverted(f, args, kwargs, options, e)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 398, in _fall_back_unconverted\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 339, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py\", line 819, in transform_and_pad_input_data_fn\r\n    tensor_dict=transform_data_fn(tensor_dict),\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py\", line 249, in transform_input_data\r\n    out_tensor_dict = data_augmentation_fn(out_tensor_dict)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py\", line 577, in augment_input_data\r\n    include_dense_pose=include_dense_pose))\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/core/preprocessor.py\", line 4591, in preprocess\r\n    results = func(*args, **params)\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/core/preprocessor.py\", line 4093, in random_square_crop_by_scale\r\n    if ymin == 0:\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 877, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 490, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 479, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\nConfig File\r\n`\r\n\r\nmodel {\r\n  faster_rcnn {\r\n    num_classes: 33\r\n    image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 800\r\n        max_dimension: 1333\r\n        pad_to_max_dimension: true\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_resnet101_keras'\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 14\r\n    maxpool_kernel_size: 2\r\n    maxpool_stride: 2\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 4\r\n  num_steps: 200000\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.01\r\n          total_steps: 200000\r\n          warmup_learning_rate: 0.0\r\n          warmup_steps: 5000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint_version: V2\r\n  fine_tune_checkpoint: \"/media/convsys/c924758e-a96b-40c5-86de-4d47f53b16db/models-master/research/object_detection/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8/checkpoint/ckpt-0\"\r\n  fine_tune_checkpoint_type: \"classification\"\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n\r\n  data_augmentation_options {\r\n    random_adjust_hue {\r\n    }\r\n  }\r\n\r\n  data_augmentation_options {\r\n    random_adjust_contrast {\r\n    }\r\n  }\r\n\r\n  data_augmentation_options {\r\n    random_adjust_saturation {\r\n    }\r\n  }\r\n\r\n  data_augmentation_options {\r\n     random_square_crop_by_scale {\r\n      scale_min: 0.6\r\n      scale_max: 1.3\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  label_map_path: \"training/fashion.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"training/train.record\"\r\n  }\r\n}\r\n\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n  batch_size: 1;\r\n}\r\n\r\neval_input_reader: {\r\n  label_map_path: \"training/fashion.pbtxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: \"training/test.record\"\r\n  }\r\n}`\r\n", "comments": ["Already seen [https://github.com/tensorflow/tensorflow/issues/40143](url)", "Are you using https://github.com/tensorflow/models/tree/master/research/object_detection?", "> Are you using https://github.com/tensorflow/models/tree/master/research/object_detection?\r\n\r\nyes", "Ok please check that you have a fresh version of that repository. If you still have a problem please close this and open an issue directly in that repository. "]}, {"number": 43174, "title": "NotFoundError:  No algorithm worked! when using Conv2D", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux fully updated\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Installed from pacman\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: Cuda is 11.0, cuDNN is 8.0.2\r\n- GPU model and memory: RTX 2070 Super\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am running the following code: https://github.com/davidADSP/GDL_code/blob/tensorflow_2/02_03_deep_learning_conv_neural_network.ipynb\r\n\r\nentry 11, when it does the fit, it fails. It returns:\r\n\r\nNotFoundError:  No algorithm worked!\r\n\t [[node functional_1/conv2d/Conv2D (defined at <ipython-input-7-10b06c61fca5>:1) ]] [Op:__inference_train_function_2021]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\nThis used to work ok previously, 1-2 months ago. I was just testing some other conv2d today, and it failed, so I went to test this example which I know it worked, and it also fails\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code should work\r\n\r\n**Standalone code to reproduce the issue**\r\nSee the previous link. It's a github jupyter code\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-7-10b06c61fca5> in <module>\r\n----> 1 model.fit(x_train\r\n      2           , y_train\r\n      3           , batch_size=32\r\n      4           , epochs=10\r\n      5           , shuffle=True\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    838         # Lifting succeeded, so variables are initialized and we can run the\r\n    839         # stateless function.\r\n--> 840         return self._stateless_fn(*args, **kwds)\r\n    841     else:\r\n    842       canon_args, canon_kwds = \\\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2827     with self._lock:\r\n   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2830 \r\n   2831   @property\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1841       `args` and `kwargs`.\r\n   1842     \"\"\"\r\n-> 1843     return self._call_flat(\r\n   1844         [t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n   1845          if isinstance(t, (ops.Tensor,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1921         and executing_eagerly):\r\n   1922       # No tape is watching; skip to running the function.\r\n-> 1923       return self._build_call_outputs(self._inference_function.call(\r\n   1924           ctx, args, cancellation_manager=cancellation_manager))\r\n   1925     forward_backward = self._select_forward_and_backward_functions(\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    543       with _InterpolateFunctionError(self):\r\n    544         if cancellation_manager is None:\r\n--> 545           outputs = execute.execute(\r\n    546               str(self.signature.name),\r\n    547               num_outputs=self._num_outputs,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nNotFoundError:  No algorithm worked!\r\n\t [[node functional_1/conv2d/Conv2D (defined at <ipython-input-7-10b06c61fca5>:1) ]] [Op:__inference_train_function_2021]\r\n\r\nFunction call stack:\r\ntrain_function\r\n", "comments": ["Do you have a very minimal  but runnable standalone code/colab example?\r\nAlso I suggest you to import and use Keras from Tensorflow `tf.keras`", "@bhack the imports are such as \"import tensorflow.keras\", I have updated the link above, which pointed to the master branch which is for tf 1, but I am using tf2\r\n\r\nThe example code is this: https://github.com/davidADSP/GDL_code/blob/tensorflow_2/02_03_deep_learning_conv_neural_network.ipynb\r\n\r\nThe code would be like:\r\n\r\n```\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.utils import to_categorical\r\nimport tensorflow.keras.backend as K \r\n\r\nfrom tensorflow.keras.datasets import cifar10\r\n\r\nNUM_CLASSES = 10\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\nx_train = x_train.astype('float32') / 255.0\r\nx_test = x_test.astype('float32') / 255.0\r\ny_train = to_categorical(y_train, NUM_CLASSES)\r\ny_test = to_categorical(y_test, NUM_CLASSES)\r\n\r\n\r\n\r\ninput_layer = Input((32,32,3))\r\n\r\nx = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\r\nx = BatchNormalization()(x)\r\nx = LeakyReLU()(x)\r\n\r\n\r\nx = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\r\nx = BatchNormalization()(x)\r\nx = LeakyReLU()(x)\r\n\r\n\r\nx = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\r\nx = BatchNormalization()(x)\r\nx = LeakyReLU()(x)\r\n\r\n\r\nx = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\r\nx = BatchNormalization()(x)\r\nx = LeakyReLU()(x)\r\n\r\n\r\nx = Flatten()(x)\r\n\r\nx = Dense(128)(x)\r\nx = BatchNormalization()(x)\r\nx = LeakyReLU()(x)\r\nx = Dropout(rate = 0.5)(x)\r\n\r\nx = Dense(NUM_CLASSES)(x)\r\noutput_layer = Activation('softmax')(x)\r\n\r\nmodel = Model(input_layer, output_layer)\r\n\r\nopt = Adam(lr=0.0005)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n\r\nmodel.fit(x_train\r\n          , y_train\r\n          , batch_size=32\r\n          , epochs=10\r\n          , shuffle=True\r\n          , validation_data = (x_test, y_test))\r\n```", "This last example It works for me. \nGenerally CUDA 11 still requires Tensorflow ningthly.\nCan you try with tf-nightly?", "@bhack I don't think it's related to CUDA.\r\n\r\nI have just tested it on another computer, same OS (Arch Linux), same libraries.\r\n\r\nThe only difference is that the computer where it doesn't work uses a RTX 2070 Super. The computer where it works uses a GTX 1080 TI.", "I think that It Is an installation/packaging issue on your machine cause It Is working on colab.\n\nCan you try an installation with pip?\nhttps://www.tensorflow.org/install/pip", "Adding the following lines fixes the issue:\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```\r\n\r\nOdd, because I didnt need them before.\r\n\r\nThis is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.", "Also double check CUDA versions cause CUDA 11 Is on nightly. I don't know what Arch Linux maintainers done but in that case It Is an Arch Linux issue. See https://github.com/tensorflow/tensorflow/issues/40227", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have the same problem on pip-installed tf-nightly with CUDA11 in nvidia's cuda-ubuntu-18.04 docker container.\r\n@thephet 's fix workaround fixed it for me.\r\nInteresting enough, the system showing the issue is using a RTX 2060 Super.\r\n", "@ninnghazad \r\nIs this still an issue.", "@thephet you saved me! I have spent 4 days fixing this on RTX 3080, nothing worked.\r\n\r\nNVidia NGC container with tensorflow https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow\r\nCard RTX 3080\r\ncuda 11.0\r\nTF 2.2.0 (default in the container, but also worked with tf-nightly)", "> Is this still an issue.\r\n@Saduf2019  yes\r\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43174\">No</a>\n", "> Adding the following lines fixes the issue:\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n> \r\n> Odd, because I didnt need them before.\r\n> \r\n> This is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.\r\n\r\nYes!!! This solved my problem on RTX 2070 card!", "> > Adding the following lines fixes the issue:\n> \n> > \n> \n> > ```\n> \n> > from tensorflow.compat.v1 import ConfigProto\n> \n> > from tensorflow.compat.v1 import InteractiveSession\n> \n> > \n> \n> > config = ConfigProto()\n> \n> > config.gpu_options.allow_growth = True\n> \n> > session = InteractiveSession(config=config)\n> \n> > ```\n> \n> > \n> \n> > Odd, because I didnt need them before.\n> \n> > \n> \n> > This is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.\n> \n> \n> \n> Yes!!! This solved my problem on RTX 2070 card!\n\n\nSame here!\nMy environment is RTX1660 Super and running on docker with `tensorflow/tensorflow:2.4.0rc3-gpu-jupyter` image.\nIt perfectly works with this few lines.\n\n", "I faced this very same error and turns out that when you read a Grayscale Image as an RGB Image (as I was using ImageDataGenerator and did not provide `color_mode=\"grayscale\"` as an argument to the generator when flowing the data), it throws the \"Algorithm did not work\" sort of error.\r\n\r\nHopefully it helps someone!", "CC @imintz ", "> Adding the following lines fixes the issue:\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n> \r\n> Odd, because I didnt need them before.\r\n> \r\n> This is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.\r\n\r\nYes, this fixed the problem for me on 1660Ti. Does anyone know the reason for this behavior?", "This just happened to me too on an RTX 2070 Super. It worked before but after restarting the container I get the same error message. It still works when I run it on CPU.\r\n\r\nThe lines mentioned by @thephet did *not* help.", "> Adding the following lines fixes the issue:\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n> \r\n> Odd, because I didnt need them before.\r\n> \r\n> This is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.\r\n\r\nThis works for me.  I have 2080 GPU and was also trying to run Colab code when running into this issue.  Thanks!", "> Adding the following lines fixes the issue:\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n> \r\n> Odd, because I didnt need them before.\r\n> \r\n> This is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.\r\n\r\nThe issue still persists. Adding those lines didn't helped me.", "same issue here with docker image `tensorflow/tensorflow:2.4.0-gpu-jupyter` RTX 2070 Super.\r\nP.S. Restarting the kernel and adding the code below resolves the issue.\r\n```\r\nimport tensorflow as tf\r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n```", "\r\n> Adding the following lines fixes the issue:\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n> \r\n> Odd, because I didnt need them before.\r\n> \r\n> This is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.\r\n\r\n\r\nI am using tf 2.4.0, cuda 11.0 on a GeForce RTX 2060 \r\nnvidia drivers 455.45.01 on Ubuntu 20.04\r\n\r\nThose lines are needed for me too, to make the gpu work.\r\nOn cpu all works fine without errors even without that.\r\n\r\nI also get another error when running RNNs, \r\n```\r\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\ntensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1514 : Unknown: Fail to find the dnn implementation.\r\n```\r\nwhich also get fixed with the preamble mentioned by thephet above.\r\nThis issue might be related to https://github.com/tensorflow/tensorflow/issues/36508\r\n", "I had the exact same error but for a much more stupid mistake than people mentioned above. Model was defined with input shape (height, width, 1) but images were RGB, not grayscale. ", "A code from other issue helped me to find a way to limit tensorflow GPU memory usage and solved the issue, please see:\r\n\r\nhttps://github.com/google-research/text-to-text-transfer-transformer/issues/57#issuecomment-704160406\r\n\r\nthe code is:\r\n\r\n```python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\r\n  try:\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Virtual devices must be set before GPUs have been initialized\r\n    print(e)\r\n ```", "> same issue here with docker image `tensorflow/tensorflow:2.4.0-gpu-jupyter` RTX 2070 Super.\r\n> P.S. Restarting the kernel and adding the code below resolves the issue.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> physical_devices = tf.config.list_physical_devices('GPU') \r\n> tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n> ```\r\n\r\nSame worked for me on 2070 Super tf 2.4.1\r\nThanks", "> same issue here with docker image `tensorflow/tensorflow:2.4.0-gpu-jupyter` RTX 2070 Super.\r\n> P.S. Restarting the kernel and adding the code below resolves the issue.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> physical_devices = tf.config.list_physical_devices('GPU') \r\n> tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n> ```\r\n\r\nSame issue on RTX 2060 Ti with TF 2.4.1 \r\nIf I upgrade TF 2.4.1 to tf-nightly, the missing CUDA library error occurs. \r\n\r\nThe proposed above solution was best for me. \r\nBut still, it has drawbacks that limit the GPU memories. \r\nIt could be a temporary solution. ", "> > same issue here with docker image `tensorflow/tensorflow:2.4.0-gpu-jupyter` RTX 2070 Super.\r\n> > P.S. Restarting the kernel and adding the code below resolves the issue.\r\n> > ```\r\n> > import tensorflow as tf\r\n> > physical_devices = tf.config.list_physical_devices('GPU') \r\n> > tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n> > ```\r\n> \r\n> Same issue on RTX 2060 Ti with TF 2.4.1\r\n> If I upgrade TF 2.4.1 to tf-nightly, the missing CUDA library error occurs.\r\n> \r\n> The proposed above solution was best for me.\r\n> But still, it has drawbacks that limit the GPU memories.\r\n> It could be a temporary solution.\r\n\r\nWorked for me:\r\nOS: Ubuntu 18.04\r\nTensorFlow version: 2.4.1\r\nPython version: 3.7.10\r\nCUDA/cuDNN version: Cuda is 11.0, cuDNN is 8.0.4\r\nGPU model and memory: GeForce GTX 1660 SUPER\r\n\r\nStill doesn't make any sense, thanks anyway :)\r\n", "This might be too late but still be of help to post it here since I too faced the exact same issue. For me, I just restricted the amount of GPU that the model was allowed to use. Also, the way I did is an older way to do it which I did it in tf 1.15 but it for sure works on the latest tf 2.4 version as well. Below is the code:\r\n\r\n```\r\ndef set_tensorflow_config(per_process_gpu_memory_fraction=0.8):\r\n    config = tf.compat.v1.ConfigProto()\r\n    # config = tf.ConfigProto()\r\n    config.gpu_options.per_process_gpu_memory_fraction = per_process_gpu_memory_fraction\r\n    config.gpu_options.allow_growth=True\r\n    # sess = tf.Session(config=config)\r\n    sess = tf.compat.v1.Session(config=config)\r\n    \r\n    print(\"== TensorFlow Config options set ==\")\r\n    print(\"\\nThis process will now utilize {} GPU Memeory Fraction\".format(per_process_gpu_memory_fraction))\r\n\r\n```\r\n\r\n`set_tensorflow_config()`\r\n\r\n\r\n**GPU:** NVIDIA GeForce RTX 2070, 8GB", "> Adding the following lines fixes the issue:\r\n> \r\n> ```\r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> ```\r\n> \r\n> Odd, because I didnt need them before.\r\n> \r\n> This is needed for the computer with the RTX 2070 Super. The one with GTX 1080 TI doesnt need them. Same libraries.\r\n\r\nConfirmed to fix the same issue\r\nOS: Kubuntu 20.04LTS\r\nTF: 2.4.0\r\nGPU: Nvidia RTX 3060 mobile (100w)\r\nPython: 3.8.5\r\nCuda: 11.0\r\n\r\nI was trying ![TFLite export using this fork](https://github.com/zldrobit/yolov5/tree/tf-android)", "I am facing the same issues ,I have tried all the solutions given above but its not resolving my issues.\r\n", "Was able to replicate the issue in TF nightly-2.6.0.dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/9c11fa36f8e99d40b11583956a18b0d7/untitled205.ipynb)..Thanks !", "Can you try rerunning with TF 2.5 and with the env var `TF_DEVICE_MIN_SYS_MEMORY_IN_MB` set to something high, like `1000`?", "anybody has a solution for this in 3080?", "In my case, this error appears because the memory is full. Try to check `nvidia-smi` via terminal. In my case, I use cloud server using `jupyter`. Shutdown all kernels (not only close the file, but shutdown), and restarting solve the issue.\r\n\r\nHope it helps anyone stumble upon this.", "I solved my problem,\r\nuse cuda 11.0 and cudnn 8.0.5(with cuda11.0), in ubuntu 20.04,and nvidia driver = 460.84, for my GTX 1660ti\r\nother version of cuda(11.2 , 11.1) will get the error", "> \r\n> \r\n> I faced this very same error and turns out that when you read a Grayscale Image as an RGB Image (as I was using ImageDataGenerator and did not provide `color_mode=\"grayscale\"` as an argument to the generator when flowing the data), it throws the \"Algorithm did not work\" sort of error.\r\n> \r\n> Hopefully it helps someone!\r\n\r\nThat works for me, thank you! I use opencv for RGB to GRAY:\r\n```python\r\ngray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\r\n```", "Yes!!! This solved my problem on GTX 1660 TI card!", "@thephet \r\nPlease confirm if this is still an issues, it seems to have been resolved, can you try on the latest version and let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43174\">No</a>\n", "To check and will solve easily:\r\n1) Check your image input, maybe you are wrong with dimensions (usually channels) and all \r\n2) Out of GPU memory\r\n\r\nSteve\r\n", "Same here today; the extra config lines for gpu_options.allow_growth = True fixed the \"NotFoundError: No algorithm worked!\" issue for me for code that was running fine on one PC but gave the above problem on another PC with AFAIK the same TensorFlow configuration, same libraries etc but a different GPU (both NVIDIA).\r\n\r\nOS: Windows 10 64-bit\r\nTF: 2.4.1\r\nGPU: NVIDIA Quadro T1000 with Max-Q Design (on the other Windows 10 64-bit system PC w/o problems it was NVIDIA Quadro P1000)\r\nPython: 3.7.6\r\nCUDA: 11.2\r\nnvcc --version: 11.2, V11.2.67 (on the PC w/o problems: 11.2, V11.2.142)\r\nRAM: 32GB (on the PC w/o problems 16GB)\r\n\r\nWhy is this fix needed only in some cases and why does it work?\r\n\r\nBTW, the shorter \"os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\" also worked fine for me.\r\n", "> ```python\r\n> gpus = tf.config.experimental.list_physical_devices('GPU')\r\n> if gpus:\r\n>   # Restrict TensorFlow to only allocate 4GB of memory on the first GPU\r\n>   try:\r\n>     tf.config.experimental.set_virtual_device_configuration(\r\n>         gpus[0],\r\n>         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\r\n>     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n>     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n>   except RuntimeError as e:\r\n>     # Virtual devices must be set before GPUs have been initialized\r\n>     print(e)\r\n> ```\r\n\r\nThis solution worked for me...Thanks for the help...", "I see there is a \"solution\", but this should never happen in a popular library in the first place. Why is this happening, and why users need to add this extra code to get tensorflow to work? I have another process that uses 6gb memory and for some reason tf cannot simply coexist with it, when I kill it the super helpful error message \"UNIMPLEMENTED: DNN library is not found\" disappears. I can understand that things can be more efficient if you map the whole gpu memory, but in case of issues at least it would be nice to fail with clear error messages. No issues with pytorch. Ehh, I can only hope Jax works out much better.\r\n\r\nBtw, for me the following worked (limiting the memory, to 2gb, single gpu case):\r\n\r\n```python\r\ngpus = tf.config.list_physical_devices('GPU')\r\ntf.config.set_logical_device_configuration(\r\n    gpus[0],\r\n    [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\r\n```"]}, {"number": 43173, "title": "subclass of tf.keras.Model throws \"NotImplementedError\" when fit() with custom data", "body": "TF 2.3.0 tested on Windows and in Colab.\r\n\r\nThere is an error that comes out only when certain type of model (subclass of tf.keras.Model) is combined with certain custom dataloaders (Python generator and subclass of tf.keras.utils.Sequence). Yet same loaders work OK with simple Functional Keras model. \r\n\r\nthe error is \"**NotImplementedError: When subclassing the `Model` class, you should implement a `call` method**\"\r\n\r\nSee Colab reproduction [here.](https://colab.research.google.com/drive/19hbGTk-RAjdHiUfM-cgDPMNTyu5FRIQf?usp=sharing)", "comments": ["I have tried in colab with TF version 2.3, nightly versions(`2.4.0-dev20200912`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/578ba17757a9a9afc21e8659b65e2440/untitled338.ipynb#scrollTo=ucG1nfsXCABQ). Thanks!", "It is not a bug. You need to implement the `call` method in your class when sublcassing. (e.g. in your `class AE1` etc.):\r\nSee https://keras.io/guides/making_new_layers_and_models_via_subclassing/\r\n", "@bhack, thank you for the suggestion. I added `call()` to my test model and it works OK.  [Gist with working model here](https://colab.research.google.com/gist/poedator/21bb68a5b18b4766b9e45ebdc6f02781/tf_loader_issues.ipynb).\r\n\r\nWhat I find confusing is:\r\n1) some combinations of models and loaders work OK in my original example. Even without `call()` in my example the subclassed `Model` worked fine with `tf.data.Dataset.from_tensor_slices`\r\n2) some examples at TF/Keras site do not use call() when subclassing models.\r\n Example: [VAE tutorial ](https://keras.io/examples/generative/vae/) by respectable @fchollet - he just feeds data directly to `Model.fit()` without loaders and everything works fine without `call()`. \r\n\r\nThere are quite a few confused people out there trying to replicate Keras examples and struggling with this inconsistency (see [one](https://stackoverflow.com/questions/63832318/keras-using-a-generator-for-the-data-vae), [two](https://stackoverflow.com/questions/63822281/issue-with-modifying-a-keras-class-to-include-call-function)).\r\nPerhaps some warnings should be made in the examples / documentations, promoting consistent use of `call()`, also see if any code changes are needed to ensure consistent enforcement of `call()` use.", "@poedator Cause It Is requested/checked for the `build` case.\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L320:L328\n\nif you want to better notify developers about this please open a documentation PR on this doc string and also on https://github.com/keras-team/keras-io/blob/master/guides/making_new_layers_and_models_via_subclassing.py\n\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43173\">No</a>\n", "[updated gist with issues demonstration](https://colab.research.google.com/gist/poedator/06e66814d0d914cfe65a70bd9595c98d/tf_loader_issues_without_call.ipynb)\r\nopened [pull request](https://github.com/keras-team/keras-io/pull/258)", "I agree with the initial assessment, that this is more like a bug than a documentation issue.\r\n\r\n1. If `call` were **required** it would be an abstract method, instead of just not-implemented.\r\n2. Changing the data-source shouldn't cause this change in requirements.\r\n\r\nLooking into it a little more I see that it's this line in `data_adaptor.GeneratorDataAdaptor` causing the inconsistency:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5d5534edf7d0b73cb23f7069135d674e1d27250b/tensorflow/python/keras/engine/data_adapter.py#L787\r\n\r\n@omalleyt12, WDYT? What's the right way to fix this?", "@MarkDaoust \r\nFor 1. Do you mean that you want to decorate `@six.add_metaclass(abc.ABCMeta)` at :\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d0cc5dfc14765d4c1e00c65bcf47d81390e64332/tensorflow/python/keras/engine/training.py#L420\r\n\r\n", "> you want to decorate @six.add_metaclass(abc.ABCMeta) at: ...\r\n\r\nI don't *want* it. I'm just pointing out that it isn't currently there, and it's not clear why fitting with a generator should require `call` (I see it's being used to `build` the model, but maybe there's a way around that, or maybe there's a clearer error we can throw here if `call` is not implemented.).", "I don't know the full story of this but it seems to me that there is \"you should\" in the raised string and in other documentations entries.\r\nSo the logic it seems to me more related to the case when the user \"voluntary\"  invoke call. \r\n\r\nIn the case of the generator instead this \"direct\" user control doesn't seems to me so explicit as it is a little bit hidden/internal. ", "Requiring definition of call when using generator as data input is very confusing to me also. I am facing this problem. Anyone was able to fix it?", "Francois fixed this here:\r\n\r\nhttps://github.com/keras-team/keras/commit/d9abcd788e5419560c4d8bd47b22ec387fe5c9c2\r\n\r\nIt will be in TensorFlow 2.7 which will be released shortly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43173\">No</a>\n", "I solved this error by adding the following `call` method.\r\n```python\r\n def call(self, data, training=False): \r\n        # You don't need this method for training.\r\n        # So just pass.\r\n        pass\r\n```"]}, {"number": 43172, "title": "Random NaN loss when using float16 dtype and batch size of 1", "body": "**System information**\r\n- OS Platform and Distribution: Google Colab\r\n- TensorFlow version (use command below): tf-nightly (2.4.0-dev20200911)\r\n\r\n**Reproducing code**\r\nsee [https://colab.research.google.com/drive/1miE61h7rlSJL0xIG4yNdVx05lVxT-oKb?usp=sharing](https://colab.research.google.com/drive/1miE61h7rlSJL0xIG4yNdVx05lVxT-oKb?usp=sharing)\r\n\r\n\r\n````\r\nimport math\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nx = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\r\ny = np.array([[0], [1], [1], [0]])\r\n\r\nloss_function = tf.keras.losses.BinaryCrossentropy()\r\n\r\nfor i in range(2000):\r\n    if i % 100 == 0:\r\n        print(\"Iteration {}\".format(i))\r\n\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Dense(units=1, dtype=tf.float16)])\r\n\r\n    model.compile(optimizer='sgd', loss=loss_function)\r\n    model.fit(x=x, y=y, epochs=1, batch_size=1, verbose=0)\r\n\r\n    loss_result = loss_function(y, model(x))\r\n\r\n    if math.isnan(loss_result):\r\n        raise RuntimeError('NAN Error in iteration {}'.format(i))\r\n````\r\n\r\n**Behavior description**\r\n\r\nSeemingly non-deterministic occurence of a NaN result when calculating loss of a very simple Dense Model. The NaN loss seems to happen randomly and can occur on the 60th or 600th iteration. In the supplied Google colab code it happened in the 343rd iteration. The bug only seems to occur using a dtype of float16 and batch_size of 1. Debugging the error lead me to see that the models producing a NaN loss seem to have been initialized with a NaN bias and kernel, though I couldn't get to the bottom of why.\r\n\r\n**Background**\r\n\r\nI already opened this issue before in the following ticket: [https://github.com/tensorflow/tensorflow/issues/38457](https://github.com/tensorflow/tensorflow/issues/38457)\r\n\r\nThe assigned Tensorflower closed the issue after supplying a hotfix, with the note that I shall reopen the issue if I am not satisfied with the solution. As the supplied solution is only a hotfix, though does not solve the underlying issue even in the current tf nightly (see the Google colab above in which I tested the current nightly) am I not satisfied with the solution, particularly since the issue is very easily reproducible by setting the dtype to float16 and batch_size to 1. The ticket was closed with the request to reopen the issue, if not satisfied, which I understood as the request to create a new issue as I am obviously no TF contributor and can't reopen issues...", "comments": ["This is an \"edge\" overflow initialization extracted from your loop:\r\n\r\nTry to run this multiple time this and e.g. change it with `tf.random.set_seed(1)` or removing the seed.\r\n```\r\ntf.random.set_seed(0)\r\nx = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\r\ny = np.array([[0], [1], [1], [0]])\r\n\r\nloss_function = tf.keras.losses.BinaryCrossentropy()\r\n\r\nl=[]\r\nw=np.array([[-0.2373],[ 1.    ]], dtype=np.float16) #weights\r\nb=np.array([0.], dtype=np.float16) #array of biases\r\nl.append(w)\r\nl.append(b)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(units=1, input_dim=2, dtype=tf.float16))\r\n\r\nopt = tf.keras.optimizers.SGD(learning_rate=0.1)\r\nmodel.layers[0].set_weights(l)\r\nprint(\"Init weights\")\r\nprint(model.get_weights())\r\nmodel.compile(optimizer=opt, loss=loss_function) \r\nmodel.fit(x=x, y=y, epochs=1, batch_size=1, verbose=0)\r\nprint(\"Fitted weights\")\r\nprint(model.get_weights())\r\n\r\nloss_result = loss_function(y, model(x, training=False))\r\n\r\nif math.isnan(loss_result):\r\n    print('NAN Error')\r\n```", "About the backward pass (in fit) see also https://www.tensorflow.org/guide/mixed_precision?hl=en#underflow_and_overflow", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you very much @bhack for supplying the reasoning behind the bug!\r\nDo I understand it correctly then that this bug is due to a mixed precision constraint by the hardware and fixing it won't be possible without slowing down the mixed precision processing significantly? Tensorflow therefore can't do anything about this issue!?\r\n\r\nIs this a known constraint or how did you manage to debug the framework and find that out? I would love to know more about doing such complicated debugging to improve my skills with the in internals of Tensorflow. I would be thankful if you could point me to some good learning resources for such things if you know any.\r\n\r\nEither way, thank you very much for the response! Also please excuse my late reply, as I was on a holiday last week. ", "If you want you can play a little bit with a custom loop:\r\nhttps://www.tensorflow.org/guide/mixed_precision?hl=en#training_the_model_with_a_custom_training_loop\r\n\r\nYou can close the ticket, for other questions that are not Bugs or Feature Requests please use our channels:\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\nhttps://www.tensorflow.org/community/forums\r\n\r\n"]}, {"number": 43171, "title": "OpenCL", "body": "I understand TensorFlow only supports CUDA. What would need to be done to add in OpenCL support?", "comments": ["It doesn't support CUDA only but also ROCM for AMD, Onednn for Intel, delagates GPU/NPU on android etc..\r\nAlso the new in progress [runtime](https://github.com/tensorflow/runtime) probably will expand the support over new hw ecosystem targets.", "> What would need to be done to add in OpenCL support?\r\n\r\nOpenCL would need to improve it's standards to become a viable option ... \r\n\r\nIt'd have to change into a single source programming model where host and device code can lie in the same source file. This ties into the previous statement but OpenCL kernels would also need to be able to support templates too ... ", "@akashakara There is OpenCL support via SYCL on Tensorflow. Please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/22#issuecomment-266050835). ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43170, "title": "Segfault with tf.linalg.cholesky", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Mint 20\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary (PyPI)\r\n- TensorFlow version (use command below): 2.4.0-dev20200911\r\n- Python version: 3.7.6 (Anaconda)\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA 11.0, cuDNN 8.0.3.33 (NVIDIA Ubuntu 20.04 repo)\r\n- GPU model and memory: GeForce GTX 1650 3.82GiB\r\n\r\n**Describe the current behavior**\r\nCalling `tf.linalg.cholesky` results in Python aborting (SIGABRT). \r\n\r\n**Describe the expected behavior**\r\nThe Cholesky decomposition succeeds.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```Python\r\npython -c \"import tensorflow as tf; tf.linalg.cholesky(tf.eye(6))\"\r\n```\r\n\r\n**Other info / logs**\r\nLog attached: [tf_error_log.txt](https://github.com/tensorflow/tensorflow/files/5212288/tf_error_log.txt)\r\n\r\n", "comments": ["@chrism0dwk \r\n\r\nI have tried in colab with TF nightly version(`2.4.0-dev20200912`) and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/835b6b031991af76988c9252b902289c/untitled340.ipynb).Please, verify once and close the issue. Thanks!", "Shucks, looks like it's my CUDA 11 stack, then.  Thanks for verifying. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43170\">No</a>\n", "Re-opening, I'm afraid.  Colab appears to be using CUDA-10.1 still (and runs error-free as above):\r\n\r\n```python\r\n!/usr/local/cuda/bin/nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n```\r\nwhereas the tf-nightly packages from PyPI appear to demand CUDA-11.0.   Does this mean that the Colab tf-nightly build is against a different CUDA version to that available more widely?\r\n\r\nChris\r\n", "A quick update: I tried installing tf-nightly via the Docker container, and I get the same error.  So, either this is a problem with my NVIDIA driver version (455.23.05), or there is an underlying bug in TF.  Terminal log from the interactive container attached.\r\n\r\n[tf_docker_abrt.txt](https://github.com/tensorflow/tensorflow/files/5321758/tf_docker_abrt.txt)\r\n\r\nIt's worth also saying I've compiled from source and still found the error.", "Update: same system as above, but now with TF version 2.5.0_dev20201210.\r\n\r\nThe bug still persists on my system, despite clean install/re-install of CUDA and TF.  \r\n\r\nI found out how to use TF_CPP_MIN_LOG_LEVEL and TF_CPP_MIN_VLOG_LEVEL.  Debug information attached.\r\n[chol_log.txt](https://github.com/tensorflow/tensorflow/files/5678190/chol_log.txt)\r\n", "Was able to run your code without any error in Tensorflow 2.5 and it's using Cuda 11, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/a8f029cd2884887e0894383bcbefc2bf/43170.ipynb) and confirm if the issue is good to close. Thanks!", "Yes, looks good with TF2.5 and CUDA 11.2.  Many thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43170\">No</a>\n"]}]