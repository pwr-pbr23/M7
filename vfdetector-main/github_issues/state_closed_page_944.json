[{"number": 25116, "title": "XLA JIT compiler fails on MLPerf Transformer reference model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo, just enabled XLA JIT compilation.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nNVIDIA container based on TF 1.12\r\n\r\n- TensorFlow version (use command below):\r\n1.12\r\n\r\n- Python version:\r\n3.5.2\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10.0.130\r\n\r\n- GPU model and memory:\r\nV100 32GB\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nProgram exits with the following error message:\r\n2019-01-22 19:29:40.093032: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at retval_op.cc:70 : Internal: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.\r\nError: Unsupported type for iota\r\n2019-01-22 19:29:40.094015: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at xla_ops.cc:408 : Internal: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.\r\nError: Unsupported type for iota\r\n\t [[{{node model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal}} = _Retval[T=DT_FLOAT, index=9](model/Transformer/decode/decoder_self_attention_bias/mul)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.\r\nError: Unsupported type for iota\r\n\t [[{{node model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal}} = _Retval[T=DT_FLOAT, index=9](model/Transformer/decode/decoder_self_attention_bias/mul)]]\r\n\t [[{{node cluster_6_1/xla_compile}} = _XlaCompile[Nresources=0, Targs=[DT_INT64, DT_INT32, DT_FLOAT, DT_INT64, DT_INT64, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tconstants=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], function=cluster_6[_XlaCompiledKernel=true, _XlaNumConstantArgs=14, _XlaNumResourceArgs=0], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model/get_train_op/Equal/y, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/Fill, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/mod, model/get_train_op/gradients/model/Transformer/encode/embedding_shared_weights/embedding_1/mul_grad/Shape_1, model/loss/smoothing_cross_entropy/softmax_cross_entropy_with_logits/concat/values_0, model/Transformer/encode/embedding_shared_weights/embedding_1/ExpandDims/dim, model/Transformer/decode/decoder_self_attention_bias/Reshape/shape/0, model/Transformer/decode/shift_targets/Pad/paddings, model/Transformer/decode/shift_targets/strided_slice/stack, model/Transformer/decode/shift_targets/strided_slice/stack_1, model/Transformer/decode/shift_targets/strided_slice/stack_2, model/get_train_op/gradients/model/Transformer/encode/dropout/div_grad/Shape_1, model/get_train_op/gradients/model/Transformer/encode/encoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, global_step/read, model/get_train_op/AssignAdd/_4383, model/Transformer/embedding_shared_weights/embedding_and_softmax/weights/read, IteratorGetNext/_4385, IteratorGetNext/_4387, model/loss/pad_to_same_length/Shape_1/_4389, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read)]]\r\n\t [[{{node model/Transformer/decode/decoder_stack/layer_4/ffn/feed_foward_network/filter_layer/Tensordot/Shape/declustered/_4241}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3582_...eclustered\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"transformer/transformer_main.py\", line 446, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"transformer/transformer_main.py\", line 356, in main\r\n    FLAGS.bleu_threshold)\r\n  File \"transformer/transformer_main.py\", line 274, in train_schedule\r\n    estimator.train(dataset.train_input_fn, steps=single_iteration_train_steps)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1241, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1471, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1156, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1255, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1240, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1312, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Error evaluating model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal input 0 as a compile-time constant.\r\nError: Unsupported type for iota\r\n\t [[{{node model_transformer_decode_decoder_self_attention_bias_mul_0_retval_RetVal}} = _Retval[T=DT_FLOAT, index=9](model/Transformer/decode/decoder_self_attention_bias/mul)]]\r\n\t [[{{node cluster_6_1/xla_compile}} = _XlaCompile[Nresources=0, Targs=[DT_INT64, DT_INT32, DT_FLOAT, DT_INT64, DT_INT64, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tconstants=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], function=cluster_6[_XlaCompiledKernel=true, _XlaNumConstantArgs=14, _XlaNumResourceArgs=0], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model/get_train_op/Equal/y, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/Fill, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/encdec_attention/layer_normalization/Mean_1_grad/mod, model/get_train_op/gradients/model/Transformer/encode/embedding_shared_weights/embedding_1/mul_grad/Shape_1, model/loss/smoothing_cross_entropy/softmax_cross_entropy_with_logits/concat/values_0, model/Transformer/encode/embedding_shared_weights/embedding_1/ExpandDims/dim, model/Transformer/decode/decoder_self_attention_bias/Reshape/shape/0, model/Transformer/decode/shift_targets/Pad/paddings, model/Transformer/decode/shift_targets/strided_slice/stack, model/Transformer/decode/shift_targets/strided_slice/stack_1, model/Transformer/decode/shift_targets/strided_slice/stack_2, model/get_train_op/gradients/model/Transformer/encode/dropout/div_grad/Shape_1, model/get_train_op/gradients/model/Transformer/encode/encoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, model/get_train_op/gradients/model/Transformer/decode/decoder_stack/layer_0/self_attention/layer_normalization/add_grad/Shape_1, global_step/read, model/get_train_op/AssignAdd/_4383, model/Transformer/embedding_shared_weights/embedding_and_softmax/weights/read, IteratorGetNext/_4385, IteratorGetNext/_4387, model/loss/pad_to_same_length/Shape_1/_4389, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/encoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_scale/read, model/Transformer/decoder_stack/layer_0/self_attention/layer_normalization/layer_norm_bias/read)]]\r\n\t [[{{node model/Transformer/decode/decoder_stack/layer_4/ffn/feed_foward_network/filter_layer/Tensordot/Shape/declustered/_4241}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3582_...eclustered\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n\r\n**Describe the expected behavior**\r\nProgram runs normally.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n1. Clone the mlperf reference from https://github.com/mlperf/training\r\n2. Pass config object to estimator.train that turns global jit compilation on. This is how I did it:\r\nInsert following at line 345 in transformer/transformer_main.py\r\n  config = tf.ConfigProto()\r\n  config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n  run_config = tf.estimator.RunConfig(\r\n      model_dir=FLAGS.model_dir,\r\n      session_config=config)\r\nAfter inserting the above, change line 350 to include run_config:\r\n  estimator = tf.estimator.Estimator(\r\n      model_fn=model_fn, model_dir=FLAGS.model_dir, config=run_config,\r\n      params=params)\r\n3. Change the path /research/transformer to wherever you cloned transformer. This needs to be done in scripts run_preprocessing.sh and run_training.sh\r\n4. Start ./run_and_time.sh. This will download and prepare the training data, which takes a long time. The second time, you can launch ./run_training.sh 1 25 directly in order to avoid downloading the training data \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["cc @jlebar @sanjoy ", "cc @thomasjoerg ", "I have quickly looked into this, and it seems the problem is that this is running with an older version of tensorflow where we didn't support F16 and BF16 in the HloEvaluator. This was added in https://github.com/tensorflow/tensorflow/commit/4143d8d30b1f7d2737426c8c181c88bcd8dba5d5#diff-a685f07f7cd6e728595ee0a72055d248\r\nAs you can see, in that change, the error message was also changed to include the type which is not supported, so now you would actually see why it fails instead of just getting \"Unsupported type for iota\".", "This issue appears to have been fixed, Transformer runs fine with XLA enabled in tf-nightly-gpu. I am closing the issue."]}, {"number": 25115, "title": "[XLA] Make BN variance calc numerically stable", "body": "- Batchnorm variance was being computed (and tested) using the equation\r\n  Var[X] = E[X^2] - E[X]^2, which is numerically unstable for large\r\n  mean:variance ratios.\r\n- This commit changes the code (and test) to use the numerically stable\r\n  Var[X] = E[(X - E[X])^2], and modifies the test to ensure that\r\n  numerical stability is a requirement.\r\n\r\nNote that this is a 2-pass algorithm. I plan to look into implementing a numerically stable 1-pass algorithm at a later time.", "comments": ["This formulation is only more numerically stable if E(X) is far from zero. If E(X) is closer to zero computing \r\nvariance = sum(X^2)-(sum(X)^2) is more numerically stable due to fewer rounding steps. The parallel reduction is very performance critical to all XLA back-ends and I cannot accept this patch without an option to the pass that defaults to enabling the parallel reduction.", "In addition to Blake's concern, I expect this change would have a significant negative performance impact on the GPU backend.\r\n\r\nRight now we compute the reductions sum(x) and sum(x^2) in parallel.  With this patch AIUI we'd no longer be able to do this, because we have to do the two reductions in series.", "> This formulation is only more numerically stable if E(X) is far from zero. If E(X) is closer to zero computing variance = sum(X^2)-(sum(X)^2) is more numerically stable due to fewer rounding steps.\r\n\r\nWhile this may technically be true, in practice it appears to me to be inconsequential because the closer E(X) is to zero, the smaller the relative error can be. It's not obvious to me how to formulate a test case where this results in more than O(1e-7) relative error.\r\n\r\nIn contrast, a large mean:variance ratio can cause the current one-pass algorithm to fail catastrophically. E.g., a ratio of 1e4 can result in a relative error of O(1) or more.\r\n\r\n> The parallel reduction is very performance critical to all XLA back-ends and I cannot accept this patch without an option to the pass that defaults to enabling the parallel reduction.\r\n\r\n> I expect this change would have a significant negative performance impact on the GPU backend.\r\n\r\nThese are indeed serious concerns. My thinking was that fixing a potential failure case was more important than the potential perf regression, but if the regression is likely to be significant then it may be best to look into an alternative one-pass algorithm now rather than later.\r\n\r\nI'm aware of two single-pass algorithms that don't suffer from catastrophic precision loss:\r\n\r\n1. [One-pass shifted](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data): Var[X] = E[(X - K)^2] - (E[X] - K)^2, where K is an approximation to the mean (typically the value of the first element of X).\r\n2. [Welford's algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm). This requires a custom reduction operator operating on 3 variables. CUDNN uses this in many cases*.\r\n\r\nI'd appreciate your feedback on potentially implementing one of these options instead.\r\n\r\n*I should note that CUDNN does not make any guarantees about which algorithms it uses in this context.", "> One-pass shifted: Var[X] = E[(X - K)^2] - (E[X] - K)^2, where K is an approximation to the mean (typically the value of the first element of X).\r\n\r\nThis looks very simple to implement, and I don't expect much of a performance cost, at least on CPU/GPU.\r\n\r\n> Welford's algorithm. This requires a custom reduction operator operating on 3 variables. CUDNN uses this in many cases*.\r\n\r\nTo check my understanding, the notion is that we split the input up into N partitions, with each thread/warp/block processing one partition.  The thread/warp/block computes mean and variance using a stable algorithm, e.g. the two-pass algorithm.  Then we combine as described in that link.\r\n\r\nI think XLA can actually represent this using a fusion with one reduction feeding into another.  This is something that the GPU backend should actually be able to codegen today, but the GPU backend will never actually *generate* this fusion.  And who knows about TPUs.  So this whole approach seems much more complicated.\r\n\r\nSeems to me we should try option (1) and see if it's good enough?", "To clarify the Welford algo, here is a simple Numpy implementation:\r\n```\r\ndef welford_merge(a, b):\r\n    Na, Ma, M2a = a\r\n    Nb, Mb, M2b = b\r\n    N = Na + Nb\r\n    M = (Ma * Na + Mb * Nb) / N\r\n    M2 = M2a + M2b + np.square(Mb - Ma) * Na * Nb / N\r\n    result = (N, M, M2)\r\n    return result\r\n\r\ndef welford_mean_variance(x):\r\n    Ndata = np.ones_like(x)\r\n    M2data = np.zeros_like(x)\r\n    N, M, M2 = reduce(welford_merge, zip(Ndata, x, M2data))\r\n    V = M2 / N # or (N - 1) for sample variance\r\n    assert(N == x.size)\r\n    return M, V\r\n```\r\nCould the reduction be implemented using a custom computation for welford_merge?", "Ah, I see.\r\n\r\nI think one could implement this reduction in XLA:GPU using a compare-and-swap loop.  Like, if you wrote the HLO code for this, it should work.  I suspect it would not be fast, although who knows.", "For a mean to standard deviation ratio less than 1 the current algorithm is the best. And the error does not exceed 10^-5 until a mean  to standard deviation ratio of of 10^6.\r\n\r\nhttps://dl.acm.org/citation.cfm?id=3221269.3223036\r\n\r\nThe cases where catastrophic precision loss will happen in the current batchnorm will not produce a variance of zero which will inevitably cause a NaN. Further keeping multiple accumulators largely avoids this cancellation\r\n\r\nHowever using the two pass approach, the shifted approach can produce very small variances which when applied can create excessively large numbers. which can create big problems. I have observed this behavior on multiple occasions when a model other used unfused batchnorm instead of fused batchnorm.\r\n\r\nI think the misconception about the goal of batchnorm is really the issue, the goal is not to get the best value of mean and variance, It is to regularize a the activations of a network to enable faster training. Since the operation is linear, the main goal is achieving centering", "WRT implementing Welford in XLA, what seems slow is the fact that we have to do a compare-and-swap loop.  (Also, actually, a CAS loop seems tricky because the thing we're CAS'ing on is larger than the atomic size on the chip.)\r\n\r\nOTOH I'd expect a deterministic tree reduction (i.e. a reduction that doesn't use atomics) should work fine, if we implemented that.  Like, I don't see us having to implement anything custom just for this.", "> For a mean to standard deviation ratio less than 1 the current algorithm is the best. And the error does not exceed 10^-5 until a mean to standard deviation ratio of of 10^6.\r\n\r\nThat result is for doubles not floats. Quote from your reference: \"All evaluated implementations use double precision internally\".\r\n\r\nFor floats, it is contradicted strongly by the modified test contained in this PR, here: https://github.com/tensorflow/tensorflow/pull/25115/files#diff-5f84797641cee51dc56ef1fe45b058bbR122\r\n(it's even more clear if the reference calculation is fixed to use either 2-pass or float64).\r\nUsing a mean:variance ratio of 1e4 causes the current implementation to produce a variance of **-16** instead of 1.\r\n\r\nThe reference also states:\r\n\"In particular, we demonstrate that the popular textbook equation E[X^2]-E[X]^2 should not be used for computing the variance. Not only has this version a low precision under certain -- not uncommon -- conditions, but there also exist alternatives with much better precision at little additional runtime cost.\"\r\n\r\n> The cases where catastrophic precision loss will happen in the current batchnorm will not produce a variance of zero which will inevitably cause a NaN.\r\n\r\nHowever, as seen in the test, the current implementation can produce a _negative_ variance, which immediately results in NaNs (due to the sqrt).\r\n\r\n> However using the two pass approach, the shifted approach can produce very small variances which when applied can create excessively large numbers. which can create big problems. I have observed this behavior on multiple occasions\r\n\r\nI'm not sure I follow the cause of this issue, but I'm interested to hear more about it (especially if it crops up in real use-cases).\r\n\r\n> the goal is not to get the best value of mean and variance\r\n\r\nTrue, but the variance must still not be catastrophically wrong. (And in general I'd argue that that's really just deep learning being good at overlooking numerical issues).\r\n\r\n> I think one could implement this reduction in XLA:GPU using a compare-and-swap loop.\r\n\r\nI might have been optimistic in my assessment of implementing Welford in XLA; that sounds trickier than I thought.", "Unassigning people because currently this PR is not waiting for review.", "I've changed the algorithm to use the one-pass shifted method, which should retain both numerical stability and performance.", "Can we make this version an option to this pass that is disabled by default. If so I am fine with the patch otherwise I have measured performance regressions I have no idea what kind of quality impact this will have. I fear many internal users would have to change hyperparameters and golden references.", "@benbarsdell thanks for your patience with us on this one.\r\n\r\nWe have some questions for you about this, but the most important one that we'd like to understand is, what is the motivation for this change?  Are you seeing this hurt in practice, on real models with real data?  If so, which ones, and do you have steps to reproduce?\r\n\r\nI ask for a few reasons.\r\n\r\n 1. Blake tells me that we have models that will train to *worse* accuracy after this change.\r\n\r\n    I admit I'm not as conversant in the mathematical details as he is, but my understanding from him is that in general, weights tend to have a mean close to 0.  Under such circumstances, this algorithm can be worse than what we currently do.\r\n\r\n    Indeed, the argument goes, if models *didn't* usually have mean close to 0, then reduced-precision training simply wouldn't work.\r\n\r\n 2. This algorithm isn't easy to implement on all platforms.  Indeed even on GPU, one of the things we've been working with you all towards is batchnorm + conv fusion.  In the simple case, we'd compute sum(x) and sum(x^2) in parallel with outputting the results of the conv.  This becomes significantly more complicated if we wanted to implement this algorithm here.\r\n\r\n3. I'm leery of a `--better-batchnorm` flag if we can avoid it; no user is going to understand what it means, and it's just going to be another knob that users have to twiddle.\r\n\r\n4. I wonder if simply having more parallel accumulators on GPU would solve your problem.  That is, right now we atomicAdd into a single float, but we could instead do a multistep process where we first reduce into multiple floats and then combine them.  In the limit, this becomes a deterministic reduction algorithm, which is something I know folks have been interested in having anyway.\r\n\r\n5. Blake suggests that folks can get the behavior of this patch *today* in TF by disabling fused batchnorm.\r\n\r\nWDYT?\r\n\r\nSorry again for leaving you hanging.", "Thank you for the detailed feedback on this, it's very useful.\r\n\r\nI made an oversight in the computation of the mean in the latest code, which probably explains the reduced accuracy you're observing. It's easy to fix, but that still leaves the performance concerns you raised.\r\n\r\nI'm going to try one more approach to see if the elusive trifecta of accuracy, robustness, and performance can be achieved. I expect to get back to you on it some time next week.", "Abandoning this due to unforeseen complications. The last thing I tried was the Welford algorithm, but immaturity of the codegen for variadic reductions prevented this from being a viable option at this stage.\r\n\r\nThe lack of a motivating example means this is not worth further investigation in the short term. Indeed it seems that networks involving BN likely have small mean:variance ratios by construction, and it is difficult to imagine a realistic scenario in which a model will hit this issue during training."]}, {"number": 25114, "title": "Fix import of tools/bazel.rc for Bazel 0.19.0 or later", "body": "This was removed in https://github.com/tensorflow/tensorflow/pull/22964/files which causes build errors with bazel 0.20.0 because tools/bazel.rc is ignored so the grpc_no_ares=true flag doesn't get set and grpc is compiled with c-ares support:\r\n\r\n```\r\nERROR: /user_data/.tmp/cache.srbo/bazel/_bazel_srbo/96431eb3bb8da25a1741c1c1ac91e19b/external/grpc/BUILD:1332:1: C++ compilation of rule '@grpc//:grpc_resolver_dns_ares' failed (Exit 1) gcc failed: error executing command /builds/gcc/4.9.4/12a8bec7dd/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 56 argument(s) skipped) \r\n\r\n  \r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox \r\n\r\nexternal/grpc/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc:30:18: fatal error: ares.h: No such file or directory \r\n\r\n#include <ares.h> \r\n\r\n                  ^ \r\n\r\ncompilation terminated. \r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build \r\n\r\nUse --verbose_failures to see the command lines of failed build steps. \r\n\r\nINFO: Elapsed time: 98.819s, Critical Path: 17.24s \r\n\r\nINFO: 750 processes: 2 local, 748 processwrapper-sandbox. \r\n\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nRe-adding this line fixes the issue.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@SimonBoorer Can you please sign the cla to move ahead with this PR? Thanks!", "tools/bazel.rc doesn't exist any more (in the master branch). It was moved to .bazelrc in the workspace root so that bazel will automatically pick it up. ", "What version of TF is this for? as @wdirons said, tools/bazel.rc is gone and moved to .bazelrc. The minimum version of bazel is also set to the right versions so everything is fine. TF1.13 also has this fixed. If this is for an older version it should be for those branches but i dont think there are going to be any more releases of 1.12 either.", "There is going to be a 1.12.1 at least, I'll prepare it in a few weeks", "@SimonBoorer Please sign the cla to move ahead with this PR. Thanks!", "This should not be merged into master, as there is no issue there", "@mihaimaruseac what should be the next step here? Should we close this PR?", "I think it should be rebased on the branches which have the issue (probably at least v1.12). Or closed and reopened if needed"]}, {"number": 25113, "title": "Can't access resource variable using GetResourceFromContext in a custom op, probably because of binary incompatibility", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- GCC/Compiler version (for custom op): g++ 4.2.1 Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n\r\n**Describe the problem**\r\n\r\nWhen I try to use `GetResourceFromContext` with `T` being `Var` from a custom op, I get the following error ```InvalidArgumentError (see above for traceback): Trying to access resource using the wrong type. Expected N10tensorflow3VarE got N10tensorflow3VarE```\r\n\r\nSo it seems like the types match up, but for whatever reason the `std::type_index`s generated from them don't? My hypothesis is it's because using `std::type_index` across a shared library is implementation defined behavior and the compiler does not produce the same `std::type_index` for the same type in both the custom op library and `libtensorflow_framework`.\r\n\r\nFor context, I'm trying to get access to the resource's underlying tensor so that I can modify it.\r\n", "comments": ["@liamuk Could you provide more details about the error and context? Thanks!", "This bug is no longer relevant to me, and I don't have the original code that triggered it.\r\n\r\nYou should be able to reproduce it, though, if you create a c++ custom op that takes an argument of type `resource` like how `ResourceApplyGradientDescent` in `ops/training_ops.cc` does, and then attempt to use `GetResourceFromContext` defined in `framework/resource_mgr.h` to access it from the op.\r\n\r\nThe context is trying to write a custom op that manipulates the buffer underlying a resource variable.", "Is this bug still an issue?", "Not an issue for me personally bc I\u2019m no longer working with custom ops.\r\n\r\nNot sure if it still exists in Tensorflow.", "@liamuk ,\r\nCan you confirm if we can proceed for closure?Thanks!", "I don't mind if you close it.", "I am closing this issue. If the issue persists with latest TF versions, please feel free to open the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25113\">No</a>\n", "I am facing the same issue when trying to compile a custom op on Mac and on Windows (it works fine on Linux though), using Tensorflow 2.3. When running on Mac, I get the same error, i.e.:\r\n\r\n```Trying to access resource using the wrong type. Expected N10tensorflow3VarE got N10tensorflow3VarE [Op:CustomOp]```\r\n\r\nOn Windows, the error is similar:\r\n\r\n```Trying to access resource using the wrong type. Expected class tensorflow::Var got class tensorflow::Var [Op:CustomOp]```\r\n\r\nOn Linux, there is no error and the op works as expected.\r\n\r\nI think this issue is due to the use of an address of a static variable ```hash_bit``` in ```TypeIndex::Make<T>``` as a hash. When the shared library with the custom is compiled, it will have its own copy of ```TypeIndex::Make<Var>::hash_bit```, so the hash codes for the same type (e.g. ```Var```) will be different. I think it works on Linux because a STB_GNU_UNIQUE symbol is generated for ```hash_bit```, so that there is only one definition of it (per type) in the whole process. Mac and Windows don't have this feature though.\r\n\r\nIs it possible to rewrite TypeIndex in a way that will make it work across shared libraries on all platforms? Or perhaps various instantiations of ```TypeIndex::Make<T>``` could be exported from tensorflow framework library?\r\n\r\nFor some reason I cannot attach a zipped reproducer to this comment, so I will try pasting the code in a reply below.", "Reproducer: a simple custom op to set a value of a variable.\r\n```\r\n#define EIGEN_USE_THREADS\r\n\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/resource_mgr.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/kernels/training_op_helpers.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\n\r\ntemplate<typename Device, typename T>\r\nclass CustomOp: public OpKernel {\r\n public:\r\n  explicit CustomOp(OpKernelConstruction* c) : OpKernel(c) {\r\n    OP_REQUIRES_OK(c, c->GetAttr(\"use_locking\", &use_exclusive_lock_));\r\n  }\r\n\r\n  void Compute(OpKernelContext* c) override {\r\n    core::RefCountPtr<Var> v;\r\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\r\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\r\n    const Tensor& value = c->input(1);\r\n    if (use_exclusive_lock_) {\r\n      mutex_lock lock(*v->mu());\r\n      DoCompute(c, *v->tensor(), value);\r\n    } else {\r\n      tf_shared_lock lock(*v->mu());\r\n      DoCompute(c, *v->tensor(), value);\r\n    }\r\n  }\r\n\r\n private:\r\n  bool use_exclusive_lock_;\r\n\r\n  void DoCompute(OpKernelContext* c, Tensor& v, const Tensor& value) {\r\n    OP_REQUIRES(c, v.IsInitialized(),\r\n                errors::FailedPrecondition(\"Null ref for var\"));\r\n    OP_REQUIRES(c, TensorShapeUtils::IsVector(v.shape()),\r\n                errors::InvalidArgument(\"var must be 1-D, got shape \",\r\n                                        v.shape().DebugString()));\r\n    OP_REQUIRES(c, TensorShapeUtils::IsScalar(value.shape()),\r\n                errors::InvalidArgument(\"value must be a scalar, got shape \",\r\n                                        value.shape().DebugString()));\r\n    if (!c->status().ok()) return;\r\n\r\n    T* data = v.flat<T>().data();\r\n    T set_to = *value.flat<T>().data(); \r\n    for (size_t i = 0; i < v.NumElements(); ++i) {\r\n      data[i] = set_to;\r\n    }\r\n  }\r\n};\r\n  \r\n\r\n#define REGISTER_CUSTOM_KERNEL(type, dev, name) \\\r\n  REGISTER_KERNEL_BUILDER(Name(name) \\\r\n                              .Device(DEVICE_##dev) \\\r\n                              .HostMemory(\"var\") \\\r\n                              .TypeConstraint<type>(\"T\"), \\\r\n                          CustomOp<dev##Device, type>)\r\n                              \r\n#define REGISTER_CUSTOM_CPU(type) \\\r\n  REGISTER_CUSTOM_KERNEL(type, CPU, \"CustomOp\");\r\n\r\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_CUSTOM_CPU);\r\n\r\n#undef REGISTER_CUSTOM_CPU\r\n#undef REGISTER_CUSTOM_KERNEL\r\n\r\nREGISTER_OP(\"CustomOp\")\r\n    .Input(\"var: resource\")\r\n    .Input(\"value: T\")\r\n    .Attr(\"T: type\")\r\n    .Attr(\"use_locking: bool = true\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      return Status::OK();\r\n    });\r\n```", "Reproducer: build scripts and Python test.\r\nLinux:\r\n```\r\n#!/bin/sh\r\nTF_CFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\n\r\ng++ -shared -O2 -std=c++11 -fPIC -o custom_op.so custom_op.cpp ${TF_CFLAGS[@]} ${TF_LFLAGS[@]}\r\n```\r\nMac:\r\n```\r\n#!/bin/sh\r\nTF_CFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\n\r\ng++ -shared -O2 -std=c++11 -fPIC -o custom_op.dylib custom_op.cpp ${TF_CFLAGS[@]} ${TF_LFLAGS[@]}\r\n```\r\nWindows:\r\n```\r\nset TF_CFLAGS=/Ic:\\\\Python\\\\lib\\\\site-packages\\\\tensorflow\\\\include\r\nset TF_LFLAGS=/LIBPATH:c:\\\\Python\\\\lib\\\\site-packages\\\\tensorflow\\\\python _pywrap_tensorflow_internal.lib\r\n\r\ncl /LD /O2 /EHsc /MD /DNOMINMAX /wd4624 /Fecustom_op.dll custom_op.cpp %TF_CFLAGS% /link %TF_LFLAGS%\r\n```\r\nPython test:\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nif sys.platform == 'win32':\r\n    custom_module = tf.load_op_library('custom_op.dll')\r\nelif sys.platform == 'darwin':\r\n    custom_module = tf.load_op_library('custom_op.dylib')\r\nelse:\r\n    custom_module = tf.load_op_library('custom_op.so')\r\n\r\ndef custom_op(var, value, use_locking=True, name=None):\r\n    custom_module.custom_op(var.handle, value, use_locking=use_locking, name=name)\r\n\r\nv = tf.Variable([1.0, 2.0, 3.0])\r\nprint(v)\r\ncustom_op(v, 2.22)\r\nprint(v)\r\n```", "Since the is no activity here, I submitted a new bug report #44209."]}, {"number": 25112, "title": "[Tensorflow GitHub] Do Inception-v3 Top1 Results match with the paper?", "body": "This issue is to inquire about the top-1 rates of the inception-v3 results in the paper and available on the Tensorflow Github.\r\n\r\nCurrently, I am using the inception-v3 model [[link](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz) - inception-2015-12-05.tgz] from the official Tensorflow Github: [https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py](https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py)\r\n\r\nWhen I calculate the top-1 accuracy rate for the 50K validation images in imageNet, I get 77.6%. However, I cannot find any reference to this result neither in the paper nor somewhere else.\r\n\r\nTherefore, can you please help me get answers to the following question:\r\n**What are the training specs of inception-2015-12-05.tgz? Single crop or basic or what? Are there references to its top1 rate results?** \r\n\r\nPlease advise on my question as I believe this is a documentation issue that should be addressed. \r\n\r\nPlease note that I am aware of this link:\r\n[https://github.com/tensorflow/models/tree/master/research/slim]( https://github.com/tensorflow/models/tree/master/research/slim)\r\nHowever, I am not also able to match the results from the updated link (inception_v3_2016_08_28.tar.gz - 78%) with the paper\r\n\r\nI have seen a lot of links report different accuracies for the inception-v3 model - Please see some below:\r\n[https://ai.googleblog.com/2016/08/improving-inception-and-image.html](https://ai.googleblog.com/2016/08/improving-inception-and-image.html)  [78% top1]\r\n[https://github.com/tensorflow/models/tree/master/research/inception](https://github.com/tensorflow/models/tree/master/research/inception)  [78.8% top1]\r\n[https://arxiv.org/pdf/1512.00567v1.pdf](https://arxiv.org/pdf/1512.00567v1.pdf) [76.6% top1]\r\nhttps://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c [76.6% top1]\r\n[](https://github.com/tensorflow/models/tree/master/research/slim#downloading-and-converting-to-tfrecord-format\r\n) [78% top1]", "comments": ["I  think what you are looking for is found in Table 2 of this paper: https://arxiv.org/abs/1512.00567 \r\n", "Thank you @malzantot for your response!\r\n\r\nNone of the results in the table are matching to 77.6% I produced on 50k imagNet validation set using [_inception-2015-12-05.tgz_] http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\n\r\nAny clue? \r\nAlso, do you know the training specs of _inception-2015-12-05.tgz_?", "Any updates on this topic? \r\n@ymodak or anyone?", "@nealwu Can you PTAL? Thanks!", "Are you still facing this? If so perhaps @tfboyd would know?", "This is not going to be a satisfying answer but it will be honest. I think you have two questions.  1) is there a model that exactly matches the paper and 2) your result with the tutorial does not match what is stated in the table, e.g. it ways 78% for 2016 and you are not getting that with the tutorial.\r\n\r\n1) is there a model that matches the paper?\r\nI do not know the inception v3 number very well. For say Resnet50, I have run it likely 1000 times and spent millions of dollars running it over and over and thus I know the variations and quirks.  The slim version is often the most accurate but their goal was not a perfect copy of the paper and they may exceed the accuracy of the paper due to whatever tweaks they may have made.  Unfortunately I do not think the SLIM authors do not publish the exact command used to train the model, e.g. what optimizer batch-size or whatever.\r\n\r\nTo draw a parallel to something I do know.  The ResNet50 paper has accuracy around 75% top_1 at maybe 120 epochs if my memory is correct, but no one runs that version they run what I have called V1.5 (which is not V2) and it trains to 76.0 to 76.5 from run to run at 90 epochs and nearly the same at 81. \r\n\r\nHaving dealt with this in trying to setup performance tests it is almost impossible to find models that are identical to the paper or even identical to each other regardless of what they say the model is.   \r\n\r\nThis is a lot of words to say I doubt you will find out and the best way would be to try and get the attention of the people that trained the SLIM version. \r\n\r\n2) The tutorial is not giving you the accuracy from the table.\r\nYour best bet is to open a ticket in models with the repro.  I have doubts it will get attention so I do not want to use your time.  This is a random and likely incorrect guess, but there are some images that a blacklisted for imagenet in the validation set and maybe they excluded those.  This is a long-shot, but an amusing thing to know.  I believe there are some images in imagenet labeled wrong in the validation set and some contests I think exclude them.  I do not think the difference is .4%, I think maybe .1% in the case of ResNet.  \r\n\r\nI do not like not providing wishy washy answers that are not direct; but I think this is a good as you will get.  I hope this leads you toward something.  Closing due to I think this is the best you will get without some luck asking SLIM people in the tensorflow/models repo or someone digging into why the tutorial is reporting lower top_1 than the table is saying it should.\r\n\r\n"]}, {"number": 25111, "title": "Trying to build tensorflow from source and fails", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): downloaded from github\r\n- Python version: 3.x\r\n- Bazel version (if compiling from source): .18\r\n- GCC/Compiler version (if compiling from source): 6\r\n- CUDA/cuDNN version: 9/7\r\n- GPU model and memory: Quadro K5000\r\n\r\nThis is my config and error message below, please advise what is a possible fix?\r\n--\r\n./configure\r\nWARNING: Processed legacy workspace file /home/robotlab/Desktop/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/robotlab/.cache/bazel/_bazel_robotlab/install/855b86426ac3a2fcf0dcf5a6fb99b79a/_embedded_binaries/A-server.jar) to field java.nio.Buffer.address\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.18.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /home/robotlab/.conda/envs/keras_gpu/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/robotlab/.conda/envs/keras_gpu/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/robotlab/.conda/envs/keras_gpu/lib/python3.5/site-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: \r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: 2.2.12\r\n\r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: \r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.0]\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/x86_64-linux-gnu-gcc-6]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: n\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n(keras_gpu) robotlab@robotlab:~/Desktop/tensorflow$ bazel build --config=opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Processed legacy workspace file /home/robotlab/Desktop/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/BUILD:1943:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/robotlab/.cache/bazel/_bazel_robotlab/0d7b53c20c90b46f9c06813fc5af2afe/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/robotlab/Desktop/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/robotlab/Desktop/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (303 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nERROR: /home/robotlab/Desktop/tensorflow/tensorflow/contrib/lite/BUILD:62:1: C++ compilation of rule '//tensorflow/contrib/lite:context' failed (Exit 1)\r\nx86_64-linux-gnu-gcc-6: error: n: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 141.101s, Critical Path: 58.27s\r\nINFO: 243 processes: 243 local.\r\nFAILED: Build did NOT complete successfully\r\n(keras_gpu) robotlab@robotlab:~/Desktop/tensorflow$ bazel build --config=opt --config=cuda --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n--\r\nCheers. \r\n\r\n", "comments": ["TensorFlow 1.6 and above require cuda compute capability of 3.5 and above for GPU support. The cuda compute capability for your config 3.0 (correct me if I am wrong), that is plausible reason for your build faiilure. However users in the past have managed to build with cuda compute capability 3.0 from sources. You can take a look at [similar attempt](https://github.com/tensorflow/tensorflow/issues/24126) to know more.", "Yes, I have tried that before and I was well aware of this issue going in... the error points to a file named N and I dont know what that is supposed to be. ", "@gunan Can you PTAL? thanks!", "I actually got a fix. I used NVCC, with Cuda 9.0, CuDNN 7.0, Bazel .20.0, gcc 6, and tf 1.12. If anyone wants a download for CUDA computing 3.0, for python 3.7 on linux, here is the built file. Hope that helps!\r\n\r\nhttps://github.com/rd16395p/tensorflowbuild"]}, {"number": 25110, "title": "[TF Lite] Add support for dilated convolution with valid padding.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\nuname -a\r\nLinux archlinux 4.19.2-arch1-1-ARCH #1 SMP PREEMPT Tue Nov 13 21:16:19 UTC 2018 x86_64 GNU/Linux\r\n```\r\n- TensorFlow installed from (source or binary): \r\n```\r\npip install tensorflow\r\n```\r\n- TensorFlow version (or github SHA if from source): \r\n```\r\n1.12.0\r\n```\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nRuntimeError: TOCO failed see console for info.\r\n2019-01-22 11:50:43.257264: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)\r\n2019-01-22 11:50:43.257374: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)\r\n2019-01-22 11:50:43.257409: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:217] Replaced sub-network with Dilated Conv2D op outputting \"conv2d/Conv2D\".\r\n2019-01-22 11:50:43.257458: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991] Check failed: height_with_paddings % block_height == 0 (12 vs. 0)\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible: [keras_model.zip](https://github.com/tensorflow/tensorflow/files/2783805/keras_model.zip)\r\n\r\n**Any other info / logs**\r\n\r\nRunning this in jupyter notebook:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.keras.backend.clear_session()\r\nprint(tf.__version__)\r\nprint(tf.keras.__version__)\r\nkeras_model_savepath = '/tmp/keras_model.h5'\r\n\r\nx_input = x = tf.keras.layers.Input(shape=(725, 725, 1))\r\nx = tf.keras.layers.Conv2D(6, 3, padding='valid', dilation_rate=25)(x)\r\nx = tf.keras.layers.Conv2D(6, 3, padding='valid', dilation_rate=19)(x)\r\nkeras_model = tf.keras.models.Model(x_input, x)\r\nprint(keras_model.summary())\r\npred = keras_model.predict(np.ones([1,725,725,1]))\r\nprint(pred[0,0:5,0,0])\r\nkeras_model.save(keras_model_savepath)\r\ntoco = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_model_savepath)\r\nmodel_tflite = toco.convert()\r\n```\r\ngives:\r\n```python\r\n1.12.0\r\n2.1.6-tf\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 725, 725, 1)       0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 675, 675, 6)       60        \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 637, 637, 6)       330       \r\n=================================================================\r\nTotal params: 390\r\nTrainable params: 390\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n[-1.9924886 -1.9924886 -1.9924886 -1.9924886 -1.9924886]\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nINFO:tensorflow:Froze 4 variables.\r\nINFO:tensorflow:Converted 4 variables to const ops.\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-48-0a572848505b> in <module>\r\n     15 keras_model.save(keras_model_savepath)\r\n     16 toco = tf.contrib.lite.TFLiteConverter.from_keras_model_file(keras_model_savepath)\r\n---> 17 model_tflite = toco.convert()\r\n\r\n~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/contrib/lite/python/lite.py in convert(self)\r\n    451           input_tensors=self._input_tensors,\r\n    452           output_tensors=self._output_tensors,\r\n--> 453           **converter_kwargs)\r\n    454     else:\r\n    455       # Graphs without valid tensors cannot be loaded into tf.Session since they\r\n\r\n~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    340   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    341                              toco_flags.SerializeToString(),\r\n--> 342                              input_data.SerializeToString())\r\n    343   return data\r\n    344 \r\n\r\n~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/contrib/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    133     else:\r\n    134       raise RuntimeError(\"TOCO failed see console for info.\\n%s\\n%s\\n\" %\r\n--> 135                          (stdout, stderr))\r\n    136 \r\n    137 \r\n\r\nRuntimeError: TOCO failed see console for info.\r\nb'2019-01-22 11:50:43.257264: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)\\n2019-01-22 11:50:43.257374: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)\\n2019-01-22 11:50:43.257409: I tensorflow/contrib/lite/toco/graph_transformations/identify_dilated_conv.cc:217] Replaced sub-network with Dilated Conv2D op outputting \"conv2d/Conv2D\".\\n2019-01-22 11:50:43.257458: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991] Check failed: height_with_paddings % block_height == 0 (12 vs. 0)\\n'\r\nNone\r\n```", "comments": ["Can you please try [converting the model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md#converting-the-model) with TFLITE_BUILTINS, then with both TFLITE_BUILTINS,SELECT_TF_OPS?", "Hi, thank you for your reply and sorry for the delay. \r\n\r\nThere is no tensorflow.lite module: \r\n```python\r\nimport tensorflow.lite                                                                                                                                                                \r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-3-c77449cadf25> in <module>\r\n----> 1 import tensorflow.lite\r\n\r\nModuleNotFoundError: No module named 'tensorflow.lite'\r\n```\r\nI just have tensorflow.contrib.lite. Also, I don't have tensorflow.contrib.lite.OpsSet, so I don't know how to do: \r\n```python\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\nFYI, I did a `pip install -U tensorflow` to make sure I have the latest version.", "You don't have to import tensorflow.lite separately. It will be imported alongside TF.\r\nYou can install tf nightly version and try converting your model using the link above.\r\n>pip install tf-nightly", "I was not using tf-nightly, that's why.\r\n\r\nSo, running this:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.keras.backend.clear_session()\r\nprint(tf.__version__)\r\nprint(tf.keras.__version__)\r\nkeras_model_savepath = '/tmp/keras_model.h5'\r\n\r\nx_input = x = tf.keras.layers.Input(shape=(725, 725, 1))\r\nx = tf.keras.layers.Conv2D(6, 3, padding='valid', dilation_rate=25)(x)\r\nx = tf.keras.layers.Conv2D(6, 3, padding='valid', dilation_rate=19)(x)\r\nkeras_model = tf.keras.models.Model(x_input, x)\r\nprint(keras_model.summary())\r\npred = keras_model.predict(np.ones([1,725,725,1]))\r\nprint(pred[0,0:5,0,0])\r\nkeras_model.save(keras_model_savepath)\r\ntoco = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_savepath)\r\ntoco.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                   tf.lite.OpsSet.SELECT_TF_OPS]\r\nmodel_tflite = toco.convert()\r\n```\r\n\r\ngives\r\n\r\n```bash\r\n1.13.0-dev20190125\r\n2.2.4-tf\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0128 15:19:21.850813 139835776061568 deprecation.py:506] From /home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 725, 725, 1)]     0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 675, 675, 6)       60        \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 637, 637, 6)       330       \r\n=================================================================\r\nTotal params: 390\r\nTrainable params: 390\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n2019-01-28 15:19:21.924065: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-01-28 15:19:21.944133: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304500000 Hz\r\n2019-01-28 15:19:21.944442: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55c6fc4f8cd0 executing computations on platform Host. Devices:\r\n2019-01-28 15:19:21.944460: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n[0.10286878 0.10286878 0.10286878 0.10286878 0.10286878]\r\nW0128 15:19:22.133749 139835776061568 deprecation.py:506] From /home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:96: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0128 15:19:22.134148 139835776061568 deprecation.py:506] From /home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:96: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0128 15:19:22.229240 139835776061568 hdf5_format.py:225] No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nW0128 15:19:22.230446 139835776061568 deprecation.py:323] From /home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:636: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nW0128 15:19:22.230571 139835776061568 deprecation.py:323] From /home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\n2019-01-28 15:19:22.237138: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-01-28 15:19:22.237224: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-01-28 15:19:22.241364: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node conv2d/kernel/Assign doesn't exist in graph\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-1-b87cda04bd64> in <module>\r\n     17 toco.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n     18                    tf.lite.OpsSet.SELECT_TF_OPS]\r\n---> 19 model_tflite = toco.convert()\r\n\r\n~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    498           input_tensors=self._input_tensors,\r\n    499           output_tensors=self._output_tensors,\r\n--> 500           **converter_kwargs)\r\n    501     else:\r\n    502       result = _toco_convert_graph_def(\r\n\r\n~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    440   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    441                              toco_flags.SerializeToString(),\r\n--> 442                              input_data.SerializeToString())\r\n    443   return data\r\n    444 \r\n\r\n~/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    203       stderr = _try_convert_to_unicode(stderr)\r\n    204       raise ConverterError(\r\n--> 205           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    206   finally:\r\n    207     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-01-28 15:19:23.593030: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 27 arrays (0 quantized)\r\n2019-01-28 15:19:23.593183: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 27 arrays (0 quantized)\r\n2019-01-28 15:19:23.593236: I tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc:220] Replaced sub-network with Dilated Conv2D op outputting \"conv2d/Conv2D\".\r\n2019-01-28 15:19:23.593327: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1302] Check failed: height_with_paddings % block_height == 0 (12 vs. 0)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f72194e3080 (most recent call first):\r\n  File \"/home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/ludovic/.pyenv/versions/3.6.7/envs/tensorflow-1.12/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/ludovic/.pyenv/versions/tensorflow-1.12/bin/toco_from_protos\", line 11 in <module>\r\n\r\n```\r\n\r\nSame error, but I get this error also:\r\n```\r\n2019-01-28 15:19:22.241364: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node conv2d/kernel/Assign doesn't exist in graph\r\n```", "Any updates?", "This should be supported for float models now, can you give it another try?", "@ltrottier,\r\nWith respect to [this comment](https://github.com/tensorflow/tensorflow/issues/25110#issuecomment-660270894), this issue should be resolved. Can you please confirm it so that we can close this issue? Thanks!", "I can confirm that this issue is resolved.\r\n\r\nHowever, I had to use\r\n```\r\ntoco = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n```\r\ninstead of\r\n```\r\nkeras_model.save(keras_model_savepath)\r\ntoco = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_savepath)\r\n```\r\nbecause of a change in the interface of TFLiteConverter:\r\n```\r\nAttributeError: type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'\r\n```\r\n\r\nThank you !"]}, {"number": 25109, "title": "Repetitive build issues inside bazel project!", "body": "tensorflow v.1.12.0\r\nbazel 0.18.0\r\n\r\nrepetitive compilation issue:\r\n```sh\r\nexport PYTHON_BIN_PATH=/usr/bin/python                                                                                                                                                                                                                                                                                                                                                                                                  \r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1                                                                                                                                                                                                                                                                                                                                                                                                    \r\nexport TF_NEED_JEMALLOC=1                                                                                                                                                                                                                                                                                                                                                                                                               \r\nexport TF_NEED_KAFKA=0                                                                                                                                                                                                                                                                                                                                                                                                                  \r\nexport TF_NEED_OPENCL_SYCL=0                                                                                                                                                                                                                                                                                                                                                                                                            \r\nexport TF_NEED_AWS=0                                                                                                                                                                                                                                                                                                                                                                                                                    \r\nexport TF_NEED_GCP=0                                                                                                                                                                                                                                                                                                                                                                                                                    \r\nexport TF_NEED_HDFS=0                                                                                                                                                                                                                                                                                                                                                                                                                   \r\nexport TF_NEED_S3=0                                                                                                                                                                                                                                                                                                                                                                                                                     \r\nexport TF_ENABLE_XLA=1                                                                                                                                                                                                                                                                                                                                                                                                                  \r\nexport TF_NEED_GDR=0                                                                                                                                                                                                                                                                                                                                                                                                                    \r\nexport TF_NEED_VERBS=0                                                                                                                                                                                                                                                                                                                                                                                                                  \r\nexport TF_NEED_OPENCL=0                                                                                                                                                                                                                                                                                                                                                                                                                 \r\nexport TF_NEED_MPI=0                                                                                                                                                                                                                                                                                                                                                                                                                    \r\nexport TF_NEED_TENSORRT=0                                                                                                                                                                                                                                                                                                                                                                                                               \r\nexport TF_NEED_NGRAPH=0                                                                                                                                                                                                                                                                                                                                                                                                                 \r\nexport TF_NEED_IGNITE=0                                                                                                                                                                                                                                                                                                                                                                                                                 \r\nexport TF_NEED_ROCM=0                                                                                                                                                                                                                                                                                                                                                                                                                   \r\nexport TF_SET_ANDROID_WORKSPACE=0                                                                                                                                                                                                                                                                                                                                                                                                       \r\nexport TF_DOWNLOAD_CLANG=0                                                                                                                                                                                                                                                                                                                                                                                                              \r\nexport TF_NCCL_VERSION=2.3                                                                                                                                                                                                                                                                                                                                                                                                              \r\nexport NCCL_INSTALL_PATH=/usr                                                                                                                                                                                                                                                                                                                                                                                                           \r\nexport CC_OPT_FLAGS=\"-march=x86-64\"                                                                                                                                                                                                                                                                                                                                                                                                     \r\nexport TF_NEED_CUDA=0                                                                                                                                                                                                                                                                                                                                                                                                                   \r\n                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\necho \\                                                                                                                                                                                                                                                                                                                                                                                                                                  \r\n    //external:protobuf_headers \\                                                                                                                                                                                                                                                                                                                                                                                                       \r\n    @protobuf_archive//:protobuf_headers \\                                                                                                                                                                                                                                                                                                                                                                                              \r\n    //external:protobuf_clib \\                                                                                                                                                                                                                                                                                                                                                                                                          \r\n    @protobuf_archive//:protoc_lib                                                                                                                                                                                                                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\nbazel build \\                                                                                                                                                                                                                                                                                                                                                                                                                           \r\n    //external:protobuf_headers \\                                                                                                                                                                                                                                                                                                                                                                                                       \r\n    @protobuf_archive//:protobuf_headers \\                                                                                                                                                                                                                                                                                                                                                                                              \r\n    //external:protobuf_clib \\                                                                                                                                                                                                                                                                                                                                                                                                          \r\n    @protobuf_archive//:protoc_lib                                                                                                                                                                                                                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\necho '@snappy//:snappy'                                                                                                                                                                                                                                                                                                                                                                                                                 \r\nbazel build '@snappy//:snappy'                                                                                                                                                                                                                                                                                                                                                                                                          \r\n                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\necho \\                                                                                                                                                                                                                                                                                                                                                                                                                                  \r\n    @grpc://atomic \\                                                                                                                                                                                                                                                                                                                                                                                                                    \r\n    @grpc://grpc \\                                                                                                                                                                                                                                                                                                                                                                                                                      \r\n    @grpc://gpr_base \\                                                                                                                                                                                                                                                                                                                                                                                                                  \r\n    //external:grpc_cpp_plugin                                                                                                                                                                                                                                                                                                                                                                                                          \r\nbazel build \\                                                                                                                                                                                                                                                                                                                                                                                                                           \r\n    @grpc//:atomic \\                                                                                                                                                                                                                                                                                                                                                                                                                    \r\n    @grpc//:grpc \\                                                                                                                                                                                                                                                                                                                                                                                                                      \r\n    @grpc//:gpr_base \\                                                                                                                                                                                                                                                                                                                                                                                                                  \r\n    //external:grpc_cpp_plugin                                                                                                                                                                                                                                                                                                                                                                                                          \r\n                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\necho \\                                                                                                                                                                                                                                                                                                                                                                                                                                  \r\n    //tensorflow:libtensorflow_cc.so                                                                                                                                                                                                                                                                                                                                                                                                \r\nbazel build \\                                                                                                                                                                                                                                                                                                                                                                                                                           \r\n    //tensorflow:libtensorflow_cc.so         \r\n\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\necho \\                                                                                                                                                                                                                                                                                                                                                                                                                                               \r\n    //tensorflow:install_headers                                                                                                                                                                                                                                                                                                                                                                                                        \r\nbazel build \\                                                                                                                                                                                                                                                                                                                                                                                                                                      \r\n    //tensorflow:install_headers                                                                                                                                                                                                                                                                                                                                                                                                                \r\n```\r\n\r\n1. With the above script //external:protobuf_headers target is being built multiple times.\r\n2. What is it the point of such a non-constructive attitude towards builds?\r\n3. How can I make sure that ```bazel build //tensorflow:install_headers``` won't recomile 5K object files after calling ```bazel build //tensorflow:libtensor_cc.so```?\r\n4. Do you recompile 5K object files each time you want to compile some 5 lines example? I'm talking about C++ API of tensorflow and libtensorflow_cc.so shared library. Which is not present on almost of all of the normal linux distributions.\r\n5. Does bazel support verbose printing of checked targets?\r\n```sh\r\nbazel build \\                                                                                                                                                                                                                                                                                                                                                                                                                           \r\n    //external:protobuf_headers \\                                                                                                                                                                                                                                                                                                                                                                                                       \r\n    @protobuf_archive//:protobuf_headers \\                                                                                                                                                                                                                                                                                                                                                                                              \r\n    //external:protobuf_clib \\                                                                                                                                                                                                                                                                                                                                                                                                          \r\n    @protobuf_archive//:protoc_lib  \r\n```\r\nIf the command above will be called second or third time. The output says built is successful, yet no target lists are being printed. Although, ninja won't say anythin as well in this case, probably.", "comments": ["If you think you found a bug in bazel, please open an issue against github.com/bazelbuild/bazel here:\r\nhttps://github.com/bazelbuild/bazel/issues/new"]}, {"number": 25108, "title": "Function plot_history in fuel efficiency, the regression example is not synchronized with the notebook", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12.0\r\n- Doc Link: https://www.tensorflow.org/tutorials/keras/basic_regression\r\n\r\n**Describe the documentation issue**\r\nThe function plot_history(history) is using pandas DataFrame \"hist\" from outside of the function scope, making it draw the same figure even when the EarlyStopping callbacks are used the second time. This error is corrected in the notebook (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_regression.ipynb), but for some reason, it is not reflected on the website.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nI will look into it later.", "comments": ["Thanks. I've started a tutorials publishing push to get the latest changes on the site", "@lamberta thanks! https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough has the same issue, so I think I would just leave it here:\r\n\r\nplt.scatter(features['petal_length'],\r\n            features['sepal_length'],\r\n            c=labels,\r\n            cmap='viridis')\r\n\r\nshould use \"c=labels.numpy()\", which is corrected in the colab notebook, but not in the tutorial page.\r\n", "Just regenerated notebook tutorials and published to: https://www.tensorflow.org/tutorials/\r\nWould you mind verifying? Thanks", "Looks great! Thanks.", "Thanks"]}, {"number": 25107, "title": "Building error on debian, too many commands in cuda-include genrule", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.17.2 (same observed with 0.15.2 and 0.19.1)\r\n- GCC/Compiler version (if compiling from source): gcc-6\r\n- CUDA/cuDNN version: 9.2.148/7.0.5\r\n- GPU model and memory: Nvidia Ti 1080 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhile trying to build tensorflow from source encounter segmentation fault from bash executable which I suspect is caused by a line in an automatically generated script that is too long.\r\n\r\nThe automatically generated script is `bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda-include.genrule_script.sh` which is generated from the genrule that appears in the automatically generated build file at `/home/myusername/.cache/bazel/_bazel_myusername/01d241adc9daa256c4cb8e009a69aaab/external/local_config_cuda/cuda/BUILD`. This genrule lists approximately 35 thousand as outputs which translates into 35 thousand chained copy statements (which succeed if placed on different lines). \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Clone github repository\r\n2. Checkout r1.12 branch\r\n3. Run `configure` script which results in following in `.tf_configure.bazelrc`\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/nmoran/anaconda3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/nmoran/anaconda3/lib/python3.7/site-packages\"\r\nbuild --python_path=\"/home/nmoran/anaconda3/bin/python\"\r\nbuild --define with_ignite_support=true\r\nbuild --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.2\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env NCCL_INSTALL_PATH=\"/usr/local/nccl_2.3.7-1+cuda9.2_x86_64/lib\"\r\nbuild --action_env NCCL_HDR_PATH=\"/usr/local/nccl_2.3.7-1+cuda9.2_x86_64/lib/../include\"\r\nbuild --action_env TF_NCCL_VERSION=\"2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc-6\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n```\r\n4, Attempt to build using\r\n`bazel build --config=opt --config=cuda --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nError message reported is:\r\n```\r\nERROR: /home/myusername/.cache/bazel/_bazel_myusername/01d241adc9daa256c4cb8e009a69aaab/external/local_config_cuda/cuda/BUILD:206:1: Executing genrule @local_config_cuda//cuda:cuda-include failed (Segmentation fault): bash failed: error executing command /bin/bash bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda-include.genrule_script.sh                                                                        \r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.546s, Critical Path: 0.69s\r\nINFO: 9 processes: 9 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nFirst portion of the genrule that is causing the issues:\r\n```\r\ngenrule(\r\n    name = \"cuda-include\",\r\n    outs = [\r\n        \"cuda/include/CL/cl.h\",\r\n        \"cuda/include/CL/cl_d3d10.h\",\r\n        \"cuda/include/CL/cl_d3d11.h\",\r\n        \"cuda/include/CL/cl_dx9_media_sharing.h\",\r\n        \"cuda/include/CL/cl_dx9_media_sharing_intel.h\",\r\n        \"cuda/include/CL/cl_egl.h\",\r\n        \"cuda/include/CL/cl_ext.h\",\r\n        \"cuda/include/CL/cl_ext_intel.h\",\r\n        \"cuda/include/CL/cl_gl.h\",\r\n        \"cuda/include/CL/cl_gl_ext.h\",\r\n        \"cuda/include/CL/cl_platform.h\",\r\n        \"cuda/include/CL/cl_va_api_media_sharing_intel.h\",\r\n        \"cuda/include/CL/cl_version.h\",\r\n        \"cuda/include/CL/opencl.h\",\r\n        \"cuda/include/CharLS/charls.h\",\r\n        \"cuda/include/CharLS/publictypes.h\",\r\n        \"cuda/include/EGL/egl.h\",\r\n        \"cuda/include/EGL/eglext.h\",\r\n        \"cuda/include/EGL/eglextchromium.h\",\r\n        \"cuda/include/EGL/eglmesaext.h\",\r\n        \"cuda/include/EGL/eglplatform.h\",\r\n        \"cuda/include/GL/gl.h\",\r\n        \"cuda/include/GL/gl_mangle.h\",\r\n        \"cuda/include/GL/glcorearb.h\",\r\n        \"cuda/include/GL/glext.h\",\r\n        \"cuda/include/GL/glu.h\",\r\n        \"cuda/include/GL/glu_mangle.h\",\r\n        \"cuda/include/GL/glx.h\",\r\n        \"cuda/include/GL/glx_mangle.h\",\r\n        \"cuda/include/GL/glxext.h\",\r\n        \"cuda/include/GL/glxint.h\",\r\n        \"cuda/include/GL/glxmd.h\",\r\n        \"cuda/include/GL/glxproto.h\",\r\n        \"cuda/include/GL/glxtokens.h\",\r\n        \"cuda/include/GL/internal/dri_interface.h\",\r\n        \"cuda/include/GL/internal/glcore.h\",\r\n        \"cuda/include/KHR/khrplatform.h\",\r\n        \"cuda/include/OpenEXR/Iex.h\",\r\n        \"cuda/include/OpenEXR/IexBaseExc.h\",\r\n        \"cuda/include/OpenEXR/IexErrnoExc.h\",\r\n        \"cuda/include/OpenEXR/IexExport.h\",\r\n        \"cuda/include/OpenEXR/IexForward.h\",\r\n...\r\n``` ", "comments": ["Looking a bit deeper, seems something is going wrong when this rule goes from the build file at `./third_party/toolchains/gpus/cuda/BUILD` which lists 980 header files to the automatically generated one in the cache folder which has 35k files. This corresponds to every header file in the `/usr` folder. \r\n\r\nI suspect this is not showing up in other builds as there is a separate folder for cuda (e.g. `/usr/local/cuda`), whereas on debian the cuda headers and libraries get installed to `/usr/include` and `/usr/lib/x86_64-linux-gnu`.", "@nmoran  Could you follow the instructions [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) and let us know how it works. I recently used those steps and successfully installed tensorflow-gpu. If you had already resolved the issue, please close the issue or let us know. Thanks!", "@jvishnuvardhan thanks for looking into this issue. The instructions you link to use the nvidia cuda packages on ubuntu which install the cuda libraries and headers to `/usr/local/cuda`. This issue concerns the debian cuda packages on debian systems which install the cuda libraries and headers to `/usr` which then lead to the above issue with the number of header files that the build process attempts to copy to the cache folder. Is it a case that it is only supported to build on systems where the cuda headers are installed in their own folder and not alongside the standard system header files?", "@nmoran I followed instructions till git clean -xdf. After that I ran the following commands \r\nsudo pip3 install numpy\r\nsudo pip3 install --upgrade tensorflow-gpu\r\n\r\nIt worked for me. No issues. Thanks!", "@jvishnuvardhan thanks for looking into this further. However you are installing a prebuilt version and not building from source which is where this issue occurs.", "If its still an issue; python 3.7 is supported in TF 1.13, TF 1.12 does not have python 3.7 support.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/RELEASE.md#release-1130\r\nPlease build with TF 1.13 for Python 3.7 support. Thanks!", "This issue is still present with 1.13 and does not appear to be related to the Python version. The issue mentioned above can be circumvented by applying the following patch \r\n\r\n```\r\n--- a/third_party/gpus/cuda_configure.bzl\r\n+++ b/third_party/gpus/cuda_configure.bzl\r\n@@ -1192,7 +1192,7 @@ def symlink_genrule_for_dir(\r\n   genrule = _genrule(\r\n       src_dir,\r\n       genrule_name,\r\n-      \" && \".join(command),\r\n+      \"\\n\".join(command),\r\n       \"\\n\".join(outs),\r\n   )\r\n   return genrule\r\n```\r\nwhich results in copy commands being placed on different lines (instead of appended to form a single command). This gets around the segmentation fault from the bash interpreter which results from passing a command that is too large.\r\n\r\nWith this change, the build gets much further, but results in the following error:\r\n```\r\nImportError: /home/nmoran/.cache/bazel/_bazel_nmoran/8036d860fa88ce032f480e5d19802b14/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN6icu_638ByteSinkE\r\n```\r\nThis happens with both python 3.7.2 and python 3.6.8 and branches r1.13 and v1.13.1."]}, {"number": 25106, "title": "docs(gan): fix tutorial link for ipythonnotebook", "body": "Closes #25101\r\n\r\nFix the link in **tensorflow/contrib/gan/README.md**, line 60", "comments": []}, {"number": 25105, "title": "Export Control Classification Number (ECCN) for Tensorflow", "body": "**System information**\r\n- TensorFlow version: 1.8.0\r\n- Doc Link:\r\n\r\nI would like to use Tensorflow in commercial software which will be sold in the U.S. For this reason, the legal department asks me about Export Control Classification Number (ECCN) for Tensorflow library. From my understanding, the open sources software is not subject to [Encryption and Export Administration Regulations (EAR)](https://www.bis.doc.gov/index.php/policy-guidance/encryption/1-encryption-items-not-subject-to-the-ear). \r\nCan anyone confirm that Tensorflow is not a subject to EAR or point a ECCN class for Tensorflow? \r\n\r\nDoes Tensorflow use any encryption functionality, which should be mention when applying for ECCN for software which uses Tensorflow?\r\n", "comments": ["We cannot give you legal advice. \r\n\r\nI can say, however, that TensorFlow is open-source, and that TensorFlow itself does not contain any encryption functionality (although its dependencies may). \r\n\r\nClosing because we won't be able to give legal advice."]}, {"number": 25103, "title": "facing this error while generating tfrecords in LabelImg", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 4.5.12\r\n- Python version: 3.5.2\r\n- GPU model and memory: GPU 0 - Intel(R) HD Graphics 620 |  GPU 1 - NVIDIA GeForce 920MX | 8gb RAM\r\n\r\n**(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection>python generate_tfrecord.py --csv_input=images\\train_labels.csv --image_dir=images\\train --output_path=train.record**\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"generate_tfrecord.py\", line 17, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\New User\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["The error message says that TensorFlow installation was not successful. Please [install](https://www.tensorflow.org/install/) the latest version of TensorFlow and try again. Thanks!", "but they are updated?\r\n\r\ntensorboard                        1.12.2\r\ntensorflow                         1.12.0\r\ntensorflow-gpu                     1.12.0", "Can you import TensorFlow successfully in your session?\r\n> import tensorflow as tf\r\n\r\nDoes this step execute successfully for you?", "in my local python, it can. \r\n\r\nbut in Anaconda python, this error appears \r\n\r\n**import tensorflow as tf**\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\New User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 102, in <module>\r\n    _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant(_pywrap_tensorflow_internal)\r\nAttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant'", "1)Have you named any of python files as 'tensorflow.py'? If yes, Can you please change the name to \r\n something else and try again?\r\n2) Can you load the script using python3 interpreter?. I think you had used python2 previously. You can try,\r\n> python3 generate_tfrecord.py\r\n\r\nThe first error is related incorrect loading of TensorFlow module. You can seek out to anaconda community to resolve it (you may have to add the variable path correctly). Since TF is successfully loaded from your terminal session.\r\n", "Ahh. Yeh, a while ago. But I already changed it.\r\n\r\n**(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection>python3 generate_tfrecord.py**\r\n\r\n'python3' is not recognized as an internal or external command,\r\noperable program or batch file.", "It's an environment variable issue. Please refer related stackoverflow links to resolve it.\r\nhttps://stackoverflow.com/questions/13596505/python-not-working-in-command-prompt\r\nhttps://github.com/spywhere/Terminality/issues/19\r\nYou can reach out to stackoverflow if the problems still persists.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25102, "title": "tutorial example for eager evaluation not working", "body": "I am using anaconda with python 3 and have tensorflow 1.12 \r\nI am trying to run [this](https://www.tensorflow.org/guide/eager) tutorial example \r\n\r\nUsing with code: \r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n```\r\n\r\nand getting this error \r\n\r\n> raise RuntimeError(\"tf.placeholder() is not compatible with \"\r\n> RuntimeError: tf.placeholder() is not compatible with eager execution.\r\n\r\nWhat am i missing here? ", "comments": []}, {"number": 25101, "title": " tensorflow/contrib/gan/README.md link compromised", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:\r\n- Doc Link:\r\nhttps://github.com/tensorflow/tensorflow/commit/3ae375aa92fbb6155f82393735d0b98d8fb9c1b2?diff=split#diff-4ffc4dce469256b24264cb6c7db54363\r\n\r\n**Describe the documentation issue**\r\nThe link to the TF-GAN tutorial is in tensorflow/contrib/gan/README.md L:59\r\npoints to http://https://github.com/tensorflow/models/tree/master/research/gan/tutorial.ipynb but it should point to https://github.com/tensorflow/models/tree/master/research/gan/tutorial.ipynb\r\nThe wrong link takes one to some sales page.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": []}, {"number": 25100, "title": "xavier_initializer documentations claims use of normal distribution but truncated normal is actually being used", "body": "**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer\r\n\r\n**Describe the documentation issue**\r\n\r\nAccording to the documentation the Xavier initializer allows use of both the uniform distribution and the normal distribution.  However, when passing `uniform=False` to `xavier_initializer`, the code in `variance_scaling_initializer` actually uses a truncated normal distribution instead, [code link](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/layers/python/layers/initializers.py#L146).\r\n\r\nThe fix would consist of adjusting the documentation to provide details about the truncated Normal sampling.", "comments": ["@MarkDaoust and @nowozin , is that okay if I amend the description in the document and submit a PR? ", "Closing this now since the `tf.contrib` module is deprecated. Thanks!"]}, {"number": 25099, "title": "TF Keras slice_arrays object is not subscriptable problem fix", "body": "slice_arrays method fails to slice when receive arrays contains object not subscriptable.\r\n\r\nIssue Reproduction:\r\ntensorflow.python.keras.utils.generic_utils.slice_arrays(list([1, 2, 3], start=0, stop=1)", "comments": ["@fchollet \r\n\r\nunit test case added. Thanks", "@fchollet, @pavithrasv Could you PTAL and approve."]}, {"number": 25098, "title": "WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS", "body": "WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops in GraphKeys.UPDATE_OPS\r\ntf.get_collection\uff08GraphKeys.UPDATE_OPS\uff09is tf.operation,but i use a simple graph is tf.tensor,how to solve it\r\n\r\nimport tensorflow as tf\r\n bn = tf.layers.batch_normalization(tf.constant([0.0]), training=True)\r\n print(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\r\nwill output:\r\n[< tf.Tensor 'batch_normalization/AssignMovingAvg:0' shape=(1,) dtype=float32_ref>, \r\n < tf.Tensor 'batch_normalization/AssignMovingAvg_1:0' shape=(1,) dtype=float32_ref>]\r\n\r\nprint(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\r\n[<tf.Variable 'batch_normalization/gamma:0' shape=(1,) dtype=float32_ref>, \r\n <tf.Variable 'batch_normalization/beta:0' shape=(1,) dtype=float32_ref>]\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 25097, "title": "Added HardShrink transfer operator for Keras", "body": "New operator along with test cases added to Keras", "comments": ["Do you have any reference that shows that this new activation function is better in certain cases?\r\n\r\nAlso, this may be better suited for keras-contrib because I don't think this is widely used.", "@nataliaponomareva & @Dref360  , this operator is supported by many frameworks like Pytorch, BIGDL and paddle , this operator is used in ADMM algorithm(there are several papaers on this which you can refer). There is a arxiv paper which uses this operator as well:-\r\nhttps://arxiv.org/pdf/1711.00905.pdf\r\nAlso all the framework use it in the same place of softmax,relu etc, so i thought of placing it here as this seems to me the correct place, however i am open to suggestions if you do not agree with me, please suggest i will change the location if you feel otherwise.", "@nataliaponomareva , pls spend some time for the review, would really appriciate your comments, this will help me to understand better this community and contribute more.", "@tanzhenyu , @nataliaponomareva and @rthadur , can you pls review the PR", "@tanzhenyu , @nataliaponomareva and @rthadur , can you pls review the PR", "Nagging Reviewer @tanzhenyu, @nataliaponomareva: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied.", "@tanzhenyu , this operator is supported by many frameworks like Pytorch, BIGDL and paddle , this operator is used in ADMM algorithm(there are several papers on this which you can refer). There is a arxiv paper which uses this operator as well:-\r\nhttps://arxiv.org/pdf/1711.00905.pdf\r\nAlso all the framework use it in the same place of softmax,relu etc, Kindly check and let me know your opinion.\r\n", "@amitsrivastava78 Best regards, but I tend to agree with @Dref360 that it would be better to merge this once you have a reference that shows this layer is statistically better than other layers, at least in some certain use cases.", "@tanzhenyu , I understand your concern, kindly refer to this paper below , refer to page 13 which has some comparison results. If you still feel that this need not be contributed here, i will close the PR and follow your suggestion.\r\n\r\nhttps://www.researchgate.net/publication/220660052_Diffusion-Inspired_Shrinkage_Functions_and_Stability_Results_for_Wavelet_Denoising\r\n\r\nRegards\r\nAmit", "> @tanzhenyu , I understand your concern, kindly refer to this paper below , refer to page 13 which has some comparison results. If you still feel that this need not be contributed here, i will close the PR and follow your suggestion.\r\n> \r\n> https://www.researchgate.net/publication/220660052_Diffusion-Inspired_Shrinkage_Functions_and_Stability_Results_for_Wavelet_Denoising\r\n> \r\n> Regards\r\n> Amit\r\n\r\nThis paper proposed a wavelet shrinkage method, and hardshrink is merely used as a benchmark for comparison purpose. I'm totally fine if you want to keep this and submit it to the add-on repo, which I think many users are relying on as well.", "@tanzhenyu, thanks a lot for the review and time you have spend on the PR, it was nice discussing with you. Hope to have more discussions in future. I am closing this PR as per the suggestion.\r\n\r\nRegards\r\nAmit", "Thank you for your contribution!"]}, {"number": 25096, "title": "TF2.0-preview: cannot use layer as activation function anymore", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20190117\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-6228-g69b9e5358b'\"\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen I try to use a layer as an activation function in a `Dense` layer, I get an `AttributeError: 'Tensor' object has no attribute 'numpy'`. This did not happen a few days ago in TF 2.0-preview.\r\n\r\n**Describe the expected behavior**\r\nI expect no error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n#### BAD (raise AttributeError):\r\nmy_softplus = keras.layers.Activation(\"softplus\")\r\n#my_softplus = keras.layers.Lambda(tf.nn.softplus)\r\n#my_softplus = keras.layers.Lambda(lambda X: tf.nn.softplus(X))\r\n#my_softplus = keras.layers.Lambda(tf.function(lambda X: tf.nn.softplus(X)))\r\n\r\n#### GOOD:\r\n#my_softplus = \"softplus\"\r\n#my_softplus = tf.nn.softplus\r\n#my_softplus = lambda X: tf.nn.softplus(X)\r\n#my_softplus = tf.function(lambda X: tf.nn.softplus(X))\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Dense(1, activation=my_softplus, input_shape=[5])\r\n])\r\n```\r\n\r\nThere is no problem when the `my_softplus` layer is used as a separate layer, but the Keras API specifies that layers can be used like any function, so I expect to be able to use them as activation functions (and it was possible before).\r\n\r\n**Other info / logs**\r\n\r\nHere is the stacktrace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 455, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 112, in __init__\r\n    self.add(layer)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\", line 455, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py\", line 167, in add\r\n    layer(x)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 564, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 979, in call\r\n    return self.activation(outputs)  # pylint: disable=not-callable\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 531, in __call__\r\n    base_layer_utils.create_keras_history(inputs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 251, in create_keras_history\r\n    _create_keras_history_helper(tensors, set())\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 288, in _create_keras_history_helper\r\n    constants[i] = backend.function([], [op_input])([])\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3174, in __call__\r\n    [x.numpy() for x in outputs])\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3174, in <listcomp>\r\n    [x.numpy() for x in outputs])\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "comments": ["I just ran into the same `AttributeError` when trying to create a custom layer:\r\n\r\n```python\r\nfrom tensorflow import keras\r\n\r\nclass MyActivation(keras.layers.Layer):\r\n    def __init__(self, activation=None, **kwargs):\r\n        self.activation = keras.layers.Activation(activation)\r\n        super(MyActivation, self).__init__(**kwargs)\r\n    def call(self, X):\r\n        return self.activation(X + 1.)\r\n\r\nmodel = keras.models.Sequential([\r\n    MyActivation(activation=\"softplus\", input_shape=[5])\r\n])\r\n```\r\n\r\nThe exception disappears if I replace `X + 1.` with `X`, or if I add `@tf.function` before `def call(...)`.", "Thanks for raising this issue, this should be fixed in the next nightly of the preview. It may take a day or two to appear"]}, {"number": 25095, "title": "Fix a memory leak in MklDnnData.", "body": "This commit fixes leak caused by field user_memory_, which I can repro\r\nand verify. From reading the code, I think the same leak may happen to\r\nfields like reorder_memory_. Since I cannot repro or verify the later,\r\nI'll leave it the authors.", "comments": []}, {"number": 25094, "title": "ImportError: DLL load failed with error code -1073741795", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 7 Home Premium 64-bit SP1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:ATI Mobility Radeon HD 6370\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nIt failed on importing tensorflow\r\n**Describe the expected behavior**\r\nIt should be able to import tensorflow. I don't have problem in Anaconda.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n#install Python3.6.8\r\n#install vc_redist.x64\r\npip install -U pip virtualenv\r\nvirtualenv --system-site-packages -p python ./keras\r\n.\\keras\\Scripts\\activate keras\r\n(keras) pip install --upgrade pip\r\n(keras) pip list\r\n(keras) pip install --upgrade tensorflow\r\n(keras) python\r\nimport tensorflow as tf\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license()\" for more information.\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Stlee\\keras\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": [">virtualenv --system-site-packages -p python ./keras\r\n\r\nCan you try setting your python interpreter to Python3 and build virtual environment?\r\nYou can try,\r\n>virtualenv --system-site-packages -p python3 ./keras\r\n", "I got Python 3.6.8 only, if I type -p python3, it'll complain.", "Hi ymodak,\n\nThanks for your response.\nI got Python 3.6.8 only. If I type -p python3, it will find no python\ninterpreter.\n\nShyhtzong Lee\n\nymodak <notifications@github.com> \u65bc 2019\u5e741\u670823\u65e5 \u9031\u4e09 \u4e0a\u53488:38\u5beb\u9053\uff1a\n\n> virtualenv --system-site-packages -p python ./keras\n> Can you try setting your python interpreter to Python3 and build virtual\n> environment?\n> virtualenv --system-site-packages -p python3 ./keras\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25094#issuecomment-456620098>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ApecmQMzMvPWm9Q0yGwyzKC3tgZhdPFaks5vF673gaJpZM4aMEgE>\n> .\n>\n", "Is this still an issue for you?\r\ncan you update your keras version, if not the latest? Thanks!", "It doesn't matter I install keras or not, it just failed me on \"import\ntensorflow\".\nI can run tensorflow and keras on Anaconda, it's got its own vs2015_runtime,\nso I don't need to install vc_redist.x64 to run Anaconda.\nBut it's still a problem for me in the windows python.\nThanks.\n\nymodak <notifications@github.com> \u65bc 2019\u5e742\u670816\u65e5 \u9031\u516d \u4e0a\u53488:38\u5beb\u9053\uff1a\n\n> Is this still an issue for you?\n> can you update your keras version, if not the latest? Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25094#issuecomment-464262255>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ApecmXmOF0wplgz5KR3bAMzKZE9UJfYIks5vN1L1gaJpZM4aMEgE>\n> .\n>\n", "I had the same problem with python 3.6.0. Updating it to python 3.6.4 helped. Try downgrading to python 3.6.4", "No, python 3.6.4 does not work for me. Thanks anyway.", "I have same problem, when I import show error like this\r\n\r\nOS : Win7 SP1 x64\r\nCPU : Intel Xeon w3550(3.07GHz)\r\nRAM : 8G\r\nVGA: Geforce 960(2GB) driver version : 391.35\r\nTensorFlow : 1.12.0\r\nPython : 3.6.4\r\n\r\n--------------------------------------------------------------------------------\r\nStart Server\r\nTraceback (most recent call last):\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"imp.py\", line 243, in load_module\r\n  File \"imp.py\", line 343, in load_dynamic\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\...\\Ez3D-i_Auto_Tooth_Segmentation.launch.pyw\", line 32, in <module>\r\n    serve()\r\n  File \"C:\\...\\pkgs\\ai_server\\aitoothseg_server.py\", line 198, in serve\r\n    teeth_center_image_inst, tooth_volume_mask_inst, sagittal_teeth_area_inst = create_runner_instance()\r\n  File \"build\\bdist.win-amd64\\egg\\tooth_segmentation\\__main__.py\", line 37, in create_runner_instance\r\n  File \"build\\bdist.win-amd64\\egg\\tooth_segmentation\\common\\VTUtil\\VTTime.py\", line 11, in wrapper\r\n  File \"build\\bdist.win-amd64\\egg\\tooth_segmentation\\processing\\sagittal_teeth_area.py\", line 15, in __init__\r\n  File \"build\\bdist.win-amd64\\egg\\tooth_segmentation\\common\\VTUtil\\VTImport.py\", line 21, in runner\r\n  File \"build\\bdist.win-amd64\\egg\\tooth_segmentation\\eaf\\runner.py\", line 11, in <module>\r\n  File \"C:\\...\\pkgs\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\...\\pkgs\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"imp.py\", line 243, in load_module\r\n  File \"imp.py\", line 343, in load_dynamic\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "I solved my problem.\r\nRefer to the below links\r\nhttps://github.com/tensorflow/tensorflow/issues/17386#issuecomment-370129452\r\nhttps://stackoverflow.com/questions/49113497/python-tensorflow-import-dll-load-failed\r\n", "@shyhtzonglee Have you installed the Microsoft Visual C++ 2015 Redistributable Update 3 ?https://www.tensorflow.org/install/pip#1.-install-the-python-development-environment-on-your-system", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "> \r\n> \r\n> I solved my problem.\r\n> Refer to the below links\r\n> [#17386 (comment)](https://github.com/tensorflow/tensorflow/issues/17386#issuecomment-370129452)\r\n> https://stackoverflow.com/questions/49113497/python-tensorflow-import-dll-load-failed\r\n\r\nThank you @youngkiu! https://stackoverflow.com/a/54702852/4439762 solved my question"]}, {"number": 25093, "title": "can't rollback to python 3.6.5 due to recursive dependency between sphinx-doc and python", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?:Homebrew\r\n\r\n\r\n**Describe the problem**\r\nI just start learning python programming and wants to use Tensorflow package which requires an order version of Python to be installed. I've searched the web and many recommends using Homebrew for installation. So I tried the following commands but got the errors which I've struggled with for several hours already ...\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere's the error message i got:\r\nError: python contains a recursive dependency on itself:\r\n  python depends on sphinx-doc\r\n  sphinx-doc depends on python\r\n", "comments": ["I do not think this is a full solution but the temporary workaround that I found to work is this: https://apple.stackexchange.com/a/349379", "Had the same issues, but following these steps got it fixed for me\r\n\r\n`$ brew unlink python`\r\n\r\nthen just install again with `--ignore-dependencies` argument\r\n\r\n`$ brew install --ignore-dependencies https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb`\r\n\r\nthen link python by switching to `python 3.6.5_1`\r\n\r\n`$ brew switch python 3.6.5_1`\r\n\r\ncheck if everything went ok\r\n\r\n`$ python3 --version`\r\n`Python 3.6.5`\r\n\r\nJust to double check if the python is loading from Homebrew\r\n\r\n`$ which python3`\r\n\r\nIf you see `/usr/local/bin/python3`  you are using the Homebrew Python . If you see `/usr/bin/python3`  then you are using the system Python and you likely need to fix your bash profile and/or source it.", "@harishperfect when doing this, brew installing 3.6.5 fails precisely when calling sphinx-build, leading to 3.6.5 not being installed at all", "I open a question about that on SO: https://stackoverflow.com/questions/54571367/downgrade-to-python-3-6-5-with-brew-error-bin-sh-sphinx-build-command-not-fo", "> Had the same issues, but following these steps got it fixed for me\r\n> \r\n> `$ brew unlink python`\r\n> \r\n> then just install again with `--ignore-dependencies` argument\r\n> \r\n> `$ brew install --ignore-dependencies https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb`\r\n> \r\n> then link python by switching to `python 3.6.5_1`\r\n> \r\n> `$ brew switch python 3.6.5_1`\r\n> \r\n> check if everything went ok\r\n> \r\n> `$ python3 --version`\r\n> `Python 3.6.5`\r\n> \r\n> Just to double check if the python is loading from Homebrew\r\n> \r\n> `$ which python3`\r\n> \r\n> If you see `/usr/local/bin/python3` you are using the Homebrew Python . If you see `/usr/bin/python3` then you are using the system Python and you likely need to fix your bash profile and/or source it.\r\n\r\nIt doesn't work for me!", "Here is a solution to downgrade python to 3.6.5\r\n\r\nhttps://www.pyimagesearch.com/2019/01/30/macos-mojave-install-tensorflow-and-keras-for-deep-learning/", "Yeah that worked for me", "@xintrac Is this still an issue for you? Did you get a chance to try the suggestions posted on this thread yet?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Sphinx-doc has updated to v2.0, the python 3.6 formula (f2a764ef9...) is attempting to install sphinx using Homebrew, which deprecates functions needed to build docs for python 3.6. Installing a previous version of sphinx won't link since its keg-only, and --force doesn't help. The only way to reinstall after python 3.7 is to not build the docs.\r\n\r\nI have uploaded a formulae that seems to work with instructions [here](https://github.com/DannyKong12/homebrew-formulae)\r\n\r\n```\r\nbrew unlink python                                          # if 3.7 is installed\r\nbrew tap dannykong12/formulae\r\nbrew install dannykong12/formulae/python\r\n```\r\n\r\nIt may still be possible to install non-latest sphinx-doc or change the dependency, but I'm not sure how to get it to work", "I am also having this problem. As with @dedeco, using @harishperfect's instructions did not work, nor did the instructions for Mojave that included commenting out the python dependency line.\r\n\r\nDetails on the error I've been fighting, after running the suggested ` brew install --ignore-dependencies https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb` to install (after unlinking 3.7):\r\n```\r\nLast 15 lines from ~/Library/Logs/Homebrew/python/05.make:\r\n2019-05-07 13:05:37 -0700\r\n\r\nmake\r\nhtml\r\n\r\nmkdir -p build\r\nUsing existing Misc/NEWS file\r\nPATH=./venv/bin:$PATH sphinx-build -b html -d build/doctrees -D latex_elements.papersize=  . build/html \r\n/bin/sh: sphinx-build: command not found\r\nmake: *** [build] Error 127\r\n\r\nDo not report this issue to Homebrew/brew or Homebrew/core!\r\n\r\nThese open issues may also help:\r\ncsound: add Python bindings https://github.com/Homebrew/homebrew-core/pull/39418\r\npython: gdbm fix https://github.com/Homebrew/homebrew-core/pull/38268\r\npython@2: zlib and gdbm fixes https://github.com/Homebrew/homebrew-core/pull/38241\r\npython@*: setuptools 41.0.1, pip 19.1.1 https://github.com/Homebrew/homebrew-core/pull/39225\r\nbrew install gdb with python 3 built, but gdb debug fail https://github.com/Homebrew/homebrew-core/issues/38934\r\n```\r\n\r\nI do have sphinx installed:\r\n```\r\n>>> which sphinx-build\r\n/usr/local/bin/sphinx-build\r\n>>> sphinx-build --version\r\nsphinx-build 2.0.1\r\n```\r\nI tried adding this directly to my path and re-attempting an install with Homebrew, but it gives the same error.\r\n\r\nI've installed the formulae from @DannyKong12 (THANK YOU) and this seems to work for now.\r\n", "> I am also having this problem. As with @dedeco, using @harishperfect's instructions did not work, nor did the instructions for Mojave that included commenting out the python dependency line.\r\n> \r\n> Details on the error I've been fighting, after running the suggested ` brew install --ignore-dependencies https://raw.githubusercontent.com/Homebrew/homebrew-core/f2a764ef944b1080be64bd88dca9a1d80130c558/Formula/python.rb` to install (after unlinking 3.7):\r\n> \r\n> ```\r\n> Last 15 lines from ~/Library/Logs/Homebrew/python/05.make:\r\n> 2019-05-07 13:05:37 -0700\r\n> \r\n> make\r\n> html\r\n> \r\n> mkdir -p build\r\n> Using existing Misc/NEWS file\r\n> PATH=./venv/bin:$PATH sphinx-build -b html -d build/doctrees -D latex_elements.papersize=  . build/html \r\n> /bin/sh: sphinx-build: command not found\r\n> make: *** [build] Error 127\r\n> \r\n> Do not report this issue to Homebrew/brew or Homebrew/core!\r\n> \r\n> These open issues may also help:\r\n> csound: add Python bindings https://github.com/Homebrew/homebrew-core/pull/39418\r\n> python: gdbm fix https://github.com/Homebrew/homebrew-core/pull/38268\r\n> python@2: zlib and gdbm fixes https://github.com/Homebrew/homebrew-core/pull/38241\r\n> python@*: setuptools 41.0.1, pip 19.1.1 https://github.com/Homebrew/homebrew-core/pull/39225\r\n> brew install gdb with python 3 built, but gdb debug fail https://github.com/Homebrew/homebrew-core/issues/38934\r\n> ```\r\n> \r\n> I do have sphinx installed:\r\n> \r\n> ```\r\n> >>> which sphinx-build\r\n> /usr/local/bin/sphinx-build\r\n> >>> sphinx-build --version\r\n> sphinx-build 2.0.1\r\n> ```\r\n> \r\n> I tried adding this directly to my path and re-attempting an install with Homebrew, but it gives the same error.\r\n> \r\n> I've installed the formulae from @DannyKong12 (THANK YOU) and this seems to work for now.\r\n\r\nHave the same issue", "I suggest using `pyenv` for managing python versions\r\n\r\n```\r\nbrew install pyenv\r\nbrew install pyenv-virtualenv\r\npyenv install 3.6.5\r\npyenv virtualenv 3.6.5 my_env\r\n```", "Used `brew link python 3.6.5_1`"]}, {"number": 25092, "title": "Data Augmentation and pre-processing in tf.data.Dataset", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-2.0-preview\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda 10\r\n- GPU model and memory: K80 \r\n\r\n\r\n\r\n**Describe the current behavior**\r\nSuppose I want to do `transfer learning`. I use the `base model` from `tf.keras.appliactions`, for example, `VGG16`. During transfer learning/fine-tuning, there are two steps that we almost always follow:\r\n1) `Pre-process` each batch. e.g  subtract imagenet mean, converting values in range [0,1] or [-1,1]\r\n2) Data Augmentation\r\n\r\nThe actual pipeline looks like this:\r\n```python\r\n\r\ndef get _model()\r\n  base_model = tf.keras.applications.vgg16.VGG16(...)\r\n  base_model_output = base_model.output\r\n\r\n  # add new layers\r\n  x = tf.keras.layers.Flatten()(base_model_output)(x)\r\n  x = tf.keras.layers.Dense(...)(x)\r\n  ...\r\n  model = Model(base_model.input, output)\r\n  model.compile(..)\r\n  return model\r\n\r\nmodel = get_model()\r\ndataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\r\ndataset = dataset.batch(32).repeat()\r\n```\r\nIn order to `pre-process` the images and perform `data augmentation`, I can write a function and map the dataset elements to it. For example,\r\n\r\n```python\r\ndef _parse_function(filename, label):\r\n  image_string = tf.read_file(filename)\r\n  image_decoded = tf.image.decode_jpeg(image_string)\r\n  image_resized = tf.image.resize_images(image_decoded, [28, 28])\r\n \r\n  # preprocess\r\n  image_resized = tf.expand_dims(image_resized, axis=0)\r\n  image_resized = vgg.preprocess_input(image_resized)\r\n  image_resized =  tf.squeeze(image_resized)\r\n  \r\n  # perform augmentation using tf or some other library like Augmenter\r\n  image_resized = ....\r\n\r\n  return image_resized, label\r\n``` \r\nThis works fine but I see two problems with this approach:\r\n1) We are not utilising `vectorization`. In order to preprocess, we expand the dimensions to reshape the image in `[1, H, W, C]` form and then we apply `vgg.preprocess_input()` function. The problem is that we are doing too many extra operations `expand_dims` and `squeeze` whereas `preprocess_input()` can operate on a batch which is the ideal case.\r\n\r\n2) Data Augmentation libraries like [Augmentor](https://github.com/mdbloice/Augmentor) or [imgaug](https://github.com/aleju/imgaug) works with batches and performing augmentation on random samples of a batch makes more sense than for each image but there is no way to achieve this if we are using `map` with `datasets`. \r\n\r\n\r\nThis is a performance bottleneck IMO.  It might be the case that I am missing something here. Please correct me if that's the case. ", "comments": ["You can use `.batch` on a dataset (after resize) to coalesce consecutive examples into a batch, and perform subsequent steps on the batch (using `.map`). This of course assumes that `vgg_preprocess` is written in a batch compatible manner.\r\n\r\nQuestions of this sort are likely better directed at StackOverflow."]}, {"number": 25091, "title": "Update generate_streaming_test_wav.py", "body": "", "comments": ["plz not create sound wav", "\u043f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430 \u043d\u0435 \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u0437\u0432\u0443\u043a"]}, {"number": 25090, "title": "TF Keras saving_utils_test missing test cases add", "body": "1- extract_model_metrics api test case", "comments": ["@k-w-w \r\n\r\nThanks for quick review comments, review comments accepted and changes done as per comments. Please reveiw the new changes and Thanks."]}, {"number": 25089, "title": "Estimator 1.13 doesn't build against Tensorflow 1.13", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL ppc64le\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13 (preRC)\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:  pip install from .whl\r\n- Bazel version (if compiling from source): 19.2\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: 10.0/7.4\r\n- GPU model and memory: P100/16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nTensorflow-Estimator built from the 1.13 branch fails to compile against fully built Tensorflow 1.13\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI used this bazel rc file to build Tensorflow (*not 2.0*)\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/furmanek/anaconda3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/furmanek/anaconda3/lib/python3.6/site-packages\"\r\nbuild --python_path=\"/home/furmanek/anaconda3/bin/python\"\r\nbuild --action_env OMP_NUM_THREADS=\"1\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"10.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/local/cuda-10.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,7.0\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=-mcpu=native\r\nbuild:opt --define with_default_optimizations=true\r\n```\r\n\r\n`bazel build tensorflow/tools/pip_package/build_pip_package`\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./`\r\npip install..\r\n`bazel build //tensorflow_estimator/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nTFE Build fail:\r\n```\r\nFile \"/home/furmanek/.cache/bazel/_bazel_furmanek/fa10fc9090d5b9056cb4055c758fdcae/sandbox/processwrapper-sandbox/1/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/home/furmanek/.cache/bazel/_bazel_furmanek/fa10fc9090d5b9056cb4055c758fdcae/sandbox/processwrapper-sandbox/1/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/estimator_lib.py\", line 54, in <module>\r\n    from tensorflow_estimator.python.estimator.mode_keys import ModeKeysV2\r\n  File \"/home/furmanek/.cache/bazel/_bazel_furmanek/fa10fc9090d5b9056cb4055c758fdcae/sandbox/processwrapper-sandbox/1/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/mode_keys.py\", line 22, in <module>\r\n    from tensorflow.python.training.mode_keys import ModeKeys\r\nModuleNotFoundError: No module named 'tensorflow.python.training.mode_keys'\r\nTarget //tensorflow_estimator/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2.044s, Critical Path: 1.96s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nThe problem is this commit (https://github.com/tensorflow/tensorflow/commit/8bcc801a7cd748e7d9d47f0f5d7ebd84a2f2eaea) was done in Tensorflow master, and this commit (https://github.com/tensorflow/estimator/commit/f7640c1aa61e798fdde5001ed42237ab7f941667) is in Estimator 1.13.\r\nIt's probably best to pull that Tensorflow commit into Tensorflow 1.13.\r\n\r\nFound on ppc64le, but is not platform specific.", "comments": ["I was able to build Estimator r1.13 against TF r1.13 without issue.\r\n\r\npip install tensorflow==1.13rc0\r\ncd ./estimator_repo\r\ngit checkout origin/r1.13\r\nbazel build //tensorflow_estimator/...\r\n\r\nTry fetching latest Estimator repo and make sure you're on the branch. Make sure you're building TF from release branch.", "Did the Estimator 1.13 branch get reset?\r\nLate last week, it was sitting at much later commit and had additions through January 14th or so.\r\nNow it looks like it branched earlier, before the mode_keys API was added.\r\n\r\nWe can close this, but man, please somehow let people know if you're going to rewrite history like that. Yikes.\r\n\r\nthx!"]}, {"number": 25088, "title": "InceptionResnetV2 quantization: block35_1/Relu is lacking min/max data", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- TensorFlow installed from (source or binary): tensorflow-gpu==1.12.0\r\n\r\n\r\n**Command as follows**\r\n    tflite_model = tf.contrib.lite.toco_convert(\r\n        frozen_graphdef, [images], [logits], inference_type=tf.contrib.lite.constants.QUANTIZED_UINT8,\r\n        quantized_input_stats=[(127.5, 127.5)])\r\n\r\n**Graph as follows**\r\n![graph_run 1](https://user-images.githubusercontent.com/22855898/51511778-926d8480-1e3d-11e9-9f24-29d09fb5c7d9.png)\r\n\r\n**Provide the text output from toco_convert**\r\n```\r\nF tensorflow/contrib/lite/toco/tooling_util.cc:1634] Array InceptionResnetV2/InceptionResnetV2/Repeat/block35_1/Relu, which is an input to the MaxPool operator producing the output array InceptionResnetV2/InceptionResnetV2/Mixed_6a/Branch_2/MaxPool_1a_3x3/MaxPool, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n```\r\n\r\n", "comments": ["During quantization the activations (and weights) are converted to 8 bit. For this conversion the tool needs information for minimum and maximum values. These minimum and maximum values can be obtained \r\n\r\nyou can get this information through :\r\n1. [quantized training](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md), this maybe hard since you may need to change the training code and retrain your model but this will give you better accuracy.\r\n2. The other option is \r\n[post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) which is simpler but may not give you as good accuracy as with quantized training.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity, please reopen if you are still facing the issue."]}, {"number": 25086, "title": "tf.keras.layers.UpSampling2D - static shapes for ResizeNearestNeighbor are NOT inferred in GraphDef (with `add_shapes=True`)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **1.12**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source): **n/a**\r\n- GCC/Compiler version (if compiling from source): **n/a**\r\n- CUDA/cuDNN version: **CUDA 9 / cuDNN 7**\r\n- GPU model and memory:  **n/a**\r\n\r\n### Premise:\r\nAssume we're given a `GraphDef` that was serialized as `graph.as_graph_def()`. To infer static shapes of every node in this graph and store them as `_output_shapes` attr, TensorFlow [provides](https://www.tensorflow.org/api_docs/python/tf/Graph#as_graph_def) the option`add_shapes=True`.\r\n\r\n### Issue:\r\nLet's consider the two APIs that internally use `ResizeNearestNeighbor` kernel:\r\n- `tf.image.resize_nearest_neighbor`\r\n- `tf.keras.layers.UpSampling2D`\r\n\r\nThe static shape attr (`_output_shapes`) for `ResizeNearestNeighbor` node is correctly inferred when created with `tf.image.resize_nearest_neighbor` API. Yet using `tf.keras.layers.UpSampling2D` results in incomplete shape information for height and width dims.\r\n\r\n### Minimal examples:\r\n\r\n**Using `tf.image.resize_nearest_neighbor` static shapes are inferred correctly:**\r\n```python3\r\nimport tensorflow as tf\r\n\r\n# Prepare `GraphDef`\r\ntf.reset_default_graph()\r\ninput = tf.placeholder(tf.float32, shape=[None, 26, 128, 256])\r\noutput = tf.image.resize_nearest_neighbor(input, (52, 256))\r\ngraph_def = tf.get_default_graph().as_graph_def()\r\n\r\n# Import `GraphDef` and restore static shapes\r\ntf.reset_default_graph()\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    tf.import_graph_def(graph_def, name='')\r\n    output_graph_def = tf.get_default_graph().as_graph_def(add_shapes=True)\r\n\r\n# Check shapes\r\nfor node in output_graph_def.node:\r\n    if node.op == 'ResizeNearestNeighbor':\r\n        print(node.attr[\"_output_shapes\"])\r\n```\r\n**Output**\r\n```\r\nlist {\r\n  shape {\r\n    dim {\r\n      size: -1\r\n    }\r\n    dim {\r\n      size: 52\r\n    }\r\n    dim {\r\n      size: 256\r\n    }\r\n    dim {\r\n      size: 256\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Using `tf.keras.layers.UpSampling2D` static shapes are NOT inferred for H, W:**\r\n```python3\r\nimport tensorflow as tf\r\n\r\n# Prepare `GraphDef`\r\ntf.reset_default_graph()\r\ninput = tf.placeholder(tf.float32, shape=[None, 26, 128, 256])\r\noutput = tf.keras.layers.UpSampling2D((2, 2))(input)\r\ngraph_def = tf.get_default_graph().as_graph_def()\r\n\r\n# Import `GraphDef` and restore static shapes\r\ntf.reset_default_graph()\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    tf.import_graph_def(graph_def, name='')\r\n    output_graph_def = tf.get_default_graph().as_graph_def(add_shapes=True)\r\n\r\n# Check shapes\r\nfor node in output_graph_def.node:\r\n    if node.op == 'ResizeNearestNeighbor':\r\n        print(node.attr[\"_output_shapes\"])\r\n```\r\n**Output**\r\n```\r\nlist {\r\n  shape {\r\n    dim {\r\n      size: -1\r\n    }\r\n    dim {\r\n      size: -1\r\n    }\r\n    dim {\r\n      size: -1\r\n    }\r\n    dim {\r\n      size: 256\r\n    }\r\n  }\r\n}\r\n```", "comments": ["Confirmed. This currently breaks TPU models on colab compiled with tf.contrib.tpu.keras_to_tpu_model. ", "@fchollet - could you please help take a look? Appreciate it!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25086\">No</a>\n"]}, {"number": 25085, "title": "How I can convert .tflite model to a .pb frozen graph or a keras model? ", "body": "I have a post-training quantized .tflite model and would like to convert it back to either a .pb frozen graph or keras model. How can I achieve this?\r\n\r\nI have searched a lot but didn't find any solutions or any hints. Anyone has ideas about this?\r\n", "comments": ["Hi,\r\nCurrently we don't support this, and no plan in the near future.\r\n\r\nThanks", "Hi @karimnosseir,\r\nIs there any way I can use a tflite model in TF serving?", "@paroque28 Not sure what do you mean by TF Serving. If you mean in python then yes.\r\nYou can use our Python API.\r\nhttps://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python", "I mean with this: https://www.tensorflow.org/tfx/guide/serving\r\nhttps://github.com/tensorflow/serving", "Same here, is there a solution?\r\n\r\nApparently [it was possible before](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#convert-a-tensorflow-lite-flatbuffer-back-into-tensorflow-graphdef-format-) and got dropped on TF2.0\r\n\r\nHowever that version was unable to convert my model due an unimplemented operation (PRElu).\r\n\r\nI've also seen it done in [hacky ways](https://gist.github.com/tworuler/bd7bd4c6cd9a8fbbeb060e7b64cfa008), but proper tool support would be great\r\n\r\n"]}]