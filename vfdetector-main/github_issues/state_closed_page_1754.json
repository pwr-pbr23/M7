[{"number": 267, "title": "Links broken", "body": "http://tensorflow.org/tutorials/mnist/pros/index.md#build_a_multilayer_convolutional_network\n\n\"See the Computation Graph section of Basic Usage for more detail.\"\n\nBasic Usage\nold: http://tensorflow.org/get_started/basic_usage.md\nnew: http://tensorflow.org/get_started/index.html#basic-usage\n\nComputation Graph\nold: http://tensorflow.org/get_started/basic_usage.md#the-computation-graph\nnew: http://tensorflow.org/get_started/index.html#the-computation-graph\n", "comments": ["I also found two broken links in this file: \ntensorflow/models/rnn/README.md\n\n```\n-* [RNN Tutorial](http://tensorflow.org/tutorials/recurrent/index.md)\n-* [Sequence-to-Sequence Tutorial](http://tensorflow.org/tutorials/seq2seq/index.md)\n```\n", "Links are working now I think.\n"]}, {"number": 266, "title": "Include reset_default_graph in the public API", "body": "`ops.reset_default_graph` is a very useful function that resets the current graph. It's particularly important in interactive sessions where loading data and setting up the kernel may take a long time, and having to restart the kernel every time one wants to start over is a hassle.\n", "comments": ["Derek, I think you looked at this last, feel free to reassign back to me if you got too much on your plate\n"]}, {"number": 265, "title": "Trouble running a tutorial file", "body": "Hi,\n\nI'm trying to run all tutorials, and I'm stuck with fully_connected_feed.py as it produced following error:\nTraceback (most recent call last):\n\n  File \"fully_connected_feed.py\", line 15, in <module>\n    import tensorflow.python.platform\n  File \"/home/pilotwarela0/Development/tensorflow/tensorflow/**init**.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/home/pilotwarela0/Development/tensorflow/tensorflow/python/**init**.py\", line 15, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\nImportError: No module named core.framework.graph_pb2\n\nI managed to track down the location of module:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/\n\nSo, added this path to PYTHONPATH with this:\n\nPYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python2.7/dist-packages/tensorflow/\"\nexport PYTHONPATH\n\nIt didn't work. So, I tried this solution by adding the lines:\n\nimport sys\nsys.path.append(\"/usr/local/lib/python2.7/dist-packages/tensorflow/\")\n\nin ~/tensorflow/tensorflow/python/platform/**init**.py\n\nNone of these solution worked out for me. What am I doing wrong here? Could you please suggest a correct way to solve this problem?\n", "comments": ["\"ImportError: No module named core.framework.graph_pb2\" happens typically when you're trying to run in the same directory as the root of the source tree.  If you change into a different directory, does the problem go away?\n", "HI,\n\nThanks for replying. I copied fully_connected_feed.py into a new folder, and executed this in command python as before:\n\npython fully_connected_feed.py\n\nFirst, it produced the same error:\n\nTraceback (most recent call last):\n  File \"fully_connected_feed.py\", line 19, in <module>\n    from tensorflow.g3doc.tutorials.mnist import input_data\nImportError: No module named g3doc.tutorials.mnist\n\nI updated the PYTHONPATH as follows:\n\nPYTHONPATH=\"${PYTHONPATH}:/home/pilotwarela0/Development/tensorflow\"\nexport PYTHONPATH\n\nand now arrived at the same error:\n\nTraceback (most recent call last):\n  File \"fully_connected_feed.py\", line 15, in <module>\n    import tensorflow.python.platform\n  File \"/home/pilotwarela0/Development/tensorflow/tensorflow/**init**.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/home/pilotwarela0/Development/tensorflow/tensorflow/python/**init**.py\", line 13, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\nImportError: No module named core.framework.graph_pb2\n\nSo, I wanted to check if I can import the module in python:\n\n> > > from tensorflow.core.framework.graph_pb2 import *\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/home/pilotwarela0/Development/tensorflow/tensorflow/**init**.py\", line 4, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/home/pilotwarela0/Development/tensorflow/tensorflow/python/**init**.py\", line 13, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > > ImportError: No module named core.framework.graph_pb2\n\nI tried to update the PYTHONPATH again, but not difference is produced.\n", "According to this, it's allowed to execute the source in root dir:\n\nhttp://tensorflow.org/get_started/os_setup.html#train-your-first-tensorflow-neural-net-model\n\nI tried the example as shown, but I encountered the same problem. I installed tensorflow on Ubuntu with this instruction:\n\n# For CPU-only version\n\n$ pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n\nAnd in python, I was able to import the tensorflow with no problem:\n\npython> import tensorflow\n", "That link is showing old contents: we've updated our git repo with the new instructions (as you note, the instructions on the website doesn't work).\n\nCan you get things to work if you use virtualenv and use the binary pip install, by any chance?\n", "Unfortunately even in virtulaenv, fully_connected_feed.py neither worked in nor out of the source root. However, I was able to execute the convolutional.py.\n\nCould you please try to run fully_connected_feed.py yourself as found in the git source?\n", "The problem is fixed. All is good.\n", "Sorry, I didn't get a chance to look at this myself -- do you mind explaining what you did so others who run into the same problem might know the answer?\n", "Turns out, I had a wrong PYTHONPATH taking the first place, so it failed to find the correct module path. Here, I had this in the first place:\n\nPYTHONPATH=\"${PYTHONPATH}:/home/pilotwarela0/Development/tensorflow\"\n\nThe solution was to clear out this incorrect PYTHONPATH, then add the correct path again to the first place with this:\n\nPYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python2.7/dist-packages/\"\nexport PYTHONPATH\n\nMake sure to check your path with this: echo $PYTHONPATH\n"]}, {"number": 264, "title": "Unable to Build Android Example", "body": "Getting an error when building the Android example.  Have run into protobuf build issues on Mac OS X before, is this related?\n\nMac OS X - 10.11.1\n\nOutput:\n$ bazel build //tensorflow/examples/android:tensorflow_demo -c opt --copt=-mfpu=neon --verbose_failures\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/arena.cc:\nsrc/main/tools/process-wrapper.c:119: execvp(argv[0], argv): No such file or directory\nERROR: /Users/astellato/Desktop/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: false failed: error executing command \n  (cd /private/var/tmp/_bazel_astellato/e1414493f08fd994c717215d0d5d2bb5/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/Users/astellato/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/MacGPG2/bin:/usr/local/share/npm/bin:/Users/astellato/Development/Android/sdk/platform-tools:/Users/astellato/Development/Android/sdk/tools:/Users/astellato/Development/Android/ndk/android-ndk-r10d:/Users/astellato/bin \\\n    TMPDIR=/var/folders/3z/18r1p7zn1x74lqpkv377thpc0000gn/T/ \\\n  /bin/false '-mfpu=neon' -iquote . -iquote bazel-out/android-stub_armeabi-v7a-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/android-stub_armeabi-v7a-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' '-frandom-seed=bazel-out/android-stub_armeabi-v7a-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/arena.pic.o' -MD -MF bazel-out/android-stub_armeabi-v7a-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/arena.pic.d -fPIC -c google/protobuf/src/google/protobuf/arena.cc -o bazel-out/android-stub_armeabi-v7a-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/arena.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: false failed: error executing command \n  (cd /private/var/tmp/_bazel_astellato/e1414493f08fd994c717215d0d5d2bb5/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/Users/astellato/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/MacGPG2/bin:/usr/local/share/npm/bin:/Users/astellato/Development/Android/sdk/platform-tools:/Users/astellato/Development/Android/sdk/tools:/Users/astellato/Development/Android/ndk/android-ndk-r10d:/Users/astellato/bin \\\n    TMPDIR=/var/folders/3z/18r1p7zn1x74lqpkv377thpc0000gn/T/ \\\n  /bin/false '-mfpu=neon' -iquote . -iquote bazel-out/android-stub_armeabi-v7a-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/android-stub_armeabi-v7a-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' '-frandom-seed=bazel-out/android-stub_armeabi-v7a-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/arena.pic.o' -MD -MF bazel-out/android-stub_armeabi-v7a-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/arena.pic.d -fPIC -c google/protobuf/src/google/protobuf/arena.cc -o bazel-out/android-stub_armeabi-v7a-opt/bin/google/protobuf/_objs/protobuf_lite/google/protobuf/src/google/protobuf/arena.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 4.001s, Critical Path: 0.18s\n", "comments": ["Hi, what is the content of your google/protobuf directory? If empty, try git cloning the repo with the --recursive flag to grab all of the Tensorflow submodules.\n", "Submodules were good to go, but the NDK directory was commented in the build file.\n"]}, {"number": 263, "title": "Archlinux Installation using pip!", "body": "Probably this should be mentioned in the documentation that for distros like archlinux that use python 3 by default users can do this use virtualenv like this:\n\nvirtualenv --python=python2.7 --system-site-packages\n", "comments": ["Or you can just use python2.7 and corresponding version of pip :) \n\nI had no troubles with that when I've been installing it.\n", "https://github.com/tensorflow/tensorflow/issues/1, when resolved, will add support for python 3 -- at that point hopefully this issue will go away :)\n", "Nb : If you are using archlinux, you are now using python 3.5 and not 3.4, so you have to rename the package.\nOtherwise you'll get a `tensorflow-0.6.0-cp34-none-linux_x86_64.whl is not a supported wheel on this platform.` message.\n", "@Zenol, could you elaborate a bit more? What needs to be renamed? I seem to be facing this issue.\n", "You can rename `tensorflow-0.6.0-cp34-none-linux_x86_64.whl` into `tensorflow-0.6.0-cp35-none-linux_x86_64.whl`. 34 / 35 is actually the version number.\n", "Thanks, will try that!\nOn 14 Jan 2016 21:27, \"Zenol\" notifications@github.com wrote:\n\n> You can rename tensorflow-0.6.0-cp34-none-linux_x86_64.whl into\n> tensorflow-0.6.0-cp35-none-linux_x86_64.whl. 34 / 35 is actually the\n> version number.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/263#issuecomment-171769845\n> .\n", "@Zenol : So I have downloaded the tensorflow-0.6.0-cp34-none-linux-x86_64.whl, renamed it into tensorflow-0.6.0-cp35-none-linux-x86_64.whl. The installation itself went successfully. But when i try the minimal example from the tutorial I get the \"magical number\" error.\n\n> Python 3.5.0 (default, Sep 20 2015, 11:28:25) \n> [GCC 5.2.0] on linux\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>  import tensorflow as tf\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n>   File \"/usr/lib/python3.5/site-packages/tensorflow/**init**.py\", line 23, in <module>\n>     from tensorflow.python import *\n>   File \"/usr/lib/python3.5/site-packages/tensorflow/python/**init**.py\", line 50, in <module>\n>     from tensorflow.python.framework.framework_lib import *\n>   File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/framework_lib.py\", line 62, in <module>\n>     from tensorflow.python.framework.ops import Graph\n>   File \"/usr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 24, in <module>\n>     import copy\n> ImportError: bad magic number in 'copy': b'\\xd1\\xf2\\r\\n'\n\nAnything I did wrong?\n"]}, {"number": 262, "title": "models/image/mnist/convolutional.py failed on GTX 980!", "body": "NVidia Driver Ver: 355.11\n\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:345] Aggregate Region Memory: 2990620672 (2.79GiB)\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:347] Aggregate Chunk Memory: 2903089152 (2.70GiB)\n\nProbably because GPU has only 4GB of memory.\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/136 - deduping\n"]}, {"number": 261, "title": "Building a REPL for tensorflow", "body": "Would an interactive CLI in the spirit of pgcli that would be build on top of python-prompt-toolkit is a good step forward? \n", "comments": ["Sounds like an interesting idea - feel free to create a repo that does this and update this issue if you do!\n", "Why did you close it? I want to hear ideas and comment, if people see any\npotential use in that...\n\nOn Wed, Nov 18, 2015 at 7:43 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Closed #261 https://github.com/tensorflow/tensorflow/issues/261.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/261#event-468224783.\n", "I would probably ask on the discuss mailing list to get more community input :).\n", "And the link for this discuss mailing list?\n\nSent with Mixmax\n\nOn Wed, Nov 18, 2015 at 7:51 PM, Vijay Vasudevan < notifications@github.com > wrote:\nI would probably ask on the discuss mailing list to get more community input :).\n\n\u2014\nReply to this email directly or view it on GitHub .\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#community\n"]}, {"number": 260, "title": "Error when building seqtoseq model", "body": "Hi all,\n\nI'm trying to understand the seq2seq models defined in seq2seq.py in tensorflow. I use bits of code I copy from the translate.py example that comes with tensorflow. I keep getting the same error and really do not understand where it comes from.\n\nA minimal code example to reproduce the error:\n\n``` python\n import tensorflow as tf\nfrom tensorflow.models.rnn import rnn_cell\nfrom tensorflow.models.rnn import seq2seq\n\nencoder_inputs = []\ndecoder_inputs = []\nfor i in xrange(350):  \n    encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                              name=\"encoder{0}\".format(i)))\n\nfor i in xrange(45):\n    decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                         name=\"decoder{0}\".format(i)))\n\nmodel = seq2seq.basic_rnn_seq2seq(encoder_inputs,\n                                  decoder_inputs,rnn_cell.BasicLSTMCell(512))\n```\n\nThe error I get when evaluating the last line (I evaluated it interactively in the python interpreter):\n\n```\n>>>  Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/tmp/py1053173el\", line 12, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/seq2seq.py\", line 82, in basic_rnn_seq2seq\n    _, enc_states = rnn.rnn(cell, encoder_inputs, dtype=dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/rnn.py\", line 85, in rnn\n    output_state = cell(input_, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/rnn_cell.py\", line 161, in __call__\n    concat = linear.linear([inputs, h], 4 * self._num_units, True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/linear.py\", line 32, in linear\n    raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\nValueError: Linear is expecting 2D arguments: [[None], [None, 512]]\n```\n\nI suspect the error comes from my side :)\nOn a sidenote. The documentation and the tutorials are really great but the example code for the sequence to sequence model (the english to french translation example) is quite dense. You also have to jump a lot between files to understand what's going on. Me atleast got lost several times in the code.\n\nA minimal example (perhaps on some toy data) of constructing and training a basic seq2seq model would really be helpful here. :)\n\nHave a nice evening\n", "comments": ["Hey,\nI figured it was maybe better to ask this in stack overflow. I got an answer here which I think is correct.\nhttp://stackoverflow.com/questions/33762831/error-when-building-seq2seq-model-with-tensorflow\n\nSo I 'm closing this.\n"]}, {"number": 259, "title": "TensorBoard: Add select all or none in side panel", "body": "It would be helpful if TensorBoard supported selecting all or none of the models listed in the side panel. I often work in a directory with many models but only want to look at a few of them and end up unchecking a lot of boxes.\n", "comments": ["There's now a toggle all runs button in the runs pane (see: [example](https://www.tensorflow.org/tensorboard/cifar.html#events))\n", "@decentralion your photo is no longer available and the button is nowhere to be found.", "update for everyone else like me who didnt see it after 3 years of it fading into the background\r\n![image](https://user-images.githubusercontent.com/4501464/150681902-e6a54b10-33f2-44ad-ab69-f9ca05202a16.png)\r\n"]}, {"number": 258, "title": "Import error on MAC OSX Yosemite 10.10.5", "body": "I installed TensorFlow through \npip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n\nWhen I tried to import tensorflow, I got following error\n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/**init**.py\", line 4, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 13, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n> > >     from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n> > >     from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n> > > File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n> > >     from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n> > >     serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"d\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tb\\x06proto3')\n> > > TypeError: __init__() got an unexpected keyword argument 'syntax'\n\nAny thoughts??\n", "comments": ["Sorry, missed the installation instruction, solution is as follows:\n\nThis is due to a conflict between protobuf versions (we require protobuf 3.0.0). The best current solution is to make sure older versions of protobuf are not installed, such as:\n\n  brew reinstall --devel protobuf\n"]}, {"number": 257, "title": "error in computation graph tutorials", "body": "http://www.tensorflow.org/get_started/basic_usage.md#the-computation-graph\n\nnew_value = tf.add(state, one)\n update = tf.assign(state, new_value)\n\nshould be:\n\nnew_value = tf.add(var, one)\n update = tf.assign(var, new_value)\n\nright?\n", "comments": ["Yes, this has been fixed already in our repo.  Thanks for reporting!\n"]}, {"number": 256, "title": "Transpose convolution layer for tensorflow (was deconvolution)", "body": "Has anyone already started implementing a deconvolutional layer for tensorflow as it's used e. g. here https://github.com/stokasto/caffe ? Is anyone else interested in such a functionality or is there any trivial way to implement this using the existing tensors?\n", "comments": ["It's here:\nhttps://github.com/tensorflow/tensorflow/blob/1d76583411038767f673a0c96174c80eaf9ff42f/tensorflow/python/ops/nn_ops.py#L24\nNot sure why it's not exposed in the documentation yet, possibly because the API isn't considered stable.\n", "Cool, thanks for pointing me there!\n", "https://github.com/tensorflow/tensorflow/blob/1d76583411038767f673a0c96174c80eaf9ff42f/tensorflow/python/ops/nn_ops.py#L29 suggests it isn't agreed that the op is doing deconvolution.  Hence why it wasn't made public yet.\n", "\"deconvolution\" has been used as a synonym for transposed convolution in recent computer vision literature, especially in ConvNet related work. I believe this \"deconv2d\" is exactly the op most people will be using for either upsampling or visualization purposes. \n", "@vincentvanhoucke: I'm happy to expose it in the public API as long as we're fine with the misleading name.\n", "@girving I remember being very, very confused when the term 'deconvolution' started popping up in the literature for this operator. I blame Matt (Zeiler) ;-) I had to go back to his papers to convince myself of what they were doing and that it had nothing to do with actually deconvolving the input.\nMatt Zeiler calls the entire stack Deconvolution Networks:\nhttp://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf\nbut only ever refers to this operator as a 'projection operator' as far as I can tell.\nCaffe calls it Deconvolution (@Yangqing) :\nhttp://caffe.berkeleyvision.org/doxygen/classcaffe_1_1DeconvolutionLayer.html\n\nSince 'deconv' is apparently here to stay, should we consider making the name a bit more explicit? On the table from my POV:\n1- transpose_deconv2d()\n2- project_deconv2d()\n3- deconv2d()\n\nNote that I actually don't know what an alternative 'deconv2d' might look like. The process of deconvolving an input is ill-posed and there are many ways to go about it, so it's possible that option 3 is just fine as long as there is an abundance of documentation. @shlens and @josh11b might have opinions on the matter. \n", "Another option is 'convt' (short for convolution transpose), which is adopted by [matconvnet](http://www.vlfeat.org/matconvnet/mfiles/vl_nnconvt/). \n", "I think `transpose_conv2d` or `conv2d_transpose` are the cleanest names.  `transpose_deconv2d` implies it is the inverse transpose of `conv2d`.  `convt` is clever but very easy to mistake for `conv`.\n", "conv2d_transpose SGTM, it would put it jut after conv2d in alphabetical ordering. We should emphasize in the doc that it's what's often referred as 'deconv' or 'deconvolution', so that it shows up if anyone searches the docs for those terms.\n", "The current implementation crashes the Python kernel when given an output shape with a -1 in the first dimension. It would be helpful to throw a Python error and document this limitation, as ops produced by similar functions (eg conv2d) handle the scenario just fine.\n", "@jeffschecter: What kind of error do you get with the kernel crash?  This may be the same thing as #449, in which case it's fixed in 5de908567355337fdebd997fb5c60993cbe9ba2e and will be part of 0.6.0 soon.  That is, it should still generate exceptions, but will no longer crash the process.\n", "@jeffschecter: Oops, no, it's an independent bug.  I'll fix that now.  Thanks for the catch.\n", "There are many algorithms to deconvolve an image. For easier extensibility in the future and to avoid being locked in a specific implementation, the public API should add a parameter to reflect which method is used and call the corresponding internal function.\n`def deconv2d(value, filter, output_shape, strides, padding = 'SAME', method = 'transpose', name = None)`\n`def _deconv2d_transpose(value, filter, output_shape, strides, padding = 'SAME', name = None)`\n\nhttps://reference.wolfram.com/language/ref/ImageDeconvolve.html\nhttp://mathworld.wolfram.com/Deconvolution.html\n", "@futurely: I'm not sure what you mean by those references, since they are to actual deconvolution ops.  There are many ways to perform deconvolution, but the transpose of convolution is not one of them. \n", "For example, if blind deconvolution is implemented too, users can switch between different deconvolution algorithm with the following code. It's stylistic difference but may be a bit easier to use than to change potentially multiple occurrences of `deconv2d_transpose` and `deconv2d_blind`.\n\n``` python\nmethod = 'blind'\n# method = 'transpose'\ndeconv2d(..., method = method)\n```\n", "@futurely: Feature requests about deconvolution should go in a separate issue.  This thread is about the transpose of convolution, which is unrelated.\n", "The transpose-convolution operator already exists in TF, I think it is one of the conv_2d_backprop_*() functions.  If we were to give it another name as part of exposing it in the api, I'd prefer conv_2d_transpose or some such and having documentation that some sources mistakenly refer to that op as deconvolution.  I think we should not contribute to the misuse of the deconvolution term -- it leads to things like futurely@'s confusion.\n", "Yep, Vincent and I agreed on `conv2d_transpose` earlier in the thread.\n", "How do you use deconv2d for upsampling?\n", "@ry: Questions like that are better suited to stackoverflow.  This thread should stay focused on the  missing `conv2d_transpose` op.  (However: it isn't related to upsampling, you may be looking for `tf.resize_bilinear` and friends).\n"]}, {"number": 255, "title": "CUDA_ERROR_NO_DEVICE", "body": "I just installed TensorFlow for Ubuntu using the instructions on the [website](http://www.tensorflow.org/get_started/os_setup.md).\n\nIf I open a session in python:\n\n``` python\nimport tensorflow as tf\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\n```\n\nI get these warnings:\n\n``` sh\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nE tensorflow/stream_executor/cuda/cuda_driver.cc:466] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:86] kernel driver does not appear to be running on this host (joao): /proc/driver/nvidia/version does not exist\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: \nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\n```\n\nWhat have I done wrong?\n", "comments": ["Do you have a GPU in your machine that you want to use?  \n\nIf not:\n1) consider using the CPU binary, not the GPU binary\n2) you should be able to ignore these warnings.  they are helpful for debugging for those who do want to use GPUs.\n", "I do have GPU and I want to use it.\n\nOn Tue, Nov 17, 2015 at 6:46 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Do you have a GPU in your machine that you want to use?\n> \n> If not:\n> 1) consider using the CPU binary, not the GPU binary\n> 2) you should be able to ignore these warnings. they are helpful for\n> debugging for those who do want to use GPUs.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/255#issuecomment-157449882\n> .\n", "Then that means that our library is unable to detect your GPU.\n\n1) What GPU do you have?\n2) What cuda library version do you have installed?\n", "My GPU is this one: NVIDIA Corporation GF108GL\nFor the cuda I first installed cuda-7.5 but then I had a few troubles and therefore installed cuda-7.0 . Maybe I messed something up when installing the cuda.. I will look more into it\n", "Just FYI, Quadro 600 might be a little bit old for running TensorFlow... We officially support cuda compute capability 3.5 (and 3.0 seems to be working well too). Quadro 600 is at 2.1, so there might be a few bumps.\n\nIf you have cuda installed, running the device query binary from nvidia's samples would usually help checking if the GPU is properly detected.\n", "Ok I did some cleaning and rebooting and now my GPU is found. But I guess it will be ignored because it's too old like @Yangqing pointed out. I have these warnings:\n\n``` sh\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: \nname: Quadro 600\nmajor: 2 minor: 1 memoryClockRate (GHz) 1.28\npciBusID 0000:01:00.0\nTotal memory: 1023.19MiB\nFree memory: 734.32MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:611] Ignoring gpu device (device: 0, name: Quadro 600, pci bus id: 0000:01:00.0) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.5.\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\n```\n\nNow I know what I have to ask for x-mas. Thank you all\n", "https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658 if you're comfortable building from source -- may allow you to run on your K600 GPU, though it may not be that much faster than CPU.\n\nClosing this for now -- feel free to re-open if there's anything else you need!\n", "so 2.1 capability devices won't be much faster than CPU?\n", "what if I don't want to use a GPU but I just want to use CPU?\n", "Simply install the CPU version of Tensorflow . Or follow the link https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html.\n", "I had a problem: failed call to cuInit: CUDA_ERROR_NO_DEVICE. And root of problem was enviroment varible CUDA_VISIBLE_DEVICES = 1. One is ID, not count. Switch it to 0.", "I had that same problem. I think the issue was, that I used the AMI from a different ec2 instance and I had to run that command again: \r\n`sudo apt-get install -y linux-headers-$(uname -r)`\r\n", "Just to expand on what @Alek-dr mentioned, if on your system the environment variable **CUDA_VISIBLE_DEVICES** is empty or set to any value other than IDs of GPUs on your system then you might encounter this issue. On my system `echo $CUDA_VISIBLE_DEVICES=NoDevFiles `for some reason. Bigger problem? May be.\r\n\r\nI have two GPUs installed so setting `export CUDA_VISIBLE_DEVICES=0,1` fixes the issue and  TensorFlow is able to use both of them.", "Setting the `CUDA_VISIBLE_DEVICES` variable to `0,1` doesn't work for me with 1 GPU and tensorflow installed with virtualenv python.", "@Deejep As I said in my previous comments, I have 2 GPUs per node in my cluster that is why I set it to  0,1. For single GPU per node, `CUDA_VISIBLE_DEVICES=0` should work.", "Just fixed the issue. You must check if the script you are running doesn't set the variable to \"\".", "I think nvidia-driver is not working.\r\n\r\nYou have to check command `nvidia-smi` \r\n\r\nIf these command is not working, you have reboot your computer.\r\n\r\nMaybe `nvidia-smi` works correctly.", "This doesn't solve my issue. I ran `tf.config.list_physical_devices()` and I still can't see the GPU displayed, despite that error message is gone."]}, {"number": 254, "title": "Tensorflow on Raspberry Pi", "body": "Hi,\nIs it possible to install Tensorflow on a Raspberry Pi? OS Ubuntu Mate, Python2.7\n", "comments": ["We have not tried -- if you (or someone else) gives it a shot, let us know!\n", "Quick notes from a quick look...\n1. According to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md a source build requires Bazel. There is currently a live thread on this in https://github.com/bazelbuild/bazel/issues/606 /cc @vmayoral\n2. re other dependencies - os_setup.md mentions 'python-numpy swig python-dev'; of these, the scariest looking is numpy but http://raspberrypi.stackexchange.com/questions/8308/how-to-install-last-scipy-version-on-raspberry-pi and http://www.pyimagesearch.com/2015/02/23/install-opencv-and-python-on-your-raspberry-pi-2-and-b/ seems optimistic...\n\nAnyone get any further?\n", "Thanks @danbri for summarizing it. I'm currently trying different things but there seems to be serious issues with `bazel` and ARM.\n\nIt's great seeing several developers interested on the Pi and although the hardware has constrained resources, the interest seems relevant. Hope Google reacts here.\n", "Nearby w.r.t. bazel, #326\n", "https://github.com/bazelbuild/bazel/issues/606 is looking more promising than last week...\n", "Some progress at https://github.com/tensorflow/tensorflow/issues/445.\n", "On the Bazel build part, http://cudamusing.blogspot.com.au/2015/11/building-tensorflow-for-jetson-tk1.html has instructions for an Arm32 build that may work on the Pi.\n", "Would love to see this too. Is it possible to just run TensorFlow compiled things on Raspberry Pi 2?\n", "Hi @nlothian, I follow this Jetson build step, use the whl file to install on RPi, but Tensorflow will include CUDA lib so file. So, it is not work.\n", ":+1:\n", "@philipz I meant those instructions maybe helpful for getting _Bazel_ building on the Pi as a necessary step towards getting TensorFlow working. If you got past that point it maybe worth looking at https://github.com/tensorflow/tensorflow/issues/445 where the specific problems around that were being tracked.\n\nYou shouldn't be linking against CUDA for a Pi build!\n", "Just to spread some work that's been done in #445, we've managed to get TensorFlow compiled and running on the Raspberry Pi running Raspbian (although there hasn't been extensive testing yet). I've posted an unofficial pre-built Python wheel as well as step-by-step instructions on building from source yourself [here](https://github.com/samjabrahams/tensorflow-on-raspberry-pi).\n", "Thanks! I can confirm that it works with Ubuntu MATE on a Pi 3.\n", "I'm also successfully running @samjabrahams 's build on a Pi 3 (Linux raspberrypi 4.1.19-v7+ #858 SMP Tue Mar 15 15:56:00 GMT 2016 armv7l GNU/Linux).\n\nhttps://twitter.com/danbri/status/709889859029213184\n", "Since #445 is now closed, I'm going to post progress and updates for running TensorFlow on Raspberry Pi here. I did some [very basic benchmarking on the Inception-v3 model](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/tree/master/benchmarks/inceptionv3) to explore whether the build is working properly. Interestingly, the Pi appears to have similar performance to a MacBook Pro when it comes to using a compiled C++ model, but _much_ worse when using the Python version (granted, the C++ version also runs slower on the Mac, which raises a separate set of questions). \n\nI haven't gotten a good look at how Python binds to C++ in the code, but I'll do my best to figure out what's going on. My hope is that there is something screwy on the Pi that can be fixed which would lead to an order-of-magnitude time increase. I don't have much experience with C++ and its compile settings, so any ideas would be much appreciated!\n\n@petewarden - you mentioned [here](https://github.com/tensorflow/tensorflow/issues/445#issuecomment-196021885) that you thought some slowdown could be due to the compiler not using NEON options. Do you think that's worth exploring here?\n\n@danbri - Could you send over the pixel dimensions of the pictures captured and used by your Raspberry Pi camera? I want to try testing images around that size to see if I end up with similar run-times\n", "All I had time to try so far was default output of raspiutil via the Python wrapper code. I started trying to build the C++ but didn't finish yet. For realtime camera access (staying within python) I believe http://picamera.readthedocs.org/en/latest/api_camera.html would be the way to go. The camera needs some warmup time to judge light balance, I read.  And I didn't break down the python timings yet either into setup vs inference. Anyway:\n\ntensorflow/tensorflow/models/image/imagenet $ \n\ntime raspistill  --timeout 3 --nopreview -o ~/Desktop/headless.jpg\n\nreal    0m0.765s\nuser    0m0.010s\nsys 0m0.030s\n\ntime python classify_image.py --model_dir $HOME/imagenet/ --image_file ~/Desktop/headless.jpg --num_top_predictions 5 2>/dev/null \nhourglass (score = 0.47746)\nlampshade, lamp shade (score = 0.19448)\ntable lamp (score = 0.06217)\nfour-poster (score = 0.01556)\nwall clock (score = 0.00718)\n\nreal    0m33.291s\nuser    0m53.120s\nsys 0m2.420s\n\nfile ~/Desktop/headless.jpg \n/home/pi/Desktop/headless.jpg: JPEG image data, Exif standard: [TIFF image data, big-endian, direntries=10, height=0, manufacturer=RaspberryPi, model=RP_OV5647, xresolution=156, yresolution=164, resolutionunit=2, datetime=2016:03:18 11:22:43, width=0], baseline, precision 8, 2592x1944, frames 3\n\nFWIW here's the picture http://imgur.com/UbsYZYC (upside down and truncated salt/pepperpots, so 'hourglass' was close ;)\n\nI'm also curious about the Python wrapper being slower than plain C++ since I thought Python offloaded all TF's heavy lifting to compiled code anyway. Is it possible it's running a different build of the TF core in one case?\n", "Thanks @danbri- I'm definitely going to take a closer look at the Python and C++ Inception code to see if there are any obvious differences in implementation between the two. I ran your image through my RPi directly in Raspbian and got the same results as I did with smaller images. My gut tells me that the Pi can do better that the performance we're getting, but I'm not sure if I should be looking at compiler options first or dive into the code first.\n\nI also checked out memory usage to cover more bases- doesn't appear to be an issue.\n", "Just to share the Python vs C++ links for others:\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc\n", "@samjabrahams ... sorry I've not had time to try your new binaries yet. Have you had any more insights into the speed issue?\n", "@danbri - Not much yet: I've been low on time myself as well. My current goal is to get Bazel 2 running on a RPi, then put out new binaries for the 0.8 release once the candidate has gone gold. Afterward, hopefully I'll have some time to dig deeper into which operations are causing headaches on the Pi.\n", "Hi, \nI am trying to install tensorflow on my RPi 2 with ubuntu 14.04 image. \nI installed @samjabrahams [package](https://github.com/samjabrahams/tensorflow-on-raspberry-pi) (Thanks for the effort btw). I encountered no error. \nHowever when I'm importing tensorflow in python I run to this error:\n\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n>     from tensorflow.python import *\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 45, in <module>\n>     from tensorflow.python import pywrap_tensorflow\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n>     _pywrap_tensorflow = swig_import_helper()\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n>     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n> ImportError: /usr/lib/arm-linux-gnueabihf/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n\nI do have \"libstdc++.so.6\" and \"libstdc++.so.6.0.19\" in /usr/lib/arm-linux-gnueabihf/ and I have gcc4.8 installed. \nHave you tried this package on RPi 2? \n\nAny advice is appreciated. \nThanks\n", "does http://stackoverflow.com/questions/20357033/how-to-fix-program-name-usr-lib-x86-64-linux-gnu-libstdc-so-6-version-cxx help?\n", "thanks. I tried it but didn't help. Same error.\nI did this. \n`export LD_LIBRARY_PATH=/usr/lib/gcc/arm-linux-gnueabihf/4.8/:$LD_LIBRARY_PATH`\n`export LD_LIBRARY_PATH=/usr/lib/arm-linux-gnueabihf/:$LD_LIBRARY_PATH`\n`sudo ldconfig`\nFirst two lines add the path to where an instance of libstdc++.so.6 exist.\n", "I can confirm that @samjabrahams instructions for the RPi install also works for an Odroid Xu4 running Ubuntu 14.04, and while it did require building from source, it only took an hour or two to do so. \n", "@austinsteamboat how is the speed? e.g. running https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py\n", "@danbri - I ran that test case on the Odroid and my laptop, there was no noticeable difference in evaluation time, definitely same order of magnitude. Looking at resource draw, it takes 4 cores at peak and ~500 MB of RAM when evaluating that imagenet model. \n\nI'm building custom classifiers one basic perceptron type model and a CNN and I can say the odroid runs 5x slower with the perceptron case (which is still plenty fast actually) and the CNN actually runs at about the same rate on both machines (really slow). When the CNN does run on the Odroid it eats between 4 and 7 of the 8 available cores and consumes ~1.9 GB of the 2 GB of RAM. I still have the swap partition turned on from the install and from what I've seen that's probably a good thing to have when running big CNNs like that. All in all pleasantly surprised by the performance so far. \n", "@austinsteamboat Thanks for the info! I just finished putting the 0.8.0 pip release out for the Raspberry Pi, so I'm hoping to have a few spare minutes over the next few weeks to check out some of the speed issues. From your info, it sounds like the problem could be at the operating system level, as Raspbian is still a 32-bit OS. The Odroid is certainly a more powerful machine than the Pi, but if the ImageNet model only takes 4 cores on both, I don't see the Pi lagging behind that badly.\n\nHopefully I (or someone) can find something a bit more conclusive and precise for this.\n", "I confirm that with @samjabrahams whl I can get the \"panda\" and \"uniform\" result in 15s(use python example. not sure the cpp example). Thanks a lot! building tensorflow and bazel on raspberry is not a easy job......I tried but blocked at the javac report out of memory(I already change the swap file to 2GB but I found the javac even does not consider any swap space).\n\n1) is it possible to put the bazel whl on github also? we need to build our own code with tensorflow.\n2) is it possible to improve the performance to such as 1s with GPU? I'm trying to let it run for a robot with ROS.if one second it can report the object it see, then it can be fun. Too long time is not fun :-\uff09\n", "@NickQian Glad the TensorFlow binaries are working for you. Couple quick responses:\n1. At this time, I'm unable to distribute the Bazel binaries directly, mainly because I don't know the exact files that need to be packaged up (I believe there's more to it than just a single binary file). If someone could help me out with this, I'd be able to do so- I just don't have much (if any) time to dedicate to this at the moment.\n   - To follow up: are you following the instructions I wrote exactly? Instead of modifying the swap file `/etc/dphys-swapfile`, the instructions call for mounting an additional USB drive. Theoretically, swap space should be swap space, but I imagine putting more bottlenecks on the SD card isn't helpful\n   - Did you edit the file `scripts/bootstrap/compile.sh` inside of the Bazel repository? The `javac` command used by Bazel is in that script, and you may need to specify the maximum heap size there (as instructed in the guide)\n2. The question of speed is where the conversation is right now. For the base Inception model, my fear is that we're going to see slow performance until a 64-bit operating system is available on RPi. However, now that [8-bit quantization is possible](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/), we may be able to prep some models to run more effectively on the Pi :)\n", "Sam, thanks for your great work.  Have you compiled & shared binaries for (raspberry pi)Tensorflow for Python3?\n", "@indiangolfer It's on my to-do list! I've streamlined my compilation process a touch, so I'll try to get the Python 3 binaries out before the end of the weekend.\n", "That would be great.  Thanks\n", "@indiangolfer Let me know if [this binary](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/tag/v0.8.0) works for you! I just updated the readme and guide to include Python 3 installation instructions as well.\n", "Hi Sam.  Perfect.  It worked perfectly on both Pi 2 and 3.  Many thanks.\n", "Hi @samjabrahams. Thanks for your GREAT job. I use your binary file to make a Docker image for RPi. [https://github.com/philipz/rpi-tensorflow](https://github.com/philipz/rpi-tensorflow)\n", "Great work @samjabrahams.\nHi @indiangolfer, could you please tell how did you install binary on RPi2B?   \n\nI follow below steps on RPi2B with Raspbian Wheezy:\nFor Python 2.7\n\n```\nsudo apt-get install python-pip python-dev\n\nwget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/raw/master/bin/tensorflow-0.8.0-cp27-none-linux_armv7l.whl\n\nsudo pip2 install tensorflow-0.8.0-cp27-none-linux_armv7l.whl\n```\n\ntensorflow installed successfully\n\n```\nInstalling collected packages: tensorflow\nSuccessfully installed tensorflow-0.8.0\n```\n\nWhen I test it \n\n```\npython\n>>> import tensorflow as tf\n```\n\nGot following error\n\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 45, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: /usr/lib/arm-linux-gnueabihf/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so)\n```\n", "Well Tensorflow binaries works on RPi2B + Rasbian Jessie. Thank you @samjabrahams.\n", "A couple recent changes are throwing a wrench into compiling full TensorFlow binaries on RPi. [These lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/platform.h#L40-L45) cause `IS_MOBILE_PLATFORM` to be defined when compiling RPi, which in turn causes errors in some of the image processing files (such as [core/platform/png.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/png.h#L23-L26) and [core/platform/gif.h](https://github.com/tensorflow/tensorflow/blob/b11907ed7292e996b911ae64cc5d97f60c41518e/tensorflow/core/platform/gif.h#L23-L24)). It also prevents non-32-bit operations from being defined [in this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/register_types.h#L107-L131). This may not be a bad thing, considering Raspbian is still a 32-bit OS, but it causes quirks when people try to use certain 64-bit-only Operations.\n\nI imagine that the current setup works when using the [Makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile), so I don't want to screw anything up. That said, any ideas on not defining `IS_MOBILE_PLATFORM` when compiling on Raspberry Pi? The best thing I could find was [this answer on Stack Overflow](http://raspberrypi.stackexchange.com/questions/754/how-can-i-detect-that-im-compiling-for-raspberry-pi)\\- it seems a little clunky, so I didn't want to jump to making a PR with it without asking first.\n\nIn the meantime, the current fix is to delete [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/platform.h#L45) before doing the Bazel build on the Pi. If there's no clean way to fix this up here, it's no big deal to tweak that file for builds moving forward.\n", "@samjabrahams perhaps this situation merits a new dedicated bug report, as it is a very specific matter?\n", "@danbri You're right- I've opened up an issue for this #3469\n", "HI, would you like to make version10 of tensorflow on Pi? Because i want to try object recognition.Thanks.\n", "Hi @puchiko939393, I'm planning on building wheels for TensorFlow 0.10 as soon as it's officially released. There is only a release candidate at this time (0.10RC0), so I'm holding off until then. Feel free to use the [guide to build it yourself](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md) if you simply can't wait!\n", "https://www.oreilly.com/learning/how-to-build-a-robot-that-sees-with-100-and-tensorflow?twitter=@bigdata also reports success with TF on Pi, seemed worth passing along here.\n", "Looks like https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile wasn't shared here. Non-bazel, makefile-based compilation; there's a \"Raspberry Pi\" section in the readme, plus https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples from @petewarden and @vrv has some nice examples. Main drawback seems to be lack of language bindings (esp. python). (and no GPU, but that's pretty much a given for Pi anyway)\n\nedit: \n- https://github.com/tensorflow/tensorflow/issues/4680 helped me get the examples running\n", "Hi,\n\nCan I install TensorFlow on the Pi zero.\n\nI tried **\"sudo pip2 install tensorflow-0.9.0-cp27-none-linux_armv7l.whl\"**\n\nbut it returned \n **tensorflow-0.9.0-cp27-none-linux_armv7l.whl is not a supported wheel on this platform.\nStoring debug log for failure in /root/.pip/pip.log**\n\nI haven't try from Docker image yet. is it possible ?\n\nthank you and regards,\nKhoa\n", "Time for a benchmark update- I posted the [latest Inception-v3 benchmarks for version 0.11.0 here](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/tree/master/benchmarks/inceptionv3), and I'll update them again once 0.12.0 is official.\r\n\r\nHighlights:\r\n\r\n* Inception-v3 average run time is now under 2 seconds (after a few warmup runs)\r\n* Benchmark script is slightly fancier, giving best/worst/average times as well as flags to modify the number of samples and warmup runs\r\n* It seems the majority of the time doing the classify_image.py script is spent importing `tensorflow` and building the graph from disk\r\n\r\nI'm still compiling the Python 3 binaries for 0.11.0, and I need to update the instructions to make sure they work on a fresh copy of Raspbian. Hopefully get all that done by the end of the weekend.", "Not strictly related but new optimized cpu and gpu ARM kernels could be available. See https://github.com/ARM-software/ComputeLibrary/issues/7", "[I've released unofficial Raspberry Pi binaries for version 1.0.1 here.](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/tag/v1.0.1)", "Dear samjabrahams \r\nThank you for all your effort.\r\nDoes the Raspberry Pi binaries apply to clone (particularily Orange Pi+ 2e - AllWinner H3 CPU + Mali 400 GPU)?\r\nI'm trying to build according to your instruction but I'm stuck with compiling Bazel (java zero fails with Internal Error (os_linux_zero.cpp:254) unhandled signal 11 using OpenJDK Zero VM (25.121-b13) build 1.8.0_121-8u121-b130ubuntu1.16.04.2-b13). The OS is armbian.\r\nIn case above binaries do not apply, do you know if switching back to Java 1.7 may help?\r\n\r\nKind regards,\r\n\r\nP.", "@pczekalski - I'm moving your question to my repo in [issue 90](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/issues/90), as this thread probably isn't the right place to discuss non-RPi TensorFlow.", "[Unofficial Raspberry Pi binaries for version 1.1.0](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/tag/v1.1.0).", "Closing since Raspberry Pi binaries exist.", "@samjabrahams what are your plans w.r.t. the pi binaries? is the build process any easier than last year? ", "@danbri - The process is a little smoother than it was at the end of last year, but it's still pretty much the same (in terms of needing to compile on-device). We don't need to compile `protoc` from scratch anymore, and there's less finagling overall- there's basically only five changes you need to make to Bazel and TensorFlow, total.\r\n\r\nI haven't had the time to put much effort into trying out cross-compilation options, unfortunately. In addition, I'm starting a new job which means that I'll need to talk to HR about continuing my efforts (I'm optimistic, but I don't make any assumptions).\r\n\r\nOn a brighter note, I'm excited to try out these [MobileNets](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md) on-device!", "@danbri  @samjabrahams \r\n\r\nThis is, reply to @samjabrahams  comment on Mar 17, 2016. Have you guys tested if the python and c++ versions have similar performance. When I ran object detection model using python, I can clearly see only one core being used and it takes 6 seconds to process 1 image(640x480 res). I tried configuring the intra_ops config parameter, but no luck. I think by default tensorflow uses all cpu cores. I am not sure if I am the only one facing performance issues..\r\n", "My experience was similar - Python was painfully slow\n\nOn 8 Aug 2017 11:12 pm, \"Kranthi\" <notifications@github.com> wrote:\n\n> @danbri <https://github.com/danbri> @samjabrahams\n> <https://github.com/samjabrahams>\n>\n> This is reply to @samjabrahams <https://github.com/samjabrahams> comment\n> on Mar 17, 2016. Have you guys tested if the python and c++ versions have\n> similar performance. When I ran object detection model using python, I can\n> clearly see only one core being used and it takes 6 seconds to process 1\n> image(640x480 res). I tried configuring the intra_ops config parameter, but\n> no luck. I think by default tensorflow uses all cpu cores. I am not sure if\n> I am the only one facing performance issues..\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/254#issuecomment-321096131>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAKZGVAOvcaBmv2UIpysKjR2CJqXKagtks5sWN02gaJpZM4Gjqny>\n> .\n>\n", "@danbri . thanks for the reply. I hope they fix it in official release.. ", "download the bazel binary for armv7l(pi 3) with this link below\r\nhttps://drive.google.com/open?id=0B7NeIJaVzSxIZmZHUnVzZjBjVDg", "whatt about Mali 400 and more and tensorflow"]}, {"number": 253, "title": "Need help, how to choose one column of a tensor", "body": "predict_op = tf.argmax(py_x, 1) # at predict time, evaluate the argmax of the logistic regression\n\nhere what to do if I want to choose one column of py_x, like py_x[1] ?\n\nAlso it would be of great help if we have example of showing how to do text classification using tensorflow( sparse feature). \n\nExample of showing how to evaluate using auc instead of  precision is greatly appreciated.\n", "comments": ["I think tf.slice(...) might be what you want for getting 1 column.  Let us know if that doesn't work!\n\nYou might also want to follow https://github.com/tensorflow/tensorflow/issues/206 \n\nAs for text classification: I'm sure that will be added over time (either by us or externally).\n"]}, {"number": 251, "title": "Install tensorflow from source ", "body": "OS: xUbuntu\npython-2.7\ngcc 4.8\nI have installed python-dev\n\nwhen I build from source using \n\n```\nbazel build --jobs 2 -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\n```\n\nI got the following errors:\n\n```\naurora@aurora-Z170X-UD5:~/workspace/tensorflow/tensorflow$ bazel build --jobs 2 -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\n....\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/python/client/tf_session_helper.cc:\nIn file included from tensorflow/python/client/tf_session_helper.cc:1:0:\n./tensorflow/python/client/tf_session_helper.h:6:31: fatal error: numpy/arrayobject.h: No such file or directory\n #include \"numpy/arrayobject.h\"\n                               ^\ncompilation terminated.\nERROR: /home/aurora/workspace/tensorflow/tensorflow/tensorflow/python/BUILD:698:1: C++ compilation of rule '//tensorflow/python:tf_session_helper' failed: gcc failed: error executing command \n  (cd /home/aurora/.cache/bazel/_bazel_aurora/67e9d46b2d6eaad7004805288b920919/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/home/aurora/software/anaconda2/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -I/usr/include/python2.7 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.d -fPIC -c tensorflow/python/client/tf_session_helper.cc -o bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command \n  (cd /home/aurora/.cache/bazel/_bazel_aurora/67e9d46b2d6eaad7004805288b920919/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/home/aurora/software/anaconda2/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -I/usr/include/python2.7 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.d -fPIC -c tensorflow/python/client/tf_session_helper.cc -o bazel-out/local_linux-opt/bin/tensorflow/python/_objs/tf_session_helper/tensorflow/python/client/tf_session_helper.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 600.219s, Critical Path: 0.40s\n```\n\nI have installed ananconda and the path of  numpy/arrayobject.h is \n\n```\n/software/anaconda2/lib/python2.7/site-packages/numpy/core/include/numpy\n```\n", "comments": ["You should add\n-I/software/anaconda2/lib/python2.7/site-packages/numpy/core/include\nas a compiler flag in tools/cpp/CROSSBUILD in bazel.\n\nBTW, the truth that Bazel do not recongnize the global env-vars was about to KILL me.\n", "Can you paste your env vars here?\n", "@ebrevdo  the env vars \n\n```\naurora@aurora-Z170X-UD5:~/Desktop$ printenv\nXDG_VTNR=7\nLC_PAPER=zh_CN.UTF-8\nLC_ADDRESS=zh_CN.UTF-8\nXDG_SESSION_ID=c2\nXDG_GREETER_DATA_DIR=/var/lib/lightdm-data/aurora\nSELINUX_INIT=YES\nLC_MONETARY=zh_CN.UTF-8\nSESSION=xubuntu\nGPG_AGENT_INFO=/run/user/1000/keyring-mjj0V2/gpg:0:1\nGLADE_PIXMAP_PATH=:\nTERM=xterm\nXDG_MENU_PREFIX=xfce-\nSHELL=/bin/bash\nWINDOWID=62920039\nLC_NUMERIC=zh_CN.UTF-8\nUPSTART_SESSION=unix:abstract=/com/ubuntu/upstart-session/1000/1741\nGNOME_KEYRING_CONTROL=/run/user/1000/keyring-mjj0V2\nUSER=aurora\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.axa=00;36:*.oga=00;36:*.spx=00;36:*.xspf=00;36:\nLC_TELEPHONE=zh_CN.UTF-8\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib:/software/anaconda2/lib/python2.7/site-packages/numpy/core/include/numpy:/usr/local/cuda/lib64:/usr/local/lib:\nXDG_SESSION_PATH=/org/freedesktop/DisplayManager/Session0\nGLADE_MODULE_PATH=:\nXDG_SEAT_PATH=/org/freedesktop/DisplayManager/Seat0\nSSH_AUTH_SOCK=/run/user/1000/keyring-mjj0V2/ssh\nDEFAULTS_PATH=/usr/share/gconf/xubuntu.default.path\nSESSION_MANAGER=local/aurora-Z170X-UD5:@/tmp/.ICE-unix/1861,unix/aurora-Z170X-UD5:/tmp/.ICE-unix/1861\nXDG_CONFIG_DIRS=/etc/xdg/xdg-xubuntu:/usr/share/upstart/xdg:/etc/xdg:/etc/xdg\nDESKTOP_SESSION=xubuntu\nPATH=/home/aurora/software/anaconda2/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\nLC_IDENTIFICATION=zh_CN.UTF-8\nPWD=/home/aurora/Desktop\nJOB=dbus\nJAVA_HOME=/home/aurora/software/jdk1.8.0_25\nGNOME_KEYRING_PID=1739\nLANG=en_US.UTF-8\nGDM_LANG=en_US\nMANDATORY_PATH=/usr/share/gconf/xubuntu.mandatory.path\nLC_MEASUREMENT=zh_CN.UTF-8\nIM_CONFIG_PHASE=1\nGDMSESSION=xubuntu\nSESSIONTYPE=\nSHLVL=1\nHOME=/home/aurora\nXDG_SEAT=seat0\nLANGUAGE=en_US\nUPSTART_INSTANCE=\nUPSTART_EVENTS=started xsession\nLOGNAME=aurora\nDBUS_SESSION_BUS_ADDRESS=unix:abstract=/tmp/dbus-AzkSJrxWxZ\nXDG_DATA_DIRS=/usr/share/xubuntu:/usr/share/xfce4:/usr/local/share/:/usr/share/:/usr/share\nLESSOPEN=| /usr/bin/lesspipe %s\nTEXTDOMAIN=im-config\nINSTANCE=\nUPSTART_JOB=startxfce4\nXDG_RUNTIME_DIR=/run/user/1000\nDISPLAY=:0.0\nGLADE_CATALOG_PATH=:\nXDG_CURRENT_DESKTOP=XFCE\nCUDA_ROOT=/usr/local/cuda\nLESSCLOSE=/usr/bin/lesspipe %s %s\nLC_TIME=zh_CN.UTF-8\nTEXTDOMAINDIR=/usr/share/locale/\nLC_NAME=zh_CN.UTF-8\nXAUTHORITY=/home/aurora/.Xauthority\nCOLORTERM=xfce4-terminal\n_=/usr/bin/printenv\n```\n", "@zhangchen-qinyinghua \nI follow your instruction and add this line\n\n```\ncxx_builtin_include_directory: \"/home/aurora/software/anaconda2/lib/python2.7/site-packages/numpy/core/include/numpy\"\n```\n\nto file \n\n```\n/usr/local/lib/bazel/base_workspace/tools/cpp/CROSSTOOL\n```\n\nI also add it to path\n\n```\naurora@aurora-Z170X-UD5:~/workspace/tensorflow/tensorflow$ echo $PATH\n/home/aurora/software/anaconda2/bin:/usr/local/cuda/bin:/home/aurora/software/anaconda2/lib/python2.7/site-packages/numpy/core/include/numpy:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/cuda/bin:/home/aurora/software/jdk1.8.0_25/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\n```\n\nbut I still encounter  this problem\n\n```\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/python/client/tf_session_helper.cc:\nIn file included from tensorflow/python/client/tf_session_helper.cc:1:0:\n./tensorflow/python/client/tf_session_helper.h:6:31: fatal error: numpy/arrayobject.h: No such file or directory\n #include \"numpy/arrayobject.h\"\n                               ^\ncompilation terminated.\nERROR: /home/aurora/workspace/tensorflow/tensorflow/tensorflow/python/BUILD:698:1: C++ compilation of rule '//tensorflow/python:tf_session_helper' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 51 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\n```\n\nI don't know how to use bazel . Is there anything wrong in my operation.\nthanks\n", "Try removing `numpy` from the include dir. Use:\n\n`cxx_builtin_include_directory: \"/home/aurora/software/anaconda2/lib/python2.7/site-packages/numpy/core/include\"`\n", "@SyncedSynapse \nI got the same result after remove `numpy`\n", "Try \"sudo pip uninstall numpy\" and \"sudo pip install numpy\"\nIf that doesn't work, try \"sudo apt-get install python-numpy\"\n\nnumpy probably needs to be version 1.10.1 so check this with \"python\", \"import numpy as np\", \"np.__version__\" to see the version.\n", "@victorv \nI'm using anaconda which contains numpy.\nI did uninstall numpy and just executed `sudo apt-get install python-numpy`.\nThe problem disappeared.\nthanks \n", "After execute the command `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`,the generated file name is `tensorflow-0.5.0-py2-none-any.whl`  which is different from the tutorial `tensorflow-0.5.0-cp27-none-linux_x86_64.whl`.\nWhat's the different from this two files?\n", "Are the files different in contents? Even if they are different, it is likely the naming is just not correct on your machine. \n", "@victorv \nthanks \nThis issue can be closed \n"]}, {"number": 250, "title": "Out-of-source builds", "body": "The build instructions have us run bazel such that all the build artifacts are dumped right in the source directory. So are we stuck creating a bunch of .gitignore rules? Seems barbaric when the usual approach is to avoid polluting source directory with build artifacts in the first place.\n\nOn behalf of all of us who don't know anything about bazel, I'll ask: does it not support out-of-source builds? I spent some time looking, and trying to do a bazel build from somewhere other than the source directory, and haven't had any luck. \n", "comments": ["We could add a .gitignore for \"bazel-*\" in the short term -- would that be palatable?\n", "Thanks for the quick reply. I guess that will have to do.\n", "De-duping with https://github.com/tensorflow/tensorflow/issues/199\n", "FWIW, Bazel does dump its build artifacts in the source tree but rather creates symlinks to the directories that contain the build artifacts. We opened bazelbuild/bazel#555 to add a Bazel .gitignore to github/gitignore which can easily be used by projects that use Bazel.\n\nFrom what I understand, most build systems create directories in the source tree that contain the build artifacts. While Autotools supports out-of-tree builds (and some Autotools projects, such as gcc, even recommends this approach), the standard practice for most other build systems seems to be using `.gitignore` to exclude build artifacts.\n"]}, {"number": 249, "title": "Documentation: 'typo' in softmax explanation image", "body": "In the top part of the attached image - is it just me or is the **X** matrix written the wrong way around? In keeping with dot product notation, shouldn't this be:\n\n`y1 = W11 * x1 + W12 * x2 + W13 * x3 + b1`\n\nand so on for y2 and y3?\n\n![wrong](http://api.tensorflow.org/system/image/body/1707/softmax-regression-scalarequation.png)\n", "comments": ["I'm not sure if it's just me, but the image isn't rendering properly. Let me try to update it.\n", "Don't worry, we've actually fixed it already in the repo.  We're trying to update the site with it soon.\n\nhttps://github.com/tensorflow/tensorflow/issues/78 -- deduping\n"]}, {"number": 248, "title": "typo in placeholder docs", "body": "The example code provided in the docs for the placeholder op doesn't work: \n\n``` python\n  x = tf.placeholder(float, shape=(1024, 1024))\n  y = tf.matmul(x, x)\n\n  with tf.Session() as sess:\n    print sess.run(y)  # ERROR: will fail because x was not fed.\n\n    rand_array = np.random.rand(1024, 1024)\n    print sess.run(y, feed_dict={x: rand_array})  # Will succeed.\n```\n\nShould it be `x = tf.placeholder(\"float\", shape=(1024, 1024))`?\n", "comments": ["This is fixed in git, pending a push to the website.  Thanks for the report!\n"]}, {"number": 247, "title": "translate module not present in binary pip installation", "body": "The translate module (among others) is a part of source, but not the pip install. Is there a plan for adding it in the binary installation?\n", "comments": ["The translate modules are indeed currently not part of the install (they're currently more of a reference).\n\nWe're thinking about how to repackage some of the files in our codebase (e.g., tutorials, docs, example models, etc), so we'll have a better answer for you soon.  Thanks!\n", "The translate modules (as well as the other model resources) are for the most part just an example that can be moved / copied.  Some of them  had a few out-of-file helper modules that were not part of the pip package that would break during execution of say, translate.py, but we have added them to the module for our next release, to get things working as written.\n\nIn general, the pip package will probably just be for installing the core tensorflow python library, and any helper libraries to get our tutorials to work.\n\nUsers should still git clone the source repo to get access to examples, tutorials, etc, and should write their code assuming they are using the public API, where the public TensorFlow API is defined as what is documented at http://www.tensorflow.org/versions/master/api_docs/index.html\n", "Fixed in 0.6.0\n"]}, {"number": 246, "title": "Examples for loop control flow ops (Enter/Leave/NextIteration)", "body": "These are mentioned tantalisingly in the whitepaper, and code for them seems to be present (although omitted from the docs).\n\nIt's not clear at all (to me at least) how to use Enter/Leave/NextIteration to actually construct a loop -- an example would be really nice here :-)\n\nIn particular I'm interested in whether recurrent networks could be implemented using these constructs, and if there's a reason you chose not to use them in the examples. Is it a case of the looping stuff not being quite ready enough yet in the open source version? Is it that loop control flow doesn't perform as well?\n", "comments": ["Take a look at: https://github.com/tensorflow/tensorflow/issues/208\n", "Gotcha. Yeah fair enough in that case -- looking forward to the public API :)\n\nI'd still be interested in a simple example even if the API is subject to change, just to get a flavour of what (might) be to come.\n\nFor anyone else curious about this, have a look at the referenced MIT paper (http://csg.csail.mit.edu/CSGArchives/memos/Memo-271.pdf ) in particular figure 13. It looks like D == NextIteration, D_reset == Exit/Leave, and that \u2297 == Merge.\n\nThis all makes sense, although it seems you need some way to set up a cyclic connection in the graph, to feed the result of NextIteration back into the loop, and I can't figure out how to do that at the moment with the current python API.\n", "De-duping with https://github.com/tensorflow/tensorflow/issues/208\n"]}, {"number": 245, "title": "word2vec tutorial plot labels are incorrect", "body": "The plot produced by the word2vec tutorial looks like random words.\n\nIn tensorflow/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py line 223 (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py#L223), the code gets labels for the first 500 words/embeddings by extracting 500 keys from the word dictionary without respect to their indices, using:\n\n```\nlabels = list(dictionary.keys())[:plot_only]\n```\n\nLine 223 should probably be something like the following, since the each key's value is the word index:\n\n```\nlabels = list(sorted(dictionary, key=dictionary.get))[:plot_only]\n```\n", "comments": ["Glad someone also saw this issue.\nThe words I got in the outputted t-SNE plot were mostly obscure, and weren't anything like the ones shown in the tutorial example plot image.\n`labels = [reverse_dictionary[i] for i in xrange(plot_only)]`\nalso worked for me.\n", "@gouwsmeister has a fix out and we'll update the repo soon.  Thanks!\n", "Should be fixed at HEAD\n"]}, {"number": 244, "title": "Example word2vec.py", "body": "Error while executing python word2vec.py:\n\nfrom tensorflow.models.embedding import gen_word2vec as word2vec\nImportError: No module named embedding\n", "comments": ["\u697c\u58a8\u8ff9HK\n", "I have submitted a fix for this. Hopefully it'll go through code review and be accepted in the not too distant future.\n\nhttps://tensorflow-review.googlesource.com/#/c/1142/\n", "@khellan Thanks!\n", "@ZhangBanger It was fixed by Google before my patch was merged :)\n", "Does this mean you can run without building from source?\n", "That's a good question. I don't really know since I build from source due to some other changes I have. The current binary install is 0.6.0. I guess it should be included. \n", "This was fixed a while ago, should be fine in 0.7 and up.\n", ":+1:  danke\n"]}, {"number": 243, "title": "Wheel for binary installation is outdated; consider automating release process", "body": "[Installation instructions](http://tensorflow.org/get_started/os_setup.md) suggest\n\n```\npip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n```\n\nAs far as I can tell, content of this wheel is pre- 6b12d081d54b89869e26d9d99828f13de381761e.\n\nIn particular, this means that advanced mnist tutorial from trunk fails when run with this binary installation (even after applying workaround from #226):\n\n```\n$ python .../tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py\n...\nTraceback (most recent call last):\n  ...\n  File \".../tensorflow/g3doc/tutorials/mnist/mnist.py\", line 94, in loss\n    indices = tf.expand_dims(tf.range(batch_size), 1)\nTypeError: range() takes at least 2 arguments (1 given)\n```\n", "comments": ["We're just about there now with jenkins, thankfully.\n", "Please close this issue. Solution provided by: https://github.com/tensorflow/tensorflow/pull/1255\n"]}, {"number": 242, "title": "Minor change for tutorial.", "body": "Added a session close to the Interactive Session, so that no exceptions are thrown when a user tests later examples in the same kernel.\n", "comments": ["As noted in our contributing.md, currently our changes have to go through gerrit (which I see you've done, and we'll try to integrate soon).  Closing for now, thanks!\n"]}, {"number": 241, "title": "Minor change for read consistency. (Highlight MD Fix)", "body": "Changed {'t[i, j, k]'} to {`t[i, j, k]`} on line 21 to highlight text. \n", "comments": ["As noted in our contributing.md, currently our changes have to go through gerrit (which I see you've done, and we'll try to integrate soon).  Closing for now, thanks!\n"]}, {"number": 240, "title": "Mistake in matrix multiplication(http://tensorflow.org/tutorials/mnist/beginners/index.md)", "body": "When single layer network math is described you have following:\n\"\"\"If we write that out as equations, we get:\"\"\"\nafter that comes picture with weight multiplications, but there should be\nW_1,1_x_1 + W_1,2_x_2 + W_1,3_x_3 + b_1\netc..\nInstead there written:\nW_1,1_x_1 + W_1,2_x_1 + W_1,3_x_1 + b_1\netc..\nPlease correct me if I am mistaken.\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/185.  Already fixed in the source but not yet pushed to the website.\n"]}, {"number": 239, "title": "Basic Usage - Variables: incorrect variable name", "body": "The state variable is named 'var', but it is referred as 'state' afterwards.\nChanging this:\nnew_value = tf.add(**state**, one)\nupdate = tf.assign(**state**, new_value)\nTo this:\nnew_value = tf.add(**var**, one)\nupdate = tf.assign(**var**, new_value)\nSolves the problem.\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/236, which is itself a duplicate of an earlier bug.\n"]}, {"number": 238, "title": "Empty input to conv2d causes floating point exception", "body": "When empty data is passed into conv2d, the following is displayed, and the program quits:\n\n```\nFloating point exception (core dumped)\n```\n\nThis problem occurs on both Mac (10.9) and Linux (Ubuntu 12.04) in CPU mode (I haven't tried GPU). Here is code that reproduces the problem. \n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\nx = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\nconv1_weights = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\nconv = tf.nn.conv2d(x, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n\nwith tf.Session() as sess:\n    tf.initialize_all_variables().run()\n    sess.run([conv], {x: np.empty((0, 28, 28, 1))})\n```\n", "comments": ["Can you build from source with the flag \"-c dbg\" and provide the crash log?\n", "I think it's just that the kernel doesn't handle the empty input (e.g., input.NumElements() == 0).\n\nI guess we could check that input or filter are empty, and if so, to produce an empty tensor of the correct shape.\n", "We added a simple check here: https://github.com/tensorflow/tensorflow/commit/04f1932f053dd7865b191719b33860270461943a#diff-f9ae051d576c85eab133e77af84c1937R170\n\nLet us know if that doesn't solve the problem (at least for this op -- I'm sure there are others that will pop up).\n", "Thanks!\n"]}, {"number": 237, "title": "Error when running code from seq2seq translate model", "body": "Hi all,\n\nI have trained my model on an english to french database, as specified in the tutorial. And yet, when I run this command \n\n```\n./bazel-bin/tensorflow/models/rnn/translate/translate  --decode DECODE --train_dir /Users/User/train/\n```\n\nI get a pageful of errors like this:\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1024] rhs shape= [256]\n     [[Node: save/Assign_3 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Candidate/Linear/Bias, save/restore_slice_3)]]\n```\n\nWhat do I do?\n", "comments": ["My best guess: did you train with --size=256 ?  If so, you need to decode with the same set of command line flags as the one you trained with.\n\nThe error is basically saying that there is a size mismatch: the variable being assigned to is of size [256] but it expected [1024].  I don't see any sizes of '256' in the translate.py example, so that's why I suspect it was overridden via flags.\n", "This worked, thanks!\n"]}]