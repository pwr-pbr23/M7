[{"number": 5122, "title": "custom CUDA op example returns random values", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nNothing\n### Environment info\n\nOperating System:\n\n```\n$ uname -a\nLinux n-62-18-47 2.6.32-642.6.1.el6.x86_64 #1 SMP Wed Oct 5 08:48:31 CDT 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nInstalled version of CUDA and cuDNN: CUDA: 8, cuDNN 5.1\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -l /appl/cuda/8.0/lib64/libcud*\n-rw-r--r-- 1 sebo root 560184 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 sebo root     16 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 sebo root     19 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 sebo root 394472 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 sebo root 737516 Sep  1 14:31 /appl/cuda/8.0/lib64/libcudart_static.a\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) 5a5a25ea3ebef623e07fb9a46419a9df377a37a5\n2. The output of `bazel version` \n\n```\n$ bazel version\nBuild label: 0.3.2- (@non-git)\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 21 15:09:04 2016 (1477062544)\nBuild timestamp: 1477062544\nBuild timestamp as int: 1477062544\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nUsing the CUDA example from: https://github.com/tensorflow/tensorflow/tree/r0.11/tensorflow/g3doc/how_tos/adding_an_op\n\n1) compile example\n\n```\nexport TF_INC=/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/include\n\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc \\\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\n\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc \\\ncuda_op_kernel.cu.o -I $TF_INC -fPIC -L /appl/cuda/8.0/lib64 -L /appl/cudnn/v5.1-prod/lib64 -lcudart\n```\n\n2) edit `tensorflow.g3doc.how_tos.adding_an_op import cuda_op` to `import cuda_op` in `cuda_op_test.py`.\n### What other attempted solutions have you tried?\n- I tried a non CUDA example, worked fine.\n- I tried a diffrent cuda kernel (square operator) also failed.\n- I added `printf` to the kernel launcher and made sure it was executed.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n```\nCUDA_VISIBLE_DEVICES=3 python3 cuda_op_test.py\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:944] Found device 0 with properties:\nname: Tesla K40c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:02:00.0\nTotal memory: 11.25GiB\nFree memory: 11.15GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:965] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\nnot equal where =  (array([0, 1, 2, 3, 4]),)\nnot equal lhs =  [ 280541332  143397048 2031878174 1533025280 1612453930]\nnot equal rhs =  [6 5 4 3 2]\nF.\n======================================================================\nFAIL: test (__main__.AddOneTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"cuda_op_test.py\", line 31, in test\n    self.assertAllEqual(result.eval(), [6, 5, 4, 3, 2])\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 499, in assertAllEqual\n    np.testing.assert_array_equal(a, b)\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 813, in assert_array_equal\n    verbose=verbose, header='Arrays are not equal')\n  File \"/zhome/ff/2/77654/stdpy3/lib/python3.5/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError:\nArrays are not equal\n\n(mismatch 100.0%)\n x: array([ 280541332,  143397048, 2031878174, 1533025280, 1612453930], dtype=int32)\n y: array([6, 5, 4, 3, 2])\n\n----------------------------------------------------------------------\nRan 2 tests in 0.213s\n\nFAILED (failures=1)\n```\n\n---\n\nIt looks like the output just contains random memory. Perhaps the GPU memory isn't copied back to the host memory.\n", "comments": ["@AndreasMadsen : That does seem fishy, but I happened to notice this line in your pasted output:\n\n```\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\n```\n\nCould that indicate some problem with the driver installation?\n", "@asimshankar I don't know, it is not my setup. Is there some Nvidia tool I can use to diagnose it?\n", "Here is the `nvidia-smi` output:\n\n```\nSun Oct 23 09:17:43 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40c          On   | 0000:02:00.0     Off |                    0 |\n| 23%   24C    P8    19W / 235W |     22MiB / 11519MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K20c          On   | 0000:03:00.0     Off |                    0 |\n| 30%   20C    P8    16W / 225W |     13MiB /  4799MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K20c          On   | 0000:83:00.0     Off |                    0 |\n| 30%   22C    P8    16W / 225W |     13MiB /  4799MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  On   | 0000:84:00.0     Off |                  N/A |\n| 22%   31C    P8    29W / 250W |    161MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n```\n\n_edit: I have reached out to the my university and asked if they can upgrade the driver._\n", "@AndreasMadsen , do let us know if the newer driver works out.\nThreads like [this one](https://devtalk.nvidia.com/default/topic/962474/cuda-setup-and-installation/ubuntu-16-04-cuda-8-cuda-driver-version-is-insufficient-for-cuda-runtime-version/post/4969346/) do suggest that an updated driver is needed.\n", "@asimshankar The driver is now updated and that did help with the issue. There are still some other issues but they are unrelated to this one.\n\nThanks for your help.\n"]}, {"number": 5121, "title": "Branch 136860272", "body": "", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @caisq and @zhangyaobit to be potential reviewers.\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 5120, "title": "Error : AttributeError: 'module' object has no attribute 'maybe_download_and_extract'", "body": "When I do : \n\nimport cifar10\ncifar10.maybe_download_and_extract()\n\nI get the error: AttributeError: 'module' object has no attribute 'maybe_download_and_extract'\nhow Do I download and extract the model?\n", "comments": ["@Yaffa1607 : Please fill in all the details of the [New issue template](https://github.com/tensorflow/tensorflow/issues/new) and most importantly the steps to reproduce the problem.\n\nI'm not quite sure what context you're using `import cifar10` in, perhaps you meant to follow this tutorial: https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html ?\n", "Closing this out due to inactivity. Feel free to reopen if there still is an issue (and do add in the requested details). Thanks!\n", "I believe that he was referring to the tutorial: \r\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/cifar10.py\r\n\r\nI'm having the same problem unfortunately. When calling \r\n\r\n`maybe_download_and_extract()`\r\n\r\nI get\r\n\r\n`module 'download' has no attribute 'maybe_download_and_extract'`\r\n\r\n(I downloaded \"download\" through pip install) \r\nIs there a way around this? Thanks! ", "I have a very similar error using the vgg16 architecture from the same tutorial spot and I also downloaded download using pip install download.\r\n"]}, {"number": 5119, "title": "When I do : ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": []}, {"number": 5118, "title": "seq2seq tutorial example not working for python 3.4/3.5", "body": "I am trying to get the sequence-to-sequence Tensorflow tutorial (https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html) up and running, but after downloading the WMT data, I am facing an error upon creating the vocabulary: \n\n```\n>> python3 translate.py --data_dir ~/tf-data/\nPreparing WMT data in /home/user/tf-data/\nCreating vocabulary /home/user/tf-data/vocab40000.fr from data /home/user/tf-data/giga-fren.release2.fixed.fr\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/home/user/tensorflow3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"translate.py\", line 287, in main\n    train()\n  File \"translate.py\", line 147, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/home/user/tensorflow3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 271, in prepare_wmt_data\n    create_vocabulary(fr_vocab_path, train_path + \".fr\", fr_vocabulary_size, tokenizer)\n  File \"/home/user/tensorflow3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 140, in create_vocabulary\n    tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n  File \"/home/user/tensorflow3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 109, in basic_tokenizer\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\nTypeError: cannot use a bytes pattern on a string-like object\n```\n\nOperating System: Ubuntu 16.04.1 LTS\n\nI tried running on two different systems (Python 3.4.3, Python 3.5.2), and the same error arises.\n\nI am running an up-to-date, CPU-only version of Tensorflow 0.11.0rc0 (for the appropriate version of Python 3.4/3.5) in a virtualenv, with an up-to-date version of the github repository.\n\nMy hunch is that this is related to a similar issue (https://github.com/tensorflow/tensorflow/issues/1432) that had been previously fixed, but this error is not identical, so am not sure if this is new.\n\nAny insight would be much appreciated!\n", "comments": ["@briangoodness : Sorry for the trouble, and I believe you are right about this being an issue of Python2 vs Python3. Looking into it.\n", "Ah, this was fixed by PR #4793 (https://github.com/tensorflow/tensorflow/commit/36ebbf1ddc3ed820b7a5572ff4ed8e9bc707b8e5)\n\nUnfortunately, this fix didn't make it into the images for the latest release, I will look into that. \nIn the mean time, an alternative would be to copy the latest files from \nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn\ninto your own directory and run them.\n", "@asimshankar, thank you for the help and direction! This makes sense-- I was wondering why the files in the models/rnn folder in the repo were not in sync with those in the python pip installation (e.g., I couldn't find the translate.py file in the python install).\n\nPer your suggestion, I copied the models/rnn folder from the master branch of the tensorflow repo into the python installation, and re-ran. It downloads and tokenizes the files, and then breaks at the same point-- after line 22,500,000 in the 'giga-fren.release2.fixed.en' file. See below for the precise error.\n\nI've replicated it on my laptop (8Gb RAM) and desktop (32Gb RAM)-- the code breaks on both machines, at the same location. It seems strange that it would be break at this specific point, not sure if this might be related to memory. In my example below, I toyed with tweaks at the command line (e.g., limiting the training size) to see if it affected success.\n\n```\n>> python3 translate.py --data_dir ~/tf-data/ --train_dir ~/tf-data/ --size=256 --num_layers=2 --steps_per_checkpoint=50 --max_train_data_size=50000\nPreparing WMT data in /home/user/tf-data/\nDownloading http://www.statmt.org/wmt10/training-giga-fren.tar to /home/user/tf-data/training-giga-fren.tar\nSuccesfully downloaded training-giga-fren.tar 2595102720 bytes\nExtracting tar file /home/user/tf-data/training-giga-fren.tar\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.fr.gz to /home/user/tf-data/giga-fren.release2.fixed.fr\nUnpacking /home/user/tf-data/giga-fren.release2.fixed.en.gz to /home/user/tf-data/giga-fren.release2.fixed.en\nDownloading http://www.statmt.org/wmt15/dev-v2.tgz to /home/user/tf-data/dev-v2.tgz\nSuccesfully downloaded dev-v2.tgz 21393583 bytes\nExtracting tgz file /home/user/tf-data/dev-v2.tgz\nCreating vocabulary /home/user/tf-data/vocab40000.fr from data /home/user/tf-data/giga-fren.release2.fixed.fr\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n  processing line 500000\n\n....\n\n  processing line 22200000\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nCreating vocabulary /home/user/tf-data/vocab40000.en from data /home/user/tf-data/giga-fren.release2.fixed.en\n  processing line 100000\n  processing line 200000\n  processing line 300000\n  processing line 400000\n\n...\n\n  processing line 22300000\n  processing line 22400000\n  processing line 22500000\nTokenizing data in /home/user/tf-data/giga-fren.release2.fixed.fr\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"translate.py\", line 287, in main\n    train()\n  File \"translate.py\", line 147, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 279, in prepare_wmt_data\n    data_to_token_ids(train_path + \".fr\", fr_train_ids_path, fr_vocab_path, tokenizer)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 243, in data_to_token_ids\n    normalize_digits)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 209, in sentence_to_token_ids\n    words = basic_tokenizer(sentence)\n  File \"/home/user/tf-env-py3/lib/python3.5/site-packages/tensorflow/models/rnn/translate/data_utils.py\", line 110, in basic_tokenizer\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\nTypeError: cannot use a bytes pattern on a string-like object\n```\n", "@briangoodness : That does seem weird, though I'm not sure it's a good idea to copy the files over the installation. An alternative would be to copy the files to their own directory and rewrite the imports a bit, so that example code isn't picked up from the installation.\n", "I'm having this same problem at the same point in the code with Python 3.5.2 and TensorFlow v0.11.0rc2. Is there a fix?\n", "@churichard : Unfortunately, this fix didn't make it to 0.11.\nI'm sorry I don't have better news, but in the mean time your options are to use Python 2.7 or copy and fix the example code as mentioned above.\n", "I also hit this issue in 0.11.0rc2, even though I pull the latest source code and build the library. \nI think the previous fix [PR #4793 (36ebbf1)] is incomplete. I am able to resolve it by adding the following line at the beginning of `sentence_to_token_ids` in tensorflow/models/rnn/translate/data_utils.py\n\n  `sentence = tf.compat.as_bytes(sentence)`\n", "@Invisibility : Thanks for the update and for tracking this down. Would you be interested in submitting a PR for this? (And perhaps instead of changing `sentence_to_token_ids`, you can change its [callsite in `data_to_token_ids`](https://github.com/tensorflow/tensorflow/blob/36ebbf1ddc3ed820b7a5572ff4ed8e9bc707b8e5/tensorflow/models/rnn/translate/data_utils.py#L242)  so that `tf.compat.as_bytes(line)` is the argument? This would make the call consistent with the documentation of `sentence_to_token_ids` (which says it takes bytes) and [the other call site](https://github.com/tensorflow/tensorflow/blob/816ecb7a342c25c19daa8b19627346e1fb38e56f/tensorflow/models/rnn/translate/translate.py#L240).\n", "@asimshankar : OK, I will work on this and try to submit a PR by the end of this week.\n", "@Invisibility : does the addition of the sentence line above solve the issue OP experienced? I'm in the same boat, and not sure if additional work is needed. If there's a version with all the working files, I'd be grateful if someone could point me in the right direction. ", "Yes, it works for me. ", "It seems like this is resolved. Please let us know if this is still an issue for anybody."]}, {"number": 5117, "title": "\"Cannot assign a device to node...\" bug in TensorArrayScatter_grad when using pre_scanned tensor in double loop of scan/while/map", "body": "### Environment info\n\ntensorflow branch : 0.11.0rc0\nCUDA version : 7.0\ncuDNN version : 6.5.48\nOS version : Ubuntu 14.04.5 LTS\nGPU : GPU0 titan x(maxwell), GPU1 Tesla K20c(not using in this code)\n(Also using anaconda2 environment and Jupyter with tf.InteractiveSession())\n### The bug (or is this intended error?)\n\nI was using tf.scan() and tf.map() to code seq-2-seq encoder decoder structure with attention mechanism. \nWhen i tried to put scanned tensor in map_fn() inside another scan(), the graph is drawn as normally and i even can evaluate the value of output tensor. \n\nHowever when i try to optimize, or get gradient of that tensor, the bug pops up saying `InvalidArgumentError: Cannot assign a device to node 'gradients/scan_1/while/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0`.\n\nI tried `config.allow_soft_placement = True`, but it only changed the error log and didn't work.\nIt was really awkward that the error log complains about AttrValue must not be the value of DT_STRING_REF when i set `config.allow_soft_placement = True`\n\nMy code is like ,in simplified version ,following : (i wrote a bug-reproducing example code at the bottom)\n`encoder_states = tf.scan(_encoder_step, encoder_inputs, initializer=encoder_initial_states)`\n`decoder_states = tf.scan(_decoder_step, decoder_inputs, initializer=encoder_states[-1]`\n\n, and in `def _decoder_step(prev_h, inputs):` i used `tf.map_fn()` to get aligned context of encoder states as attention mechanism in https://arxiv.org/abs/1409.0473.\n\nIt looks like following : \nin `_decoder_step(prev_h, inputs)`:\n\n```\n    def alignment_model(inputs):\n        # prev_h has the shape of [num_layer, num_batch, num_hidden]; prev_h[0] is the value of the first layer in decoder.\n        alignment_state = tf.nn.tanh(_linear([inputs, prev_h[0], output_dim=num_slot, bias=False))\n        return _linear([alignment_state], output_dim=1, bias=False)\n    alignment = tf.map_fn(alignment_model, encoder_states) # and here the bug comes.\n```\n### error message\n\nwith `config.allow_soft_placement = False` :\n\n```\nInvalidArgumentError: Cannot assign a device to node 'gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\nColocation Debug Info:\nColocation group had the following types and devices: \nTensorArrayWrite: GPU CPU \nTensorArray: GPU CPU \nStackPush: GPU CPU \nTensorArrayRead: GPU CPU \nRange: GPU CPU \nStack: GPU CPU \nStackPop: GPU CPU \nConst: GPU CPU \nTensorArrayScatter: GPU CPU \nRefEnter: GPU CPU \nEnter: GPU CPU \nTensorArrayGather: GPU CPU \nStridedSlice: GPU CPU \nTensorArrayGrad: GPU CPU \nIdentity: GPU CPU \nShape: GPU CPU \n     [[Node: gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc = Stack[_class=[\"loc:@Decoder/scan/while/Attention/map/TensorArray\"], elem_type=DT_INT32, stack_name=\"\"]()]]\n\nCaused by op u'gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc', defined at:\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-940fe231159a>\", line 3, in <module>\n    grads, _ = tf.clip_by_global_norm(tf.gradients(model.loss, tvars, aggregation_method=tf.AggregationMethod.ADD_N), 1)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 469, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 213, in _TensorArrayScatterGrad\n    grad = g.gather(indices)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 301, in gather\n    element_shape=element_shape)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1302, in _tensor_array_gather\n    element_shape=element_shape, name=name)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1302, in __init__\n    self._control_flow_context.AddOp(self)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1941, in AddOp\n    self._AddOpInternal(op)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1965, in _AddOpInternal\n    self.AddValue(x)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1900, in AddValue\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 987, in GetRealValue\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 861, in AddForwardAccumulator\n    acc = gen_data_flow_ops._stack(value.dtype.base_dtype, name=\"f_acc\")\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1104, in _stack\n    stack_name=stack_name, name=name)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter', defined at:\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n[elided 17 identical lines from previous traceback]\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-153e1e7c7c3b>\", line 6, in <module>\n    num_slot=_num_slot)\n  File \"<ipython-input-13-1ccd26e81475>\", line 34, in __init__\n    initializer=self.decoder_initial_states) # shape of [time, num_layer, batch, num_hidden]\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 563, in scan\n    back_prop=back_prop, swap_memory=swap_memory)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2518, in while_loop\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2356, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2306, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 553, in compute\n    a_out = fn(packed_a, packed_elems)\n  File \"<ipython-input-13-1ccd26e81475>\", line 98, in _decoder_step\n    context = self._get_context(encoder_last_states, states[0])\n  File \"<ipython-input-13-1ccd26e81475>\", line 81, in _get_context\n    alignment = tf.map_fn(alignment_hidden_layer, encoder_states) # shape of [time, batch, 1]\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 333, in map_fn\n    elem_ta.unpack(elem) for elem_ta, elem in zip(elems_ta, elems_flat)]\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 391, in unpack\n    indices=math_ops.range(0, num_elements), value=value, name=name)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 412, in scatter\n    name=name)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1447, in _tensor_array_scatter\n    name=name)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\nColocation Debug Info:\nColocation group had the following types and devices: \nTensorArrayWrite: GPU CPU \nTensorArray: GPU CPU \nStackPush: GPU CPU \nTensorArrayRead: GPU CPU \nRange: GPU CPU \nStack: GPU CPU \nStackPop: GPU CPU \nConst: GPU CPU \nTensorArrayScatter: GPU CPU \nRefEnter: GPU CPU \nEnter: GPU CPU \nTensorArrayGather: GPU CPU \nStridedSlice: GPU CPU \nTensorArrayGrad: GPU CPU \nIdentity: GPU CPU \nShape: GPU CPU \n     [[Node: gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc = Stack[_class=[\"loc:@Decoder/scan/while/Attention/map/TensorArray\"], elem_type=DT_INT32, stack_name=\"\"]()]]\n```\n\nwith `config.allow_soft_placement = True`:\n\n```\nInvalidArgumentError: AttrValue must not have reference type value of string_ref\n     for attr 'tensor_type'\n    ; NodeDef: scan_1/while/map/TensorArray/_211 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_666_scan_1/while/map/TensorArray\", tensor_type=DT_STRING_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^_cloopscan_1/while/map/TensorArrayPack_1/range/delta/_37); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\n     [[Node: scan_1/while/map/TensorArray/_211 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_666_scan_1/while/map/TensorArray\", tensor_type=DT_STRING_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^_cloopscan_1/while/map/TensorArrayPack_1/range/delta/_37)]]\n```\n### reproducible example code\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nin_data_for_pre_scan = np.ones(shape=[2, 8, 5], dtype=np.float64)\nin_data_for_post_scan = np.ones(shape=[2, 8, 5], dtype=np.float64)\ninitial_state_data_for_pre_scan = np.zeros(shape=[8, 5], dtype=np.float64)\ninitials_state_data_for_post_scan = np.zeros(shape=[8, 5], dtype=np.float64)\n\ninputs_for_pre_scan = tf.placeholder(shape=[None, None, 5], dtype=tf.float64)\ninputs_for_post_scan = tf.placeholder(shape=[None, None, 5], dtype=tf.float64)\ninitial_state_for_pre_scan = tf.placeholder(shape=[None, 5], dtype=tf.float64)\ninitial_state_for_post_scan = tf.placeholder(shape=[None, 5], dtype=tf.float64)\n\nweight = tf.get_variable('W', [5, 5], dtype=tf.float64)\n\ndef pre_scan(states, inputs):    \n    return states + tf.matmul(inputs, weight)\n\ndef post_scan(states, inputs):\n    def inner_map(inputs):\n        return inputs\n    loop_output = tf.map_fn(inner_map, pre_scanned[0]) \n\n    return states + loop_output + inputs\n\npre_scanned = tf.scan(pre_scan, inputs_for_pre_scan, initializer=initial_state_for_pre_scan)\nres = tf.scan(post_scan, inputs_for_post_scan, initializer=initial_state_for_post_scan)\n\nopt_func = tf.train.AdamOptimizer()\ntvars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(tf.reduce_mean(tf.square(res)), tvars, aggregation_method=tf.AggregationMethod.ADD_N), 1)\noptimizer = opt_func.apply_gradients(zip(grads, tvars))\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n#config.allow_soft_placement = True\nsess = tf.InteractiveSession(config=config)\nsess.run(tf.initialize_all_variables())\n\nsess.run(res, feed_dict={inputs_for_pre_scan:in_data_for_pre_scan, \n                         initial_state_for_pre_scan:initial_state_data_for_pre_scan, \n                         inputs_for_post_scan:in_data_for_post_scan, \n                         initial_state_for_post_scan:initials_state_data_for_post_scan}) # this runs as just fine.\n\nsess.run(optimizer, feed_dict={inputs_for_pre_scan:in_data_for_pre_scan, \n                               initial_state_for_pre_scan:initial_state_data_for_pre_scan, \n                               inputs_for_post_scan:in_data_for_post_scan, \n                               initial_state_for_post_scan:initials_state_data_for_post_scan}) # this doesn't work.\n```\n\nThe log says about 'scatter()' in 'ops/tensor_array_ops.py' and `_tensor_array_scatter' in`ops/gen_data_flow_ops.py`, which is written in this branch.\n\n@ebrevdo would anybody get me some hints about this?\n\n---\n### edited :\n\nif i only run the optimizer part without running res, such as,\n\n```\n'''\nsess.run(res, feed_dict={inputs_for_pre_scan:in_data_for_pre_scan, \n                         initial_state_for_pre_scan:initial_state_data_for_pre_scan, \n                         inputs_for_post_scan:in_data_for_post_scan, \n                         initial_state_for_post_scan:initials_state_data_for_post_scan}) # this runs as just fine.\n'''\nsess.run(optimizer, feed_dict={inputs_for_pre_scan:in_data_for_pre_scan, \n                               initial_state_for_pre_scan:initial_state_data_for_pre_scan, \n                               inputs_for_post_scan:in_data_for_post_scan, \n                               initial_state_for_post_scan:initials_state_data_for_post_scan}) # this doesn't work.\n```\n\nthen it works fine. \n\nbut still if i run both of them, sess.run(optimizer) raises `InvalidArgumentError`. could it be problem in my GPU config? actually when i execute `nvidia-smi`, it says that GPU0 is Tesla k20c and GPU1 is Geforce gtx titan x, but tensorflow says in the reverse order.\n", "comments": ["@lukaszkaiser this started from seq2seq, but seems isolated to something else. Maybe you have some insight.\n", "Maybe it is a problem with dynamic loops and TensorArrays? I see a lot of scan use. I think ebrevdo@ or Yuan might know better.\n", "@ebrevdo Any clue?\n", "When you call tf.gradiwnts, there's an option collocate_with_...  Do you\nenable it?\n\nOn Oct 21, 2016 8:24 AM, \"yhg0112\" notifications@github.com wrote:\n\n> Environment info\n> \n> tensorflow branch : 0.11.0rc0\n> CUDA version : 7.0\n> cuDNN version : 6.5.48\n> OS version : Ubuntu 14.04.5 LTS\n> GPU : GPU0 titan x(maxwell), GPU1 Tesla K20c(not using in this code)\n> (Also using anaconda2 environment and Jupyter with tf.InteractiveSession())\n> The bug (or is this intended error?)\n> \n> I was using tf.scan() and tf.map() to code seq-2-seq encoder decoder\n> structure with attention mechanism.\n> When i tried to put scanned tensor in map_fn() inside another scan(), the\n> graph is drawn as normally and i even can evaluate the value of output\n> tensor.\n> \n> However when i try to optimize, or get gradient of that tensor, the bug\n> pops up saying InvalidArgumentError: Cannot assign a device to node\n> 'gradients/scan_1/while/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc':\n> Could not satisfy explicit device specification '' because the node was\n> colocated with a group of nodes that required incompatible device\n> '/job:localhost/replica:0/task:0/GPU:0.\n> \n> I tried config.allow_soft_placement = True, but it only changed the error\n> log and didn't work.\n> It was really awkward that the error log complains about AttrValue must\n> not be the value of DT_STRING_REF when i set config.allow_soft_placement\n> = True\n> \n> My code is like ,in simplified version ,following : (i wrote a\n> bug-reproducing example code at the bottom)\n> encoder_states = tf.scan(_encoder_step, encoder_inputs,\n> initializer=encoder_initial_states)\n> decoder_states = tf.scan(_decoder_step, decoder_inputs,\n> initializer=encoder_states[-1]\n> \n> , and in def _decoder_step(prev_h, inputs): i used tf.map_fn() to get\n> aligned context of encoder states as attention mechanism in\n> https://arxiv.org/abs/1409.0473.\n> \n> It looks like following :\n> in _decoder_step(prev_h, inputs):\n> \n> ```\n> def alignment_model(inputs):\n>     # prev_h has the shape of [num_layer, num_batch, num_hidden]; prev_h[0] is the value of the first layer in decoder.\n>     alignment_state = tf.nn.tanh(_linear([inputs, prev_h[0], output_dim=num_slot, bias=False))\n>     return _linear([alignment_state], output_dim=1, bias=False)\n> alignment = tf.map_fn(alignment_model, encoder_states) # and here the bug comes.\n> ```\n> \n> error message\n> \n> with config.allow_soft_placement = False :\n> \n> InvalidArgumentError: Cannot assign a device to node 'gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n> Colocation Debug Info:\n> Colocation group had the following types and devices:\n> TensorArrayWrite: GPU CPU\n> TensorArray: GPU CPU\n> StackPush: GPU CPU\n> TensorArrayRead: GPU CPU\n> Range: GPU CPU\n> Stack: GPU CPU\n> StackPop: GPU CPU\n> Const: GPU CPU\n> TensorArrayScatter: GPU CPU\n> RefEnter: GPU CPU\n> Enter: GPU CPU\n> TensorArrayGather: GPU CPU\n> StridedSlice: GPU CPU\n> TensorArrayGrad: GPU CPU\n> Identity: GPU CPU\n> Shape: GPU CPU\n>      [[Node: gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc = Stack[_class=[\"loc:@Decoder/scan/while/Attention/map/TensorArray\"], elem_type=DT_INT32, stack_name=\"\"]()]]\n> \n> Caused by op u'gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc', defined at:\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n>     \"__main__\", fname, loader, pkg_name)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n>     exec code in run_globals\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n>     app.launch_new_instance()\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n>     app.start()\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n>     ioloop.IOLoop.instance().start()\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n>     super(ZMQIOLoop, self).start()\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n>     handler_func(fd_obj, events)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n>     return fn(_args, *_kwargs)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n>     self._handle_recv()\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n>     self._run_callback(callback, msg)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n>     callback(_args, *_kwargs)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n>     return fn(_args, *_kwargs)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n>     return self.dispatch_shell(stream, msg)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n>     handler(stream, idents, msg)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n>     user_expressions, allow_stdin)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n>     shell.run_cell(code, store_history=store_history, silent=silent)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n>     interactivity=interactivity, compiler=compiler, result=result)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n>     if self.run_code(code, result):\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n>     exec(code_obj, self.user_global_ns, self.user_ns)\n>   File \"<ipython-input-16-940fe231159a>\", line 3, in <module>\n>     grads, _ = tf.clip_by_global_norm(tf.gradients(model.loss, tvars, aggregation_method=tf.AggregationMethod.ADD_N), 1)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 469, in gradients\n>     in_grads = _AsList(grad_fn(op, *out_grads))\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 213, in _TensorArrayScatterGrad\n>     grad = g.gather(indices)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 301, in gather\n>     element_shape=element_shape)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1302, in _tensor_array_gather\n>     element_shape=element_shape, name=name)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n>     op_def=op_def)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1302, in __init__\n>     self._control_flow_context.AddOp(self)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1941, in AddOp\n>     self._AddOpInternal(op)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1965, in _AddOpInternal\n>     self.AddValue(x)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1900, in AddValue\n>     real_val = grad_ctxt.grad_state.GetRealValue(val)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 987, in GetRealValue\n>     history_value = cur_grad_state.AddForwardAccumulator(cur_value)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 861, in AddForwardAccumulator\n>     acc = gen_data_flow_ops._stack(value.dtype.base_dtype, name=\"f_acc\")\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1104, in _stack\n>     stack_name=stack_name, name=name)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n>     op_def=op_def)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n>     self._traceback = _extract_stack()\n> \n> ...which was originally created as op u'Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter', defined at:\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n>     \"__main__\", fname, loader, pkg_name)\n> [elided 17 identical lines from previous traceback]\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n>     exec(code_obj, self.user_global_ns, self.user_ns)\n>   File \"<ipython-input-15-153e1e7c7c3b>\", line 6, in <module>\n>     num_slot=_num_slot)\n>   File \"<ipython-input-13-1ccd26e81475>\", line 34, in __init__\n>     initializer=self.decoder_initial_states) # shape of [time, num_layer, batch, num_hidden]\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 563, in scan\n>     back_prop=back_prop, swap_memory=swap_memory)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2518, in while_loop\n>     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2356, in BuildLoop\n>     pred, body, original_loop_vars, loop_vars, shape_invariants)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2306, in _BuildLoop\n>     body_result = body(*packed_vars_for_body)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 553, in compute\n>     a_out = fn(packed_a, packed_elems)\n>   File \"<ipython-input-13-1ccd26e81475>\", line 98, in _decoder_step\n>     context = self._get_context(encoder_last_states, states[0])\n>   File \"<ipython-input-13-1ccd26e81475>\", line 81, in _get_context\n>     alignment = tf.map_fn(alignment_hidden_layer, encoder_states) # shape of [time, batch, 1]\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 333, in map_fn\n>     elem_ta.unpack(elem) for elem_ta, elem in zip(elems_ta, elems_flat)]\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 391, in unpack\n>     indices=math_ops.range(0, num_elements), value=value, name=name)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 412, in scatter\n>     name=name)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1447, in _tensor_array_scatter\n>     name=name)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n>     op_def=op_def)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/home/youaredeadl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n>     self._traceback = _extract_stack()\n> \n> InvalidArgumentError (see above for traceback): Cannot assign a device to node 'gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\n> Colocation Debug Info:\n> Colocation group had the following types and devices:\n> TensorArrayWrite: GPU CPU\n> TensorArray: GPU CPU\n> StackPush: GPU CPU\n> TensorArrayRead: GPU CPU\n> Range: GPU CPU\n> Stack: GPU CPU\n> StackPop: GPU CPU\n> Const: GPU CPU\n> TensorArrayScatter: GPU CPU\n> RefEnter: GPU CPU\n> Enter: GPU CPU\n> TensorArrayGather: GPU CPU\n> StridedSlice: GPU CPU\n> TensorArrayGrad: GPU CPU\n> Identity: GPU CPU\n> Shape: GPU CPU\n>      [[Node: gradients/Decoder/scan/while/Attention/map/TensorArrayPack/TensorArrayScatter_grad/TensorArrayGather/f_acc = Stack[_class=[\"loc:@Decoder/scan/while/Attention/map/TensorArray\"], elem_type=DT_INT32, stack_name=\"\"]()]]\n> \n> with config.allow_soft_placement = True:\n> \n> InvalidArgumentError: AttrValue must not have reference type value of string_ref\n>      for attr 'tensor_type'\n>     ; NodeDef: scan_1/while/map/TensorArray/_211 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_666_scan_1/while/map/TensorArray\", tensor_type=DT_STRING_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^_cloopscan_1/while/map/TensorArrayPack_1/range/delta/_37); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\n>      [[Node: scan_1/while/map/TensorArray/_211 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_666_scan_1/while/map/TensorArray\", tensor_type=DT_STRING_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^_cloopscan_1/while/map/TensorArrayPack_1/range/delta/_37)]]\n> \n> reproducible example code\n> \n> import tensorflow as tf\n> import numpy as np\n> \n> in_data_for_pre_scan = np.ones(shape=[2, 8, 5], dtype=np.float64)\n> in_data_for_post_scan = np.ones(shape=[2, 8, 5], dtype=np.float64)\n> initial_state_data_for_pre_scan = np.zeros(shape=[8, 5], dtype=np.float64)\n> initials_state_data_for_post_scan = np.zeros(shape=[8, 5], dtype=np.float64)\n> \n> inputs_for_pre_scan = tf.placeholder(shape=[None, None, 5], dtype=tf.float64)\n> inputs_for_post_scan = tf.placeholder(shape=[None, None, 5], dtype=tf.float64)\n> initial_state_for_pre_scan = tf.placeholder(shape=[None, 5], dtype=tf.float64)\n> initial_state_for_post_scan = tf.placeholder(shape=[None, 5], dtype=tf.float64)\n> \n> weight = tf.get_variable('W', [5, 5], dtype=tf.float64)\n> \n> def pre_scan(states, inputs):\n>     return states + tf.matmul(inputs, weight)\n> \n> def post_scan(states, inputs):\n>     def inner_map(inputs):\n>         return inputs\n>     loop_output = tf.map_fn(inner_map, pre_scanned[0])\n> \n> ```\n> return states + loop_output + inputs\n> ```\n> \n> pre_scanned = tf.scan(pre_scan, inputs_for_pre_scan, initializer=initial_state_for_pre_scan)\n> res = tf.scan(post_scan, inputs_for_post_scan, initializer=initial_state_for_post_scan)\n> \n> opt_func = tf.train.AdamOptimizer()\n> tvars = tf.trainable_variables()\n> grads, _ = tf.clip_by_global_norm(tf.gradients(tf.reduce_mean(tf.square(res)), tvars, aggregation_method=tf.AggregationMethod.ADD_N), 1)\n> optimizer = opt_func.apply_gradients(zip(grads, tvars))\n> \n> config = tf.ConfigProto()\n> config.gpu_options.allow_growth = True\n> #config.allow_soft_placement = True\n> sess = tf.InteractiveSession(config=config)\n> sess.run(tf.initialize_all_variables())\n> \n> sess.run(res, feed_dict={inputs_for_pre_scan:in_data_for_pre_scan,\n>                          initial_state_for_pre_scan:initial_state_data_for_pre_scan,\n>                          inputs_for_post_scan:in_data_for_post_scan,\n>                          initial_state_for_post_scan:initials_state_data_for_post_scan}) # this runs as just fine.\n> \n> sess.run(optimizer, feed_dict={inputs_for_pre_scan:in_data_for_pre_scan,\n>                                initial_state_for_pre_scan:initial_state_data_for_pre_scan,\n>                                inputs_for_post_scan:in_data_for_post_scan,\n>                                initial_state_for_post_scan:initials_state_data_for_post_scan}) # this doesn't work.\n> \n> The log says about 'scatter()' in 'ops/tensor_array_ops.py' and _tensor_array_scatter'\n> inops/gen_data_flow_ops.py`, which is written in this branch.\n> \n> @ebrevdo https://github.com/ebrevdo would you get me some hints about\n> this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5117, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim8SbUObZ5JPp0bRIVlbjfFw2UlLHks5q2NlBgaJpZM4KdWKH\n> .\n", "I've just run with `tf.gradients` `colocate_gradients_with_ops option`. Either setting it True or False, same error had been raised. \n", "We just pushed some better debugging for this to master. It should be\nsynced by EOD.  Can you try using a build off master tonight or tomorrow\nand report back?\n\nOn Oct 25, 2016 1:24 PM, \"yhg0112\" notifications@github.com wrote:\n\n> I've just run with tf.gradients colocate_gradients_with_ops option.\n> Either setting it True or False, same error had been raised.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5117#issuecomment-256164788,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim6PHvDcfkdFUr8gYcCQD7--fokZCks5q3mWVgaJpZM4KdWKH\n> .\n", "alright. i've just did test with master branch, `/tensorflow-0.11.0rc1-py2-none-any.whl`. \n\nEvaluation works fine as well, but optimization with gradients have been broken again with same error. \n\ni guess it's not merged yet. \n", "Please provide a minimal failing example, full code.  Try to reduce it to\nas few ops as possible.  Are you running rnns on separate GPUs?\n\nOn Oct 25, 2016 10:46 PM, \"yhg0112\" notifications@github.com wrote:\n\n> alright. i've just did test with master branch, /tensorflow-0.11.0rc1-py2-\n> none-any.whl.\n> \n> Evaluation works fine as well, but optimization with gradients have been\n> broken again with same error.\n> \n> i guess it's not merged yet.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5117#issuecomment-256255121,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim9S91NVmwvjyQSgEUeCvN3hb-QILks5q3ukrgaJpZM4KdWKH\n> .\n", "i'm really sorry about late response.\n\ni have just pulled the master branch and re-installed tensorflow (`tf.__version__` is 0.11.0rc1), and nothing changed yet.\n\nIsn't my bug-reproducing code example simple enough? the error seems to happen when i try to put `scan`ed tensor into double `map_fn` (or `scan`) loop. It's really strange that it works if i first run `sess.run(optimizer, ...` but it doesn't work if i first run `sess.run(res, ...` before i run the optimizer.\n", "i've run the example with pip installed version tensorflow (`tf.__version__` is 0.11.0rc2). \n\nAnd the error didn't happen. i think it is solved. thank you all.\n", "Yay!\n"]}, {"number": 5116, "title": "Fixes and improvements to docker build script and dockerfiles", "body": "1) Clean up large Bazel build cache. Total filesystem size reduction as seen by du -sh /:\n  devel image: 1.5 GB (Before: 2.9 GB; After: 1.4 GB)\n  devel-gpu image: 2.3 GB (Before: 4.7 GB; After: 2.4 GB)\n2) Using nvidia-docker for GPU docker build.\n3) Upgrade Bazel version from 0.3.1 to 0.3.2.\n4) Add missing libcurl3-dev build dependency to devel images.\n5) Add scipy and sklearn to Dockerfile.devel-gpu to enhance consistency with other image types (e.g., Dockerfile.devel).\n6) Remove the obsolete and unnecessary --recurse-submodules flag for git clone.\n\nRelated to GH issues: https://github.com/tensorflow/tensorflow/issues/4116 and https://github.com/tensorflow/tensorflow/issues/4117\n\nHowever, not using the \"git clone --depth 1\" suggested by issue #4117, because the size of the git repo is only reduced by about 50 MB by the \"--depth 1\" flag. This space saving is small compared to the space saving due to bazel cache removal. The complete history of the git repo can be useful for certain development purposes.\nChange: 136302103\n", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @gunan to be potential reviewers.\n", "This is cherry-pick of 8a724211a9d4871338afe874276abf1d358f799b from master to r0.11.\n", "OK the final decision is we hold onto this today, release rc1,\nThen merge this and release rc2 on Monday\n", "Thanks, @gunan \n", "We already built docker images, i think at this point it is best to not\nmerge this, as we might need to rebuildbthem with this.\n\nOn Oct 21, 2016 8:05 AM, \"Shanqing Cai\" notifications@github.com wrote:\n\n> This is cherry-pick of 8a72421\n> https://github.com/tensorflow/tensorflow/commit/8a724211a9d4871338afe874276abf1d358f799b\n> from master to r0.11.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5116#issuecomment-255401793,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AHlCObbZmheEswyCUPiLEoOUWChlQ6vvks5q2NTFgaJpZM4KdUyl\n> .\n", "@gunan But this is already merged?\n", "Dont know how this mail got delayed this much. I left this about 5 days ago.\nIt will be included in rc2.\n"]}, {"number": 5115, "title": "timeout breaks FIFOQueue", "body": "Ubuntu 14.04.5 LTS\n0.10.0rc0\n\nUsing timeout with notebooks is very useful in case you dequeue an empty queue or enqueue a full one. The problem is that after a timeout occurs a enqueue or dequeueop  throws an error\n\n```\nimport tensorflow as tf\nwith tf.device(\"/cpu:0\"):\n    ph = tf.placeholder(tf.float32)\n    q = tf.FIFOQueue(2, tf.float32)\n    enq = q.enqueue(ph)\n    deq = q.dequeue()\n    timeout_option = tf.RunOptions(timeout_in_ms=1000)\nsess = tf.Session()\nsess.run(deq, options=timeout_option)\n```\n\nhere i get the usual timeout error. The problem is that when I then run\n`sess.run(enq, feed_dict={ph:2}, options=timeout_option)`\n\ni get the error:\n\n```\n---------------------------------------------------------------------------\nCancelledError                            Traceback (most recent call last)\n<ipython-input-24-9067a9d62797> in <module>()\n----> 1 sess.run(Q, options=timeout_option)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n    715     try:\n    716       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 717                          run_metadata_ptr)\n    718       if run_metadata:\n    719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    913     if final_fetches or final_targets:\n    914       results = self._do_run(handle, final_targets, final_fetches,\n--> 915                              feed_dict_string, options, run_metadata)\n    916     else:\n    917       results = []\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n    963     if handle is None:\n    964       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n--> 965                            target_list, options, run_metadata)\n    966     else:\n    967       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n    983         except KeyError:\n    984           pass\n--> 985       raise type(e)(node_def, op, message)\n    986 \n    987   def _extend_graph(self):\n\nCancelledError: Dequeue operation was cancelled\n     [[Node: fifo_queue_Dequeue = QueueDequeue[_class=[\"loc:@fifo_queue\"], component_types=[DT_FLOAT, DT_INT64, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fifo_queue)]]\n     [[Node: PlaceholderWithDefault/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_14_PlaceholderWithDefault\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nCaused by op u'fifo_queue_Dequeue', defined at:\n  File \"<string>\", line 1, in <module>\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n    app.start()\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cgel/.local/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code):\n  File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-73bf93787995>\", line 76, in <module>\n    input_state, action, Y = q.dequeue()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 418, in dequeue\n    self._queue_ref, self._dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 863, in _queue_dequeue\n    timeout_ms=timeout_ms, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 747, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2372, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["I tried from a recent version and it doesn't happen there. Do you want to upgrade?\n", "I can also reproduce this on 0.11rc0\nIf dequeue on empty queue is cancelled because of DeadlineExceededError,\nthe queue becomes unusable -- it's no longer possible to enqueue anything\nonto the queue.\n\nOn Fri, Oct 21, 2016 at 6:48 AM, cgel notifications@github.com wrote:\n\n> Ubuntu 14.04.5 LTS\n> 0.10.0rc0\n> \n> Using timeout with notebooks is very useful in case you dequeue an empty\n> queue or enqueue a full one. The problem is that after a timeout occurs a\n> enqueue or dequeueop throws an error\n> \n> import tensorflow as tf\n> with tf.device(\"/cpu:0\"):\n>     ph = tf.placeholder(tf.float32)\n>     q = tf.FIFOQueue(2, tf.float32)\n>     enq = q.enqueue(ph)\n>     deq = q.dequeue()\n>     timeout_option = tf.RunOptions(timeout_in_ms=1000)\n> sess = tf.Session()\n> sess.run(deq, options=timeout_option)\n> \n> here i get the usual timeout error. The problem is that when I then run\n> sess.run(enq, feed_dict={ph:2}, options=timeout_option)\n> \n> i get the error:\n> \n> ---\n> \n> CancelledError                            Traceback (most recent call last)\n> <ipython-input-24-9067a9d62797> in <module>()\n> ----> 1 sess.run(Q, options=timeout_option)\n> \n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\n>     715     try:\n>     716       result = self._run(None, fetches, feed_dict, options_ptr,\n> --> 717                          run_metadata_ptr)\n>     718       if run_metadata:\n>     719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n> \n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\n>     913     if final_fetches or final_targets:\n>     914       results = self._do_run(handle, final_targets, final_fetches,\n> --> 915                              feed_dict_string, options, run_metadata)\n>     916     else:\n>     917       results = []\n> \n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n>     963     if handle is None:\n>     964       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n> --> 965                            target_list, options, run_metadata)\n>     966     else:\n>     967       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n> \n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\n>     983         except KeyError:\n>     984           pass\n> --> 985       raise type(e)(node_def, op, message)\n>     986\n>     987   def _extend_graph(self):\n> \n> CancelledError: Dequeue operation was cancelled\n>      [[Node: fifo_queue_Dequeue = QueueDequeue[_class=[\"loc:@fifo_queue\"], component_types=[DT_FLOAT, DT_INT64, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fifo_queue)]]\n>      [[Node: PlaceholderWithDefault/_25 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_14_PlaceholderWithDefault\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n> Caused by op u'fifo_queue_Dequeue', defined at:\n>   File \"<string>\", line 1, in <module>\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n>     app.start()\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n>     ioloop.IOLoop.instance().start()\n>   File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n>     super(ZMQIOLoop, self).start()\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n>     handler_func(fd_obj, events)\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n>     return fn(_args, *_kwargs)\n>   File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n>     self._handle_recv()\n>   File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n>     self._run_callback(callback, msg)\n>   File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n>     callback(_args, *_kwargs)\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n>     return fn(_args, *_kwargs)\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n>     return self.dispatch_shell(stream, msg)\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n>     handler(stream, idents, msg)\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n>     shell.run_cell(code, store_history=store_history, silent=silent)\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n>     interactivity=interactivity, compiler=compiler)\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n>     if self.run_code(code):\n>   File \"/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n>     exec(code_obj, self.user_global_ns, self.user_ns)\n>   File \"<ipython-input-1-73bf93787995>\", line 76, in <module>\n>     input_state, action, Y = q.dequeue()\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 418, in dequeue\n>     self._queue_ref, self._dtypes, name=name)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 863, in _queue_dequeue\n>     timeout_ms=timeout_ms, name=name)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 747, in apply_op\n>     op_def=op_def)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2372, in create_op\n>     original_op=self._default_original_op, op_def=op_def)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n>     self._traceback = _extract_stack()\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5115, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHEAbq2SwT4ovuH_LMYQUf622LxEqks5q2MLEgaJpZM4KdPpU\n> .\n", "@gunan there seems to be a enqueue/dequeue issue in 0.11, that does not appear in HEAD. A candidate for cherry-picking?\n", "Same behavior in 11rc1. To clarify, the problem happens if you call enqueue\nafter previous dequeue operation timed out:\n\nconda create -n tf11rc1-cpu python=3.5\nsource activate tf11rc1-cpu\n\nexport TF_BINARY_URL=\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py3-none-any.whl\npip install --upgrade $TF_BINARY_URL\n\nexport CUDA_VISIBLE_DEVICES=\npython\nimport tensorflow as tf\nprint(tf.**version**)\nq = tf.FIFOQueue(2, tf.float32)\nenq = q.enqueue(1.)\ndeq = q.dequeue()\ntimeout_option = tf.RunOptions(timeout_in_ms=1000)\n\nsess = tf.Session()\nprint(sess.run(enq))  # works\nprint(sess.run(deq))  # works\nprint(sess.run(deq, options=timeout_option)) # times out\nprint(sess.run(enq))  # doesn't work\n\n---\n\n(tf11rc1-cpu) bash-3.2$ export CUDA_VISIBLE_DEVICES=\n(tf11rc1-cpu) bash-3.2$ python\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12)\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow as tf\n> > > print(tf.**version**)\n> > > 0.11.0rc1\n> > > q = tf.FIFOQueue(2, tf.float32)\n> > > enq = q.enqueue(1.)\n> > > deq = q.dequeue()\n> > > timeout_option = tf.RunOptions(timeout_in_ms=1000)\n> > > sess = tf.Session()\n> > > print(sess.run(enq))\n> > > None\n> > > print(sess.run(deq))  # works\n> > > 1.0\n> > > print(sess.run(deq, options=timeout_option)) # times out\n> > > W tensorflow/core/kernels/queue_base.cc:302] _0_fifo_queue: Skipping\n> > > cancelled dequeue attempt with queue not closed\n> > > Traceback (most recent call last):\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\n> > > line 972, in _do_call\n> > >     return fn(*args)\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\n> > > line 954, in _run_fn\n> > >     status, run_metadata)\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/contextlib.py\",\n> > > line 66, in __exit__\n> > >     next(self.gen)\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\",\n> > > line 463, in raise_exception_on_not_ok_status\n> > >     pywrap_tensorflow.TF_GetCode(status))\n> > > tensorflow.python.framework.errors.DeadlineExceededError: Timed out waiting\n> > > for notification\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 717, in run\n    run_metadata_ptr)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 915, in _run\n    feed_dict_string, options, run_metadata)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 965, in _do_run\n    target_list, options, run_metadata)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 985, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.DeadlineExceededError: Timed out waiting\nfor notification\n\n> > > print(sess.run(enq))  # doesn't work\n> > > Traceback (most recent call last):\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\n> > > line 972, in _do_call\n> > >     return fn(*args)\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\n> > > line 954, in _run_fn\n> > >     status, run_metadata)\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/contextlib.py\",\n> > > line 66, in __exit__\n> > >     next(self.gen)\n> > >   File\n> > > \"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\",\n> > > line 463, in raise_exception_on_not_ok_status\n> > >     pywrap_tensorflow.TF_GetCode(status))\n> > > tensorflow.python.framework.errors.CancelledError: Enqueue operation was\n> > > cancelled\n> > > [[Node: fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT],\n> > > _class=[\"loc:@fifo_queue\"], timeout_ms=-1,\n> > > _device=\"/job:localhost/replica:0/task:0/cpu:0\"](fifo_queue,\n> > > fifo_queue_enqueue/component_0)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 717, in run\n    run_metadata_ptr)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 915, in _run\n    feed_dict_string, options, run_metadata)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 965, in _do_run\n    target_list, options, run_metadata)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\",\nline 985, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.CancelledError: Enqueue operation was\ncancelled\n[[Node: fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT],\n_class=[\"loc:@fifo_queue\"], timeout_ms=-1,\n_device=\"/job:localhost/replica:0/task:0/cpu:0\"](fifo_queue,\nfifo_queue_enqueue/component_0)]]\n\nCaused by op 'fifo_queue_enqueue', defined at:\n  File \"<stdin>\", line 1, in <module>\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py\",\nline 329, in enqueue\n    return gen_data_flow_ops._queue_enqueue(self._queue_ref, vals,\nname=scope)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\",\nline 982, in _queue_enqueue\n    name=name)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\",\nline 756, in apply_op\n    op_def=op_def)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\",\nline 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File\n\"/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\",\nline 1298, in __init__\n    self._traceback = _extract_stack()\n\nCancelledError (see above for traceback): Enqueue operation was cancelled\n[[Node: fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT],\n_class=[\"loc:@fifo_queue\"], timeout_ms=-1,\n_device=\"/job:localhost/replica:0/task:0/cpu:0\"](fifo_queue,\nfifo_queue_enqueue/component_0)]]\n\n\nOn Mon, Oct 24, 2016 at 3:36 PM, drpngx notifications@github.com wrote:\n\n> I tried from a recent version and it doesn't happen there. Do you want to\n> upgrade?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5115#issuecomment-255885879,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHIyD8MLoO3AHyWZdJooYn2zy_hphks5q3TL4gaJpZM4KdPpU\n> .\n", "sure, if we have a fix let's go ahead and cherrypick that into 0.11\n", "I didn't test after timeout, let me see.\n", "Yeah, also broken at `HEAD`.\n", "Right. The dequeue has a closure which calls Cancel, but that is ignored, because the queue is not `closed_`.\n\n```\nW1025 18:29:29.356150   47689 queue_base.cc:303] _0_fifo_queue: Skipping cancelled dequeue attempt with queue not closed\n```\n\n@ebrevdo any clue?\n", "Looks like [this](https://github.com/tensorflow/tensorflow/blob/32d1dcc10e1fdf33dc6742337c6e0869f7b3c557/tensorflow/core/common_runtime/direct_session.cc#L1166) is the code responsible:\n\n``` c++\n      // TODO(sherrym): This cancels all steps in the session, even ones that\n      // have not exceeded their deadline. An alternative would be to use a\n      // two-level cancellation manager with a Session-global one containing\n      // several step-local ones. Probably the RunState should have its own\n      // CancellationManager.\n      cancellation_manager_->StartCancel();\n```\n", "Do we have a fix for this? We are still accepting cherry-picks for the RC until EOD today.\n", "I have some time to look at this now, so I'll attempt a quick fix.\n"]}, {"number": 5114, "title": "Issues with tf.get_shape().as_list() and creating index for tf.gather()", "body": "Environment: \nubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root   558720 Sep 14 16:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 14 16:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 14 16:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rw-r--r-- 1 root root   415432 Sep 14 16:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root   775162 Sep 14 16:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       17 Oct 11 12:52 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5.1.5\nlrwxrwxrwx 1 root root       17 Oct 16 00:02 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\nlrwxrwxrwx 1 root root       17 Oct 11 12:52 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Oct 11 12:51 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Oct 11 12:51 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n```\n1. The commit hash (`git rev-parse HEAD`): 4bac93835b585cc44fe502f7617ad7ab54265c14\n2. The output of `bazel version` \n\n```\n$ bazel version\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nhttps://gist.github.com/odellus/b520c8990997672ef84c0465c7c337fb\n### What other attempted solutions have you tried?\n\nI tried to use `tf.gather_nd()` but it doesn't work on the GPU at this time, so I'm using an alternative.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\nhttps://gist.github.com/odellus/d4b01bf0e7179f19458f12f93fee517d\n", "comments": ["If I pass in the `batch_size` as a parameter when initializing the class, then I can get it to work, but `batch_size` varies with each iteration so having it as part of the model initialization isn't ideal.\n", "I should have been using [`tf.shape()`](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#shape) instead of `foo.get_shape().as_list()`. Thanks TensorFlow community. You've been so helpful!\n"]}, {"number": 5113, "title": "Memory corrupt when running in distributed mode with CUDA 8.0 and Tesla M40", "body": "We have implemented tested the distributed TensorFlow applications. It works in CPU environment and the GPU environment with CUDA 7.5 and Tesla K40. But it fails with CUDA 8.0 and Tesla M40.\n\n![screen shot 2016-10-21 at 19 10 37](https://cloud.githubusercontent.com/assets/2715000/19597024/9882b2de-97c4-11e6-930a-3c373524d21b.png)\n\n![screen shot 2016-10-21 at 19 11 16](https://cloud.githubusercontent.com/assets/2715000/19597035/ab8502ec-97c4-11e6-9028-80d10347dbf2.png)\n\nNot sure if it's related to CUDA 8.0. The test code is [here](https://github.com/tobegit3hub/distributed_tensorflow) and 100% re-produce if I run it with CUDA 8.0 and Tesla M40.\n### Environment info\n\nOperating System: CentOS 7.0\nCUDA and cuDNN: CUDA 8.0 and cuDNN v5\nTensorFlow version: 0.11.0rc0\n", "comments": ["We have tested and it works with CUDA 7.5 and M40.\n\nIt seems to be the problem of TensorFlow 0.11.0rc0 with CUDA 8.0.\n", "Supporting CUDA 8.0 is released by `v0.11.0rc1`. I test with the latest release version and everything works with CUDA 8.0 and Nvidia M40.\n\nThanks for the contribution \ud83d\ude03 \n", "This seems like a lot of work for maintainers to reproduce, it would be\nproductive to isolate the issue further -- ie, finding which ops/conditions\ngive an unexpected result\n\nOn Fri, Oct 21, 2016 at 4:30 AM, tobe notifications@github.com wrote:\n\n> We have implemented tested the distributed TensorFlow applications. It\n> works in CPU environment and the GPU environment with CUDA 7.5 and Tesla\n> K40. But it fails with CUDA 8.0 and Tesla M40.\n> \n> [image: screen shot 2016-10-21 at 19 10 37]\n> https://cloud.githubusercontent.com/assets/2715000/19597024/9882b2de-97c4-11e6-930a-3c373524d21b.png\n> \n> [image: screen shot 2016-10-21 at 19 11 16]\n> https://cloud.githubusercontent.com/assets/2715000/19597035/ab8502ec-97c4-11e6-9028-80d10347dbf2.png\n> \n> Not sure if it's related to CUDA 8.0. The test code is here\n> https://github.com/tobegit3hub/distributed_tensorflow and 100%\n> re-produce if I run it with CUDA 8.0 and Tesla M40.\n> Environment info\n> \n> Operating System: CentOS 7.0\n> CUDA and cuDNN: CUDA 8.0 and cuDNN v5\n> TensorFlow version: 0.11.0rc0\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5113, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHGOvzEjkp4_ucNaS3LXT-slfLNelks5q2KJmgaJpZM4KdIrA\n> .\n"]}, {"number": 5112, "title": "Does tensorflow support to write checkpoint to hdfs?", "body": "now tensorflow 0.11 can read data from hdfs, so how can I write checkpoint to hdfs? for example:\n\n``` python\nsaver.save(save_path='hdfs://some_dir')\n```\n", "comments": ["@jhseu \n", "Not test yet but I think so. TensorFlow has implemented the C++ filesystem interfaces which is used for almost all data interface. Now it supports to read data from HDFS, it's really possible to support writing into HDFS.\n\n@qingzew It would be great to test by yourself or ask in StackOverview/Google group/Mail list instead of opening an issue \ud83d\ude03 \n", "ok, I have tested it. It can do that, but sometimes with an error:\n\n``` shell\nTraceback (most recent call last):\n  File \"cifar10_train.py\", line 134, in <module>\n    tf.app.run()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"cifar10_train.py\", line 130, in main\n    train()\n  File \"cifar10_train.py\", line 122, in train\n    saver.save(sess, checkpoint_path, global_step=step)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1281, in save\n    self.last_checkpoints, latest_filename)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 743, in update_checkpoint_state\n    file_io.rename(temp_pathname, coord_checkpoint_filename, overwrite=True)\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 295, in rename\n    compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\n  File \"/usr/lib64/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.UnknownError\n```\n\nyou just to modify cifar10 like this:\n\n``` python\ntf.app.flags.DEFINE_string('train_dir', 'hdfs://localhost:9000/cifar10_train',\n                           \"\"\"Directory where to write event logs \"\"\"\n                           \"\"\"and checkpoint.\"\"\"\n```\n", "@jhseu is this resolved in master, or is this a legitimate issue we need to resolve.\nIf this is resolved let's close the issue.\n", "I rebuild tensorflow from master branch, it works now, if you saw this, maybe it's ok:\n\n``` shell\nWARNING:tensorflow:*******************************************************\nWARNING:tensorflow:TensorFlow's V1 checkpoint format is deprecated; V2 will become the default shortly after 10/31/2016.\nWARNING:tensorflow:Consider switching to the more efficient V2 format now:\nWARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\nWARNING:tensorflow:to prevent breakage.\nWARNING:tensorflow:*******************************************************\n\n```\n", "I use the tensorflow1.0, but when I write saver.save(sess, hdfs_path+\"save_net.ckpt\"), it tells me that parent directory is not exists. How can I tackle this problem? Thanks @qingzew @asimshankar @tobegit3hub @gunan @ry ", "@kelly-tlz try to create root dir first?", "> I use the tensorflow1.0, but when I write saver.save(sess, hdfs_path+\"save_net.ckpt\"), it tells me that parent directory is not exists. How can I tackle this problem? Thanks @qingzew @asimshankar @tobegit3hub @gunan @ry\r\n\r\nHi, Do you tackled this problem ? I also meet this problem now.@kelly-tlz"]}, {"number": 5111, "title": "android example with a new model which has a large .pb file(180mb)", "body": "hi guys\ni tried to run the android example with a new model which has a large .pb file(180mb), and i get the error.\n\n10-21 15:32:02.172 19751-19816/org.tensorflow.demo A/native: jni_utils.cc:128 Check failed: message->ParseFromArray(memory, data_size) \n10-21 15:32:02.172 19751-19816/org.tensorflow.demo A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 19816 (ImageListener)\n\ni thought it's a limitation of reading file with native code.\ndoes anyone get the same problem or have a way to deal it?\nthank for your time.\n", "comments": ["I suspect this has to do with the [64MB limit in protobufs](https://github.com/google/protobuf/blob/fd046f6263fb17383cafdbb25c361e3451c31105/src/google/protobuf/io/coded_stream.h#L625)\n\n@petewarden @andrewharp @keveman may have suggestions\n", "@asimshankar  thanks for your help.\ni found the solution.\n.pb file will be compressed when it is bigger than some size, this caused model parsing to fail.\n[I disabled the compression for .pb files in gradle.](https://goo.gl/DjcyrN)\n", "For Bazel builds, `nocompress_extensions = [\".pb\",],` can be added to the android_binary target to effect a similar workaround.\n\nThe code should be able to gracefully handle larger compressed PBs, though; reopening issue to track.\n", "This issue was rendered obselete by 1a9769dc79fdd27c347633df210ff64f48de8d07, which replaces the native PB loading code with the TF Java API equivalent."]}, {"number": 5110, "title": "Fail to export model with exporter with distributed session", "body": "We have try `exporter` and `signature` to export our TensorFlow models. The code works in standalone mode but not in distributed mode.\n\nWe got `InvalidArgumentError` when calling `exporter.Exporter(saver).init()`. It asks to feed the placeholder but it's not training process and I just want to get the `op`'s name to pass as model's signatures. The error log looks like this.\n\n![screen shot 2016-10-21 at 17 27 17](https://cloud.githubusercontent.com/assets/2715000/19593672/52ddc706-97b4-11e6-8b5e-f278d2197dc2.png)\n\nDo anyone has tried to export models with distributed session? Here is my sample code if it helps.\n\n![screen shot 2016-10-21 at 17 28 22](https://cloud.githubusercontent.com/assets/2715000/19593691/67092bf8-97b4-11e6-872a-e6b4daf05005.png)\n### Environment info\n\nOperating System: Ubuntu 16.04\nCUDA and cuDNN: Not install\nTensorFlow version: 0.11.0rc0\n", "comments": ["@tobegit3hub : Is it possible to provide a minimal reproducible example. As text instead of a screenshot so we can copy the exact code used ourselves?\n\nWith some more context we might be able to provide a useful diagnosis. The error log and snippet you've provided are just a little shy of being very useful. For example, it's not clear to me which line corresponds to task.py, line 120. And also, would be good to have the stacktrace of the InvalidArgumentException.\n\nDo chime in with a small, reproducible example that we can run. Thanks!\n", "Thanks for quick response. @asimshankar \n\nI have moved the code into this project, https://github.com/tobegit3hub/distributed_tensorflow .\n\nYou can follow the README to run in standalone and distributed mode. It's easy to re-produce the issue. If it's the problem of our code, please let me know and any feedback is really appreciated.\n", "@tobegit3hub : Can you trim your code down considerably so that it focuses on this problem?Currently, there is a lot of code there that seems unrelated to the problem at hand. It would also be nice to factor out the common parts between distributed and standalone versions so its easier to see what the two share and what they do not.\n", "Thanks @asimshankar . \n\nOf course I can trim it down and refactor the code. But it's only ~200 lines and I have tried to make it easy to read and runnable. \n\nYou can run the `standalone` part to make sure the `exporter` will work with this model. And run the similar code of `distributed` part and checkout the error message.\n", "@concretevitamin , @wicke, do you have any insight on if this could be caused by the new checkpoint format (it's 11.0rc0 and it was in by this time). Could it be using placeholders in the signature? The mnist_export.py in our example uses an Identity tensor.\n", "It's not related to the checkpoint format, I'll say.  (Biggest reason being 0.11rc0 does not have complete support for V2.)  Adding @sukritiramesh @nfiedel to see if this is something obvious in the Exporter.\n", "I have tried with 0.12.0 and it failed in the same way. The I trace and log the exception like this.\r\n\r\n```\r\nipdb> pp e\r\nInvalidArgumentError()\r\nipdb> pp e.error_code\r\n3\r\nipdb> pp e.message\r\nu'You must feed a value for placeholder tensor \\'Placeholder_2\\' with dtype float\\n\\t [[Node: Placeholder_2 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:master/replica:0/task:0/cpu:0\"]()]]'\r\nipdb> e.op\r\n<tensorflow.python.framework.ops.Operation object at 0x7ff87957a050>\r\n```\r\n\r\nI was trying to export the generic signature and not sure why it need to feed the placeholder. It's caused by this line of code while it works in standalone mode.\r\n\r\n```\r\nexporter.generic_signature({\"keys\": keys_placeholder,\r\n                                                  \"X\": X})\r\n```\r\n\r\nIt still failed even I try to export nothing in signature. May not be related to `exporter`.\r\n\r\n```\r\n              model_exporter = exporter.Exporter(saver)\r\n              model_exporter.init(\r\n                  sess.graph.as_graph_def(),\r\n                  named_graph_signatures={\r\n                      'inputs':\r\n                      exporter.generic_signature({}),\r\n                      'outputs':\r\n                      exporter.generic_signature({})\r\n                  })\r\n              model_exporter.export(FLAGS.model_path,\r\n                                    tf.constant(FLAGS.model_version), sess)\r\n```", "Thanks for @asimshankar @aselle and @concretevitamin .\r\n\r\nIt's easy to reproduce. Can anyone help to verify the issue or give the proper example for distributed TensorFlow application to export the mode?\r\n\r\nWe can run `sess.graph.as_graph_def()` and `exporter.Exporter(saver)` separately and throw exception when running `model_exporter.init`.", "Pinging @sukritiramesh", "There are two problems we need to fix for this code and now it works.\r\n\r\nThe placeholder problem is similar to http://stackoverflow.com/questions/38825741/running-distributed-tensorflow-with-invalidargumenterror-you-must-feed-a-value?rq=1 . We should not use `summary_op` in `Supervisor` when using placeholder(not sure if we need to fix this later). Now it doesn't complain about `You must feed a value for placeholder tensor` with this code.\r\n\r\n```\r\n        sv = tf.train.Supervisor(is_chief=(task_type == \"master\"),\r\n                                 logdir=FLAGS.checkpoint_path,\r\n                                 init_op=init_op,\r\n                                 summary_op=None,\r\n                                 saver=saver,\r\n                                 global_step=global_step,\r\n                                 save_model_secs=60)\r\n``` \r\n\r\nBut I still have the problem about `Graph is finalized and cannot be modified`. It's related to https://github.com/tensorflow/tensorflow/issues/5439 witch can be fixed by defining `exporter` and any other `op` before `Supervisor`.\r\n\r\nThanks for the help from Github issues and StackOverflow. Still need to improve the document and example code for any other TensorFlow developers.\r\n"]}, {"number": 5109, "title": "Segmentation fault with basic indexing and slicing ", "body": "The following code generates a segmentation fault.\nIt should obviously return an error because op has only one dimension and a ask for two.\nBut an alert is probably more informative than a segmentation fault.\n(Tensorflow 0.11.0rc0)\n\n```\nimport tensorflow as tf\n\nv = tf.Variable([1,1,1])\nop = v * v\nvalue = op[0,:]\n\nsess = tf.Session()\nprint(sess.run(value))\n```\n", "comments": ["Thanks very much for the report @guillaumeBellec . Fortunately, this bug was inadvertently fixed in abc663f2efabf9feed92461c91c7212df004162e. Unfortunately, that change is not included in the 0.11 release.\n\nI'll see if we can include this in a release candidate for 0.11, but it will surely be fixed in the next release. (At head the program above results in the following exception:\n\n```\nTraceback (most recent call last):\n  File \"/x/test.py\", line 5, in <module>\n    value = op[0,:]\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 384, in _SliceHelper\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 538, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3028, in strided_slice\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 748, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2405, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1790, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1638, in _DelegateStridedSliceShape\n    return common_shapes.call_cpp_shape_fn(op, input_tensors_needed=[1, 2, 3])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 596, in call_cpp_shape_fn\n    raise ValueError(err.message)\nValueError: Index out of range using input dim 1; input has only 1 dims for 'strided_slice' (op: 'StridedSlice') with input shapes: [3], [2], [2], [2].\n```\n\n)\n"]}, {"number": 5108, "title": "doc: fix possible copy&paste error in cumprod docs", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@temporaer, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @mrry to be potential reviewers.\n", "@rmlarsen Could you take a look?\n", "Jenkins, test this please.\n", "`graph_io_test` seems to be failing again...\n", "Jenkins, test this please.\n"]}, {"number": 5107, "title": "Fix cmake build by restoring changes from c25f125.", "body": "Accidentally cmake build started depending on system zlib.\nThis PR fixes that.\n", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @mrry and @guschmue to be potential reviewers.\n", "Nice, thanks!\n"]}, {"number": 5106, "title": "Problems with TF_GraphGetTensorNumDims when applied to \"Variable\" operations", "body": "For some reason, when applying _TF_GraphGetTensorNumDims_ to a \"Variable\" operation,\nthe functions returns -1, even though the shape and number of dimensions are \nwell defined.\n\nA simple example:\n\n``` C++\n#include <string>\n#include <iostream>\n#include \"tensorflow/c/c_api.h\"\n\nint main() {\n\n  // creating a TF_Graph\n  TF_Graph* graph = TF_NewGraph();\n\n  // creating a \"Variable\" operation\n  TF_OperationDescription* opDesc = TF_NewOperation(graph, \"Variable\", \"w\");\n  const long long dims[2] = {2, 2};\n  TF_SetAttrShape(opDesc, \"shape\", dims, 2);\n  TF_SetAttrType(opDesc, \"dtype\", TF_DOUBLE);\n\n  TF_Status* status = TF_NewStatus();\n  TF_Operation* w = TF_FinishOperation(opDesc, status);\n  std::string finish_message = std::string(TF_Message(status));\n\n  TF_Port w_port = {w, 0};\n  int w_num_dims = TF_GraphGetTensorNumDims(graph, w_port, status);\n  std::string num_dims_message = std::string(TF_Message(status));\n\n  std::cout << \"finish_message: \" << finish_message << '\\n';\n  std::cout << \"w_num_dims: \" << w_num_dims << '\\n';\n  std::cout << \"num_dims_message: \" << num_dims_message << '\\n';\n\n  TF_DeleteGraph(graph);\n  TF_DeleteStatus(status);\n\n  return 0;\n}\n```\n\nThe program returns:\n\n```\nfinish_message:\nw_num_dims: -1\nnum_dims_message:\n```\n\nHowever, if I do the same for a \"Placeholder\" operation (by simply replacing \n\"Variable\" with \"Placeholder), the returned number of dimensions is 2 (as expected).\n\nIt seems to me like a bug. It might be related to the previous issue that \nI reported (#5059), since the inability to determine the shape of a tensor at a particular node propagates through the graph.\n", "comments": ["Thanks for the detailed report @juraj-bicikl . This particular case is because the C++ function for shape inference for the Variable op is explicitly returning an unknown shape ([state_ops.cc](https://github.com/tensorflow/tensorflow/blob/d2c2d134dfc3e31f6d1296576f76e9abd4fc6c1c/tensorflow/core/ops/state_ops.cc#L31)). That does seem fishy to me, I'll investigate further and get back to you.\n", "Unfortunately, for historical reasons, the \"Variable\" op's shape function will not be able to distinguish between an unknown shape or a scalar shape (similar to the [Placeholder op](https://github.com/tensorflow/tensorflow/blob/41ba1e0e6fbf443f1a972ff0130ba6741b9b7a50/tensorflow/core/ops/array_ops.cc#L2480)). Before the 1.0 release, we are going to try and fix this up. However, in the mean time:\n- I'm going to try and fix this up for non-scalar shapes \n- You might consider the workaround for Variable ops where you explicitly set the shape using something like:\n\n``` c\nTF_GraphSetTensorShape(graph, w_port, dims, 2, status);\n```\n", "Thanks a lot!\nI just tried using _TF_GraphSetTensorShape_, and it seems like a nice workaround. \n"]}, {"number": 5105, "title": "Branch 136793750", "body": "- Same as backward-merge:\n  - .gitignore: merged.\n  - tensorflow/contrib/distributions/python/ops/beta.py\n- contrib/cmake: keep HEAD and /MP option.\n- port.cc: keep both includes.\n- tensor_bundle/BUILD: keep additional no-warning flag.\n- md files: override gitbub with internal change.\n- tensorflow/stream_executor/dso_loader.cc: delete unused dynload_flags.\n- tensorflow/tools/pip_package/simple_console_for_windows.py: keep version without carriage returns\n- tensorflow/configure: keep ours (136269932)\n- tensorflow.bzl: keep ours (136269932), all 3 places.\n", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @dsmilkov and @RNabel to be potential reviewers.\n"]}, {"number": 5104, "title": "Branch 136793750", "body": "- Same as backward-merge:\n  - .gitignore: merged.\n  - tensorflow/contrib/distributions/python/ops/beta.py\n- contrib/cmake: keep HEAD and /MP option.\n- port.cc: keep both includes.\n- tensor_bundle/BUILD: keep additional no-warning flag.\n- md files: override gitbub with internal change.\n- tensorflow/stream_executor/dso_loader.cc: delete unused dynload_flags.\n- tensorflow/tools/pip_package/simple_console_for_windows.py: keep version without carriage returns\n- tensorflow/configure: keep ours (136269932)\n", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @dsmilkov and @RNabel to be potential reviewers.\n", "Bogus manual merge.\n"]}, {"number": 5103, "title": "Updating README adjusting to GPU support ", "body": "Updating current limitations and adding instructions to use GPU on Cmake build.\n", "comments": ["Can one of the admins verify this patch?\n", "@Carmezim, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @ebrevdo and @guschmue to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Maybe mention that a ^ has to be added to \nMore? -DPYTHON_LIBRARIES=C:/Users/%USERNAME%/AppData/Local/Continuum/Anaconda3/libs/python35.lib\nif you want to add more lines.\n", "@thepok May bad. I am doing it now. Thank you.\n\nedit: sorry for the delay, github was not working for me due to DDoS attack that happened today.\nedit 2: build with cmake version 3.7 is not working. Versions 3.5 and 3.6 are built successfully.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "The last steps in the Readme miss, it took me a long time to figure it out. I did not know what to do with the genereatet PIP file. Maybe add a line describing that you have to do \"PIP install genereted_file.whl\" or what ever is the real name :)\n", "@Carmezim do you want to update the README?\n", "@drpngx I just wanted to propose those changes on the Cmake build readme to avoid confusing folks regarding GPU support as it is already available and the readme states otherwise.\n", "Jenkins, test this please.\n", "What happens after CMake 3.6?\n", "@drpngx when building with v3.7 it generates an error referring to a stray character \" after \"MSVC\" in line 81 of `tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.RelWithDebInfo.cmake`, earlier versions build successfully [#5071](https://github.com/tensorflow/tensorflow/pull/5071#issuecomment-255501356).\n", "Got it, @mrry and @guschmue might be interested in knowing that CMakeList is broken after 3.6.\n", "@drpngx @guschmue knows as he figured it out with folks testing. Now it seems [#5127](https://github.com/tensorflow/tensorflow/pull/5127) came with something that is generating other errors though that even with v3.6 it is not building successfully.\n", "Thanks for identifying the CL!\n", "I looked at #5127 and think I have a simple fix for it. Just building a fix and if good I'll send a PR.\n", "@guschmue, could you look at the latest changes and see if you can LGTM? Thanks. \n", "@guschmue, could you do one final pass before we merge this? Thanks. \n"]}, {"number": 5102, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@techjogger, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @gunan and @vrv to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I think this is created by mistake?\nDo you have a specific change in mind that we missed in master branch?\n"]}, {"number": 5101, "title": "Branch 136782838", "body": "- Same as backward-merge:\n  - .gitignore: merged.\n  - tensorflow/contrib/distributions/python/ops/beta.py\n- contrib/cmake: keep HEAD and /MP option.\n- port.cc: keep both includes.\n- tensor_bundle/BUILD: keep additional no-warning flag.\n- md files: override gitbub with internal change.\n- tensorflow/stream_executor/dso_loader.cc: delete unused dynload_flags.\n- tensorflow/tools/pip_package/simple_console_for_windows.py: keep version without carriage returns\n", "comments": ["@drpngx, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @dsmilkov and @RNabel to be potential reviewers.\n", "Failed windows build, pushing fix internally.\n"]}, {"number": 5100, "title": "Temporarily remove tests that fail running with a single GPU.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @ebrevdo to be potential reviewers.\n"]}, {"number": 5099, "title": "lookup_embedding returning 0 on missing index", "body": "If I create a variable with the first dimension equal to k and I use it for looking up vectors with lookup_embeddings, if I input an invalid index (higher than k or lower than 0) the lookup just returns 0s.\nIn the code below I create a 10x2 embedding matrix e, so that each embedding is of dimension 2. i then lookup for a number that is the step number. This has no use, it's just for showing the issue. Obviously for the first 10 steps the value I fetch is a random embedding of dimension 2, after the tenth iteration, I start getting all 0s.\n\nI think this should be documented. I'm not sure if it is a good default or not, probably it is, but at least there should be a warning or something similar telling that you are trying to access an index that is not there. It would be really useful for debugging. I would have saved few hours of work if I noticed this before.\n### Environment info\n\nOperating System: Ubuntu 16.04 64bit\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1\nTensorflow version: 0.10.0 compiled for cuda 8.0 (but it is not the issue)\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\nimport numpy as np\nimport tensorflow as tf\n\nnum_embedding = 10\nembedding_size = 2\n\nx = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]])\ny = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1])\n\ngraph = tf.Graph()\nwith graph.as_default():\n    x_p = tf.placeholder(tf.float32, shape=[None, 2])\n    e_p = tf.placeholder(tf.int64, shape=[])\n\n    y_p = tf.placeholder(tf.float32, shape=[None])\n    y_pe = tf.expand_dims(y_p, 1)\n\n    w = tf.Variable(tf.random_normal([2, 1]))\n    b = tf.Variable(tf.random_normal([1]))\n    e = tf.Variable(tf.random_normal([num_embedding, embedding_size]))\n\n    embed = tf.nn.embedding_lookup(e, e_p)\n\n    logits = tf.matmul(x_p, w) + b + tf.reduce_sum(embed)\n\n    xe = tf.nn.sigmoid_cross_entropy_with_logits(logits, y_pe)\n    loss = tf.reduce_mean(xe)\n\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.nn.sigmoid(logits) > 0.5, tf.float32), y_pe), tf.float32))\n    optimizer = tf.train.AdamOptimizer(0.5).minimize(loss)\n\n    #Dangling nodes in the computational graph\n    z_p = tf.placeholder(tf.float32, shape=[None, 2])\n    u = tf.Variable(tf.random_normal([2, 1]))\n    l = tf.matmul(z_p, u)\n\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    for step in range(201):\n        if step % 100 == 0:\n            save_path = saver.save(session, \"lc_{:04d}.ckpt\".format(step))\n        _, loss_val, acc_val, embed_val = session.run(\n            [optimizer, loss, accuracy, embed],\n            feed_dict={x_p: x, e_p: step, y_p: y})\n        print(\"step {step} - loss: {loss_val:.6f}, acc: {acc_val:.4f}\".format(step=step, loss_val=loss_val, acc_val=acc_val))\n        print(\"embed_val: {}\".format(embed_val))\n\n```\n", "comments": ["Thanks for the report @w4nderlust. It seems like documenting this is an appropriate thing to do. I will defer to @theweiho and/or @ysuematsu to make the change, or if you're interested do feel free to send in a pull request. \n", "Yes! :) We'd definitely welcome pull request to help update the documentation to reflect what would have been helpful to you - to make it clearer what happens in this case.\n", "I want to work on it.", "Actually I think the change would be as simple as saying that, for a tensor with first dimension _d_, trying to lookup for the row i where _i < 0_ or _i >= d_ results in a tensor of the expected shape filled with 0s", "I get your point.", "@theweiho @asimshankar @vrv @drpngx   I don't know how to judge a tensor's value inside tensorflow.\r\nI get a bool tensor in the python file of tensorflow. But how to compare the tensor to a native `bool` of python ? ", "@guotong1988 Sorry could you elaborate? Is that related to the comment above? Are you thinking of `tf.cond`?", "@drpngx Loot at here. https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/python/ops/embedding_ops.py#L109\r\nThis issue is that `ids` may be out of the range of `params[0]`.\r\nSo I want to judge the each of the `ids` value.\r\nDo you have any better idea?", "Maybe `tf.assert_less`?", "Back on the original topic: in Tensorflow 1.0.0 the same code run on CPU warns the user of accessing an index out of bounds, while when run on the GPU keeps returning 0 filled vectors.", "@w4nderlust @theweiho  is this still an issue?", "On a related but similar issue I was told that those kind of checks are impossible to do on GPU. i still believe that there should be a way to do some warining.\n\n> Il giorno 16 giu 2017, alle ore 22:41, Skye Wanderman-Milne <notifications@github.com> ha scritto:\n> \n> @w4nderlust <https://github.com/w4nderlust> @theweiho <https://github.com/theweiho> is this still an issue?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/5099#issuecomment-309128422>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAVUSIY2x-VyiDhr18CgLCrFDrpiDbFeks5sEuhcgaJpZM4KckGq>.\n> \n\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5098, "title": "Boston.py tutorial runs but, no predictions produced", "body": "Hard to get going when Tutorial does not work as expected..  I'm sure it is simple but not for me yet!\nNOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\nUbuntu 16.05 Cudnn 5.1 Cuda 8 Nvidia 1070\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   \n   I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so locally\n   I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so locally\n   0.11.0rc0\n\nJust running the Boston.py tutorial as is from the current repo unmodified:\nIt seems to train But just produces an estimator object and does not produce predictions, no errors (the warning are a bit of a concern.\nthis is the last train cycle and the report of the estimator object (see end).\n\n```\n: GeForce GTX 1070, pci bus id: 0000:01:00.0)\nINFO:tensorflow:Restored model from /tmp/tmpq10r1l1a\nINFO:tensorflow:Eval steps [0,1) for training step 5000.\nINFO:tensorflow:Saving evaluation summary for 5000 step: loss = 21.8338\nLoss: 21.833767\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column.       Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\nPredictions: <generator object BaseEstimator._infer_model_as_iterable at 0x7f34042198e0>\n```\n", "comments": ["@dartdog : Sorry you're having trouble getting started. Hopefully we can sort this out and make things better for future newbies.\n\nI'm unable to reproduce the error. I tried both python 2.7 and python 3.5 and got something like:\n\n```\nPredictions: [ 33.93679047  19.3759346   21.71503448  36.20479584  15.1063385  18.72967148]\n```\n\n(You mentioned that you're using Ubuntu 16.05, but I'm guessing you meant 16.04?)\nFor example, here's an isolated, reproducible environment using docker and no GPU (just because the environment is easier to reproduce):\n\n```\ndocker run -it  ubuntu:16.04 bash \napt-get update\napt-get install -y python3 python3-dev python3-pandas python3-pip wget\npip3 install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl\nmkdir /tmp/boston\ncd /tmp/boston\n# Download the files\nwget http://download.tensorflow.org/data/boston_train.csv\nwget http://download.tensorflow.org/data/boston_test.csv\nwget http://download.tensorflow.org/data/boston_predict.csv\nwget https://raw.githubusercontent.com/tensorflow/tensorflow/r0.11/tensorflow/examples/tutorials/input_fn/boston.py\npython3 boston.py\n```\n\nAre you able to try this out? If it works then it would suggest something in your environment. If not, then we'll have more information to debug :)\n", "Hmm that ran correctly\nYes it is Ubuntu 16.04\nWell as is my habit I made a typo on copying your code and so I pasted it all in\nThe docker image failed so I updated my base system..\nSo It reinstalled Python and tensorflow.. It seems the main thing is that it is now no longer using the GPU...\nDumb of me\nIt still seems to be using the Anaconda Python 3.5.2 (both \"python' and \"python3' bring the same Anaconda up in terminal) though but Tensorflow is no longer GPU enabled?\nSo I guess I need to reinstall it from source? Advice??\nAnd I'm guesssing that my error may be due to the GPU enabling?\nOr perhaps this version is slightly more up to date since I built from source a week ago?\nAny chance there is a prebuilt gpu version?\n", "I also tried this on Ubuntu 14.04 and CUDA 7.5 (thus GPU)\n(There is no release pip wheel for TensorFlow with CUDA 8 yet, hopefully it will be out soon - we intend 0.11rc1 to support CUDA 8, so I tried CUDA 7.5. And there are no nvidia/cuda docker images for CUDA 7.5 on Ubuntu 16.04, so I ended up with 14.04)\n\nHere's what I did:\n\n```\nnvidia-docker run -it nvidia/cuda:7.5-cudnn5-devel\napt-get update\napt-get install -y python3 python3-dev python3-pandas python3-pip wget\npip3 install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl\nmkdir /tmp/boston\ncd /tmp/boston\n# Download the files\nwget http://download.tensorflow.org/data/boston_train.csv\nwget http://download.tensorflow.org/data/boston_test.csv\nwget http://download.tensorflow.org/data/boston_predict.csv\nwget https://raw.githubusercontent.com/tensorflow/tensorflow/r0.11/tensorflow/examples/tutorials/input_fn/boston.py\npython3 boston.py\n```\n\nThe fact that this works, suggests that it isn't a GPU problem per se.\nTo summarize, we found that:\n- CPU/Ubuntu16.04: Works\n- GPU/Ubuntu14.04/CUDA7.5: Works\n\nBut we're seeing that whatever your setup (using anaconda) with Ubuntu16.04, CUDA8, Python 3.5 doesn't seem to work? It could be that specific combination, but I'd be surprised.\n\nCould you provide details on how you setup your environment?\n", "This prior problem ticket has all my configuration details and history!\nhttps://github.com/tensorflow/tensorflow/issues/5017\nSo I'll await your instructions, I'm happy to try and build gain for cuda8 ? Since I got this spiffy new board..\n", "That setup involves copying shared libraries into the anaconda environment. I'm not well versed with anaconda enough to know what effect that may have.\n\nCan you use docker instead? Either use the release versions (as I mentioned above for Ubuntu14.04, CUDA 7.5) or build from source using `nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04` as the base image.\n\nI apologize that I'm basically suggesting \"do something else with docker\", but at this point it seems that you've run into some trouble and customizations with Anaconda that is beyond my ability to debug without being able to reproduce that exact environment.\n", "Well perhaps If I build from source in a Python 3.5 env  rather than in the root Anaconda . Things might be better isolated.. and easier to create new environments.. going forward. I'll try that out tomorrow and report back :+1: \n", "deleted message due to DNS outage on github..\n", "Ok once the internet cleared enough today I completely wiped my Anaconda and Tensorflow from the system and started over I created a new env just for GPU tensorFlow and reinstalled and built from scratch and once I used the solution in the other post to copy the missing library all is right with the world..  The Boston.py file runs correctly to completion and produces the scores. \n\nNot sure who should make sure this library below is in the right spot though..between TensorFlow and Anaconda?\n\n```\n# cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /home/tom/anaconda3/lib/\n```\n\n(modify to install in the env lib directory)\n\nFeel free to close.\n", "I'm a little worried about having to copy over libstdc++, particularly because it means that we're changing the library that programs expecting to use the anaconda version. You might want to check in on the anaconda forums about how to use an updated libstdc++ there.\n\nBut....glad to hear things are working.\n"]}, {"number": 5097, "title": "Update doc and release note for 0.11.0rc1", "body": "", "comments": []}, {"number": 5096, "title": "Cannot run  \"bazel build tensorflow/examples/image_retraining:retrain\"", "body": "Hi,\n\nI've been having issues in building the [image retraining](https://www.tensorflow.org/versions/r0.11/how_tos/image_retraining/index.html#training-on-flowers) example. When I run bazel build tensorflow/examples/image_retraining:retrain I get the following error:\n\nERROR: $TF_HOME/tensorflow/core/BUILD:991:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate  \"bazel-out/host/genfiles/tensorflow/core/util/version_info.cc\"': com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTraceback (most recent call last):\n  File \"tensorflow/tools/git/gen_git_source.py\", line 221, in <module>\n    generate(args.generate)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 152, in generate\n    spec, head_symlink, _, dest_file = arglist\nValueError: need more than 1 value to unpack\nTarget //tensorflow/examples/image_retraining:retrain failed to build\n\nThe file gen_git_source.py exists but the file \"bazel-out/host/genfiles/tensorflow/core/util/version_info.cc\" does not seem to exist. \n\nCould this be happening because the generate() method inside gen_git_source expects multiple arguments which are not being provided?\n## **Environment info**\n\nDebian 4.6.4-1\n## **What other attempted solutions have you tried?**\n\nIf I comment out the line 991 in core/BUILD, that doesn't help either. It gives an array of other errors.\n\nI found another related error: https://github.com/tensorflow/tensorflow/issues/4701\n", "comments": ["@deeptigp : I'm unable to reproduce, but perhaps you can provide some more information. Please try the following when you get a chance:\n\n``` sh\nbazel build -s //tensorflow/core:version_info_gen\n```\n\nThis should print out something like:\n\n``` sh\ntensorflow/tools/git/gen_git_source.py --generate tensorflow/tools/git/gen/spec.json tensorflow/tools/git/gen/head tensorflow/tools/git/gen/branch_ref \"bazel-out/local-fastbuild/genfiles/tensorflow/core/util/version_info.cc\"'\n```\n\nIf you can execute that snippet, it should print out the details of why the python script that generates `version_info.cc` failed. Let us know what that is, and we might be able to pinpoint the problem.\n\nThanks.\n", "Hi Asim,\n\nThanks for looking into this! When I run the above command (with --verbose_failures), I get the following error message:\n\n```\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\n>>>>> # //tensorflow/core:version_info_gen [action 'Executing genrule //tensorflow/core:version_info_gen']\n(cd /home/deepti/.cache/bazel/_bazel_deepti/2ff5020c8dabdcb4abc9307fab6c0bfa/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate  \"bazel-out/local-fastbuild/genfiles/tensorflow/core/util/version_info.cc\"')\nERROR:$TFLOW/tensorflow/core/BUILD:991:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command \n  (cd /home/deepti/.cache/bazel/_bazel_deepti/2ff5020c8dabdcb4abc9307fab6c0bfa/execroot/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate  \"bazel-out/local-fastbuild/genfiles/tensorflow/core/util/version_info.cc\"'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTraceback (most recent call last):\n  File \"tensorflow/tools/git/gen_git_source.py\", line 221, in <module>\n    generate(args.generate)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 152, in generate\n    spec, head_symlink, _, dest_file = arglist\nValueError: need more than 1 value to unpack\nTarget //tensorflow/core:version_info_gen failed to build\nINFO: Elapsed time: 0.143s, Critical Path: 0.03s\n\n```\n\nLine 991 in the BUILD file under core/ directory has the following call: tf_version_info_genrule()\n\nI hope this message is helpful to you. \n\nThanks!\n", "@deeptigp : Which version of Bazel are you using? If less than 0.3.2, please upgrade to 0.3.2 and retry. (You can get the version with `bazel version`).\n\n(In the future, please do try an include all information requested in the [New issue template](https://github.com/tensorflow/tensorflow/issues/new) )\n\nLet us know if upgrading bazel helped. Thanks.\n", "Hi Asim,\n\nThanks for your response. The bazel version installed on my machine was 0.3.2, but I had an older version of tensorflow. Upgrading to 0.11 resolved this issue.\n\nThanks for your help!\n", "In my case I had branched off after running configure. As suggested here, running this exposed what was making tf unhappy, and re-running configure make it happy again:\r\n\r\n````\r\nbazel build -s //tensorflow/core:version_info_gen\r\nINFO: Found 1 target...\r\n>>>>> # //tensorflow/core:version_info_gen [action 'Executing genrule //tensorflow/core:version_info_gen']\r\n(cd /home/rrosa/.cache/bazel/_bazel_rrosa/0bc32e86368550ea439dcf6daa0b227b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ....\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate tensorflow/tools/git/gen/spec.json tensorflow/tools/git/gen/head tensorflow/tools/git/gen/branch_ref \"bazel-out/local-opt/genfiles/tensorflow/core/util/version_info.cc\"')\r\nERROR: /home/rrosa/tensorflow/tensorflow/core/BUILD:1416:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 264, in <module>\r\n    generate(args.generate)\r\n  File \"tensorflow/tools/git/gen_git_source.py\", line 215, in generate\r\n    (old_branch, new_branch))\r\nRuntimeError: Run ./configure again, branch was 'refs/heads/r1.3' but is now 'refs/heads/mynewbranch'\r\nTarget //tensorflow/core:version_info_gen failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.901s, Critical Path: 0.03s\r\n```", "check whether you install git package\r\n```\r\nsudo apt install git\r\n```"]}, {"number": 5095, "title": "Error in build phase (iOS and Android sample)", "body": "Hi, \n   I followed all the instructions to build the iOS sample app but I encountered many errors like this: \n\n```\n./tensorflow/core/framework/types.h:112:48: error: unknown type name 'DataType'\ninline bool TypesCompatible(DataType expected, DataType actual) {\n                                               ^\n./tensorflow/core/framework/types.h:142:11: error: unknown type name 'DataType'\ntemplate <DataType VALUE>\n```\n\nand all in the same file: **types.h**\n\nI dind't found anything on the web (or Stackoverflow) about these errors. \n### Environment info\n\nOperating System: MacOS Sierra version 10.12 (16A323)\n\nI used Docker quickstart terminal to run tensorflow and build the apps. \nAlso with the Android sample app I have the same issues. \n\nI followed also this [blog](https://petewarden.com/2016/02/28/tensorflow-for-poets/) and all the instruction here in the github project, but I receive the same errors also opening the project with xCode Version 8.0 (8A218a). \n\nCould you please help me to understand how to solve the problem with the DataType? \n\nThanks\nFabio\n", "comments": ["@fabiodimicco : Which instructions did you follow for iOS? Have you seen this: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#ios\n", "Yes I tried exactly all these steps and during the compiling phase I receive 20 errors about the DataType (error: unknown type name 'DataType'). \n\nI tried with the build_all_ios.sh and also with the separated steps (download_dependencies.sh, compile_ios_protobuf.sh and compile_ios_tensorflow.sh). \n", "Hmm...`DataType` is defined in `types.pb.h`, which is generated from the .proto file. Your issue might possibly be related to #4997 \n\n@petewarden might have some thoughts on this.\n", "yes...this is the problem. the file `types.pb.h` is not generated :( \n\nI tried also the solution by @petewarden, but I have already installed the libtool using this line od code `brew install libtool`. \n\nDo you have any suggestions to create this file? I noticed now that also the file `resource_handle.pb.h` is not generated. \n", "Hi @petewarden \n  I discovered that the issue happens only on MacOS Sierra because I have tried with El Capitan and everything is working fine. \n\nDo you think this issue can been solved also for MacOS Sierra? \n", "I'm also having the same problem on MacOS Sierra and Xcode 8. Help please!\n", "This issue is quite old by now, MacOS has been used for awhile now. Be sure to try the latest build instructions (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) \r\non the latest version of TensorFlow, and please create a new bug if it is still not working. Thanks.\r\n"]}, {"number": 5094, "title": "add support for fetch and feed session.run conversion functions", "body": "#2068\r\n", "comments": ["Can one of the admins verify this patch?\n", "@suiyuan2009, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @tensorflower-gardener and @keveman to be potential reviewers.\n", "ping for @mrry\n", "I removed the tf.contrib.client.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@mrry only windows failed, `reader_ops_test` has nothing to do with changed code.\n", "Windows tests should now be fixed, I'll run the tests one more time and hopefully we should be good....\n", "@tensorflow-jenkins test this please.\n", "@mrry sad story.\n", "@benoitsteiner This should be good to merge, despite the failing presubmit (which is unrelated).\n", "@mrry Indeed, I'm merging this now.\n"]}, {"number": 5093, "title": "Do not set testonly to //tensorflow/python:construction_fails_go", "body": "This target is shipped into the pip_package so it is not testonly. Current\nversion of Bazel does not enforce testonly but the next version will.\nTested by building build_pip_package.\n\nTracking bug: bazelbuild/bazel#1967\n", "comments": ["@damienmg, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @vrv to be potential reviewers.\n", "The change looks benign to me, but the \"test\" prefix in filename looks fishy to me.\nVijay, can you comment on this OP, is it a test only OP?\n", "Closing in favor of cl/136944330. Thank you Herr Damien.\n"]}]