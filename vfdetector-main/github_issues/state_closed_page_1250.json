[{"number": 15650, "title": "Unable to compile tensorflow r1.4 from source with cuda 8.0 and cudnn 7 and after downgrading bazel?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.4\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.8.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 8.0/7.0.4\r\n- **GPU model and memory**: 1080 Ti\r\n- **Exact command to reproduce**: \r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"`\r\n\r\n\r\n### Describe the problem\r\nI'm trying to compile TF r1.4 from source but didn't manage to get this working although I tried several fixes:\r\n\r\n1. Downgrading bazel from 0.9.0 to 0.8.1 based on #15492 \r\n\r\n2. `sudo sh -c \"echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"`  and `sudo ldconfig` based on #13481\r\n\r\n3. adding the `action_env` argument based on https://stackoverflow.com/questions/47080760/tensorflow-fails-to-compile/47295278#47295278\r\n\r\nNote: When installing cudnn, I used both the runtime library and the tar file which i extracted and placed it in the /usr/local/cuda library respective folders.\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/kwotsin/tensorflow/tensorflow/core/BUILD:2131:1: C++ compilation of rule '//tensorflow/core:gpu_runtime_impl' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64 \\\r\n    PATH=/usr/local/cuda/bin:/home/kwotsin/bin:/home/kwotsin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/common_runtime/gpu/gpu_device.cc -o bazel-out/host/bin/tensorflow/core/_objs/gpu_runtime_impl/tensorflow/core/common_runtime/gpu/gpu_device.pic.o)\r\nIn file included from ./tensorflow/stream_executor/stream_executor.h:35:0,\r\n                 from ./tensorflow/core/platform/stream_executor.h:38,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_event_mgr.h:28,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_device.h:30,\r\n                 from tensorflow/core/common_runtime/gpu/gpu_device.cc:22:\r\n./tensorflow/stream_executor/stream_executor_pimpl.h:87:63: internal compiler error: Segmentation fault\r\n   PlatformKind platform_kind() const { return platform_kind_; }\r\n                                                               ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 145.189s, Critical Path: 22.73s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\nThank you.\r\n\r\n====\r\n\r\nFurther updates:\r\n\r\n1. I tried switching to the more updated 7.0.5 CuDNN (although some mentioned in #12052 that their build worked with 7.0.4. I'm now using CUDA 9.0 + CuDNN 7.05 for CUDA 9.0, and with bazel 0.8.1. The build unfortunately still doesn't work.\r\n\r\n2. CUDA 8.0 + CuDNN 6.0.21 also doesn't work, with the similar reasons as such:\r\n\r\n```\r\nC++ compilation of rule '//tensorflow/core/kernels:sparse_conditional_accumulator_op' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/sparse_conditional_accumulator_op.cc:19:0:\r\n./tensorflow/core/kernels/sparse_conditional_accumulator.h: In destructor 'tensorflow::SparseConditionalAccumulator<Device, T>::~SparseConditionalAccumulator() [with Device = Eigen::ThreadPoolDevice; T = float]':\r\n./tensorflow/core/kernels/sparse_conditional_accumulator.h:68:3: internal compiler error: Segmentation fault\r\n   };\r\n   ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 505.239s, Critical Path: 114.19s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\n```\r\nERROR: /home/kwotsin/tensorflow/tensorflow/core/kernels/BUILD:2554:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=9.0 \\\r\n    TF_CUDNN_VERSION=7.0.5 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' '-D_GLIBCXX_USE_CXX11_ABI=0' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DSNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/kernels/cwise_op_floor_div.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/cwise_op/tensorflow/core/kernels/cwise_op_floor_div.pic.o)\r\nIn file included from tensorflow/core/kernels/cwise_op_floor_div.cc:16:0:\r\n./tensorflow/core/kernels/cwise_ops_common.h: In instantiation of 'void tensorflow::functor::BinaryFunctor<Eigen::ThreadPoolDevice, Functor, NDIMS, false>::BCast(const CPUDevice&, typename tensorflow::TTypes<typename Functor::out_type, NDIMS>::Tensor, typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor, Eigen::array<long int, NDIMS>, typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor, Eigen::array<long int, NDIMS>, bool*) [with Functor = tensorflow::functor::floor_div_real<double>; int NDIMS = 4; tensorflow::functor::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<typename Functor::out_type, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<double, 4, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<typename Functor::in_type, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, long int>, 16, Eigen::MakePointer>]':\r\n./tensorflow/core/kernels/cwise_ops_common.h:136:7:   required from 'void tensorflow::BinaryOp<Device, Functor>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; Functor = tensorflow::functor::floor_div_real<double>]'\r\ntensorflow/core/kernels/cwise_op_floor_div.cc:53:1:   required from here\r\n./tensorflow/core/kernels/cwise_ops_common.h:417:3: internal compiler error: Segmentation fault\r\n   }\r\n   ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 430.853s, Critical Path: 77.17s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "comments": ["@reedwm can you comment?", "The fixes (2) and (3) both seems to be resolving the same issue. Can you try only fixes (1) and (2), and not setting `action_env`?\r\n\r\n", "@reedwm I have just tried fixes 1 and 2, but I got the same problem once again with the following error:\r\n\r\n```\r\nERROR: /home/kwotsin/tensorflow/tensorflow/core/BUILD:1858:1: C++ compilation of rule '//tensorflow/core:core_cpu_base' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64 \\\r\n    PATH=/usr/local/cuda/bin:/home/kwotsin/bin:/home/kwotsin/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -MD -MF bazel-out/host/bin/tensorflow/core/_objs/core_cpu_base/tensorflow/core/graph/graph_constructor.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/core_cpu_base/tensorflow/core/graph/graph_constructor.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/graph/graph_constructor.cc -o bazel-out/host/bin/tensorflow/core/_objs/core_cpu_base/tensorflow/core/graph/graph_constructor.pic.o)\r\ntensorflow/core/graph/graph_constructor.cc: In constructor 'constexpr std::pair<_T1, _T2>::pair(const _T1&, _U2&&) [with _U2 = std::_Rb_tree_node_base*&; <template-parameter-2-2> = void; _T1 = std::_Rb_tree_node_base*; _T2 = std::_Rb_tree_node_base*]':\r\ntensorflow/core/graph/graph_constructor.cc:1067:1: internal compiler error: Segmentation fault\r\n }  // namespace tensorflow\r\n ^\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 112.241s, Critical Path: 23.80s\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\nNot sure if it makes a difference, but I enabled jemalloc for this build, according to the installation guide. However, I also tried without jemalloc and the same problem occurs.\r\n\r\nExact commands I followed this time:\r\n\r\n```\r\n$ bazel clean\r\n$ ./configure\r\n$ sudo sh -c \"echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"\r\n$ sudo ldconfig\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --verbose_failures\r\n```\r\n\r\nMy configuration:\r\n\r\n```\r\n$ bazel clean\r\n........\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nkwotsin@darn-that-dream:~/tensorflow$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.8.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: n\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 8.0\r\n\r\n\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-8.0\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 6.0.21\r\n\r\n\r\nPlease specify the location where cuDNN 6.0.21 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-8.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n```\r\n", "I cannot reproduce with similar settings (Tensorflow 1.4, Bazel 0.8.1, CUDA 9.0, cuDNN 7.0.5, compute capabilities 6.0 and 6.1, gcc 5.4.0, Python 2.7.12, Ubuntu 16.04).\r\n\r\n/CC @gunan, any ideas what the issue could be?", "on windows we saw some compiler failures in nvcc that came bundled with cuda toolkit 9.\r\nMaster or 1.5 branch has workarounds for those compiler issues.\r\nCould you try with the master branch?\r\n\r\nIf you get a successful build, this is a nvcc bug.", "Thanks for the suggestions everyone. I think it ended up being a nvcc problem, since the build started to work after I removed cuda completely and rebooted, and re-installed again. The build now works."]}, {"number": 15649, "title": "AttributeError:  'Tensor' object has no attribute 'assign_add'", "body": "### Describe the problem\r\nI have constructed an object detector architecture based on ResNet-101. APIs in TF-Slim are mainly used in the structure. However when I finally create a train op by 'slim.learning.create_train_op' I receive such error. Tracebacks are showing below. \r\nMy environments: Ubuntu 16.04, CUDA 8.0, tensorflow 1.3.0\r\n\r\n### Source code / logs\r\n![screenshot from 2017-12-27 10-08-30](https://user-images.githubusercontent.com/30883678/34368357-4c744366-eaee-11e7-8dd2-df459b85314c.png)\r\nI found another issue same as mine but I can assure there is no variable accidentally named as 'tf' in my code as that issue suggests. I just try to build the model and no images or labels are feeded since both of them are set as tf.placeholder. It seems that such error emerges during the process of building computation graph for update ops. \r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15648, "title": "Predictor fixes for core estimators", "body": "Creating a predictor from a core estimator was broken due to:\r\n\r\n- a wrong instance check\r\n- wrong initialization of the ChiefSessionCreator\r\n\r\nThis PR fixes both.", "comments": ["Can one of the admins verify this patch?", "@jhseu could you take a look perhaps?", "Math grad tests appears to be flaking, occasionally.", "Jenkins, test this please."]}, {"number": 15647, "title": "Tensorflow.org - master version is not updated", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not Relevant\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Not Relevant\r\n- **TensorFlow installed from (source or binary)**: Not Relevant\r\n- **TensorFlow version (use command below)**: Not Relevant\r\n- **Python version**: Not Relevant\r\n- **Bazel version (if compiling from source)**: Not Relevant\r\n- **GCC/Compiler version (if compiling from source)**: Not Relevant\r\n- **CUDA/cuDNN version**: Not Relevant\r\n- **GPU model and memory**: Not Relevant\r\n- **Exact command to reproduce**: Not Relevant\r\n\r\n### Describe the problem\r\ntensorflow.org master version should be updated to master, however it seems that it hasn't been regenerated for a while.\r\n\r\nFor example the latest addition of the performance guide for `tf.data` (https://github.com/tensorflow/tensorflow/commit/ba32ea1547af74d549f35a42e4de83c88652a636) hasn't yet made it to the website:\r\n\r\nDoc source: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/performance_guide.md\r\n\r\nGenerated web page in tensorflow.org (master version):\r\nhttps://www.tensorflow.org/versions/master/performance/performance_guide\r\n\r\n(Currently shows \"Last updated November 14, 2017.\" in the bottom)\r\n\r\n", "comments": ["@gunan ", "@av8ramit @MarkDaoust @wolffg \r\n\r\n", "Fixed (manually)."]}, {"number": 15646, "title": "Branch 180147476", "body": "", "comments": ["Argh. Did a squash and merge. Will fix it at the next push."]}, {"number": 15645, "title": "Tensorflow lite 0.1.1 causing Build to fail", "body": "I am trying to use tensrflow-lite in Android. When I add \r\n\r\ncompile 'org.tensorflow:tensorflow-lite:0.1.1'\r\n\r\nI get:\r\n\r\n```\r\nError:Execution failed for task ':sample:transformClassesWithJarMergingForDebug'.\r\n> com.android.build.api.transform.TransformException: java.util.zip.ZipException: duplicate entry: R.class\r\n```\r\n\r\nI am using multidex and AGP 2.3.3. \r\n\r\nWhen I take tensorflow-lite off, the app builds correctly. When I put it back, the build fails. I believe this is a bug in the library. ", "comments": []}, {"number": 15643, "title": "fp16 inference is slower than fp32 on Nvidia Jetson TX2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.5\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: Cuda 8.0 and cuDNN 7.0\r\n- **GPU model and memory**: Jetson TX2\r\n\r\n### The problem\r\nI created a fully convolutional float 16 (half precision) neural network in tensorflow. When I run this network with some inputs, the inference time is slower than when I run the same network in float 32 (full precision) mode. \r\nI should also note that the following variables are set:\r\n\r\n`\r\nos.environ['TF_FP16_CONV_USE_FP32_COMPUTE'] = '0'\r\nos.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE'] = '0'\r\n`\r\n\r\nAs Nvidia Jetson TX2 support FP16 operations, I expected an inference time not worse than when I use FP32, but surprisingly it is about 1.5 times worse! (36 miliseconds vs 22 miliseconds). I guess it is becuase of the overhead of internal type conversion in the tensorflow core between float16 and float32!\r\n\r\nIs it a problem with Tensorflow or TX2?", "comments": ["@yzhwang any idea?\r\n", "Thank you @alirezadavoudi ! We are aware of this and we have observed this on P100. We are communicating with NVIDIA with solution for this.\r\n\r\nFirst of all, could you try with CUDA 9 + cudnn 7 and see if the same result still shows up?\r\n\r\nSecond, in your setting, you are forcing cudnn to use TRUE_HALF_CONFIG and this mode is only supported by one internal kernel: CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM as for now.  Sometimes for the same input shape, switching from FP32 to FP16 will cause the change of the tile size for this kernel. For example, an original 128*32 tile size could become 256*64 when switching from FP32 to FP16, while in this case, the gain from doubling the math is lost for the tile utilization.\r\n\r\nIn the future we will make PSEUDO_HALF_CONFIG and TRUE_HALF_CONFIG transparent to the users by adding these two configs to autotune.", "@yzhwang, unfortunately, CUDA 9 is still not released for TX2. \r\nIt seems right now the best practice is to use TensorRT. But it would be great if Tensorflow could support native FP16.", "CUDA 9.0 is actually available in JetPack 3.2 Developer Preview.\r\nhttps://developer.nvidia.com/embedded/jetpack-notes", "@alirezadavoudi \r\nI'm not experienced with Jetson, but would it be possible to profile your application with nvprof? In #15585 I described some limiting behavior with FP16, maybe this is an issue for you, too?\r\n\r\nWould love to hear your thoughts on this, since it seems that we have similar performance issues.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "This comment from @yzhwang seems to be the main issue (although it's unknown if this will significantly increase performance):\r\n>In the future we will make PSEUDO_HALF_CONFIG and TRUE_HALF_CONFIG transparent to the users by adding these two configs to autotune.\r\n\r\nSo, I'm assigning to @yzhwang.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This appears to be a duplicate of #5592. Please follow that issue. If you feel this issue should be reopened, please provide new evidence and I'll do that."]}, {"number": 15642, "title": "contrib/all_reduce not update to latest nccl", "body": "send_op, dst_tensors = nccl.broadcast(level_2_output[w], dst_devices)\r\n\r\nthis line is out of date, hope update to latest", "comments": ["this file's op not support IndexedSlices Gradient,  such as flatten_tensor \r\nhope someone can help, thanks!", "@jlebar @taehoonlee can you help? thanks", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "@skye  thank you for the reply.\r\nmy question just relate to contrib/all_reduce  module, my version is tf-1.4 + tf_cnn_benchmark-nighty, i checked that it doesn't matter to build env which i use.\r\n\r\n i use tf_cnn_benchmark with distributed_all_reduce + nccl which depends contrib/all_reduce.  but contrib/all_reduce uses nccl version that is older than contrib/nccl, that's not updated to latest nccl\r\n\r\nit's same as #15425 \r\n\r\nthanks.\r\n\r\n"]}, {"number": 15641, "title": "Compile with selective register on meta file", "body": "I want to apply the selective registration feature on my model to decrease the lib size. However, I need to apply it on a meta file saved via `saver` rather than a frozen pb file as shown in many posts I have found. When I try to run \r\n\r\n`bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=model_test.ckpt-390760.meta`\r\n\r\nit comes to the error\r\n\r\n`[libprotobuf ERROR external/protobuf/src/google/protobuf/wire_format_lite.cc:621] String field 'tensorflow.NodeDef.op' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.`\r\n\r\nI tried to add `--proto_fileformat=textproto` but another error comes up:\r\n\r\n`raise self.ParseError('Expected identifier or number.')\r\ngoogle.protobuf.text_format.ParseError: 2:1 : Expected identifier or number.`\r\n\r\nIs it even possible to do this at all? The ultimate goal of compiling this lib is to restore a pretrained model and incrementally train it on Android.", "comments": ["Maybe try tf.train.write_graph(sess.graph_def, \"/tmp/load\", \"test.pb\") and load the pb file?\r\n", "Thanks @bignamehyp it worked.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied."]}, {"number": 15640, "title": "adding ps_strategy to run_config to enable different placement strate\u2026", "body": "\u2026gy in estimator\r\n\r\nThis is my first pull request. I noticed that there isn't any way to set the `ps_strategy` in `tf.train.replica_device_setter` if `tf.estimator.Estimator` is used in distributed settings. I guess the best way to notify the estimator which strategy to use is through the `RunConfig` class. I have added a property `ps_strategy` to `RunConfig` and filled in some of the unit tests. I have also modified the `_get_replica_device_setter` in `estimator.py` to return a device setter with the `ps_strategy` included. Please kindly review and let me know if there is anything I need to correct or modify", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "The way to achieve this is the device_fn. There's already a private field in Estimator (_device_fn) which is used for that purpose. \r\n\r\nAs a hack you can overwrite this after construction, but if we want to make this official, I would suggest we add a @property device_fn() to RunConfig, which is used to create the _device_fn field instead of the module private function used at the moment. You could then change what that property returns to change the PS placement strategy.\r\n\r\n@xiejw WDYT?", "Thanks @dave-msk.\r\n\r\n@martinwicke  This is fine. I prefer device_fn over ps_strategy due to the fact that device_fn is more general but ps_strategy assumes ps is not the cluster.\r\n\r\nHowever, it will be user's own risk to debug this. device_fn or ps_strategy is not easy to write. ", "@martinwicke @xiejw ,\r\nThanks for reviewing. It makes sense to me to add @property to Run_Config() that client code could just do this by an extra `config.device_fn = foo`. I'll try to get this updated asap", "I see the current approach for distributed training is that the RunConfig first parses the `TF_CONFIG` environment variable to get details like `master`, `task_id`, `num_ps_replicas` etc. These are then used to create a replica device setter when Estimator is constructed through the `training.replica_device_setter` call. To allow a dynamic `device_fn` override while preserving the convenience of the current approach, I'd suggest we do it this way:\r\n\r\n1. We add a @property `device_fn` to RunConfig, set it `None` as default\r\n2. We call `self._config.device_fn` to get the most updated version of `device_fn` in `_train_model` in Estimator.\r\n3. If the `device_fn` is `None` (not overridden by user) in RunConfig, we use the current approach (via `training.replica_device_setter` call) to return a \"default\" device function, with the `ps_strategy` I added in my previous commit.\r\n4. If the `device_fn` is not `None` (overridden by user) in RunConfig, we return it directly.\r\n\r\nIf one chooses to write his own device function, he would have to define the `device_fn` (`config.device = foo`) and take care of everything manually. If the user just wanted to specify a `ps_strategy` other than the default round-robin (e.g. the `tf.contrib.training.GreedyLoadBalancingStrategy`), he could just construct the RunConfig with `ps_strategy` or do a replace (`config = config.replace(ps_strategy=ps_strategy)`).\r\n\r\n@xiejw @martinwicke what do you think?", "Hi @dave-msk I think most of part of your suggestions sound great. \r\n\r\nWe should allow user to specify the device_fn in RunConfig and respect that. If the device_fn is None, we create a default one (old behavior). \r\n\r\nI left some comments in the files. I think we should drop the ps_strategy in the RunConfig as device_fn provides higher level support for that. Advanced users could create device_fn directly and use different ps_strategy. ", "The `ps_strategy` is removed. So the usage will now be as follows:\r\n\r\n1. The user has a self-defined device function `device_fn`:\r\n```\r\nconfig = RunConfig(device_fn=device_fn)\r\n```\r\n\r\n2. The user wants to change only the ps strategy to `ps_strategy` in the default device function used by `Estimator`. In this case the easiest way is to use the code from `_get_replica_device_setter(config)` function in `estimator.py`, with an addition parameter `ps_strategy`:\r\n\r\n```\r\ndef get_replica_device_setter(config, ps_strategy):\r\n  ps_ops = [\r\n      'Variable', 'VariableV2', 'AutoReloadVariable', 'MutableHashTable',\r\n      'MutableHashTableV2', 'MutableHashTableOfTensors',\r\n      'MutableHashTableOfTensorsV2', 'MutableDenseHashTable',\r\n      'MutableDenseHashTableV2', 'VarHandleOp'\r\n  ]\r\n\r\n  if config.task_type:\r\n    worker_device = '/job:%s/task:%d' % (config.task_type, config.task_id)\r\n  else:\r\n    worker_device = '/job:worker'\r\n\r\n  if config.num_ps_replicas > 0:\r\n    return training.replica_device_setter(\r\n        ps_tasks=config.num_ps_replicas,\r\n        worker_device=worker_device,\r\n        merge_devices=True,\r\n        ps_ops=ps_ops,\r\n        cluster=config.cluster_spec,\r\n        ps_strategy=ps_strategy)  # This is the only difference from the original code\r\n  else:\r\n    return None\r\n\r\nconfig = RunConfig()\r\nconfig = config.replace(device_fn=get_replica_device_setter(config, ps_strategy))\r\n```\r\n\r\n3. If the user wants to use the default device function, no code change is required.\r\n\r\nAdding a `device_fn` gives a great flexibility to users, but it doesn't feel right if we need to dive deep into the source code and define a slightly modified version of `_get_replica_device_setter` if the ps_strategy is the only thing one wants to change. What do you think?", "Thanks. The code looks good to me. This is a tradeoff we need to decide. Currently, Estimator mostly works with ps in distributed model. But in future, this assumption might not hold anymore. Device_fn provides a better abstraction for that. \r\n\r\nRegarding the issue changing ps_strategy: The get_replica_device_setter can be much simplified as\r\n\r\n     def get_replica_device_setter(config, ps_strategy):\r\n      ps_ops = [\r\n          'Variable', 'VariableV2', 'AutoReloadVariable', 'MutableHashTable',\r\n          'MutableHashTableV2', 'MutableHashTableOfTensors',\r\n          'MutableHashTableOfTensorsV2', 'MutableDenseHashTable',\r\n          'MutableDenseHashTableV2', 'VarHandleOp'\r\n      ]\r\n\r\n     worker_device = '/job:%s/task:%d' % (config.task_type, config.task_id) \r\n     return training.replica_device_setter(\r\n            ps_tasks=config.num_ps_replicas,\r\n            worker_device=worker_device,\r\n            merge_devices=True,\r\n            ps_ops=ps_ops,\r\n            cluster=config.cluster_spec,\r\n            ps_strategy=ps_strategy)  # This is the only difference from the original code\r\n \r\n\r\nAs user is changing the ps_strategy, all the if-else branches are not necessary. The only remaining silly thing here is the ps_ops list. I think it should not be defined here and user needs to copy/paste. I will move it as a constant list to replica_device_setter. such that user can easily refer them. With that, user code could be significantly reduced. Does this sound reasonable? ", "Makes sense. So I guess the code becomes:\r\n\r\n```\r\ndef get_replica_device_setter(config, ps_strategy):\r\n  worker_device = '/job:%s/task:%d' % (config.task_type, config.task_id) \r\n  return training.replica_device_setter(\r\n      ps_tasks=config.num_ps_replicas,\r\n      worker_device=worker_device,\r\n      merge_devices=True,\r\n      ps_ops=tf.train.some_ref_to_ps_ops,  # just some reference to the original ps_ops\r\n      cluster=config.cluster_spec,\r\n      ps_strategy=ps_strategy)\r\n```\r\n\r\nIt looks cool, couldn't think of a better way to do it right now", "This is good for API review", "I have noticed that there is a merge conflict between `device_fn` and the new `distribute` variable. Should I resolve the merge conflict and commit again? @xiejw ", "Need to update the API goldens:\r\n\r\n```\r\n\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```", "Also errors such as this:\r\n\r\n```\r\nERROR: testPoissonRegression (__main__.LinearEstimatorTest)\r\nTests that loss goes down with training.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/linear_test.py\", line 1792, in testPoissonRegression\r\n    head=head_lib.poisson_regression_head())\r\nFAIL: //tensorflow/contrib/learn:debug_test (shard 3 of 4) (see /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/testlogs/tensorflow/contrib/learn/debug_test/shard_3_of_4/test.log)\r\nINFO: From Testing //tensorflow/contrib/learn:debug_test (shard 3 of 4):\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/linear.py\", line 980, in __init__\r\n    feature_engineering_fn=feature_engineering_fn)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1179, in __init__\r\n    super(Estimator, self).__init__(model_dir=model_dir, config=config)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 250, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 455, in __init__\r\n    self._config = self._config.replace(model_dir=self._model_dir)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/python/estimator/run_config.py\", line 744, in replace\r\n    **kwargs)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/python/estimator/run_config.py\", line 779, in _replace\r\n    _validate_properties(config)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/python/estimator/run_config.py\", line 288, in _validate_properties\r\n    message='device_fn must be callable with exactly one argument \"op\".')\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/python/estimator/run_config.py\", line 257, in _validate\r\n    property_value = getattr(run_config, property_name)\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/contrib/learn/linear_test.runfiles/org_tensorflow/tensorflow/python/estimator/run_config.py\", line 593, in device_fn\r\n    return self._device_fn\r\nAttributeError: 'RunConfig' object has no attribute '_device_fn'\r\n```", "Added manual initialization to old RunConfig, please review. @xiejw ", "LGTM. Thanks for the fix. I have kicked the tests again for you.", "The golden API of estimator is updated through\r\n```\r\n1. bazel build //tensorflow/tools/api/tests:api_compatibility_test\r\n2. bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\n```\r\nThis should pass the API test I think. Please review @xiejw", "@jhseu please let me know if I need review here. I think the code was looking good but got a merge conflict later. I reviewed again. The current status LGTM"]}, {"number": 15639, "title": "fix doc for benchmark_model for android", "body": "after configure.py set framework_shared_object=true,\r\nbenchmark_model won't build for Android without tweeks. Add\r\n'--config monolithic' to avoid confusion.", "comments": ["Can one of the admins verify this patch?", "@gunan is moving to monolithic OK?", "@allenlavoie @petewarden Do we want to build monolothic TF for android?\r\nOr would you rather we fix the issues around the shared libraries on android.", "When I did the split, I included the implementation rules explicitly from Android build targets. My hope was that Android would then just build without extra config options. It's possible I missed some (un-tested?) targets.\r\n\r\nSo one solution would be to figure out which header-only rules don't have their implementation rules included and then add the implementation rules explicitly (I might be able to tell you based on which symbols are undefined). But building with --config=monolithic does accomplish the same thing. I don't have a strong preference.", "FYR. It's possible to build it successfully when \"-lpthread\" in tensorflow/core/BUILD removed (Android doesn't have libpthread), see patch [here](https://github.com/freedomtan/tensorflow/commit/3f76476b5959c042e625cbfbbc8b563a7c6fd157). But the binary won't running on new Android devices because of protocol buffer mismatch. On my Pixel 2 running Android 8.1, I saw the following runtime error message:\r\n\r\n`[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/stubs/common.cc:79] This program was compiled against version 3.0.0 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.4.0).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"out/soong/.intermediates/frameworks/av/drm/libmediadrm/libmediadrm/android_arm64_armv8-a_cortex-a73_shared_core/gen/proto/frameworks/av/drm/libmediadrm/protos/plugin_metrics.pb.cc\".)terminate called after throwing an instance of 'google::protobuf::FatalException'`\r\n\r\nI didn't try to fix this.\r\n  ", "Interesting, thanks. So the issue is that the default non-monolithic build omits a protocol buffer implementation (it lives in libtensorflow_framework.so), which is then supplied by Android, and the version doesn't necessarily match.\r\n\r\nIf that's correct, then adding \"@protobuf_archive//:protobuf\" as a dep for the build target would also fix this (i.e. would statically include a protocol buffer implementation which matches the header version). It would be important that libtensorflow_framework.so *not* be linked in (should be true for Android anyway), since multiple protocol buffer implementations will cause issues.", "@allenlavoie: Nope, it seems to me it's the included protobuf (\"@protobuf_archive//:protobuf\") which has new version (3.4.0) somehow conflicts with what is used to build Android (3.0.0). Further information,\r\n \r\n1. for non-monolithic benchmark_model, as you know, we need libtensorflow_framework.so\r\n2. protobuf 3.4.0 is in the the libtensorflow_framework.so already\r\n3. to use the libtensorflow_framework.so, I need LD_LIBRARY_PATH because Android Bionic dynamic linker doesn't support rpath.\r\n4. after  I set LD_LIBRARY_PATH, other system components which use protobuf will use protobuf in libtensorflow_framework.so. I don't why function resolving works this way :-(  \r\n\r\nAnother example, on an older phone running Android 6.0.x, I got the following message:\r\n\r\n`\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/stubs/common.cc:79] This program was compiled against version 2.6.1 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.4.0).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"out/target/product/msm8916_64/gen/SHARED_LIBRARIES/libGLES_trace_intermediates/proto/frameworks/native/opengl/libs/GLES_trace/proto/gltrace.pb.cc\".)terminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  This program was compiled against version 2.6.1 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.4.0).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"out/target/product/msm8916_64/gen/SHARED_LIBRARIES/libGLES_trace_intermediates/proto/frameworks/native/opengl/libs/GLES_trace/proto/gltrace.pb.cc\".)\r\n`", "Interesting. If it works, I don't have a problem with adding --config=monolithic to the instructions. Maybe we should consider building monolithically automatically when targeting Android?"]}, {"number": 15638, "title": "PS:0 runs nothing but seizes the network in the distributed training", "body": "   It's strange but I think is a bug in tensorflow. When I use multi machine as PS, I specify the operations in PS:* but not PS:0. And I not use the PS:0, but the variable transfer between the PS:* and PS:0. By the way, I  use the lenet5 about 1.6M parameters, but when iterating one time, the data between PS:* and PS:0 is 4.7M in the network using tcpdump to calculate it. I don't know why. The following is my main code in worker machine and PS server just run server.join().\r\nhttps://github.com/niewuya/tensorflow-distributed-training/blob/master/code.py\r\nI will appreciate your reply.\r\n\r\n```python\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\n\r\nparameter_servers = [\"172.16.101.248:2225\",\"172.16.101.249:2225\",\"172.16.101.105:2225\"]\r\nworkers = [\"172.20.110.94:2225\"]\r\ncluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\nserver = tf.train.Server(\r\n    cluster,\r\n    job_name=\"worker\",\r\n    task_index=0)\r\n\r\ndef dataset_input_fn():\r\n    buffer_size = 1024\r\n    batch = 128\r\n    num_epochs = 50\r\n    filenames = ['../datasets/mnist/train.tfrecord']\r\n    dataset = tf.data.TFRecordDataset(filenames)\r\n\r\n    def parser(record):\r\n        keys_to_features = {\r\n            \"image/encoded\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n            \"image/class/label\": tf.FixedLenFeature((), tf.int64,\r\n                                                    default_value=tf.zeros([], dtype=tf.int64)),\r\n        }\r\n        parsed = tf.parse_single_example(record, keys_to_features)\r\n\r\n        image = tf.image.decode_jpeg(parsed[\"image/encoded\"])\r\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\r\n        image = tf.reshape(image, [28, 28, 1])\r\n        label = tf.cast(parsed[\"image/class/label\"], tf.int32)\r\n        label = slim.one_hot_encoding(label, 10)\r\n        return image, label\r\n\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.map(parser, num_parallel_calls=8)\r\n    dataset = dataset.prefetch(buffer_size=batch)\r\n    dataset = dataset.prefetch(buffer_size=buffer_size)\r\n    dataset = dataset.shuffle(buffer_size)\r\n    dataset = dataset.batch(batch)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    images, labels = iterator.get_next()\r\n    return images, labels\r\n\r\ndef lenet(images,labels,flag):\r\n    num_classes = 10\r\n    dropout_keep_prob = 0.5\r\n    scope = tf.variable_scope(\"lenet\", reuse=flag)\r\n    with scope, slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                               activation_fn=tf.nn.relu,\r\n                               weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\r\n                               weights_regularizer=slim.l2_regularizer(0.0)):\r\n        net = slim.conv2d(images, 20, [5, 5], scope='conv1_1')\r\n        net = slim.max_pool2d(net, [2, 2], 2, scope='pool1_1')\r\n        net = slim.conv2d(net, 50, [5, 5], scope='conv1_2')\r\n        net = slim.max_pool2d(net, [2, 2], 2, scope='pool1_2')\r\n        net = slim.flatten(net)\r\n        net = slim.fully_connected(net, 500, scope='fc1_3')\r\n        net = slim.dropout(net, dropout_keep_prob, scope='dropout1_3')\r\n        logits = slim.fully_connected(net, num_classes, activation_fn=None, scope='fc1_4')\r\n        loss = tf.losses.softmax_cross_entropy(\r\n            logits=logits, onehot_labels=labels)\r\n        loss = tf.reduce_mean(loss)\r\n        optimizer = tf.train.AdagradOptimizer(0.01)\r\n        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n        grads = optimizer.compute_gradients(loss)\r\n    return grads, accuracy, optimizer\r\n\r\nwith tf.Graph().as_default():\r\n    with tf.device('/CPU:0'):\r\n        all_grads = []\r\n        with tf.device('/job:ps/task:2'):\r\n            images, labels = dataset_input_fn()\r\n            grads, accuracy_ps, optimizer =lenet(images,labels,False)\r\n\r\n        all_grads.clear()\r\n        all_grads.append(grads)\r\n        train_op_all=[]\r\n        for i, grad_and_vars in enumerate(zip(*all_grads)):\r\n            grads = []\r\n            for g, _ in grad_and_vars:\r\n                g = tf.expand_dims(g, 0)\r\n                grads.append(g)\r\n            grad = tf.concat(axis=0, values=grads)\r\n            grad = tf.reduce_mean(grad, 0)\r\n            v = grad_and_vars[0][1]\r\n            train_op_all.append(optimizer.apply_gradients([(grad, v)]))\r\n        train_op=tf.group(*train_op_all)\r\n\r\n    with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=True\r\n                                           )as mon_sess:\r\n        frequency=10\r\n        for i in range(1000):\r\n            _ ,accuracy= mon_sess.run([train_op,accuracy_ps])\r\n            if i % frequency ==0 :\r\n                print(\"accuracy is :%f\"%accuracy)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 15637, "title": "Fix an lib error while building with CUDA9.1", "body": "Some header files in CUDA9.1 was moved into dir cuda/include/crt causing when building with CUDA9.1 headers like math_functions.hpp could no long be loaded.  Adding the directory into BUILD file fixs the issue. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "@nluehr This looks to be a breaking change coming to us from CUDA side. Is there a documentation, recommended action for this?", "Also, This is a PR to the release branch. We do not accept any PRs to the release branch, please send this change against master first.", "Can one of the admins verify this patch?"]}, {"number": 15636, "title": "Read tflite file failed on iOS", "body": "Hi\r\n\r\nI tried the examples on iOS according to TF Lite guide, but failed when assign data to tflite because address \"out\" is NULL. probably my tflite file is incorrect, but I am not sure, can anybody give some help? \r\n\r\nMy test step is as follows:\r\n1.  Try the following code(https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite),  get the tflite file converteds_model.tflite\r\n\r\n```\r\nimport tensorflow as tf\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nval = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])\r\n  open(\"converteds_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n2. Integrated the tflite into my app, which is from iOS sample code \"simple\"(/Users/Sensteer/Software/tensorflowclone/tensorflow/tensorflow/contrib/lite/examples/ios/simple).But exception happed because address \"out\" is NULL\r\n\r\n```\r\nint input = interpreter->inputs()[0];                                 //input is 3\r\nfloat* out = interpreter->typed_tensor<float>(input);          //out is NULL\r\n```\r\n\r\nSo my questions are:\r\n1. The tflite created above is right or not?\r\n2. The reading tflite code is right or not?\r\n2. If the tflite file is not right, do I must create tflite with  \"pb\", \"ckpt\" and \"FrozenGraphDef\" mentioned in guide?\r\n\r\nThanks\r\n", "comments": ["I had the same problem.  It seemed that the converted part failed. \r\nYou'd better use bazel_bin command to build the conversion(Not python code). And you should check the tflite file size(1/3 of .data file size). Then when you move the file to you mac os, its small file icon shows the black window which means it is a binary file. That means your conversion is success.\r\nGood luck.", "@andrehentz @aselle - Any guidance here?", "@llyyun Can you verify that your converteds_model.tflite is not empty?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "yes, the tflite file is incorrect. finally we used bazel command build the model file", "I'm having the same issue. @llyyun, how did you resolve this? I'm trying to use visualize.py to check out my model. Unfortunately, that is failing too! Is there a good way to check the model?\r\n\r\n", "Here's the output from visualize.py\r\n```\r\ntylers-iMac:tensorflow tyler$ bazel run tensorflow/contrib/lite/tools:visualize --  /Users/tyler/face-it/facenet-models/frozen_models/mobilenet_v0.1.tflite /Users/tyler/Desktop/mobilenet_v0.1.tflite.html\r\nINFO: Analysed target //tensorflow/contrib/lite/tools:visualize (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/tools:visualize up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/tools/visualize\r\nINFO: Elapsed time: 0.252s, Critical Path: 0.00s\r\nINFO: Build completed successfully, 1 total action\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/tools/visualize /Users/tyler/face-it/facenet-models/frozen_models/mobilenet_v0.1.tflite /Users/tyler/Desktop/mobilenet_v0.1.tflite.html\r\nsh: third_party/flatbuffers/flatc: No such file or directory\r\nthird_party/flatbuffers/flatc -t --strict-json --defaults-json -o /tmp third_party/tensorflow/contrib/lite/schema/schema.fbs -- /Users/tyler/face-it/facenet-models/frozen_models/mobilenet_v0.1.tflite\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_tyler/980b2407727d0312ad05cca671189978/execroot/org_tensorflow/bazel-out/darwin-py3-opt/bin/tensorflow/contrib/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/contrib/lite/tools/visualize.py\", line 391, in <module>\r\n    main(sys.argv)\r\n  File \"/private/var/tmp/_bazel_tyler/980b2407727d0312ad05cca671189978/execroot/org_tensorflow/bazel-out/darwin-py3-opt/bin/tensorflow/contrib/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/contrib/lite/tools/visualize.py\", line 387, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/private/var/tmp/_bazel_tyler/980b2407727d0312ad05cca671189978/execroot/org_tensorflow/bazel-out/darwin-py3-opt/bin/tensorflow/contrib/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/contrib/lite/tools/visualize.py\", line 307, in CreateHtmlFile\r\n    data = json.load(open(real_output))\r\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/mobilenet_v0.1.json'\r\nERROR: Non-zero return code '1' from command: Process exited with status 1\r\n```", "Here's the output of toco:\r\n```\r\nWARNING:tensorflow:From /usr/local/Cellar/python/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\n2018-05-01 07:11:47.154904: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 225 operators, 419 arrays (0 quantized)\r\n2018-05-01 07:11:47.163571: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 225 operators, 419 arrays (0 quantized)\r\n2018-05-01 07:11:47.193564: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 31 operators, 88 arrays (0 quantized)\r\n2018-05-01 07:11:47.194331: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 31 operators, 88 arrays (0 quantized)\r\n2018-05-01 07:11:47.194877: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 3276800 bytes, theoretical optimal value: 2457600 bytes.\r\n2018-05-01 07:11:47.195146: I tensorflow/contrib/lite/toco/toco_tooling.cc:307] Estimated count of arithmetic ops: 0.582426 billion (note that a multiply-add is counted as 2 ops).\r\n```\r\n\r\nShould there be some success message at the end?", "I figured it out. The example is quantized...but I'm using floats. Here's how I changed it to work.\r\n```\r\n    \r\n    int input = interpreter->inputs()[0];\r\n    \r\n\r\n    \r\n    auto* out = interpreter->typed_tensor<float>(input);\r\n    for (int y = 0; y < wanted_input_height; ++y) {\r\n        auto* out_row = out + (y * wanted_input_width * wanted_input_channels);\r\n        for (int x = 0; x < wanted_input_width; ++x) {\r\n            const int in_x = (y * image_width) / wanted_input_width;\r\n            const int in_y = (x * image_height) / wanted_input_height;\r\n            auto* in_pixel = in + (in_y * image_width * image_channels) + (in_x * image_channels);\r\n            auto* out_pixel = out_row + (x * wanted_input_channels);\r\n            for (int c = 0; c < wanted_input_channels; ++c) {\r\n                out_pixel[c] = in_pixel[c];\r\n            }\r\n        }\r\n    }\r\n    \r\n    double startTimestamp = [[NSDate new] timeIntervalSince1970];\r\n    if (interpreter->Invoke() != kTfLiteOk) {\r\n        LOG(FATAL) << \"Failed to invoke!\";\r\n    }\r\n    double endTimestamp = [[NSDate new] timeIntervalSince1970];\r\n    total_latency += (endTimestamp - startTimestamp);\r\n    total_count += 1;\r\n    NSLog(@\"Time: %.4lf, avg: %.4lf, count: %d\", endTimestamp - startTimestamp,\r\n          total_latency / total_count, total_count);\r\n    \r\n      const int output_size = 256;\r\n      const int kNumResults = 5;\r\n      const float kThreshold = 0.1f;\r\n    \r\n    std::vector<std::pair<float, int>> top_results;\r\n    \r\n    auto* output = interpreter->typed_output_tensor<float>(0);\r\n    \r\n    NSData *dataData = [NSData dataWithBytes:output length:sizeof(output)];\r\n    NSLog(@\"output %@\", dataData);\r\n```", "how did you fix the problem \"sh: third_party/flatbuffers/flatc: No such file or directory\"? I encounter the same problem, and can't find the proper way to solve. THANK YOU. @tslater ", "@kuanzi This will hopefully be fixed soon. Meanwhile, you might be able to workaround the issue by changing: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tools/visualize.py#L35"]}, {"number": 15635, "title": "Android: Op type not registered 'GatherTree' in binary running on localhost.", "body": "-----------------------\r\n\r\n# System information\r\n- **OS Platform and Distribution : Linux Ubuntu 14.04\r\n- **TensorFlow installed from source \r\n- **TensorFlow version 1.4.0\r\n- **Python 2.7\r\n- **Bazel version [bazel release 0.8.0]\r\n- **GCC/Compiler version gcc version 4.9.4 (GCC)\r\n- **CUDA/cuDNN (only for cpu)\r\n\r\n\r\n# Describe the problem\r\nHi.  I have tried to load the rnn model inside Android that I generated from python for machine translation. When I want to use beam_search in decode I meet an (can't find ** op) error at Android. I have tried below solutions but the error is continue.\r\n## solution 1:\r\nuse python script \u201ctensorflow/python/tools/print_selective_registration_header.py\u201d to create \"ops_to_register.h\", then copy it into \"tensorflow/core/framework/\", then generate \"//tensorflow/contrib/android:libtensorflow_inference.so\"\r\n### I change \"print_selective_registration_header.py\":\r\n`parser.add_argument(\r\n'--default_ops',\r\ntype=str,\r\n#default='NoOp:NoOp,_Recv:RecvOp,_Send:SendOp',\r\n default='all',`\r\n...)\r\n### cmd like below:\r\n`bazel build tensorflow/python/tools:print_selective_registration_header &&   bazel-bin/tensorflow/python/tools/print_selective_registration_header     --graphs=/path/to/my/model/decode-model_1213_real_model_with_beam.pb > ops_to_register.h`\r\n`bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\"     --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\"   //tensorflow/contrib/android:libtensorflow_inference.so     --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a`\r\n\r\n## solution 2:\r\nI met the same type of mistake before, and with \"Op type not registered ListDiff\" error. I find \"listdiff_op.cc\" is at \"tensorflow/core/kernels\". Then I add a line at \"tensorflow/core/kernels/BUILD\", and re-generate libtensorflow_inference.so to solve that error. But the \"beam_search_ops.cc\" is at \"tensorflow/contrib/seq2seq/kernels\", when I add it like \"ListDiff\" it's still reporting that error at android studio.\r\n### add line like below:\r\n` filegroup(\r\nname = \"android_extended_ops_group1\",\r\nsrcs = [\r\n\"listdiff_op.cc\",\r\n#\"//tensorflow/contrib/seq2seq/kernels/beam_search_ops.cc\",\r\n...]\r\n)\r\n`\r\n\r\nI need some help for solving this problem. thanks.\r\n\r\n# Source code / logs\r\n## error log at android studio:\r\n`E/AndroidRuntime: FATAL EXCEPTION: main\r\n                  Process: com.example.phua.mt_1201, PID: 20817\r\n                  java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.phua.mt_1201/com.example.phua.mt_1201.MainActivity}: org.tensorflow.TensorFlowException: Op type not registered 'GatherTree' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2955)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3030)\r\n                      at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1696)\r\n                      at android.os.Handler.dispatchMessage(Handler.java:105)\r\n                      at android.os.Looper.loop(Looper.java:164)\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6938)\r\n                      at java.lang.reflect.Method.invoke(Native Method)\r\n                      at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:327)\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1374)\r\n                   Caused by: org.tensorflow.TensorFlowException: Op type not registered 'GatherTree' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.\r\n                      at org.tensorflow.Graph.importGraphDef(Native Method)\r\n                      at org.tensorflow.Graph.importGraphDef(Graph.java:118)\r\n                      at org.tensorflow.Graph.importGraphDef(Graph.java:102)\r\n                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:396)\r\n                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:97)\r\n                      at com.example.phua.mt_1201.tfonandroid.Loadmodel(tfonandroid.java:37)\r\n                      at com.example.phua.mt_1201.MainActivity.onCreate(MainActivity.java:52)\r\n                      at android.app.Activity.performCreate(Activity.java:7174)\r\n                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1220)\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2908)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3030)\u00a0\r\n                      at android.app.ActivityThread.-wrap11(Unknown Source:0)\u00a0\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1696)\u00a0\r\n                      at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n                      at android.os.Looper.loop(Looper.java:164)\u00a0\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6938)\u00a0`\r\n\r\n\r\n", "comments": ["@aselle ", "@andrewharp, could you take a look?", "Nagging Assignee @andrewharp: It has been 268 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 15634, "title": "ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory with python 2.7", "body": "Ubuntu 16.04\r\nGPU: 1080 Ti\r\nCuda 9.0\r\nCuDDN 7.0 v7\r\nUsing my PC, not a VM\r\nInstalled Intel MKL-DNN by this [guilde](https://github.com/mind/wheels#mkl), but looks like something wrong, because when trying to make a test for tensorflow, got error:\r\n\r\n```\r\ngagazet@woof:~/Desktop$` python 123.py \r\nTraceback (most recent call last):\r\n  File \"123.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nRecently someone created same problem and it was closed with reason: PR [#12975](https://github.com/tensorflow/tensorflow/pull/12975), but it was for a VM.\r\nCan anyone explain, please, what i can be and how to fix it? Dont have any build_pip_package scrips at the TF folder. \r\nIts doesnt looks like same problem as PR [#12975](https://github.com/tensorflow/tensorflow/pull/12975)\r\n", "comments": ["The error message suggests a linker configuration or MKL installation issue. Does `libmkl_intel.so` exist on your filesystem and is it available to the linker? (You might get some clues from `ldd /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so`)\r\n\r\nFYI @tfboyd in case he has other suggestions.\r\n\r\nHowever, since the distribution (`.whl` file) you're using is not something maintained by the TensorFlow team, but rather maintained by TinyMind in their github project at https://github.com/mind/wheels - I'm going to suggest that you seek help there."]}, {"number": 15633, "title": "[Question&Error] Is there detection model like a SSD-Mobile-net in tensorflow-lite?", "body": "HI.\r\n\r\nDeveloping an android application using tensorflow-lite.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md\r\nNot found detection model.\r\n\r\nAlso, I try to convert SSD-Inceptionv2 using tensorflow-lite-API. But there seems to be a problem.\r\n\r\n##Command\r\n<pre><code>\r\nbazel run --config=opt --copt=-msse4.1 --copt=-msse4.2 \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/home/danshin/tensorflow_lite/lite_model/fire_incpetion_v2.pb \\\r\n  --output_file=/home/danshin/tensorflow_lite/lite_model/fire_inception_v2.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shape=1,300,300,3 \\\r\n  --input_array=image_tensor \\\r\n  --output_array={detection_boxes,detection_scores,detection_classes,num_detections}\r\n</code></pre>\r\n\r\n##Error code\r\n<pre><code>\r\n2017-12-26 14:59:25.159220: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 2029 operators, 3459 arrays (0 quantized)\r\n2017-12-26 14:59:25.251633: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \r\n</code></pre>\r\n\r\nThe fire_inception_v2 file is created, but its size is zero bytes.\r\nWhat is a problem?\r\n\r\n\r\nalso,\r\n**please let me know what's the best way to deploy custom model for object detection?**\r\n\r\nSomebody help me plz!.\r\n\r\nthank you.", "comments": ["@aselle  can you please take a look at this issue? Thanks.", "We are currently working to convert mobilenet SSD (and then inception ssd after that) , but it contains ops that are not supported completely.  I will update this issue once we have that done.", "Great, I have asked similar question here: https://github.com/tensorflow/tensorflow/issues/14731\r\n\r\nHow long do you reckon until you guys add support from ssd-mobilenet?\r\n\r\nThanks,\r\nMartin Peniak", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any updates?\r\nI'm also facing a similar issue. Thanks in advance.", "@yucheeling", "Could you please suggest any dataset like \"`ssd_mobilenet_v1_coco_2017_11_17.tar`\" which can be used in a retail shop for different apparel identification like t-shirts, jeans etc. ", "@rana3579, please ask such a question on stackoverflow. A quick update on mobilenet ssd. This is progressing and we hope we will have an example out soon.", "@rana3579 check my video, got this running on movidius, nvidia gpus as well as arm processors. I cannot share the dataset but if you are part of a company we could talk about potential collaboration: https://www.youtube.com/watch?v=3MinI9cCJrc", "@aselle thanks for the update! Where to look for the notifications on this? I would like to be notified as soon as it is out if that is possible. Thank you, I appreciate your hard-work on this!\r\n", "@andrewharp, is working on this and will be updating the Java TF Mobile app to use tflite. So watch for those changes in the repository. I'll leave this issue open for now.", "This is functional internally; should have something out in the next week or two.", "@andrewharp thats awesome!! Does that also go for the iOS camera example?\r\nAlso what is the size of the weights and performance looking like?\r\nThe TFLite classification mobilenet is tiny and the performance on iOS is buttery smooth so im really excited for TFLite.\r\n\r\nSome others already converted the existing SSD Mobilenet pb to a coreml model and wrote the missing output layers in Swift:\r\nhttps://github.com/vonholst/SSDMobileNet_CoreML\r\n\r\nBut thats only really like 8-12 fps on an iPhone 7.", "Hi,\r\nAny update on this?", "I am also curious :)", "I have a commit porting the Android TF demo to tflite currently under review, should show up on github this week hopefully.\r\n\r\n@madhavajay It's Android only, but you should be able to adapt it for iOS. The only thing is that some of the pre-processing (image resizing/normalization) and post-processing (non-max suppression and adjustment by box priors) is done in Java as tflite doesn't fully support all the operators used by MobileNet SSD.", "@andrewharp That\u2019s awesome. Can you briefly explain why those operations are not available currently in TF lite. Seems the same case for the tfcoreml conversion tool on regular SSD. Not complaining just asking out of technical interest, do they do something that\u2019s particularly difficult to implement in the mobile stack or is it just low priority?", "Looking forwards to seeing your epic effort on the Android code!!! Thanks a lot. I know im not the only one looking forwards to this!", "@andrewharp,  and @aselle    Any update on getting demo for using SSD based Object Localization example for TFLite?", "It's live now at [tensorflow/contrib/lite/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android)! This is a more complete port of the original TF Android demo (only lacking the Stylize example), and will be replacing the other demo in tensorflow/contrib/lite/java/demo going forward.\r\n\r\nA converted TF Lite flatbuffer can be found in [mobilenet_ssd_tflite_v1.zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_ssd_tflite_v1.zip), and you can find the Java inference implementation in [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java). Note that this differs from the original TF implementation in that the boxes must be manually decoded in Java, and a box prior txt file needs to be packaged in the apps assets (I think the one included in the model zip above should be valid for most graphs). \r\n\r\nDuring TOCO conversion a different input node (Preprocessor/sub) is used, as well as different output nodes (concat,concat_1). This skips some parts that are problematic for tflite, until either the graph is restructured or TF Lite reaches TF parity.\r\n\r\nHere are the quick steps for converting an SSD MobileNet model to tflite format and building the demo to use it:\r\n```\r\n# Download and extract SSD MobileNet model\r\nwget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\r\ntar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz \r\nDETECT_PB=$PWD/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\r\nSTRIPPED_PB=$PWD/frozen_inference_graph_stripped.pb\r\nDETECT_FB=$PWD/tensorflow/contrib/lite/examples/android/assets/mobilenet_ssd.tflite\r\n\r\n# Strip out problematic nodes before even letting TOCO see the graphdef\r\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n--alsologtostderr\r\n\r\n# Run TOCO conversion.\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$STRIPPED_PB --output_file=$DETECT_FB \\\r\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \\\r\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\r\n\r\n# Build and install the demo\r\nbazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo\r\nadb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\r\n```", "@andrewharp Happy Easter \ud83e\udd5a\ud83c\udf6b you LEGEND! :) Gonna see if I can get this running.", "HI,is there any quantize version?", "I got it working with the instructions above but needed:\r\n- Android SDK 15 because of my bazel version\r\n- I also cannot open the project in Android Studio\r\n\r\n@andrewharp is this a new Android Studio thing you guys are heading towards that uses bazel to build projects instead of Gradle, or just missing some project settings for now because of the short time frame to get it working?\r\n\r\nHappy to supply a PR if I understand what the problem is.\r\n\r\nAlso regarding performance, it seems to be slow on my LG G6 on Android 7.\r\nIs that because NN API is only on Android 8?\r\n\r\nAnyone able to test it on Android 8?", "I see, I thought the instructions were only for the conversion. I stopped reading after the first part of the sentence saying this is how you convert the model lol.\n\nYeah I got pixel xl, I suppose your phone doesn\u2019t have hardware that can accelerate inference or that hardware isn\u2019t supported by software.\n\nI\u2019ll try and let you know. I was assuming I could build that with android studio doh...\n\nSent from my iPhone\n\n> On 31 Mar 2018, at 20:05, Madhava Jay <notifications@github.com> wrote:\n> \n> I got it working with the instructions above but needed:\n> \n> Android SDK 15 because of my bazel version\n> I also cannot open the project in Android Studio\n> @andrewharp is this a new Android Studio thing you guys are heading towards that uses bazel to build projects instead of Gradle, or just missing some project settings for now because of the short time frame to get it working?\n> \n> Happy to supply a PR if I understand what the problem is.\n> \n> Also regarding performance, it seems to be slow on my LG G6 on Android 7.\n> Is that because NN API is only on Android 8?\n> \n> Anyone able to test it on Android 8?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Yes, I did the same thing and went straight to the code and Android studio. Then after you pinged this morning I was about to reply that I had the same problem and then RTFM'ed again. \ud83e\udd23\r\n\r\nFrom what I can tell the LG G6 should be able to support NN API since it has the Qualcomm 821 SoC the same as the Pixel 1. But, unfortunately LG hasn't released Android 8 or 8.1 and the latest LineageOS builds look a bit sketchy so im gonna hold off unless I KNOW that it works better on Android 8.1. If you can fire it up on the Pixel that would be awesome! \ud83d\udc4d ", "I've managed to test this but the demo runs really slow...even slower than the original version.\r\nI am using Pixel XL (first version) and I have previously compiled the old demo for 64bit arch, which made it to run almost twice as fast even without tfLite...the inference time in this case is around 450ms. When I try this demo, it runs at around 850ms and sometimes even over a second. Have I done something incorrectly or was I just being over-optimistic to expect a decent speed-up? Thanks.", "@mpeniak I got the same speeds on the LG G6, with debug on or off (Thought it was debug at first). I suspect NNAPI isn't being used. Perhaps we need to do something special with the nnapi_lib build?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/BUILD\r\n\r\nThe dependency is listed but might need building for a specific architecture?\r\nPerhaps something in ./configure\r\n(btw I enabled XLA in my ./configure in case that was related but it didnt change the speed)", "@andrewharp \r\nI want to use NNAPI, but I do not know how to use it.\r\nAccording to document, Neural Networks API is available in Android 8.1 >\r\nIf it is 8.1 or higher, is it basically applied? or Do I need additional NDK work? [Document Link](https://github.com/googlesamples/android-ndk/tree/master/nn_sample)\r\nHave a nice day XD", "@andrewharp, I tried to enable NNAPI for tflite_demo and run the apk, however I found the apk crashed\r\nwhen call AddOpsAndParams, the operation tflite::BuiltinOperator_SQUEEZE is not supported and\r\nnn_op_type is set to -1, which will cause FATAL called, and exit(-1) . I think that's the\r\nroot cause. Would you please tell if it will be supported in future version ? Is there any other way to work\r\naround to test NNAPI path? thanks.", "@andrehentz \r\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n--alsologtostderr\r\n\r\nWhy is not input_names image_tensor?\r\nI tried this way and I encountered an error.", "@nanamare\r\nYou should use frozen_inference_graph_stripped.pb instead of frozen_inference_graph.pb.\r\ntry \"bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=frozen_inference_graph_stripped.pb\"\r\nand you can see the following output:\r\nFound 1 possible inputs: (name=Preprocessor/sub, type=float(1), shape=None) \r\nNo variables spotted.\r\nFound 2 possible outputs: (name=concat, op=ConcatV2) (name=concat_1, op=ConcatV2) \r\n\r\nThe input name is Preprocessor/sub abd output name is concat.", "@nanamare\r\nThe latest tensorflow lite code include a java interface to enable NNAPI.\r\n\r\nclass Interpreter has function called: setUseNNAPI(true);\r\nYou can directly call such interface.", "@zhangbo0325 \r\nI already try to call setUserNNAPI(true);, But there was no effect. \r\nIt was almost similar inference not using NNAPI.\r\nandroid specification: 8.1 version.", "@nanamare, are your running ssd-mobilenet? For such network, there is a SQUEEZE operation which is not supported by android NNAPI.  I asked the question above.  For mobilenet-v1, it is OK.", "    @andrewharp  Hi, andrewharp. i just followed your quick steps for converting an SSD MobileNet model to tflite format, and then i tried to  build the demo to use it. But something accurred in apk.\r\n    for the tflite from mobilenet_ssd_tflite_v1.zip, everything is ok! i can use mobile to detecter things.\r\n    And then i tried to use pet data to fine tune the model from the checkpoint in mobilenet_ssd_tflite_v1.zip. this process is also ok. i check the generated frozen_inference_graph.pb with the object_detection_tutorial.ipynb(https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb). the result shown this pb can used to object detection. And then i followed the script to convert frozen pb to tflite. Then build demo with tflite, unfortunately something wrong ocurred. Then log is written below. \r\n    It seems the Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4]. Because i am new to use object detection api, i donot know how to deal with the problem. \r\n    Hope you can point out some solutions, Thx! \r\n\r\nMobile Log here:\r\n04-04 19:46:36.099 28864-28882/org.tensorflow.lite.demo E/AndroidRuntime: FATAL EXCEPTION: inference Process: org.tensorflow.lite.demo, PID: 28864  java.lang.IllegalArgumentException: Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4].       at org.tensorflow.lite.Tensor.copyTo(Tensor.java:44)   at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:139)   at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:226) at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:248)   at android.os.Handler.handleCallback(Handler.java:761)  at android.os.Handler.dispatchMessage(Handler.java:98)at android.os.Looper.loop(Looper.java:156)  at android.os.HandlerThread.run(HandlerThread.java:61)", "Amazing! Trying to get this working on iOS. How do I parse the Tensor output?\r\n```\r\ninterpreter->Invoke();\r\nfloat* output = interpreter->typed_output_tensor<float>(0);\r\n```", "The DetectorActivity interface is stuck in my project, do you exist, how can I solve it?", "@zhangbo0325 Thanks for the details. Since the squeeze is not supported by the NNAPI, does that mean that the NNAPI is not used at all and inference will remain as slow as it is? As I mentioned in the earlier comment, I get really poor performance on Pixel XL. I would expect inference times somewhere around 80-120ms. Thanks!", "@mpeniak , I asked the same question to andrewharp.  I just ran the ssd-mobilenet with the help of tensorflow-lite cpu implementation and also got the poor performance. ", "The TensorFlow Lite talk at Dev Summit 2018 showed 3x performance on MobileNet:\r\nhttps://youtu.be/FAMfy7izB6A?t=530\r\n\r\nMaybe thats not for SSD though?\r\nPossibly requires weight quantization first?\r\n\r\n", "I have tried mobilnet and it is much faster, this does not apply to mobilnet-ssd though...", "Sad Panda \u2639\ufe0f\ud83d\udc3c\r\n@andrewharp Any idea when a performant SSD implementation will be available? Is it a question of Weight Quantization?", "I also got the poor performance for ssd-mobilenet on TensorFlowLite :(\r\nBut, I  have another question. Why does the score of the result exceed 1? Is it not the probability?", "@a1103304122 as what I understand, the score is output of node \"concat\", before softmax, so, it isn't the probability.\r\n> During TOCO conversion a different input node (Preprocessor/sub) is used, as well as different output nodes (concat,concat_1). This skips some parts that are problematic for tflite, until either the graph is restructured or TF Lite reaches TF parity.", "Does anybody have idea why TFlite is slower than TFmobile in this model?", "@andrewharp is it possible to comment on the performance of the TF Lite SSD? Also is quantization possible / coming? I know you guys are working hard to make this all happen but it would be good to know if this is just a short term hiccup or there is a solution we can apply. \ud83d\ude04 ", "@andrewharp Thanks for your great posting. However, I have one question for your steps.\r\n\r\n# Strip out problematic nodes before even letting TOCO see the graphdef\r\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n--alsologtostderr\r\n\r\nIf I don't misunderstand, here you want to produce STRIPPED_PB, right? If so, currently, our input file's input should be image_tensor. So, I don't very understand why we use Preprocessor/sub. Could you explain more detail? \r\n\r\nSecondly, here we use optimize_for_inference, can we use transform_graph tool? because new tensorflow documentation recommend transform_graph instead of optimize_for_inference.", "@mpeniak how do u do it? Please say some details.", "org.tensorflow.lite.demo E/AndroidRuntime: FATAL EXCEPTION: inference Process: org.tensorflow.lite.demo, PID: 28864 java.lang.IllegalArgumentException: Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4]. \r\n\r\n@Haijunlv Have you solved the problem? Can you share the solution?", "Upon importing the TF Lite new Android demo I'm getting `Error:Plugin with id 'com.android.application' not found.` on OS X.", "same problem as @csmith105 ! I managed to build and install the demo with bazel but I can't compile or run the project on Android Studio ... any solution for this problem ?", "@Eddy-zheng If you saw the node \"concat\"  in the frozen graph, you will find the squeeze op is excuted after concat op. I think that is the reason why the shape is incompatable. I did not test the speed of the squeeze op. But I think there are two ways to solve the problem.\r\n1. change the order of squeeze and concat op. In the ssd_meta_arch.py, slightly change \" box_encodings = tf.squeeze(tf.concat(prediction_dict['box_encodings'], axis=1), axis=2)\"\r\n2. directly kill the shape 1 at the axis 2. In the box_predictor.py, slightly change \" box_encodings =tf.reshape(\r\n            box_encodings, tf.stack([combined_feature_map_shape[0],\r\n            combined_feature_map_shape[1] *\r\n            combined_feature_map_shape[2] *\r\n            num_predictions_per_location,\r\n            1, self._box_code_size]))\"\r\n\r\nActually i donot understand the reason why reshape tensor with extra \"1\" shape. May be it is a redundant\r\nop. \r\nI have tried way 1 and success to run model in mobile. But still a little slow.  later i will try way 2 to see if it can get a better speed \r\n\r\n ", "@Haijunlv How good are the detections? The lite model from @andrewharp 's demo simply removes all preprocess and postprocess nodes (thousands of them) from the graph and replace them with several lines of code. I am not sure how it'll work..", "I think there is a solution to the android studio and gradle issue. (please correct me if I'm wrong or if there is a better solution ) : \r\n\r\n- It's not the best approach but there is a Bazel plugin that we can install inside Android Studio to \"replace\" Gradle and we can build and run our project using Bazel through AS.\r\n\r\n- I read several articles about Bazel and I came across this [question](https://www.quora.com/Why-did-Google-decide-to-use-Bazel-with-TensorFlow) in Quora.. and according to the answer tensorflow will continue using Bazel since it exploits the framework better and gives better results.. so I think as developers  in this particular case we should adapt to it and leave Gradle behind until tensorflow supports it completely.", "@davidfant \r\nDid you manage to get to the multiple outputs in TensorFlow Lite C++?\r\n\r\n```\r\ninterpreter->Invoke();\r\n???? output = interpreter->typed_output_tensor<?????>(0);\r\n```", "I am progressing, but still I don't know how to get the outputs in C++. **Is there any documentation about this?** This is what I have at the moment. **How do I have to access the data array to get the scores?**\r\n\r\n```\r\n(fill inputs)\r\n.......\r\nintepreter->Invoke();\r\nconst std::vector<int>& results = interpreter->outputs();\r\nTfLiteTensor* outputLocations = interpreter->tensor(results[0]);\r\nTfLiteTensor* outputClasses   = interpreter->tensor(results[1]);\r\nfloat *data = tflite::GetTensorData<float>(outputClasses);\r\nfor(int i=0;i<NUM_RESULTS;i++)\r\n{\r\n   for(int j=1;j<NUM_CLASSES;j++)\r\n   {\r\n      float score = expit(data[i*NUM_CLASSES+j]); // \u00bf?\r\n    }\r\n}\r\n```\r\n", "@JaviBonilla I did similar thing and found it doesn't work. Using the cutoff from the android demo app, it just outputs too much noise. If you use tensorboard to read the graph, you'll find that the lite model prunes thousands of postprocess nodes. I don't think the current way would work. I hope tensorflow lite will support those postprocess nodes in the future, instead of asking people to do such not-working hacks..", "Thanks @YijinLiu. I saw your repository tf-cpu, I will have a look at your code to check that my implementation is correct, and see the results even if they are not good.", "@JaviBonilla please let us know when you've figured out how to run with C++! \ud83d\ude4c", "Hi @davidfant, \r\n\r\nI still have to test it, but @YijinLiu already figured it out!. \r\n\r\nHave a look at his repository (https://github.com/YijinLiu/tf-cpu). In particular, you can find how to get the outputs in the `tf-cpu/benchmark/obj_detect_lite.cc` file, `AnnotateMat()` function, which is executed after the `Interpreter->Invoke()`.", "@JaviBonilla I didn't finish obj_detect_lite.cc, specially, to use priors to decode the detection boxes.\r\nWhat I found is that the scores doesn't make sense in all scenarios. For some cases, it generate too much noise. For other cases, it may lose some good detections. I looked at those nodes to convert these intermediate scores to the final possibility scores. There are thousands of nodes...", "@YijinLiu thanks for clarifying this. Then, I think it is better to wait until more improvements are included in TensorFlow Lite for object detection. Anyway, I will try to decode the detection boxes in C++ if I have time.", "Hi @andrewharp ,\r\n\r\nThanks for your effort for making the new android demo project, but could you please write a readme.md or some description document in [tensorflow/contrib/lite/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android) so that we could all easily understand the process to make tensorflow lite? thanks~!", "Hello,I have runned the ssd_mobilenet_v1_coco_2017_11_17 demo successfully,then I get a fine-tuned model.When I run the @andrehentz 's process on it,problem occured:\r\n`bazel run tensorflow/contrib/lite/toco:toco -- --input_file=$STRIPPED_PB --output_file=$DETECT_FB --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr`\r\n> Before Removing unused ops: 586 operators, 871 arrays (0 quantized)\r\n2018-06-12 15:29:54.273221: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 586 operators, 871 arrays (0 quantized)\r\n2018-06-12 15:29:54.300213: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 409 operators, 688 arrays (0 quantized)\r\n2018-06-12 15:29:54.309735: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 409 operators, 688 arrays (0 quantized)\r\n2018-06-12 15:29:54.317395: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 2880256 bytes, theoretical optimal value: 2880128 bytes.\r\n2018-06-12 15:29:54.319173: F tensorflow/contrib/lite/toco/tflite/export.cc:330] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: RSQRT, SquaredDifference, Stack, TensorFlowShape.\r\n\r\nHere is my model[https://drive.google.com/open?id=1IxRSU4VSmVmhUtUpSQew_5anEfxTg3Ca](https://drive.google.com/open?id=1IxRSU4VSmVmhUtUpSQew_5anEfxTg3Ca)\r\n\r\nCan anyone help me?\r\n@andrehentz \r\n", "@JaviBonilla and @YijinLiu I have a [Python implementation](https://github.com/freedomtan/tensorflow/blob/object_detection_tflite_object_dtection_python/tensorflow/contrib/lite/examples/python/object_detection.py) that I tested with Google's pretained SSD MobileNet V{1,2} and SSDLIte MobileNet V2 models. See simple documentation [here](https://github.com/freedomtan/tensorflow/blob/object_detection_tflite_object_dtection_python/tensorflow/contrib/lite/examples/python/object_detection_ssd_coco.md).", "@freedomtan  which version of tf do you use ? tf 1.8 ? ", "@hengshanji master branch after tflite interpreter Python binding (29c129c6). I don't think 1.8 has the binding.", "@freedomtan tf1.8 has interpreter Python binding, but I meet such kind of problem \"nnapi error: unable to open library libneuralnetworks.so\". where to get this .so or how to generate it? Thanks.", "Ignore it :) It's for Android NNAPI.", "@freedomtan Did you test the example on the device or on the pc? When I test it on the pc, use the android-28/x86 libneuralnetworks.so, it shows error  \"Aborting since tflite returned failure\".", "As I said, please ignore that NNAPI problem. You are not expected to have a working `libneuralnetwork.so`. I tested my scripts on both an x86 running Ubuntu and an ARMv8 board running Debian.", "@freedomtan, thanks for sharing the code and documentation.", "Based on repository (https://github.com/YijinLiu/tf-cpu). I have updated the  tf-cpu/benchmark/obj_detect_lite.cc to get the outputs. In function AnnotateMat() add  decodeCenterSizeBoxes code to handle the output_locations, Then do nms for these results.\r\nAt the same time, using the https://github.com/tensorflow/tensorflow/issues/14688 to generate the libtensorflow-lite.a, it can run on both an x86 running Ubuntu and an android device with tflite model in ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz.\r\nThanks all.", "@WeiboXu Can you share the code and model here?", "Here is the updated code for obj_detect_lite.cc\r\n[obj_detect_lite.cc.zip](https://github.com/tensorflow/tensorflow/files/2153563/obj_detect_lite.cc.zip)\r\n\r\nThe model is http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz \r\n", "@freedomtan In your python implementation code, there is one file \"/tmp/box_priors.txt\", do you know how  to generate this file ? Or how the data in this file was calculated? There is no problem to do inference for the image with size 300X300, but, the inference accuracy will drop down when do inference for the image with size 224X224", "@freedomtan, @andrewharp , The prev model by following [this instructions](https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630) can not work in [the latest TFLite Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android), Whether it is quantified or float, because the tflite model in the latest TFLite demo requires 4 outputs , but prev model only have 2 outputs( concat, concat1).\r\n\r\nPlease help , Thanks!", "These instructions are being updated at the moment. Will provide link for updated instructions in the next week.", "@frontword What in `/tmp/box_priors.txt` are boxes for post processing. If you use the newer one mentioned by @WenguoLi, you don't need it. However, as far as I can tell, those post-processing ops are implemented as TF Lite custom ops. Meaning that you cannot accelerated them with NNAPI accelerators without further efforts.\r\n\r\nFor image size problems, yes, I think feeding 224x224 images to SSD300 (the models released by Google were trained with 300x300 images) and getting worse accuracy is not unexpected.\r\n\r\n@WenguoLi It seems to me the updated model you mentioned is quite easy to handle. See my [update scirpt](https://github.com/freedomtan/tensorflow/blob/freedom/object_detection_tflite_object_dtection_python_w_postprocessing/tensorflow/contrib/lite/examples/python/object_detection.py). The following figure is generated by \r\n```\r\npython  tensorflow/contrib/lite/examples/python/object_detection.py --image /tmp/image2.jpg  --show_image True\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/3395998/42428690-8ee7a76c-8367-11e8-8487-1bf207771147.png)\r\n", "To fix the score of the inference result exceeding 1 is it possible to use the Java method TrackedObject.getCurrentCorrelation() because that always seems to return something less than 1 (not sure if its correct or not though). The TFLite Android example uses Recognition.getConfidence()  which always seems to return something greater than 1", "@mpeniak You ran the ssd mobilenet tflite model on Movidius. I am also planning to do something similar. Can you please guide a bit on how you did it?", "@achowdhery Hi, I saw some updates of the building instructions for the latest android demo here (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android/app), but it didn't indicate how we can actually convert the frozen pb model to tflite model (the quantized detect.tflite which was used in the latest demo). Any further instructions on the quantized model conversion flow? Also, I think we should first run quantized training with fake quantization operations as instructed here (https://www.tensorflow.org/performance/quantization) and then perform the model conversion, correct? Also, is it possible to enable NNAPI in the latest android demo? I tried to use tfLite.setUseNNAPI(true) in TFLiteObjectDetectionAPIModel.java but it crashed on my Pixel 2 running Android 8.1 (it can work well without NNAPI). Any suggestions? Thanks!", "@tenoyart The short answer for \"is it possible to enable NNAPI in the latest android demo?\" should be NO. Not so short answer is that it's kinda possible if you modify TF Lite interpreter do something like splitting the model or adding corresponding custom ops to NNAPI.", "@achowdhery I saw a TensorFlow blog [article](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) by you. Is this the instructions you mentioned or more are coming?", "Yes. This are the instructions for training and serving the object detection model on Android.", "@freedomtan Thanks for sharing the script.\r\nIn your [latest script with postprocessing](https://github.com/freedomtan/tensorflow/blob/freedom/object_detection_tflite_object_dtection_python_w_postprocessing/tensorflow/contrib/lite/examples/python/object_detection.py) which model file are you using?\r\nDid you specifiy argument of optimize_for_inference.py like\r\n--input_names=\"Preprocessor/sub\"\r\n--output_names=\"detection_boxes,detection_scores,num_detections,detection_classes\"\r\n\r\nDo you see any difference with/without postprocessing?\r\n\r\nThanks!\r\n", "Is there any way to convert SqueezeNet models with 4 outputs to tflite?", "@chanchanzhang  Please follow the new instructions toward the end of tutorial in https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\nNote that this uses a different workflow than using optimize_for_inference.py", "@ashwaniag If you would like to replace Mobilenet with SqueezeNet classifier while keeping SSD after that for detection, that is fine for current workflow.", "@achowdhery Great to see the TF Lite model on ssd mobilenet v1. Does TF Lite fully support ssdlite mobilenet v2?", "@tenoyart Yes. Any Mobilenet SSD will work through this pipeline. We have not released the corresponding tflite files in open source. If you encounter issues, please file a bug.", "@chanchanzhang as @achowdhery said, please use `object_detection/export_tflite_ssd_graph.py` rather than `optimized_for_inference.py`. And the tflite model file I used is from the one used by Android example. You can get it [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_0.75_quant_2018_06_29.zip).\r\n\r\n@achowdhery I think there are not FakeQuant nodes and tensors in checkpoints of [ssd_mobilenet_v1_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_03.tar.gz)  and [ssd_mobilenet_v1_0.75_depth_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_03.tar.gz). Could you check them?", "@freedomtan I do see weight_quant and act_quant nodes in the exported graph after using object_detection/export_tflite_ssd_graph.py.\r\nPlease give a screenshot or exact instructions of how you verified it has no Fakequant nodes.\r\nI am also able to successfully convert the checkpoints", "@achowdhery Thanks for checking. When I ran `export_tflite_ssd_graph.py` on those two, I couldn't get tflite models, so I inspected checkpoints. What I did is something like\r\n\r\n```\r\ncurl http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_03.tar.gz | tar xzvf -\r\ncd ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_03\r\nstrings model.ckpt.index  |grep quant\r\n```\r\nNothing shows up.", "@andrewharp Thank you so much for your cutosm inference class [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java)  , I've tried it with your ssd mobilenet v1 tflite  [mobilenet_ssd_tflite_v1.zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_ssd_tflite_v1.zip) but when the app starts seems there is  problem in the function recognizeImage(final Bitmap bitmap) when i call tfLite.runForMultipleInputsOutputs(inputArray, outputMap); it throws this exception \r\n\r\n```\r\n07-18 10:37:02.416 19957-19996/com.app.cerist.realtimeobjectdetectionapi E/AndroidRuntime: FATAL EXCEPTION: Camera\r\n    Process: com.app.cerist.realtimeobjectdetectionapi, PID: 19957\r\n    java.lang.IllegalArgumentException: Output error: Outputs do not match with model outputs.\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:170)\r\n        at com.app.cerist.realtimeobjectdetectionapi.ImageClassifierTFLiteAPI.recognizeImage(ImageClassifierTFLiteAPI.java:207)\r\n        at com.app.cerist.realtimeobjectdetectionapi.MainActivity.classifyFrame(MainActivity.java:421)\r\n        at com.app.cerist.realtimeobjectdetectionapi.MainActivity.access$1000(MainActivity.java:48)\r\n        at com.app.cerist.realtimeobjectdetectionapi.MainActivity$4.run(MainActivity.java:455)\r\n        at android.os.Handler.handleCallback(Handler.java:739)\r\n        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n        at android.os.Looper.loop(Looper.java:159)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n07-18 10:37:02.436 19957-19996/com.app.cerist.realtimeobjectdetectionapi V/Process: killProcess [19957] Callers=com.android.internal.os.RuntimeInit$UncaughtHandler.uncaughtException:99 java.lang.ThreadGroup.uncaughtException:693 java.lang.ThreadGroup.uncaughtException:690 <bottom of call stack> \r\n07-18 10:37:02.436 19957-19996/com.app.cerist.realtimeobjectdetectionapi I/Process: Sending signal. PID: 19957 SIG: 9\r\n```\r\n\r\nthe error said that the length of outputs array is bigger than the length of inputs array\r\nHere is the condition in Interpreter.java\r\n\r\n``` \r\npublic void runForMultipleInputsOutputs(Object[] inputs, @NonNull Map<Integer, Object> outputs) {\r\n        if (this.wrapper == null) {\r\n            throw new IllegalStateException(\"Internal error: The Interpreter has already been closed.\");\r\n        } else {\r\n            Tensor[] tensors = this.wrapper.run(inputs);\r\n            if (outputs != null && tensors != null && outputs.size() <= tensors.length) {\r\n                int size = tensors.length;\r\n                Iterator var5 = outputs.keySet().iterator();\r\n            }\r\n       }\r\n}\r\n```\r\n\r\nand this is my inputs and outputs arrays : \r\n\r\n```      \r\nd.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);\r\nd.imgData.order(ByteOrder.nativeOrder());\r\nd.intValues = new int[d.inputSize * d.inputSize];\r\n```\r\n\r\n```\r\n imgData.rewind();\r\n        for (int i = 0; i < inputSize; ++i) {\r\n            for (int j = 0; j < inputSize; ++j) {\r\n                int pixelValue = intValues[i * inputSize + j];\r\n                if (isModelQuantized) {\r\n                    // Quantized model\r\n                    imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n                    imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n                    imgData.put((byte) (pixelValue & 0xFF));\r\n                } else { // Float model\r\n                    imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n                    imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n                    imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n```\r\n\r\nThe outputs array :\r\n\r\n```\r\n// Copy the input data into TensorFlow.\r\n        Trace.beginSection(\"feed\");\r\n        outputLocations = new float[1][NUM_DETECTIONS][4];\r\n        outputClasses = new float[1][NUM_DETECTIONS];\r\n        outputScores = new float[1][NUM_DETECTIONS];\r\n        numDetections = new float[1];\r\n\r\n        Object[] inputArray = {imgData};\r\n        Map<Integer, Object> outputMap = new HashMap<>();\r\n        outputMap.put(0, outputLocations);\r\n        outputMap.put(1, outputScores);\r\n        outputMap.put(2, numDetections);\r\n        outputMap.put(3, outputClasses);\r\n        Trace.endSection();\r\n```\r\n\r\nAnd the Inference :\r\n```\r\n// Run the inference call.\r\n        Trace.beginSection(\"run\");\r\n        Log.d(\"TAG_INPUT\",\"\"+String.valueOf(inputArray.length));\r\n        Log.d(\"TAG_OUTPUT\",\"\"+String.valueOf(outputMap.size()));\r\n\r\n        tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n        Trace.endSection();\r\n```\r\n\r\nI didn't understand the meaning of this Error cuz i did exactly the same as your TFLiteObjectDetectionAPIModel.java class .\r\nthank you for Help\r\n", "@achowdhery  Hi, following your blog I converted the model from ssd_mobilenet_v1_coco_2017_11_17. However when I used the converted mobilenet_ssd.tflite in tflite_demo.apk, I got the following error:\r\n```\r\n    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 10, 4].\r\n```\r\n\r\nAny ideas why I got it? Thanks.", "This is a shape mismatch because the expected output tensor is 1,10,4 in size not 1,1917,4. For the old model file, you will need to regress to the demo app version of May. Otherwise please use the latest released models for conversion.", "@achowdhery I converted my own model to tflite and when I run it. The interpreter->invoke() call throws a seg fault. Any idea what might be wrong?", "@ashwaniag Do the downloaded models on https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\ncompile for you?\r\nIf yes, then please provide new instructions you are using now when you get seg fault?\r\nThere could be a mismatch in input type/size etc.", "@achowdhery  It worked. I was giving the wrong input_arrays. Thanks anyways!", "updated [ssd_mobilenet_v1_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz) and [ssd_mobilenet_v1_0.75_depth_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_18.tar.gz) work, if you followed the tutorial. Thanks @achowdhery.", "I was looking at `TFLiteObjectDetectionAPIModel.java` in the example demo app. Is there a reason that `outputLocations`, `outputClasses`, `outputScores` and `numDetections` are allocated on each `recognizeImage` call [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L179)? It seems that they are meant to be preallocated.\r\nI've tried running the code with preallocation and it seems to work fine, but I just wanted to make sure there's nothing going under the covers that would introduce problems later.", "Preallocating is probably more efficient. Where are you preallocating that you may foresee problems?", "thanks for the reply @achowdhery. I'm leaving preallocation in the static `create` method as is. My only concern was that the code seems to be written to use preallocation (the static method preallocates the arrays), but for some reason the arrays are reallocated on each call.", "hi @achowdhery , I tested the new Android tflite app demo. It works perfectly for **ssd_mobilenet_v1_coco, ssd_mobilenet_v1_0.75_depth_coco, ssd_mobilenet_v1_quantized_coco**.\r\nBut I got this exception for the other ssd-mobilenet models:\r\n```\r\n07-25 07:41:25.292 31515-31532/org.tensorflow.lite.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.demo, PID: 31515\r\n    java.lang.ArrayIndexOutOfBoundsException: length=160; index=-2147483648\r\n        at java.util.Vector.elementData(Vector.java:734)\r\n        at java.util.Vector.get(Vector.java:750)\r\n        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:218)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:249)\r\n        at android.os.Handler.handleCallback(Handler.java:790)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n\r\n```\r\nThe tflite model produced wrong class index. The exception makes app crash after detect well for couple of seconds.\r\n**ssd_mobilenet_v1_ppn_coco** produces wrong messy bounding box, label as well.", "PPN is a float model: are you converting the TFLITE model using float conversion commands.\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\nThen you also need to change the following in DetectorActivity.java:\r\nprivate static final boolean TF_OD_API_IS_QUANTIZED = true;", "I knew that config. Actually when that config is wrong, the app cannot run at all.\r\nCould you see the ArrayIndexOutOfBoundsException? I also tried your tutorial's docker, but it's same.", "Okay. please file a new GitHub issue with exact repro instructions.  PPN model is a new feature request for java app -  We will reply when we can prioritize it", "Thanks. The ArrayIndexOutOfBoundsException also happen to ssd_mobilenet_v1_0.75_depth_quantized_coco, ssdlite_mobilenet_v2_coco. The difference from PPN is that makes correct results before the app crashed by that exception.", "@achowdhery Is there anyway to train quantize model with 4 outputs for tflite using legacy/train.py since the new model_main.py has bugs?\r\nhttps://github.com/tensorflow/models/issues/4798", "@ashwaniag You can diff the two code and add the part that adds quantization: Note that graph_rewriter function is where quantization ops get added.", "@achowdhery : https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md \r\nis there an example or sample code of how to do the same in iOS. So far the closest thing i found has been this https://github.com/YijinLiu/tf-cpu/blob/master/benchmark/obj_detect_lite.cc which it does not always work.\r\n\r\nthe current iOS demo app does not work with ssd and float model. ", "@achowdhery I trained my model using tensorflow v1.9. Converted to tflite using the steps in the blog. I do not get any detections. Do you have any idea about this?", "@ashwaniag COCO or pets? Please open a new bug with exact repro instructions. Other GitHub users have confirmed its working with Tensorflow 1.10", "@achowdhery It is my own dataset. I trained for mobilenetv2 architecture. When I run the .pb model (tensorflow model), I get \r\nNot found: Op type not registered 'NonMaxSuppressionV3' in binary running on VAL5-04. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\nDo you think its related?", "@ashwaniag Please open a new bug and provide exact reproducible instructions", "@ashwaniag check these both issues, i had a similar problem : [#10254](https://github.com/tensorflow/tensorflow/issues/10254) and [#19854](https://github.com/tensorflow/tensorflow/issues/19854)", "@achraf-boussaada Thank you! I fixed it. It was a version mismatch issue.\r\n@achowdhery Now, the problem is that the full tensorflow model gives me great results but the tflite model gives very bad results.", "@ashwaniag Please define very bad results. Do you have small objects? Please attach a model checkpoint, pipeline config and label file as well as a sample image to help us reproduce the issue. Thanks", "@oopsodd   hello, I get a wrong class index either . it complained \"java.lang.ArrayIndexOutOfBoundsException: length=10; index=-739161663\", Can you help me ?", "Note I have created TensorFlow Lite SSD (Object Detection) minimal working examples for iOS and Android; https://github.com/baxterai/tfliteSSDminimalWorkingExample. The iOS version is based on obj_detect_lite.cc by YijinLiu (with nms function by WeiboXu), and the Android version is based on https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android tflDetect. It removes all overhead like the internal camera, and isolates the core code required to detect objects and display the detection boxes.", "@baxterai great work! thanks, I will test it.", "Thanks for your amazing work everybody! I have another question regarding the recently added postprocessing operation.\r\n\r\nThe output of the pretrained **ssd_mobilenet_v1_quantized_coco**\r\nis currently limited to the top **10** detections in the frame, even though the default configs in models/research/object_detection/samples/configs/ like  \r\nssd_mobilenet_v1_quantized_300x300_coco14_sync.config all specify a higher limit of total detections.\r\n\r\n `    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }`\r\n\r\nis this resolved by retraining the network with this pipeline configuration or is the dimensionality of \r\n'TFLite_Detection_PostProcess' fixed to 10 by other configurations?", "@Georg-W You will need to change max detection in export_tflite_ssd_graph.py as well. There is a command line option.", "@achowdhery ah thank you ! Thats what I missed.", "> @andrewharp Thank you so much for your cutosm inference class [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java) , I've tried it with your ssd mobilenet v1 tflite [mobilenet_ssd_tflite_v1.zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_ssd_tflite_v1.zip) but when the app starts seems there is problem in the function recognizeImage(final Bitmap bitmap) when i call tfLite.runForMultipleInputsOutputs(inputArray, outputMap); it throws this exception\r\n> \r\n> ```\r\n> 07-18 10:37:02.416 19957-19996/com.app.cerist.realtimeobjectdetectionapi E/AndroidRuntime: FATAL EXCEPTION: Camera\r\n>     Process: com.app.cerist.realtimeobjectdetectionapi, PID: 19957\r\n>     java.lang.IllegalArgumentException: Output error: Outputs do not match with model outputs.\r\n>         at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:170)\r\n>         at com.app.cerist.realtimeobjectdetectionapi.ImageClassifierTFLiteAPI.recognizeImage(ImageClassifierTFLiteAPI.java:207)\r\n>         at com.app.cerist.realtimeobjectdetectionapi.MainActivity.classifyFrame(MainActivity.java:421)\r\n>         at com.app.cerist.realtimeobjectdetectionapi.MainActivity.access$1000(MainActivity.java:48)\r\n>         at com.app.cerist.realtimeobjectdetectionapi.MainActivity$4.run(MainActivity.java:455)\r\n>         at android.os.Handler.handleCallback(Handler.java:739)\r\n>         at android.os.Handler.dispatchMessage(Handler.java:95)\r\n>         at android.os.Looper.loop(Looper.java:159)\r\n>         at android.os.HandlerThread.run(HandlerThread.java:61)\r\n> 07-18 10:37:02.436 19957-19996/com.app.cerist.realtimeobjectdetectionapi V/Process: killProcess [19957] Callers=com.android.internal.os.RuntimeInit$UncaughtHandler.uncaughtException:99 java.lang.ThreadGroup.uncaughtException:693 java.lang.ThreadGroup.uncaughtException:690 <bottom of call stack> \r\n> 07-18 10:37:02.436 19957-19996/com.app.cerist.realtimeobjectdetectionapi I/Process: Sending signal. PID: 19957 SIG: 9\r\n> ```\r\n> the error said that the length of outputs array is bigger than the length of inputs array\r\n> Here is the condition in Interpreter.java\r\n> \r\n> ```\r\n> public void runForMultipleInputsOutputs(Object[] inputs, @NonNull Map<Integer, Object> outputs) {\r\n>         if (this.wrapper == null) {\r\n>             throw new IllegalStateException(\"Internal error: The Interpreter has already been closed.\");\r\n>         } else {\r\n>             Tensor[] tensors = this.wrapper.run(inputs);\r\n>             if (outputs != null && tensors != null && outputs.size() <= tensors.length) {\r\n>                 int size = tensors.length;\r\n>                 Iterator var5 = outputs.keySet().iterator();\r\n>             }\r\n>        }\r\n> }\r\n> ```\r\n> and this is my inputs and outputs arrays :\r\n> \r\n> ```\r\n> d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);\r\n> d.imgData.order(ByteOrder.nativeOrder());\r\n> d.intValues = new int[d.inputSize * d.inputSize];\r\n> ```\r\n> ```\r\n>  imgData.rewind();\r\n>         for (int i = 0; i < inputSize; ++i) {\r\n>             for (int j = 0; j < inputSize; ++j) {\r\n>                 int pixelValue = intValues[i * inputSize + j];\r\n>                 if (isModelQuantized) {\r\n>                     // Quantized model\r\n>                     imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n>                     imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n>                     imgData.put((byte) (pixelValue & 0xFF));\r\n>                 } else { // Float model\r\n>                     imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n>                     imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n>                     imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n> ```\r\n> The outputs array :\r\n> \r\n> ```\r\n> // Copy the input data into TensorFlow.\r\n>         Trace.beginSection(\"feed\");\r\n>         outputLocations = new float[1][NUM_DETECTIONS][4];\r\n>         outputClasses = new float[1][NUM_DETECTIONS];\r\n>         outputScores = new float[1][NUM_DETECTIONS];\r\n>         numDetections = new float[1];\r\n> \r\n>         Object[] inputArray = {imgData};\r\n>         Map<Integer, Object> outputMap = new HashMap<>();\r\n>         outputMap.put(0, outputLocations);\r\n>         outputMap.put(1, outputScores);\r\n>         outputMap.put(2, numDetections);\r\n>         outputMap.put(3, outputClasses);\r\n>         Trace.endSection();\r\n> ```\r\n> And the Inference :\r\n> \r\n> ```\r\n> // Run the inference call.\r\n>         Trace.beginSection(\"run\");\r\n>         Log.d(\"TAG_INPUT\",\"\"+String.valueOf(inputArray.length));\r\n>         Log.d(\"TAG_OUTPUT\",\"\"+String.valueOf(outputMap.size()));\r\n> \r\n>         tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n>         Trace.endSection();\r\n> ```\r\n> I didn't understand the meaning of this Error cuz i did exactly the same as your TFLiteObjectDetectionAPIModel.java class .\r\n> thank you for Help\r\n\r\ni have the same issue.. got solution?\r\nthanks..", "> @Georg-W You will need to change max detection in export_tflite_ssd_graph.py as well. There is a command line option.\r\n\r\nHi \r\n\r\nI'm trying to detect more than 10 objects in the image ( which is default )\r\nI'm usin the following commands:\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=$OUTPUT_DIR/tflite_graph.pb --output_file=$OUTPUT_DIR/mobile_net_500.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --max_detections=500 --max_classes_per_detection=1 --allow_custom_ops \r\n\r\nI also modified \r\nexport_tflite_ssd_graph.py\r\nflags.DEFINE_integer('max_detections', 500 <--- instead of 10,\r\n'Maximum number of detections (boxes) to show.')\r\nflags.DEFINE_integer('max_classes_per_detection', 1,\r\n'Number of classes to display per detection box.')\r\n\r\nbut still giving 10 objects as output in the android [1,10,4].\r\n\r\nany idea?", "I would be also interested in the solution of  @KaviSanth issue.", "This solution of @Stevelb should work. You may want to visualize the frozen graph to make sure that max_detections is set correctly.", "@achowdhery  Thank you for your reply. I tried to execute the commands written by @andrewharp but I get the following error. Indeed, toco isn't located at this place. I am using the master version and the r1.95 version from the github repository.\r\n\r\nbazel run tensorflow/contrib/lite/toco:toco -- --input_file=$STRIPPED_PB --output_file=$DETECT_FB --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\r\nINFO: Invocation ID: 0e58a5ef-9fee-4619-b760-aeb1c83c9661\r\nERROR: Skipping 'tensorflow/contrib/lite/toco:toco': no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package 'tensorflow/contrib/lite/toco': BUILD file not found on package path\r\nINFO: Elapsed time: 0.179s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n I have to amend that I am executing those commands from my local tensorflow folder that was pulled from the git.\r\n\r\nI could find a toco under tensorflow/lite/toco and I am just testing whether it works.\r\nok, it seems to work using this toco and apart from that you have to change the $DETECT_FB path to $PWD/ssd_mobilenet.tflite since in the contrib/lite folder only some python is located an nothing else. ", "There appears a runtime error when adding the .tflite file in the DetectorActivity from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java) with the line \r\n```\r\nprivate static final String TF_OD_API_MODEL_FILE =\r\n            \"file:///android_asset/ssd_mobilenet_v1.tflite\";\r\n```\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: myProcess, PID: 32611\r\n    java.lang.RuntimeException: Failed to find input Node 'image_tensor'\r\n        at myPackage.myClass.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:106)\r\n\r\nIs it not possible to use .tflite models in that app?", "@defaultUser3214 you are using a classifier model in the detection app. MobileNet v1 is classification model. Please use MobileNet SSD model", "@achowdhery Thank you! Using the model from wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz resulted in that error. But I thought that this was the ssd version?\r\n\r\nBut using the ssd_mobilenet_v1_android_export.pb converted to .tflite that worked as .pb before produces the same error.", "@defaultUser3214 Thats an old version of the model that will not work in latest demo app released in July 2018. Please download the latest models in July 2018 in detection model zoo : they do work in the app. Please open a new issue if this is still blocked.", "@SteveIb   You also need to change  NUM_DETECTIONS = 500 in TFLiteObjectDetectionAPIModel.java", "not able to convert ssdmobilenet v1 .pb to .tflite \r\npb generated through Tensorflow object detection api @aselle  @achowdhery ", "Any progress on this? Trying to convert frozen_inference_graph.pb to .TFLITE file but getting error\r\n\r\n`java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 49152 bytes and a ByteBuffer with 270000 bytes`\r\n\r\nFor custom object detection in Android. Any ideas on different conversion methods? Transfer learned ssd_mobilenet_v1_pets on Windows 10 following the tutorial here: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10", "> Any progress on this? Trying to convert frozen_inference_graph.pb to .TFLITE file but getting error\r\n> \r\n> `java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 49152 bytes and a ByteBuffer with 270000 bytes`\r\n> \r\n> For custom object detection in Android. Any ideas on different conversion methods? Transfer learned ssd_mobilenet_v1_pets on Windows 10 following the tutorial here: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\r\n\r\nJust to follow up on this and to help anyone else who was having the same error - this is caused by using an incorrect model checkpoint to train from. To work on Android with .tflite, the initial model must MobileNet and must also be quantized and will have this section of code or something similar in the .config file: \r\n\r\n`graph_rewriter {\r\n  quantization {\r\n    delay: 48000\r\n    weight_bits: 8\r\n    activation_bits: 8\r\n  }\r\n}`\r\n", "> It's live now at [tensorflow/contrib/lite/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/examples/android)! This is a more complete port of the original TF Android demo (only lacking the Stylize example), and will be replacing the other demo in tensorflow/contrib/lite/java/demo going forward.\r\n> \r\n> A converted TF Lite flatbuffer can be found in [mobilenet_ssd_tflite_v1.zip](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_ssd_tflite_v1.zip), and you can find the Java inference implementation in [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/android/src/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java). Note that this differs from the original TF implementation in that the boxes must be manually decoded in Java, and a box prior txt file needs to be packaged in the apps assets (I think the one included in the model zip above should be valid for most graphs).\r\n> \r\n> During TOCO conversion a different input node (Preprocessor/sub) is used, as well as different output nodes (concat,concat_1). This skips some parts that are problematic for tflite, until either the graph is restructured or TF Lite reaches TF parity.\r\n> \r\n> Here are the quick steps for converting an SSD MobileNet model to tflite format and building the demo to use it:\r\n> \r\n> ```\r\n> # Download and extract SSD MobileNet model\r\n> wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\r\n> tar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz \r\n> DETECT_PB=$PWD/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\r\n> STRIPPED_PB=$PWD/frozen_inference_graph_stripped.pb\r\n> DETECT_FB=$PWD/tensorflow/contrib/lite/examples/android/assets/mobilenet_ssd.tflite\r\n> \r\n> # Strip out problematic nodes before even letting TOCO see the graphdef\r\n> bazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n> --input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n> --input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n> --alsologtostderr\r\n> \r\n> # Run TOCO conversion.\r\n> bazel run tensorflow/contrib/lite/toco:toco -- \\\r\n> --input_file=$STRIPPED_PB --output_file=$DETECT_FB \\\r\n> --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n> --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \\\r\n> --output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\r\n> \r\n> # Build and install the demo\r\n> bazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo\r\n> adb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\r\n> ```\r\n\r\nThis works like a charm!"]}, {"number": 15632, "title": "Branch 180053468", "body": "", "comments": ["No rush. Not too many commits today."]}, {"number": 15631, "title": "ImportError: libmklml_intel.so: cannot open shared object file: No such file or directory", "body": "GPU: 1080 Ti\r\ncuda 9.0\r\ncuddn 7.0 v7\r\ncentos 7.\r\ninstalled Intel MKL-DNN but the error is still present.\r\n\r\n\r\nhere is the output:\r\n````\r\nipython\r\nPython 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     70 for some common reasons and solutions.  Include the entire stack trace\r\n     71 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 72   raise ImportError(msg)\r\n     73 \r\n     74 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/sb0709/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/sb0709/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/sb0709/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n`\r\n\r\n\r\n\r\n", "comments": ["Good day. Totally same setup with same error. Can anyone help, please?\r\nUsed this guide to install mkl https://github.com/mind/wheels#mkl\r\nFile with name libmklml_intel.so locared at the /usr/local/lib, but looks like something wrong...", "Please see PR #12975", "Missed, but i am using python 2.7, not 3.6 :(", "@bignamehyp  is not the same issue, the build works fine with cuda 8 and is not a vm as in the article pointed by you. \r\n\r\nAlso I use GPU version and https://github.com/mind/wheels/releases/download/tf1.4-gpu/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n\r\nMKL is for running NN on CPU faster and the version installed on my machine is GPU. \r\nI installed libraries, build from source and still same issue. \r\n\r\nyou closed the issue without pointing to a solution ...", "I am using python 3.5 with Linux mint. After I installed cudnn (https://developer.nvidia.com/cudnn) and mkl-dnn (https://github.com/01org/mkl-dnn), I uninstalled tensorflow-gpu once:\r\nsudo python3.5 -m pip uninstall tensorflow-gpu\r\nThen installed again:\r\nsudo python3.5 -m pip install tensorflow-gpu\r\nAnd tensorflow worked without \"libmklml_intel.so\" error for some reason.\r\n", "@lechatthecat so you use python3.5 and the issue i have is due the python 3.6 is is not installed as you pointing out. ", "@sergiubuciumas\r\nMaybe Python 3.6 should be already installed in your PC. I tried with python3.6 also but it wprks too.\r\nWhat I did was:\r\n(Installed gpu driver and cuda and rebooted the PC)\r\n1. Installed Tensorflow-gpu (and cuda8.0)\r\n2. Installed cudnn.\r\n3. Installed mkl-dnn. \r\n(Here I got the libmklml error)\r\n4. Uninstalled tensorflow-gpu\r\nsudo python3.6 -m pip uninstall tensorflow-gpu\r\n5. Installed the tensorflow again and rebooted the PC.\r\nsudo python3.6 -m pip install tensorflow-gpu\r\n6. The error was gone.\r\nI'm not sure this work for you too.", "I missed the \u2018 , \u2018 after the python3.6, so not working in centos, will just wait for tensorflow 1.5 stable release, for now just downgraded to cuda 8 because the building from source is not working as well, tried everything posible to make it work under centos 7 and none works, if fixing the intel MKL than comes up the naming of libcudnn naming problem.\r\n\r\nWill double check the whl to modify but as for now source compile fails so need to look more on requirements and is getting hard to upgrade some tools in Centos 7. "]}, {"number": 15630, "title": "tf.name_scope does not work with tf.layers", "body": "When I define a layer with\r\n```\r\nwith tf.name_scope(\"MY_SCOPE\"):\r\n    layer1 = tf.layers.dense(inputs = X,\r\n                             units = 100,\r\n                             kernel_initializer = he_init,\r\n                             activation = tf.nn.elu,\r\n                             name = \"layer1\")\r\n\r\n\r\n```\r\n\r\nand try to get its variables with\r\n\r\n`tf.global_variables(scope=\"MY_SCOPE\")`\r\n\r\nit returns nothing because name scope did not apply to the layers from tf.layers\r\n\r\nShouldn't their name be\r\n\r\n[<tf.Variable 'MY_SCOPE/layer1/kernel:0' shape=(784, 100) dtype=float32_ref>,\r\n <tf.Variable 'MY_SCOPE/layer1/bias:0' shape=(100,) dtype=float32_ref>]\r\n\r\ninstead of\r\n\r\n[<tf.Variable 'layer1/kernel:0' shape=(784, 100) dtype=float32_ref>,\r\n <tf.Variable 'layer1/bias:0' shape=(100,) dtype=float32_ref>]\r\n\r\nso that I can use my scope to reach my layers? This works with everything but tf.layers module.", "comments": ["Hi, @aybberk . \r\nBecause `name_scope` is only for operations, you should use `tf.variable_scope(\"MY_SCOPE\")` which is designed for variables. \r\n\r\nYou might be interested on the post: [What's the difference of name scope and a variable scope in tensorflow?](https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow)", "Thanks for the response @facaiy ", "@asimshankar Would it be a good idea if TensorFlow could explain the two scopes in the official site in details? This is not the first time when I found people confuse `tf.name_scope` with `tf.variable_scope`.", "@asimshankar  I'd like to second @facaiy here, I would love to see explanation in the docs, it would save me quite some time today."]}, {"number": 15629, "title": "Update: HTTP -> HTTPS", "body": "URLs updated to use HTTPS protocol where appropriate to improve security and privacy.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15628, "title": "Support truly pluggable protocol/transport for distributed mode", "body": "At this moment Distributed TensorFlow supports only one type of transport/protocol which is gRPC, however, it does seem to be configurable (cluster, server, session).\r\n\r\nSo, there are at least three things that need to be covered:\r\n1. Document the intercommunication protocol (session to a server), i.e. what session sends to a server, what server should respond, etc. The good example here would some kind of swagger spec.\r\n2. Make protocol configurable from Python.\r\n3. Give all a Python interface to implement new types of protocols.\r\n\r\nThe whole idea is to let developers an ability to implement the protocol that would let distributed TensorFlow spin up and talk a number compute units (serverless, containers, VMs, etc.) instead of having a bunch of processes (cluster servers) running during computations within the session.\r\n\r\nPlease note, I wasn't able to find any corresponding issues related to given topic.", "comments": ["FYI @saeta ", "@poxvoculi this seems close to things you work on; can you offer any guidance?\r\n@mrry I think this fits the rubric of distributed training; might we want to expose this kind of functionality? \r\n\r\nOr alternatively, is this relatively open, and should I mark \"contributions welcome\"?", "@cy89 I do really look for help here. It does seem like changing the whole transport thing is not that simple, so any help is appreciated.", "@denismakogon I'd like to have a better understanding of what you'd like to be able to do.\r\n\r\nCurrently TF's runtime execution component is pretty much entirely coded in C++.  Communication between distributed instances generally occurs at this level.  The TF level communication protocol is documented in protocol buffer declarations, e.g.:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/master.proto\r\n\r\nTasks communicate with one another via RPCs that follow these protocols.  The specifics of the RPC system and transport layer don't matter much (other than with respect to performance and reliability :-) ).  gRPC is the only official fully supported transport, but there is support in contrib for sending RecvTensor requests over other RPC interfaces including MPI and verbs.   The intent of the TensorFlow design is to make it relatively easy to substitute another RPC layer by implementing (parts of the) Worker and Master interfaces.  This is configurable at runtime by specifying the any of the available protocols (e.g. 'grpc' but could also be 'grpc+mpi' as [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mpi/mpi_server_lib.cc#L86)).\r\n\r\nThat said, what is it that you'd like to do, and what's missing from TF to make that happen?\r\n\r\n \r\n\r\n\r\n", "@poxvoculi thanks for the reply.\r\n\r\nSo, here's what i would like to get from distributed mode:\r\n\r\n1. Make transport truly pluggable. gRPC is the good thing, but i would like to have HTTP as well. So, i do see the need to give to developers a choice of transports as well as the way to implement transports without recompiling TensorFlow from the source (think of Python interface for transport).\r\n\r\n2. Provide task encoder that will accept computation task and return its byte representation to send over the transport. Or make task serializable so the transport layer will decide what to do with that.\r\n\r\nSo, the feature i would like to have is to let developers decide how would that run their computations within the distributed mode, for instance, i don't want to create the bunch of workers that I would have to maintain by my own processes but delegate it to any orchestration engine (serverless, containers, clear containers, etc.)\r\n\r\n\r\nSide note\r\n=======\r\nI've found quite interesting note [here](https://www.tensorflow.org/deploy/distributed):\r\n```\r\nManually specifying these cluster specifications can be tedious, especially for large clusters. We are working on tools for launching tasks programmatically, e.g. using a cluster manager like Kubernetes. If there are particular cluster managers for which you'd like to see support, please raise a GitHub issue.\r\n```\r\n\r\nMy particular area of the interest is serverless technologies, having Docker or Kubernetes as the backend for the cluster is a good thing, but imagine that get computations units (serverless functions) only when you have something to calculate. So, when i create a computation session (`with tf.Session(\"http://worker7.example.com:8080\") as sess:` where service that stands behind that URL nothing but serverless gateway)  that talks to a serverless function all computations are serialized and send within HTTP request(s) can be deserialized and executed within the serverless function.\r\n", "The `protocol` argument that you provide when creating a `tf.train.Server` and/or in the prefix of the `target` argument to `tf.Session()` provides a way to switch to a different transport protocol. As @poxvoculi notes, we currently support `\"grpc\"` in the core, but folks have contributed `\"grpc+verbs\"` and `\"grpc+mpi\"` implementations of these. It's conceivable that you could use the same extension mechanism for (e.g.) `\"http\"`. If that's not truly pluggable, please let us know what's missing, and we can then consider the feature request.\r\n\r\nI'm not sure what a \"task encoder\" is in this context. Work items in TensorFlow are typically serialized as `GraphDef` protocol buffers, which I think should be sufficient, but if you could expand on this point, we might understand better.", "I don't understand what you mean by making transport pluggable.  Communication between TF distributed processes is designed to take place only via methods on the Master and Worker interfaces.  These methods need to be implemented as RPCs.  gRPC is the default RPC implementation but others are possible.  (Inside Google we use an additional two RPC systems, selected among by startup configuration options.  The contrib support cited above are examples of how to do this.)  Unless I'm mistaken, gRPC actually sits on top of HTTP, i.e. the server side uses an HTTP server to field requests.   By 'transport' do you mean a particular implementation of the Master and Worker interfaces, or do you want TF processes to communicate via some completely different methods than are in those interfaces? \r\n\r\n", "> By 'transport' do you mean a particular implementation of the Master and Worker interfaces, or do you want TF processes to communicate via some completely different methods that are in those > interfaces?\r\n\r\nExactly, I want to make both master and worker use different protocols (anything else different from gRPC). I want to control every send/receive computation request along with being able to serialize/deserialize computation task object (similar to `cloudpickle` but in terms of TF).\r\n\r\ngRPC or any other RPC is not the case for me because I don't want to build an additional layer in-between service I run and TF that translates RPC requests into the actual computation task to run.\r\n", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "I'm afraid I still don't have a good idea what you're suggesting.   I don't understand what you mean by protocol, since you seem to be rejecting the use of the master.proto and worker.proto interfaces, which are the only way TF processes intercommunicate.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@poxvoculi by saying protocol i meant the actual interaction protocol (who sends what?). At this moment all interactions are being described by protobuf, so there's no way to build another one interaction protocol for all components, for instance, if i want to get rid of protobuf and use, let's say, HTTP 1.1 for that.\r\n\r\n>  since you seem to be rejecting the use of the master.proto and worker.proto interfaces, which are the only way TF processes intercommunicate.\r\n\r\nThis is what i would like to change, i don't like the idea of mandatory protocols, this is very critical to those use cases when people want to build their own workers to achieve massive scale-in/out without exposing too much information about the infrastructure.\r\n", "@denismakogon I agree with @poxvoculi in that I'm still not quite sure what you're proposing. It seems like you can replace gRPC with the RPC package of your choice. I think that if you wanted to replace protobufs with some other portable marshalling solution, you'd have a huge amount of work on your hands for not necessarily a great deal of benefit (I agree that protos can be slow, but faster solutions that retain portability are hard work). \r\n\r\nThinking constructively, I wonder if a GitHub issue isn't quite the right way to have this discussion. Could we get you to write a somewhat longer-form document that describes (a) the problem you want to address, (b) the limitations of the current solutions, and (c) the way forward that you'd like to explore? It seems to me that a document would allow you to make a reasoned argument for what you want in a way that lets readers better understand where you're coming from. ", "Just my 2 cents, you can still use HTTP1.1 by serializing protobuf to JSON. I cannot see any benefits doing this though.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I'm going to close this to stop the auto-nagging.  Feel free to reopen if you want to respond to @cy89 's comment."]}, {"number": 15627, "title": "Cannot compute output tensor of tf.keras.layers.Lambda + tf.unstack", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: tf-nightly 1.5.0-dev20171224\r\n- **Python version**: 3.5.4\r\n\r\n### Source code\r\n\r\nThis is a tiny code that creates a model which stacks and unpacks input tensors.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python import keras\r\n\r\nx = [keras.layers.Input([2]) for _ in range(3)]\r\nh = keras.layers.Lambda(lambda inputs: tf.stack(inputs, axis=1))(x)\r\ny = keras.layers.Lambda(lambda inputs: tf.unstack(inputs, axis=1))(h)\r\n\r\nmodel = keras.models.Model(x, y)\r\nmodel.compile('adam', 'mse')\r\n```\r\n\r\nThis code raises a `AssertionError`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"example.py\", line 9, in <module>\r\n    model.compile('adam', 'mse')\r\n  File \".../tf-nightly/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 681, in compile\r\n    masks = self.compute_mask(self.inputs, mask=None)\r\n  File \".../tf-nightly/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 787, in compute_mask\r\n    _, output_masks = self._run_internal_graph(inputs, masks)\r\n  File \".../tf-nightly/lib/python3.5/site-packages/tensorflow/python/layers/network.py\", line 927, in _run_internal_graph\r\n    assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\r\nAssertionError: Could not compute output Tensor(\"lambda_1/unstack:1\", shape=(?, 2), dtype=float32)\r\n```", "comments": ["I found that this is an error that occurs when the number of outputs of `Layer.output_masks()` does not match the number of outputs of `Layer.call()`.", "Can you give some hint about how to debug with this error message: \"assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\"? It troubles me too, and I can't find what's wrong.", "_run_internal_graph() forget to put all outputs in tensor_map. I encounter this in TF1.4, but looks like it is gone in newer versions such as TF1.8."]}, {"number": 15626, "title": "TensorFlowInferenceInterface's feed method - a performance bottleneck", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Compiled on macos 10.13.2, Observed on Android \r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: commit ab0fcaceda on master - between release of 1.4.0 and 1.4.1 (compared to 1.0.1)\r\n- **Python version**: N/A (java android code)\r\n- **Bazel version (if compiling from source)**: 0.7.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: Observed on a Huawei Nexus 6P\r\n- **Exact command to reproduce**: N/A (java android code)\r\n\r\n### Describe the problem\r\nFollowing the move to Java API in commit 1a9769dc79fdd27c347633df210ff64f48de8d07, it seems that it is very ineffective to feed nodes.\r\nI am feeding my model an input array of about 1100 floats, and when I sample CPU usage using CPU Profiler on Android Studio (Instrumented) it seems that the feed method takes ~x4 time than running the inference. If I leave everything the same but I use android libs compiled in tensorflow 1.0.1 CPU time of feed method (used to be fillNodeFloat) becomes negligible.\r\nIt seems that putting a float array into the Tensor's FloatBuffer is a very costly operation.\r\n\r\n### Source code / logs\r\n**TF 1.4.0:**\r\nRelevant inference code:\r\n```\r\ntensorflow.feed(INPUT_NODE_NAME, input, shape);\r\ntensorflow.run(OUTPUT_NAMES);\r\ntensorflow.fetch(OUTPUT_NAMES[0], output);\r\n```\r\n\r\nScreenshot of CPU Profiler's call chart: https://www.dropbox.com/s/nx5q730l0fbd05x/TF_1.4_CallChart.png?dl=0\r\nScreenshot of CPU Profiler's top-down breakdown of the 2 methods: https://www.dropbox.com/s/gh9vza3jmb5uzrn/TF_1.4_TopDown.png?dl=0\r\n\r\n**TF 1.0.1:**\r\nRelevant inference code:\r\n```\r\ntensorflow.fillNodeFloat(INPUT_NODE_NAME, shape, input);\r\ntensorflow.runInference(OUTPUT_NAMES);\r\ntensorflow.readNodeFloat(OUTPUT_NAMES[0], output);\r\n```\r\n\r\nScreenshot of CPU Profiler's call chart: https://www.dropbox.com/s/jrl2ggnsncx97ry/TF_1.0.x_CallChart.png?dl=0\r\nScreenshot of CPU Profiler's bottom-up breakdown of the 2 methods: https://www.dropbox.com/s/tz5ldp4qnavhx17/TF_1.0.x_BottomUp.png?dl=0", "comments": ["Thanks for the report. This is surprising since from a cursory look I don't think there is any added inefficiency (for example, in terms of number of copies from Java to C++). I'll dig in a bit more.\r\n\r\nHowever, in the mean time, is it easy for you to test your application right before and right after the commit that moved to the Java API? (Just to rule out any other changes between TF 1.0.1 and 1.4 that may be the cause of the slowdown)?", "Hi, verified. With Android Studio 3.1 (Canary) it's easier to see method runtime.\r\nFor commit 1b960c30ab (1 before move to Java): `runInference` took 6ms, `fillNodeFloat` took <1ms\r\nFor commit 1a9769dc79fdd27c347633df210ff64f48de8d07: `runInference` took 6ms, `fillNodeFloat` took 49ms.\r\n\r\nMost of the time goes on `Tensor.create()`. Relevant line:\r\n```\r\n        t.buffer().asFloatBuffer().put(data);\r\n```\r\n", "FloatBuffer.put(float[]) and get(float[]) are very costly operations on Android:\r\nhttps://code.google.com/archive/p/javacpp/issues/11", "FYI, it's also possible to use the C++ interface directly via JavaCPP and bypass all the slow paths that way too: https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow\r\nhttps://medium.com/google-cloud/how-to-invoke-a-trained-tensorflow-model-from-java-programs-27ed5f4f502d\r\n", "Closing due to staleness. Please check with the latest version of TensorFlow. Feel free to reopen if the issue still persists. Thanks!"]}, {"number": 15625, "title": "cross compile tensorflow C++ API for armeabi-v7a runtime erreors", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS Sierra 10.13.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.3\r\n- **Python version**: 3.5.3\r\n- **Bazel version (if compiling from source)**:0.9.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**:4.2.1\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\ni follow tensorflow/contrib/makefile/README.md to compile static library and there are two *.a file under tensorflow/contrib/makefile/gen. but when i link those two libs with my test application (Android Application with Cmake), there ara some errors.\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nError:error: undefined reference to 'google::protobuf::io::CodedInputStream::ReadTagFallback(unsigned int)'\r\nError:error: undefined reference to 'google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)'\r\nError:error: ld returned 1 exit status", "comments": ["@gunan can you please take a look at this issue, thanks.", "i also try it on Ubuntu 14.0.4. use bazel 0.5.4 and NDK r12b to compile static library .\r\nthen i create an Android application link compiled librayies.\r\n my native method like this:\r\n\r\ntensorflow::GraphDef graphDef;\r\n    tensorflow::string path = \"sdcard/frozen_inference_graph.pb\";\r\n\r\n    tensorflow::Status status = ReadBinaryProto(tensorflow::Env::Default(), path, &graphDef);\r\n    if(!status.ok()) {\r\n        return env->NewStringUTF(\"not found graph\");\r\n    }\r\n\r\n    std::unique_ptr<tensorflow::Session> session;\r\n\r\n    (&session)->reset(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n\r\n    status = session->Create(graphDef);\r\n\r\n    if(!status.ok()) {\r\n        return env->NewStringUTF(\"create session error\");\r\n    }\r\n\r\nand CMakeLists.txt\r\n\r\ncmake_minimum_required(VERSION 3.4.1)\r\n\r\nset( CMAKE_BUILD_TYPE  \"Release\")\r\n\r\nset( CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} -Wl,--allow-multiple-definition -Wl,--whole-archive\")\r\n\r\nset( tf_include  /home/yuanhao/Android/tf/tf)\r\n\r\ninclude_directories( ${tf_include})\r\n\r\n\r\nadd_library( pb  STATIC  IMPORTED )\r\nset_target_properties( pb  PROPERTIES  IMPORTED_LOCATION  /home/yuanhao/Android/tf/libprotobuf.a)\r\n\r\nadd_library( tf  STATIC  IMPORTED )\r\nset_target_properties( tf  PROPERTIES  IMPORTED_LOCATION  /home/yuanhao/Android/tf/libtensorflow-core.a)\r\n\r\nfile( GLOB  src  \"src/main/cpp/*.cpp\")\r\n\r\nadd_library( native-lib  SHARED  ${src} )\r\n\r\n\r\ntarget_link_libraries( native-lib  log  tf  pb )\r\n\r\nwhen i build my application, errors \r\n\r\n Build command failed.\r\n  Error while executing process /home/yuanhao/Android/Sdk/cmake/3.6.4111459/bin/cmake with arguments {--build /home/yuanhao/AndroidStudioProjects/Test/app/.externalNativeBuild/cmake/debug/armeabi-v7a --target native-lib}\r\n  [1/2] Building CXX object CMakeFiles/native-lib.dir/src/main/cpp/native-lib.cpp.o\r\n  [2/2] Linking CXX shared library ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so\r\n  FAILED: : && /home/yuanhao/Android/android-ndk-r12b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-g++  --sysroot=/home/yuanhao/Android/android-ndk-r12b/platforms/android-21/arch-arm -fPIC -g -DANDROID -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -mthumb -Wa,--noexecstack -Wformat -Werror=format-security -fno-exceptions -fno-rtti -g -DANDROID -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -mthumb -Wa,--noexecstack -Wformat -Werror=format-security -fno-exceptions -fno-rtti -std=c++11 -frtti -fexceptions -Wl,--allow-multiple-definition -Wl,--whole-archive -Os -DNDEBUG -Os -DNDEBUG  -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--fix-cortex-a8 -Wl,--no-undefined -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,now -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--fix-cortex-a8 -Wl,--no-undefined -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,now -shared -Wl,-soname,libnative-lib.so -o ../../../../build/intermediates/cmake/debug/obj/armeabi-v7a/libnative-lib.so CMakeFiles/native-lib.dir/src/main/cpp/native-lib.cpp.o  -llog /home/yuanhao/Android/tf/libtensorflow-core.a /home/yuanhao/Android/tf/libprotobuf.a -lm \"/home/yuanhao/Android/android-ndk-r12b/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a\" && :\r\n  /home/yuanhao/Android/tf/libprotobuf.a(gzip_stream.o):gzip_stream.cc:function google::protobuf::io::GzipInputStream::~GzipInputStream(): error: undefined reference to 'inflateEnd'\r\n  /home/yuanhao/Android/tf/libprotobuf.a(gzip_stream.o):gzip_stream.cc:function google::protobuf::io::internalInflateInit2(z_stream_s*, google::protobuf::io::GzipInputStream::Format): error: undefined reference to 'inflateInit2_'\r\n  /home/yuanhao/Android/tf/libprotobuf.a(gzip_stream.o):gzip_stream.cc:function google::protobuf::io::GzipInputStream::Inflate(int): error: undefined reference to 'inflate'\r\n  /home/yuanhao/Android/tf/libprotobuf.a(gzip_stream.o):gzip_stream.cc:function google::protobuf::io::GzipInputStream::Next(void const**, int*): error: undefined reference to 'inflateEnd'\r\n  /home/yuanhao/Android/tf/libprotobuf.a(gzip_stream.o):gzip_stream.cc:function google::protobuf::io::GzipOutputStream::Init(google::protobuf::io::ZeroCopyOutputStream*, google::protobuf::io::GzipOutputStream::Options const&): error: undefined reference to 'deflateInit2_'\r\n  /home/yuanhao/Android/tf/libprotobuf.a(gzip_stream.o):gzip_stream.cc:function google::protobuf::io::GzipOutputStream::Deflate(int): error: undefined reference to 'deflate'\r\n  /home/yuanhao/Android/tf/libprotobuf.a(gzip_stream.o):gzip_stream.cc:function google::protobuf::io::GzipOutputStream::Close(): error: undefined reference to 'deflateEnd'\r\n  collect2: error: ld returned 1 exit status\r\n  ninja: build stopped: subcommand failed.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to inactivity. Please reopen if this is still a problem on master.", "@nullian  I have the same issue as you, have you solved this problem? "]}, {"number": 15623, "title": "I cannot use Binomial.sample(), could you please help me?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nI apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 15622, "title": "Unable to compile from source on High Sierra (10.13.2) with bazel 0.9.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: tf 1.4\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **Exact command to reproduce**: bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=- msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI'm unable to compile tensorflow from source. I get too many errors as shown below in the logs:\r\n\r\n### Source code / logs\r\n```\r\nRakshiths-MacBook-Pro:tensorflow rakshithgb$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.9.0-homebrew installed.\r\nPlease specify the location of python. [Default is /Users/rakshithgb/miniconda3/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /Users/rakshithgb/miniconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/rakshithgb/miniconda3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: n\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\nRakshiths-MacBook-Pro:tensorflow rakshithgb$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=- msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package\r\n..............\r\nERROR: Skipping 'msse4.1': no such target '//:msse4.1': target 'msse4.1' not declared in package '' defined by /Users/rakshithgb/Documents/Tensorflow/tensorflow/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD\", line 27\r\n\t\tcc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n\tFile \"/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n\t\tsycl_library_path\r\nname 'sycl_library_path' is not defined\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/platform/default/build_config/BUILD:115:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//tensorflow/core/platform/default/build_config:platformlib'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3169:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pooling_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3798:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:variable_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:717:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:compare_and_bitpack_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3776:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:scatter_nd_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3764:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:dense_update_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:643:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:gather_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:607:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:bitcast_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:619:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:constant_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:625:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:diag_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:631:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:edit_distance_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:687:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:mirror_pad_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:711:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:quantize_and_dequantize_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:787:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:split_v_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:774:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:slice_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3770:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:scatter_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:3758:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:count_up_to_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/BUILD:2215:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//tensorflow/core:sycl_runtime'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:780:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:split_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:681:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_set_diag_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:601:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:bcast_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:756:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reverse_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:699:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pack_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:813:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:transpose_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:705:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:pad_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:693:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:one_hot_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:649:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:identity_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:661:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:listdiff_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:533:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:immutable_constant_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:655:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:identity_n_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:613:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:concat_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:675:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_diag_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:839:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:where_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:762:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reverse_sequence_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:794:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:inplace_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:827:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:unique_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:768:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:shape_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:667:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:matrix_band_part_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:637:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:gather_nd_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:750:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:reshape_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:800:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:tile_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:833:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:unpack_op'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/core/kernels/BUILD:550:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '//tensorflow/core/kernels:debug_ops'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@com_googlesource_code_re2//:LICENSE' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@local_config_sycl//sycl:LICENSE.text' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'\r\nWARNING: errors encountered while analyzing target '//tensorflow/tools/pip_package:build_pip_package': it will not be built\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (204 packages loaded).\r\nINFO: Found 0 targets...\r\nERROR: command succeeded, but there were errors parsing the target pattern\r\nINFO: Elapsed time: 51.902s, Critical Path: 0.02s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n", "comments": ["During the build configuration step, you didn't define (leaving blank) any optimization flags you'd like to compile with which by default will set the ones available and enabled in your system:\r\n`Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: `\r\n\r\nWhen compiling with bazel though you defined a set of optimization flags not previously configured in the previous step:\r\n`Rakshiths-MacBook-Pro:tensorflow rakshithgb$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=- msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nYou should have used `--config=opt` instead when compiling. \r\nIf you want specific flags available in your system to be enabled in the build you need to specify in them in the configuration step.\r\n\r\n\r\n", "Also, there is an errant space in your copt specifying `msse4.1` which is probably leading to the Bazel complaints.", "As @quaeler pointed even if you set default flags and have all the ones, including `msse4.1`, enabled in your system that you specified whan building I suspect it should've worked if properly defined either way.\r\n\r\nOn Mon, Dec 25, 2017, 3:01 PM loki der quaeler <notifications@github.com>\r\nwrote:\r\n\r\n> Also, there is an errant space in your copt specifying msse4.1 which is\r\n> probably leading to the Bazel complaints.\r\n>\r\n> \u2014\r\n> You are receiving this because you commented.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/15622#issuecomment-353874371>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/APwNCotu5nE5oMV1Mvf35-vA-nl2sCyEks5tD7i8gaJpZM4RMI8b>\r\n> .\r\n>\r\n", "@Carmezim using `--config=opt` just added more errors. ", "Could you post the stack trace with commands? Have you tried specifying the flags when configuring the build as well? I built from source without problems. https://github.com/yaroslavvb/tensorflow-community-wheels/issues/36", "```\r\nRakshiths-MacBook-Pro:tensorflow rakshithgb$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.9.0-homebrew installed.\r\nPlease specify the location of python. [Default is /Users/rakshithgb/miniconda3/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /Users/rakshithgb/miniconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/rakshithgb/miniconda3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: n\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: --config=opt\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n\r\nRakshiths-MacBook-Pro:tensorflow rakshithgb$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```", "Oh, you're confusing steps. During configuration you leave it set to\ndefault e.g. press return leaving it blank.\n\n`--config=opt` it's to be defined when you're building with bazel as in\n`bazel build --config=opt ...`\n\nOn Mon, Dec 25, 2017, 5:18 PM Rakshith G B <notifications@github.com> wrote:\n\n> Rakshiths-MacBook-Pro:tensorflow rakshithgb$ ./configure\n> WARNING: Running Bazel server needs to be killed, because the startup options are different.\n> You have bazel 0.9.0-homebrew installed.\n> Please specify the location of python. [Default is /Users/rakshithgb/miniconda3/bin/python]:\n>\n>\n> Found possible Python library paths:\n>   /Users/rakshithgb/miniconda3/lib/python3.6/site-packages\n> Please input the desired Python library path to use.  Default is [/Users/rakshithgb/miniconda3/lib/python3.6/site-packages]\n>\n> Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\n> No Google Cloud Platform support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\n> No Hadoop File System support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\n> No Amazon S3 File System support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with XLA JIT support? [y/N]: n\n> No XLA JIT support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with GDR support? [y/N]: n\n> No GDR support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with VERBS support? [y/N]: n\n> No VERBS support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with OpenCL support? [y/N]: n\n> No OpenCL support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with CUDA support? [y/N]: n\n> No CUDA support will be enabled for TensorFlow.\n>\n> Do you wish to build TensorFlow with MPI support? [y/N]: n\n> No MPI support will be enabled for TensorFlow.\n>\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: --config=opt\n>\n>\n> Add \"--config=mkl\" to your bazel command to build with MKL support.\n> Please note that MKL on MacOS or windows is still not supported.\n> If you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\n> Configuration finished\n>\n> Rakshiths-MacBook-Pro:tensorflow rakshithgb$ bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=opt -k //tensorflow/tools/pip_package:build_pip_package\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15622#issuecomment-353881019>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APwNCrqZWoPMvIZY3BHwP3OwzplOnX8dks5tD9jogaJpZM4RMI8b>\n> .\n>\n", "But my build command does have `--config=opt`. However I tried just with `--config=opt` without all the flags too. The build is still not successful. ", "But your configuration command shouldn't,  just be empty if you're\ncompiling for your native architecture. I strongly recommend you visit the\nofficial docs under installation from source. It has everything well\nexplained step by step including the commands.\n\nOn Mon, Dec 25, 2017, 5:45 PM Rakshith G B <notifications@github.com> wrote:\n\n> But my build command does have --config=opt\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15622#issuecomment-353882159>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APwNCjIv-cNlDIQq7bOhF8fL9O83D-Kcks5tD98vgaJpZM4RMI8b>\n> .\n>\n", "@Carmezim Yes I did. This is what happened when I did:\r\n\r\n```\r\nRakshiths-MacBook-Pro:tensorflow rakshithgb$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.9.0-homebrew installed.\r\nPlease specify the location of python. [Default is /Users/rakshithgb/miniconda3/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /Users/rakshithgb/miniconda3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/rakshithgb/miniconda3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: n\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\nRakshiths-MacBook-Pro:tensorflow rakshithgb$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n................\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD\", line 27\r\n\t\tcc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n\tFile \"/private/var/tmp/_bazel_rakshithgb/fde7bc60972656b0c2db4fd0b79e24fb/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n\t\tsycl_library_path\r\nname 'sycl_library_path' is not defined\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@com_googlesource_code_re2//:LICENSE' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: /Users/rakshithgb/Documents/Tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@local_config_sycl//sycl:LICENSE.text' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 3.602s\r\nFAILED: Build did NOT complete successfully (69 packages loaded)\r\n    currently loading: tensorflow/core ... (2 packages)\r\n```", "Thanks for explaining. I am not able to properly take a look on that right now but will Cc @gunan here or wait for another TensorFlower to check it.", "@Carmezim looks like the new version of bazel is causing this problem. Many are facing this issue both on Mac and Ubuntu. [#15492](https://github.com/tensorflow/tensorflow/issues/15492) And it also looks like the next release of Tensorflow has this fixed.\r\n\r\nBuilding it with `--incompatible_load_argument_is_label=false` in the bazel build command solved the issue. My build command now is:\r\n`bazel build --config=opt --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package`", "Ah I see. Cool, great job, thanks for digging this.\n\nOn Mon, Dec 25, 2017, 7:33 PM Rakshith G B <notifications@github.com> wrote:\n\n> @Carmezim <https://github.com/carmezim> looks like the new version of\n> bazel is causing this problem. Many are facing this issue both on Mac and\n> Ubuntu. #15492 <https://github.com/tensorflow/tensorflow/issues/15492>\n> And it also looks like the next release of Tensorflow has this fixed.\n>\n> Building it with --incompatible_load_argument_is_label=false in the bazel\n> build command solved the issue. My build command now is:\n> bazel build --config=opt --incompatible_load_argument_is_label=false\n> //tensorflow/tools/pip_package:build_pip_package\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15622#issuecomment-353886994>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APwNCqhZGNqoVYrEtrT30fgDUsXgpCMUks5tD_h9gaJpZM4RMI8b>\n> .\n>\n", "Did you have any success with [this suggestion](https://github.com/tensorflow/tensorflow/issues/15492#issuecomment-353742990)?", "Yes it compiled without any errors. ", "Cool, glad you got it working. Maybe we should close this one as a dupe and keep track there then.", "Closing this as a duplicate of #15492 "]}, {"number": 15621, "title": "Errors_impl-NotFoundError-Undefined symbol-fused_conv2d_bias_activation_op", "body": "**What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?**\r\nNone.\r\n\r\n### System information\r\n- **Operating System**: CentOS 7.3\r\n- **Linux Kernel**: 3.10.0-514\r\n- **TensorFlow installed from source**: https://github.com/tensorflow/tensorflow/\r\n- **TensorFlow version**: 1.4.0\r\n- **Python version**: Python 2.7.5\r\n- **The output of bazel version**:\r\n```\r\nBuild target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Jan 01 00:00:00 1970 (0)\r\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\r\nBuild timestamp as int: 0\r\n```\r\n- **GCC/Compiler version (if compiling from source)**: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\n- **CUDA/cuDNN version**: cuda 8.0/cudnn 7.0\r\n- **GPU model and memory**: NVIDA Tesla P4\r\n\r\n### Describe the problem\r\nWhen exec fused_conv2d_bias_activation_op.py or import the package using `import tensorflow.contrib.fused_conv.python.ops` \uff0c'undefined symbol' error occurs\r\n\r\nThe source code file path is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/fused_conv/python/ops/fused_conv2d_bias_activation_op.py\r\n\r\nThe output of `python fused_conv2d_bias_activation_op_test.py` is as follows:\r\n`tensorflow.python.framework.errors_impl.NotFoundError: /usr/lib/python2.7/site-packages/tensorflow/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so: undefined symbol: _ZN10tensorflow7functor8PadInputIN5Eigen9GpuDeviceEiiLi4EEclERKS3_NS2_9TensorMapINS2_6TensorIKiLi4ELi1EiEELi16ENS2_11MakePointerEEERKSt5arrayIiLm2EESG_NS7_INS8_IiLi4ELi1EiEELi16ESB_EENS_12TensorFormatE`\r\n\r\nIt seems that something wrong when loading Eigen library\r\n\r\n### The other logs\r\nThe output of `objdump -t /usr/lib/python2.7/site-packages/tensorflow/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so | grep UND | grep Eigen` is as follows:\r\n`0000000000000000       F *UND*  0000000000000000              _ZN10tensorflow25BrainPadding2EigenPaddingENS_7PaddingE\r\n0000000000000000         *UND*  0000000000000000              _ZN10tensorflow7functor8PadInputIN5Eigen9GpuDeviceEiiLi4EEclERKS3_NS2_9TensorMapINS2_6TensorIKiLi4ELi1EiEELi16ENS2_11MakePointerEEERKSt5arrayIiLm2EESG_NS7_INS8_IiLi4ELi1EiEELi16ESB_EENS_12TensorFormatE\r\n0000000000000000         *UND*  0000000000000000              _ZN10tensorflow7functor8PadInputIN5Eigen9GpuDeviceEfiLi4EEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi4ELi1EiEELi16ENS2_11MakePointerEEERKSt5arrayIiLm2EESG_NS7_INS8_IfLi4ELi1EiEELi16ESB_EENS_12TensorFormatE\r\n0000000000000000       F *UND*  0000000000000000              _ZNK10tensorflow15OpKernelContext12eigen_deviceIN5Eigen9GpuDeviceEEERKT_v\r\n0000000000000000         *UND*  0000000000000000              _ZN5Eigen8internal14TensorExecutorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorIfLi4ELi1EiEELi16ENS_11MakePointerEEEKNS_17TensorReshapingOpIKNS_6DSizesIiLi4EEEKNS_17TensorShufflingOpIKNS9_IiLi3EEEKNS8_ISE_KNS3_INS4_IKfLi4ELi1EiEELi16ES6_EEEEEEEEEENS_9GpuDeviceELb0EE3runERSQ_RKSR_\r\n0000000000000000         *UND*  0000000000000000              _ZN10tensorflow7functor10NHWCToNCHWIN5Eigen9GpuDeviceEfLi4EEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi4ELi1ElEELi16ENS2_11MakePointerEEENS7_INS8_IfLi4ELi1ElEELi16ESB_EE`\r\n", "comments": ["I have tried Tensorflow 1.3.0 (built from source), and the same \"fused_conv2d_bias_activation_op\" from\r\n`import tensorflow as tf\r\nimport tensorflow.contrib.fused_conv`\r\n\r\nand the same issue occurs with error:\r\n`Traceback (most recent call last):\r\n  File \"/mnt/sdb/xt/machine_learning/project/fused_conv_test.py\", line 2, in <module>\r\n    import tensorflow.contrib.fused_conv\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/fused_conv/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op import *\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/fused_conv/python/ops/fused_conv2d_bias_activation_op.py\", line 26, in <module>\r\n    resource_loader.get_path_to_datafile(\"_fused_conv2d_bias_activation_op.so\"))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/lib64/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/lib/python2.7/site-packages/tensorflow/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so: undefined symbol: _ZN5Eigen8internal14TensorExecutorIKNS_14TensorAssignOpINS_9TensorMapINS_6TensorIfLi4ELi1EiEELi16ENS_11MakePointerEEEKNS_17TensorReshapingOpIKNS_6DSizesIiLi4EEEKNS_17TensorShufflingOpIKNS9_IiLi3EEEKNS8_ISE_KNS3_INS4_IKfLi4ELi1EiEELi16ES6_EEEEEEEEEENS_9GpuDeviceELb0EE3runERSQ_RKSR_`\r\n\r\nthe missing symbol is demangled as:\r\nEigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<int, 4> const, Eigen::TensorShufflingOp<Eigen::DSizes<int, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<int, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, int>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<int, 4> const, Eigen::TensorShufflingOp<Eigen::DSizes<int, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<int, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, int>, 16, Eigen::MakePointer> const> const> const> const> const&, Eigen::GpuDevice const&)\r\n\r\nDoes this mean that the **_fused_conv2d_bias_activation_op.so** is incorrectly built or lacks some deps in its BUILD file?", "@yzhwang can you please take a look? Thanks", "It seems the float type  GPU declaration somehow hit a library-missing issue.  The problem code is as below:\r\nIn \"tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc\" :\r\n```// Registration of the GPU implementations.\r\nREGISTER_KERNEL_BUILDER(\r\n    Name(\"FusedConv2DBiasActivation\")\r\n        .Device(DEVICE_GPU)\r\n        .TypeConstraint<float>(\"T\")\r\n        .TypeConstraint<float>(\"Tbias\")\r\n        .HostMemory(\"conv_input_scale\")\r\n        .HostMemory(\"side_input_scale\"),\r\n    FusedConv2DBiasActivationOp<GPUDevice, float, float, float>);\r\n```\r\n\r\nI have worked out a workaround for this issue, and I will post it here in case someone wants to use this fused_conv2d_bias_activation_op:\r\n\r\nFirst add addtional link opt \r\n```linkopts = [\"-shared-libgcc /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so -Xlinker --unresolved-symbols=ignore-in-shared-libs\"] ```\r\nto the BUILD file in tensorflow/contrib/fused_conv/BUILD:\r\n\r\n```tf_custom_op_library(\r\n    name = \"python/ops/_fused_conv2d_bias_activation_op.so\",\r\n    srcs = [\r\n        \"kernels/fused_conv2d_bias_activation_op.cc\",\r\n        \"kernels/fused_conv2d_bias_activation_op.h\",\r\n        \"kernels/fused_conv_ops_gpu.h\",\r\n        \"ops/fused_conv2d_bias_activation_op.cc\",\r\n    ],\r\n    deps = [\r\n        \"//tensorflow/core:lib_proto_parsing\",\r\n        \"//tensorflow/core/kernels:bounds_check_lib\",\r\n        \"//tensorflow/core/kernels:conv_2d_hdrs\",\r\n        \"//tensorflow/core/kernels:conv_ops_gpu_hdrs\",\r\n        \"//tensorflow/core/kernels:gpu_util_hdrs\",\r\n        \"//tensorflow/core/kernels:ops_util_hdrs\",\r\n    ],\r\n    linkopts = [\"-shared-libgcc /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so -Xlinker --unresolved-symbols=ignore-in-shared-libs\"],\r\n)\r\n```\r\n\r\nThis is to explicitly link against the _pywrap_tensorflow_internal.so since some symbols are hidden by Bazel in new version (from [Issue 15582](https://github.com/tensorflow/tensorflow/issues/15582) ). And this \"--unresolved-symbols=ignore-in-shared-libs\" option will help you get the line that causes the \"undefined symbol\" error.\r\n\r\nAfter this is done, use bazel to rebuild the op and you can find out the reason causing the problem:\r\n\r\n```bazel-out/local_linux-opt/bin/tensorflow/contrib/fused_conv/_objs/python/ops/_fused_conv2d_bias_activation_op.so/tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.pic.o: In function :\r\n**tensorflow::LaunchFusedConv2DBiasActivationOp<Eigen::GpuDevice, float, float, float>::launch(tensorflow::OpKernelContext*, bool, tensorflow::Tensor const&, float, tensorflow::Tensor const&, int, int, Eigen::PaddingType const&, tensorflow::Tensor const&, float, tensorflow::Tensor const&, tensorflow::ActivationMode, tensorflow::TensorFormat, tensorflow::FilterTensorFormat, tensorflow::Tensor*)**\r\nfused_conv2d_bias_activation_op.cc:(.text._ZN10tensorflow33LaunchFusedConv2DBiasActivationOpIN5Eigen9GpuDeviceEfffE6launchEPNS_15OpKernelContextEbRKNS_6TensorEfS8_iiRKNS1_11PaddingTypeES8_fS8_NS_14ActivationModeENS_12TensorFormatENS_18FilterTensorFormatEPS6_[_ZN10tensorflow33LaunchFusedConv2DBiasActivationOpIN5Eigen9GpuDeviceEfffE6launchEPNS_15OpKernelContextEbRKNS_6TensorEfS8_iiRKNS1_11PaddingTypeES8_fS8_NS_14ActivationModeENS_12TensorFormatENS_18FilterTensorFormatEPS6_]+0x1b5b): undefined reference to Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<int, 4> const, Eigen::TensorShufflingOp<Eigen::DSizes<int, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<int, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, int>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, int>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<int, 4> const, Eigen::TensorShufflingOp<Eigen::DSizes<int, 3> const, Eigen::TensorReshapingOp<Eigen::DSizes<int, 3> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, int>, 16, Eigen::MakePointer> const> const> const> const> const&, Eigen::GpuDevice const&)'\r\n```\r\n\r\nThen we go to the \"tensorflow/contrib/fused_conv/kernels/fused_conv2d_bias_activation_op.cc\" file and **comment out** the GPU-Float specific:\r\n```// Registration of the GPU implementations.\r\n/*\r\nREGISTER_KERNEL_BUILDER(\r\n    Name(\"FusedConv2DBiasActivation\")\r\n        .Device(DEVICE_GPU)\r\n        .TypeConstraint<float>(\"T\")\r\n        .TypeConstraint<float>(\"Tbias\")\r\n        .HostMemory(\"conv_input_scale\")\r\n        .HostMemory(\"side_input_scale\"),\r\n    FusedConv2DBiasActivationOp<GPUDevice, float, float, float>);\r\n*/\r\n```\r\n\r\nThen you can rebuild tensorflow and now the FusedConv2DBiasActivation should be OK to use, But only the INT8(qint8) type input is allowed (since we eliminated the float implementation).", "Is there any news about this issue?", "I apologize, but nobody is working on this, and since it is in contrib, it is not officially supported.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "Closing this issue due to staleness. Feel free to reopen when new information is available. Thanks!"]}, {"number": 15620, "title": "Tf Lite only support 4D l2_normalize?", "body": "I build some feature extract network model and converted tflite using by toco successfully.\r\nBut I got error `\"tensorflow/contrib/lite/kernels/l2norm.cc:47 NumDimensions(input) != 4 (2 != 4)`, when run interpreter->AllocateTensors().\r\n\r\nI extract feature using by tf.nn.l2_normalize.\r\n`embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')`\r\nwhere prelogits is 2D tensor.\r\nHow can I extract normalized feature with tflite?\r\n", "comments": ["@aselle @andrehentz for thoughts here.", "The limitation is a bit artificial at this point. I'll have a fix for that soon. In the meantime, you can try the following changes to kernels/l2norm.cc:\r\n\r\n```\r\n46,47c46\r\n<   // TODO(ahentz): Our current implementations rely on the inputs being 4D.\r\n<   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\r\n---\r\n>   TF_LITE_ENSURE(context, NumDimensions(input) <= 4);\r\n57,62c56\r\n<   TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\r\n<   output_size->data[0] = input->dims->data[0];\r\n<   output_size->data[1] = input->dims->data[1];\r\n<   output_size->data[2] = input->dims->data[2];\r\n<   output_size->data[3] = input->dims->data[3];\r\n<\r\n---\r\n>   TfLiteIntArray* output_size = TfLiteIntArrayCopy(input->dims);\r\n```\r\n\r\n  ", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 15619, "title": "Crelu should have axis", "body": "Currently, the `tf.nn.crelu` activation concatenates along the last axis. This would work fine for dense layers and conv layers where the `data_fromat=channels_last`, but this would be incorrect if invoked on the widely used `data_format=channels_first` for conv layers on the GPU.\r\n", "comments": ["@vincentvanhoucke ", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!", "Added a PR #15836 for axis support."]}]