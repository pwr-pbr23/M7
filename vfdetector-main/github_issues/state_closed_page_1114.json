[{"number": 19824, "title": "[tfdbg]tensorflow debugger doesn't work", "body": " I tried to use tfdbg to debug my code. I run `>tfdbg run`  to debug while the error is : \r\n`Non-OK-status:env->NewWritableFile(file_path, &f) status: Not found: Failed to create a NewWriteableFile: C:\\Users\\64484\\AppData\\Local\\Temp\\tfdbg_6ipwfvi_/_tfdbg_device_,job_localhost,replica_0,task_0,device_GPU_0/gradients/seq2seq/encoder/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc/max_size_0_DebugIdentity_1528265028602834 : \u7cfb\u7edf\u627e\u4e0d\u5230\u6307\u5b9a\u7684\u8def\u5f84\u3002` \r\n\r\nI have gone to stackoverflow for methods, while there's no answers. Is this a bug of tfdbg? Thanks for help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Actually, I think it doesn't belong to any items from the list but debug part. Therefore, I've changed the title with tfdbg. Is that OK?", "Nagging Assignee @jart: It has been 180 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19823, "title": "Problem with MirroredStrategy, PerDeviceDataset requires batch function", "body": "When MirroredStrategy is used with tf.estimator for multi-gpu train, the dataset has to be batched. However, for detection jobs, the input image shape differs one by one, so it cannot be batched. What is the recommended way to solve this problem? I think padding is not a good way to do this although it works. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Could you clarify your question? Are you trying to do training or inference? Are you saying your input shape is not consistent?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19822, "title": "bazel error:cannot be loaded: Not found: Op type not registered 'ClipByValue' in binary running on", "body": "when i start a tensorflow serving :bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=8000 --model_name=aS_24443.1000 --model_base_path=...\r\ni get the error :\r\n\r\n2018-06-07 07:49:54.236996: I tensorflow_serving/util/retrier.cc:34] Retrying of Loading servable: {name: aS_24443.1000 version: 2} retry: 5\r\n2018-06-07 07:49:54.283112: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /home/work/ics-contrib/freechat-serving/serving/load/aS_24443.1000/2\r\n2018-06-07 07:49:54.283150: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:240] Loading SavedModel with tags: { serve }; from: /home/work/ics-contrib/freechat-serving/serving/load/aS_24443.1000/2\r\n2018-06-07 07:49:54.483091: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:289] SavedModel load for tags { serve }; Status: fail. Took 199577 microseconds.\r\n2018-06-07 07:49:54.483500: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: aS_24443.1000 version: 2} failed: Not found: Op type not registered 'ClipByValue' in binary running on sandbox03. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\ncentOs\r\ntensorflow1.8\r\nkeras 2.1\r\n\r\nbut i can start the serving on another computer which has the same environment", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19821, "title": "Quantized Model is 50% slower in performance. Is this expected?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.1\r\n- **Python version**: 3.6.5 :: Anaconda Inc\r\n- **Bazel version (if compiling from source)**: 0.13.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: GeForce GTX 1080 Ti/PCIe/SSE2\r\n- **Exact command to reproduce**: python tensorflow/examples/label_image/label_image.py\r\n\r\n### Describe the problem\r\n\r\nQuantized Inception-3 model using transform_graph.py script. Tested on 1000 Imagenet images. Un-quantized model takes 800 seconds whereas quantized model takes 1600 seconds. Is this the expected behavior? \r\n\r\nPS. Let me know please if I should move this to SO?\r\n\r\n### Source code / logs\r\n\r\nModel files:\r\n\r\n>  model_file = \\\r\n>     \"tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb\"\r\n>   label_file = \"tensorflow/examples/label_image/data/imagenet_slim_labels.txt\"\r\n> \r\n> \r\n\r\n```\r\n$ bazel build tensorflow/tools/graph_transforms:transform_graph\r\n\r\n$ bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=./tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb --out_graph=./tensorflow/examples/label_image/data/eight_bits.pb --inputs=Mul:0 --outputs=final_result:0 --transforms='fold_constants(ignore_errors=true)  fold_batch_norms fold_old_batch_norms quantize_weights'\r\n```\r\n\r\n-----------After quantization, I used label_image.py script to test images in a loop as below.\r\n\r\n```\r\n  ts = time.time()\r\n\r\n  for file in os.listdir('~/test_images'):\r\n      file_name = 'tensorflow/test_images_imagenet/' + file\r\n      print (file_name)\r\n      t = read_tensor_from_image_file(\r\n          file_name,\r\n          input_height=input_height,\r\n          input_width=input_width,\r\n          input_mean=input_mean,\r\n          input_std=input_std)\r\n\r\n      with tf.Session(graph=graph) as sess:\r\n          results = sess.run(output_operation.outputs[0], {\r\n          input_operation.outputs[0]: t\r\n      })\r\n      results = np.squeeze(results)\r\n\r\n  tf = time.time()\r\n  diff = tf - ts\r\n  print(diff)\r\n\r\n```\r\n\r\n", "comments": ["@tfboyd @petewarden  Could you please guide if this is the best way to do quantization on a pre-trained model. Commands given in [older documentation](https://www.tensorflow.org/versions/r1.3/performance/quantization)  do not work anymore. [New documentation](https://www.tensorflow.org/performance/quantization) seems to expect creating and evaluating graph before running TOCO script. Please help.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Quantized inference is now done using tensorflow lite instead of tensorflow core. Please look at the docs at: https://www.tensorflow.org/performance/quantization. ", "Closing due to inactivity, only ARM optimized kernels exist for quantization at the moment, other kernels can be added but are currently not prioritized.\r\n\r\nPlease follow the new documentation (https://www.tensorflow.org/performance/quantization) to create a trained quantized model and convert to TFLite with tflite_convert. We will be provided more tools and docs in the upcoming weeks so look out :)\r\n\r\nThanks!"]}, {"number": 19820, "title": "Adding a constraint for the setuptools version.", "body": "", "comments": ["Could you let us know what is the reason to have an upper constraint on `setuptools`?\r\nHave this been discussed somewhere else and the link/reference to that is missing?", "At one point, tensorflow imports were failing because of a specific setuptools release.\r\nWe applied this patch as an emergency fix.\r\nI do not think we were able to fully investigate the problem."]}, {"number": 19819, "title": "Force downgrade setuptools for tests after tf whl is installed.", "body": "", "comments": []}, {"number": 19818, "title": "Fixing the setuptools issue for pip builds.", "body": "", "comments": []}, {"number": 19817, "title": "Iteritems is deprecated in python 3. Using items instead.", "body": "", "comments": ["Thanks for fixing Amit. "]}, {"number": 19816, "title": "Branch 199474340", "body": "", "comments": ["Timeout, retrying (`dataset_optimize_op_test`)."]}, {"number": 19815, "title": "Update __init__.py", "body": "Whitelist the operators module in the main library.", "comments": []}, {"number": 19814, "title": "How to make statistics script using summary?", "body": "Hi, all\r\n\r\nI want parameter distribution analysis script for pretrained models.\r\nI do not want special script for each model, just want single program to do it.\r\n\r\nSome person advised me to use the summary graph.\r\ntensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc\r\n\r\nI check the code and fell that the code does not support extracting parameters from pb file.\r\n\r\nI wrote a draft code to analyze;\r\nhttps://github.com/ElectronNest/dist_nn/blob/master/testloads_nn.py\r\n\r\nAny suggestion is welcome, and I am beginner, please explain softly.\r\n\r\nBest,\r\nSyouyu\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "N/A or how to make statics script ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19813, "title": "Documentation code for ScipyOptimizerInterface crashes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have copied code from the documentation.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux zenon 4.15.0-22-generic #24-Ubuntu SMP Wed May 16 12:15:17 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"18.04 LTS (Bionic Beaver)\"\r\nVERSION_ID=\"18.04\"\r\nVERSION_CODENAME=bionic\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary using pip\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n- **Python version**: \r\n3.6\r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0 / 7.1\r\n- **GPU model and memory**:\r\n```\r\n    +-----------------------------------------------------------------------------+\r\n    | NVIDIA-SMI 390.59                 Driver Version: 390.59                    |\r\n    |-------------------------------+----------------------+----------------------+\r\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n    |===============================+======================+======================|\r\n    |   0  GeForce GTX 965M    Off  | 00000000:01:00.0  On |                  N/A |\r\n    | N/A   43C    P5     8W /  N/A |    379MiB /  2002MiB |      0%      Default |\r\n    +-------------------------------+----------------------+----------------------+\r\n```\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nExample code from the documentation of `tf.contrib.opt.ScipyOptimizerInterface` does not work. Was tried with tensorflow 1.8.0 and 1.7.1 on CPU and GPU.\r\n\r\nThe code crashed with an exception.\r\n\r\n### Source code / logs\r\n\r\nThe code is mainly copied from [the documentation of ScipyOptimizerInterface](https://www.tensorflow.org/api_docs/python/tf/contrib/opt/ScipyOptimizerInterface), pasted into a file with additionnal imports.\r\n\r\ntest.py file:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.opt import ScipyOptimizerInterface\r\n\r\nvector = tf.Variable([7., 7.], 'vector')\r\n\r\n# Make vector norm as small as possible.\r\nloss = tf.reduce_sum(tf.square(vector))\r\n\r\noptimizer = ScipyOptimizerInterface(loss, options={'maxiter': 100})\r\n\r\nwith tf.Session() as session:\r\n  optimizer.minimize(session)\r\n```\r\nexecuted by:\r\n\r\n```bash\r\n$ python test.py\r\n```\r\n\r\nWith the following output:\r\n\r\n```\r\n2018-06-06 17:34:36.869162: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable\r\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 12, in <module>\r\n    optimizer.minimize(session)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/contrib/opt/python/training/external_optimizer.py\", line 195, in minimize\r\n    initial_packed_var_val = session.run(self._packed_var)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable\r\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable)]]\r\n\r\nCaused by op 'Variable/read', defined at:\r\n  File \"./test.py\", line 4, in <module>\r\n    vector = tf.Variable([7., 7.], 'vector')\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\r\n    constraint=constraint)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 397, in _init_from_args\r\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 142, in identity\r\n    return gen_array_ops.identity(input, name=name)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3187, in identity\r\n    \"Identity\", input=input, name=name)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/mnt/data/amignon/Projets/mypython3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\r\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Variable)]]\r\n```\r\n", "comments": ["The problem is that you have not ran the initialization operation before doing the optimization.\r\nIf you add `session.run(tf.global_variables_initializer())` before calling `optimizer.minimize(session)` the code will work fine.", "Did that solution work?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing for now due to lack of activity."]}, {"number": 19812, "title": "[feature request] Improve multinomial sampling efficiency", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'v1.8.0-0-g93bc2e2072'\r\n- **Python version**:  3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**:  Quadro M1200\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI need to perform several samplings from a multinomial distribution. The problem is that it is very slow. After debugging, using a **Profiler**, I've realized that it comes from this piece of code:\r\n\r\n`math_ops.reduce_sum(array_ops.one_hot(x, depth=k), axis=-2)` (line 257 at this moment)\r\n\r\nfrom [this python file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/distributions/multinomial.py)\r\n\r\nIndeed, the code actually allocate a matrix for each row we want to sample. So it allocate too much memory for no actual reason! Here is my [stackoverflow post](https://stackoverflow.com/questions/50704004/tensorflow-efficient-multinomial-sampling-theano-x50-faster/50723793#50723793) for more detail about the problem\r\n\r\n### Source code / logs\r\nActually Theano's implementation run x25 faster as described in my stackoverflow post. Also my (not so generic and not so fast solution) runs x3 times faster than the native TensorFlow sample function. Here is a snippet for comparison:\r\n\r\nUsing native implementation:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow.contrib.distributions as ds\r\nimport time\r\n\r\ntf.reset_default_graph()\r\n\r\nnb_distribution = 100 # number of probabilities distribution\r\n\r\nu = np.random.randint(2000, 3500, size=nb_distribution) # define number of counts (vector of size 100 with int in 2000, 3500)\r\n\r\n# probsn is a matrix of probability:\r\n# each row of probsn contains a vector of size 30 that sums to 1\r\nprobsn = np.random.uniform(size=(nb_distribution, 30))\r\nprobsn /= np.sum(probsn, axis=1)[:, None]\r\n\r\ncounts = tf.Variable(u, dtype=tf.float32)\r\nprobs = tf.Variable(tf.convert_to_tensor(probsn.astype(np.float32)))\r\n\r\n# sample from the multinomial\r\ndist = ds.Multinomial(total_count=counts, probs=probs)\r\nout = dist.sample()\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    res = sess.run(out) # if remove this line the code is slower...\r\n    start = time.time()\r\n    res = sess.run(out)\r\n    print(time.time() - start)\r\n    print(np.all(u == np.sum(res, axis=1)))\r\n```\r\n\r\nOn my computer it runs in **0.05** seconds\r\n\r\nAnd here is my own (not generic) implementation of multinomial sampling that uses\r\n`tf.scatter_nd()` function:\r\n\r\n\r\nUsing my own multinomial sampling:\r\n```python\r\ndef vmultinomial_sampling(counts, pvals, seed=None):\r\n    k = tf.shape(pvals)[1]\r\n    logits = tf.expand_dims(tf.log(pvals), 1)\r\n\r\n    def sample_single(args):\r\n        logits_, n_draw_ = args[0], args[1]\r\n        x = tf.multinomial(logits_, n_draw_, seed)\r\n        indices = tf.cast(tf.reshape(x, [-1,1]), tf.int32)\r\n        updates = tf.ones(n_draw_) # tf.shape(indices)[0]\r\n        return tf.scatter_nd(indices, updates, [k])\r\n\r\n    x = tf.map_fn(sample_single, [logits, counts], dtype=tf.float32)\r\n\r\n    return x\r\n\r\nxx = vmultinomial_sampling(u, probsn)\r\n# check = tf.expand_dims(counts, 1) * probs\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    res = sess.run(xx) # if remove this line the code is slower...\r\n    start_t = time.time()\r\n    res = sess.run(xx)\r\n    print(time.time() -start_t)\r\n    #print(np.sum(res, axis=1))\r\n    print(np.all(u == np.sum(res, axis=1)))\r\n```\r\n\r\nOn my computer the code took **0.016** seconds to execute.\r\n\r\nFor comparison Theano's implementation takes **0.0025** seconds...\r\n\r\n\r\nIndeed, my implementation doesn't take advantage of parallelization (changing the `parallel_iterations` in `map_fn` doesn't change anything. I'm using my GPU) while it totally makes sense that using parallel computations will improve the code because sampling from a specific row is independent from sampling from any other rows...\r\n\r\n\r\nDo you think you can improve the code in order to avoid useless memory usage and allow parallel samplings?\r\n\r\nThank you.", "comments": ["@jvdillon - thoughts on the speed of sampling described by @Twice22 here?", "Hi!\r\n  This can certainly be sped up to be much faster. The main issue is that the multinomial sampler uses a categorical sampler (unfortunately named tf.multinomial) underneath. We can actually construct an efficient binomial sampler, using a mix of inversion methods and Hormann's: https://www.tandfonline.com/doi/abs/10.1080/00949659308811496. From there, one can sample from a multinomial distribution through repeated binomial random variates.", "Hey.\r\n\r\nI didn't look too much into the paper you mentioned, but it seems that, if I'm not mistaken, that Theano implementation is based on the \"naive\" idea. See [perform](https://github.com/Theano/Theano/blob/master/theano/sandbox/multinomial.py) function at line 155. I didn't take the time to look at the GPU implementation [here](https://github.com/Theano/Theano/blob/master/theano/gpuarray/multinomial.py).\r\n\r\nHere is some benchmarks I did using numpy, C++ and I've also tried CUDA but didn't manage to fully implement it:\r\n\r\nAll the Benchmarks are more or less based on the same set-up (the random numbers associated to the \r\ncounts are sampled in **[2000, 3500]**. For this reason the numbers to sample might vary from one experiment to another one but on average the time remains the same).\r\n\r\nMy CPU: Intel(R) Xeon(R) E3-1505M v6 @ 3.00Ghz\r\nMy GPU (Cuda compatible): Quadro M1200, Driver: 390.65\r\n\r\n- Theano: 0.0025s\r\n- TensorFlow [actual code]: 0.045s\r\n- TensorFlow [my version]: 0.015s\r\n- Numpy code: 0.62s\r\n- C++ [Compile with Ox]: 0.25s\r\n- CUDA  [couldn't make it work]\r\n\r\n**Numpy version:**\r\n\r\n```python\r\nimport numpy as np\r\nimport time\r\n\r\ndef vmultinomial(counts, pvals):\r\n    N, K = pvals.shape\r\n    Z = np.zeros_like(pvals, dtype='int32')\r\n    cumsum = np.cumsum(pvals, axis=1, dtype='float64')\r\n    \r\n    for n in range(N):\r\n        size_ = counts[n]\r\n        rnd_numbers = np.random.uniform(size=size_)\r\n        for i in range(counts[n]):\r\n            Z[n, np.searchsorted(cumsum[n], rnd_numbers[i])] += 1\r\n    \r\n    return Z\r\n\r\nnb_distribution = 100\r\n\r\nu = np.random.randint(2000, 3500, size=nb_distribution)\r\nprobsn = np.random.uniform(size=(nb_distribution, 30))\r\nprobsn /= np.sum(probsn, axis=1)[:, None]\r\n\r\nstart_t = time.time()\r\nres = vmultinomial(u, probsn)\r\nprint(time.time() - start_t)\r\n```\r\n\r\n**C++ code: **\r\n\r\n```cpp\r\n#include \"stdafx.h\"\r\n#include <iostream>\r\n#include <Eigen/Dense>\r\n#include <algorithm>\r\n#include <iostream>\r\n#include <random>\r\n#include <vector>\r\n#include <chrono>\r\n\r\nusing namespace std;\r\nusing Eigen::MatrixXd;\r\nusing namespace std::chrono;\r\n\r\n\r\nstatic vector<int> random_uniform(size_t size, int inf, int sup) {\r\n\tstatic uniform_int_distribution<int> distribution(inf, sup);\r\n\tstatic default_random_engine generator;\r\n\r\n\t// allocate a vector of size size\r\n\tvector<int> data(size);\r\n\tgenerate(data.begin(), data.end(), []() { return distribution(generator); });\r\n\r\n\treturn data;\r\n}\r\n\r\nstatic vector<double> double_random_uniform(size_t size) {\r\n\tstatic uniform_real_distribution<double> distribution(0.0, 1.0);\r\n\tstatic default_random_engine generator;\r\n\r\n\t// allocate a vector of size size\r\n\tvector<double> data(size);\r\n\tgenerate(data.begin(), data.end(), []() { return distribution(generator); });\r\n\r\n\treturn data;\r\n}\r\n\r\nMatrixXd probs_mat(size_t rows, size_t cols) {\r\n\tMatrixXd m = (MatrixXd::Random(rows, cols) + MatrixXd::Constant(rows, cols, 1.)) / 2;\r\n\tMatrixXd m2 = m.rowwise().sum();\r\n\r\n\tfor (int i = 0; i < rows; i++) {\r\n\t\tfor (int j = 0; j < cols; j++) {\r\n\t\t\tm(i, j) /= m2(i);\r\n\t\t}\r\n\t}\r\n\t\r\n\treturn m;\r\n}\r\n\r\nMatrixXd cumsum(MatrixXd pvals) {\r\n\tint N = pvals.rows(), K = pvals.cols();\r\n\tMatrixXd Z(N, K);\r\n\r\n\t#pragma omp parallel for\r\n\tfor (int i = 0; i < N; i++) {\r\n\t\tdouble sum = 0;\r\n\t\tfor (int j = 0; j < K; j++) {\r\n\t\t\tsum += pvals(i, j);\r\n\t\t\tZ(i, j) = sum;\r\n\t\t}\r\n\t}\r\n\r\n\treturn Z;\r\n}\r\n\r\nint searchsorted(MatrixXd csum, double rnd) {\r\n\tint beg = 0, end = csum.size();\r\n\r\n\twhile (beg <= end) {\r\n\t\tint mid = (beg + end) / 2;\r\n\t\tif (csum(mid) < rnd) {\r\n\t\t\tbeg = mid + 1;\r\n\t\t}\r\n\t\telse {\r\n\t\t\tend = mid - 1;\r\n\t\t}\r\n\t}\r\n\treturn beg;\r\n}\r\n\r\n\r\nMatrixXd multinomial(vector<int> counts, MatrixXd probs) {\r\n\tint N = probs.rows(), K = probs.cols();\r\n\tMatrixXd Z = MatrixXd::Zero(N, K);\r\n\r\n\tMatrixXd csum = cumsum(probs);\r\n\t\r\n#pragma omp parallel for\r\n\tfor (int i = 0; i < N; i++) {\r\n\t\tint length = counts[i];\r\n\t\tvector<double> rnd_numbers = double_random_uniform(length);\r\n\r\n\t\tfor (int j = 0; j < length; j++) {\r\n\t\t\tZ(i, searchsorted(csum.row(i), rnd_numbers[j])) += 1;\r\n\t\t}\r\n\t}\r\n\treturn Z;\r\n}\r\n\r\nint main()\r\n{\r\n\tint nb_samples = 100, pvals = 30;\r\n\tint low = 2000, high = 3500;\r\n\tvector<int> counts = random_uniform(nb_samples, low, high);\r\n\tMatrixXd probs = probs_mat(nb_samples, pvals);\r\n\t\r\n\r\n\thigh_resolution_clock::time_point t1 = high_resolution_clock::now();\r\n\tMatrixXd csum = multinomial(counts, probs);\r\n\thigh_resolution_clock::time_point t2 = high_resolution_clock::now();\r\n\r\n\tauto duration = duration_cast<microseconds>(t2 - t1).count();\r\n\r\n\tcout << duration << endl;\r\n\tsystem(\"pause\");\r\n}\r\n```\r\n\r\n**CUDA attempt:**\r\n```cuda\r\n#include <iostream>\r\n#include <math.h>\r\n#include \"cuda_runtime.h\"\r\n#include \"device_launch_parameters.h\"\r\n#include <stdio.h>\r\n#include <ctime>\r\n\r\n#include <thrust/device_vector.h>\r\n#include <thrust/transform.h>\r\n#include <thrust/iterator/counting_iterator.h>\r\n#include <thrust/random.h>\r\n\r\n#include <cublas_v2.h>\r\n#include <curand.h>\r\n#include <thrust/functional.h>\r\n#include <thrust/iterator/counting_iterator.h>\r\n#include <thrust/iterator/constant_iterator.h>\r\n\r\nusing namespace std;\r\nusing namespace thrust;\r\n\r\nstruct prg\r\n{\r\n\tint a, b;\r\n\r\n\t__host__ __device__\r\n\t\tprg(int _a = 0, int _b = 1) : a(_a), b(_b) {};\r\n\r\n\t__host__ __device__\r\n\t\tint operator()(const unsigned int n) const\r\n\t{\r\n\t\tdefault_random_engine rng;\r\n\t\tuniform_int_distribution<int> dist(a, b);\r\n\t\trng.discard(n);\r\n\r\n\t\treturn dist(rng);\r\n\t}\r\n};\r\n\r\nstruct float_prg\r\n{\r\n\tint a, b;\r\n\r\n\t__host__ __device__\r\n\t\tfloat_prg(float _a = 0.0, float _b = 1.0) : a(_a), b(_b) {};\r\n\r\n\t__host__ __device__\r\n\t\tfloat operator()(const unsigned int n) const\r\n\t{\r\n\t\tdefault_random_engine rng;\r\n\t\tuniform_real_distribution<float> dist(a, b);\r\n\t\trng.discard(n);\r\n\r\n\t\treturn dist(rng);\r\n\t}\r\n};\r\n\r\n\r\nstatic device_vector<int> random_uniform(size_t size, int inf, int sup) {\r\n\r\n\t// allocate a vector of size size\r\n\tdevice_vector<int> data(size);\r\n\tcounting_iterator<unsigned int> index_sequence_begin(0);\r\n\r\n\ttransform(index_sequence_begin, index_sequence_begin + size, data.begin(), prg(inf, sup));\r\n\r\n\treturn data;\r\n}\r\n\r\n__host__ __device__\r\nstatic device_vector<float> double_random_uniform(size_t size) {\r\n\t// allocate a vector of size size\r\n\tdevice_vector<float> data(size);\r\n\tcounting_iterator<unsigned int> index_sequence_begin(0);\r\n\r\n\ttransform(index_sequence_begin, index_sequence_begin + size, data.begin(), float_prg(0.0, 1.0));\r\n\r\n\treturn data;\r\n}\r\n\r\n// don't care if this function doesn't use CUDA, I don't eval\r\n// its performance\r\nvoid normalize_l1(device_vector<float> &A, int rows, int cols) {\r\n\tfloat *row_sum = (float *)malloc(rows * sizeof(float));\r\n\r\n\tfor (int i = 0; i < rows; ++i) {\r\n\t\tfloat sum = 0.0f;\r\n\t\tfor (int j = 0; j < cols; ++j) {\r\n\t\t\tsum += A[cols*i + j];\r\n\t\t}\r\n\t\trow_sum[i] = sum;\r\n\t}\r\n\r\n\tfor (int r = 0; r < rows; ++r) {\r\n\t\tfor (int c = 0; c < cols; ++c) {\r\n\t\t\tA[r*cols + c] /= row_sum[r];\r\n\t\t}\r\n\t}\r\n}\r\n\r\nvoid GPU_fill_rand(device_vector<float> &A, int nr_rows_A, int nr_cols_A) {\r\n\t// Create a pseudo-random number generator\r\n\tcurandGenerator_t prng;\r\n\tcurandCreateGenerator(&prng, CURAND_RNG_PSEUDO_DEFAULT);\r\n\r\n\t// Set the seed for the random number generator using the system clock\r\n\tcurandSetPseudoRandomGeneratorSeed(prng, (unsigned long long) clock());\r\n\r\n\t// Fill the array with random numbers on the device\r\n\tcurandGenerateUniform(prng, raw_pointer_cast(&A[0]), nr_rows_A * nr_cols_A);\r\n\r\n\t// normalize the matrix\r\n\tnormalize_l1(A, nr_rows_A, nr_cols_A);\r\n\r\n}\r\n\r\ntemplate<class T>\r\nvoid print_matrix(const device_vector<T> &A, int nr_rows_A, int nr_cols_A) {\r\n\r\n\tfor (int i = 0; i < nr_rows_A; ++i) {\r\n\t\tfor (int j = 0; j < nr_cols_A; ++j) {\r\n\t\t\tcout << A[i * nr_cols_A + j] << \" \";\r\n\t\t}\r\n\t\tcout << endl;\r\n\t}\r\n\tcout << endl;\r\n}\r\n\r\n__device__\r\nint searchsorted(float* csum, int beg, int end, double rnd) {\r\n\r\n\twhile (beg <= end) {\r\n\t\tint mid = (beg + end) / 2;\r\n\t\tif (csum[mid] < rnd) {\r\n\t\t\tbeg = mid + 1;\r\n\t\t}\r\n\t\telse {\r\n\t\t\tend = mid - 1;\r\n\t\t}\r\n\t}\r\n\treturn beg;\r\n}\r\n\r\n// compute the cumulative sum over the row of the input matrix Z\r\nvoid cumsum(device_vector<float> &Z, int N, int K) {\r\n\r\n\tdevice_vector<float> cumProb(N*K, 0.);\r\n\tdevice_vector<float> sub(N*K);\r\n\r\n\tinclusive_scan(Z.begin(), Z.end(), Z.begin());\r\n\r\n\ttransform(make_counting_iterator(0), make_counting_iterator(N*K), make_constant_iterator(K), sub.begin(), thrust::divides<int>());\r\n\r\n\ttransform(Z.begin(), Z.end(), sub.begin(), Z.begin(), thrust::minus<float>());\r\n}\r\n\r\n// Try to transform kernel into global function. Failed\r\n__global__ void kernel(int* Z, int* counts, float* ptr_probs, int N, int K) {\r\n\tint idx = threadIdx.x + blockIdx.x * blockDim.x;\r\n\tint stride = blockDim.x * gridDim.x;\r\n\r\n\tfor (int i = idx; i < N; i += stride) {\r\n\t\tint length = counts[i];\r\n\t\tdevice_vector<float> rnd_numbers = double_random_uniform(length);\r\n\t\tfloat* ptr_rnd_numbers = raw_pointer_cast(rnd_numbers.data());\r\n\r\n\t\tfor (int j = 0; j < length; j++) {\r\n\t\t\tZ[searchsorted(ptr_probs, i*K, (i + 1)*K, ptr_rnd_numbers[j])] += 1;\r\n\t\t}\r\n\t}\r\n}\r\n\r\ndevice_vector<int> multinomial(device_vector<int> &counts, device_vector<float> probs, int N, int K) {\r\n\t// probs -> cumulative sum over rows\r\n\tcumsum(probs, N, K);\r\n\tdevice_vector<int> Z(N*K, 0);\r\n\r\n\tint* ptr_counts = raw_pointer_cast(counts.data());\r\n\tfloat* ptr_probs = raw_pointer_cast(probs.data());\r\n\r\n\tkernel<<< 32, 64 >>>(raw_pointer_cast(Z.data()), ptr_counts, ptr_probs, N, K);\r\n\r\n\treturn Z;\r\n}\r\n\r\nint main(void)\r\n{\r\n\r\n\tint nb_samples = 3, pvals = 4;\r\n\tint low = 2000, high = 3500;\r\n\t\r\n\tdevice_vector<int> counts = random_uniform(nb_samples, low, high);\r\n\r\n\t// Allocate 1 matrix on GPU\r\n\tdevice_vector<float> d_probs(nb_samples * pvals);\r\n\r\n\t// fill d_probs with random number s.t sum(rows) = 1\r\n\tGPU_fill_rand(d_probs, nb_samples, pvals);\r\n\r\n\tdevice_vector<int> res = multinomial(counts, d_probs, nb_samples, pvals);\r\n\r\n\tprint_matrix(res, nb_samples, pvals);\r\n\r\n\treturn 0;\r\n}\r\n```\r\n\r\nThe idea for the CUDA implementation would be to use multiple threads to parallelize the double loop in the multinomial function. I think that should speed up the process quite a bit? I think having a x100 speed-up as compare to the simple CPU C++ implementation is reachable ?\r\n", "Closing this issue.\r\n\r\nTF distributions are deprecated / removed. We've had a similar issue in TensorFlow Probability for multinomial. The implementation there is based on a map_fn as well. We can still do better by using the random_binomial sampler in TF (which is rejection based, and will probably work much better in the limit of large counts). Feel free to reach out at https://github.com/tensorflow/probability."]}, {"number": 19811, "title": "Change Number of Classes in MNIST Tutorial", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0-rc1\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nHello, I'm trying to use the code in the [MNIST tutorial](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/tutorials/layers/cnn_mnist.py) with my own dataset. The only thing I modified from the code was the data fed into the Estimator. The code runs well, but the issue is when I try to change the number of classes. The MNIST has 10 classes in total, while my dataset only has 3. For that, I change this specific line:\r\n\r\n`logits = tf.layers.dense(inputs=dropout, units=10)`\r\n\r\nto this:\r\n\r\n`logits = tf.layers.dense(inputs=dropout, units=3`\r\n\r\nThen it gives me the error:\r\n\r\n> InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1024,3] rhs shape= [1024,10]\r\n\t [[Node: save/Assign_7 = Assign[T=DT_FLOAT, _class=[\"loc:@dense_1/kernel\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_1/kernel, save/RestoreV2_7)]]\r\n\r\nI cannot, for the life of me, figure out which tensor creates the shape [1024, 10], other than that line I specified.\r\n\r\nI tried searching in the Issues section for similar case, but couldn't find any. Forgive me if there is in fact the same issue posted here.", "comments": ["Nagging Assignee @tatatodd: It has been 106 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19810, "title": "[Request] Implement Stochastic Quasi-Newton optimizer(s)", "body": "Would be nice to have optimizers based on stochastic quasi-Newton methods. In particular, the SQN algorithm described in _Byrd, Richard H., et al. \"A stochastic quasi-Newton method for large-scale optimization.\" SIAM Journal on Optimization 26.2 (2016): 1008-1031._ seems very promising.\r\n\r\nHere is a reference repository with MATLAB implementations of such algorithms:\r\nhttps://github.com/keskarnitish/minSQN\r\n\r\nAnd this is the paper describing this particular method:\r\nhttps://arxiv.org/pdf/1401.7020\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19809, "title": "Illegal Instruction Error when Importing TensorFlow Module", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary (native pip and Anaconda), also attempted compiling source and received same error\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: Tried 2.7 and 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.14.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **GPU model and memory**: CPU install only with 4GB memory\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\n```\r\n\r\n### Describe the problem\r\n\r\nWhen attempting to import the TensorFlow module into python the error `illegal instruction (core dumped)` is thrown.\r\n\r\nI am installing TensorFlow as part of university research on a home lab rack server (HP Proliant DL380 G7 with Intel Xeon 5675 processors) with a Ubuntu 18.04 VM on a Hyper V host. When I complete the installation instructions and attempt to validate the instruction, I receive the above error message.\r\n\r\nI've attempted in a local workstation virtual environment with VMware, and it worked fine (albeit with instruction warnings that did not impede expected results).\r\n\r\n### Troubleshooting steps taken\r\n\r\n* Reviewed StackOverflow community, wasn't able to find a resolution\r\n  * We are required to use r1.8 so we're not able to downgrade to r1.5 at this time.\r\n* Attempted both binary and compiled installation.\r\n* Attempted installation via Anaconda and native pip for Python 2.7.n and Pyhon 3.6.5\r\n* Tried a separate VM (using a local workstation).", "comments": ["If it helps, while I was trying to troubleshoot, I ran the validation script below through python3.6-dbg:\r\n\r\n```python\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n```\r\n\r\nDuring execution I saw the same few errors thrown a couple of times :\r\n\r\n```\r\nError in sys.excepthook:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/apport_python_hook.py\", line 63, in apport_excepthook\r\n    from apport.fileutils import likely_packaged, get_recent_crashes\r\n  File \"/usr/lib/python3/dist-packages/apport/__init__.py\", line 5, in <module>\r\n    from apport.report import Report\r\n  File \"/usr/lib/python3/dist-packages/apport/report.py\", line 30, in <module>\r\n    import apport.fileutils\r\n  File \"/usr/lib/python3/dist-packages/apport/fileutils.py\", line 23, in <module>\r\n    from apport.packaging_impl import impl as packaging\r\n  File \"/usr/lib/python3/dist-packages/apport/packaging_impl.py\", line 24, in <module>\r\n    import apt\r\n  File \"/usr/lib/python3/dist-packages/apt/__init__.py\", line 23, in <module>\r\n    import apt_pkg\r\nModuleNotFoundError: No module named 'apt_pkg'\r\n\r\nOriginal exception was:\r\nTraceback (most recent call last):\r\n  File \"~/.local/lib/python3.6/site-packages/numpy/core/__init__.py\", line 16, in <module>\r\n    from . import multiarray\r\nImportError: cannot import name 'multiarray'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf-fail.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"~/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 47, in <module>\r\n    import numpy as np\r\n  File \"~/.local/lib/python3.6/site-packages/numpy/__init__.py\", line 142, in <module>\r\n    from . import add_newdocs\r\n  File \"~/.local/lib/python3.6/site-packages/numpy/add_newdocs.py\", line 13, in <module>\r\n    from numpy.lib import add_newdoc\r\n  File \"~/.local/lib/python3.6/site-packages/numpy/lib/__init__.py\", line 8, in <module>\r\n    from .type_check import *\r\n  File \"~/.local/lib/python3.6/site-packages/numpy/lib/type_check.py\", line 11, in <module>\r\n    import numpy.core.numeric as _nx\r\n  File \"~/.local/lib/python3.6/site-packages/numpy/core/__init__.py\", line 26, in <module>\r\n    raise ImportError(msg)\r\nImportError:\r\nImporting the multiarray numpy extension module failed.  Most\r\nlikely you are trying to import a failed build of numpy.\r\nIf you're working with a numpy git repo, try `git clean -xdf` (removes all\r\nfiles not under version control).  Otherwise reinstall numpy.\r\n\r\nOriginal error was: cannot import name 'multiarray'\r\n```\r\n\r\nI've uninstalled and reinstalled numpy a couple of times on my machine and went through numpy's basic troubleshooting steps to verify that the expected version is being pulled.\r\n\r\nAre there any known version limits with numpy? I'm assuming not, but I want to check. I'm using v1.14.3", "I figured it out, it's my processor. From what I recall, TensorFlow started compiling versions after r1.5 to use AVX support. However, my server's processor (Intel X5675) doesn't have AVX support.\r\n\r\nI was looking to perform some upgrades to my server anyway, guess the processors need to be first for this project. I'll go ahead with closing this out as this is a hardware issue, and not software.", "I got the same problem.\r\n\r\n`import tensorflow as ts`\r\n\r\nit returns `illegal instruction`.\r\n\r\n```\r\ntensorflow 2.1.0\r\ncuda 10.2\r\ngcc 7.4\r\nubuntu 1804\r\npython 3.6.9\r\n```"]}, {"number": 19808, "title": "[INTEL MKL] [Do not merge, WIP] Enable compilation of TF without MKL ML dependency ", "body": "Added ifdef INTEL_MKL_ML around code that should only be included when compiling for MKL ML. Also added define DO_NOT_USE_ML to control if MKL binary will linked if INTEL_MKL is enabled.", "comments": ["@agramesh1 Thank you for the PR! I'll merge it via internal CL process. You'll see this PR as closed once the merge goes through."]}, {"number": 19807, "title": "tf.as_string: not support tf.string", "body": "In tf.as_string, integers or boolean are mostly supported (int8, int32, int64,bool) but not tf.string. Why? Will it be added in the feature?", "comments": []}, {"number": 19806, "title": "<fcntl.h>", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n\r\n\r\n### Describe the problem\r\ntensorflow/contrib/lite/allocation.cc:16:19: fatal error: fcntl.h: No such file or directory\r\n #include <fcntl.h>\r\n                   ^\r\ncompilation terminated.\r\nIn file included from tensorflow/contrib/lite/arena_planner.cc:15:0:\r\n./tensorflow/contrib/lite/arena_planner.h:18:18: fatal error: memory: No such file or directory\r\n #include <memory>\r\n                  ^\r\ncompilation terminated.\r\nIn file included from ./tensorflow/contrib/lite/context.h:33:0,\r\n                 from tensorflow/contrib/lite/context.c:16:\r\n/Users/chenjiao04/Documents/android-ndk-r14b/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/include/stdint.h:9:26: fatal error: stdint.h: No such file or directory\r\n # include_next <stdint.h>\r\n                          ^\r\ncompilation terminated.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Ok\r\nI use the command:\r\n./tensorflow/contrib/lite/download_dependencies.sh\r\n./tensorflow/contrib/lite/download_dependencies.sh\r\nthe problem is occured\r\nI install automake, libtool, bazel(the new version)\r\nand Androidsdk(API27, API26, API25, API21),  AndroidNDK(r14), Android studio", "Nagging Assignees @miaout17, @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @miaout17, @petewarden: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still a problem @chenjiaoAngel . Can you paste the command you are using to build.", "@chenjiaoAngel : Please reopen this issue, if you are still having this problem."]}, {"number": 19805, "title": "Fix failing tests.", "body": "Whitelist the operators module in the main library.", "comments": ["cc @mdanatg ", "ah - you are the author @mdanatg how could i have missed that :)", "np :)"]}, {"number": 19804, "title": "Update backprop.py", "body": "fixed this bug:\r\nthe gradient function returned by tfe.implicit_value_and_gradients() doesn't support keyword argument\r\n\r\nthe issue is at here:\r\nhttps://github.com/tensorflow/tensorflow/issues/19718", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it! @googlebot ", "CLAs look good, thanks!\n\n<!-- ok -->", "Failing on unrelated error, I think (FYI @tatatodd), retrying.\r\n```\r\notalMemory: 11.17GiB freeMemory: 11.00GiB\r\n2018-06-06 15:26:53.146639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-06-06 15:26:53.476084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-06 15:26:53.476134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0\r\n2018-06-06 15:26:53.476156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N\r\n2018-06-06 15:26:53.476479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10665 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:07.0, compute capability: 3.7)\r\n2018-06-06 15:26:54.148206: I tensorflow/compiler/xla/service/service.cc:178] XLA service 0xf4f4440 executing computations on platform CUDA. Devices:\r\n2018-06-06 15:26:54.148235: I tensorflow/compiler/xla/service/service.cc:186]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\r\n2018-06-06 15:26:54.882302: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x69fd0b)[0x7f05daf00d0b]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x10330)[0x7f063e855330]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x5d283a9)[0x7f05e15be3a9]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x5d29c83)[0x7f05e15bfc83]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x5d2a928)[0x7f05e15c0928]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x5d2aaa8)[0x7f05e15c0aa8]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x5e3c2dd)[0x7f05e16d22dd]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x5d20f9b)[0x7f05e15b6f9b]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x5d221a4)[0x7f05e15b81a4]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(+0x595e5eb)[0x7f05e11f45eb]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla12LocalService17CompileExecutableERKNS_14XlaComputationEN10tensorflow3gtl10ArraySliceIPKNS_5ShapeEEERKNS_22ExecutableBuildOptionsE+0xcad)[0x7f05e01b231d]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN3xla11LocalClient7CompileERKNS_14XlaComputationEN10tensorflow3gtl10ArraySliceIPKNS_5ShapeEEERKNS_22ExecutableBuildOptionsE+0x20e)[0x7f05e01a947e]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow19XlaCompilationCache15BuildExecutableERKNS_11XlaCompiler7OptionsERKNS1_17CompilationResultEPSt10unique_ptrIN3xla15LocalExecutableESt14default_deleteISA_EE+0x142)[0x7f05e00c0642]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow19XlaCompilationCache11CompileImplERKNS_11XlaCompiler7OptionsERKNS_12NameAttrListERKSt3mapIiNS_6TensorESt4lessIiESaISt4pairIKiS9_EEERKS8_IiNS_14OptionalTensorESB_SaISC_ISD_SJ_EEEPNS_15OpKernelContextEPPKNS1_17CompilationResultEPPN3xla15LocalExecutableEPKNS1_14CompileOptionsEb+0x2090)[0x7f05e00c31c0]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow19XlaCompilationCache7CompileERKNS_11XlaCompiler7OptionsERKNS_12NameAttrListERKSt3mapIiNS_6TensorESt4lessIiESaISt4pairIKiS9_EEERKS8_IiNS_14OptionalTensorESB_SaISC_ISD_SJ_EEEPNS_15OpKernelContextEPPKNS1_17CompilationResultEPPN3xla15LocalExecutableEPKNS1_14CompileOptionsE+0x1f)[0x7f05e00c401f]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow18XlaLocalLaunchBase7ComputeEPNS_15OpKernelContextE+0x57f)[0x7f05e000509f]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow9XlaDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0xa3)[0x7f05e000adb3]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x610845)[0x7f05dae71845]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(+0x610c0f)[0x7f05dae71c0f]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x218)[0x7f05daed46f8]\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7f05daed3462]\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x915b0)[0x7f05e9bf95b0]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x8184)[0x7f063e84d184]\r\n/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f063e57a03d]\r\n*** END MANGLED STACK TRACE ***\r\n*** Begin stack trace ***\r\n\ttensorflow::CurrentStackTrace()\r\n\txla::LocalService::CompileExecutable(xla::XlaComputation const&, tensorflow::gtl::ArraySlice<xla::Shape const*>, xla::ExecutableBuildOptions const&)\r\n\txla::LocalClient::Compile(xla::XlaComputation const&, tensorflow::gtl::ArraySlice<xla::Shape const*>, xla::ExecutableBuildOptions const&)\r\n\ttensorflow::XlaCompilationCache::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompiler::CompilationResult const&, std::unique_ptr<xla::LocalExecutable, std::default_delete<xla::LocalExecutable> >*)\r\n\ttensorflow::XlaCompilationCache::CompileImpl(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::map<int, tensorflow::Tensor, std::less<int>, std::allocator<std::pair<int const, tensorflow::Tensor> > > const&, std::map<int, tensorflow::OptionalTensor, std::less<int>, std::allocator<std::pair<int const, tensorflow::OptionalTensor> > > const&, tensorflow::OpKernelContext*, tensorflow::XlaCompiler::CompilationResult const**, xla::LocalExecutable**, tensorflow::XlaCompiler::CompileOptions const*, bool)\r\n\ttensorflow::XlaCompilationCache::Compile(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::map<int, tensorflow::Tensor, std::less<int>, std::allocator<std::pair<int const, tensorflow::Tensor> > > const&, std::map<int, tensorflow::OptionalTensor, std::less<int>, std::allocator<std::pair<int const, tensorflow::OptionalTensor> > > const&, tensorflow::OpKernelContext*, tensorflow::XlaCompiler::CompilationResult const**, xla::LocalExecutable**, tensorflow::XlaCompiler::CompileOptions const*)\r\n\ttensorflow::XlaLocalLaunchBase::Compute(tensorflow::OpKernelContext*)\r\n\ttensorflow::XlaDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*)\r\n\tEigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n\tstd::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n\tclone\r\n*** End stack trace ***\r\n/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/depthwise_conv_op_test_gpu.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 40: 24348 Aborted                 (core dumped) $@\r\n```\r\n"]}, {"number": 19803, "title": "[r1.9] Expose `tf.broadcast_to` op", "body": "This fix is a follow up on #19753 to expose `tf.broadcast_to` op to r1.9.\r\n\r\n**NOTE: Please note this PR is to r1.9, feel free to close the PR if not appropriate.**\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["/cc @martinwicke @Hoeze Not sure if this PR should be merged to r1.9 or not. Feel free to close the PR if not.", "@av8ramit ", "This is a new feature. As a policy, we do not accept features into release branches."]}, {"number": 19802, "title": "TensorFlow lite .pb turn .tflite An error", "body": "Error message  \r\n  \r\n\r\n``     F tensorflow/contrib/lite/toco/toco.cc:76] Check failed: has_input_file != has_savedmodel_dir (0 vs. 0)Specify either input_file or savedmodel_directory flag.`\r\n`\r\nfrozen_graph\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz\r\n\r\nConvert command\r\n\r\n```\r\n    bazel run toco  \\\r\n      input_file=/Users/dchealth/Desktop/mobilenet/frozen_graph.pb \\\r\n      output_file=/Users/dchealth/Desktop/mobilenet/frozen_graphnew.tflite \\\r\n      output_format=TFLITE \\\r\n      input_format=TENSORFLOW_GRAPHDEF \\\r\n      inference_type=FLOAT \\\r\n      input_type=FLOAT \\\r\n      input_shapes=1,128,128,3 \\\r\n      input_arrays=input \\\r\n      output_arrays=MobilenetV1/Predictions/Reshape_1\r\n\r\n```\r\n\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:mac os\r\n Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\nHave I written custom code\uff1aNO\r\nTensorFlow installed from:Install directly with the PIP command\r\nTensorFlow version\uff1a1.8.0\r\nBazel version:\r\nBuild label: 0.14.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jun 1 14:26:58 2018 (1527863218)\r\nBuild timestamp: 1527863218\r\nBuild timestamp as int: 1527863218\r\n\r\nCUDA/cuDNN version\uff1ano\r\nGPU model and memory:no\r\nExact command to reproduce\uff1ano", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Did you try to specify the directory or input file as requested?", "Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\nHave I written custom code\uff1aNO\r\nTensorFlow installed from:Install directly with the PIP command\r\nTensorFlow version\uff1a1.8.0\r\nBazel version:\r\nBuild label: 0.14.0-homebrew\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jun 1 14:26:58 2018 (1527863218)\r\nBuild timestamp: 1527863218\r\nBuild timestamp as int: 1527863218\r\nCUDA/cuDNN version\uff1ano\r\nGPU model and memory:no\r\nExact command to reproduce\uff1ano", "Both output_file and input_file have specified paths", "What's the reason", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@baihualinxin : You are not prefix parameters with '--' use --input_file=path instead of input_file=path.\r\n\r\nAlso prefer using tflite_convert: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#savedmodel\r\n\r\nPrefer using tflite_convert.\r\ntflite_convert \\\r\n      --graph_def_file=/Users/dchealth/Desktop/mobilenet/frozen_graph.pb  \\\r\n      --output_file=/Users/dchealth/Desktop/mobilenet/frozen_graphnew.tflite \\\r\n      --input_arrays=input \\\r\n      --output_arrays=MobilenetV1/Predictions/Reshape_1"]}, {"number": 19801, "title": "\"TypeError: Cannot interpret feed_dict key as Tensor: The name 'tf_new_X:0' refers to a Tensor which does not exist. The operation, 'tf_new_X', does not exist in the graph.\"This error I get while I was deploying my model on the server. Can anyone please help ?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n\r\n", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of activity. Please reopen and provide additional information if this is still an issue and you think it is a bug. Please direct questions to [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). \r\n\r\n"]}, {"number": 19800, "title": "Tensorflow 1.8 build failed while enabling MPI", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.5.1804 and RHEL 7.3 (both) \r\n\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.8.0-2735-g5d44932', '1.8.0')\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.14\r\n- **GCC/Compiler version (if compiling from source)**:  4.8.5 20150623\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: build from source instructions\r\n\r\n### Describe the problem\r\n\r\nTensorflow -1.8 build failed while enabling MPI in the configuration. Tensorflow is build using the normal build from source instructions.It was possible to enable MPI in building TF 1.7.\r\n\r\n### Source code / logs\r\n\r\nC++ compilation of rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so' failed (Exit 1)\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:76:18: error: expected type-specifier before 'se'\r\n using StatusOr = se::port::StatusOr<T>;\r\n\r\n\r\n", "comments": ["@jthestness Any ideas?", "@jlebar Looks like a bug introduced by this commit 80fc6618 probably causing a missing `#include` or Bazel dependency. Can you help?\r\n", "I don't have a setup to do these builds, sorry.  I'm happy to help you debug and prepare a PR, though.\r\n\r\nTry moving this line\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mpi_collectives/mpi_ops.cc#L77\r\n\r\ninto namespace tensorflow.  Or alternatively change `se::` to `stream_executor::`.", "\r\nChanging se:: to stream_executor:: worked.  moving line 77 to namespace tensorflow did not.\r\n", "Nagging Assignee @poxvoculi: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "https://github.com/tensorflow/tensorflow/pull/20147 fixes this, just waiting on them...", "Nagging Assignee @jlebar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Assigning to @qlzh727 because the PR just needs to be merged.", "Oh, I take it back, it has been merged.  This should be fixed, then."]}, {"number": 19799, "title": "Problems in switching to different datasets with iterator after restoring a pre-trained model for fine-tuning", "body": "Have I written custom code Yes\r\nOS Platform and Distribution ubuntu 16.04 LTS\r\nTensorFlow installed from pip3 install tensorflow-gpu\r\nTensorFlow version the latest version\r\nBazel version NA\r\nCUDA/cuDNN version CUDA Toolkit 9.0\r\nGPU model and memory GeForce GTX 1080\r\nExact command to reproduce NA\r\n\r\nI write a separate script to load the pre-trained model for fine-tuning with different datasets, then how do I get reference to the iterator and initialize it with fine_tune_dataset?\r\n\r\nThe following is a part of code in the script for training:\r\n\r\n```\r\ntrain_dataset = dataset_input_fn([\u2018/path/to/training_dataset\u2019])\r\n validate_dataset = dataset_input_fn([\u2018/path/to/validation_dataset\u2019])\r\n \r\n # create general iterator by the from_structure() method which needs the information of output data size/shape\r\n iterator = tf.data.Iterator.from_structure(train_dataset.output_types)\r\n data_batch, label_batch = iterator.get_next()\r\n\r\n# make datasets that we can initialize seperately. \r\n train_init_op = iterator.make_initializer(train_dataset)\r\n validate_init_op = iterator.make_initializer(validate_dataset)\r\n\r\naccuracy, loss, train_op = model_function(data_batch, label_batch)\r\n\r\nThe following is the part of code in a separate script for restoring pre-trained model and fine_tune:\r\n\r\nfine_tune_dataset = dataset_input_fn(['/path/to/fine_tune_model'])\r\nvalidate_dataset = dataset_input_fn(['/path/to/validate_model'])\r\n# make datasets that we can initialize seperately.\r\niterator = tf.data.Iterator.from_structure(fine_tune_dataset.output_types)\r\nfine_tune_init_op = iterator.make_initializer(fine_tune_dataset)\r\nvalidate_init_op = iterator.make_initializer(validate_dataset)\r\n```\r\nHowever, when I ran the script for fine-tuning, an error was given:\r\n\r\nFailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.\r\n\r\nI did call sess.run(fine_tune_init_op) in the fine-tuning part which did not show in the above. Could you please help me with how to switch to fine_tune_dataset after restoring pre-trained model in a different script? Thanks for your time in advance.\r\n\r\n", "comments": ["Nagging Assignee @angersson: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19798, "title": "tflite toco build failed in version of 3daa07aa2dde379388beb2a557a78bc5dd1b86ba", "body": "### System information\r\n- Have I written custom code : No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: 6.4\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: GeForce GTX 750\r\n- **Exact command to reproduce**: tensorflow code :\r\n commit 3daa07aa2dde379388beb2a557a78bc5dd1b86ba\r\nAuthor: Courtial Florian <floriancourtial@gmail.com>\r\nDate:   Wed Jun 6 01:02:07 2018 +0200\r\n    Add C++ no gradient for Floor operation. (#19662)\r\n\r\n### Describe the problem\r\nUse cmd below: \r\nbazel build --config android_arm --cxxopt=-std=c++11  //tensorflow/contrib/lite/toco:toco   --config monolithic\r\n\r\nMy WORKSPACE as below:\r\nWORKSPACE\r\n\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 22,\r\n    # Ensure that you have the build_tools_version below installed in the\r\n    # SDK manager as it updates periodically.\r\n    build_tools_version = \"27.0.3\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/home/hwh/Android/Sdk\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/home/hwh/Android/Ndk/android-ndk-r14b\",\r\n    # This needs to be 14 or higher to compile TensorFlow.\r\n    # Please specify API level >= 21 to build for 64-bit architecture\r\n    # otherwise the Android NDK will automatically select the latest\r\n    # API level it does support without notice.\r\n    # Note that the NDK version is not the API level.\r\n    api_level=14)\r\n\r\nbuild failed as below:\r\nINFO: Found 1 target...\r\nERROR: /home/hwh/.cache/bazel/_bazel_hwh/a5d93fc2a8e4cfbb4b042a3b705d2c38/external/gif_archive/BUILD.bazel:8:1: C++ compilation of rule '@gif_archive//:gif' failed (Exit 1)\r\nexternal/gif_archive/lib/openbsd-reallocarray.c:33:19: error: use of undeclared identifier 'SIZE_MAX'\r\n            nmemb > 0 && SIZE_MAX / nmemb < size) {\r\n                         ^\r\n1 error generated.\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 167.962s, Critical Path: 29.34s\r\nINFO: 374 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Now I can build toco by the command below:\r\nbazel build tensorflow/contrib/lite/toco:toco", "I don't believe that the `toco` binary is meant to be compiled with `--config=android_arm`, so I think this is working as intended.\r\n\r\nCC @suharshs @aselle ", "Yes, I found I needn't set to arm platform as you figured out this.\nAnd I had a mistake to used this opt of  --config=android_arm because the\ncomment below:\n\ncommit 320d8056af7799ab20e339757cf379963148425a\nAuthor: freedom\" Koan-Sin Tan <koansin.tan@gmail.com>\nDate:   Mon Jun 4 12:49:17 2018 +0800\n\n    make toco build for android (#17885)\n\n    * make toco build for android\n\n\n\n2018-06-07 17:35 GMT+08:00 Asim Shankar <notifications@github.com>:\n\n> I don't believe that the toco binary is meant to be compiled with\n> --config=android_arm, so I think this is working as intended.\n>\n> CC @suharshs <https://github.com/suharshs> @aselle\n> <https://github.com/aselle>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19798#issuecomment-395356230>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AcPuIUWZYz8vDXjpvzg8DHsjz1tc6syQks5t6PPWgaJpZM4UcRqK>\n> .\n>\n", "Great, so if I understand correctly, there is no issue now?", "Yes, No issue for me, thanks a lot.\n\n2018-06-09 0:41 GMT+08:00 Asim Shankar <notifications@github.com>:\n\n> Great, so if I understand correctly, there is no issue now?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19798#issuecomment-395817378>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AcPuIe9h8S8BwtxCWi4lDtkooGpjUzB0ks5t6qlVgaJpZM4UcRqK>\n> .\n>\n"]}, {"number": 19797, "title": "Use CUDA Runtime API instead of Driver API to allow interception using LD_PRELOAD", "body": "This pull request is to allow low-level MPI libraries like MVAPICH2-GDR that use LD_PRELOAD style interception of CUDA calls to optimize performance. Currently, a CUDA_INVALID_CONTEXT error appears if cuMemAlloc is used. The change to cudaMalloc will allow interception. There are no known side effects of this patch. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Just checking if this will be reviewed any time soon?", "I\u2019m reluctant to see this PR merged; I\u2019m not sure if TF uses CUDA Runtime API in tensorflow/core (is it linked at all?)\r\n\r\nIt seems the stream executor package is supposed to replace the proprietary CUDA Runtime API.", "@protoget any advice here?", "@byronyi - can you please explain the objection a bit more? \r\n\r\nBoth CUDA Runtime (cudaMalloc) and CUDA Driver (cuMemAlloc) APIs are proprietary, right? \r\n\r\nNVIDIA folks suggest applications to use the CUDA Runtime API and not the Driver API. Is there any particular need to use the CUDA Driver API in TensorFlow? ", "The reason is simple: we could have more control using lower level API that higher level ones. Since TF community is working hard to support non-NVidia devices, it is crucial to reduce the proprietary dependencies we are relying on.\r\n\r\nI am glad to know if you have any other reasons why this should be a PR rather than a private fork that specifically suits your own needs. ", "Nagging Assignee @drpngx: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@drpngx - I am closing this pull request because this problem only exists for builds that use contrib.mpi_collectives or contrib.mpi. \r\n\r\nI have tried the default builds without MPI collectives and MPI support and MPI libraries including MVAPICH2-GDR work fine via the Horovod-MPI system available at: https://github.com/uber/horovod/"]}, {"number": 19796, "title": "TFLite: fix format mismatching warning.", "body": "format \u2018%s\u2019 expects a matching \u2018char*\u2019 argument.", "comments": []}, {"number": 19795, "title": "Tensorflow logs everything twice while training", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS Sierra version 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC 4.2.1\r\n\r\n### Describe the problem\r\nI'm training an object detection model using the new `ssdlite_mobilenet_v2_coco_2018_05_09` and it's configuration file `ssdlite_mobilenet_v2_coco.config` and tensorflow installed from source. When I launch the training tensorflow starts printing the same info twice. \r\n\r\nThis problem didn't happen while training the same network I'm trying to get, with a different model (checkpoint)  `ssd_mobilenet_v1_coco_2017_11_17` and the configuration file `ssd_mobilenet_v1_pets.config` and with tensorflow installed from pip (I tested with version 1.6.0 and 1.8.0) \r\n\r\nNOTE : I didn't change the code in both cases and I wonder what's the cause of this.\r\n\r\n### Source code / logs\r\n\r\n```\r\nINFO:tensorflow:global step 3292: loss = 3.2832 (2.960 sec/step)\r\nINFO:tensorflow:global step 3292: loss = 3.2832 (2.960 sec/step)\r\nINFO:tensorflow:global step 3293: loss = 3.5285 (3.675 sec/step)\r\nINFO:tensorflow:global step 3293: loss = 3.5285 (3.675 sec/step)\r\nINFO:tensorflow:global step 3294: loss = 2.3972 (3.564 sec/step)\r\nINFO:tensorflow:global step 3294: loss = 2.3972 (3.564 sec/step)\r\nINFO:tensorflow:Recording summary at step 3294.\r\nINFO:tensorflow:Recording summary at step 3294.\r\nINFO:tensorflow:global_step/sec: 0.294019\r\nINFO:tensorflow:global_step/sec: 0.294019\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I'm using CPU only and the command to execute the training is (for both cases) : \r\n\r\n    python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/name_of_config_file.config", "I got the same problem while training detectors using tools from the \"models\" repo, under models/research/object_detection. I am using tensorflow-gpu version.", "Could you reopen this issue in the `tensorflow/models` repo issues?", "@robieta done [#4517](https://github.com/tensorflow/models/issues/4517)"]}]