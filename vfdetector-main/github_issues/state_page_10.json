[{"number": 54336, "title": "tf.TensorArray as a FIFO ???", "body": "Hello,\r\n\r\n  [here](https://github.com/keras-team/keras/issues/16015) I was pointed to use `tf.TensorArray` instead of `tf.Variable` or `tf.queue.FIFOQueue` for making FIFO contained in custom layer. Is it an effective way? Exist any alternative here?\r\n\r\nIf it's the most effective method how can I replace `self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0))` with methods of `tf.TensorArray`?\r\n\r\n## Code\r\n```python\r\nclass FIFOLayer(Layer):\r\n    def __init__(self, window_size, **kwargs):\r\n        super(FIFOLayer, self).__init__(**kwargs)\r\n\r\n        self.window_size = window_size\r\n        self.count = 0\r\n\r\n    def build(self, input_shape):\r\n        super(FIFOLayer, self).build(input_shape)\r\n\r\n        self.queue = self.add_weight(\r\n            name=\"queue\",\r\n            shape=(self.window_size, input_shape[-1]),\r\n            initializer=tf.initializers.Constant(value=np.nan),\r\n            trainable=False,\r\n        )\r\n\r\n    def call(self, inputs, training):\r\n        timesteps = tf.shape(inputs)[0]\r\n\r\n        # check if batch_size is more than queue capacity\r\n        if timesteps > self.window_size:\r\n            raise ValueError()\r\n\r\n        # 1. append new state to queue\r\n        self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0))\r\n        self.count += timesteps\r\n\r\n        # 2. feed-forward\r\n        if self.count < self.window_size:\r\n            # generate mask\r\n            attention_mask = tf.cast(\r\n                tf.math.reduce_all(\r\n                    tf.math.logical_not(tf.math.is_nan(self.queue)), axis=-1\r\n                ),\r\n                dtype=tf.float32,\r\n            )\r\n            attention_mask = tf.matmul(\r\n                attention_mask[..., tf.newaxis],\r\n                attention_mask[..., tf.newaxis],\r\n                transpose_b=True,\r\n            )\r\n            return self.queue[tf.newaxis, ...], attention_mask\r\n        # !!! check overflow\r\n        elif self.count > self.window_size:\r\n            self.count = self.window_size\r\n\r\n        return self.queue[tf.newaxis, ...], None\r\n\r\n    @property\r\n    def is_full(self):\r\n        return self.count == self.window_size\r\n\r\n    def clear(self):\r\n        self.count = 0\r\n        self.queue.assign(tf.fill(self.queue.shape, np.nan))\r\n\r\n\r\nl = FIFOLayer(window_size=10)\r\nfor i in range(6):\r\n    x = tf.random.normal((2, 12))\r\n    y = l(x)\r\n    print(y)\r\n\r\nprint(l.is_full, \"\\n\\n\")\r\n\r\nl.clear()\r\n\r\nprint(l(x))\r\nprint(l.is_full, \"\\n\\n\")\r\n```\r\n\r\nThanks a lot for your time.\r\nHave a nice day.", "comments": ["@markub3327 ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error]", "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Monterey 12.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: Python 3.9.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Apple M1\r\n\r\nThis question is about what is the best practice with TensorFlow.\r\n\r\n1. Is the best practice using `tf.TensorArray` instead of `tf.Variable` or `tf.queue.FIFOQueue` for making FIFO contained in the custom layer (above is the example code).\r\n2. If it's the most effective method how can I replace self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0)) with methods of tf.TensorArray?\r\n\r\nThanks a lot.\r\nHave a nice day.\r\n", "Now I have an issue when I applied `tf.TensorArray` to `tf.data.dataset.map`.\r\n\r\n```python\r\nclass FIFOLayer(tf.keras.layers.Layer):\r\n    def __init__(self, window_size, **kwargs):\r\n        super(FIFOLayer, self).__init__(**kwargs)\r\n\r\n        self.window_size = window_size\r\n        self.count = 0\r\n\r\n    def build(self, input_shape):\r\n        super(FIFOLayer, self).build(input_shape)\r\n        self.features = input_shape[-1]\r\n\r\n        self.queue_array = tf.TensorArray(dtype=tf.float32, size=self.window_size)\r\n        self.queue_array = self.queue_array.scatter(\r\n            tf.range(self.window_size),\r\n            tf.fill((self.window_size, self.features), np.nan),\r\n        )\r\n\r\n    def call(self, inputs, training):\r\n        timesteps = tf.shape(inputs)[0]\r\n        tf.print(timesteps)\r\n\r\n        # check if batch_size is more than queue capacity\r\n        #if timesteps > self.window_size:\r\n        #    raise ValueError()\r\n\r\n        self.queue_array = self.queue_array.scatter(\r\n            tf.range(self.window_size),\r\n            tf.concat(\r\n                [\r\n                    self.queue_array.gather(tf.range(timesteps, self.window_size)),\r\n                    inputs,\r\n                ],\r\n                axis=0,\r\n            ),\r\n        )\r\n        queue_tensor = self.queue_array.stack()\r\n        self.count += timesteps\r\n        # 2. feed-forward\r\n        if self.count < self.window_size:\r\n            # generate mask\r\n            attention_mask = tf.cast(\r\n                tf.math.reduce_all(\r\n                    tf.math.logical_not(tf.math.is_nan(queue_tensor)), axis=-1\r\n                ),\r\n                dtype=tf.float32,\r\n            )\r\n            attention_mask = tf.matmul(\r\n                attention_mask[..., tf.newaxis],\r\n                attention_mask[..., tf.newaxis],\r\n                transpose_b=True,\r\n            )\r\n            return queue_tensor[tf.newaxis, ...], attention_mask\r\n        # !!! check overflow\r\n        elif self.count > self.window_size:\r\n            self.count = self.window_size\r\n\r\n        return queue_tensor[tf.newaxis, ...], None\r\n\r\n    @property\r\n    def is_full(self):\r\n        return self.count == self.window_size\r\n\r\n    def clear(self):\r\n        self.count = 0\r\n        self.queue_array = self.queue_array.scatter(\r\n            tf.range(self.window_size),\r\n            tf.fill((self.window_size, self.features), np.nan),\r\n        )\r\n```\r\n\r\n```python\r\npreprocessing_layer = FIFOLayer(\r\n    window_size = 300,\r\n)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((\r\n    np.random.normal(size=(64, 10)),\r\n    np.ones((64,))\r\n))\r\ndataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\r\n\r\nfor features in dataset.take(1):\r\n    print(features)\r\n```\r\n\r\n## Issue\r\n\r\n> ---------------------------------------------------------------------------\r\n> OperatorNotAllowedInGraphError            Traceback (most recent call last)\r\n> /var/folders/7v/fqqcktvs23qc8fwgftjpz_gh0000gn/T/ipykernel_19005/360468340.py in <module>\r\n>      15     np.ones((64,))\r\n>      16 ))\r\n> ---> 17 dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\r\n>      18 \r\n>      19 for features in dataset.take(1):\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic, name)\r\n>    2002         warnings.warn(\"The `deterministic` argument has no effect unless the \"\r\n>    2003                       \"`num_parallel_calls` argument is specified.\")\r\n> -> 2004       return MapDataset(self, map_func, preserve_cardinality=True, name=name)\r\n>    2005     else:\r\n>    2006       return ParallelMapDataset(\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\r\n>    5453     self._use_inter_op_parallelism = use_inter_op_parallelism\r\n>    5454     self._preserve_cardinality = preserve_cardinality\r\n> -> 5455     self._map_func = StructuredFunctionWrapper(\r\n>    5456         map_func,\r\n>    5457         self._transformation_name(),\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\r\n>    4531         fn_factory = trace_tf_function(defun_kwargs)\r\n>    4532 \r\n> -> 4533     self._function = fn_factory()\r\n>    4534     # There is no graph to add in eager mode.\r\n>    4535     add_to_graph &= not context.executing_eagerly()\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)\r\n>    3242          or `tf.Tensor` or `tf.TensorSpec`.\r\n>    3243     \"\"\"\r\n> -> 3244     graph_function = self._get_concrete_function_garbage_collected(\r\n>    3245         *args, **kwargs)\r\n>    3246     graph_function._garbage_collector.release()  # pylint: disable=protected-access\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n>    3208       args, kwargs = None, None\r\n>    3209     with self._lock:\r\n> -> 3210       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n>    3211       seen_names = set()\r\n>    3212       captured = object_identity.ObjectIdentitySet(\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n>    3555 \r\n>    3556           self._function_cache.missed.add(call_context_key)\r\n> -> 3557           graph_function = self._create_graph_function(args, kwargs)\r\n>    3558           self._function_cache.primary[cache_key] = graph_function\r\n>    3559 \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n>    3390     arg_names = base_arg_names + missing_arg_names\r\n>    3391     graph_function = ConcreteFunction(\r\n> -> 3392         func_graph_module.func_graph_from_py_func(\r\n>    3393             self._name,\r\n>    3394             self._python_function,\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\r\n>    1141         _, original_func = tf_decorator.unwrap(python_func)\r\n>    1142 \r\n> -> 1143       func_outputs = python_func(*func_args, **func_kwargs)\r\n>    1144 \r\n>    1145       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapped_fn(*args)\r\n>    4508           attributes=defun_kwargs)\r\n>    4509       def wrapped_fn(*args):  # pylint: disable=missing-docstring\r\n> -> 4510         ret = wrapper_helper(*args)\r\n>    4511         ret = structure.to_tensor_list(self._output_structure, ret)\r\n>    4512         return [ops.convert_to_tensor(t) for t in ret]\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_helper(*args)\r\n>    4438       if not _should_unpack(nested_args):\r\n>    4439         nested_args = (nested_args,)\r\n> -> 4440       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\r\n>    4441       if _should_pack(ret):\r\n>    4442         ret = tuple(ret)\r\n> \r\n> ~/miniforge3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n>     697       except Exception as e:  # pylint:disable=broad-except\r\n>     698         if hasattr(e, 'ag_error_metadata'):\r\n> --> 699           raise e.ag_error_metadata.to_exception(e)\r\n>     700         else:\r\n>     701           raise\r\n> \r\n> OperatorNotAllowedInGraphError: in user code:\r\n> \r\n>     File \"/var/folders/7v/fqqcktvs23qc8fwgftjpz_gh0000gn/T/ipykernel_19005/360468340.py\", line 17, in None  *\r\n>         lambda x, y: (preprocessing_layer(x), y)\r\n>     File \"/Users/martin/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\r\n>         raise e.with_traceback(filtered_tb) from None\r\n> \r\n>     OperatorNotAllowedInGraphError: Exception encountered when calling layer \"fifo_layer_18\" (type FIFOLayer).\r\n>     \r\n>     in user code:\r\n>     \r\n>         File \"/var/folders/7v/fqqcktvs23qc8fwgftjpz_gh0000gn/T/ipykernel_19005/1883461050.py\", line 26, in call  *\r\n>             self.queue_array = self.queue_array.scatter(\r\n>     \r\n>         OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n>     Call arguments received:\r\n>       \u2022 inputs=tf.Tensor(shape=(10,), dtype=float32)\r\n>       \u2022 training=None", "@markub3327 ,\r\nLooks like code is incomplete. Request you to provide tensorflow version you are using and complete code to reproduce the issue in our environment. It helps us in localizing the issue faster.", "**Python:** Python 3.9.7\r\n**Clang:** Clang 11.1.0 ] on darwin\r\n**TF Version:** 2.7.0\r\n**Platform:** Apple M1\r\n\r\n## Code\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass FIFOLayer(tf.keras.layers.Layer):\r\n    def __init__(self, window_size, **kwargs):\r\n        super(FIFOLayer, self).__init__(**kwargs)\r\n\r\n        self.window_size = window_size\r\n        self.count = 0\r\n\r\n    def build(self, input_shape):\r\n        super(FIFOLayer, self).build(input_shape)\r\n        self.features = input_shape[-1]\r\n\r\n        self.queue_array = tf.TensorArray(dtype=tf.float32, size=self.window_size)\r\n        self.queue_array = self.queue_array.scatter(\r\n            tf.range(self.window_size),\r\n            tf.fill((self.window_size, self.features), np.nan),\r\n        )\r\n\r\n    def call(self, inputs, training):\r\n        timesteps = tf.shape(inputs)[0]\r\n        tf.print(timesteps)\r\n\r\n        # check if batch_size is more than queue capacity\r\n        #if timesteps > self.window_size:\r\n        #    raise ValueError()\r\n\r\n        self.queue_array = self.queue_array.scatter(\r\n            tf.range(self.window_size),\r\n            tf.concat(\r\n                [\r\n                    self.queue_array.gather(tf.range(timesteps, self.window_size)),\r\n                    inputs,\r\n                ],\r\n                axis=0,\r\n            ),\r\n        )\r\n        queue_tensor = self.queue_array.stack()\r\n        self.count += timesteps\r\n        # 2. feed-forward\r\n        if self.count < self.window_size:\r\n            # generate mask\r\n            attention_mask = tf.cast(\r\n                tf.math.reduce_all(\r\n                    tf.math.logical_not(tf.math.is_nan(queue_tensor)), axis=-1\r\n                ),\r\n                dtype=tf.float32,\r\n            )\r\n            attention_mask = tf.matmul(\r\n                attention_mask[..., tf.newaxis],\r\n                attention_mask[..., tf.newaxis],\r\n                transpose_b=True,\r\n            )\r\n            return queue_tensor[tf.newaxis, ...], attention_mask\r\n        # !!! check overflow\r\n        elif self.count > self.window_size:\r\n            self.count = self.window_size\r\n\r\n        return queue_tensor[tf.newaxis, ...], None\r\n\r\n    @property\r\n    def is_full(self):\r\n        return self.count == self.window_size\r\n\r\n    def clear(self):\r\n        self.count = 0\r\n        self.queue_array = self.queue_array.scatter(\r\n            tf.range(self.window_size),\r\n            tf.fill((self.window_size, self.features), np.nan),\r\n        )\r\n\r\npreprocessing_layer = FIFOLayer(\r\n    window_size = 300,\r\n)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((\r\n    np.random.normal(size=(64, 10)),\r\n    np.ones((64,))\r\n))\r\ndataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\r\n\r\nfor features in dataset.take(1):\r\n    print(features)\r\n```\r\n\r\n## Result\r\n\r\n> main* $ python3 x.py                                                                                                          [12:03:44]\r\n> /Users/martin/miniforge3/lib/python3.9/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\r\n>   warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\r\n> Metal device set to: Apple M1\r\n> \r\n> systemMemory: 16.00 GB\r\n> maxCacheSize: 5.33 GB\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/Users/martin/Documents/Projects/HAR-Transformer/x.py\", line 81, in <module>\r\n>     dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2004, in map\r\n>     return MapDataset(self, map_func, preserve_cardinality=True, name=name)\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 5455, in __init__\r\n>     self._map_func = StructuredFunctionWrapper(\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4533, in __init__\r\n>     self._function = fn_factory()\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3244, in get_concrete_function\r\n>     graph_function = self._get_concrete_function_garbage_collected(\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3210, in _get_concrete_function_garbage_collected\r\n>     graph_function, _ = self._maybe_define_function(args, kwargs)\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3557, in _maybe_define_function\r\n>     graph_function = self._create_graph_function(args, kwargs)\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\", line 3392, in _create_graph_function\r\n>     func_graph_module.func_graph_from_py_func(\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\", line 1143, in func_graph_from_py_func\r\n>     func_outputs = python_func(*func_args, **func_kwargs)\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4510, in wrapped_fn\r\n>     ret = wrapper_helper(*args)\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 4440, in wrapper_helper\r\n>     ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\r\n>   File \"/Users/martin/miniforge3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 699, in wrapper\r\n>     raise e.ag_error_metadata.to_exception(e)\r\n> tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:\r\n> \r\n>     File \"/Users/martin/Documents/Projects/HAR-Transformer/x.py\", line 81, in None  *\r\n>         lambda x, y: (preprocessing_layer(x), y)\r\n>     File \"/Users/martin/miniforge3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\r\n>         raise e.with_traceback(filtered_tb) from None\r\n> \r\n>     OperatorNotAllowedInGraphError: Exception encountered when calling layer \"fifo_layer\" (type FIFOLayer).\r\n>     \r\n>     in user code:\r\n>     \r\n>         File \"/Users/martin/Documents/Projects/HAR-Transformer/x.py\", line 29, in call  *\r\n>             self.queue_array = self.queue_array.scatter(\r\n>     \r\n>         OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n>     \r\n>     \r\n>     Call arguments received:\r\n>       \u2022 inputs=tf.Tensor(shape=(10,), dtype=float32)\r\n>       \u2022 training=None\r\n\r\n", "@chunduriv ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3fde4ef8a063b8ce7749fa193d1ddb6c/54336.ipynb)."]}, {"number": 54331, "title": "Batch processing for tflite_runtime", "body": "**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBatch processing of images in object detection models that are running in a tflite_runtime environment would be a great feature to add. Right now, it is possible to resize the input tensor to something like [x, h, w, c] where x is the number of images in a batch, and run it through the interpreter. However, the predictions come out looking like an image classification model, where there are probabilities but no bounding boxes.\r\n\r\n**Will this change the current api? How?**\r\nYes. It will allow for developers to leverage the savings associated with batch processing.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to break large images into x number of tiles and run inference on them all at once. This will greatly improve object detection capabilities on EdgeTPU devices.\r\n\r\n**Any Other info.**\r\n[Here](https://discuss.tensorflow.org/t/tflite-batch-inference-bug/7495) is a post I made on batch processing with an EfficientDet model.\r\n", "comments": []}, {"number": 54325, "title": "Multi-GPU training not starting or freezing. GPUs at 100%", "body": "Very same script runs without issues on 1 GPU (setting NVIDIA_VISIBLE_DEVICES to one GPU) but freezes with 2 or 3 GPUs.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Server 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): 2.7 (Anaconda), 2.8 (official docker), 2.8 (self compiled docker)\r\n- TensorFlow version (use command below): 2.8 & 2,7\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): 9\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: 3x RTX A6000 / 3x 48GB\r\n\r\n**Describe the current behavior**\r\nTraining script hangs at executing `model.fit()`. Terminal shows `Loaded cuDNN version 8201` thrice (once for each GPU) and displays \"TensorFloat-32 will be used...\" once. GPUs turn to 100% but nothing gets calculated, CPU usage is nearly 0%, \"Epoch 1/50\" is displayed by `model.fit()` but no progress bar and nothing happens. Script is also unresponsive, even to Ctrl+C => process has to be killed manually.\r\n\r\n**Describe the expected behavior**\r\nAfter showing \"Loaded cuDNN 8201\" training should start and progress bar of `model.fit()` call should show up.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): -\r\n\r\n**Standalone code to reproduce the issue**\r\nMy code is a more complex scenario but even this simple code here shows the same issue (executes right away on 1 GPU but freezes with 2 or 3 GPUs):\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n \r\n \r\nX = np.random.random((1000, 128, 128, 3)).astype(np.float32)\r\nY = np.random.random((1000, 10)).astype(np.float32)\r\n \r\ndat_x = tf.data.Dataset.from_tensor_slices(X)\r\ndat_y = tf.data.Dataset.from_tensor_slices(Y)\r\nds = tf.data.Dataset.zip((dat_x, dat_y))\r\nds = ds.batch(96)\r\nds = ds.repeat(50)\r\n \r\nstrategy = tf.distribute.MirroredStrategy()\r\n \r\nwith strategy.scope():\r\n    inputs = tf.keras.Input(shape=(128, 128, 3))\r\n    x = tf.keras.layers.Conv2D(32, (3, 3))(inputs)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n    x = tf.keras.layers.Conv2D(32, (3, 3))(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(10)(x)\r\n    model = tf.keras.Model(inputs=inputs, outputs=x)\r\n    optim = tf.optimizers.Adam()\r\n \r\n    model.compile(optim, loss=tf.keras.losses.CategoricalCrossentropy())\r\n \r\n \r\nmodel.fit(ds, epochs=50)\r\n\r\n```\r\n\r\n\r\n", "comments": ["I have the same problems, I have checked the driver(ok), cuda(ok), cudnn(ok), nccl(ok), all of these were normal operation. However, the distribution training was broken and frozen. It is so disappointing.", "Do you have any logs that I can look into?", "Commenting just to say that I'm seeing the same behavior though with `cuDNN version 8400`\r\n\r\nEDIT: Felt this might be useful - If I use \r\n```python\r\nstrategy = tf.distribute.MirroredStrategy(devices=['/gpu:0'])\r\n```\r\nthings work fine. If I switch it to `/gpu:1`, I get the following errors:\r\n```\r\n...\r\nNode: 'Assert/Assert'\r\n3 root error(s) found.\r\n  (0) INVALID_ARGUMENT:  assertion failed: [loop must iterate at least once to initialize outputs]\r\n         [[{{node Assert/Assert}}]]\r\n         [[while/body/_1/while/div_no_nan_1/ReadVariableOp/_95]]\r\n  (1) INVALID_ARGUMENT:  assertion failed: [loop must iterate at least once to initialize outputs]\r\n         [[{{node Assert/Assert}}]]\r\n         [[while/body/_1/gradient_tape/while/sequential/global_average_pooling2d/ones/_86]]\r\n  (2) INVALID_ARGUMENT:  assertion failed: [loop must iterate at least once to initialize outputs]\r\n         [[{{node Assert/Assert}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_5209]\r\n```\r\n\r\nIn fact, using `with tf.device('/gpu:1'):` instead of `with strategy` leads to the same error. However, I can remove the error by adding \r\n```\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\r\n...\r\nwith tf.device('/GPU:1'):\r\n```\r\n\r\nSo I'm guessing (at least in my case), that there's something going on with multiple GPUs more generally and MirroredStrategy just isn't catching the errors."]}, {"number": 54324, "title": "Expose CreateLoopInvariantCopy", "body": "Summary:\r\nIn order to use this function in other passes that act on while loops\r\nmake this function public.", "comments": ["Hi @r4nt Can you please review this PR ? Thank you!"]}, {"number": 54319, "title": "Add support for VariablePolicy to saved_model.load", "body": "This PR adds python level support for VariablePolicy to saved_model.load when soft placement is disabled. A related issue is [53743 ](https://github.com/tensorflow/tensorflow/issues/53743).", "comments": ["@ccrusius  Can you please review this PR ? Thank you!"]}, {"number": 54310, "title": "TFTRT is not optimizing Resize Bilinear", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: [nvcr.io/nvidia/tensorflow:21.12-tf2-py3](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow)\r\n- TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt. \r\n- TensorFlow version (use command below): via docker image. \r\n- Python version: Python 3.8.10\r\n- CUDA/cuDNN version: CUDA 11.5.0\r\n- GPU model and memory: Telsla T4 16 GB, but also on GTX 1080 TI. \r\n\r\n**Describe the current behaviour**\r\nWhen training and optimizing the model, logs indicate that an operation called ResizeBilinear is not optimized. Initially, I thought that's because I am using dynamic_shapes = True (and this operation does not support dynamic shapes), but when I disabled that, the log persists. \r\n\r\n**My question is also, what kind of improvement should I expect if this operation is supported?**\r\n\r\n**Describe the expected behaviour**\r\nI think a lot of applications are using Resizing on the input, and from my research [ONNX](https://github.com/onnx/onnx/blob/main/docs/Operators.md#resize)  and [Tesnorrt](https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_resize_layer.html) is supporting this operation. I also checked that ``tf.resize`` via nearest neighbour interpolation is supported which gives me no indication why it is not working. \r\n\r\n**Standalone code to reproduce the issue**\r\nI was trying to create a collab, but it is a nightmare to set up TensorRT. Here are my tries and standalone code for script which trains a simple model, saves it and then optimizes it via TFTRT. \r\n- Google Colab: [link](https://colab.research.google.com/drive/1K-6Qoeo-3GxINUJw6Xi3N_6V0y0SHBON?usp=sharing)\r\n- Python script:  [link](https://gist.github.com/rpytel1/de52d7e3116c6d3da5c7bd3f9e65a738)\r\n\r\n**Other info / logs**:\r\nUnsupported comment looks like this: \r\n<img width=\"1016\" alt=\"Screenshot 2022-02-09 at 11 57 59\" src=\"https://user-images.githubusercontent.com/12116282/153184823-51f0835b-7db1-4f70-a656-1309d723e879.png\">\r\n\r\n", "comments": []}, {"number": 54299, "title": "`tf.data` shuffle uses 2.5x more memory than necessary during `model.fit`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0 and 2.9.0.dev20220208\r\n- Python version: 3.9\r\n\r\n**Describe the current behavior**\r\n\r\nWhen training a Keras model with a `tf.data.Dataset` that has not been cached in memory before applying `tf.data.Dataset.shuffle(buffer)`, it seems like the shuffle buffer is never properly freed thus memory usage increases far beyond the memory expected from the shuffle buffer.\r\n\r\nWhen running the code below using `dataset.cache().shuffle(num_examples)` the entire dataset will be loaded into memory and shuffled. This is expected and the code will require **~145 GB** of memory which is equivalent to the size of the dataset.\r\n\r\nHowever using `dataset.shuffle(num_examples // 2)` without prior in memory caching (like what would be require on smaller machines) the code requires **~160 GB** of memory which is more than the entire size of the dataset, making `.shuffle()` unusable with large datasets and buffers. The same behaviour can be observed when setting `reshuffle_each_iteration=False`. It seems like after the first epoch the memory usage just continues to go up rather than staying at roughly the size that is required to store the shuffle buffer.\r\n\r\n**Describe the expected behavior**\r\nI would expect that `tf.data` and `model.fit` do not use memory beyond what's set required by the shuffle buffer, so in this example around **~73 GB**. Ideally the buffer would even be shared across epochs so that it doesn't need to be filled again before every epoch. A bit of memory overhead is expected to do this efficiently but the overhead should not be twice the size of the shuffle buffer and should certainly not exceed the total size of the dataset since that defeats the purpose of using a shuffle buffer that is smaller than the dataset.\r\n\r\nI don't think I am doing anything non-standard in the code below. In fact I follow strickly the recommended usage of both `tf.data` and Keras so I am very surprised about the larger than expected memory usage.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nConsider the following simple example that trains a no-op model on a large dataset (in this case ImageNet):\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom tensorflow import keras\r\n\r\nbatch_size = 1024\r\n\r\ndataset, info = tfds.load(\r\n    \"imagenet2012:5.0.*\",\r\n    decoders={\"image\": tfds.decode.SkipDecoding()},\r\n    split=\"train\",\r\n    with_info=True,\r\n    data_dir=\"gs://my_data_bucket\",\r\n)\r\nnum_examples = info.splits[\"train\"].num_examples\r\n\r\n\r\ndef _decode_and_center_crop(image_bytes):\r\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\r\n    shape = tf.image.extract_jpeg_shape(image_bytes)\r\n    image_height, image_width = shape[0], shape[1]\r\n    image_size = 224\r\n\r\n    padded_center_crop_size = tf.cast(\r\n        (\r\n            (image_size / (image_size + 32))\r\n            * tf.cast(tf.minimum(image_height, image_width), tf.float32)\r\n        ),\r\n        tf.int32,\r\n    )\r\n\r\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\r\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\r\n    crop_window = tf.stack(\r\n        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]\r\n    )\r\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\r\n    image = tf.image.resize(image, [image_size, image_size])\r\n    return tf.cast(image, dtype=tf.float32)\r\n\r\n\r\ndef preprocessing(data):\r\n    return _decode_and_center_crop(data[\"image\"]), data[\"label\"]\r\n\r\n\r\ndataset = (\r\n    dataset.shuffle(num_examples // 2)\r\n    .map(preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nmodel = keras.models.Sequential(\r\n    [\r\n        keras.layers.GlobalMaxPool2D(input_shape=(224, 224, 3)),\r\n        keras.layers.Dense(1000, activation=\"softmax\"),\r\n    ]\r\n)\r\n\r\nmodel.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\")\r\n\r\nmodel.fit(dataset, epochs=3)\r\n```\r\n\r\nUnfortunately this cannot be reproduced on Colab since not enough memory is available there. This issue is not related the the exact dataset used. I am just using ImageNet streamed from a GCS bucket, since this should be a pretty standard task. However this can be reproduced with any significantly large dataset.\r\n\r\nThe underlying cause might similar to what caused #36240, but I haven't been able to look into the `tf.data` or Keras code to debug what's going on.\r\n", "comments": ["Hi @lgeiger i am ne in this open source stuff and want to learn to work on this issues so can you assign me for this issue and provide me resources to learn for solving this issue?", "> Hi @lgeiger i am ne in this open source stuff and want to learn to work on this issues so can you assign me for this issue and provide me resources to learn for solving this issue?\r\n\r\nUnfortunately I can't. I am not familiar with the `tf.data` code myself and don't have the required repo access either.", "I am a absolute beginner so i have to learn from start  can you just tell me form where i can learn all these things?", "> I am a absolute beginner so i have to learn from start can you just tell me form where i can learn all these things?\r\n\r\nA good resource for learning how to contribute to open source software is https://opensource.guide/how-to-contribute/. For this repository in particular you can checkout the [contributor guide](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md).", "Sorry but i want to know about that what to learn for solving this issue. Do i have learn about somthing specific in tensorflow lib or anything else .\n\nI am asking for material to work on source code not for how to contribute ", "@lgeiger Could the issue be reproduced in colab by using a smaller shuffle buffer? Also, how are you measuring memory usage?", "> @lgeiger Could the issue be reproduced in colab by using a smaller shuffle buffer?\r\n\r\nI haven't tried since I am not sure how one would properly measure the memory usage within Colab. But in general it might be a bit tricky to reproduce the issue with a smaller shuffle buffer as the memory increase could be hidden by the normal memory usage of TensorFlow which tends to fluctuate a bit.\r\n\r\n> Also, how are you measuring memory usage?\r\n\r\nI ran the code on a fresh GCP VM with lots of memory and visually look at the memory usage reported by [`bottom`](https://github.com/ClementTsang/bottom) (but any other system monitor should do)."]}, {"number": 54294, "title": "Do you have plan to build with C++17 by default?", "body": "Hi,\r\n\r\nI found TensorFlow has build option to build with C++17, but seems the release version binary did not use this one.\r\nDo you have plan to build with C++17 in release binary?", "comments": ["@guizili0 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 20.04\r\n- Mobile device : NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.8\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nWant to confirm the C++ standard of the release binary, if it is not C++17, do you have plan to use C++17? thanks.", "@guizili0 Tensorflow 2.0.0 is tested and build with \r\nMicrosoft Visual C++ 2017 Redistributable. Take a look at [Tested build configurations](https://www.tensorflow.org/install/source_windows#tested_build_configurations).Please let us know if it helps?\r\nThank you!", "@sushreebarsa  Thanks.\r\nHow about the linux side? And from the compiler version how can I figure out the C++ standard? thanks.", "@guizili0 GCC compiler 7.3.1. supports c++17.\r\nWhile building Tensorflow from source you can mention gcc version as 7.3.1 in the **./configure**.Please check [this](https://github.com/tensorflow/tensorflow/blob/master/.bazelrc) link for reference.Thanks!", "@sushreebarsa Thanks.\r\nYes, you are right, TF can configure C++17, so I try to know the [TF release binary](https://pypi.org/project/tensorflow/) C++ standard. thanks. ", "Hi @guizili0 \r\n\r\nI am currently working on enabling C++17 dialect in entirety of TF. While `.bazelrc` supports the `--config=c++1z` (and similar), this was experimental and only for some targets, not the pip package.\r\n\r\nCurrently I got the macos build to fully work in C++17, have 3 failing tests on Linux and some compile errors on Windows. If all goes well, by next TF release we should be on C++17", "@mihaimaruseac  Thanks for this information! Can you help to share how to check C++17 is enabled? Or do you have somewhere to get these information? I can check later. Thanks.", "I will announce on this issue and the [forum](https://discuss.tensorflow.org/latest) when the switch is done. Probably we'll also remove C++14 stuff from `.bazelrc`", "@mihaimaruseac  great thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54294\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54294\">No</a>\n", "Keeping open util the switch"]}, {"number": 54284, "title": "Tensorflow unsupported within MSYS2?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): wanted binary .whl via `python3 -m pip install tensorflow`\r\n- TensorFlow version: any would be fine, was trying for 2.6.3\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: no GPU\r\n\r\n**Describe the problem**\r\n\r\nOn MSYS2, using pip via the mingw64-compiled Python, the message is\r\n```\r\n# python3 -m pip install --upgrade https://files.pythonhosted.org/packages/f2/ff/32e0f1e039fb6cabc88a5e03a0a2b2f01cd50b69cc1b915daf80ca0345c5/tensorflow-2.6.3-cp39-cp39-win_amd64.whl\r\nERROR: tensorflow-2.6.3-cp39-cp39-win_amd64.whl is not a supported wheel on this platform.\r\n```\r\n\r\nIt's the same if I just use the name eg `python3 -m pip install tensorflow`.\r\n\r\nI have an extensive codebase that currently depends on MSYS2 and gcc in the Windows context, and we have a module that successfully uses Tensorflow on our Linux version of the code. As above, however, I can't use Tensorflow via Python under MSYS2 using the version Python that MSYS2 provides.\r\n\r\nI wonder if I could get some clarity on what the options are for Windows/MSYS2 therefore:\r\n\r\n- can tensorflow be built on Windows using gcc in MSYS2? The instructions, even if they use MSYS2, seem to be referring to MSVC as the compilier. What are the reasons why 'full' MSYS2 support is not possible? *\r\n- should I instead switch, if possible, to building my software to link against Windows Python instead of MSYS2 Python. This would be less convenient for users, but might work.\r\n- would it be possible for the situation regarding MSYS2 support to be clarified in the MSYS2 documentation somehow?\r\n\r\n[*] I note that https://www.tensorflow.org/install/source_windows says that there is some issue with path support in Windows by Bazel, but obviously Tensorflow builds find on Linux, where such paths exist. Is this a Bazel configuration issue?\r\n\r\n\r\n", "comments": ["@jdpipe ,\r\nCan you please confirm is there any specific reason to use tf v2.6.3, please try with stable v2.7 and let us know if you are facing same issue.Thanks!", "Hi @tilakrayal,\r\nI tried the following instead:\r\n```\r\npye@DESKTOP-6ADQVP0 MINGW64 ~/solartherm\r\n# python3 -m pip install --upgrade \"https://files.pythonhosted.org/packages/dd/0b/6fd33732d436a6735094b6e44359e13c10901e71aeaf348ff7c9e4ee08c9/tensorflow-2.8.0-cp39-cp39-win_amd64.whl\"\r\nERROR: tensorflow-2.8.0-cp39-cp39-win_amd64.whl is not a supported wheel on this platform.\r\n```\r\nand also \r\n```\r\npye@DESKTOP-6ADQVP0 MINGW64 ~/solartherm\r\n# python3 -m pip install --upgrade \"https://files.pythonhosted.org/packages/af/e0/d1e53ca7411b0dbe8d82afafd39aab5aafaa02fa5d22c3c226a61f349f19/tensorflow-2.7.1-cp39-cp39-win_amd64.whl\"\r\nERROR: tensorflow-2.7.1-cp39-cp39-win_amd64.whl is not a supported wheel on this platform.\r\n```\r\nand also \r\n```\r\npye@DESKTOP-6ADQVP0 MINGW64 ~/solartherm\r\n# python3 -m pip install tensorflow\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n\r\npye@DESKTOP-6ADQVP0 MINGW64 ~/solartherm\r\n# which python3\r\n/mingw64/bin/python3\r\n```\r\n\r\nNote here that I am using the Python that is provided by MSYS2 (MinGW64), which is not the same as 'regular' CPython for Windows. It seems that the PyPI packages for Tensorflow are being deliberately blocked for use on MSYS2 somehow, and I'm not sure if that's deliberate (nothing in the documentation that I could see) or on purpose (would be nice to know the reason).\r\n\r\n", "Further to the above, I note that I typed `python3 -m pip debug --verbose` and got the following output:\r\n```\r\nCompatible tags: 33\r\n  cp39-cp39-mingw_x86_64\r\n  cp39-abi3-mingw_x86_64\r\n  cp39-none-mingw_x86_64\r\n  cp38-abi3-mingw_x86_64\r\n  cp37-abi3-mingw_x86_64\r\n  cp36-abi3-mingw_x86_64\r\n  cp35-abi3-mingw_x86_64\r\n  cp34-abi3-mingw_x86_64\r\n  cp33-abi3-mingw_x86_64\r\n  cp32-abi3-mingw_x86_64\r\n  py39-none-mingw_x86_64\r\n  py3-none-mingw_x86_64\r\n  py38-none-mingw_x86_64\r\n  py37-none-mingw_x86_64\r\n  py36-none-mingw_x86_64\r\n  py35-none-mingw_x86_64\r\n  py34-none-mingw_x86_64\r\n  py33-none-mingw_x86_64\r\n  py32-none-mingw_x86_64\r\n  py31-none-mingw_x86_64\r\n  py30-none-mingw_x86_64\r\n  cp39-none-any\r\n  py39-none-any\r\n  py3-none-any\r\n  py38-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```\r\nMeanwhile, if I type ` python3 -m pip download tensorflow --platform win32 --python-version 3.9 -vv --no-deps` then I get this output:\r\n```\r\n[...]\r\nd72c28c6c6fdda814fd68b957a3d206f4df5/tensorflow-2.8.0-cp37-cp37m-macosx_10_14_x86_64.whl#sha256=fa4a723368d5f748b6f4ec305cf7c26b98e4a6a8c2ce1425f8ae10383a37bcfc (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp37-cp37m-manylinux2010_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/31/66/d9cd0b850397dbd33f070cc371a183b4903120b1c103419e9bf20568456e/tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl#sha256=05fb161c6b2a6c4b8317a703a0a6d7f7aa6b5e3c6ea31bbc4f44ef96b89c3344 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp37-cp37m-win_amd64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/fe/36/7c7c9f106e3026646aa17d599b817525b139e2870f75b532318573cbabd4/tensorflow-2.8.0-cp37-cp37m-win_amd64.whl#sha256=291fa84f1022914580810ad76732fb254e44a8a609128e1c58873a12b2f81559 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp38-cp38-macosx_10_14_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/e9/6b/a4b872afe267184051a716aae58f1f9d8ebb456ecce7edece70a07c15f56/tensorflow-2.8.0-cp38-cp38-macosx_10_14_x86_64.whl#sha256=dd0f9f113ebc21b73fcd349db1629e187b8686395b8146d100eb1706a943bbc0 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp38-cp38-manylinux2010_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/2f/45/f5c91d69c2121e8e60673164bdcd2c6cda7b89e37decbc3c01b0466ca990/tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl#sha256=8489b4f1771e146f752b0eaeb57acf183bd07357e4550464e7dff18b3b656b5d (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp38-cp38-win_amd64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/17/5b/dac6e0607e4186b9e157597cd96d945aa769c60ef9f9f1b7ddc174f39332/tensorflow-2.8.0-cp38-cp38-win_amd64.whl#sha256=da38d4043185267e7316ae5dc98d18e89c8af4170859f64798e7a3607fd606e3 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp39-cp39-macosx_10_14_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/80/26/2990ee24ea5ef5ece557828ce295abc75e5b6c0c8ebf571a842f6e1125db/tensorflow-2.8.0-cp39-cp39-macosx_10_14_x86_64.whl#sha256=52f225fecc688281b3ae2cba2b52d3ed6215ed4a3ffb686b9cfd09885ca65563 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp39-cp39-manylinux2010_x86_64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/b9/39/88799b7257f73482d4c77bc18c068902a91629c6d380db88cc28ede44fd5/tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl#sha256=9d91a989e5455ae713c03fd7236071ab3f232ad8ff2831f2658072933546091f (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: none of the wheel's tags (cp39-cp39-win_amd64) are compatible (run pip debug --verbose to show compatible tags): https://files.pythonhosted.org/packages/dd/0b/6fd33732d436a6735094b6e44359e13c10901e71aeaf348ff7c9e4ee08c9/tensorflow-2.8.0-cp39-cp39-win_amd64.whl#sha256=b360c13b3e58b9a5c0780cbdb6b549eea73f620275fa203f8508fe418ae02735 (from https://pypi.org/simple/tensorflow/)\r\n[...]\r\n```\r\nWhich clearly indicates that it is the 'platform tag' that is causing the problem here. In short, `cp39-cp39-win_amd64` doesn't match with `cp39-cp39-mingw_x86_64`. \r\n\r\nThe question is, does the platform need to be so restrictive here? If Python/Wheel wasn't blocking me, would this module actually work, or not? Is this a MSYS2 issue or a TensorFlow issue, I wonder?\r\n\r\nhttps://www.python.org/dev/peps/pep-0425/#platform-tag", "One more thing. If I manually unzip the .whl file\r\n```\r\nwget \"https://files.pythonhosted.org/packages/dd/0b/6fd33732d436a6735094b6e44359e13c10901e71aeaf348ff7c9e4ee08c9/tensorflow-2.8.0-cp39-cp39-win_amd64.whl\"\r\nmkdir ~/tmpwhl\r\ncd ~/tmpwhl\r\nbsdtar xvf ../tensorflow-2.8.0-cp39-cp39-win_amd64.whl\r\n```\r\nand then 'trick' Python into importing that package:\r\n```\r\npye@DESKTOP-6ADQVP0 MINGW64 ~/tmpwhl\r\n# PYTHONPATH=. ipython3\r\nPython 3.9.7 (default, Nov 21 2021, 22:02:56)  [GCC 11.2.0 64 bit (AMD64)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.30.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:/msys64/home/pye/tmpwhl/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\nC:/msys64/home/pye/tmpwhl/tensorflow/__init__.py in <module>\r\n     39 import sys as _sys\r\n     40\r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43\r\n\r\nC:/msys64/home/pye/tmpwhl/tensorflow/python/__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39\r\n---> 40 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n     41 from tensorflow.python.eager import context\r\n     42\r\n\r\nC:/msys64/home/pye/tmpwhl/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     77     sys.setdlopenflags(_default_dlopen_flags)\r\n     78 except ImportError:\r\n---> 79   raise ImportError(\r\n     80       f'{traceback.format_exc()}'\r\n     81       f'\\n\\nFailed to load the native TensorFlow runtime.\\n'\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:/msys64/home/pye/tmpwhl/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\r\n\r\nIn [2]:\r\n```\r\nI opened `_pywrap_tensorflow_internal.pyd` using Dependency Walker, and found that the file that was missing was `PYTHON39.DLL`. On MinGW64, this file is called `libpython3.9.dll`, so I made a copy of the MinGW64 file, and renamed it to the expected name `python39.dll`.\r\n\r\nI then made a tiny bit more progress, namely:\r\n```\r\npye@DESKTOP-6ADQVP0 MINGW64 ~/tmpwhl\r\n# ipython3\r\nPython 3.9.7 (default, Nov 21 2021, 22:02:56)  [GCC 11.2.0 64 bit (AMD64)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.30.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow\r\n2022-02-13 07:08:14.285665: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2022-02-13 07:08:14.400592: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\nThis looks like it's getting a little bit further, and attempting to `dlopen` some CUDA-related stuff, and my system doesn't have CUDA, so I guess it fails. But presumably, it should not be crashing out of Python in this case.\r\n\r\nAll in all, it looks like the `mingw_x86_64` 'platform tag' makes sense, because the DLL for Python has a different name on this platform compared to `win_amd64`, and that means that platform-sensitive Python modules need to be compiled differently on this platform. \r\n\r\nHaving established that, are there any suggestions for how to compile Tensorflow such that it works with the MSYS2-MinGW64 version of Python?", "@jdpipe ,\r\nCan you please take a look at this [link](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) for the issue with the similar DLL error.It helps.Thanks!", "Hi Tilakrayal -- thanks for the link. However, my comments above were basically \"deep hacking\" of the available TensorFlow PyPI package to try to identify why it was refusing to install. It obviously still refuses to install on MSYS2, and our first need here is to figure out why that is, and then secondly, assuming that's correct, to figure out how to install it otherwise.\r\n\r\nI think that the issue is that MSYS2 doesn't support CUDA (AFAICT) and that, hence, the TensforFlow build process basically doesn't use the MSYS2 compiler, and tries to mostly not use MSYS2 at all, except for some bash tricks perhaps. I don't actually want/need CUDA for my application, so this is annoying for me, but I do understand.\r\n\r\nOur only path forward seems to be to attempt to install Windows Python, then install TensorFlow with the `pip` from Windows Python, then compile our local codebase against Windows Python instead of MinGW64 Python."]}, {"number": 54279, "title": "Extremely slow grads computation in dilatied convlution with a large dilation rate", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux & Windows & colab\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below): 2.7, 2.8rc, 2.8\r\n- Python version: 3.8\r\n- GPU model and memory: Titan RTX\r\n\r\nhttps://colab.research.google.com/drive/1-64VeSW-3SB3rS4mrzUzU1FnubX5TvnD?usp=sharing\r\n\r\nSuggest to try both CPU and GPU.\r\n\r\nIn CPU mode, large dilation rate can run 100x slower than smaller rate.\r\n", "comments": ["@edwardyehuang I tried to replicate the issue on colab using  TF v2.7.0 ,2.8 and tf-nightly using both [cpu](https://colab.research.google.com/gist/sushreebarsa/e8fc64b1e5dbb22423e83bda4dd79b37/issue-slow-dilation-conv.ipynb#scrollTo=Bto-pELt8w28) , [gpu](https://colab.research.google.com/gist/sushreebarsa/8a10563bcfafa56d219a356111150589/issue-slow-dilation-conv.ipynb#scrollTo=Bto-pELt8w28) runtime , and faced an error in gpu runtime .Could you please find the attached gists and confirm the same?\r\nThanks!", "> \r\nYou have reproduced CPU. For GPU, seems it is another bug that Colab does not support TensorFlow 2.8.\r\n\r\nFor the GPU slow grads issue, I found this in Windows version of TensorFlow 2.8\r\n", "Is it just the gradient that's slow, or the entire convolution?\r\n\r\nIIRC we simply extend the filter (filled with zeros) to match the dilation, so a large dilation would lead to large filters/gradients.  How \"large\" are we talking about?", "> Is it just the gradient that's slow, or the entire convolution?\r\n> \r\n> IIRC we simply extend the filter (filled with zeros) to match the dilation, so a large dilation would lead to large filters/gradients. How \"large\" are we talking about?\r\n\r\nSlow gradient only "]}, {"number": 54278, "title": "[oneDNN]Threading fix - Not For Merge", "body": null, "comments": ["@Srini511 Could you please rebase the PR ? Thank you!"]}, {"number": 54276, "title": "Deterministic GPU implementation of unsorted segment reduction op not available on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H2\r\n- TensorFlow installed from (source or binary): from PyPI\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: 3.10.2\r\n- CUDA/cuDNN version: 11.2, 8.1.1\r\n- GPU model and memory: GeForce RTX 2060\r\n\r\n**Describe the current behavior**\r\nThe code below works on Linux, but not on Windows where I am seeing\r\n\r\n> tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\r\n> Detected at node 'UnsortedSegmentSum_1' defined at (most recent call last):\r\n> Node: 'UnsortedSegmentSum_1'\r\n> Deterministic GPU implementation of unsorted segment reduction op not available.\r\n>          [[{{node UnsortedSegmentSum_1}}]] [Op:__inference_train_function_517]\r\n\r\n**Describe the expected behavior**\r\nIt works on both OSs.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\ntf.random.set_seed(0)\r\ntf.config.experimental.enable_op_determinism()\r\ndata = tf.ones((1, 1))\r\nlayer = tf.keras.layers.Input(shape=[1])\r\nmodel = tf.keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(loss=\"categorical_crossentropy\", metrics=\"AUC\")\r\nmodel.fit(x=data, y=data)\r\n```\r\n\r\nThis is due to the `AUC` metric as discussed in https://github.com/tensorflow/tensorflow/issues/51978. It was resolved for Linux, but not Windows in https://github.com/tensorflow/tensorflow/pull/51861. A workaround is given by `set TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS=True`. I am posting a new issue here as recommended in https://github.com/tensorflow/tensorflow/issues/39751#issuecomment-982919265.", "comments": ["@benbarsdell, do you know why the kernels had to be disabled on Windows, and how hard it would be to enable them for Windows?\r\n\r\n/CC @duncanriach", "Hi @bersbersbers ! Did not get any error either in [Colab ](https://colab.sandbox.google.com/gist/mohantym/1989e72f47d012c552fc953e0912c019/github_54276.ipynb#scrollTo=koYvPob1U-i9) or Windows 20H2 for 2.8.0. Could you try again with following code in Windows 21H2 and let us know?  Thanks!\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\ntf.keras.utils.set_random_seed(1)\r\ntf.config.experimental.enable_op_determinism()\r\ndata = tf.ones((1, 1))\r\nlayer = tf.keras.layers.Input(shape=[1])\r\nmodel = tf.keras.models.Model(inputs=layer, outputs=layer)\r\nmodel.compile(loss=\"CategoricalCrossentropy\", metrics=\"AUC\")\r\nmodel.fit(x=data, y=data)\r\n```", "Colab is running on Linux, I guess, in which case it won't reproduce there because it doesn't reproduce on Linux.\r\n\r\nWhen you try to reproduce on Windows, are you sure you are using GPU acceleration? Because it doesn't reproduce on a CPU.\r\n\r\n*Edit*: for the record:\r\n\r\n<details>\r\n\r\n```\r\n>python\r\nPython 3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.8.0\r\n>>> tf.keras.utils.set_random_seed(1)\r\n>>> tf.config.experimental.enable_op_determinism()\r\n>>> data = tf.ones((1, 1))\r\n2022-02-07 06:55:55.047679: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-07 06:55:56.218623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3969 MB memory:  -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n>>> layer = tf.keras.layers.Input(shape=[1])\r\n>>> model = tf.keras.models.Model(inputs=layer, outputs=layer)\r\n>>> model.compile(loss=\"CategoricalCrossentropy\", metrics=\"AUC\")\r\n>>> model.fit(x=data, y=data)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\r\n\r\nDetected at node 'UnsortedSegmentSum_1' defined at (most recent call last):\r\n    File \"<stdin>\", line 1, in <module>\r\n    File \"Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\r\n      tmp_logs = self.train_function(iterator)\r\n    File \"Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\r\n      return step_function(self, iterator)\r\n    File \"Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\r\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\r\n      outputs = model.train_step(data)\r\n    File \"Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 864, in train_step\r\n      return self.compute_metrics(x, y, y_pred, sample_weight)\r\n    File \"Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 957, in compute_metrics\r\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    File \"Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 459, in update_state\r\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\r\n    File \"Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\r\n      update_op = update_state_fn(*args, **kwargs)\r\n    File \"Python310\\lib\\site-packages\\keras\\metrics.py\", line 178, in update_state_fn\r\n      return ag_update_state(*args, **kwargs)\r\n    File \"Python310\\lib\\site-packages\\keras\\metrics.py\", line 2347, in update_state\r\n      return metrics_utils.update_confusion_matrix_variables(\r\n    File \"Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 628, in update_confusion_matrix_variables\r\n      return _update_confusion_matrix_variables_optimized(\r\n    File \"Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 429, in _update_confusion_matrix_variables_optimized\r\n      fp_bucket_v = tf.math.unsorted_segment_sum(\r\nNode: 'UnsortedSegmentSum_1'\r\nDeterministic GPU implementation of unsorted segment reduction op not available.\r\n         [[{{node UnsortedSegmentSum_1}}]] [Op:__inference_train_function_517]\r\n>>>\r\n```\r\n</details>", "The issue we ran into is described here:\r\nhttps://github.com/tensorflow/tensorflow/blob/92b294f/tensorflow/core/kernels/segment_reduction_ops_gpu_0.cu.cc#L97-L103", "I can reproduce on a Windows VM, where I can edit and quickly get a build error. The error is:\r\n\r\n```\r\nC:\\Users\\kbuilder\\AppData\\Local\\Temp\\nvcc_inter_files_tmp_dir\\tmpmapuno6t\\segment_reduction_ops_gpu_0.cu.cudafe1.stub.c(4558): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned\r\nC:\\Users\\kbuilder\\AppData\\Local\\Temp\\nvcc_inter_files_tmp_dir\\tmpmapuno6t\\segment_reduction_ops_gpu_0.cu.cudafe1.stub.c(5323): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned\r\nC:\\Users\\kbuilder\\AppData\\Local\\Temp\\nvcc_inter_files_tmp_dir\\tmpmapuno6t\\segment_reduction_ops_gpu_0.cu.cudafe1.stub.c(5428): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned\r\nC:\\Users\\kbuilder\\AppData\\Local\\Temp\\nvcc_inter_files_tmp_dir\\tmpmapuno6t\\segment_reduction_ops_gpu_0.cu.cudafe1.stub.c(5525): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned\r\nC:\\Users\\kbuilder\\AppData\\Local\\Temp\\nvcc_inter_files_tmp_dir\\tmpmapuno6t\\segment_reduction_ops_gpu_0.cu.cudafe1.stub.c(5530): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned\r\nC:\\Users\\kbuilder\\AppData\\Local\\Temp\\nvcc_inter_files_tmp_dir\\tmpmapuno6t\\segment_reduction_ops_gpu_0.cu.cudafe1.stub.c(5567): error C2719: 'unnamed-parameter': formal parameter with requested alignment of 128 won't be aligned\r\n```\r\n\r\nOpening the file on the first file, the specified line, 4558, is:\r\n\r\n```\r\n__cudaLaunch(((char *)((void ( *)(_ZN10tensorflow12CastIteratorINS_13AlignedVectorIdLi16EEES2_idE10IteratorTyE,  _ZN10tensorflow13AlignedVectorIdLi16EEE *, const int *, const int *, int,  _ZN10tensorflow7functor3SumE, _ZN3cub2IfILb0EN10tensorflow13AlignedVectorIdLi16EEES3_E4TypeE))cub::DeviceSegmentedReduceKernel< ::cub::DeviceReducePolicy< ::tensorflow::AlignedVector<double, (int)16> ,  ::tensorflow::AlignedVector<double, (int)16> , int,  ::tensorflow::functor::Sum> ::Policy600,  ::cub::TransformInputIterator< ::tensorflow::AlignedVector<double, (int)16> ,  ::tensorflow::LookupAndScaleAndCastInputsFunctor< ::tensorflow::AlignedVector<double, (int)16> ,  ::tensorflow::AlignedVector<double, (int)16> , int, double> ,  ::cub::CountingInputIterator<int, long long> , long long> ,  ::tensorflow::AlignedVector<double, (int)16>  *, const int *, int,  ::tensorflow::functor::Sum,  ::tensorflow::AlignedVector<double, (int)16> > )));\r\n```\r\n\r\nLooks like your comment is correct: the line above references both AlignedVector and cub. Any advice on how to fix? I don't have time to spend very long on this unfortunately, so we may have to leave this unfixed.", "So I was able to fix the error by changing [this line](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/util/gpu_kernel_helper.h;l=329;drc=0d53fe6214d99fc8ab85cfc45bb02b8d1fbf3411) to not instantiate `Functor<16>`, giving a max value of 8 on Windows. But I got several other errors, such as:\r\n\r\n```\r\n.\\tensorflow/core/util/gpu_kernel_helper.h(259): error: more than one operator \"*=\" matches these operands:\r\n            function \"Eigen::complex_operator_detail::operator*=(std::complex<float> &, const std::complex<float> &)\"\r\n            function \"std::complex<float>::operator*=(const std::complex<float> &)\"\r\n            operand types are: tensorflow::complex64 *= const tensorflow::complex64\r\n          detected during:\r\n            instantiation of \"tensorflow::AlignedVector<T, N> &tensorflow::AlignedVector<T, N>::operator*=(const tensorflow::AlignedVector<T, N> &) [with T=tensorflow::complex64, N=8]\"\r\n.\\tensorflow/core/kernels/segment_reduction_ops_gpu.cu.h(460): here\r\n```\r\n\r\nand\r\n\r\n```\r\n.\\tensorflow/core/util/gpu_kernel_helper.h(288): error: no instance of overloaded function \"tensorflow::max\" matches the argument list\r\n            argument types are: (const int, const int)\r\n          detected during instantiation of class \"tensorflow::AlignedVector<T, N> [with T=int, N=8]\"\r\n```\r\n\r\nThe first seems like a legitimate error: I don't why why eigen is redefining `operator*=` on complex numbers. I'm confused by the second, as I cannot find any `tensorflow::max` function at all, so I'm not sure why it's not simply using the CUDA max function.\r\n\r\nUnfortunately, I don't have more time to work on this.", "@reedwm @benbarsdell I don't have context for this problem.\r\n\r\nIs `SparseSegmentReductionFunctor` only used after `tf.config.experimental.enable_op_determinism()` is called?"]}, {"number": 54270, "title": "Getting incorrect value of lengthOutputTensor on iOS", "body": "**System information**\r\n- Using a stock example script provided in TensorFlow\r\n- iOS\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9.10\r\n- Bazel version (if compiling from source):3.7.2\r\n\r\n**Current behavior**\r\nI've used the stock example code snippet as mentioned below.\r\n\r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> tflModel = nullptr;\r\ntflite::ops::builtin::BuiltinOpResolver tflResolver;\r\nstd::unique_ptr<tflite::Interpreter> tflInterpreter;\r\ntflModel = tflite::FlatBufferModel::BuildFromFile(pakgFilePath.c_str());\r\ntflite::InterpreterBuilder(*tflModel, tflResolver)(&tflInterpreter);\r\ntflInterpreter->AllocateTensors();\r\n\r\nint32_t lengthInputTensor = tflInterpreter->input_tensor(0)->dims->data[1];\r\nint32_t lengthOutputTensor = tflInterpreter->output_tensor(0)->dims->data[1];\r\n\r\ntflInterpreter->Invoke();\r\n```\r\n\r\nThe above code is executed on Android and iOS both but getting an incorrect value of **lengthOutputTensor** in iOS for the same file.\r\n\r\n**Expected behavior**\r\nThe value of **lengthOutputTensor** on iOS should be the same as it's on Android for the same file.\r\n\r\nAlso, while going through the documentation, I've found below details. So is the above mentioned issue due to this work in progress for iOS?\r\n\r\n<img width=\"1344\" alt=\"Screenshot 2022-02-04 at 5 57 12 PM\" src=\"https://user-images.githubusercontent.com/98508996/152529171-e67e17f3-9e2b-4a84-b564-e340d4386648.png\">\r\n\r\n", "comments": ["Could anyone please let us know by when we can expect an update on this issue?", "@mercksamir Sorry for the late response!\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "I've mentioned the code snippet in my initial comment though here you go.\r\n\r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> tflModel = nullptr;\r\ntflite::ops::builtin::BuiltinOpResolver tflResolver;\r\nstd::unique_ptr<tflite::Interpreter> tflInterpreter;\r\ntflModel = tflite::FlatBufferModel::BuildFromFile(pakgFilePath.c_str());\r\ntflite::InterpreterBuilder(*tflModel, tflResolver)(&tflInterpreter);\r\ntflInterpreter->AllocateTensors();\r\n\r\nint32_t lengthInputTensor = tflInterpreter->input_tensor(0)->dims->data[1];\r\nint32_t lengthOutputTensor = tflInterpreter->output_tensor(0)->dims->data[1];\r\n\r\ntflInterpreter->Invoke();\r\n```", "Could anyone at least just confirm whether it's an issue due to some work in progress for iOS or not? ", "Hi @jvishnuvardhan - Any update on this?", "Could anyone please give an update in this issue?", "Hello @mercksamir, does your model have metadata? Can you try to run run inference without metadata and let us know if you face the same issue? https://www.tensorflow.org/lite/guide/inference"]}, {"number": 54269, "title": "GPU delegate for tflite not finding libOpenCL when deployed on linux", "body": "**System information**\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow):* yes\r\n- *OS Platform and Distribution (e.g., Linux Ubuntu 16.04):* Built on and deployed to Ubuntu 18.04 x86_64 \r\n- *TensorFlow installed from (source or binary):* Package of tflite built from source, deployed as deb\r\n- *TensorFlow version (use command below):* tflite v2.5.0 (2.6 was slower and we've not yet had time to assess 2.7)\r\n- *Python version:* n/a\r\n- *Bazel version (if compiling from source):* 4.2.1\r\n- *GCC/Compiler version (if compiling from source):* 7.5.0-3ubuntu1~18.04\r\n- *CUDA/cuDNN version:* n/a\r\n- *GPU model and memory:* Deployed to systems with nvidia GTX 1070\r\n\r\n**Describe the current behavior**\r\nOn a linux system the GPU delegate attempts to dynamically load libOpenCL but uses the \"dev\" name of the library. \r\nWhen deploying a solution on an Ubuntu system the filename for the installed OpenCL library from package \"ocl-icd-libopencl1\" is `libOpenCL.so.1`. The symlink `libOpenCL.so` would only be present via the \"ocl-icd-opencl-dev\" package which would usually be installed when building not deploying. \r\n\r\n**Describe the expected behavior**\r\nWhen deployed it should load OpenCL from the library package and not the dev package.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/cl/opencl_wrapper.cc#L92\r\n\r\n", "comments": []}, {"number": 54268, "title": "Can't build libtensorflowlite.so with `--config=elinux_armhf` - external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:36:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.8, e994fb9c3ad250d38fd07511aaa445eda728f9af\r\n- Python version: Python 3.8.10\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n\r\n**Describe the problem**\r\n\r\n```\r\n$ git clone --branch r2.8 https://github.com/tensorflow/tensorflow\r\n$ git checkout e994fb9c3ad250d38fd07511aaa445eda728f9af\r\n$  bazel build --verbose_failures --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=158\r\nINFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:elinux_armhf in file /root/tensorflow/.bazelrc: --config=elinux --cpu=armhf --distinct_host_configuration=true\r\nINFO: Found applicable config definition build:elinux in file /root/tensorflow/.bazelrc: --crosstool_top=@local_config_embedded_arm//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /root/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /root/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/XNNPACK/BUILD.bazel:6811:19: Compiling src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c failed: (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-4.2.1-linux-x86_64/bin:/root/bin:/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neonfp16_prod_microkernels/1/vcvt-neonfp16-x16.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neonfp16_prod_microkernels/1/vcvt-neonfp16-x16.pic.o' -fPIC -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/armhf-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/armhf-opt/bin/external/FP16 -iquote external/pthreadpool -iquote bazel-out/armhf-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/armhf-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/armhf-opt/bin/external/cpuinfo -iquote external/clog -iquote bazel-out/armhf-opt/bin/external/clog -Ibazel-out/armhf-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/armhf-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/armhf-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/armhf-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/armhf-opt/bin/external/clog/_virtual_includes/clog -isystem external/XNNPACK/include -isystem bazel-out/armhf-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/armhf-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/armhf-opt/bin/external/FP16/include -isystem external/pthreadpool/include -isystem bazel-out/armhf-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/armhf-opt/bin/external/FXdiv/include -w -DAUTOLOAD_DYNAMIC_KERNELS -Iinclude -Isrc -marm '-march=armv7-a' '-mfpu=neon-fp16' '-std=c99' -O2 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c -o bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neonfp16_prod_microkernels/1/vcvt-neonfp16-x16.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c: In function 'xnn_f32_f16_vcvt_ukernel__neonfp16_x16':\r\nexternal/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:36:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int'\r\n     const uint16x8_t vh0 = vreinterpretq_u16_f16(vcombine_f16(vcvt_f16_f32(vf0), vcvt_f16_f32(vf1)));\r\n                            ^~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:37:28: error: incompatible types when initializing type 'uint16x8_t' using type 'int'\r\n     const uint16x8_t vh1 = vreinterpretq_u16_f16(vcombine_f16(vcvt_f16_f32(vf2), vcvt_f16_f32(vf3)));\r\n                            ^~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:45:27: error: incompatible types when initializing type 'uint16x4_t' using type 'int'\r\n     const uint16x4_t vh = vreinterpret_u16_f16(vcvt_f16_f32(vf));\r\n                           ^~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f32-f16-vcvt/gen/vcvt-neonfp16-x16.c:55:21: error: incompatible types when initializing type 'uint16x4_t' using type 'int'\r\n     uint16x4_t vh = vreinterpret_u16_f16(vcvt_f16_f32(vf));\r\n                     ^~~~~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 2.279s, Critical Path: 1.96s\r\nINFO: 293 processes: 236 internal, 57 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["FWIW the build works fine on the `master` branch.", "```\r\n4ec84aa99372ca7899fb12715edd2bfe3c947c88 is the first bad commit\r\ncommit 4ec84aa99372ca7899fb12715edd2bfe3c947c88\r\nAuthor: Terry Heo <terryheo@google.com>\r\nDate:   Wed Dec 29 16:31:35 2021 -0800\r\n\r\n    lite: Fix XNNPACK build for elinux_armhf\r\n\r\n    PiperOrigin-RevId: 418863236\r\n    Change-Id: I066c4c4c10a98975a40a6ea820781e8bcdb75d76\r\n\r\n .bazelrc | 1 +\r\n 1 file changed, 1 insertion(+)\r\nbisect run success\r\n```", "Hm, but that change is already applied on r2.8... Let me try commenting it out.", "Oh wait, it isn't! Applying 4ec84aa99372ca7899fb12715edd2bfe3c947c88 on r2.8 fixes the build for me!", "Tensorflow 2.8 branch cut of rc0 and rc1 was made before the above commit on Dec 29th. \r\nYou need to wait for the next branch release for these changes to reflect.\r\nTill then you can build using master branch. Thanks!", "I understand that it missed the branch, but considering 2.8.0 is already out as a stable release and that target is [documented explicitly](https://www.tensorflow.org/lite/guide/build_arm#step_3_build_arm_binary), you might want to consider uplifting the change.", "Hi, Since the fix will be available in Tensorflow 2.9 and it will be released soon and I'm closing the linked PR, since this issue is not critical for cherrypick. \r\nYou can close this issue. Thanks!", "I'd say being able to build the project for a supported target is quite critical \ud83d\ude05", "You can build against the master branch and release of Tensorflow 2.9 is around the corner.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 54265, "title": "Reduce axis produce wrong results in tf.function mode when there are duplicate dimensions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.7.11\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: running on CPU\r\n- GPU model and memory: running on CPU\r\n\r\n**Describe the current behavior**\r\nThe two API: tf.math.reduce_sum, tf.math.reduce_mean will have wrong output in tf.function when there are duplicate dimension in axis.\r\n\r\n**Describe the expected behavior**\r\nIt should either raise an exception or clear the duplicate dimension.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport traceback\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput = np.array([[  79, -126,   95,   39,  -50,  -59,   -6,  -82,   95,  124,  -94,\r\n        -126,  -45,   46,   33,  123, -126],\r\n       [-108,   68,   91,   -5,   53,  122,  -56,  106, -107,  126,   64,\r\n         -25, -118,  -17,  -59,  -99,   98],\r\n       [ -25, -119,  -28,   44,   13,  -80,   30,  105,   42,  119,   79,\r\n          38,   34,  -89,  -23, -110,    1],\r\n       [ -44, -121,   -7,  -62,   -8,  120,   24,  110,  -68,  -22,  -67,\r\n         -10, -118,   21,  -47,  -94,  108],\r\n       [ 108,   82,  -14,   26,  120,  -11,   19,  -77,  -47,  -35,  109,\r\n          -3,   67,   20,  -90,   42,   95],\r\n       [-125,  -80,  -18,   72,  -90,   55,   39,   -2,  -81,   -6,  -84,\r\n          18,   96, -100,   68,   73,  -57],\r\n       [  67,   46,  -49,   47,  -11,  -66,  -14,  107,   43, -105,  -71,\r\n        -108,   23,  -21,   77,  -63, -107],\r\n       [-115,  -14,  -92,   68, -118,   73,   92,   27,  -21,  -99,  -99,\r\n         124,   47,  -70,  -93,  122,   69],\r\n       [ -96,  -47,  -23,   43,   12,   12,  -80,   65,   39, -110,    2,\r\n          -4,  110,  -23,   94,   67,   39]], dtype=np.int8)\r\naxis = np.array([0, 0], dtype=np.int32)\r\n\r\nfun_list = [tf.math.reduce_sum, tf.math.reduce_mean]\r\nfun = tf.math.reduce_sum\r\n\r\nfor fun in fun_list:\r\n    try:\r\n        output1 = fun(input, axis)\r\n    except:\r\n        print(traceback.format_exc())  # the eager mode will raise an execption\r\n\r\n    @tf.function\r\n    def fun_wrapper(x, y):\r\n        return fun(x, y)\r\n\r\n    output2 = fun_wrapper(input, axis)\r\n    print(output2)  # the results of tf function is wrong\r\n```\r\nThe tf function mode should either raise an Exception like the eager mode or clear the duplicate dimension and then do the execution.\r\n", "comments": ["Hi @jiannanWang ! I was getting invalid argument error in above code which got resolved by disabling eager execution . Attaching resolved [gist ](https://colab.sandbox.google.com/gist/mohantym/08933c56ef3f271d6cd6bd3a26e6e15b/github_54265.ipynb#scrollTo=wI4TxI9ecfDH)for reference.", "Hi mohantym, thank you for your response! From my perspective, the results for disabling eager execution are wrong, while the invalid argument error is kind of expected. In the above example, when the axis is [0, 0], I would expect it to act like axis = [0] instead of axis = [0, 1]. \r\n\r\nMoreover, I found that in tf.function mode, as long as len(axis) = len(input_tensor.shape), tf.math.reduce_sum, tf.math.reduce_mean will ignore the actual value of the axis argument and reduce all dimensions of the input_tensor, which is a wrong assumption without checking duplicate dimensions in axis.", "Hi @sachinprasadhs ! Could you please look at this issue? It is replicating  in [2.7](https://colab.sandbox.google.com/gist/mohantym/1a40f4940de5e619cc7bf8b7a0b47923/github_54265.ipynb#scrollTo=wI4TxI9ecfDH), [2.8](https://colab.sandbox.google.com/gist/mohantym/7f0eebdbc6ae4b8a1764552a1452b0c7/github_54265.ipynb#scrollTo=t37fp5I8vsMf) and [nightly](https://colab.sandbox.google.com/gist/mohantym/44d2418969979eeea99152d35d0e65c0/github_54265.ipynb#scrollTo=t37fp5I8vsMf) in eager mode."]}, {"number": 54254, "title": "Dataset error when using with XLA device (No unary variant device copy function found)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below):  v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- tensorflow-2.8.0-cp38-cp38-manylinux2010_x86_64.whl\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nerror in script: \r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:XLA_CPU:0 in order to run __inference_f_27: No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper [Op:__inference_f_27]\r\n\r\n**Describe the expected behavior**\r\nscript runs with no errors and produces values as expected \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no - internal error \r\n- Briefly describe your candidate solution(if contributing): no - internal error \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport os\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices' \r\n\r\nimport tensorflow as tf\r\n\r\ndataset = tf.data.Dataset.range(10)\r\n    \r\n@tf.function\r\ndef f():\r\n    for x in dataset:\r\n        tf.print(x)\r\n\r\nwith tf.device('/device:XLA_CPU:0'):\r\n    f()\r\n```", "comments": ["@jvishnuvardhan ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and  nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/07573cb93790424e0b640394b574b142/untitled212.ipynb).", "adding more context: \r\nit appears like this failure is related to the fact that XLA is (rightfully) not supporting dataset structs and operations. however, the failure is not clear and it would be better to try and generate a clear failure statement that explains that. \r\nthe user is expected to explicitly place all dataset related work on the CPU and hand over only pure tensors to the function, when must-compile is used (in this case, the must-compile is implicitly triggered because of the explicit placement on an XLA device). ", "@penpornk "]}, {"number": 54251, "title": "[MHLO]: add ops with regions lowering in hlo-legalize-to-lhlo", "body": "It mainly supports mhlo ops with regions lowering to lhlo with the same regions (in which ops are still mhlo). \r\nThey are all_reduce, map, reduce_scatter, scatter, select_and_scatter, and sort, based on existing tests. \r\n\r\nI am not very sure about lhlo.select_and_scatter, since there is no existing example in the repo, \r\nbut the lowered one seems passing the verifier. \r\nPlease let me know if it is wrong. \r\nAlso, lhlo.select_and_scatter would fail buffer-deallocation without RegionBranchOpInterface. \r\nTherefore, I also modified lhlo_ops.td and lho_ops.cc by adding RegionBranchOpInterface and the related definition.\r\nPlease let me know whether it still align with the design. \r\n", "comments": ["This is likely fine for now, but please take note that the team intend to retire the LHLO operations in favor of another design where the bufferization would be carried by a single LHLO operation containing the fused HLO for a given buffer. Details TBD\r\n\r\n@sherhut do you have more info?", "@joker-eph\r\nThanks for the information. That looks nice. \r\nWhen the detail is available, please let me know. \r\n", "@lchang20  Can you please resolve conflicts? Thanks!", "@lchang20 Any update on this PR? Please. Thank you!"]}, {"number": 54249, "title": "Segmentation Fault After Canonicalization Pass", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source. \r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):GCC 8.3\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nSegment Fault.\r\n\r\n**Describe the expected behavior**\r\nRun canonicalization pass successfully.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): I have some findings but I can't solve it independently.\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n- First create the `reproduce.mlir` as below.\r\n```\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 0 : i32}} {\r\n  func @main(%arg0: tensor<1xf32>, %arg1: tensor<1xf32>) -> tensor<1xf32> {\r\n    %0 = mhlo.constant dense<9.99999997E-7> : tensor<f32>\r\n    %1 = mhlo.constant dense<0.000000e+00> : tensor<1xf32>\r\n    %2 = shape.shape_of %1 : tensor<1xf32> -> tensor<1xindex>\r\n    %3 = shape.shape_of %0 : tensor<f32> -> tensor<0xindex>\r\n    %4 = shape.cstr_broadcastable %2, %3 : tensor<1xindex>, tensor<0xindex>\r\n    %5 = shape.assuming %4 -> (tensor<1xi1>) {\r\n      %8 = shape.const_shape [1] : tensor<1xindex>\r\n      %9 = \"mhlo.dynamic_broadcast_in_dim\"(%1, %8) {broadcast_dimensions = dense<0> : tensor<1xi64>} : (tensor<1xf32>, tensor<1xindex>) -> tensor<1xf32>\r\n      %10 = \"mhlo.dynamic_broadcast_in_dim\"(%0, %8) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<f32>, tensor<1xindex>) -> tensor<1xf32>\r\n      %11 = \"mhlo.compare\"(%9, %10) {comparison_direction = \"LT\"} : (tensor<1xf32>, tensor<1xf32>) -> tensor<1xi1>\r\n      shape.assuming_yield %11 : tensor<1xi1>\r\n    }\r\n    %6 = shape.const_witness true\r\n    %7 = shape.assuming %6 -> (tensor<1xf32>) {\r\n      %8 = \"mhlo.select\"(%5, %arg0, %arg1) : (tensor<1xi1>, tensor<1xf32>, tensor<1xf32>) -> tensor<1xf32>\r\n      shape.assuming_yield %8 : tensor<1xf32>\r\n    }\r\n    return %7 : tensor<1xf32>\r\n  }\r\n}\r\n```\r\n- `bazel  build //tensorflow/compiler/mlir/hlo:mlir-hlo-opt`\r\n- `./bazel-bin/tensorflow/compiler/mlir/hlo/mlir-hlo-opt -canonicalize reproduce.mlir`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n- backtrace: `readBits (rawData=0x0, bitPos=0, bitWidth=1)`\r\n- If `mlir-hlo-opt` is built by CMake, the command could run successfully.\r\n- If `mlir-hlo-opt` is built by Bazel with debug mode (`-c dbg`), the command could run successfully.\r\n- The bug start to appear from tensorflow commit `51ab810dd80114a463d6703f`.\r\n", "comments": ["@yaochengji \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@sushreebarsa Thanks for your reply. This content is already from the `bug issue template`. It is a `mlir` and `bazel` related bug, I hope the additional infomation could help you find the right person to assign the issue to.", "Is this still happening? I just tried it, and for me it seems to work now.", "> Is this still happening? I just tried it, and for me it seems to work now.\r\n\r\n@akuegel Thanks. But this bug still appears on my side. I'm using the latest commit `3362b358bbad2e6d`.\r\n\r\n\r\n\r\n", "My guess is that it depends on which compiler you use. Are you using gcc by any chance? And if yes, which version?", "I'm using gcc 8.3.0.\r\n```\r\ngcc (Debian 8.3.0-6) 8.3.0\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```", "I think we have seen segfaults also in other cases when using the gcc 8.3 compiler. Can you please try a newer version? gcc-9.3.1 should work for example:\r\n\r\nhttps://github.com/tensorflow/build/blob/nitin/manylinux2014/tf_sig_build_dockerfiles/builder.devtoolset/build_devtoolset.sh#L115", "Thanks, @akuegel . I tried gcc-11 and it works."]}, {"number": 54246, "title": "CONV_ADD and CONV_ADDV2 Fusion", "body": "Why do we need to check the broadcast compatibility before CONV_ADD and CONV_ADDV2 Fusion?\r\n\r\nRefer line 818 of [remapper.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc)\r\n\r\n`if (!IsAddN(*node_def) && !IsAddWithNoBroadcast(ctx, *node_def)) return false;`\r\n\r\nWhy do we need _**IsAddWithNoBroadcast(ctx, *node_def)**_ check? \r\n\r\nRefer line 795 to  805 of [remapper.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper.cc) for the definition of  _**IsAddWithNoBroadcast**_\r\n\r\n`bool IsAddWithNoBroadcast(const RemapperContext& ctx, const NodeDef& node) {`\r\n`  if (!IsAdd(node)) return false;`\r\n\r\n  `// Check if this is case of broadcasting - Add node supports broadcasting.`\r\n  `const auto& props = ctx.graph_properties.GetInputProperties(node.name())`;\r\n  `if (props.size() == 2 &&`\r\n      `ShapesSymbolicallyEqual(props[0].shape(), props[1].shape())) {`\r\n   ` return true;`\r\n ` }`\r\n  `return false;`\r\n`}`\r\n\r\nIf we have Add op defined in our network, then both of its inputs are either same or broadcast compatible otherwise we can not perform Add.\r\n\r\nConsider the third ADD node of [resnet50_v1.pb](https://zenodo.org/record/2535873/files/resnet50_v1.pb) in the attached figure. \r\n\r\n<img width=\"421\" alt=\"Resnet50_v1_Add3\" src=\"https://user-images.githubusercontent.com/40749307/152197427-69dd56ca-16f9-4bc0-844a-0757ca2c62cd.PNG\">\r\n\r\nPlease clarify why we need a Broadcast compatibility check. If inputs to Add are not the same or broadcast compatible, then How is it possible to have an Add op in the network? ", "comments": []}, {"number": 54244, "title": "On-device-training fails using Tflite_runtime: Node number 54 (FlexReluGrad) failed to prepare.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Custom embedded Linux distribution (Kernel 5.15)**\r\n- Mobile device: **Raspberry Pi 4B**\r\n- TensorFlow installed from (source or binary): **Source (Building on Tflite_runtime using pip_package scripts)**\r\n- TensorFlow version (use command below): **2.7.0**\r\n- Python version: **3.8**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **9.3 (OpenEmbedded GNU Toolchain)**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nI am following the tutorial on how to do [on-device-training](https://www.tensorflow.org/lite/examples/on_device_training/overview). The first step was to create and train the Fashion_mnist model on google Colab which was successful since I managed to download as an output the tflite model after converting (I made sure to mention all the necessary signatures while saving the model). The documentation is providing an example only on Java for android Apps, but I'm trying to explore whether this is feasible with tflite_runtime wheel.\r\nI sent then the tflite model to the target (raspberry pi) where the Tflite_runtime has been installed using [this article](https://www.tensorflow.org/lite/guide/build_cmake_pip) and I'm trying to run the training function by feeding my neural network with arrays of zeros just to prove that it is working. This is the code snippet I'm running on my target. \r\n\r\n```\r\nimport numpy as np\r\nimport struct as st\r\nimport tflite_runtime.interpreter as tflr\r\nnImg = 10000\r\nnR = 28\r\nnC =28\r\nimages_array = np.zeros((1, 28, 28), dtype=\"float32\") \r\nlabels_array = np.zeros((1,10), dtype=\"float32\")\r\nmodelpath=\"fashion_mnist_model.tflite\"\r\ninterpreter = tflr.Interpreter(modelpath)\r\nsignatures = interpreter.get_signature_list()\r\n```\r\n\r\n```\r\nprint('Signature:', signatures)\r\n>>> Signature: {'infer': {'inputs': ['x'], 'outputs': ['logits', 'output']}, 'restore': {'inputs': ['checkpoint_path'], 'outputs': ['dense_1/bias:0', 'dense_1/kernel:0', 'dense_2/bias:0', 'dense_2/kernel:0']}, 'save': {'inputs': ['checkpoint_path'], 'outputs': ['checkpoint_path']}, 'train': {'inputs': ['x', 'y'], 'outputs': ['loss']}}\r\n```\r\n```\r\ntrain = interpreter.get_signature_runner('train')\r\ninfer = interpreter.get_signature_runner('infer')\r\ntrain(x=images_array, y=labels_array)\r\n```\r\nThe script fails at the train function call and return the following traces:\r\n```\r\n`RuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. \r\nMake sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. \r\nSee instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nNode number 54 (FlexReluGrad) failed to prepare.`\r\n```\r\n**Describe the expected behavior**\r\nThe expected behavior is to get a complete and functional training on target. \r\n\r\nAny help would be appreciated, thank you in advance.", "comments": ["Another (probably helpful) and relevant information, delegate/flex sources seems to integrated in the  **`_pywrap_tensorflow_interpreter_wrapper.so`** as it can be shown below:\r\n```\r\nroot@rpi:~# nm -a /usr/lib/python3.8/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so   | grep -i \"Flex\"\r\n001552c8 W _ZN11flexbuffers10KeyCompareIhEEiPKvS2_\r\n001552d8 W _ZN11flexbuffers10KeyCompareIjEEiPKvS2_\r\n001552d0 W _ZN11flexbuffers10KeyCompareItEEiPKvS2_\r\n001552c0 W _ZN11flexbuffers10KeyCompareIyEEiPKvS2_\r\n0011a360 W _ZN6tflite19AcquireFlexDelegateEv\r\n00123a74 T _ZN6tflite8IsFlexOpEPKc\r\n002c3194 r _ZN6tfliteL21kFlexCustomCodePrefixE\r\n001557b0 W _ZNK11flexbuffers9Reference7AsInt64Ev\r\n001f4300 W _ZNK11flexbuffers9Reference8AsDoubleEv\r\n002c4860 u _ZZN11flexbuffers3Map8EmptyMapEvE9empty_map\r\n0011a1bc t _ZZN6tflite19AcquireFlexDelegateEvENUlP14TfLiteDelegateE_4_FUNES1_\r\n```", "@ahlzouao ,\r\nI was facing different error while executing the code.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/ebcb91fd4d6fd224a5cf843144e506aa/untitled210.ipynb).Please provide the complete code and dependencies to reproduce the issue.Thanks!", "@tilakrayal ,\r\nThank your for your answer, I have prepared a a gist [here](https://colab.research.google.com/drive/1rzvK_FtR0NZ0YUH9ya0L3uOoY3ZiZUM5?usp=sharing) with the whole code. \r\n\r\nUnfortunately, it is working just fine in the google colab environment and the train function returns the expected value, so that's not really the way to go. If you want to reproduce exactly the same issue, I would suggest you run the code on target by doing the following:\r\n1. Run the notebook in the link I sent you, \r\n2. then from **Files** section download the **fashion_mnist_model.tflite** file. \r\n3. Send this model file to your board (Raspberry Pi for example) via scp or any other file transfer protocol.\r\n4. From your board's terminal, run the following command.\r\n`BOARD$ pip3 install tflite-runtime`\r\n5. Then you will need to open a Python prompt \r\n`BOARD$ python3`\r\n6. Type the following instructions:\r\n```\r\nimport numpy as np\r\nimport struct as st\r\nimport tflite_runtime.interpreter as tflr\r\nimages_array = np.zeros((1, 28, 28), dtype=\"float32\") \r\nlabels_array = np.zeros((1,10), dtype=\"float32\")\r\nmodelpath=\"fashion_mnist_model.tflite\"\r\ninterpreter = tflr.Interpreter(modelpath)\r\nsignatures = interpreter.get_signature_list()\r\nprint('Signature:', signatures)\r\ntrain = interpreter.get_signature_runner('train')\r\ninfer = interpreter.get_signature_runner('infer')\r\ntrain(x=images_array, y=labels_array)\r\n```\r\nThat's the output showing the error:\r\n```\r\n`RuntimeError: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. \r\nMake sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_selectNode number 54 (FlexReluGrad) failed to prepare.`\r\n```", "@ahlzouao ,\r\nI was using colab to reproduce the issue.Can you please provide the  tf-lite file along with the code which you are facing issue.It helps to debug the issue.Thanks!", "@tilakrayal ,\r\nThe issue is not reproduceable on google colab, it will work as expected with no errors. You will need to run it on an armv7 or aarch64 architecture target to reproduce the issue, since the goal of this is to do on-device-training. [Here](https://drive.google.com/file/d/1i_emRlbfM_cSVltgkP81JqiN1rltM5Et/view?usp=sharing) is a link to the tf-lite file.", "@sachinprasadhs ,\r\nI was able to execute the code in colab without any issues.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/d4261dcd1bc6f900a1ff720b5e0205c5/untitled213.ipynb).", "@tilakrayal @sachinprasadhs \r\nI already mentioned that the code will run without any issues on Google Colab and the issue is not reproduce-able on its environement, I'm actually facing the issue while running the code on device (on a raspberry Pi aarch64 architecture). The bug might be in the tflite-runtime python wheel of aarch64. ", "Hello @sachinprasadhs ,\r\nHave you been able to reproduce the issue on some device ? I can confirm you that it is reproduce-able on both armv8 and armv7 architectures.\r\nThank you in advance,", "Hello @MeghnaNatraj , @sachinprasadhs \r\n I'm doubting this is isn't a bug but more of a missing feature. I have tried an interesting experimentation, I tried running the same script on my x86 host machine with no tensorflow package installed, only the tflite-runtime Python pip_package installed, this reproduces the same issue. After installing Tensorflow package, the error traces disappear. This made me think about some operation features missing in the interpreter I build using CMake.\r\nI was trying to add debug some traces into the tensorflow source code to be able to figure it out. I have reached some conclusions:\r\n\r\n- The traces I added were mainly near the intepreter function and allowed me to notice that the FlexReluGrad had no prepare function as you may notice below: \r\n```\r\n[DEBUG][PrepareOpsStartingAt]:[1003]: Node index: 54\r\n[DEBUG][OpPrepare][957]:Entering function ... \r\n[DEBUG][OpPrepare][959]:Op having a null prepare function \r\n[DEBUG][OpPrepare][962]:Unresolved custom op ... \r\n```\r\n- Based on the documentation, in order to do on-device-training, it is necessary to add the **Flex Delegate** to the interpreter during the build and select some tensorflow ops so the FlexOps could be understood by the interpreter (Hence the error, I'm getting each time : **_Node number 54 (FlexReluGrad) failed to prepare._** \r\n- By getting a closer look inside the libtensorflow-lite.a generated by  CMake, I can see that none of the flex delegate function has been integrated in the build since the variable [TFLITE_DELEGATES_FLEX_SRCS](https://github.com/tensorflow/tensorflow/blob/r2.7/tensorflow/lite/CMakeLists.txt#L399) is empty.\r\n- By digging deeper inside the Bazel BUILD scripts, I was trying to duplicate the same [build dependencies](https://github.com/tensorflow/tensorflow/blob/r2.7/tensorflow/lite/delegates/flex/BUILD#L126) and sources to build using CMake. I have also noticed that many dependencies from tensorflow/core/framework are used.\r\n\r\n=> To make it short, I'm trying to build a shared library containing the flex delegates with the _with_select_tf_ops=true_ but using **Cmake** instead of **Bazel** since it's not an option for me. This may be pushed as a new feature in the [tensorflow/lite/CMakeList.txt](https://github.com/tensorflow/tensorflow/blob/r2.7/tensorflow/lite/CMakeLists.txt)  upon completion, so I'm looking for some guidance on the dependencies and sources to build for that purpose. \r\n", "@haozha111 triaging for on-device related issues.", "Hi,\r\n\r\nSorry for the late reply. ReluGrad isn't a supported built-in op of TF Lite. So you will need to link in the Flex delegate to make it able to run on-device.", "Hello @haozha111, \r\nFirst of all, thank you for your reply.\r\nThat's what I figured out, then I tried to build a new libtensorflow-lite-flex.so library with the Flex Delegates and the necessary TF Ops kernels (ReluOp, SaveOp and RestoreOp in this case) using SELECTIVE_REGISTRATION Mode, but the build does not seem to be working properly for aarch64 as stated in this [issue](https://github.com/tensorflow/tensorflow/issues/54500).\r\n\r\nAlso, do you have a clearer example on how to link the flex delegate to my interpreter ? I tried doing this : \r\n```\r\n//Building the FlexDelegate instance\r\n       auto delegate = tflite::FlexDelegate::Create();\r\n\r\n       //Building the interpreter\r\n       std::unique_ptr<tflite::Interpreter> interpreter;\r\n       builder(&interpreter);\r\n\r\n       // Modifying the graph with the FlexDelegate instance\r\n       if(delegate){\r\n            interpreter->ModifyGraphWithDelegate(std::move(delegate));\r\n      }\r\n```\r\nFor now, I cannot tell if it is working properly or not because I still don't have a properly working shared library libtensorflow-lite-flex.so to link against.\r\n\r\nThank you for your support,\r\nOthmane\r\n", "Will https://github.com/tensorflow/tensorflow/issues/52018 help you resolve the build issue?\r\n\r\n", "Hello @haozha111, \r\nSorry, I was mistaken in the link of the issue. The one I mentioned was actually closed and the #52018 did help, hence it was closed. The issue I'm facing during the build is related to some dependency \"icu\". A ticket has been opened in [here](https://github.com/tensorflow/tensorflow/issues/54517) but no answers so far. \r\n\r\nThank you again for your support, "]}, {"number": 54242, "title": " symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS monterey 12.0.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version: r2.7\r\n- Python version: python3.9\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source): bazel 3.7.2\r\n- GCC/Compiler version (if compiling from source):  Apple clang version 13.0.0 (clang-1300.0.29.3)\r\n- CUDA/cuDNN version: no enabled\r\n- GPU model and memory:: no enabled(Just CPU model)\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhen running the \"import tensorflow as tf\" , I got the exception below:\r\n\"Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/usr/local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/xxxxx/PycharmProjects/pythonProject1/tensorflow/testdebug.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 79, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/usr/local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\r\n\r\nProcess finished with exit code 1\r\n\"\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n steps:\r\n \r\n0) download source code and \" git checkout r2.7\"\r\n1)  ./configure  (choose \"N\" for all of the questions)\r\n2)  bazel build --config=dbg --strip=never -c dbg --copt='-g' --cxxopt='-g'   //tensorflow/tools/pip_package:build_pip_package\r\n3) ./bazel-bin/tensorflow/tools/pip_package/build_pip_package pkt2/tensorflow_pkg\r\n4) pip install /Users/xxxxx/ai/tf/tensorflow/pkt2/tensorflow_pkg/tensorflow-2.7.0-cp39-cp39-macosx_12_0_x86_64.whl --force-reinstall\r\n5) try to run \"import tensorflow as tf\" and got the exception:\"ImportError: dlopen(/usr/local/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE'\r\n\"\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["BTW, got the same exception if using the build command \"bazel build --config=dbg --per_file_copt=+tensorflow/core/kernels/identity_op.*@-g //tensorflow/tools/pip_package:build_pip_package\"", "Hi @sushreebarsa  , do you have any idea or can duplicate for this issue?", "The issue also happens at TF2.8 (all of the other configs and commands are the same as  TF2.7) ", "@shenyufeng1234,\r\nCan you please take a look at [link1](https://github.com/tensorflow/tensorflow/issues/41682), [link2](https://stackoverflow.com/a/70305743/14290681) which discusses about the similar issue and let us know if it helps? Thanks!", "> \r\nlink1 \uff09\uff0cNo,I already updated Xcode to the most recent.\r\nlink2\uff09It is a boost-python related error,however my case is not the boost-python related error,", "I found a \"-force_flat_namespace\" option for Clang of macos (I don't know if it is helpful) ,however I cannot use it with command \"bazel build\", it is not supported:\"ERROR: -force_flat_namespace :: Invalid options syntax: -force_flat_namespace\".", "I found a big diff between r2.6 and r2 .7, the variable  \"__ZN4mlir2TF6detail25ResourceAliasAnalysisInfo18kMaxResourceTypeIdE\"  (static constexpr int64_t kMaxResourceTypeId  )doesn't exist in  r2.6, it is new from r2 .7 to master, maybe the new  variable  kMaxResourceTypeId cuased the issue at the mac os.\r\n\r\nIt seems TF needs to move the two variables \"static constexpr int64_t kMaxResourceTypeId = 9999;\" and \"static constexpr int64_t kUnknownResourceId = -1;\" from resource_alias_analysis.h to resource_alias_analysis.cc ,otherwise it would cause the issue at mac os.", "BTW ,if I build with \"bazel build --config=opt  //tensorflow/tools/pip_package:build_pip_package  \" everything is ok,  however if I build with  enable any debug info , it would be failed with exception above.", "I was trying to build with a different command but encountered the same issue. Adding the following one line fix this issue for me.\r\n\r\n```diff\r\ndiff --git a/tensorflow/compiler/mlir/tensorflow/analysis/resource_alias_analysis.cc b/tensorflow/compiler/mlir/tensorflow/analysis/resource_alias_analysis.cc\r\nindex 5c366896349..40ef08483d5 100644\r\n--- a/tensorflow/compiler/mlir/tensorflow/analysis/resource_alias_analysis.cc\r\n+++ b/tensorflow/compiler/mlir/tensorflow/analysis/resource_alias_analysis.cc\r\n@@ -262,6 +262,7 @@ bool IsResourceAllocatingOp(Operation* op) {\r\n }  // namespace\r\n \r\n constexpr int64_t ResourceAliasAnalysisInfo::kUnknownResourceId;\r\n+constexpr int64_t ResourceAliasAnalysisInfo::kMaxResourceTypeId;\r\n \r\n void IncrementResourceTypeId(int64_t& resource_type_id) {\r\n   if (resource_type_id == ResourceAliasAnalysisInfo::kMaxResourceTypeId) {\r\n\r\n```"]}, {"number": 54235, "title": "Cannot build tflite wheels for Windows", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows (github actions)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): cloned from github\r\n- TensorFlow version: from github\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): using cmake\r\n- GCC/Compiler version (if compiling from source): Visual Studio 17.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to build tensorflow lite wheels for Windows on the github actions runner. I am using the `build_pip_package_with_cmake.sh` I can get compiler to run, but in the end it fails with 1738 errors. Here is a small snippet of these:\r\n\r\n```\r\n(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4977,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4977,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4978,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4978,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4979,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4979,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4980,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(4980,1): error C2780: 'bool flatbuffers::Table::VerifyField(const flatbuffers::Verifier &,flatbuffers::voffset_t,size_t) const': expects 3 arguments - 2 provided [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(5067,12): error C2672: 'flatbuffers::Table::VerifyField': no matching overloaded function found [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n         D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow/lite/schema/schema_generated.h(5067,1): fatal error C1003: error count exceeds 100; stopping compilation [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\tensorflow-lite.vcxproj]\r\n```\r\n\r\nBuilding wheels for MacOS and Linux works fine. I would be open for any suggestions. The goal is to make it possible to get wheels for Windows. I know that there are wheels available from coral, but not for Python 3.10.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n``` \r\ntensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh windows\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Looks like recently-introduced breakage due to a recent change to the flatbuffers library:\r\n\r\nhttps://github.com/google/flatbuffers/commit/a42e898979cc828a35aefcc469fec896175fa8e6#diff-b141302ec1fbb79cf5dcc83cc8a211f34556c807361e58b18f3dc2b2f33faf06\r\n\r\nThat change modified the declaration of flatbuffers::Table::VerifyField to take an extra argument.\r\n\r\nSo one possible work-around is to use an earlier version of the flatbuffers library.", "Ah, this is a mismatch between the file\r\n`tensorflow/lite/schema/schema_generated.h`\r\nand the new version of the flatbuffers library.\r\nThat file is automatically generated by the flatbuffers tool, and I guess it may be using internal details in the flatbuffer headers that might not be considered part of the flatbuffers API?\r\nSo another, better work-around would be to regenerate that file, which I think you can probably do using a command something along the lines of\r\n\r\n```\r\n  flatc -I ./ --no-union-value-namespacing --gen-object-api --gen-generated -c schema.fbs\r\n```\r\n\r\n(possibly with some additional \"-I\" or \"-o\" options).", "Thanks for looking into this @fergushenderson! I will try to look into your suggestions.\r\n\r\nI failed to mention that I first had to patch the flatbuffers CMakeLists.txt to turn off the `/WX` (warnings becomes errors) flag. Some warnings were emitted that failed the compilation. Perhaps these are clues:\r\n\r\n```\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(757,1): error C2220: the following warning is treated as an error [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(757,1): error C2220: the following warning is treated as an error [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(757,1): error C2220: the following warning is treated as an error [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\istream(517,1): error C2220: the following warning is treated as an error [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n```\r\n\r\nContext for one of these:\r\n\r\n```\r\n   52>C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(757,1): error C2220: the following warning is treated as an error [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n       C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(724): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n                 with\r\n                 [\r\n                     _Ty=uint8_t\r\n                 ]\r\n       D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n                 with\r\n                 [\r\n                     _Ty=uint8_t\r\n                 ]\r\n       D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n    52>C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(757,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n     3>ClCompile:\r\n         C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\bin\\HostX64\\x64\\CL.exe /c /IC:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\Include /I\"C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\lib\\site-packages\\pybind11\\include\" /I\"C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\lib\\site-packages\\numpy\\core\\include\" /I\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\xnnpack\\include\" /I\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\xnnpack\\src\" /I\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\clog\\deps\\clog\\include\" /I\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\cpuinfo\\include\" /I\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\pthreadpool-source\\include\" /I\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\FXdiv-source\\include\" /I\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\FP16-source\\include\" /Zi /nologo /W1 /WX- /diagnostics:column /Od /Ob0 /D _MBCS /D XNN_LOG_LEVEL=5 /D restrict= /D _WIN32_WINNT=0x0601 /D EIGEN_MPL2_ONLY /D NOMINMAX=1 /D XNN_ENABLE_ASSEMBLY=1 /D XNN_ENABLE_MEMOPT=1 /D XNN_ENABLE_SPARSE=1 /D CPUINFO_SUPPORTED_PLATFORM=1 /D PTHREADPOOL_NO_DEPRECATED_API=1 /D FXDIV_USE_INLINE_ASSEMBLY=0 /D \"CMAKE_INTDIR=\\\"Debug\\\"\" /Gm- /RTC1 /MDd /GS /fp:precise /Zc:wchar_t /Zc:forScope /Zc:inline /Fo\"XNNPACK.dir\\Debug\\/src/qs8-vaddc/gen/minmax-sse2-mul16-ld64-x8.c.obj\" /Fd\"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\xnnpack-build\\Debug\\XNNPACK.pdb\" /external:W1 /Gd /TC /wd4146 /errorReport:queue \"D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\xnnpack\\src\\qs8-vaddc\\gen\\minmax-sse2-mul16-ld64-x8.c\"\r\n    17>ClCompile:\r\n         charconv_parse.cc\r\n    52>ClCompile:\r\n         idl_gen_text.cpp\r\n     3>ClCompile:\r\n         minmax-sse2-mul16-ld64-x8.c\r\n    17>ClCompile:\r\n         memutil.cc\r\n         match.cc\r\n         numbers.cc\r\n    52>C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(757,1): error C2220: the following warning is treated as an error [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n       C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.30.30705\\include\\vector(724): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [D:\\a\\tflite-runtime-wheels\\tflite-runtime-wheels\\tensorflow\\tensorflow\\lite\\tools\\pip_package\\gen\\tflite_pip\\python3\\cmake_build\\_deps\\flatbuffers-build\\flatbuffers.vcxproj]\r\n                 with\r\n                 [\r\n                     _Ty=uint8_t\r\n                 ]\r\n```", "Isn't the tflite build always using flatbuffers v1.12 by the way? `tensorflow/lite/tools/cmake/modules/flatbuffers.cmake` is using that tag and I checked that this is the one being used."]}, {"number": 54231, "title": "Make necessary validations in the file image_ops_impl.py and array_ops.py", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/54115 and https://github.com/tensorflow/tensorflow/issues/54116 and https://github.com/tensorflow/tensorflow/issues/54848\r\n\r\n<br> \r\n<b>What does this PR do? </b><br><br>\r\n1) As per the documentation for   `tf.image.adjust_brightness` (https://www.tensorflow.org/api_docs/python/tf/image/adjust_brightness) and `tf.image.ajust_hue` (https://www.tensorflow.org/api_docs/python/tf/image/adjust_hue?hl=en) the parameter delta in both the functions needs to be in the range [-1,1], this PR adds necessary validations in image_ops_impl.py to ensure that\r\n<br><br>\r\n2) As per the documentation of 'tf.linalg.tensor_diag_part' the input matrix should be of the rank 2k, however there are no validations for this, I have added those validations,\r\n", "comments": ["Thank you for your pull request. I have some comments from an initial triage tell you in advance about what potentially might have to be defended, changed or otherwise addressed. This was NOT a review and NOT a final decision. While you are welcome to address any or all of these now if it is convenient, you can also wait for comments from a review.\r\n\r\n* A potential performance concern is the computation of min and max in `adjust_brightness`.\r\n\r\n* A potential concern is that this Python-only change will not add error checking for graph mode.\r\n\r\n* Instead of adding error checking, should documentation be changed to say like that `delta` has an expected range and \"safe but unspecified behavior occurs if it is out of range\"?\r\n\r\nThank you for your patience as this is reviewed.", "> Thank you for your pull request. I have some comments from an initial triage tell you in advance about what potentially might have to be defended, changed or otherwise addressed. This was NOT a review and NOT a final decision. While you are welcome to address any or all of these now if it is convenient, you can also wait for comments from a review.\r\n> \r\n> * A potential performance concern is the computation of min and max in `adjust_brightness`.\r\n> * A potential concern is that this Python-only change will not add error checking for graph mode.\r\n> * Instead of adding error checking, should documentation be changed to say like that `delta` has an expected range and \"safe but unspecified behavior occurs if it is out of range\"?\r\n> \r\n> Thank you for your patience as this is reviewed.\r\n\r\n@SeeForTwo thanks for your time! I will surely look into this. Actually I was told that using tf - api functions in python is graph-mode friendly, thus had used reduce_min and reduce_max. But your point on performance seems valid ", "Please don't use `update <file>` as title. It makes it harder later to understand what the commit does when looking at the change history", "> Please don't use `update <file>` as title. It makes it harder later to understand what the commit does when looking at the change history\r\n\r\n@mihaimaruseac can I please know some set of rules to keep a pr name, asking this as I am new to this", "Hi, please for example have a look at https://reflectoring.io/meaningful-commit-messages/\r\n\r\nIf you have just one commit in the PR, the title will be based on that. If you have more commits, pick a title that tells a story about all commits (and maybe also add more text in the first comment)", "> Hi, please for example have a look at https://reflectoring.io/meaningful-commit-messages/\r\n> \r\n> If you have just one commit in the PR, the title will be based on that. If you have more commits, pick a title that tells a story about all commits (and maybe also add more text in the first comment)\r\n\r\nThanks a lot @mihaimaruseac ", "The issue is actually in PyObject conversion side I believe. It should have been fixed in: https://github.com/tensorflow/tensorflow/pull/54441", "> The issue is actually in PyObject conversion side I believe. It should have been fixed in: #54441\r\n\r\n@yongtang did not know about it, thanks for mentioning, however I believe the necessary validations for image_ops_impl.py still are necessary, will remove the changes I made in math_ops.py", "Can you make sure the required builds are all passing, please?", "@mihaimaruseac for the py+cpp test suite I am getting a build error because the array in the test case does not have a rank of 2k, so my modified code is throwing a value error as written in the code, and in the issue and documentation of TF, it says the rank should 2k itself, so can you see if this an error in writing tests or the issue and docs are wrong?", "Please add/update the tests too."]}, {"number": 54221, "title": "Allow to change thread timeout in collective_ops with an ENV variable", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.6.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis issue happens with XLA when using JAX in a multi-host system, where I get `This thread has been waiting for 5000ms for and may be stuck` when the pmap function is getting compiled and rapidly followed by `Thread is unstuck!  Warning above was a false-positive.`. This gets annoying when I know that it is not really stuck and I get it on most of the devices (so the log gets repeated `num_devices` times). \r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nLooking at the code, the 5000ms are hardcoded. Because I understand the need to log these cases, I would propose adding an ENV var where you could change the 5000ms to be another value, and if the variable is not set, then keep the 5000ms which works on most cases. \r\n\r\nIt would only require to modify this function at `tensorflow/tensorflow/compiler/xla/service/collective_ops_utils.h`\r\n```\r\ntemplate <typename DescFn>\r\nvoid WaitAndLogIfStuck(tensorflow::BlockingCounter* counter,\r\n                       const DescFn& desc_fn) {\r\n  VLOG(3) << \"Begin: \" << desc_fn();\r\n  const std::chrono::milliseconds timeout(5000);\r\n  bool ok = counter->WaitFor(timeout);\r\n  if (ok) {\r\n    VLOG(3) << \"Finished: \" << desc_fn();\r\n    return;\r\n  }\r\n  LOG(ERROR) << \"This thread has been waiting for \" << timeout.count()\r\n             << \"ms for and may be stuck: \" << desc_fn();\r\n  counter->Wait();\r\n  LOG(ERROR) << \"Thread is unstuck!  Warning above was a false-positive.  \"\r\n                \"Perhaps the timeout is too short: \"\r\n             << desc_fn();\r\n}\r\n```\r\n**Who will benefit with this feature?**\r\n\r\nCleaner logs and avoid known false positive.\r\n\r\n\r\n**Any Other info.**\r\n\r\nAlso, there's a `for` repeated on the log which could be fixed in the same PR :)\r\n", "comments": ["I can happily do a quick PR if this is something you may want and I get the env var name :)"]}, {"number": 54220, "title": "TensorFlow Lite CMake dependencies versions automatically synced with Bazel configuration", "body": "With Bazel being the preferred build system for `TensorFlow`, `TensorFlow Lite` CMake build is determining versions of its dependencies based on those listed in Bazel configuration files. The corresponding version update in CMake configuration, however, needs to be done **manually** every time the relevant Bazel configuration changes.\r\n\r\nThis PR implements a mechanism to **automatically** synchronize `TensorFlow Lite` CMake dependencies based on values currently present in Bazel configuration files.\r\n\r\n", "comments": ["@madaosik Can you please resolve conflicts? Thanks!", "> @terryheo conflicts were resolved.\r\n\r\n", "Hi @madaosik Can you please check @terryheo's comments and resolve conflicts?. Thank you!"]}, {"number": 54217, "title": "corrections to cross compilation instructions", "body": "* added fortran compiler to the prerequisites\r\n* made a note that the cmake command is run from the tflite_build directory and adapted the path\r\n* added the missing  \"mkdir -p ${HOME}/toolchains\" to the commands for the Pi Zero\r\n* added a note on the BUILD_NUM_JOBS parameter", "comments": ["@WillemD61 Can you please sign CLA. Thanks!", "Thank you @WillemD61 ! Would you be able to format certain items (check out https://developers.google.com/style/code-in-text?hl=en#some-specific-items-to-put-in-code-font, https://developers.google.com/style/code-syntax)\r\n\r\ncc @mihaimaruseac ", "Hi @WillemD61 Can you please check @8bitmp3's comments and keep us posted ? Thank you!"]}, {"number": 54197, "title": "Extra GPU-CPU memory transfer when broadcasting operations between integer tensors", "body": "Hello,\r\n\r\n<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): v2.6.1-9-gc2363d6d025 2.6.2\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version: NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5\r\n- GPU model and memory: GeForce 1080Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThe code runs fine but when we look in detail in Tensorboard we observe that there are silent memory transfers between CPU and GPU, when doing operation that involve integer tensors that need broadcasting.\r\n\r\nIn the picture below (The green block \"MemCpyH2D\" in the Stream23, that has been clicked to show the name of the culprit op, should not exist) : \r\n\r\n![2022-01-29-175200_1920x1080_scrot](https://user-images.githubusercontent.com/11304248/151670751-2498d7a0-a26a-415e-9f2d-0c6f92312d12.png)\r\n\r\n**Describe the expected behavior**\r\nI expect 0 memory transfer for ops as simple as additions, bitwise element operation like bitwise_and with a mask that is a constant, integer reduction along various axis, expand_dims, reshape and squeeze, gather_nd and scatter_nd should also not result in data transfer between CPU and GPU\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nCreate a log folder if necessary\r\nAnd install tensorboard profiler (See https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras ) : \r\n`pip install -U tensorboard_plugin_profile`\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom datetime import datetime\r\n\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        #N must be divisible by 4\r\n        super(CustomLayer, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(100,use_bias=False)\r\n\r\n    @tf.function\r\n    def call(self, query):\r\n        c = tf.cast(query,tf.int32,name=\"castQueryToInt32\")\r\n        d = tf.expand_dims( c, axis=1,name=\"expandDimsAxis1\")\r\n        e = tf.expand_dims( c, axis=2,name=\"expandDimsAxis2\")\r\n        g = tf.add(d , e ,name=\"additionBroadcasted\")\r\n        #h = tf.reduce_sum(g, axis=1) #This also create extra memory transfer between gpu and cpu\r\n        f = tf.cast(g,tf.float32, name=\"castGToFloat\")\r\n        f = tf.reduce_sum(f,axis=1,name=\"reduceSumF\")\r\n        rem = (query - f)* (query - f)\r\n        out = self.dense1(rem)\r\n        return out\r\n\r\n\r\n\r\ninputs = tf.keras.Input(shape=(100,))\r\nout = CustomLayer()(inputs)\r\n\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=out)\r\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),loss=\"mse\")\r\n\r\n\r\nxtrain = tf.random.uniform((600000,100),dtype=tf.float32)\r\nytrain = tf.random.uniform((600000,100),dtype=tf.float32)\r\n\r\nlogs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\n#We investigate the slowness due the extra memory transfer between GPU and CPU because of broadcasting behavior of integer tensors\r\ntboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,\r\n                                                 histogram_freq=1,\r\n                                                 profile_batch=\"10,20\")\r\n\r\nmodel.fit( xtrain,ytrain, batch_size=100,epochs=1,callbacks=[tboard_callback])\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI had simple code (additions and reductions between integer tensors of broadcastable shapes)  that had very low GPU utilization (25% and abysmal performance 0.1x), so I investigate with Tensorboard, observed plenty of non-necessary GPU-CPU transfer and extracted this simple example. (Once you fix this example, I can provide additional test cases).\r\n\r\nMaybe some additional ops need to be registered/created to handle the integer tensors properly.\r\n\r\nThe ugly work-around is to cast every int32 to float64 do the operations that need broadcasting there and cast back to int32, it almost works (except for the backward pass of a gather_nd that I haven't managed to suppress yet).\r\nI have tried using repeat with integer tensor but it result in extra transfer too.\r\n\r\nThank you", "comments": ["Additional related bug : \r\nWhen I try to multiply integer tensor of the same shape on the GPU (or by an integer scalar), they also get moved to the CPU.\r\n\r\nI looked at the code and it seems the culprit is here : \r\nhttps://github.com/tensorflow/tensorflow/blob/fd16fa9fc6741bc7bb7fcc9dd70c1e527834c133/tensorflow/core/kernels/cwise_op_mul_1.cc#L39-L48\r\n\r\nCan you also fix it please ?\r\n\r\nThanks", "@unrealwill \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "@sushreebarsa Here is the cross-post : https://github.com/keras-team/keras/issues/15986 \r\n\r\nImho, this issue is not a Keras one but a tensorflow one, as it has nothing to do with the CustomLayer, but everything to do with the low level op like additions and multiplications not properly registered and defined for the GPU.\r\n\r\nThanks", "@unrealwill Sorry for the late response!\r\nAs we see that you have raised this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues), please move this issue to closed status as we will track the other ticket there.Thanks!", "@sushreebarsa No sorry, I won't close this thread here until it has been resolved as all the information is in this thread.\r\nFurthermore, no one has answered the case there in 14 days, and the bug is not a Keras issue but a Tensorflow one.\r\n\r\nSending bugs to die in other hidden threads is not the way to increase your \"closed case\" performance metric. \r\n\r\nThis bug is a basic low-level bug about integer tensors (we are talking about addition and multiplication here) that silently hit, with a huge performance impact, a lot of advanced users that try to use Tensorflow for state of the art research in everything that deals with sparsity and hashes. \r\n\r\nYou should be glad I saw the hard to spot bug and reported it. You should have fixed it already !\r\n", "@chunduriv Was able to reproduce on colab using TF v2.6.0 , 2.8.0 ,please find the attached gists for [cpu ](https://colab.research.google.com/gist/sushreebarsa/e3c1e12aabe6f9b551c0f6b40c997a24/reproduce.ipynb#scrollTo=ddQbVvIJAXTz)and [gpu](https://colab.research.google.com/gist/sushreebarsa/c56fb812c7b5697267425c2174c806fe/reproduce.ipynb#scrollTo=ddQbVvIJAXTz) .Thanks!", "Wondering if you happened to try with `tf.device` to attempt to ping the ops on CPUs only? If not, that may be a reasonable solution to try.", "@unrealwill,\r\nCan you please check above [comment](https://github.com/tensorflow/tensorflow/issues/54197#issuecomment-1043283380) by @rchao and let us know? Thanks!", "@chunduriv I tried to use tf.device(\"/GPU:0\") to force the computation to happen on the GPU, (I don't care at all about the CPU performance) but it has no effect whatsoever."]}, {"number": 54173, "title": "Fix OpenCL GPU Delegate segfault on exit on NVidia ", "body": "- AssignOffsetsToTensors with GREEDY_BY_SIZE causes a segfault on NVidia\r\n- Fixed undefined/duplicate definition error for Android CMake build of C++ API", "comments": ["Someone from TFLite team is better suited to review these.", "@terryheo Please see this issue https://github.com/tensorflow/tensorflow/issues/53800 for more details on the bug.", "@DwayneDuane  Can you please resolve conflicts? Thanks!", "@gbaned Done.", "These memory strategies are implementing the algorithms described in https://arxiv.org/abs/2001.03288 and have larger implications on runtime memory footprint for existing apps.  I would rather have someone fix it for real, rather than setting it to something that works for their particular use case which isn't even fully supported (desktop use cases are not fully supported).\r\n\r\nIf you're not comfortable with academic papers, here's a dumbed down blog post version of it: https://blog.tensorflow.org/2020/10/optimizing-tensorflow-lite-runtime.html", "I am not doubting the validity of the algorithm, merely that there could be a bug in the implementation. Even if desktop use case if not fully supported, the fact that a segfault can be reproduced on desktop only hints at some undefined behavior somewhere. In my experience, Qualcomm's OpenCL can be more lax than NVidia's when it comes undefined behaviors at runtime. I agree that it would be best to fix this for real. Could you bring this to the attention of the person who originally implemented this to have them take a second look?", "Do you plan to run it via C++ or Python Binding/API @DwayneDuane \r\nhttps://github.com/tensorflow/tensorflow/issues/55522\r\nhttps://github.com/tensorflow/tensorflow/pull/38806", "> Qualcomm's OpenCL can be more lax than NVidia's when it comes undefined behaviors at runtime\r\n\r\nThis is absolutely true.  We do cut corners and rely on undefined behavior of the supported platforms, e.g. out of bound reads etc.", "@DwayneDuane /@impjdi  Any update on this PR? Please. Thank you!", "I think that relying on undefined behaviors is dangerous, even if they happen to work on platforms that you care about. For example, Qualcomm might change their OpenCL driver tomorrow, and as a result, what is a harmless out-of-bound read today could cause a hard crash upon the release of the next Android security patch. I understand that switching to MemoryStrategy::EQUALITY may impact memory footprint on existing apps, but leaving this unaddressed could be equal dangerous. If the Tensorflow team has no bandwidth to fix the root issue here, could you at least add an option to switch between MemoryStrategy::EQUALITY and MemoryStrategy::GREEDY_BY_SIZE? You could adopt MemoryStrategy::GREEDY_BY_SIZE as default value.", "> could you at least add an option to switch between MemoryStrategy::EQUALITY and MemoryStrategy::GREEDY_BY_SIZE?\r\n\r\nYeah, this should be doable.  I'll ask.", "I forwarded your bug to the owner of the code base, and he came back with this response, copy&pasted below:\r\n\r\n> Hi,\r\n> \r\n> can you try to change 224 line in gpu/cl/inference_context.cc(inside GetBufferAsignment,\r\n> *is_sub_buffers_supported = ...)\r\n> to *is_sub_buffers_supported = false?\r\n> If this will solve problems with your hardware, we can add this fix:\r\n> *is_sub_buffers_supported = (*is_sub_buffers_supported && !gpu_info.IsNvidia());\r\n> \r\n> Second what we can try(if first solution works), is to change gpu/cl/environment.cc line 262,\r\n> bool CanUseSubBufferForImage2d(const GpuInfo& gpu_info). Inside of CanUseSubBufferForImage2d\r\n> you can try to add if (gpu_info.IsNvidia()) {return false;}\r\n> Can you try both and told us results?\r\n> \r\n> Thank you very much", "I tested both method on the v2.9.0-rc0 branch and I can confirm that in both cases the segfault on exit are eliminated. Even without any change to `GetBufferAsignment` in gpu/cl/inference_context.cc(, adding `if (gpu_info.IsNvidia()) {return false;}` to `CanUseSubBufferForImage2d` is sufficient."]}, {"number": 54171, "title": "TensorArray.stack should accept axis as argument", "body": "**Describe the feature and the current behavior/state.**\r\n\r\nCurrently there is:\r\n```python\r\nTensorArray.stack(name=None) -> tf.Tensor\r\n```\r\n\r\nIf we want to have stacking along a different axis, we must manually transpose using:\r\n```python\r\nmyTensorArray = tf.TensorArray(dtype=tf.float32, size=6, element_shape=(2, 3))\r\n# [6, 2, 3]\r\nres = myTensorArray.stack()\r\naxis = 1\r\n*perm, = range(len(res.shape))\r\nperm[0], perm[axis] = perm[axis], perm[0]\r\n# [2, 6, 3]\r\nres_correct = tf.transpose(res, perm)\r\n```\r\n\r\nNOTE: I could have directly written out the `perm` parameter, but what if you're writing a library where you want to be able to stack along user-defined axis?\r\n\r\n**Will this change the current api? How?**\r\n\r\nNew definition could directly do this:\r\n```ptyhon\r\nTensorArray.stack(axis=0, name=None) -> tf.Tensor\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who uses `TensorArray.stack` and doesn't want to stack on 0th dimension.\r\n\r\n**Any Other info.**\r\n\r\nPerformance implication of stacking along non-0 axis should be considered and clearly explained in docs if significant.\r\n\r\nEDIT: Other APIs in TensorArray should also be considered to be consistent. For example\r\n```python\r\nTensorArray.concat(name=None) ->tf.Tensor\r\nTensorArray.unstack(value, name=None) ->tf.Tensor\r\n```\r\n", "comments": []}]