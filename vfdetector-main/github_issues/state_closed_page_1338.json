[{"number": 12951, "title": "--config=mkl leads to libmklml_intel.so: error: undefined reference to 'dladdr'", "body": "Cloning master (702d595822e9e5f5232b8140c6296683612c33a9), running `configure` with \"No\" throughout, and trying to build a pip package (Bazel 0.5.4, Ubuntu 16.10) with the new `--config=mkl` flag crashes.\r\n\r\n```sh\r\nbazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n\r\nERROR: /home/carl/tensorflow/tensorflow/python/BUILD:1289:1: Linking of rule '//tensorflow/python:gen_resource_variable_ops_py_wrappers_cc' failed (Exit 1)\r\nbazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so: error: undefined reference to 'dladdr'\r\n```\r\n\r\n```sh\r\nldd bazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so\r\n\r\nlibiomp5.so => not found\r\n```\r\n\r\n", "comments": ["@tfboyd do you know what the issue is?", "495cc8e474e608bbd0b6c5bf91d5a22962ede7cd  (Head/Master synced at 1:15 PST 11-SEP for reference)\r\n./configure all defaults\r\nbazel build --c opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbazel 0.5.2\r\npython 2.7\r\nUbuntu 14.04 (internally customized but core is the same)\r\n\r\nCompiled fine, maybe there was an issue at head when you synced.  Closing but feel free to comment if you have more details.  Tags are normally safer, but Gunhan was working on the build scripts for this and I am not sure everything ended up on 1.3.  If you do not mind 1.2.1, I know that Tag works and you can build with this\r\n```bazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package```\r\n\r\nI do not think ```--copt=\"-DEIGEN_USE_VML\"``` does anything those were the instructions and I what I uses so I like to stick with it.  If you want a precompiled binary [here](https://storage.googleapis.com/tf-performance/tf_binary/tensorflow-1.2.0.tag.12f033d.MKL_NOGPU-cp27-cp27mu-linux_x86_64.whl) is a link to 1.2 that I did for Python 2.7 linux.  Sharing just because I have it.\r\n\r\n\r\n", "Hmm, very strange. Could you share your `cat /proc/cpuinfo`? I'm running on a Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz.\r\n\r\n```sh\r\ngit clone https://github.com/tensorflow/tensorflow\r\n\r\ngit checkout 495cc8e474e608bbd0b6c5bf91d5a22962ede7cd\r\n\r\ncd tensorflow\r\n\r\nPYTHON_BIN_PATH=/home/carl/anaconda3/bin/python\r\n USE_DEFAULT_PYTHON_LIB_PATH=1 CC_OPT_FLAGS=\"-march=native\"\r\n  TF_ENABLE_XLA=0 TF_NEED_MKL=0 TF_NEED_MPI=0 TF_NEED_JEMALLOC=0 TF_NEED_GCP=0 TF_NEED_HDFS=0\r\n  TF_NEED_VERBS=0 TF_NEED_OPENCL=0 TF_NEED_CUDA=0 TF_NEED_GDR=0 ./configure\r\n\r\nbazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n> error: undefined reference to 'dladdr'\r\n\r\nRemoving `--config=mkl` works fine.", "And just to be sure it's not Anaconda related, I tried `PYTHON_BIN_PATH=/usr/bin/python` as well (Python 2.7.12+) but got the same error.", "All the data I can think to share:  \r\nPython 2.7.6\r\nI did have jemalloc as that is a default.\r\n\r\nMy CPU, which would not matter.\r\nmodel name\t: Intel(R) Xeon(R) CPU E5-1650 v2 @ 3.50GHz\r\n\r\nYou might want to try by just running ./configure and not using the ENV variables.  It is possible I did something weird, but I am very certain (some margin for error) that I just git cloned did ./configure (enter enter enter enter :-)) and ran the build.\r\n\r\nI do not know this is frustrating.  I am not sure what the problem might be.  \r\n", "Hey guys, i have a different but related issue, i compiled with --config=mkl, in my case it compiles fine and creates the wheel file, i install it fine, but when i create a python session and do import tensorflow as tf, i get  libmklml_intel.so: cannot open shared object file: No such file or directory , any ideas(that do not include remove --config=mkl )?", "@llealgt maybe consider opening a separate issue for this?", "So, any progress? i have similar errors when compiling tensorflow v1.4.0.\r\n\r\n```/opt/rh/devtoolset-6/root/usr/bin/gcc -o bazel-out/host/bin/tensorflow/python/gen_lookup_ops_py_wrappers_cc '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_Stensorflow_Spython_Cgen_Ulookup_Uops_Upy_Uwrappers_Ucc___Utensorflow' '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib' -Lbazel-out/host/bin/_solib_k8/_U_S_Stensorflow_Spython_Cgen_Ulookup_Uops_Upy_Uwrappers_Ucc___Utensorflow -Lbazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/opt/rh/devtoolset-6/root/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,-S -Wl,@bazel-out/host/bin/tensorflow/python/gen_lookup_ops_py_wrappers_cc-2.params)\r\nbazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so: error: undefined reference to 'dlopen'\r\nbazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so: error: undefined reference to 'dlerror'\r\nbazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so: error: undefined reference to 'dlsym'\r\nbazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so: error: undefined reference to 'dladdr'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n```\r\nldd bazel-out/host/bin/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib/libmklml_intel.so\r\n\tlinux-vdso.so.1 =>  (0x00007ffca8917000)\r\n\tlibiomp5.so => not found\r\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007fc4f8c30000)\r\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007fc4f892e000)\r\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007fc4f856b000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x000055607a1eb000)\r\n```\r\n\r\n", "@tfboyd This issue still happens when mkl is enabled on  [tensorflow 1.5.0 release](https://github.com/tensorflow/tensorflow/archive/v1.5.0.zip) for the following configuration:\r\n\r\n- Centos 7 docker image \r\n- Devtoolset 6 \r\n- Python 27\r\n- bazel 0.8.0\r\n\r\nSome of the things I tried:\r\n- Passing `--linkopt=-ldl` to `bazel build`.\r\n- Patch `mk/BUILD` to accept `linkopts = [\"-ldl\"]`\r\n- Patch `tf_extension_linkopts` in `tensorflow/tensorflow.bzl` to `return [\"-ldl\"]`.\r\n\r\nNone of them seem to work. \r\n\r\n----\r\n\r\nThe command used to build:\r\n\r\n```\r\nexport CC_OPT_FLAGS=\"-march=x86-64\"\r\n\r\nexport TF_NEED_GCP=\"1\"\r\nexport TF_NEED_MKL=\"1\"\r\nexport TF_NEED_JEMALLOC=\"0\"\r\nexport TF_NEED_HDFS=\"1\"\r\nexport TF_NEED_S3=\"0\"\r\nexport TF_NEED_GDR=\"0\"\r\nexport TF_NEED_CUDA=\"0\"\r\nexport TF_ENABLE_XLA=\"0\"\r\nexport TF_NEED_VERBS=\"0\"\r\nexport TF_NEED_OPENCL=\"0\"\r\nexport TF_NEED_MPI=0\r\n\r\nbazel clean\r\n./configure\r\n\r\nbazel build -c opt \\\r\n      --copt=-mfpmath=both --config=mkl \\\r\n      //tensorflow/tools/pip_package:build_pip_package\r\n      //tensorflow/java:tensorflow \\\r\n      //tensorflow/java:libtensorflow_jni\r\n```", "We need to compile gcc 6 from source code. We shouldn't use devtoolset. :)", "Removing the mkl option isn't a fix and that DEIGEN_USE_VML flag definitely does nothing. Getting same errors on Centos 6.5, Python 2.7, TF 1.6, Devtoolset 6. Any other viable solutions?\r\n\r\nbazel build --linkopt='-lrt' --verbose_failures --action_env PATH=\"$PATH\" --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package", "@agramesh1  Could somebody from Intel help to figure out why this build can't find OpenMP library?", "On some systems, esp. a few Ubuntu versions we have seen Tensorflow fail to build due to missing libraries when compiling with \u2013config=mkl. You will see errors similar to this\r\n```\r\nlibmklml_intel.so: error: undefined reference to 'dladdr' \r\n```\r\nThe linker is unable to find libdl.so. For a work around, you need a private copy of MKL with the missing library included in. Follow the steps below. \r\n\r\n1. Download the version of MKL used by the version of Tensorflow you are compiling, you can get the URL from the file tensorflow/workspace.bzl, search for mkl_repository.\r\n\r\n2.  Find the shared library libdl.so on your system ( it is usually in /usr/lib/) and copy over to the lib folder of the copy of MKL that you downloaded.\r\n\r\n3.  Set the environment variable TF_MKL_ROOT to  point to the copy of MKL from step above.\r\n\r\n4.  Modify the file third_party/mkl/BUILD as follows\r\n\r\n5. add a line containing the following  linkopts = [\"-ldl\",],  after the line containing   name = \"intel_binary_blob\", \r\n\r\n6  add a line containing \"@mkl//:libdl.so\",  after the line containing srcs = if_mkl([ \r\n\r\n7. run configure and build\r\n\r\nWe tried many ways to pass the -ldl flag through bazel but none of them worked, this is the best solution we have.", "Perhaps I didn't explain well about this question in the former comment. What I mean is that devtoolset-6 may lead to the ldopen or libiomp5.so not found errors. \r\nAnybody who faces this problem can try to build a gcc-6 from source code to fix it.\r\nbuild gcc-6:\r\n```\r\n$ tar xzf gcc-6.4.0.tar.gz\r\n$ cd gcc-6.4.0\r\n$ ./configure --prefix=/opt/gcc-6 --enable-languages=c,c++\r\n$ make\r\n$ make install\r\n$ export PATH=/opt/gcc-6/bin:$PATH\r\n$ export LD_LIBRARY_PATH=/opt/gcc-6/lib64:LD_LIBRARY_PATH\r\n```\r\nbuild tensorflow:\r\n```\r\n$ cd /path/to/tensorflow_src\r\n$ bazel clean\r\n$ rm .cache/bazel -rf\r\n$ ./configure\r\n$ bazel build --config=mkl -copt=\"-DEIGEN_USE_MKL_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nIt has been compiled successfully with gcc-6 on centos 7.2. But we have to take the compiled gcc library wherever we use tensorflow. \r\n\r\nMaybe there're some better ways to solve the problem, but this is the only way that i have found.", "@SarshesJ Thanks for letting us know. Glad to hear the solution worked for you.", "@agramesh1 On centos7, the issue was libmklml_intel.so was being linked ahead of libdl.so. Your suggestions helped, but the patch required is slightly different and can be seen here: https://github.com/pavanky/tensorflow/commit/bd90d1f086a830ad7cf8edf981954af00c7ef53c\r\n\r\n(I still followed the suggestions requiring custom MKL root as well as copying libdl)\r\n\r\nThis is based on tensorflow 1.7", "Updated instructions with latest TF, If there is still an issue like below\r\n```\r\n...libmklml_intel.so: error: undefined reference to 'dlopen'\r\n...libmklml_intel.so: error: undefined reference to 'dlerror'\r\n...libmklml_intel.so: error: undefined reference to 'dlsym'\r\n...libmklml_intel.so: error: undefined reference to 'dladdr'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\nThe linker is unable to find `libdl.so`. For a work around, you need a private copy of MKL with the missing library included in. Follow the steps below.\r\n\r\n1. Download the version of MKL used by the version of Tensorflow you are compiling, you can get the URL from the file `tensorflow/workspace.bzl`, search for `mkl_repository`.\r\n\r\n```\r\nwget https://github.com/intel/mkl-dnn/releases/download/v0.14/mklml_lnx_2018.0.3.20180406.tgz\r\ntar -xf mklml_lnx_2018.0.3.20180406.tgz\r\n```\r\n\r\n2. Find the shared library libdl.so on your system ( it is usually in /usr/lib/) and copy over to the lib folder of the copy of MKL that you downloaded.\r\n```\r\n#If you donot have locate, install locate tool and update the database for mlocate\r\nyum install -y mlocate\r\nupdatedb\r\n\r\n#Find the location of libdl.so\r\nlocate libdl.so\r\n# output would look like below\r\n# /usr/lib64/libdl.so\r\n# /usr/lib64/libdl.so.2\r\n\r\n#Usually libdl.so is a sym link, find the actual file.\r\nll /usr/lib64/libdl.so.2 \r\n#lrwxrwxrwx. 1 root root 10 Apr  7 11:38 /usr/lib64/libdl.so.2 -> libdl-2.17.so\r\n\r\n#Copy the libdl-2.17.so file to the downloaded MKL lib folder as libdl.so\r\ncp /usr/lib64/libdl-2.17.so <path_to_mklml_lnx_2018.0.3.20180406>/lib/libdl.so \r\n```\r\n3. Set the environment variable TF_MKL_ROOT to point to the copy of MKL from step above.\r\n```\r\nexport TF_MKL_ROOT=<path_to_mklml_lnx_2018.0.3.20180406>/\r\n```\r\n\r\n4. Modify the file `tensorflow/third_party/mkl/BUILD` to add the line `linkopts = [\"-ldl\",],` after the line  `name = \"intel_binary_blob\",`. It should look like below\r\n```\r\ncc_library(\r\n    name = \"intel_binary_blob\",\r\n    linkopts = [\"-ldl\",],\r\n    visibility = [\"//visibility:public\"],\r\n    deps = select({\r\n```\r\n\r\n5. Modify the file `tensorflow/third_party/mkl/mkl.BUILD` to add the line `\"lib/libdl.so\"`, after the line `name = \"mkl_libs_linux\", srcs = [`. It should look like below\r\n```\r\ncc_library(\r\n    name = \"mkl_libs_linux\",\r\n    srcs = [\r\n        \"lib/libdl.so\",\r\n        \"lib/libiomp5.so\",\r\n        \"lib/libmklml_intel.so\",\r\n    ],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\n\r\n6. run configure and build\r\n"]}, {"number": 12950, "title": "random_crop option to return the crop offset", "body": "I have a segmentation application where I need to know the selected random crop offset to correctly crop the label image in the same manner. This PR adds the `random_crop(get_offset=True)` option to return that additional information to the user.", "comments": ["Can one of the admins verify this patch?", "That looks a bit backward. Maybe passing in the precomputed random offset is better.", "(For API review: +1 to @drpng's suggestion: A new operation that requires the offset and crops to that offset sounds better)", "@drpngx wouldn't that either be redundant with `tf.slice` or be better named `tf.crop` since it is not random?\r\n\r\nI think for specific use cases like segmentation getting the behavior defined by `random_crop` and just having it feed you exactly what it cropped might be more useful. Or what if `random_crop` could take in additional tensors that would all be cropped the same way?", "Right, why not just use `tf.crop` and `tf.random`?", "Fair enough, I might as well just write it in user code. Thanks for the feedback!"]}, {"number": 12949, "title": "Very Slow Run time on A Pre Trained Network - Inception V1", "body": "------------------------\r\n\r\n### System information\r\n- **example script (Very Similar label_Image.py but simpler)**:\r\n- **Windows 10 X64 - TensorFlow installed from source **:\r\n- **TensorFlow version (1.3)**:\r\n- **Python version - 3.5.3**: \r\n- **Cmake 3.8.1**:\r\n- **CUDA/cuDNN version - CUDA 8.0/ cuDNN 6.1**:\r\n- **GPU model and memory - Nvidia GTX 1050, 4GB**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI encountered a disturbing issue regarding the run time for processing an arbitrary image through an inception V1/GoogleNet pre-trained model the runtime took approximately 9.2 ms\r\n\r\nWhen using the same image and exactly the same model with Caffe, It was running X4 faster than Tensorflow (2.3 ms).  The models  I used (both for Caffe and Tensorflow)  : \r\nhttps://github.com/beniz/deepdetect/issues/89\r\nLabels file is attached. \r\n\r\nHow is it even possible ?!?!  \r\nAm I doing something incorrectly ? \r\n\r\nBy the way, I carefully checked (with GPU-Z) and it seems Tensorflow  properly uses the GPU resources.\r\n\r\n### Source code / logs ###\r\nAttahced, along with the used label file. \r\nIn order to use the (python) script, simply put it in the same folder as the model and label file., and rename it to :  TF_Running_Inception.py\r\n\r\n[TF_Running_Inception.txt](https://github.com/tensorflow/tensorflow/files/1290863/TF_Running_Inception.txt)\r\n[imagenet_slim_labels.txt](https://github.com/tensorflow/tensorflow/files/1290864/imagenet_slim_labels.txt)\r\n\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nMore specifically, you should ask on StackOverflow when debugging performance problems with your own script. If you find a performance issue with a TensorFlow op, model or any other specific part of Tensorflow, then file an issue. Try [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) if you're benchmarking."]}, {"number": 12948, "title": "feature request: shared memory concat with new allocation for subsequent operations", "body": "DenseNet is an effective network design that relies on applying nn layers on recursive concatenations of data along the channel axis. Unfortunately, this has the side effect of quadratic memory growth in TensorFlow as completely new blocks of memory are allocated after each concat operation, resulting in poor performance during all phases of execution.\r\n\r\nThis is a feature request for a new `allocation='shared'` option for operations such as `tf.concat(allocation='shared')` which works seamlessly with later operations that might modify the data such as BatchNorm, which might also need to share memory. This would make it is possible to utilize the [Memory-Efficient Implementation of DenseNets](http://arxiv.org/abs/1707.06990), a paper which demonstrates that this memory utilization can be dramatically reduced through sharing of allocations. This image from the paper + pytorch implementation illustrates the shared memory approach:\r\n\r\n![densenet shared memory](https://camo.githubusercontent.com/d3370ec4935b4bc92b736a122bc226abf48988fd/68747470733a2f2f7261772e6769746875622e636f6d2f67706c656973732f656666696369656e745f64656e73656e65745f7079746f7263682f6d61737465722f696d616765732f666f72776172642e706e67)\r\n\r\n- [Pytorch efficient DenseNet implementation](https://github.com/gpleiss/efficient_densenet_pytorch)\r\n- [Keras DenseNet Implementation](https://github.com/titu1994/keras-contrib/blob/ff47da56fbd54cf6cdc2ac2218529fbdadf99296/keras_contrib/applications/densenet.py) with \"naive\" allocations, works with TensorFlow backend.\r\n\r\nThis functionality would also be useful for any other application or future network design that employs recursive concatenations. Just in case I didn't find it in my search, perhaps a mechanism already exists that can meet these goals?\r\n", "comments": ["@zheng-xq, can you please comment?", "I'd like to provide some additional motivation for this feature request. \r\n\r\nI'm able to train a Resnet50 based architecture for my problem with a batch size of 32 on the older 12GB Titan X GPU. However, a DenseNet based architecture that should actually be fairly small, with 40 layers and 500k parameters can only fit a batch size of 6, produces some tf memory warnings, and runs 8x slower during training, leading a day of training to take over a week, and smaller batch sizes were even slower. Obviously this isn't a perfect 1:1 comparison since I was using the naive implementation, but the authors indicate they are able to create networks that outperform ResNet in training and evaluation performance on pytorch. To my knowledge, I don't think TF supports any easy to use operations that could be used to re-create this memory efficient implementation.\r\n\r\nDenseNet won the 2017 CVPR best paper award, so I think support for the capabilities necessary to implement DenseNet efficiently seems particularly worthwhile for TF. I'd look into implementing it myself but I've already used up my free cycles for infrastructure on a number of other contributions here and in Keras, so I'm sticking to ResNet based solutions at this time. Thanks for your consideration!\r\n", "Totally agree the necessity of memory shared feature. In addition, to implement memory efficient DenseNet, we also need a feature re-computation function. There are already memory efficient implementations of DenseNet on other platforms; I think Tensorflow, labeled as one of the most flexible platform, falls behind somewhat on this feature. Hope both would be added soon. Thank you.", "this issue is still valid", "is project started?, I'd look into implementing it myself ,but don't familiar enough\u3002", "I have the same issue when I'm using densenet from keras-contrib as well as my self implemented unet. Both are using a lot of concat operations. Hope this new feature can be prioritized. Thanks. ", "It looks like some might use tf.Defun for this, see https://github.com/tensorflow/tensorflow/issues/13160#issuecomment-331914859.\r\nA similar feature (or perhaps a different feature?) for reducing memory with tensorflow might have been implemented outside of tf itself, see https://github.com/tensorflow/tensorflow/issues/13160#issuecomment-357773335.\r\n", "I just want to say as for 3D medical images, the 3D DenseNet takes up even more memory dues to the extra dimension of parameters, pushing me to use only 1 or 2 images in a mini-batch, which causes a lot of headache issues during training (Batch normalization won't work in such a small batch, even the batch re-normalization can't help). ", "Right, I think we are working on something that might help. Stay tuned.", "@drpngx care to elaborate what's the new feature that might help, and what's the progress so far?\r\nI hope it's more than gradient-checkpointing:\r\nhttps://github.com/openai/gradient-checkpointing\r\n", "So, we finally pushed the feature. You can implement gradient checkpointing with that as well.\r\n\r\nYou can take a look at [`recurrent.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/recurrent/python/ops/recurrent.py). It uses the [`inplace_ops`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/inplace_ops.py) that allow to  write to parts of a tensor. Effectively you can manage a large tensor as a random memory access buffer.", "Cool! so we could simply add `alias_inplace_concat` much like `alias_inplace_add`?\r\n\r\n```python\r\n\r\ndef alias_inplace_add(x, i, v):\r\n  \"\"\"Applies an inplace add on input x at index i with value v. Aliases x.\r\n  If i is None, x and v must be the same shape. Computes\r\n    x += v;\r\n  If i is a scalar, x has a rank 1 higher than v's. Computes\r\n    x[i, :] += v;\r\n  Otherwise, x and v must have the same rank. Computes\r\n    x[i, :] += v;\r\n  Args:\r\n    x: A Tensor.\r\n    i: None, a scalar or a vector.\r\n    v: A Tensor.\r\n  Returns:\r\n    Returns x.\r\n  \"\"\"\r\nreturn _inplace_helper(x, i, v, gen_array_ops.inplace_add)\r\n```", "Yes. I think you can write or customize recurrent to do what you want. If you don't care that the memory is not freed during the computation, then you can pre-allocate to the maximum size and overwrite the parts that you need.", "@drpngx Will this impact in general memory allocation in Shared layers models like https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-385662734?\r\n\r\n", "Interesting. So far, this is unconnected, as far as I know. It's an optimization worth considering, if applicable.", "@ahundt Did you manage to implement a memory efficient version? ", "This appears to still be an open issue. Calling the ops linked by @drpngx seems to throw a 'gradients not implemented' error for the inplace ops.", "Update: I wrote custom gradient operations for the inplace operations for a 2D convolutional densenet block. However, since the inplace update only operates on the 0th dimension, there has to be a transpose on either write or read to the memory block, so training speed doesn't seem to be so hot with that. So this seems to still be in an unresolved state.", "There is the `xla_dynamic_slice_update` that can work for slices. I am guessing it should work for strided updates.", "@joeyearsley sorry I only wanted to document the problem when I noticed it so any fixes (or lack of fixes) can be tracked. I'm not planning to address this myself since it is not critical path for me.", "Nagging Assignee @drpngx: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", ":+1: for this feature, @tnbalsam any progress? If not, happy to try and base off of what you have currently", "I've made a memory efficient one which mirrors the pytorch version with gradient checkpointing:\r\nhttps://github.com/joeyearsley/efficient_densenet_tensorflow\r\n\r\nWould still be nice to have a shared memory concat though for the future.", "Awesome, thanks! This looks completely resolved so I'll close the issue now.", "@joeyearsley this is great, thanks for the reference code\r\n:+1: for the future memory concat feature", "@ahundt it may be worth re-opening this as having concat shared memory is still a huge memory advantage even with gradient checkpointing.", "@drpngx I was wondering if you can provide some guidance here\r\n\r\nThe [`inplace_ops`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/inplace_ops.py) are great that they allow you to do **writes** to an already memory allocated tensor, but it would be nice to have inplace **reads**.\r\n\r\n* Perhaps we could modify `tf.slice` to have an option to not allocate new memory for an output tensor.\r\nedit: seems like this is already the case but the slices must be [0 dim aligned](https://stackoverflow.com/questions/50779869/does-tensorflow-tf-slice-incur-allocation-and-or-memory-copy/53343932#53343932).\r\n\r\n* I was also wondering if `tf.TensorArray` could be utilized here instead of the `inplace_ops` where we just `concat` as the `tf.TensorArray` grows. However, it seems like this would be inefficient on GPUs (#19733)", "The writes work like `assign`. After they are executed, the tensor is modified. If you want to read memory, as you point out, you can use `tf.slice` or `tf.Tensor.__getitem__` (the `[]` operator).", "@ahundt @Ouwen If you are still interested, the [ScopeAllcator](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/scoped_allocator.h) introduced lately could be of help for implementing this feature."]}, {"number": 12944, "title": "No proper documentation to use transfer learning using pretrained model in .ckpt format", "body": "Hi all,\r\n\r\nTensorflow had provided a retrain sample  code for inception v3 pretrained model to apply transfer learning usinng .pb file of the model.\r\n\r\nBut there is  no documentation on how  to use a .ckpt file for pretrained  models on Imagenet such as  Resnet,inception_resnet, etc.\r\n\r\nNot much information of  how  to get a .pb file  for these pretrained model in .ckpt format.\r\n\r\nHas anyone tried transfer learning on these models?\r\nIf yes, did they use .ckpt  or .pb  version oof the model?\r\nHow to get a .pb and how to perform transfer learning usinng .ckpt version ?\r\n\r\nPlease help me on the same.\r\n\r\nthank and regards", "comments": ["It looks to me like @Bruczzz faced the same issue in #504, which you found and also commented on. It seems worth seeing if he can help you out.", "Hi @cy89,\r\n\r\nI think the issue is different from #504 as that refers to on tutorial to retrain inceptionv3 using .pb files. But as many models in tensorflow such as Resnet50 have only pretrained .ckpt instead of.pb. so it's a bit unclear on how to retrain such models the same way as suggested in #504", "Can you use one of the slim based retrainings?\r\nhttps://github.com/tensorflow/models/blob/master/slim/scripts/finetune_resnet_v1_50_on_flowers.sh\r\n\r\nIt appears to do resnet from resnet checkpoints. Slim provides all the python scripts that build the graph structure, so you can do retraining with just a .ckpt.\r\n\r\n# As some background information: saved models come in several flavors\r\n\r\n1. GraphDef (.pb) + ckpt  -- the graph structure is stored in the graph def, the ckpt contains the trained variable values separately. this is usually what you are producing when you are training new models\r\n\r\n2. Frozen GraphDef (.pb) -- no ckpt is needed since all variables. This is what mobile TensorFlow and some serving applications used. This can be produced using freeze_graph.py.\r\n\r\n3. Saved Model - this contains both checkpoints and graphdefs encapsulated in one unit.\r\n\r\n4. ckpt - this is only the values of the variables. If this is all you have, you need to have the python code that generated the model graph, and then you could load this (which would replace the variable values in the python code iniialization with the trained values). \r\n\r\nThis is explained in more detail here: https://www.tensorflow.org/extend/tool_developers/\r\n\r\n\r\n", "hi @aselle ,\r\n\r\nThank you for the suggestion. hi reported some issue with models in slim in https://github.com/tensorflow/models/issues/2356\r\n\r\nAs I was not able to execute the finetune_resnet_v1_50.sh ,I reported for more clear documentation for transfer learning using .ckpt\r\nThe document for transfer learning using .ckpt will help a lot for using converted models from caffe or other framework where a .ckpt is generated.\r\n\r\nThanks and Regards  ", "Closing this issue in favor of the models issue you created."]}, {"number": 12943, "title": "Add float64 support for conv1d and conv2d", "body": "This fix tries to address the issue raised in  #12941 where `float64` for conv1d and conv2d is incomplete.\r\n\r\nThis fix adds:\r\n- float64 support for conv1d\r\n- float64 support for conv2d\r\n- float16 support for conv3d\r\n- updated docs for float16 support of conv1d (already supported).\r\n\r\nThis fix fixes #12941.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @tensorflower-gardener and @keveman to be potential reviewers.", "@drpngx Thanks for the review. The unit tests has been updated. Please take a look.", "Jenkins, test this please.", "@drpngx The Jenkins failure was caused by pylint sanity check. I have updated the PR with pylint issue fixed. Please take a look.", "Jenkins, test this please\n\nOn Sep 11, 2017 1:40 PM, \"Yong Tang\" <notifications@github.com> wrote:\n\n@drpngx <https://github.com/drpngx> The Jenkins failure was caused by\npylint sanity check. I have updated the PR with pylint issue fixed. Please\ntake a look.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/12943#issuecomment-328650961>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbbhZvCurceMMTk7k8kHYoBi2UdYqks5shZq7gaJpZM4PSQ27>\n.\n", "Jenkins, test this please.", "@yongtang there is a compilation error in some of the tests. Can you take a look.", "Thanks @sb2nov. The PR has been rebased and pushed. I tested locally and it passes. Please take a look.", "Jenkins, test this please.", "This doesn't appear to build:\r\n\r\n```\r\ntensorflow/core/kernels/conv_grad_filter_ops.cc:945:1:   required from here\r\n./tensorflow/core/kernels/eigen_spatial_convolutions.h:730:34: error: no matching function for call to 'ptranspose(Eigen::internal::PacketBlock<__vector(2) double, 4>&)'\r\n```", "@drpngx @sb2nov The Jenkins failure is for real, though I am not able to reproduce with my local dev machine from the command line `bazel test -s --config=opt --verbose_failures //tensorflow/core/kernels:all` yet.\r\n\r\nI will take a look at the Jenkins setup and see if I could reproduce with:\r\n```sh\r\nexport USER=jenkins\r\nexport TF_BUILD_CONTAINER_TYPE=CPU\r\nexport TF_BUILD_PYTHON_VERSION=PYTHON2\r\nexport TF_BUILD_IS_OPT=OPT\r\nexport TF_BUILD_IS_PIP=NO_PIP\r\nexport TF_BUILD_APPEND_CI_DOCKER_EXTRA_PARAMS=-e TF_BUILD_FILTER_INSTALL_TESTS_BY_TAG=-manual\r\ntensorflow/tools/ci_build/ci_parameterized_build.sh\r\n```", "I am able to reproduce the error with docker/ci script though I am still not able to reproduce with standalone command line:\r\n```\r\nbazel test -s --config=opt --verbose_failures //tensorflow/python/kernel_tests:conv1d_test\r\n```\r\n\r\nOne difference between docker/CI script and the above standalone command line seems to be `--distinct_host_configuration=false`.\r\n\r\nWill need to investigate further to see what is exactly the difference.", "Jenkins, test this please.", "@yongtang can you resolve the conflicts?", "@martinwicke This PR probably is not easy to fix.\r\n\r\nOn my dev machine if running bazel directly without docker (and gcc 5.4.0+ubuntu 16.04) everything is fine.\r\n\r\nHowever, when running inside docker (similar to what is invoked in Jenkins CI with `tensorflow/tools/ci_build/ci_parameterized_build.sh`) then build will fail.\r\n\r\nI have been investigating this issue for some time. I would guess the reason is that, certain flags are passed by bazel inside/outside container differently. And I tend to guess that `ptranspose` in Eigen might have been hidden by certain flags. However, I am still relatively new to bazel's build system and haven't been able to figure out what exactly is the issue here.\r\n\r\nCurrently there are several pending issues in TF (#12941 and #13097) and Keras (fchollet/keras#7176) that is related to this PR. But I am afraid this may not be resolved soon.\r\n\r\nI think maybe this issue could be labeled as `stalled` or we could closed it for now. I will try to revisit this issue after I have a better understanding of the bazel's flags.\r\n\r\nSorry about the inconvenience.", "@yongtang Yes, that's right, we run the build under `ci_parameterized_build.sh`. Just to be clear, you want to know how blaze is run, or what commands blaze runs?", "Thanks. I will mark it stalled for now. If you need help with bazel, we can help.", "I'll close this for now -- please reopen a new one (or ping me to reopen it).", "Hi @yongtang, any news about float64 support for conv2d? thanks", "@redouanelg The last time I worked on this issue is in #16273. The problem has been reduced to the configurations/flags in Eigen library. Though I haven't been able to find out the solution yet. Will looking into it again if possible."]}, {"number": 12942, "title": "I am facing this issue after installing it on Windows 7 32 Bit , AMD processor, python 3.5.2, TensorFlow 1.0.0", "body": "Microsoft Windows [Version 6.1.7601]\r\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n\r\nC:\\Users\\SEM>python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_help\r\ner\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: %1 is not a valid Win32 application.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_help\r\ner\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_help\r\ner\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: %1 is not a valid Win32 application.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_help\r\ner\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@Shiv751 , Windows 32-bit is not a platform supported by TensorFlow. \r\n\r\ncc @gunan ", "Correct. TF out of the box strictly needs a 64 bit operating system & hardware on a desktop or laptop.\r\nWe are working on adding this information to our docs."]}, {"number": 12941, "title": "float64 support for conv1d and conv2d", "body": "This is a feature request for `float64` support with `nn.conv1d`, `nn.conv2d`, and `nn.convolution`. Also, `conv1d` does not correctly document which types it supports. See below for details.\r\n\r\n`conv1d` (which seems to be a wrapper for `conv2d`) and `conv2d` throw a TypeError when passed `float64` tensors. (I have only tested using a CPU; I don't have GPU access.) This occurs despite the fact that the documentation for `conv1d` claims support for `float64`. It is also worth mentioning that (at least on my CPU), `conv1d` seems to support `float16` even though this isn't mentioned in the documentation.\r\n\r\n`conv3d` does however seem to support `float64`. Fortunately, this can at least be used as a temporary workaround to obtain `float64` support for 1d and 2d convolutions.\r\n\r\n| Command | float16 | float32 | float64 |\r\n| --- | --- | --- | --- |\r\n| conv1d | works, but undocumented | works | TypeError, despite documentation |\r\n| conv2d | works | works | TypeError |\r\n| conv3d | TypeError | works | works |\r\n\r\nThe above table summarizes the (rather inconsistent) current state of tensorflow's support for various types according to my tests on a CPU using the code below. `convolution` appears to be a wrapper for `conv1d`, `conv2d`, and `conv3d`, and thus fails on the same types for each dimension.\r\n\r\n```python\r\nimport tensorflow as tf\r\n# t = tf.float16\r\n# t = tf.float32\r\nt = tf.float64\r\nb, fw1, fw2, fw3, ic, oc, iw1, iw2, iw3 = range(2,11)\r\ntf.nn.conv1d(tf.zeros([b, iw1, ic], t), tf.zeros([fw1, ic, oc], t), 1, \"VALID\")\r\ntf.nn.conv2d(tf.zeros([b, iw1, iw2, ic], t), tf.zeros([fw1, fw2, ic, oc], t), [1]*4, \"VALID\")\r\ntf.nn.conv3d(tf.zeros([b, iw1, iw2, iw3, ic], t), tf.zeros([fw1, fw2, fw3, ic, oc], t), [1]*5, \"VALID\")\r\ntf.nn.convolution(tf.zeros([b, iw1, ic], t), tf.zeros([fw1, ic, oc], t), \"VALID\")\r\ntf.nn.convolution(tf.zeros([b, iw1, iw2, ic], t), tf.zeros([fw1, fw2, ic, oc], t), \"VALID\")\r\ntf.nn.convolution(tf.zeros([b, iw1, iw2, iw3, ic], t), tf.zeros([fw1, fw2, fw3, ic, oc], t), \"VALID\")\r\n```\r\n\r\n### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Arch Linux (up to date)\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: 1.3.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version**: 0.5.4\r\n", "comments": ["Added a PR #12943 to address the above mentioned issues.", "Marking as contributions welcome since there is a PR.", "+1", "I've added support for conv2d float64 in e3468b56d323783fdfb79fa2d6c24effc58bcaa9", "I guess this implicitly covers conv1d due to the internal impl of conv1d as reshape-then-conv2d. LMK if you have other open issues."]}, {"number": 12940, "title": "tf.nn.separable_conv2d is slower than conv2d on GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: TF 1.3\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA8.0 /cuDNN6\r\n- **GPU model and memory**: GTX1080ti  11G\r\n\r\n\r\n### Describe the problem\r\nIn theory, `separable_conv2d` should be more efficient than `conv2d`, but when I test a simple model on Cifar10, the result shows that `nn.separable_conv2d` run slower on GPU, but is indeed faster on CPU. \r\n\r\nHere is my test results on GPU: \r\n```\r\ntraining time for normal_conv after 2000 step: 8.18395892999979 sec\r\ntime for normal_conv after one forward step:  0.003980965999289765 sec\r\ntraining time for separable_conv after 2000 step: 9.158266903999902 sec\r\ntime for separable_conv after one forward step:  0.0036441169995669043 sec\r\n\r\n```\r\n\r\n### Source code / logs\r\n\r\nBelow is a fully self-contained example, I first define a model with two  `conv2d` , than I define another model with one `conv2d` followed by one  `separable_conv2d`, both model have 32 channels for each conv_layer and identical fc_layer.\r\n\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport timeit\r\nimport numpy as np\r\nfrom tensorflow.contrib.keras.python.keras.datasets.cifar10 import load_data\r\n\r\n(x_train, y_train), (x_val, y_val) = load_data()\r\nlearning_rate = 0.001\r\nnum_steps = 1000\r\nn_classes = 10\r\nbatch_size = 32\r\n\r\ndef reformat(labels):\r\n    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\r\n    labels = (np.arange(n_classes) == labels[:,None]).astype(np.float32)\r\n    return  labels.reshape(labels.shape[0],10)\r\ntrain_labels = reformat(y_train)\r\ntf.reset_default_graph()\r\nx = tf.placeholder(tf.float32, [None, 32, 32, 3])\r\ny = tf.placeholder(tf.float32, [None, 10])\r\nweights1 = {}\r\nweights2 = {}\r\ndtype = tf.float32\r\nwith tf.name_scope('INIT_OP'):\r\n    conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)\r\n    fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=dtype)\r\n\r\nk = 3\r\nkernel = 16\r\n\r\n# Define weights for normal ConvNet\r\nwith tf.name_scope('VARIABLES_1'):\r\n    weights1['conv1'] = tf.get_variable('conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights1['b1'] = tf.get_variable('b1', initializer=tf.zeros([kernel]))\r\n    weights1['conv2'] = tf.get_variable('conv2', [k, k, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights1['b2'] = tf.get_variable('b2', initializer=tf.zeros([kernel]))\r\n\r\n    weights1['wd1'] = tf.get_variable('wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights1['bd1'] = tf.get_variable('bd1',  initializer=tf.zeros([512]) )\r\n    weights1['wd2'] = tf.get_variable('wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights1['bd2'] = tf.get_variable('bd2',  initializer=tf.zeros([10]) )\r\n\r\n\r\n#Define weights for separable ConvNet\r\nwith tf.name_scope('VARIABLES_sep'):\r\n    weights2['conv1'] = tf.get_variable('2_conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights2['conv_dw2'] = tf.get_variable('conv_dw2', [k, k, kernel, 1], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights2['conv_pw2'] = tf.get_variable('conv_pw2', [1, 1, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n\r\n    weights2['b1'] = tf.get_variable('2_b1', initializer=tf.zeros([kernel]))\r\n    weights2['b2'] = tf.get_variable('2_b2', initializer=tf.zeros([kernel]))\r\n\r\n    weights2['wd1'] = tf.get_variable('2_wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights2['bd1'] = tf.get_variable('2_bd1',  initializer=tf.zeros([512]) )\r\n    weights2['wd2'] = tf.get_variable('2_wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights2['bd2'] = tf.get_variable('2_bd2',  initializer=tf.zeros([10]) )\r\n\r\ndef forward_conv_sep( inp, weights):\r\n    hidden = conv_block(inp, weights2['conv1'], weights2['b1'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = conv_block_dw(hidden, weights2['conv_dw2'], weights2['conv_pw2'], weights2['b2'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\r\n    fc1 = tf.matmul(hidden, weights2['wd1']) + weights2['bd1']\r\n    fc1 = tf.nn.relu(fc1)\r\n    return tf.matmul(fc1, weights2['wd2']) + weights2['bd2']\r\n\r\ndef forward_conv( inp, weights):\r\n    hidden = conv_block(inp, weights1['conv1'], weights1['b1'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = conv_block(hidden, weights1['conv2'], weights1['b2'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\r\n    fc1 = tf.matmul(hidden, weights1['wd1']) + weights1['bd1']\r\n    fc1 = tf.nn.relu(fc1)\r\n    return tf.matmul(fc1, weights1['wd2']) + weights1['bd2']\r\n\r\n\r\ndef conv_block_dw(inp, cweight_w, cweight_p, bweight):\r\n    no_stride =  [1,1,1,1]\r\n    conv_output = tf.nn.separable_conv2d(inp, cweight_w, cweight_p, no_stride, 'SAME') + bweight\r\n    return tf.nn.relu(conv_output)\r\n\r\ndef conv_block(inp, cweight, bweight, activation=tf.nn.relu):\r\n    no_stride =  [1,1,1,1]\r\n    conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight\r\n    return tf.nn.relu(conv_output)\r\n\r\ndef maxpool2d(inp, k=2):\r\n    return tf.nn.max_pool(inp, ksize=[1, k, k, 1], strides=[1, k, k, 1],\r\n                          padding='SAME')\r\n\r\n#logits for normal ConvNet\r\nwith tf.name_scope(\"forward_conv\"):\r\n    pred1 = forward_conv(x, weights1)\r\n\r\n#Cost for normal ConvNet\r\nwith tf.name_scope(\"cost1\"):\r\n    loss1 = tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=y)\r\n    cost1 = tf.reduce_mean(loss1)\r\n\r\n#training op for normal ConvNet\r\nwith tf.name_scope('train_op1'):\r\n    train_op1 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost1)    \r\n\r\n#logits for separable ConvNet\r\nwith tf.name_scope(\"forward_conv_sep\"):\r\n    pred2 = forward_conv_sep(x, weights2)\r\n\r\n#Cost for separable ConvNet\r\nwith tf.name_scope(\"cost2\"):\r\n    loss2 = tf.nn.softmax_cross_entropy_with_logits(logits=pred2, labels=y)\r\n    cost2 = tf.reduce_mean(loss2)\r\n\r\n# training op for separable ConvNet\r\nwith tf.name_scope('train_op2'):\r\n    train_op2 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost2)\r\n\r\n\r\nwith tf.name_scope('INIT'):\r\n    init = tf.global_variables_initializer()\r\n\r\n\r\nwith tf.Session() as sess:\r\n\r\n    sess.run(init)\r\n\r\n    #train normal ConvNet for 2000 steps\r\n    start = timeit.default_timer()\r\n    for step in range(num_steps):\r\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\r\n        batch_data = x_train[r]\r\n        batch_labels = train_labels[r]\r\n\r\n        feed_dict = {x : batch_data, y: batch_labels}\r\n        _ , l = sess.run([train_op1,cost1], feed_dict=feed_dict)\r\n\r\n    stop = timeit.default_timer()\r\n    print ('training time for normal_conv after '+str(num_steps)+' step:',stop - start) \r\n\r\n\r\n    start = timeit.default_timer()\r\n    feed_dict = {x : batch_data, y: batch_labels}\r\n    predictions1 = sess.run(pred1, feed_dict=feed_dict)\r\n    stop = timeit.default_timer()\r\n    print ('time for normal_conv after one forward step: ',stop - start)\r\n\r\n\r\n\r\n    # train separable ConvNet for 2000 steps\r\n    start = timeit.default_timer()\r\n    for step in range(num_steps):\r\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\r\n        batch_data = x_train[r]\r\n        batch_labels = train_labels[r]\r\n\r\n        feed_dict = {x : batch_data, y: batch_labels}\r\n        _ , l = sess.run([train_op2,cost2], feed_dict=feed_dict)\r\n\r\n\r\n    stop = timeit.default_timer()\r\n    print ('training time for sep_conv after '+str(num_steps)+' step:',stop - start) \r\n\r\n    start = timeit.default_timer()\r\n    feed_dict = {x : batch_data, y: batch_labels}\r\n    predictions = sess.run(pred2, feed_dict=feed_dict)\r\n    stop = timeit.default_timer()\r\n    print ('time for sep_conv after one forward step: ',stop - start)\r\n```", "comments": ["@vrv my recollection is that there are some issues with backprop for separable convolutions on GPUs, which had been somewhat improved lately. Can you comment please on the state of the art?", "I'm not aware of any specific issues, kernels get faster as important models need them!  In general, convolutions of different sizes will have different performance characteristics, and it's possible our separable convolution implementations may be slow for some combinations of shapes.  I'm not sure whether that's the case here, but it could be.  I also don't know whether theory matches practice here, since separable convolutions are less compute dense than normal convolutions.  I believe the benefits you get are that you get to use fewer parameters to express a larger capacity convolution.", "At what kernel sizes will convolutions be computed via FFT instead of directly? Anyway, a speedup by doing a separable convolution is more noticeable for larger kernels, so for small kernels the overhead involved in doing two convolutions might be larger than the speedup, especially for what I assume is a highly-optimized convolution setting with the 3x3 kernel ([Winograd](https://arxiv.org/abs/1509.09308)).\r\n\r\nEssentially, for a `[m, n]` kernel it would take `m*n` calculations for a convolution and `m+n` calculations for the separable convolution, if I'm not mistaken.", "@carlthome @vrv  I do notice that when the number of filters get larger (128 or 192 etc. ), separable_conv2d is faster than conv2d, but in my case, I applied separable_conv2d on cifar10 with small number of filters, and it is actually slower than conv2d on my GPU, what could be the cause?", "Same principle. For small ~~kernels~~ _matrices_ the overhead involved in doing two convolutions might be larger than the speedup. ", "Understood, thanks for the explanation!  ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Why is this closed?  I implemented UNet with separable conv2d and it was around 80% slower than using a standard conv2d.  Is there anything in the works to optimize this like using groups?", "I am getting immense slowdown as well, is there a more efficient implementation known?", "Same problem. @HouseOfFinwe Did you figure out to fix this? ", "@keunwoochoi I did not, please let me know if either of you managed.", "@HouseOfFinwe Ok thanks. https://github.com/tensorlayer/tensorlayer/issues/416#issuecomment-374180396 says the speed issue was fixed in tf 1.5, but I'm having it with 1.13 now. Do you remember which versions have you tried?", "Sadly I do not", "> @HouseOfFinwe Ok thanks. [tensorlayer/tensorlayer#416 (comment)](https://github.com/tensorlayer/tensorlayer/issues/416#issuecomment-374180396) says the speed issue was fixed in tf 1.5, but I'm having it with 1.13 now. Do you remember which versions have you tried?\r\n\r\nStill exited on 1.14.", "why close ??? still happen with tf1.13", "This should be fixed by #33836. It currently requires tf-nightly but will ship in the coming 2.2 release."]}, {"number": 12939, "title": "cudnn_rnn_ops.py cannot find _cudnn_rnn_ops.so", "body": "https://github.com/tensorflow/tensorflow/blob/702d595822e9e5f5232b8140c6296683612c33a9/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L37", "comments": [" `Using TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/home/apurvnit/Downloads/test.py\", line 3, in <module>\r\n    import keras\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/keras/__init__.py\", line 4, in <module>\r\n    from . import activations\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/keras/activations.py\", line 6, in <module>\r\n    from .engine import Layer\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/keras/engine/__init__.py\", line 8, in <module>\r\n    from .training import Model\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/keras/engine/training.py\", line 25, in <module>\r\n    from .. import callbacks as cbks\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/keras/callbacks.py\", line 26, in <module>\r\n    from tensorflow.contrib.tensorboard.plugins import projector\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/tensorflow/contrib/__init__.py\", line 25, in <module>\r\n    from tensorflow.contrib import cudnn_rnn\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/__init__.py\", line 21, in <module>\r\n    from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnGRU\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 28, in <module>\r\n    resource_loader.get_path_to_datafile(\"_cudnn_rnn_ops.so\"))\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/tensorflow/contrib/util/loader.py\", line 42, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/home/apurvnit/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/apurvnit/anaconda/lib/python3.5/site-packages/tensorflow/contrib/cudnn_rnn/python/ops/_cudnn_rnn_ops.so: cannot open shared object file: No such file or directory`", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "@apuayush I am having the same issue right now, how did you resolve the issue"]}, {"number": 12938, "title": "Minor wording change in timeseries module's README", "body": "", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @allenlavoie to be a potential reviewer.", "Thanks for fixing it!"]}, {"number": 12937, "title": "Possible Bug: while_loop, map_fn do not parallize ", "body": "The `parallel_iterations` parameter of `while_loop` and `map_fn` do not reduce the runtime as expected.\r\nIn comparison to the same operations created in a python loop, `while_loop` and `map_fn` are at least 4 times slower. `while_loop` does not scale at all\r\n\r\nMore details:\r\nFor my project, I need to calculate a matrix blockwise. \r\nI execute the calculation in a loop, each loop iteration working on a small part of the matrix, not accessing the rest of the matrix.\r\nTo achieve maximum performance, the loop has to be executed in parallel.\r\nIn addition, I need to control the number of parallel threads because of memory limitations.\r\nI tried to use `parallel_iterations`, but changing this values doesn't change the runtime at all.\r\n\r\nUsing a python-for loop to create the same number of operations and syncing them with `control_dependencies` is around 4 times faster.\r\nIn contrast to `while_loop`, `map_fn` is getting faster with bigger `parallel_iterations` but remains noticeable slower then the python creation.\r\nIf this issue is known (didn't find anything at stackoverflow or here) and not resolvable, \r\nthe documentation of while_loop needs to be improved heavily - to avoid this operation if the loop should be parallized.\r\n\r\nBenchmark-Code:\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\nif __name__ == '__main__':\r\n      runs = 100\r\n      N = 100\r\n      M = 30\r\n\r\n      KernelRow = tf.Variable(tf.zeros([M, N*M]), name='KernelRow')\r\n\r\n      def calEntry(KernelVariable, pos):\r\n            op = tf.assign(KernelVariable[:, pos*M:(pos+1)*M], \r\n                  tf.eye(M) * 123. + tf.exp(tf.random_uniform([M, M]))) #perform some operations \r\n            with tf.control_dependencies([op]):\r\n                  return tf.constant(0)\r\n\r\n      def calRow(KernelVariable, N, parallelNum):\r\n            ops = []\r\n            for ic in range(0, N, parallelNum):\r\n                  with tf.control_dependencies(ops):\r\n                        for j in range(parallelNum):\r\n                              if ic + j < N:\r\n                                    ops += [calEntry(KernelVariable, ic+j)]\r\n                        \r\n            with tf.control_dependencies(ops):\r\n                  return tf.identity(KernelVariable)\r\n\r\n      def calRow_while(KernelVariable, N, parallelNum):\r\n            i = tf.constant(0)\r\n            condition = lambda i: tf.less(i, N)\r\n            def body(ic):\r\n                  updateKernel = calEntry(KernelVariable, ic)\r\n                  with tf.control_dependencies([updateKernel]):\r\n                        return ic + 1\r\n            return tf.while_loop(condition, body, [i], back_prop=False, parallel_iterations=parallelNum)\r\n            \r\n      def calRow_map(KernelVariable, N, parallelNum):\r\n            f = lambda x: calEntry(KernelVariable, x)\r\n            return tf.map_fn(f, tf.range(N, dtype=tf.int32), parallel_iterations=parallelNum)\r\n\r\n      paraOps = []\r\n      paraOpsWhile = []\r\n      paraOpsMap = []\r\n      for paraEntry in [1, 10, 50, 100]:\r\n            paraOps += [(calRow(KernelRow, N, paraEntry), paraEntry )]\r\n            paraOpsWhile += [(calRow_while(KernelRow, N, paraEntry), paraEntry )]\r\n            paraOpsMap += [(calRow_map(KernelRow, N, paraEntry), paraEntry )]\r\n\r\n      init = tf.global_variables_initializer()\r\n      #perform calculation\r\n      with tf.Session() as sess:\r\n            sess.run(init)\r\n            for op in paraOps:\r\n                  runtime = []\r\n                  sess.run(op[0]) #warm up\r\n                  for i in range(runs):\r\n                        startTime = time.time()\r\n                        sess.run(op[0])\r\n                        runtime += [time.time() - startTime]\r\n                  print(\"Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\r\n            \r\n            for op in paraOpsWhile:\r\n                  runtime = []\r\n                  sess.run(op[0]) #warm up\r\n                  for i in range(runs):\r\n                        startTime = time.time()\r\n                        sess.run(op[0])\r\n                        runtime += [time.time() - startTime]\r\n                  print(\"WHILE: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\r\n                \r\n            for op in paraOpsMap:\r\n                  runtime = []\r\n                  sess.run(op[0]) #warm up\r\n                  for i in range(runs):\r\n                        startTime = time.time()\r\n                        sess.run(op[0])\r\n                        runtime += [time.time() - startTime]\r\n                  print(\"MAP: Calculation using {} threads took {:.4f} +- {:.4f}\".format(op[1], np.mean(runtime), np.std(runtime) ))\r\n```\r\n Example Output:\r\n\r\n```\r\nCalculation using 1 threads took 0.0044 +- 0.0004\r\nCalculation using 10 threads took 0.0026 +- 0.0006\r\nCalculation using 50 threads took 0.0018 +- 0.0000\r\nCalculation using 100 threads took 0.0014 +- 0.0001\r\nWHILE: Calculation using 1 threads took 0.0063 +- 0.0001\r\nWHILE: Calculation using 10 threads took 0.0063 +- 0.0007\r\nWHILE: Calculation using 50 threads took 0.0063 +- 0.0001\r\nWHILE: Calculation using 100 threads took 0.0062 +- 0.0004\r\nMAP: Calculation using 1 threads took 0.0072 +- 0.0004\r\nMAP: Calculation using 10 threads took 0.0035 +- 0.0003\r\nMAP: Calculation using 50 threads took 0.0035 +- 0.0002\r\nMAP: Calculation using 100 threads took 0.0035 +- 0.0002\r\n\r\n```\r\n\r\n \r\n\r\n\r\n### System information\r\nUbuntu, Tensorflow installed using pip, version 1.3\r\ntested on two different systems", "comments": ["@skye can you take a look?", "@skye @reedwm Any update on this?", "Looking at calRow_while, it appears you're using control dependencies to serialize the while loop iterations. This is probably why you're not seeing speedups from increasing the parallel_iterations argument (since it can't actually run anything in parallel).", "This could be the case, but how would you perform it without the dependencies?\r\nI thought that is the default way to use while_loop?", "It depends on the body of your while loop. Sometimes there are independent calculations that can be run in parallel (e.g. if you have many different loop variables and they don't all depend on each other). Also, I suspect you're seeing the manual calculation run faster than the while loop because the body computation is small compared to the overhead of running the while loop. A more complicated or expensive body would probably see less of a difference.", "The above calculation is only an example, the body I use takes quiet some time.\r\nBut for the above body - how would you write it down to achieve parallel execution with while_loop?\r\n\r\nAnd building the graph for calRow takes a lot of time if you use bigger numbers. \r\nIs there no easy way to achieve loop parallization?  \r\n", "Remove the control dependency block? It depends on what you're trying to achieve... if the variable assigns can happen in any order, you don't need the control dependencies and you can run in parallel. If the assigns must happen in order, then you can't run them in parallel and this is enforced via control deps.\r\n\r\nNote that map_fn calls tf.while_loop() under the hood, but doesn't add any control dependencies. So that's an example of parallel execution using while loop.", "Ok now I got what you are suggesting. \r\nThe following does **not** work:\r\n```\r\n def body(ic):\r\n     updateKernel = calEntry(KernelVariable, ic)\r\n     return ic + 1\r\n```\r\nThis will never execute calEntry.\r\n\r\nBut if you use \r\n```\r\ni = tf.constant(0)\r\nm0 = tf.constant(0)\r\ncondition = lambda i, m: tf.less(i, N)\r\ndef body(ic, dontcare):\r\n     updateKernel = calEntry(KernelVariable, ic)\r\n     return ic + 1, updateKernel\r\nreturn tf.while_loop(condition, body, [i,m0], back_prop=False, parallel_iterations=parallelNum)\r\n```\r\nIt works and is as fast as using map or foldl, foldr\r\nThank you a lot for your help, even though it does not change much because now my code already uses map and fold everywhere. \r\n\r\nYou can still gain some speed if you use \r\n```\r\n            for ic in range(0, N, parallelNum):\r\n                  with tf.control_dependencies(ops):\r\n                        for j in range(parallelNum):\r\n                              if ic + j < N:\r\n                                    ops += [calEntry(KernelVariable, ic+j)]\r\n```\r\nbut this may result in a very slow graph creation. \r\n@skye is there any plan to add a parallel while_loop with lesser overhead?  \r\n\r\n\r\n\r\n\r\n", "You're welcome! Using the functional ops will probably make your program easier to understand than using while_loop directly (like I said, these functions call while_loop under the hood).\r\n\r\nThere are no plans to change while_loop right now, no. I suggest benchmarking your actual application, as I expect you'll find the overhead to be a smaller percentage of the total runtime than in this microbenchmark.\r\n\r\nI'm gonna close this issue now.", "Hi @skye  I encountered something similar when running tf.map_fn where the function was a neural network that did a bunch of gradient steps. The inputs were just slices of a minibatch. \r\n\r\nBasically this: https://github.com/cbfinn/maml/blob/master/maml.py#L78\r\n\r\nI noticed that on CPU, there was parallelization, but on GPU no. Might there be a reason for this?\r\n\r\nIt seems like the post here was not the same issue. \r\n\r\n"]}, {"number": 12936, "title": "change cnn_mnist example to use Adam optimizer; added a 'loss' summary", "body": "I remember that other (non-estimator) versions of this example used the Adam optimizer, which has nicer convergence -- could we use it in this example?\r\nAlso, I added a summary for `loss`, so that it would show up on TensorBoard.\r\n\r\n(I'd also like to add support for passing in `num_steps`, `model_dir`, etc. as command-line args with defaults, but I'll make that a separate PR unless you'd like it bundled with this one).", "comments": ["@amygdala, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @sandersk and @isaprykin to be potential reviewers.", "Can one of the admins verify this patch?", "btw, @ispirmustafa might make sense as the reviewer (as I saw you did an internal review of a previous iteration of this file).", "@ispirmustafa please review", "Hi Amy,\r\nCould you please point me to the none Estimator version, so that I can compare.", "Sorry for the delay, I've been traveling.  Will dig it up shortly.  Was in one of the earlier releases.", "@ispirmustafa Here's one instance: https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_deep.py", "Thank you for the pointer @amygdala \r\nCould you please run your cl and original code and compare the performance/convergence? It would be great to add that information here.\r\nAlso head version of Estimator should add loss automatically.\r\n", "Sure, will do shortly. ", "Marking as stalled.", "arghh (sigh).  I actually hope to get to this during the break.  I know it's a simple thing but it just hasn't made it to the top of my stack. (I'll sync up first as I see there's now a conflict).", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "okay, I'm really REALLY going to get to this next week.", "Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "closing due to inactivity."]}, {"number": 12935, "title": "Feature request: nightly build for python 3.6", "body": "The [`tf-nightly`](https://pypi.org/project/tf-nightly/) pip packages are great! Would it be possible to add support for python 3.6 and/or provide a wheel for python 3.6 as part of the [standard nightly build](https://github.com/tensorflow/tensorflow#installation)?\r\n\r\nIn particular, the following succeeds\r\n\r\n```\r\n$ docker run --rm python:3 pip install tensorflow\r\n```\r\n\r\nwhereas the following fails\r\n\r\n```\r\n$ docker run --rm python:3 pip install tf-nightly\r\nCollecting tf-nightly\r\n  Could not find a version that satisfies the requirement tf-nightly (from versions: )\r\nNo matching distribution found for tf-nightly\r\n```", "comments": ["/CC @gunan", "@av8ramit Do you think this would be easy to add?", "Thanks @tillahoffmann, that is on our roadmap and I'll add that as soon as I can. ", "@av8ramit, that's great, thank you. ", "@av8ramit, do you have a rough estimate on when this is likely to happen? It would help us a lot with planning, e.g. should we push a binary to our internal pip server?", "@tillahoffmann I'm aiming to have a 3.6 binary for CPU by late next week", "@av8ramit, that's great thank you! We'll keep building a GPU version for now.", "Hey @tillahoffmann can you give the [nightlies](https://pypi.python.org/pypi?:action=display&name=tf-nightly&version=1.4.0.dev20170921) from today a chance? ", ":tada:, thank you for the CPU build!", "No problem! Please reopen if there are any issues.", "It seems that some parts of the nightly build still relies on 3.5.\r\n\r\n```\r\n/usr/local/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```", "Yes, we unfortunately copy the 3.5 binary for 3.6 I'll look into creating a specific 3.6 binary for Linux.", "Looks like we may need to change this, we used to not have such issues.\r\nWe will look into enhancing our builds to actually build with different python versions.", "Hi @gunan @av8ramit ..is the fix available for 3.6 binary in linux?", "Any idea when the PR will be merged till?", "There's no PR, we will need to make this change in the release workflow.", "how i do on Mac\uff1fi hava this issue\uff01", "I also have this issue! Please help! \r\n\r\npip install tf-nightly-gpu\r\nCollecting tf-nightly-gpu\r\n  Could not find a version that satisfies the requirement tf-nightly-gpu (from versions: )\r\nNo matching distribution found for tf-nightly-gpu", "There is now a python binary for tf-nightly and tf-nightly-gpu that is built from python3.6 as of 12/06!\r\n\r\nClosing this issue for now. @cmccarthy15 can you please provide more information? What OS and python version are you running?"]}, {"number": 12934, "title": "Added support for JVM callback ops", "body": "This PR is a proposed workaround for a limitation related to my Scala API and discussed in #12895 and #12915. Given that we currently cannot load external op libraries through languages other than Python, this PR adds a JVM callback op to the main TensorFlow library. I understand this is not ideal and may not be acceptable but it's just offered as a possible workaround.\r\n\r\nOn the plus side, this op is not specific to Scala. It allows any JVM language to create ops that call functions in that language. The arguments and outputs of such functions are arrays of pointers to \"eager tensors\". The only limitation is that the op invocation has access to the original JVM instance that was used when constructing it. This is similar to the current `py_func` op limitation though and so, should be fine for now.\r\n\r\n@asimshankar Do you think that this is acceptable for now?\r\n\r\nP.S. The replication of a few lines from the C API code is ugly but I'm not sure what's best to do here and I didn't want to introduce more changes until you take a look.", "comments": ["Can one of the admins verify this patch?", "@eaplatanios, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @josh11b and @vrv to be potential reviewers.", "@eaplatanios : IIUC, this is to allow distribution of your Scala bindings using then nightly builds for the period during which @allenlavoie 's changes to the shared libraries aren't in effect. Given the estimated timeline (O(weeks)), and the fact that in the interim you could build your own libraries instead of using the nightly builds for your distribution, I'm tempted to suggest against adding this operation to the core TensorFlow libraries.\r\n\r\nThanks for for your understanding.", "@asimshankar No problem, I totally understand and was kind of expecting that. I'll look into using the CI build scripts you provide on one of our servers."]}, {"number": 12933, "title": "Reverse change eb75ded6 so that internal tests will pass.", "body": "As support for int64 global steps is not ready in TPUs, I am reversing this change so that our internal performance and regression tests will pass.", "comments": ["@frankchn, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @xiejw and @jhseu to be potential reviewers.", "Jenkins, test this please."]}, {"number": 12932, "title": "Turning on grappler makes SLIM Resnet_v1_50 slower on AWS K80", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source binary link [here](https://s3-us-west-2.amazonaws.com/tf-benchmark/tf_binary/tensorflow-1.4.0.32ffc5a-cp27-cp27mu-linux_x86_64.whl)\r\n- **TensorFlow version (use command below)**: 1.3+ the sha-hash is in the link to download the binary I compiled.  \r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 4.5\r\n- **CUDA/cuDNN version**: cuDNN 6 / CUDA 8\r\n- **GPU model and memory**: K80 on AWS p2.8xlarge\r\n- **Exact command to reproduce**: Running SLIM from tensorflow/models/slim I used the following command: `CUDA_VISIBLE_DEVICES=1 python train_image_classifier.py     --train_dir=${TRAIN_DIR}     --dataset_name=imagenet     --dataset_split_name=train     --dataset_dir=${DATASET_DIR}     --model_name=resnet_v1_50    --num_clones=1    --optimizer=sgd    --batch_size=64    --max_number_of_steps=110`\r\n\r\n\r\nI added the following code to train_image_classifier.py:  \r\n\r\n```python\r\n\r\n    rewrite_options = rewriter_config_pb2.RewriterConfig()\r\n    rewrite_options.optimizers.append('constfold')\r\n    rewrite_options.optimizers.append('layout')\r\n    graph_options = tf.GraphOptions(rewrite_options=rewrite_options, infer_shapes=True)\r\n    config = tf.ConfigProto(graph_options=graph_options)\r\n\r\n    ###########################\r\n    # Kicks off the training. #\r\n    ###########################\r\n    slim.learning.train(\r\n        train_tensor,\r\n        session_config=config,\r\n\r\n\r\n```\r\n\r\nI used this binary to test and it includes the sha-hash.  The build was done from head on 09-SEP-2017 using this command `bazel build -c opt --copt=-march=\"haswell\" --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nFor /configure.  I did not include XLA I did all of the defaults with the exception of adding CUDA and cuDNN.  Nothing additional was included.  \r\n\r\nWith out grappler I get **1.5 seconds per step and with grappler 2.0 seconds per step**.  There is some variance of plus or minus .1 but it is definitely slower in my testing which was not expected.  \r\n\r\n**Edited:** I was running v1 but I think v2 gave a very similar result.  ", "comments": ["One possibility is that layout optimizer doesn't play well with non-fused batch norm (I will look into that).\r\n\r\nIn the meantime, could you try turn on fused batch norm by adding \"'fused': True,\" [here](https://github.com/tensorflow/models/blob/master/slim/nets/inception_utils.py#L48).\r\n\r\nThis may get you 15%-20% speedup. After that, you can turn on layout optimizer, this may get you another 15% speedup (fused batch norm + layout optimizer together may get about 30% speedup).", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We recently had lots of enhancements for layout optimizer. This issue should have been fixed: I have run train_image_classifier.py for resnet with the latest version of TensorFlow on my machine, layout optimizer makes the training 23.8% faster: 0.25 steps/sec or 4 steps/sec with layout optimizer, 0.31 steps/sec or  3.23 steps/sec without layout optimizer. 4/3.23-1 = 23.8% faster.", "@zhangyaobit   Thank you for retesting.  "]}, {"number": 12931, "title": "getting no attribute key error in ubuntu virtual box", "body": "bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~'/dataset/dosa/'\r\nERROR:tensorflow:Image directory '~/dataset/dosa/' not found.\r\nTraceback (most recent call last):\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 989, in main\r\n    class_count = len(image_lists.keys())\r\nAttributeError: 'NoneType' object has no attribute 'keys'\r\n\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Linux Ubuntu 16.04\r\nsource \r\ntensorflow 0.12.1\r\npython 2.7\r\nbazel 0.5.4\r\n", "when i give this command\r\n\r\nbazel-bin/tensorflow/examples/image_retraining/retrain --image_dir '/home/dile/dataset/dosa'\r\n\r\nam getting\r\nERROR:tensorflow:No valid folders of images found at /home/dile/dataset/dosa\r\n\r\n\r\nbut the folder is present in tat directory\r\n", "Does `/home/dile/dataset/dosa` contain several folders, each which contains images? What is the output of `ls /home/dile/dataset/dosa`?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12930, "title": "Unable to build C++ example by bazel on windows 10", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Windows 10 64Bit\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: master\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: 0.5.4, binary\r\n\r\nin windows powershell:\r\n```\r\n>>protoc --version\r\nlibprotoc 3.4.0\r\n>>bazel version\r\nBuild label: 0.5.4\r\nBuild target: bazel-out/msvc_x64-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Aug 25 09:59:45 2017 (1503655185)\r\nBuild timestamp: 1503655185\r\nBuild timestamp as int: 1503655185\r\n```\r\n### Describe the problem\r\nI want to learn tensorflow C++ api and build a example from this [page](https://www.tensorflow.org/api_guides/cc/guide), but I get the following error:\r\n```\r\nERROR: E:/projects/deeplearning/tensorflow/tensorflow/cc/example/BUILD:1:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl\", line 122\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl\", line 103, in _apply_patch\r\n                fail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it and referenced by '//tensorflow/cc/example:example'.\r\nERROR: Analysis of target '//tensorflow/cc/example:example' failed; build aborted.\r\nINFO: Elapsed time: 13.336s\r\nERROR: Build failed. Not running target.\r\n```\r\nI have already run ```pacman -Syuu --noconfirm patch``` in msys2.exe.  \r\nbazel works well in simple c++ example.\r\n### Source code / logs\r\nin C:/msys64/usr/bin/bash.exe:\r\n```\r\n$ patch --version\r\nGNU patch 2.7.5\r\nCopyright (C) 2003, 2009-2012 Free Software Foundation, Inc.\r\nCopyright (C) 1988 Larry Wall\r\n\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n\r\nWritten by Larry Wall and Paul Eggert\r\n```\r\nin windows powershell:\r\n```\r\nPS E:\\Projects\\deeplearning\\tensorflow> python configure.py\r\nYou have bazel 0.5.4 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\yanyan\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\yanyan\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\yanyan\\Anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: y\r\nGDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: y\r\nVERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]:\r\n\r\n\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]:\r\n\r\n\r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\n\r\n\r\nPS E:\\Projects\\deeplearning\\tensorflow> bazel run -c opt //tensorflow/cc/example:example\r\nERROR: E:/projects/deeplearning/tensorflow/tensorflow/cc/example/BUILD:1:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl\", line 122\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"E:/projects/deeplearning/tensorflow/tensorflow/workspace.bzl\", line 103, in _apply_patch\r\n                fail(\"patch command is not found, ple...\")\r\npatch command is not found, please install it and referenced by '//tensorflow/cc/example:example'.\r\nERROR: Analysis of target '//tensorflow/cc/example:example' failed; build aborted.\r\nINFO: Elapsed time: 15.968s\r\nERROR: Build failed. Not running target.\r\n```", "comments": ["@jrvb, any suggestions? ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "bazel in ubuntu is fine, closed."]}, {"number": 12929, "title": "error when giving this command bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/home/dile/dataset/dosa", "body": "Traceback (most recent call last):\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 108, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py\", line 76, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"/home/dile/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 28, in <module>\r\n    from autograd import core as ag_core\r\nImportError: No module named autograd", "comments": ["What was the solution to this problem?"]}, {"number": 12928, "title": "Improvements to Java API handling of datatypes and arrays", "body": "This is a set of connected changes broken out from #11535 (Generic Java API) and improved\r\na bit further; still synergistic with that PR  and should allow whittling that PR down toward a digestible size.\r\n\r\n* Changed how datatypes are computed from arrays to not crawl over the array but instead just inspect the array's class object. This seems somewhat simpler, more efficient, and easier to maintain going forward.\r\n* Added the notion that a given array component type can correspond to more than one tensor datatype. I think that we already would like this to be able to support both uint8 and string using byte arrays, and the need will become more acute as more TensorFlow types are added.\r\n* Resurrected the test case for creating and extracting uint8 tensors (requires previous change)\r\n", "comments": ["@andrewcmyers, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @tensorflower-gardener and @jhseu to be potential reviewers.", "Can one of the admins verify this patch?", "The resurrected test case was written for #12797 but wouldn't work without this additional flexibility for tensor creation.", "Jenkins, test this please", "Windows should be fixed now.\r\n\r\nJenkins, test this please."]}, {"number": 12927, "title": "No op named GatherTree using BeamSearchDecoder", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5\r\n- **GPU model and memory**: GeForce GTX 1080 (8 GB)\r\n\r\n### Describe the problem\r\nI was implementing a seq2seq model, and inference went well using greedy algorithm(GreedyEmbeddingHelper). But when I tried to use BeamSearchDecoder to infer from a trained model, I encountered \"No op named GatherTree in defined operations.\". Strangely enough, I couldn't find the same error elsewhere. \r\n\r\n### Error message\r\n```\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 88, in <module>\r\n    out_file='result/result.out', checkpoint=checkpoint)\r\n  File \"infer.py\", line 48, in predict\r\n    loader = tf.train.import_meta_graph(checkpoint + '.meta')\r\n  File \"/home/user0/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1686, in import_meta_graph\r\n    **kwargs)\r\n  File \"/home/user0/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 504, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/home/user0/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 283, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named GatherTree in defined operations.\r\n```\r\n\r\n### Source code to reproduce the problem\r\n```\r\n# Inference module\r\nloaded_graph = tf.Graph()\r\nwith tf.Session(graph=loaded_graph) as sess:\r\n\tloader = tf.train.import_meta_graph(checkpoint + '.meta')\r\n\tloader.restore(sess, checkpoint)\r\n\r\n\tinput_data = loaded_graph.get_tensor_by_name('inputs:0')\r\n\tlogits = loaded_graph.get_tensor_by_name('inferences:0')\r\n\tsrc_seq_len = loaded_graph.get_tensor_by_name('source_sequence_length:0')\r\n\ttgt_seq_len = loaded_graph.get_tensor_by_name('target_sequence_length:0')\r\n\r\n\tfor i in range(len(text)):\r\n\t\ttext_seq = src2seq_word(text[i], True)\r\n\t\tanswer = sess.run(logits, {input_data: [text_seq] * batch_size,\r\n\t\t\t                                  tgt_seq_len: [len(text_seq)] * batch_size,\r\n\t\t\t                                  src_seq_len: [len(text_seq)] * batch_size}\r\n\t\t\t                         )[:, :, 0]\r\n```\r\nProgram failed at ```loader = tf.train.import_meta_graph(checkpoint + '.meta')```\r\n\r\n```\r\n# Related code\r\ntraining_logits = tf.identity(train_decoder_output.rnn_output, name='logits')\r\ninference_logits = tf.identity(infer_decoder_output.predicted_ids, name='inferences')\r\n```", "comments": ["@ebrevdo, any ideas what the problem is?", "Try a newer version of TensorFlow?", "i have occured the same problem, do you have any ideas about it? or do you solved it? thanks", "I have same problem using TF 1.3 (CPU version). What should we do to use this model for interference? ", "Try doing this early on:\r\n\r\nfrom tensorflow.contrib.seq2seq.python.ops import beam_search_ops\r\n\r\nI have the feeling that when importing a graphdef that the dynamic loading of the .so with the GatherTree ops hasn't happened.  So adding that import should force the library to load.", "@allenlavoie @sherrym any ideas how to ensure we load the proper dynamic library as part of import meta graph?  Alternatively should we just eagerly import contrib .so's during import tensorflow?", "If we're going to eagerly load the contrib shared objects, we might as well just statically link them into the binary (which I don't have a particular problem with, but may be somewhat contentious). We could also catch op/kernel not found errors in a few places and try again after loading contrib, which might be more palatable.\r\n\r\nLonger-term there are hopes of making some sort of repository where sessions can find the kernels (and ops in this case) they need and load them dynamically. @martinwicke would know more about whether it's being worked on.", "It's too far out to really reason about. \r\n\r\nWe will for now eagerly load all the dlls we find (which are appropriate for the machine architecture we're on, e.g., we may skip the one containing GPU kernels on a machine without GPU, or the one built with AVX512 on a machine without that SIMD instruction set. @gunan FYI.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I'd like to bring this back to our attention.  As we try to make TF more\nmodular, including loading of external kernels, this kind of issue will\ncome up over and over again.  We need a way to describe that a certain\nplugin/kernel is needed when loading a metagraph, and at least issuing an\nerror message like \"please import the following python module before\nloading the metagraph\"; perhaps with a centralized hint table that maps\nkernels to c++ library names / python module names.\n\nOn Tue, Oct 31, 2017 at 9:23 AM, Martin Wicke <notifications@github.com>\nwrote:\n\n> It's too far out to really reason about.\n>\n> We will for now eagerly load all the dlls we find (which are appropriate\n> for the machine architecture we're on, e.g., we may skip the one containing\n> GPU kernels on a machine without GPU, or the one built with AVX512 on a\n> machine without that SIMD instruction set. @gunan\n> <https://github.com/gunan> FYI.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12927#issuecomment-340817682>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2PmzOTyrhae7zMGp5Zix6mjdy2nks5sx0lxgaJpZM4PR3m6>\n> .\n>\n", "+anna\n\nOn Fri, Jan 12, 2018 at 9:46 AM, ebrevdo <notifications@github.com> wrote:\n\n> I'd like to bring this back to our attention. As we try to make TF more\n> modular, including loading of external kernels, this kind of issue will\n> come up over and over again. We need a way to describe that a certain\n> plugin/kernel is needed when loading a metagraph, and at least issuing an\n> error message like \"please import the following python module before\n> loading the metagraph\"; perhaps with a centralized hint table that maps\n> kernels to c++ library names / python module names.\n>\n> On Tue, Oct 31, 2017 at 9:23 AM, Martin Wicke <notifications@github.com>\n> wrote:\n>\n> > It's too far out to really reason about.\n> >\n> > We will for now eagerly load all the dlls we find (which are appropriate\n> > for the machine architecture we're on, e.g., we may skip the one\n> containing\n> > GPU kernels on a machine without GPU, or the one built with AVX512 on a\n> > machine without that SIMD instruction set. @gunan\n> > <https://github.com/gunan> FYI.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 12927#issuecomment-340817682>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> ABtim2PmzOTyrhae7zMGp5Zix6mjdy2nks5sx0lxgaJpZM4PR3m6>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12927#issuecomment-357302105>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxfT3wDDN44Q8mVoOiws0CP5OGJoks5tJ5fIgaJpZM4PR3m6>\n> .\n>\n", "cc/ @gunan \r\n\r\nThe plan so far is to make this less terrible by *removing* the need for kernels to be loaded in python modules, which would address this particular issue. \r\n\r\nBut in general, you are correct: We have no mechanism to map a kernel/op name to a package in which is might be defined. This is no different between static or dynamic linking, if you build a static TF with a specific op, and save a Graph and load it on a TF without that Op, things don't work. \r\n\r\nWe can (and should) make this better (by namespacing, and probably, the equivalent of a TF package manager), but again, this is some time away -- @gunan has the plan.", "OK; I'll assign to @gunan as he can close it out when the master plan is executed ;)", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @allenlavoie, @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @allenlavoie, @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @allenlavoie: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 12926, "title": "Elastic Average optimizer ", "body": "I have implemented a new distributed optimizer \"ElasticAverageOptimizer\". It is mentioned in the following issue:\r\n[#12472](https://github.com/tensorflow/tensorflow/issues/12472#issuecomment-327976993)\r\n\r\nEASGD is an async optimizer. During the training, Each worker will update the local variables and maintains its own local_step, which starts from 0 and is incremented by 1 after each update of local variables. Whenever the communication period divides the local step, the worker requests the current global center variables and then computed the elastic difference between global center variables and local variables. The elastic difference then be used to update both local variables and global variables.\r\n\r\n**[Reference]**: https://arxiv.org/pdf/1412.6651.pdf", "comments": ["Can one of the admins verify this patch?", "@jinxin0924, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @strategist333 to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 12925, "title": "[RFE] tensorboard: provide a File->Open -like UI for saved model.pb files", "body": "This was originally solicited as https://github.com/tensorflow/tensorflow/issues/8854 by @brandondutra but got derailed with a very nice tools contribution from @jubjamie / @brandondutra\r\n\r\nThe original request would still, imo, make for a nice addition to tensorboard; specifically, were the UI of a running instance of TB to have something akin to a File->Open function to load a saved model .pb file, users who were tasked with the use case of \"person gives them a saved model to inspect\" would appreciate it.\r\n", "comments": ["As stated in our github issue template, you should file this under tensorflow/tensorboard github repository.\r\n"]}, {"number": 12924, "title": "How to fetch the last batch in a multigpu model", "body": "The new dataset API is awesome, but I don't know to fetch the last batch in my model \r\n![image](https://user-images.githubusercontent.com/25046619/30235975-10aad7ce-9543-11e7-9b68-3c25330ee899.png)\r\n![image](https://user-images.githubusercontent.com/25046619/30235993-3d38c9a4-9543-11e7-9698-d13b49e37050.png)\r\n\r\n", "comments": ["@TomorrowIsAnOtherDay I'm sorry, but I'm not understanding from your screenshot what the problem is. You've set a batch size of 12, and the dataset returns 8 batches of that size, consuming 96/100 of the values in your dataset. Are you expecting the batch on the next call to be filled out somehow? ", "@cy89 yes, I wanted to feed the last examples(97/98/99)into my multi-gpu model ", "Thanks, @TomorrowIsAnOtherDay. I think the interface is working-as-intended. It's probably a bad idea for datasets to automatically pad your data for you out to the desired batch size, as this might have weird effects on your training. I would expect that randomization plus setting some number of epochs is the answer to uniformly using data that doesn't exactly match the batch size. \r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I agree it's a bad idea to pad data, but it'll be helpful to return a tensor of smaller batch.\r\nFor example, if we want to calculate accuracy on a validation set, each sample needs to be fetched before an OutOfRangeError is raised. In tensorpack we have a [`remainder=False` option](http://tensorpack.readthedocs.io/en/latest/modules/dataflow.html#tensorpack.dataflow.BatchData) for this purpose.", "@cy89 @ppwwyyxx Thanks, Maybe I should provide an parameter to specific whether to drop the last batch or not.\r\n@ppwwyyxx But in my case, The problem is not whether to use a small batch size or not to fetch the last batch.I can't fetch the last batch in multigpu model.If I use only one gpu and the problem doesn't exist, because Dataset API will provide the last batch with a smaller batch.\r\n#12855 "]}, {"number": 12923, "title": "Corrected hyperlink for audio training tutorial", "body": "", "comments": ["Can one of the admins verify this patch?", "@joshkyh, thanks for your PR! By analyzing the history of the files in this pull request, we identified @MarkDaoust, @andrewharp and @petewarden to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @joshkyh!"]}, {"number": 12922, "title": "mmap file", "body": "I added a memory mapped file option for the model file. Since the model files have to be copied out of the apk due to assets directory readonly status and inability to create the FileChannel directly, it is assumed that you have done so to internal storage with the same context that is passed into the overloaded constructor. Internal storage eliminates problems of SD card not being present or M runtime permissions requests but is also in danger of being full etc. Expansion files are the only other option, but this requires a download from the play store, which is an extra step. I therefore did the most simple thing.", "comments": ["Can one of the admins verify this patch?", "@cloudbank, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @asimshankar and @tensorflower-gardener to be potential reviewers.", "@andrewharp  Ideally, this heavy create operation involving the big graph file is done off heap avoiding IO exactly once and the classifier object persisted for further usage. Of course all of this is taken care of outside of the TFII in my work and I could add an example in another PR. \r\nWhile doing the benchmarking, I started to get \r\n ```\r\n Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef\r\n                                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph\r\n\r\nfor the change to remove the loadGraph(InputStream ...\r\n replaced by loadGraph(byte[] and the byte[] creation in \r\n public TensorFlowInferenceInterface(AssetManager assetManager, String model) {\r\n```\r\nI reverted my code wrt those changes, but did not push them back.\r\n benchmarks:\r\nIO :TensorFlowInferenceInterface(AssetManager assetManager, String model)  ~ 1.6 secs\r\nmmap: TensorFlowInferenceInterface(Context ctx, String model)  ~.6 secs w/o copy, 2.2secs w copy\r\n \r\nMemmap is faster by far the second time/copied already (without the copy), which is useful if called more than once, or if separate from the blasted copy.", "I am using the google standard template to reformat code of 2 indent, 4 continuation, 8 tab in AS.  Line 82 where it adds/deletes a line is an example that I cannot seem to get rid of since I brought down the file from upstream and did not edit those lines.  If you know what is happening I would appreciate a suggestion as to get rid of that behavior.  I went through here and took out any added ws.", "@andrewharp  Did you review the changes?", "@andrewharp I changed it to copy the file from res/raw to cache temporarily, then map it and delete the copy.  Even with the copy, this version takes the create to **.6 seconds on average,** from 1.6 seconds that I previously benchmarked using the assets/ and IO, with min .3 seconds/max 1.2seconds. It requires that you pass in the model file without the pb extension as a  resource id and of course store the file in res/raw.  I have tried to clean up the ws again.", "@yifeif  Hi: I changed my solution but it seems Andrew is busy. Is it possible to re-assign it?", "@andrewharp this version works best in terms of average speed.", "@yifeif  Can you give me a hint as to how I work this through with @andrewharp  to get it finished?  I don't know what he wants in addition to what I have changed.", "@cloudbank Apologies for the delay, up against some deadlines internally.", "@andrewharp NP.  I don't mind at all as long as you eventually get back to\r\nme.  : )\r\n\r\n\r\n\r\n\r\n\r\n\r\nOn Mon, Sep 25, 2017 at 3:16 PM, Andrew Harp <notifications@github.com>\r\nwrote:\r\n\r\n> @cloudbank <https://github.com/cloudbank> Apologies for the delay, up\r\n> against some deadlines internally.\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/pull/12922#issuecomment-332029044>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ACLqpqKr5HFcd_P9KnDzZ7d9YrGzQoIkks5smCY0gaJpZM4PRzkg>\r\n> .\r\n>\r\n", "@andrewharp Do you have any estimate (next week?) when you will be able to finish this?", "@cloudbank I still have reservations about limiting this to a res/raw implementation, would like the questions mentioned before clarified:\r\n\r\n- Does noCompressExtensions not solve the compression problem for assets/?\r\n- Why does it give you an OOM error when mapping from the cache dir, but not from assets/?\r\n\r\n", "@andrewharp\r\n-no compression either using gradle or bazel leads to OOM.  It is\r\nunreasonable I think to not make every effort to compress such a large file\r\nin a 100MB limit and kind of violates best practices.  I tried to map the\r\nuncompressed file directly and ran out of memory right away.  I am building\r\nthe java and native libraries with bazel and then using them in a gradle\r\nbuilt app.  We want people to build stuff with it, right?\r\n-using the res/raw and cache files dir doesn't run out of memory.  It might\r\nrun a little faster than using assets and cache since res are indexed.  I\r\nwill do a benchmark for both and get back to you.  If it is not a big\r\ndifference and you really want assets dir I can move it back there.\r\n-I have not given up on trying to map the file without copying but\r\n  1. every SO thread, and advice about this says it is not possible from\r\nexpert level people including Pete Warden and Mark Murphy.\r\n  2. I have tried every reasonable approach without getting into a hack so\r\nit might be best to start with this approach and if I, or someone else\r\nstumbles upon a better solution, it can be improved.  Given the natural way\r\nandroid is set up right now it seems improbable that copying can be avoided.\r\n\r\n\r\n\r\n\r\n\r\nOn Wed, Sep 27, 2017 at 11:34 AM, Andrew Harp <notifications@github.com>\r\nwrote:\r\n\r\n> @cloudbank <https://github.com/cloudbank> I still have reservations about\r\n> limiting this to a res/raw implementation, would like the questions\r\n> mentioned before clarified:\r\n>\r\n>    - Does noCompressExtensions not solve the compression problem for\r\n>    assets/?\r\n>    - Why does it give you an OOM error when mapping from the cache dir,\r\n>    but not from assets/?\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/pull/12922#issuecomment-332613959>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ACLqptmrY2axZzCVTdzDA96kJ1J_DCMWks5smpU2gaJpZM4PRzkg>\r\n> .\r\n>\r\n", "@andrewharp I benchmarked it and the res/raw solution comes back on average in just under a second with a best time average around .5 seconds.  The assets solution never comes back any where near that fast on average, with around 1.2-1.3seconds.  Could you live with the idea that if you want to use the overloaded create that uses a memmap, you need to move your files over to res/raw?  Not that big a deal.  You also have to remove the file extension, and I could add a check for that.", "@yifeif Can you give me a hint as to how I work this through with @andrewharp to get it finished? I don't know what he wants in addition to what I have changed.", "@cloudbank we always try to look at the PRs as soon as possible, but sometimes we might also have internal deadlines to meet and can have some delay. Thanks again for your patience.\r\n\r\n@andrewharp could you let @cloudbank know whenever you get a chance? Thanks a lot!\r\n\r\n", "@cloudbank I'm not sure I understand why no compression leads to OOM. Where is the 100mb limit coming from? For a properly trained model, the entropy should be high enough that you won't get any significant compression gains anyway. For reference this is the unzip -v output for the graphs in the nightly build of tensorflow_demo.apk:\r\n\r\n```\r\n Length   Method    Size  Cmpr    Date    Time   CRC-32   Name\r\n--------  ------  ------- ---- ---------- ----- --------  ----\r\n 3771239  Stored  3771239   0% 1980-01-01 00:00 9225f2cc  assets/conv_actions_frozen.pb\r\n29083865  Defl:X 25434578  13% 1980-01-01 00:00 ac57f5c3  assets/ssd_mobilenet_v1_android_export.pb\r\n  563897  Defl:X   257084  54% 1980-01-01 00:00 7d7f4ff3  assets/stylize_quantized.pb\r\n53884595  Stored 53884595   0% 1980-01-01 00:00 6d3b8d15  assets/tensorflow_inception_graph.pb\r\n\r\n```\r\nCompression was not deliberately turned off here, zip just decided it wasn't worth it for 2/4 graphs. The savings from the other two are very minimal overall. The fact that stylize (already by far the smallest graph) gets such a large boost probably points to an opportunity to shrink its design even more.\r\n\r\nWhat I think might be ideal would be to have the res/raw path as you have it now, and also the assets/ path if the user has not chosen to compressed the pb file. Then the user automatically gets the improvement if they set things up the default way with assets (e.g. the default classifier demo will be improved), and get the even bigger speed boost you've found if they take the time to move things to res/raw. Unless I am misunderstanding something, and there's some intrinsic difference between memmapping from uncompressed assets and e.g. /sdcard/tmp\r\n\r\n", "@andrewharp\nI was just thinking to change it to detect compressed or not myself...great\nidea. I will change it over the next few days.\n\nBest regards,\n\nSabine G. Vogel\n\n\n\n\nOn Wed, Oct 11, 2017 at 1:16 PM, Andrew Harp <notifications@github.com>\nwrote:\n\n> @cloudbank <https://github.com/cloudbank> I'm not sure I understand why\n> no compression leads to OOM. Where is the 100mb limit coming from? For a\n> properly trained model, the entropy should be high enough that you won't\n> get any significant compression gains anyway. For reference this is the\n> unzip -v output for inception in tensorflow_demo.apk:\n>\n>  Length   Method    Size  Cmpr    Date    Time   CRC-32   Name\n> --------  ------  ------- ---- ---------- ----- --------  ----\n>  3771239  Stored  3771239   0% 1980-01-01 00:00 9225f2cc  assets/conv_actions_frozen.pb\n> 29083865  Defl:X 25434578  13% 1980-01-01 00:00 ac57f5c3  assets/ssd_mobilenet_v1_android_export.pb\n>   563897  Defl:X   257084  54% 1980-01-01 00:00 7d7f4ff3  assets/stylize_quantized.pb\n> 53884595  Stored 53884595   0% 1980-01-01 00:00 6d3b8d15  assets/tensorflow_inception_graph.pb\n>\n>\n> Compression was not deliberately turned off here, zip just decided it\n> wasn't worth it for 2/4 graphs. The savings from the other two are very\n> minimal overall.\n>\n> What I think might be ideal would be to have the res/raw path as you have\n> it now, and also the assets/ path if the user has not chosen to compressed\n> the pb file. Then the user automatically gets the improvement if they set\n> things up the default way with assets (e.g. the default classifier demo\n> will be improved), and get even more speed you've found if they take the\n> time to move things to res/raw.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12922#issuecomment-335933716>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACLqphn-f7Ie3UiuI1E-RHsH7bKFtUtJks5srSIKgaJpZM4PRzkg>\n> .\n>\n", "@cloudbank any update?", "@andrewharp @martinwicke I will work on it today.  I am just super busy.\r\n\r\n\r\n\r\n\r\nOn Tue, Nov 7, 2017 at 1:00 PM, Martin Wicke <notifications@github.com>\r\nwrote:\r\n\r\n> @cloudbank <https://github.com/cloudbank> any update?\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/pull/12922#issuecomment-342619505>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ACLqpsG_lVruq5VnduIve4y4yIP3cHXtks5s0MTRgaJpZM4PRzkg>\r\n> .\r\n>\r\n", "@andrewharp prior to Android 2.3, any compressed asset file with an uncompressed size of over 1 MB cannot be read from the assets, but then who uses prior to 2.3?  I am using gradle with compression and both my graphs are compressedly by 25 and 53 % in the apk.  I do not see any way to control whether or not the files are to be compressed---even if I do get into the build programmatically, obviously from your input it may not matter. So, I will check both places, and cross their fingers that mem map will open the file.\r\n", "@andrewharp @martinwicke I had to upgrade bazel,ruby, AS 3.0, build tools... I reset my fork because I was so far behind on time and  I wanted a clean slate.  The new PR is  #14351", "Thanks, closing this one."]}, {"number": 12921, "title": "autoconf error when running build_all_ios.sh", "body": "When running the build_all_ios.sh script on iOS I am getting the following error:\r\n\r\n`+ autoreconf -f -i -Wall,no-obsolete\r\nconfigure.ac:30: error: possibly undefined macro: AC_PROG_LIBTOOL\r\n      If this token and others are legitimate, please use m4_pattern_allow.\r\n      See the Autoconf documentation.\r\nautoreconf: /usr/local/Cellar/autoconf/2.69/bin/autoconf failed with exit status: 1`\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Not getting to the point of running code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX High Sierra\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: Python 2.7.10\r\n- **Bazel version (if compiling from source)**: release 0.5.4-homebrew\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: NVIDIA GeForce GT 750M 2GB\r\n- **Exact command to reproduce**: ./tensorflow/contrib/makefile/build_all_ios.sh\r\n", "comments": ["I was able to solve this by uninstalling libtool, autoconf, and automake and deleting all local cahecs that brew had.  From there a fresh install of automake, libtool fixed it", "Good to see you figured it out!"]}, {"number": 12920, "title": "Workaround for NVCC 9.0 internal error", "body": "Fixes 'Internal Compiler Error (codegen)' encountered when building with nvcc 9.0.", "comments": ["@nluehr, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener to be a potential reviewer.", "Can one of the admins verify this patch?", "Just one minor comment.  Unfortunately some internal code just changed that will interact with this code.  I'm going to hold off on approval so that the merge can be resolved here rather than when trying to pull internally.", "@ekelsen is there an ETA on when the internal changes will be published so that we can complete this PR?", "Sorry the internal changes are done and the fix for the sum compiler bug is already integrated.", "Great. I already rebased to incorporate those changes. So hopefully this PR is ready to merge.", "Jenkins, test this please."]}, {"number": 12919, "title": "Changed number to float in documentation example \"Get Started\" in order to be consistent", "body": "Stumbled upon this while going through the \"Get Started\".\r\nadded a dot to the `7` to make clear it's a float (like every other number)", "comments": ["Can one of the admins verify this patch?", "Thanks @sauercrowd!"]}]