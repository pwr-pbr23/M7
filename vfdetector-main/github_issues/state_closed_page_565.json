[{"number": 36742, "title": "[Intel MKL] Fix dequantize accuracy issue and re-enable this OP", "body": "", "comments": ["Could you please resolve the conflicts?", "@penpornk Thanks for the quick review. The conflict has been resolved, please help to check. Thank you!", "It seems we have a conflict again. Could you please resolve it? Thank you!\r\nWe also get a `mkl_dequantize_test` failure in `MacOS CPU Python3` and `MacOS Python2 and CC`. [Log](https://source.cloud.google.com/results/invocations/30c8254d-20bd-4a5e-92ab-101a8fafbb2f/targets/%2F%2Ftensorflow%2Fcore%2Fkernels:mkl_dequantize_op_test/log). But this might be because of the merge conflict. I'll rerun the tests after the conflict is resolved.", "@penpornk Thanks for the remind, the conflict has been resolved, please help to check. Thank you!", "tensorflow/core/kernels:mkl_dequantize_op_test seems to be failing and this PR seems to be a related change.\r\n\r\nPlease take a look.\r\n\r\nRelated log:\r\n\r\nTest output for //tensorflow/core/kernels:mkl_dequantize_op_test:\r\ndyld: malformed mach-o: load commands size (40648) > 32768", "@smit-hinsu Thanks for the reminder, I will check.", "@guizili0 Have you had a chance to look at this yet? Would it be possible to get it fixed before the 2.2 branch cut this Wednesday? Currently the tests are disabled in commit https://github.com/tensorflow/tensorflow/commit/ebf01547f5f76ae0c65e708c09d60aa8e06c30a9.", "@penpornk Sorry for this late fix. The fix PR is here https://github.com/tensorflow/tensorflow/pull/37030 please help to check. Thank you!\r\n", "@guizili0 Just approved it. Thank you very much!"]}, {"number": 36741, "title": "Fix a typo in imagenet run_eval readme", "body": "", "comments": []}, {"number": 36740, "title": "Image Classification pretrained model breaks for batch size 1 with BaseCollectiveExecutor::StartAbort error", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No, code used from https://github.com/tensorflow/tensorrt/tftrt/examples/image-classification\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): RHEL 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): Binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: - Bazel\r\nversion (if compiling from source): \r\n- GCC/Compiler version (if compiling from\r\nsource): 7.3.0\r\n- CUDA/cuDNN version: 10.2/7.6.5\r\n- GPU model and memory: Tesla T4\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRelated issue: https://github.com/tensorflow/tensorrt/issues/169: Link: https://github.com/tensorflow/tensorrt/issues/169#issuecomment-572383212 \r\nThis example uses a frozen graph that was downloaded from here: http://download.tensorflow.org/models/official/resnet_v1_imagenet_savedmodel.tar.gz\r\n\r\nThis issue happens for synthetic as well as a validation runs. I was making my runs using this call:\r\n```\r\npython image_classification.py --input_saved_model_dir data/resnet_v1_50/1523293981 --data_dir /home/mayroy13/Mayank/Mayank/test/imagenet --calib_data_dir /home/mayroy13/Mayank/Mayank/test/imagenet --minimum_segment_size 3 --output_saved_model_dir trt_engine --batch_size 1 --use_trt --num_iterations 100 --precision FP16 --mode validation --max_workspace_size $((2**32))\r\n```\r\nAnd it fails with the following error:\r\n```\r\nBenchmark arguments:\r\n  batch_size: 1\r\n  calib_data_dir: /home/mayroy13/Mayank/Mayank/test/imagenet\r\n  data_dir: /home/mayroy13/Mayank/Mayank/test/imagenet\r\n  display_every: 100\r\n  gpu_mem_cap: 0\r\n  input_saved_model_dir: data/resnet_v1_50/1523293981\r\n  input_size: 224\r\n  max_workspace_size: 4294967296\r\n  minimum_segment_size: 3\r\n  mode: validation\r\n  num_calib_inputs: 50\r\n  num_classes: 1001\r\n  num_iterations: 100\r\n  num_warmup_iterations: 50\r\n  optimize_offline: False\r\n  output_saved_model_dir: trt_engine\r\n  precision: FP16\r\n  preprocess_method: vgg\r\n  target_duration: None\r\n  use_synthetic: False\r\n  use_trt: True\r\nTensorRT Conversion Params:\r\n  is_dynamic_op: True\r\n  max_batch_size: 1\r\n  max_workspace_size_bytes: 4294967296\r\n  maximum_cached_engines: 100\r\n  minimum_segment_size: 3\r\n  precision_mode: FP16\r\n  rewriter_config_template: None\r\n  use_calibration: False\r\nConversion times:\r\n  conversion: 40.5s\r\n2020-02-13 22:40:45.510560: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 1) and num_split 2\r\n         [[{{node PartitionedCall/split_inputs/split}}]]\r\n2020-02-13 22:40:45.510560: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 1) and num_split 2\r\n         [[{{node PartitionedCall/split_inputs/split}}]]\r\n         [[PartitionedCall/split_inputs/split/_2]]\r\n2020-02-13 22:40:45.511446: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Invalid argument: Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 1) and num_split 2\r\n         [[{{node PartitionedCall/split_inputs/split}}]]\r\n         [[PartitionedCall/TRTEngineOp_0/_6]]\r\nTraceback (most recent call last):\r\n  File \"image_classification.py\", line 464, in <module>\r\n    target_duration=args.target_duration)\r\n  File \"image_classification.py\", line 231, in run_inference\r\n    batch_preds = graph_func(batch_images)[0].numpy()\r\n  File \"/home/mayroy13/anaconda3/envs/wml17/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1551, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/home/mayroy13/anaconda3/envs/wml17/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1591, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/home/mayroy13/anaconda3/envs/wml17/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/mayroy13/anaconda3/envs/wml17/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/home/mayroy13/anaconda3/envs/wml17/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 1) and num_split 2\r\n         [[node PartitionedCall/split_inputs/split (defined at image_classification.py:134) ]]\r\n         [[PartitionedCall/split_inputs/split/_2]]\r\n  (1) Invalid argument:  Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 1) and num_split 2\r\n         [[node PartitionedCall/split_inputs/split (defined at image_classification.py:134) ]]\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_pruned_25839]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n```\r\n**Describe the expected behavior**\r\nThe expected behaviour would be for the code to complete and print metrics for the run.\r\n", "comments": ["Unfortunately the original author of image_classification.py has moved on to other things.  @DEKHTIARJonathan do you want to take a look?", "@DEKHTIARJonathan did you get a chance to have a look at this?", "Hi sorry,\r\nI'm not able to look into this one currently", "@mankeyboy  It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.5 or 2.4.1 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36740\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36740\">No</a>\n"]}, {"number": 36739, "title": "How to get original string data back from TFRecordData", "body": "I followed TensorFlow guide to save my string data using:\r\n```\r\ndef _create_string_feature(values):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values.encode('utf-8')]))\r\n```\r\nI also used ```[\"tf.string\", \"FixedLenFeature\"]``` as my feature original type, and ```\"tf.string\"``` as my feature convert type.\r\n\r\nHowever, during my training when I run my session and I create iterators, my string feature for a batch size of 2 (for example: ['food fruit', 'cupcake food' ]) would be like below. The problem is that this list is of size 1, and not 2 (batch_size=2), why instances in one batch are sticked (clubbed) together rather than being splitted?\r\n\r\n```\r\n[b'food fruit' b'cupcake food']\r\n```\r\nFor my other features which are int or float, they are numpy arrays of shape (batch_size, feature_len) which are fine but not sure why string features are not separated in a single batch?\r\n\r\nAny help would be appreciated.", "comments": ["@Hannabrahman please provide us with complete stand alone code so we could replicate the issue in our environment, along with tensorflow version .", "@Saduf2019 thanks for reply.\r\nActually It's not a part of code. However, this is what I want to do. I have an script which prepare my tfrecord data like following:\r\n```\r\n        def _create_int_feature(values):\r\n            return tf.train.Feature(\r\n                int64_list=tf.train.Int64List(value=list(values)))\r\n\r\n        def _create_floats_feature(values):\r\n            return tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\r\n\r\n        def _create_string_feature(values):\r\n            return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values.encode('utf-8')])) # m.encode('utf-8') for m in values\r\n\r\n        features = collections.OrderedDict()\r\n        features[\"x1_ids\"] = _create_int_feature(fea[\"x1_ids\"])\r\n        features[\"x1_len\"] = _create_int_feature([fea[\"x1_len\"]])\r\n        features[\"x1x4_ids\"] = _create_int_feature(fea[\"x1x4_ids\"])\r\n        features[\"x1x4_len\"] = _create_int_feature([fea[\"x1x4_len\"]])\r\n        features[\"x4_m\"] = _create_floats_feature(m_fea)\r\n        features[\"label\"] = _create_string_feature(label)\r\n```\r\nHere x1_ids, x1_len, x1x4_ids, x1x4_len, and x4_m are all integers and float. However label is string.\r\nI want to use label as string during my training. I want to do some string matching.\r\nWhen running my session in tf and creating iterators, my other features format and dimension are numpy array (batch_size, feature_len). However my ```features[\"label\"]``` is a list of size one.\r\nthis is how I do this:\r\n```\r\n    datasets = {}\r\n    train_dataset = tx.data.TFRecordData(hparams=config_train.train_hparam)\r\n    datasets['train'] = train_dataset\r\n    dev_dataset = tx.data.TFRecordData(hparams=config_train.dev_hparam)\r\n    datasets['dev'] = dev_dataset\r\n    test_dataset = tx.data.TFRecordData(hparams=config_train.test_hparam)\r\n    datasets['test'] = test_dataset\r\n    iterator = tx.data.FeedableDataIterator(datasets)\r\n    batch = iterator.get_next()\r\n    batch_size = tf.shape(batch['x1x4_ids'])[0]\r\n\r\n    with tf.Session(config=session_config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n        sess.run(tf.tables_initializer())\r\n\r\n###### SOME CODE TO INITIALIZE MY MODEL ########\r\n\r\n\r\n        iterator.initialize_dataset(sess)\r\n\r\n        if FLAGS.do_train:\r\n            for epoch in range(config_train.max_train_epoch):\r\n                print(\"Training epoch {}\".format(epoch))\r\n                _train_epoch(sess, epoch==0)\r\n```\r\n\r\nHere is the first part of my train function:\r\n```\r\n    def _train_epoch(sess, initial=False):\r\n        \"\"\"Trains on the training set, and evaluates on the dev set\r\n        periodically.\r\n        \"\"\"\r\n        iterator.restart_dataset(sess, 'train')\r\n\r\n        while True:\r\n            try:\r\n                # (1) Get data and yy sample\r\n                fetches_data = {\r\n                    'batch': batch,\r\n                    'batch_size': batch_size,\r\n                }\r\n                feed_dict_data = {\r\n                    iterator.handle: iterator.get_handle(sess, 'train'),\r\n                    tx.global_mode(): tf.estimator.ModeKeys.PREDICT,\r\n                }\r\n                rets_data = sess.run(fetches_data, feed_dict_data)\r\n```\r\n\r\nMy issue is that the ```rets_data[\"label\"]``` is not a list of length batch_size. As I said in my previous post, for example if label before creating tfrecord is ```['food fruit', 'cupcake food' ]``` (just the first two element for example) after doing tfrecord when I print ```rets_data[\"label\"]``` for one batch it would be like:\r\n```\r\n[b'food fruit' b'cupcake food']\r\n```\r\nwhy they are not separated?", "@Hannabrahman Closing this issue as it is not related to bug/performance, build/install, feature request or docs related issues.\r\n\r\nPlease post this question in stack overflow as there is a bigger community to respond. Thanks! ", "@gowthamkpr I have already asked it [here](https://stackoverflow.com/questions/60177218/how-to-get-original-string-data-back-from-tfrecorddata). But received no response. So TensorFlow github doesn't allow asking questions?"]}, {"number": 36738, "title": "[Intel MKL] Updating MatMul kernels with MKLDNN 1.x API changes", "body": "This PR updates QMatMul and FusedMatMul MKL CPU kernels with MKLDNN 1.0 API.\r\nIt also updates MatMul and BatchMatMul BFloat16 kernels for MKL CPU with MKLDNN 1.2 API.\r\nSome of the changes are suggested by clang formet check tool 8.0.1 version.", "comments": ["hi @penpornk, thanks for review. I've addressed your comments. Pls take a look.", "@penpornk Thanks a lot for quick review and approval!", "@nhasabni Can you please resolve conflicts? Thanks!", "@penpornk I've resolved merge conflict. Pls take a look."]}, {"number": 36737, "title": "Updating TF to start using Bazel 2.0.0", "body": "Updating TF to start using Bazel 2.0.0\r\n", "comments": ["issue is fixed with https://github.com/YINZHI-keji/tensorflow/pull/4634", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36737\">No</a>\n"]}, {"number": 36736, "title": "Sudden OOM error with Tensorflow after several successfully trained batches", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Ubuntu 18.04.3 LTS \r\n- TensorFlow installed from (source or binary): tensorflow installed from docker image\r\n- TensorFlow version (use command below): v2.1.0\r\n- Python version: 3.6.9\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: V10.1.243\r\n- GPU model and memory: NVIDIA Corporation GM200 [GeForce GTX TITAN X] [10de:1132] and 12 GB of memory\r\n\r\n**Description**: I'm running an Encoder-Decoder architecture to do video captioning. For this I created three classes based on keras.Model. The first model has several 3D convolutional layers, normalization and 3D max pooling layers and I use it to describe the video, it is initialized and called inside the Encoder model. This encoder model is called once for each video and passes information to the decoder model which is called several times (as many as the number of words in the caption).\r\n\r\n**Input shape**: The dimensions of the input video are (128,224,224,3) where 128 indicates the number of frames and the dimensions of the sentences are (1,55) where 55 indicates the number of tokens.\r\n\r\n**Problem**: I'm using a batch of 1 (video) and the model trains well up to a certain number of videos and then throws an OOM error as seen in the picture. I have tried to lower the number of frames to 64 and 32 but the same error keeps coming up but, interestingly, the error takes longer to come up as I lower the frames, this makes me think that in a certain place the memory is being used in a cumulative way. From the error I can deduce that it appears in the backpropagation of the first layer maxpoling3d. However, I don't understand why the model runs well for some videos and then throws the error, if it was by memory, it should break in the first video, right? I attach the training function I'm using and the data generator.\r\n\r\n![Selecci\u00f3n_002](https://user-images.githubusercontent.com/18648306/74474042-b057be00-4e72-11ea-8de4-6cd0486ac4c0.png)\r\n![Selecci\u00f3n_003](https://user-images.githubusercontent.com/18648306/74474072-bcdc1680-4e72-11ea-961c-dd4532beda0c.png)\r\n![Selecci\u00f3n_001](https://user-images.githubusercontent.com/18648306/74473583-d7fa5680-4e71-11ea-8c86-f8f7420cd139.png)\r\n", "comments": ["@JotaRodriguez94, Thanks for reporting this issue. \r\nCould you provide the complete standalone code to replicate the reported issue. Thanks!", "@JotaRodriguez94, Can you share the complete code to analyze the issue. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Check batch length in your training set may help. In my situation (nlp task), sequence length is different between batches (which depends on the longest sequence within that batch). So chances are that the records in that OOM batch may contain very long sequence."]}, {"number": 36735, "title": "Re-compiling keras model causes save / load_model to fail", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: docker container running RHEL 7.7\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: 2.1\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: running on CPU\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras import metrics\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(64, activation='relu', input_shape=(13,)))\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dense(3, activation='relu'))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy', metrics=[\"accuracy\"], optimizer=\"SGD\")\r\n\r\n# Compiling again with the same loss, optimizer and metrics\r\nmodel.compile(loss=model.loss, optimizer=model.optimizer, metrics=model.metrics)\r\n\r\nmodel.save(\"my_model.h5\")\r\nloaded = tf.keras.models.load_model(\"my_model.h5\")\r\n```\r\n\r\n### Describe the problem\r\nCompiling keras model again and saving it as h5 file causes `load_model` to fail with `TypeError: __init__() missing 1 required positional argument: 'fn'`. \r\n\r\nThis issue does not happen if the model is compile once.\r\n\r\nHappening in both tensorflow 2.0 and 2.1\r\n\r\n### Source code / logs\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-a65550ed8fb1> in <module>\r\n     15 \r\n     16 model.save(\"my_model.h5\")\r\n---> 17 loaded = tf.keras.models.load_model(\"my_model.h5\")\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    144   if (h5py is not None and (\r\n    145       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\r\n--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n    147 \r\n    148   if isinstance(filepath, six.string_types):\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\r\n    182       # Compile model.\r\n    183       model.compile(**saving_utils.compile_args_from_training_config(\r\n--> 184           training_config, custom_objects))\r\n    185 \r\n    186       # Set optimizer weights.\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    427     with K.get_graph().as_default():\r\n    428       # Save all metric attributes per output of the model.\r\n--> 429       self._cache_output_metric_attributes(metrics, weighted_metrics)\r\n    430 \r\n    431       # Set metric attributes on model.\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in _cache_output_metric_attributes(self, metrics, weighted_metrics)\r\n   1840         output_shapes.append(output.shape.as_list())\r\n   1841     self._per_output_metrics = training_utils.collect_per_output_metric_info(\r\n-> 1842         metrics, self.output_names, output_shapes, self.loss_functions)\r\n   1843     self._per_output_weighted_metrics = (\r\n   1844         training_utils.collect_per_output_metric_info(\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py in collect_per_output_metric_info(metrics, output_names, output_shapes, loss_fns, is_weighted)\r\n    878     metrics_dict = OrderedDict()\r\n    879     for metric in metrics:\r\n--> 880       metric_name = get_metric_name(metric, is_weighted)\r\n    881       metric_fn = get_metric_function(\r\n    882           metric, output_shape=output_shapes[i], loss_fn=loss_fns[i])\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py in get_metric_name(metric, weighted)\r\n   1072       return metric\r\n   1073 \r\n-> 1074     metric = metrics_module.get(metric)\r\n   1075     return metric.name if hasattr(metric, 'name') else metric.__name__\r\n   1076   else:\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py in get(identifier)\r\n   3060 def get(identifier):\r\n   3061   if isinstance(identifier, dict):\r\n-> 3062     return deserialize(identifier)\r\n   3063   elif isinstance(identifier, six.string_types):\r\n   3064     return deserialize(str(identifier))\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py in deserialize(config, custom_objects)\r\n   3054       module_objects=globals(),\r\n   3055       custom_objects=custom_objects,\r\n-> 3056       printable_module_name='metric function')\r\n   3057 \r\n   3058 \r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    303                 list(custom_objects.items())))\r\n    304       with CustomObjectScope(custom_objects):\r\n--> 305         return cls.from_config(cls_config)\r\n    306     else:\r\n    307       # Then `cls` may be a function returning a class.\r\n\r\n/opt/conda/envs/py-tensorflow_2_0_0/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in from_config(cls, config)\r\n    517         A layer instance.\r\n    518     \"\"\"\r\n--> 519     return cls(**config)\r\n    520 \r\n    521   def compute_output_shape(self, input_shape):\r\n\r\nTypeError: __init__() missing 1 required positional argument: 'fn'\r\n```\r\n", "comments": ["@ding-c3 please check [this link](https://github.com/tensorflow/tensorflow/issues/30521) and let us know if you could resolve the issue.", "@Saduf2019 Thanks for getting back! \r\n\r\nI don't think that link resolves my issue. Our use case is to load keras model from the model file for further training.\r\n\r\nThe workaround it provided is to set `compile=False` when calling `load_model`. It will cause loading of loss / optimizer / metrics to be skipped and therefore loaded model won't be able to be trained.", "@ding-c3 I am not running into any error using this code below\r\n\r\n```\r\n# Compiling again with the same loss, optimizer but different metrics\r\nmodel.compile(loss=model.loss, optimizer=model.optimizer, metrics=[\"accuracy\"])\r\n\r\nmodel.save(\"my_model.h5\")\r\nloaded = tf.keras.models.load_model(\"my_model.h5\")\r\n```\r\nwhich means the problem is occuring during the deserialization of metrics", "I believe this issue should be fixed soon, I have a change waiting to be reviewed.", "@ding-c3 As mentioned by @k-w-w This issue was fixed recently. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/7533cdc5cc01b747424f7397af8db775/36735.ipynb). Thanks!\r\n\r\nAs it was resolved, I am closing this issue. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36735\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36735\">No</a>\n"]}, {"number": 36734, "title": "AttributeError: 'BatchGen' object has no attribute 'shape'", "body": "I am using TensorFlow version 1.15 for a project. I have converted a BioBert pre-trained model into a Keras layer following the code [here](https://towardsdatascience.com/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2). However, when I run my code, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/jupyter-belona/.conda/envs/mimic-proj/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/jupyter-belona/.conda/envs/mimic-proj/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/jupyter-belona/Untitled_Folder/deep-learning-clinical-forecast/mimic3newmodels/decompensation/main.py\", line\r\n152, in <module>\r\n    verbose=args.verbose)\r\n  File \"/home/jupyter-belona/.conda/envs/mimic-proj/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/trainin\r\ng.py\", line 1296, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/jupyter-belona/.conda/envs/mimic-proj/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/trainin\r\ng_generator.py\", line 144, in model_iteration\r\n    shuffle=shuffle)\r\n  File \"/home/jupyter-belona/.conda/envs/mimic-proj/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/trainin\r\ng_generator.py\", line 477, in convert_to_generator_like\r\n    **num_samples = int(nest.flatten(data)[0].shape[0])**\r\nAttributeError: 'BatchGen' object has no attribute 'shape'\r\n```\r\nPlease, how do I fix this error? I really need your help.", "comments": ["@etetteh Could you please provide us with a simple stand alone code so we could replicate the issue faced by you.", "Please, the steps involve \r\n1. cloning this [repo](https://github.com/YerevaNN/mimic3-benchmarks)\r\n2. creating the tf.Module Keras layer following this [colab file](https://colab.research.google.com/drive/1ofSfThTBlWjOx5dqXmdsIol-MdiqCyZC)\r\nThe model to use is:\r\n```\r\nfrom __future__ import print_function\r\nfrom __future__ import absolute_import\r\n\r\nimport logging\r\nfrom mimic3newmodels.biobert.biobertlayer import BioBertLayer\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.models import Model\r\n\r\n\r\nlog = logging.getLogger('tensorflow')\r\nlog.handlers = []\r\n\r\n\r\nclass Network(Model):\r\n\r\n    def __init__(self, dim, task, num_classes=1, input_dim=76, **kwargs):\r\n\r\n        self.dim = dim\r\n        \r\n        if task in ['decomp', 'ihm', 'ph']:\r\n            final_activation = 'sigmoid'\r\n        elif task in ['los']:\r\n            if num_classes == 1:\r\n                final_activation = 'relu'\r\n            else:\r\n                final_activation = 'softmax'\r\n        else:\r\n            raise ValueError(\"Wrong value for task\")\r\n\r\n        print(\"==> not used params in network class:\", kwargs.keys())\r\n        \r\n        X = Input(shape=(None, input_dim), name='X')\r\n        inputs = [X]\r\n        \r\n        encoder = BioBertLayer(bert_path=\"mimic3newmodels/biobert/bert-module/\", seq_len=48, tune_embeddings=False,\r\n                    pooling='cls', n_tune_layers=3, verbose=False)\r\n                    \r\n        pred = tf.keras.layers.Dense(num_classes, activation=final_activation)(encoder(inputs))\r\n        outputs = [pred]\r\n\r\n        super(Network, self).__init__(inputs=inputs, outputs=outputs)\r\n\t\r\n    def say_name(self):\r\n        return \"{}.n{}\".format('biobert_pubmed',\r\n                                self.dim,\r\n                                )\r\n\r\n```\r\n\r\n3. Download the dataset [here](https://drive.google.com/drive/folders/1KwXMENqypwDW5kQwRPa945ijUVta428v?usp=sharing)\r\n4. run this line of code\r\n`python -um mimic3newmodels.decompensation.main --network mimic3newmodels/biobert/biobert.py --dim 128 --timestep 1.0 --mode train --batch_size 8 --output_dir mimic3newmodels/decompensation/BIOBERT`", "@etetteh \r\nAs per the code shared  imported layer is from \"from mimic3newmodels.biobert.biobertlayer import BioBertLayer\" , whereas the repository shared \"https://github.com/YerevaNN/mimic3-benchmarks/tree/master/mimic3models\" mimic3newmodels is not present in \"mimic3-benchmarks\".\r\n\r\nAlso please refer to this [link](https://github.com/keras-team/keras/issues/12586) if it helps resolve the issue.", "@etetteh \r\nCould  you please respond to the above comment.", "Please find the needed files here:\nhttps://drive.google.com/drive/folders/1i_CGyn5vQ0WTs6KXjB6KMd6nvZ_vqxlo?usp=sharing\n\nOn Mon, Mar 2, 2020 at 9:02 AM Saduf2019 <notifications@github.com> wrote:\n\n> @etetteh <https://github.com/etetteh>\n> Could you please respond to the above comment.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36734?email_source=notifications&email_token=AGZQ72GUAQGOKDOHP2TOVWLRFNKWZA5CNFSM4KUYXM2KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOENOEXAA#issuecomment-593251200>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AGZQ72GVPEUN4SQQWKLL3YLRFNKWZANCNFSM4KUYXM2A>\n> .\n>\n", "@etetteh \r\nI am unable to access the link shared.", "@etetteh \r\nIf possible can you please share a colab link of the same with us, we are unable to replicate with the information shared as it still shows same error as informed earlier.", "> @etetteh\r\n> As per the code shared imported layer is from \"from mimic3newmodels.biobert.biobertlayer import BioBertLayer\" , whereas the repository shared \"https://github.com/YerevaNN/mimic3-benchmarks/tree/master/mimic3models\" mimic3newmodels is not present in \"mimic3-benchmarks\".\r\n> \r\n> Also please refer to this [link](https://github.com/keras-team/keras/issues/12586) if it helps resolve the issue.\r\n\r\nI don't have a colab file as I wasn't working in Colab. \r\nPlease use this instead\r\n`from mimic3models.biobert.biobertlayer import BioBertLayer\"`\r\n\r\nPlease, check the subdirectories well and create __init__.py file if necessary. \r\nCreate a folder inside the mimic3models called biobert and copy the files [here](https://drive.google.com/drive/folders/1i_CGyn5vQ0WTs6KXjB6KMd6nvZ_vqxlo?usp=sharing) \r\ninside it. Please, let me know if you need more info", "@etetteh As this issue is not not related to build/install, bug/performance, feature request or doc related to issues please post this issue in stackoverflow as there is a wider community to respond. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@etetteh\r\nplease update as per above comment"]}, {"number": 36733, "title": "tf.split for unequal splits when num is odd and size_splits is not available", "body": "**System information**\r\n- TensorFlow version: 1.14.0\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen I have a tensor of size say [99, 100, 100, 1] and want to split (batch size 99) it into 4 parts (because I have 4 GPUs). It can't be split with the existing function. I don't always know the batch size so I can't use size_splits as well.\r\n", "comments": ["@kamalravi, Can you provide the sample code snippet to analyze the reported issue. Thanks!", "`    def make_parallel(fn, num_gpus, **kwargs):\r\n        \r\n        in_splits = {}\r\n        for k, v in kwargs.items():\r\n            in_splits[k] = tf.split(v, num_gpus)`\r\n\r\nwhere \r\nfn is the model\r\nkwargs.items() are input ( [99, 100, 100, 1]) and output data ( [99, 100, 100, 1])\r\nnum_gpus is the number of available GPUs.\r\n\r\nI want to split input and output almost evenly for each GPU. Thanks!", "@kamalravi, Did you use Distribution strategy to employ multiple GPUs. Please take a look at different ways to use multiple GPUs in this [doc](https://www.tensorflow.org/guide/distributed_training). Thanks!", "Thanks for the suggestion! Closing the issue."]}, {"number": 36732, "title": "Fix issue templates", "body": "Fixes #36721 and also makes sure style is consistent in all headers.", "comments": []}, {"number": 36731, "title": "Entity <function pfor.<locals>.f at 0x644d84950> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.14.6 (18G2022)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: binary, from Anaconda\r\n- **TensorFlow version (use command below)**: unknown 1.14.0\r\n- **Python version**: 3.7.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cudatoolkit-9.0\r\n- **GPU model and memory**: not used\r\n- **Exact command to reproduce**: run the attached script `get_partial_derivatives.py`.\r\n\r\n\r\n### Describe the problem\r\nI ran into a bug when I run the following small code:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_v2_behavior()\r\n\r\ninput = tf.Variable([[1.0, 2.0], [3.0, 4.0]])\r\nw = tf.Variable([[1.0, 7.0], [3.0, 5.0]])\r\n\r\nwith tf.GradientTape(persistent=True) as t:\r\n    f = tf.matmul(input, tf.transpose(w))\r\n\r\nprint(\"f:\")\r\nprint(f)\r\n\r\nprint(\"Derivative:\")\r\nprint(t.jacobian(f, input))\r\n```\r\n\r\n\r\n### Source code / logs\r\nLog of the error:\r\n\r\n```\r\nWARNING:tensorflow:Entity <function pfor.<locals>.f at 0x644d84950> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function pfor.<locals>.f at 0x644d84950>: AssertionError: Bad argument number for Name: 3, expecting 4\r\n```\r\n\r\nThe full log (with AUTOGRAPH_VERBOSITY=10) is attached.\r\n\r\n[get_partial_derivatives.py.txt](https://github.com/tensorflow/tensorflow/files/4199640/get_partial_derivatives.py.txt)\r\n[logfile.txt](https://github.com/tensorflow/tensorflow/files/4199641/logfile.txt)", "comments": ["@dmitry-kabanov I have executed the code provided by you [on 1.14 as mentioned in by you] and do not find any warning/issue faced by you, please find the gist [here](https://colab.sandbox.google.com/gist/Saduf2019/3365818269fe056f5fae5b39f737d9fe/36731.ipynb) for the same[.]\r\n\r\nif you still face an issues please share a gist of the issue.", "Well, I do not know how to explain it. I can see this bug but I have Tensorflow provided by Anaconda distribution.\r\n\r\nIf it is irreproducible, then should I close it?", "@dmitry-kabanov could you please install tensorflow using pip and try on virtual environment, let us know if that helps resolve the issue.", "@Saduf2019 I installed Python 3.7 via conda in a conda environment and install tensorflow and all dependencies via `pip`. I still get the same issue. Please see the attached logfile.\r\n\r\nMy version of Python is `3.7.6 h359304d_2`.\r\n\r\n[logfile.txt](https://github.com/tensorflow/tensorflow/files/4213714/logfile.txt)\r\n", "@dmitry-kabanov \r\nCan you please refer to [link](https://github.com/tensorflow/tensorflow/issues/32859) where the workaround is by installing gast ( pip install gast==0.2.2).\r\nlet us know if that helps.", "Somehow I replied three days ago but my last comment is not here.\r\n\r\nI confirm that the workaround works. Installing gast 0.2.2 removed gast 0.3.3 and then it worked correctly.\r\n\r\n```\r\n~/dev/2020-02-17-tensorflow-jacobian-bug via \ud83c\udd52 /Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env\r\n\u279c pip install \"gast==0.2.2\"\r\nProcessing /Users/dima/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp37-none-any.whl\r\nInstalling collected packages: gast\r\n  Attempting uninstall: gast\r\n    Found existing installation: gast 0.3.3\r\n    Uninstalling gast-0.3.3:\r\n      Successfully uninstalled gast-0.3.3\r\nSuccessfully installed gast-0.2.2\r\n\r\n~/dev/2020-02-17-tensorflow-jacobian-bug via \ud83c\udd52 /Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env took 2s\r\n\u279c python get_partial_derivatives.py\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/Users/dima/dev/2020-02-17-tensorflow-jacobian-bug/conda-env/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING:tensorflow:From get_partial_derivatives.py:2: The name tf.enable_v2_behavior is deprecated. Please use tf.compat.v1.enable_v2_behavior instead.\r\n\r\n2020-02-18 12:33:29.302126: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nf:\r\ntf.Tensor(\r\n[[15. 13.]\r\n [31. 29.]], shape=(2, 2), dtype=float32)\r\nDerivative:\r\ntf.Tensor(\r\n[[ 4. 12.]\r\n [ 4. 12.]], shape=(2, 2), dtype=float32)\r\nJacobian:\r\ntf.Tensor(\r\n[[[[1. 7.]\r\n   [0. 0.]]\r\n\r\n  [[3. 5.]\r\n   [0. 0.]]]\r\n\r\n\r\n [[[0. 0.]\r\n   [1. 7.]]\r\n\r\n  [[0. 0.]\r\n   [3. 5.]]]], shape=(2, 2, 2, 2), dtype=float32)\r\n```\r\n", "@dmitry-kabanov Moving this issue to resolved status and as the issue is resolved."]}, {"number": 36730, "title": "tensorflow-gpu=2.1.0 AttributeError: module 'tensorflow' has no attribute 'placeholder'", "body": "when I used the tensorflow-gpu=2.1.0 to run in keras-python3.6 env, it said:\r\n\r\nUsing TensorFlow backend.\r\n2020-02-13 21:47:27.592762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\nTraceback (most recent call last):\r\n  File \"i:/Git_Lip/XWLip_lite/DC_kares_LipReading_P70_R18/code/network.py\", line 574, in <module>\r\n    models=Lip_net(**params)\r\n  File \"i:/Git_Lip/XWLip_lite/DC_kares_LipReading_P70_R18/code/network.py\", line 367, in Lip_net\r\n    input_data = Input(name='the_input', shape=(24,112,112,3), dtype='float32')\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\engine\\input_layer.py\", line 178, in Input\r\n    input_tensor=tensor)\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\engine\\input_layer.py\", line 87, in __init__\r\n    name=self.name)\r\n  File \"D:\\Anaconda3\\envs\\keras36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 517, in placeholder\r\n    x = tf.placeholder(dtype, shape=shape, name=name)\r\nAttributeError: module 'tensorflow' has no attribute 'placeholder'\r\n\r\nhow can I  sovle the problem?Thanks for replying.", "comments": ["@PingYufeng  please share implementable stand alone code for the issue faced by you, for us to help you resolve the issue.", "From your error stack, it looks like you are trying to execute TensorFlow 1 code (e.g. creating placeholder tensors) whose syntax is no longer supported in TensorFlow 2, which introduced numerous non-backward-compatible API changes.\r\n\r\nPlease [read about the API changes](https://www.tensorflow.org/guide/effective_tf2) to upgrade your code, or run your code with TensorFlow 1.15.", "@PingYufeng Please let us know if the above comment helps resolve the issue.", "@PingYufeng Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 36729, "title": "TensorRT profile generation mode", "body": "This PR adds a profile generation to dynamic shapes mode. \r\n\r\nThis features completes the ground work for dynamic shapes mode. Profile generation mode is enabled during build(), and collects information of the input shapes for TRT engines. This information is used to generate a singe engine with multiple profiles. \r\n\r\nDynamic shape mode is an experimental feature. The advantage of dynamic shapes is that we can use a single TensorRT engine with multiple profiles to handle input data with different shapes. This saves memory: all input profiles for an engine can share the same weights. Input dimensions that do not influence the number of model parameters can be dynamic (batch size, sequence length, even image dimensions for fully convolutional networks). Currently we generate a separate profile for each input shape that was provided during build mode.\r\n\r\nThis feature does not support defining profiles during runtime, only during build mode. This makes it important for users to feed data to build() with all interesting shapes so that the single engine can execute those shapes. Currently this feature does not support INT8 calibration.\r\n\r\nThis feature is not supposed to change any of the existing behavior of TF-TRT in implicit batch mode. Explicit batch mode is changed: it is now required to use build() to generate the engines.\r\n\r\nIt is expected to use this feature for elementwise ops. We are soon going to enable all the existing converters to work with this feature. An example on how to use this feature:\r\n\r\n```python\r\n    root = _model()\r\n    save.save(root, input_saved_model_dir,\r\n              {signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: root.run})\r\n\r\n    rewriter_config_with_trt = rewriter_config_pb2.RewriterConfig()\r\n    rewriter_config_with_trt.optimizers.extend(\r\n        [\"constfold\", \"layout\", \"constfold\"])\r\n    rewriter_config_with_trt.meta_optimizer_iterations = (\r\n        rewriter_config_pb2.RewriterConfig.ONE)\r\n    optimizer = rewriter_config_with_trt.custom_optimizers.add()\r\n    rewriter_config_with_trt.custom_optimizers.add().name = \"constfold\"\r\n    optimizer.name = \"TensorRTOptimizer\"\r\n    optimizer.parameter_map[\"minimum_segment_size\"].i = 1\r\n    optimizer.parameter_map[\"is_dynamic_op\"].b = True\r\n    optimizer.parameter_map[\"maximum_cached_engines\"].i = 1\r\n    optimizer.parameter_map[\"use_implicit_batch\"].b = False\r\n\r\n    conversion_params = trt_convert.DEFAULT_TRT_CONVERSION_PARAMS._replace(\r\n        rewriter_config_template=rewriter_config_with_trt)\r\n    converter = trt_convert.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir,\r\n                                    conversion_params=conversion_params)\r\n    converter.convert()\r\n\r\n    input_shapes = [(2, 4), (3, 9), (64, 128)]\r\n    def my_input_fn():\r\n      for x in input_shapes:\r\n        yield (np.random.normal(size=x).astype(np.float32),)\r\n\r\n    converter.build(input_fn=my_input_fn)\r\n\r\n    output_saved_model_dir = self.mkdtemp()\r\n    converter.save(output_saved_model_dir=output_saved_model_dir)\r\n```", "comments": ["@tfeher Could you please resolve the conflicts? Thanks!", "@tfeher Still, conflicts are appearing. Can you please resolve? Thanks!"]}, {"number": 36728, "title": "Iterator Resource Error ", "body": "While running this code - [Link](https://github.com/cerlymarco/MEDIUM_NoteBook/blob/master/Time2Vec/Time2Vec2.ipynb)\r\n\r\nPlease help me with this.\r\n\r\n```\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-12-db1663a8e74a> in <module>()\r\n      5 #nnT2V.summary()\r\n      6 \r\n----> 7 nnT2V.fit(X_train, y_train, epochs=1, batch_size=256, verbose=3, shuffle=True, validation_split=0.2)\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    485       # In this case we have created variables on the first call, so we run the\r\n    486       # defunned version which is guaranteed to never create variables.\r\n--> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    488     elif self._stateful_fn is not None:\r\n    489       # Release the lock early so that multiple threads can perform the call\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\Anaconda3\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nNotFoundError:  Resource AnonymousIterator/AnonymousIterator1/class tensorflow::data::IteratorResource does not exist.\r\n\t [[node IteratorGetNext (defined at C:\\Users\\yogesh\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_7034]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\n", "comments": ["@yug95, Please provide the Tensorflow version. Thanks!", " tensorflow  - 2.0.0", "is there any update ?", "@yug95, I tried with  Tf2.0. I didn't receive any error. \r\nChange these imports as \r\n```\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import *\r\nfrom tensorflow.keras.callbacks import * \r\n```\r\nTry running the code. Thanks!", "@yug95, Closing this issue Since it is resolved. Thanks"]}, {"number": 36727, "title": "TFRecordDataset causes seg fault with parallel reads and parallel map of an empty file list", "body": "== check python ===================================================\r\npython version: 3.7.4\r\npython branch:\r\npython build version: ('default', 'Sep 7 2019 18:27:02')\r\npython compiler version: Clang 10.0.1 (clang-1001.0.46.4)\r\npython implementation: CPython\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.262/RELEASE_X86_64\r\nos release version: 19.0.0\r\nos platform: Darwin-19.0.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.15', ('', '', ''), 'x86_64')\r\nuname: uname_result(system='Darwin', node='Andrews-MacBook.local', release='19.0.0', version='Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.262/RELEASE_X86_64', machine='x86_64', processor='i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nxcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\r\n\r\n== check pips ===================================================\r\nnumpy 1.17.4\r\nprotobuf 3.10.0\r\ntensorflow 2.1.0\r\ntensorflow-estimator 2.0.1\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.1.0\r\ntf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d382ca\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.1.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /Users/andrew/.local/share/virtualenvs/lib_andrew_scratch--yvJ8pLH/lib/python3.7/site-packages\r\nRequired-by:\r\n\r\n== python version ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 4, 'final', 0)\r\n\r\n== bazel version ===============================================\r\nDescribe the current behavior\r\nSegfault when next is called on the Itterable.\r\n\r\nDescribe the expected behavior\r\nCrash/Error gracefully\r\n\r\nCode to reproduce the issue\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef _parse_function(example_proto):\r\n    feature_description = {'features': tf.io.FixedLenFeature([], tf.string)}\r\n    return tf.io.parse_single_example(example_proto, feature_description)\r\n\r\ndataset = tf.data.TFRecordDataset([], num_parallel_reads = 2)\r\ndataset = dataset.map(_parse_function, num_parallel_calls = tf.data.experimental.AUTOTUNE)\r\n\r\ncount = sum([1 for i in dataset])\r\n```\r\n\r\nI have reproduced this is Colab \r\nhttps://colab.research.google.com/drive/1C6sJ0BVjbK1h78LojPxp1DAuILJC_9e8\r\n", "comments": ["@andrewstanfordjason I have executed the code shared by you on nightly and do not face any issues, please find the [gist](https://colab.sandbox.google.com/gist/Saduf2019/cf71b3aba378304db1bd1b7ef496bf10/36727.ipynb) for the same.\r\n\r\nThe colab link shared by you does not have any error.", "Clearly this is not an issue in the nightly. Thanks"]}, {"number": 36726, "title": "tfconfig_cluster_resolver: Fix task index override", "body": "The `task_id` method checked `self._task_type` instead of `self._task_id`.", "comments": ["Looks all green now, can we merge it?", "The merge process is automatic once the internal tests pass -- should be in within a day or so.", "Great, thanks for the quick reply.", "`Traceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/framework/memory_checker_test.py\", line 120, in testNoNewPythonObjectsEmpty\r\n    threshold={'builtins.weakref': 1})\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/profiler/traceme.py\", line 59, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/framework/memory_checker.py\", line 141, in assert_no_new_python_objects\r\n    self._python_memory_checker.assert_no_new_objects(threshold=threshold)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/profiler/traceme.py\", line 59, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/build/work//google3/runfiles/google3/third_party/tensorflow/python/framework/python_memory_checker.py\", line 131, in assert_no_new_objects\r\n    threshold, original_count_diff.most_common()))\r\nAssertionError: New Python objects exceeded the threshold.\r\nPython object threshold:\r\n{'builtins.weakref': 1}\r\n\r\nNew Python objects:\r\n[('builtins.function', 1)]`", "@hakos can you please check above error ?", "Hmm that doesn't really look related. @hakos can you re-sync and @rthadur we can reimport the PR and try agian?", "@frankchn sure , thank you ", "OK, I rebased it now. Please re-review."]}, {"number": 36725, "title": "Can't open the camera/webcam due to a tensorflow installation problem.", "body": "Hello, i have some problems with the installation of tensorflow and the opening of the webcam. Now i am nowhere near experienced with tensorflow, let alone the anaconda prompt menu. I hope you guys could provide me with some help. Thank you in advanced.\r\n\r\n\r\n(tfpose) C:\\Users\\gebruiker\\Desktop\\tf-pose-estimation>python run_webcam.py --model=mobilenet_thin --resize=432x368 --camera=0\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Kan opgegeven module niet vinden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_webcam.py\", line 8, in <module>\r\n    from tf_pose.estimator import TfPoseEstimator\r\n  File \"C:\\Users\\gebruiker\\Desktop\\tf-pose-estimation\\tf_pose\\__init__.py\", line 5, in <module>\r\n    from tf_pose.runner import infer, Estimator, get_estimator\r\n  File \"C:\\Users\\gebruiker\\Desktop\\tf-pose-estimation\\tf_pose\\runner.py\", line 7, in <module>\r\n    from tf_pose import common\r\n  File \"C:\\Users\\gebruiker\\Desktop\\tf-pose-estimation\\tf_pose\\common.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\gebruiker\\Anaconda3\\envs\\tfpose\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Kan opgegeven module niet vinden.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@Zayr0,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) comment on a similar issue and let us know if it works. Thanks!", "@Zayr0,\r\nAny updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36725\">No</a>\n"]}, {"number": 36724, "title": "Bug for TF2.x + TensorRT(7) failing when minimum_segment_size=2", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, code used from https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/object_detection/object_detection.py and modified to remove logical errors\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 2.1.0\r\n- **Python version**: 3.7.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: 10.2/7.6.5\r\n- **GPU model and memory**: Tesla T4\r\n- **Exact command to reproduce**: python object_detection.py  --input_saved_model_dir models/ssd_inception_v2_coco_2018_01_28/saved_model --output_saved_model_dir trt_engine --data_dir coco/val2017  --annotation_path coco/annotations/instances_val2017.json --input_size 640 --batch_size 1 --num_warmup_iterations 10  --minimum_segment_size 2 --num_iterations 20 --use_trt --precision FP16\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nRelated issue: https://github.com/tensorflow/tensorrt/issues/178\r\nWhen minimum_segment_size is set to 2 for TensorRT conversion, the code fails with this assertion error and the error log suggests this is a Tensorflow issue.\r\n```\r\nBenchmark arguments:\r\n  annotation_path: coco/annotations/instances_val2017.json\r\n  batch_size: 1\r\n  calib_data_dir: None\r\n  data_dir: coco/val2017\r\n  display_every: 100\r\n  gpu_mem_cap: 0\r\n  input_saved_model_dir: models/ssd_inception_v2_coco_2018_01_28/saved_model\r\n  input_size: 640\r\n  max_workspace_size: 1073741824\r\n  minimum_segment_size: 2\r\n  mode: validation\r\n  num_calib_inputs: 128\r\n  num_iterations: 20\r\n  num_warmup_iterations: 10\r\n  optimize_offline: False\r\n  output_saved_model_dir: trt_engine\r\n  precision: FP16\r\n  target_duration: None\r\n  use_synthetic: False\r\n  use_trt: True\r\nTensorRT Conversion Params:\r\n  is_dynamic_op: True\r\n  max_batch_size: 1\r\n  max_workspace_size_bytes: 1073741824\r\n  maximum_cached_engines: 1\r\n  minimum_segment_size: 2\r\n  precision_mode: FP16\r\n  rewriter_config_template: None\r\n  use_calibration: False\r\nConversion times:\r\n  conversion: 51.9s\r\nloading annotations into memory...\r\nDone (t=0.96s)\r\ncreating index...\r\nindex created!\r\n2020-02-13 02:32:55.475297: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Preprocessor/map/while/ResizeImage/TRTEngineOp_293 with input shapes: [[1,640,640,3]]\r\n2020-02-13 02:32:55.475382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7\r\n2020-02-13 02:32:55.476308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7\r\n2020-02-13 02:32:56.821045: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:32:56.821587: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_0 with input shapes: [[1,300,300,3]]\r\n2020-02-13 02:33:40.565597: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.568191: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_292 with input shapes: [[1,1083,91], [1,600,91], [1,150,91], [1,54,91], [1,24,91], [1,6,91]]\r\n2020-02-13 02:33:40.572484: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_19 with input shapes: [[6,2]]\r\n2020-02-13 02:33:40.633713: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.633784: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_20 with input shapes: [[6,2]]\r\n2020-02-13 02:33:40.662646: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.662798: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_294 with input shapes: [[1,1083,1,4], [1,600,1,4], [1,150,1,4], [1,54,1,4], [1,24,1,4], [1,6,1,4]]\r\n2020-02-13 02:33:40.670319: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.670384: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_9 with input shapes: [[1083,2]]\r\n2020-02-13 02:33:40.670419: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_8 with input shapes: [[6,2], [6,2]]\r\n2020-02-13 02:33:40.711256: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.720622: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.720687: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_10 with input shapes: [[1083,2]]\r\n2020-02-13 02:33:40.726760: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.750769: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.750833: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_11 with input shapes: [[600,2]]\r\n2020-02-13 02:33:40.750850: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_3 with input shapes: [[1083,2], [1083,2]]\r\n2020-02-13 02:33:40.774449: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.782585: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.782647: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_12 with input shapes: [[600,2]]\r\n2020-02-13 02:33:40.808488: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.808552: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_13 with input shapes: [[150,2]]\r\n2020-02-13 02:33:40.808570: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_4 with input shapes: [[600,2], [600,2]]\r\n2020-02-13 02:33:40.831022: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.840426: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.840488: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_14 with input shapes: [[150,2]]\r\n2020-02-13 02:33:40.866507: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.866570: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_15 with input shapes: [[54,2]]\r\n2020-02-13 02:33:40.866587: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_5 with input shapes: [[150,2], [150,2]]\r\n2020-02-13 02:33:40.890943: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.899113: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.899176: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_16 with input shapes: [[54,2]]\r\n2020-02-13 02:33:40.925454: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.925518: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_17 with input shapes: [[24,2]]\r\n2020-02-13 02:33:40.925538: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_6 with input shapes: [[54,2], [54,2]]\r\n2020-02-13 02:33:40.950457: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.958686: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.958751: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_18 with input shapes: [[24,2]]\r\n2020-02-13 02:33:40.984527: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.984605: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_7 with input shapes: [[24,2], [24,2]]\r\n2020-02-13 02:33:40.998919: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:40.999116: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_2 with input shapes: [[1917], [1917], [1917], [1917]]\r\n2020-02-13 02:33:41.113058: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:41.113152: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_1 with input shapes: [[1917], [1917], [1917], [1917]]\r\n2020-02-13 02:33:41.229240: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:41.229401: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/TRTEngineOp_291 with input shapes: [[1,1917,4]]\r\n2020-02-13 02:33:41.249654: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:41.252100: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/TRTEngineOp_171 with input shapes: [[1917,1]]\r\n2020-02-13 02:33:41.272919: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:41.272990: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/TRTEngineOp_175 with input shapes: [[1917,1]]\r\n2020-02-13 02:33:41.275116: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_63/TRTEngineOp_81 with input shapes: [[0,4]]\r\n2020-02-13 02:33:41.276148: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: ../builder/builder.cpp::setMaxBatchSize::135, condition: batchSize > 0 && batchSize <= MAX_BATCH_SIZE\r\n2020-02-13 02:33:41.304032: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:41.304124: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/TRTEngineOp_176 with input shapes: [[1917,1]]\r\n2020-02-13 02:33:41.316747: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-13 02:33:41.316786: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: engine.cpp::enqueue::292, condition: batchSize > 0 && batchSize <= mEngine.getMaxBatchSize(). Note: Batch size was: 0, but engine max batch size was: 1\r\n2020-02-13 02:33:41.316800: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:635] Failed to enqueue batch for TRT engine: StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_63/TRTEngineOp_81\r\n2020-02-13 02:33:41.316816: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:506] Failed to execute engine, retrying with native segment for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_63/TRTEngineOp_81\r\n2020-02-13 02:33:41.317449: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_67/TRTEngineOp_85 with input shapes: [[0,4]]\r\n2020-02-13 02:33:41.317620: F tensorflow/core/framework/op_kernel.cc:898] Check failed: mutable_output(index) == nullptr (0x7ff68c02e790 vs. nullptr)\r\nAborted\r\n```\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I have tested for other batch sizes and this is failing for all batch sizes so updating the issue name. Output for batch size:8 \r\n```\r\nINFO:tensorflow:Assets written to: trt_engine/assets\r\nBenchmark arguments:\r\n  annotation_path: coco/annotations/instances_val2017.json\r\n  batch_size: 8\r\n  calib_data_dir: None\r\n  data_dir: coco/val2017\r\n  display_every: 100\r\n  gpu_mem_cap: 0\r\n  input_saved_model_dir: models/ssd_inception_v2_coco_2018_01_28/saved_model\r\n  input_size: 640\r\n  max_workspace_size: 1073741824\r\n  minimum_segment_size: 2\r\n  mode: validation\r\n  num_calib_inputs: 128\r\n  num_iterations: 20\r\n  num_warmup_iterations: 10\r\n  optimize_offline: False\r\n  output_saved_model_dir: trt_engine\r\n  precision: FP16\r\n  target_duration: None\r\n  use_synthetic: False\r\n  use_trt: True\r\nTensorRT Conversion Params:\r\n  is_dynamic_op: True\r\n  max_batch_size: 8\r\n  max_workspace_size_bytes: 1073741824\r\n  maximum_cached_engines: 1\r\n  minimum_segment_size: 2\r\n  precision_mode: FP16\r\n  rewriter_config_template: None\r\n  use_calibration: False\r\nConversion times:\r\n  conversion: 54.3s\r\nloading annotations into memory...\r\nDone (t=1.00s)\r\ncreating index...\r\nindex created!\r\n2020-02-14 03:09:13.413110: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Preprocessor/map/while/ResizeImage/TRTEngineOp_293 with input shapes: [[1,640,640,3]]\r\n2020-02-14 03:09:13.413199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7\r\n2020-02-14 03:09:13.414121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7\r\n2020-02-14 03:09:14.736143: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:14.738335: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_0 with input shapes: [[8,300,300,3]]\r\n2020-02-14 03:09:36.084633: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.087380: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_292 with input shapes: [[8,1083,91], [8,600,91], [8,150,91], [8,54,91], [8,24,91], [8,6,91]]\r\n2020-02-14 03:09:36.094687: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_19 with input shapes: [[6,2]]\r\n2020-02-14 03:09:36.144579: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.144642: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_20 with input shapes: [[6,2]]\r\n2020-02-14 03:09:36.172929: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.173070: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_294 with input shapes: [[8,1083,1,4], [8,600,1,4], [8,150,1,4], [8,54,1,4], [8,24,1,4], [8,6,1,4]]\r\n2020-02-14 03:09:36.178845: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.178905: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_9 with input shapes: [[1083,2]]\r\n2020-02-14 03:09:36.179688: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_8 with input shapes: [[6,2], [6,2]]\r\n2020-02-14 03:09:36.228372: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.238279: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.238449: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.238517: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_10 with input shapes: [[1083,2]]\r\n2020-02-14 03:09:36.265017: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.265074: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_11 with input shapes: [[600,2]]\r\n2020-02-14 03:09:36.265847: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_3 with input shapes: [[1083,2], [1083,2]]\r\n2020-02-14 03:09:36.295690: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.303126: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.303194: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_12 with input shapes: [[600,2]]\r\n2020-02-14 03:09:36.328146: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.328205: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_13 with input shapes: [[150,2]]\r\n2020-02-14 03:09:36.328229: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_4 with input shapes: [[600,2], [600,2]]\r\n2020-02-14 03:09:36.379730: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.383956: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.384015: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_14 with input shapes: [[150,2]]\r\n2020-02-14 03:09:36.409362: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.409420: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_15 with input shapes: [[54,2]]\r\n2020-02-14 03:09:36.409441: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_5 with input shapes: [[150,2], [150,2]]\r\n2020-02-14 03:09:36.434521: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.442060: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.442117: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_16 with input shapes: [[54,2]]\r\n2020-02-14 03:09:36.467682: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.467740: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_17 with input shapes: [[24,2]]\r\n2020-02-14 03:09:36.467755: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_6 with input shapes: [[54,2], [54,2]]\r\n2020-02-14 03:09:36.492090: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.501487: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.501550: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_18 with input shapes: [[24,2]]\r\n2020-02-14 03:09:36.526715: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.526788: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/MultipleGridAnchorGenerator/TRTEngineOp_7 with input shapes: [[24,2], [24,2]]\r\n2020-02-14 03:09:36.541361: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.541568: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_2 with input shapes: [[15336], [15336], [15336], [15336]]\r\n2020-02-14 03:09:36.655707: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.655796: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/TRTEngineOp_1 with input shapes: [[15336], [15336], [15336], [15336]]\r\n2020-02-14 03:09:36.769422: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.769577: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/TRTEngineOp_291 with input shapes: [[8,1917,4]]\r\n2020-02-14 03:09:36.788197: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.794357: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/TRTEngineOp_156 with input shapes: [[1917,1]]\r\n2020-02-14 03:09:36.816404: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.816500: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/TRTEngineOp_167 with input shapes: [[1917,1]]\r\n2020-02-14 03:09:36.817234: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66 with input shapes: [[0,4]]\r\n2020-02-14 03:09:36.818561: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: ../builder/builder.cpp::setMaxBatchSize::135, condition: batchSize > 0 && batchSize <= MAX_BATCH_SIZE\r\n2020-02-14 03:09:36.854504: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.854604: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:736] Building a new TensorRT engine for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/TRTEngineOp_200 with input shapes: [[1917,1]]\r\n2020-02-14 03:09:36.854802: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-14 03:09:36.854850: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: engine.cpp::enqueue::292, condition: batchSize > 0 && batchSize <= mEngine.getMaxBatchSize(). Note: Batch size was: 0, but engine max batch size was: 1\r\n2020-02-14 03:09:36.854868: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:635] Failed to enqueue batch for TRT engine: StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.854885: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:506] Failed to execute engine, retrying with native segment for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.854904: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: engine.cpp::enqueue::292, condition: batchSize > 0 && batchSize <= mEngine.getMaxBatchSize(). Note: Batch size was: 0, but engine max batch size was: 1\r\n2020-02-14 03:09:36.854925: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:635] Failed to enqueue batch for TRT engine: StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.854937: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:506] Failed to execute engine, retrying with native segment for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.854952: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: engine.cpp::enqueue::292, condition: batchSize > 0 && batchSize <= mEngine.getMaxBatchSize(). Note: Batch size was: 0, but engine max batch size was: 1\r\n2020-02-14 03:09:36.854971: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:635] Failed to enqueue batch for TRT engine: StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.854990: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:506] Failed to execute engine, retrying with native segment for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.854999: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: engine.cpp::enqueue::292, condition: batchSize > 0 && batchSize <= mEngine.getMaxBatchSize(). Note: Batch size was: 0, but engine max batch size was: 1\r\n2020-02-14 03:09:36.855016: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:635] Failed to enqueue batch for TRT engine: StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.855031: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:506] Failed to execute engine, retrying with native segment for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.855046: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger Parameter check failed at: engine.cpp::enqueue::292, condition: batchSize > 0 && batchSize <= mEngine.getMaxBatchSize(). Note: Batch size was: 0, but engine max batch size was: 1\r\n2020-02-14 03:09:36.855074: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:635] Failed to enqueue batch for TRT engine: StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.855085: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:506] Failed to execute engine, retrying with native segment for StatefulPartitionedCall/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_5/TRTEngineOp_66\r\n2020-02-14 03:09:36.855407: F tensorflow/core/framework/op_kernel.cc:898] Check failed: mutable_output(index) == nullptr (0x7ff5bc000b60 vs. nullptr)\r\n2020-02-14 03:09:36.855417: F tensorflow/core/framework/op_kernel.cc:898] Check failed: mutable_output(index) == nullptr (0x7ff4a0000bf0 vs. nullptr)\r\n2020-02-14 03:09:36.855453: F tensorflow/core/framework/op_kernel.cc:898] Check failed: mutable_output(index) == nullptr (0x7ff5d00485b0 vs. nullptr)\r\n2020-02-14 03:09:36.855461: F tensorflow/core/framework/op_kernel.cc:898] Check failed: mutable_output(index) == nullptr (0x7ff6c4002570 vs. nullptr)\r\n2020-02-14 03:09:36.855484: F tensorflow/core/framework/op_kernel.cc:898] Check failed: mutable_output(index) == nullptr (0x7ff77c001290 vs. nullptr)\r\nAborted\r\n```", "is there any extra flags to be added non-max suppression in 2.x?? below error is seen while benchmarking on all object detection models\r\n**tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Incorrect batch dimension, for Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_84/TRTEngineOp_86: [[0,4]]\r\n         [[node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_84/TRTEngineOp_86 (defined at object_detection.py:118) ]]\r\n         [[Postprocessor/BatchMultiClassNonMaxSuppression/map/TensorArrayStack_4/range/_68]]\r\n  (1) Invalid argument:  Incorrect batch dimension, for Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_84/TRTEngineOp_86: [[0,4]]\r\n         [[node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_84/TRTEngineOp_86 (defined at object_detection.py:118) ]]**", "Thank you for the report. I could reproduce the problem. It happens also with TRT6. There are at least two problems here:\r\n- A possible problem with the conversion. We try to enqueue zero batch size data:\r\n```\r\nFailed to enqueue batch for TRT engine: ...\r\nDefaultLogger Parameter check failed at: engine.cpp::enqueue::292, condition: batchSize > 0 && batchSize <= mEngine.getMaxBatchSize(). Note: Batch size was: 0, but engine max batch size was: 1\r\n```\r\n- Error handling the outputs. I do not have a stack trace yet, but I suspect that it fails during [this call](https://github.com/tensorflow/tensorflow/blob/27da548f1aaefebf56f57eb906848025b9ac9116/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L372).\r\n\r\nWe will look into these problems.\r\n", "The problem with NMS conversion can be circumvented by increasing the minimum segment size (to 3 or above). The subsequent segfault seems to be fixed.\r\n\r\nI have tested the conversion using TF  master compiled from source, and that works if minimun_segment_size >= 3.\r\n\r\nThe converter for NMS needs to be fixed to allow smaller minimum segment size. ", "@tfeher : thank you. working with segment size 3. \r\nWhen I am converting the MASKRCNN to INT8 using trt, I am getting out of memory. is there a workaround for it?\r\n\r\n**Docker** : nvcr.io/nvidia/tensorflow:20.02-tf2-py3\r\n**cmd**: python object_detection.py --input_saved_model_dir /local/obj_models/mask_rcnn_resnet101_atrous_coco_2018_01_28/saved_model/  --output_saved_model_dir /local/obj_out_dir --optimize_offline  --data_dir /local/coco_data/val2017 --annotation_path /local/coco_data/annotations/instances_val2017.json --batch_size 1 --use_trt   --mode benchmark  --precision INT8 --input_size 300 --minimum_segment_size 3 --calib_data_dir /local/coco_data/val2017\r\n\r\n> 2020-02-26 08:46:18.974573: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.51G (4839756032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n\r\n2020-02-26 08:46:19.138149: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:38] DefaultLogger Current optimization profile is: 0. Please ensure there are no enqueued operations pending in this context prior to switching profiles\r\n2020-02-26 08:46:19.147584: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2020-02-26 08:46:19.148565: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2020-02-26 08:46:19.148622: E tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:841] Calibration failed: Internal: Failed to build TensorRT engine\r\n2020-02-26 08:46:19.148728: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to feed calibration data\r\n         [[{{node TRTEngineOp_2}}]]\r\n         [[BatchMultiClassNonMaxSuppression_1/map/TensorArrayUnstack_4/range/_178]]\r\n2020-02-26 08:46:19.149142: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to feed calibration data\r\n         [[{{node TRTEngineOp_2}}]]\r\n2020-02-26 08:46:19.161013: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2020-02-26 08:46:19.162439: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger ../rtSafe/safeContext.cpp (105) - Cudnn Error in initializeCommonContext: 4 (Could not initialize cudnn, please check cudnn installation.)\r\n2020-02-26 08:46:19.162540: E tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:841] Calibration failed: Internal: Failed to build TensorRT engine\r\nTraceback (most recent call last):\r\n  File \"object_detection.py\", line 410, in <module>\r\n    optimize_offline=args.optimize_offline)\r\n  File \"object_detection.py\", line 127, in get_graph_func\r\n    input_fn, calib_data_dir, num_calib_inputs//batch_size))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compiler/tensorrt/trt_convert.py\", line 1004, in convert\r\n    self._converted_func(*map(ops.convert_to_tensor, inp))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1551, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1591, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.\r\n  (0) Internal:  Failed to feed calibration data\r\n         [[node TRTEngineOp_2 (defined at object_detection.py:127) ]]\r\n         [[BatchMultiClassNonMaxSuppression_1/map/TensorArrayUnstack_4/range/_178]]\r\n  (1) Internal:  Failed to feed calibration data\r\n         [[node TRTEngineOp_2 (defined at object_detection.py:127) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_125086]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n\r\nterminate called without an active exception\r\nAborted (core dumped)", "set up:  batch_size: 1 or batch_size:4\r\ngood luck\r\n", "@mankeyboy Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36724\">No</a>\n"]}, {"number": 36723, "title": "Allow TrtGraphConverterV2 to accept Frozen Graph input as well as saved_model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.1\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, the trt API for tensorflow [here](https://github.com/tensorflow/tensorflow/blob/39cfd72c068a290cfff3fb5f61009ccc88ec0064/tensorflow/python/compiler/tensorrt/trt_convert.py#L1039) takes only a saved_model dir as input when it internally converts the saved_model to frozen graph before working on it. Before TF2.x, there was support for both in the form of either giving the saved_model directory or giving the frozen graph def. This change has caused codebases which used frozen_graph to break in functionality even after modifying the code to TF2.x supported APIs.\r\n**Will this change the current api? How?**\r\nThis will modify the current API to support the previous TrtGraphConverter API\r\n**Who will benefit with this feature?**\r\nAnyone who has been using frozen_graph for converting using TensorRT inside Tensorflow.\r\n**Any Other info.**\r\n", "comments": ["@mankeyboy frozen graph is not supported by TF2.0. So instead could you build a SavedModel using your frozen graph, and then use the converter?", "@aaroey I've been able to convert a frozen graph to a saved_model but I'm somehow unable to get it to save the variables which is why I've been looking for the ability to be able to use the frozen graph directly like the previous APIs.", "> @aaroey I've been able to convert a frozen graph to a saved_model but I'm somehow unable to get it to save the variables which is why I've been looking for the ability to be able to use the frozen graph directly like the previous APIs.\r\n\r\n@mankeyboy Thanks for mentioning this issue! I would like to contribute too to TRTGraphconverterV2 accepting an input as frozen_graph.pb. It seems a lot of trouble while benchmarking/inferencing the model with the only TensoRT optimized savedmodel graph. Even if you have an input savedmodel with frozen_graph.pb , saved_model.pb and variables, the TRTGraphConverterV2 will only return an optimized TensorRT savedmodel(containing saved_model.pb and variables) as the API and the codebase of trt_convert.py does that.\r\n\r\nLooking forward to contribute too", "@mankeyboy \r\n\r\nPlease update if this is still an issue in later version of tf.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36722, "title": "Bug: Cannot load model with tf.keras.layers.Lambda & eager functions enabled", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 31\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.1.0-1-ga9af83a149 2.1.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n- CUDA/cuDNN version: 10.2.89 / 7.6.5.33\r\n- GPU model and memory: Nvidia GeForce GTX 1070 TI 8GB\r\n\r\n**Describe the current behavior**\r\nWhen attempting to load a saved model that contains a `tf.keras.layers.Lambda` and eager functions are enabled, it throws the following exception\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/test.py\", line 13, in <module>\r\n    tf.keras.models.load_model(\"/tmp/foo\")\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 119, in __init__\r\n    self._finalize()\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 157, in _finalize\r\n    created_layers={layer.name: layer for layer in node.layers})\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1903, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1851, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 59, in return_outputs_and_add_losses\r\n    outputs, losses = fn(inputs, *args, **kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 113, in wrap_with_training_arg\r\n    lambda: replace_training_and_call(False))\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 59, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py\", line 59, in smart_cond\r\n    name=name)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1174, in cond\r\n    return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/cond_v2.py\", line 83, in cond_v2\r\n    op_return_value=pred)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 112, in <lambda>\r\n    lambda: replace_training_and_call(True),\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 108, in replace_training_and_call\r\n    return wrapped_call(*args, **kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 555, in __call__\r\n    return self._python_function(*args, **kwds)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"input_1_1:0\", shape=(None, 3), dtype=float32)\r\n  Keyword arguments: {'training': True}\r\n\r\nExpected these arguments to match one of the following 2 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 3), dtype=tf.float32, name='inputs')\r\n    * None\r\n    * True\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 3), dtype=tf.float32, name='inputs')\r\n    * None\r\n    * False\r\n  Keyword arguments: {}\r\n```\r\n**Describe the expected behavior**\r\nNo exception\r\n\r\nThis issue does kinda make sense, in that eager means to run native python instead of as a graph, in which case the python lambda can't be created from a saved model. But I think there's probably a better way to handle it than this. Maybe forcing this one part of the model to run as a graph.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\n\r\nds = tf.data.Dataset.from_tensor_slices([[1,2,3],[4,5,6],[7,8,9]])\r\n\r\ninput = tf.keras.Input(shape=ds.element_spec.shape)\r\n\r\noutput = tf.keras.layers.Lambda(lambda tb: tb)(input)\r\n\r\nmodel = tf.keras.Model(inputs=[input],outputs=[output])\r\nmodel.save(\"/tmp/foo\")\r\ntf.keras.models.load_model(\"/tmp/foo\")\r\n```", "comments": ["@phemmer,\r\nWas able to reproduce the error in [TF 2.1](https://colab.sandbox.google.com/gist/amahendrakar/2fe3544585360b1152ede567878dccc3/36722.ipynb). Works without any issues in nightly version [TF 2.2.0.dev20200212](https://colab.sandbox.google.com/gist/amahendrakar/aad6174deb95ae40bc122f784322a3c7/36722_nightly.ipynb). Please find the Gist attached. Thanks!", "Thanks. I'll try again once the next release following 2.1.0 is out."]}, {"number": 36721, "title": "Github issue creation for bugs missing", "body": "Note that the option for bug reports is missing.\r\n\r\n![image](https://user-images.githubusercontent.com/1826947/74403305-c9ba2500-4df4-11ea-8ec4-bc18da185557.png)\r\n\r\nI suspect this is related to #36636 which was recently merged.\r\n\r\nWhen I look at the [`00-bug-issue.md` file](https://raw.githubusercontent.com/tensorflow/tensorflow/master/.github/ISSUE_TEMPLATE/00-bug-issue.md), there is a space instead of a newline before `about:`. Don't know if this would cause it to go missing, but definitely seems not right.\r\n![image](https://user-images.githubusercontent.com/1826947/74403503-6381d200-4df5-11ea-8d05-39f44b5c7ed8.png)\r\n\r\nOthers appear missing as well, such as the build/installation issue, & performance issue.\r\n", "comments": ["Working on a fix"]}, {"number": 36720, "title": "Getting SavedModel format from Checkpoint automatically", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.1.0\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** None\r\n\r\n**Will this change the current api? How?** Add a method to the savedModel API for getting savedModel from checkpoint directly. \r\n\r\n**Who will benefit with this feature?** Everyone who is facing trouble converting to a SavedModel from a checkpoint graph\r\n\r\n**Any Other info.**\r\nI've faced this challenge \r\n", "comments": ["What do you mean by \"checkpoint graph\"? A GraphDef and a training checkpoint?", "@allenlavoie Yes, precisely. My feature request stems from this issues I've been facing in getting to a SavedModel given that I have a training checkpoint with all my weights, for eg: \r\n```\r\n'ssd_inception_v2_coco':\r\n    Model(\r\n        'ssd_inception_v2_coco',\r\n        'http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz',\r\n        'ssd_inception_v2_coco_2018_01_28',\r\n    )\r\n```\r\n\r\nThe challenge I've been facing is in getting from a training checkpoint and a GraphDef to a saved_model which is a wrapping around a frozen graph with signatures and a variables folder where it maintains the trained weights. \r\nSomething of this form: https://gist.github.com/zhanwenchen/d628ef70e9f76525fd47d6213c30730d where I can just specify to the builder API my checkpoint and it can figure out what signatures to use or how to export based on the checkpoint.", "So the GraphDef here is already frozen?", "Can you explain what the difference would mean? I'm not sure what difference would occur from a frozen graph def or otherwise.", "If the GraphDef is frozen, it doesn't refer to a training checkpoint anymore. It just has one set of values for the variables hard-coded. Making a SavedModel from an un-frozen GraphDef that has pointers to variables plus a training checkpoint is easy, but making a SavedModel from a frozen GraphDef and a new training checkpoint isn't really feasible.", "No, my concern was more in the first direction. Let's say I have a training checkpoint and a frozen graph.pb file, getting to a Saved_model should be something that we should be able to allow directly. From what I've understood, I have to get the GraphDef from the frozen graph and then load the checkpoint. Once I'm here, I have to apply the right input and output placeholders and the signature and this is something that has to be chosen for this. That is what I want to provide. Can you provide some guidance to some docs for the relationship between a training checkpoint and a Saved_Model?", "Frozen GraphDefs don't really fit into SavedModels. Un-frozen GraphDefs reference a training checkpoint, which a SavedModel has.\r\n\r\nFor non-frozen GraphDefs you can load (into Python) and re-save as a SavedModel along with a training checkpoint. For frozen GraphDefs this won't work, and I don't know of any efforts in that direction.", "@allenlavoie Thanks for the clarification. What you're saying basically means that we have no way to get a trained Saved_model from a frozen GraphDef and a training checkpoint because the information about the graph is lost on freezing it. Would the only way around this be to get an untrained saved_model from the graph def and then retrain it to target accuracy?", "I'm sure you could extract the values of the constants in the GraphDef with Session.run and then assign them to variables, but that's a manual process. Retraining works too."]}, {"number": 36719, "title": "name: Bug Issue about: why the performance of nnapi is much lower than cpu . labels: 'type:bug_template'", "body": "#36088 # System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: android O\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: GLK-AL00,Mi 6,vivo NEX,MI 8 SE\r\n\r\n### Describe the problem\r\nI write  nnapidelegate in my Engine just like nnapidelegate in tensorflowlite. When i run the same model, the performance on devices GLK-AL00,Mi 6,vivo NEX,MI 8 SE is more than 200ms, on Pixel 2 XL is 25ms, on mi 9 ,SamSung G9700  is lower than 10ms. Who can tell me why the performance on GLK-AL00,Mi 6,vivo NEX,MI 8 SE is such lower than Pixel 2 XL.\r\n\r\n\r\n\r\n", "comments": ["nnapi is implemented by OEM, it's possible the hardware vendors have not provided fast implementation yet. "]}, {"number": 36718, "title": "[TPU Colab] [TF2.1] Many issues only on TPU due to data types, batch sizes and memory (gist provided with explanations)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see gists with code on Colab [not working on TPU](https://colab.research.google.com/drive/1yk4Emyzxju85gqflEii9FVuMD6VF5vta) and [working on GPU](https://colab.research.google.com/drive/1d4_RPQ2wHg6q4wr-J8VzjSi_DMTLRZDp).\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **TensorFlow installed from (source or binary)**: `%tensorflow_version 2.x` command\r\n- **TensorFlow version (use command below)**: `2.1.0`\r\n\r\n### Describe the problem\r\nAll the issues are shown and described in the [gist provided](https://colab.research.google.com/drive/1yk4Emyzxju85gqflEii9FVuMD6VF5vta). Here is the list of the issues:\r\n- Issue 1: Models fitted on TPU does not deal number of samples not divisible by batch size.\r\n- Issue 1bis: With argument validation_split on TPU, issue 1 may occur with number of samples not divisible by batch size.\r\n- Issue 2: List of supported dtypes on TPU is restrictive.\r\n- Issue 2bis: Predict on TPU has the same dtype constraints than fit on TPU.\r\n- Issue 3: Number of samples may cause error for prediction on TPU\r\n- Issue 4: Batch size may cause error for prediction on TPU\r\n- Issue 4bis: Warnings when predicting on TPU\r\n- Issue 5: Trying to fit or predict on TPU a model on a supported dtype after having tried on a not supported dtype may raise an error.\r\n- Issue 6: `UnavailableError: Socket closed` on TPU and related errors\r\n\r\n### Source code / logs\r\n[See Gist provided.](https://colab.research.google.com/drive/1yk4Emyzxju85gqflEii9FVuMD6VF5vta)\r\n", "comments": ["Thanks for reporting the issues. \r\n\r\nThe [batch size](https://cloud.google.com/tpu/docs/troubleshooting#batch-too-small) and [dtype](https://cloud.google.com/tpu/docs/troubleshooting#unsupported_data_type) limitations are known.\r\n\r\nPractically speaking, the TPU systems have significant amounts of host memory, so converting INT8 to INT32 data types manually should not make it OOM unless the batch and data sizes are truly enormous.\r\n\r\nAssigning the rest of to the current TPU oncall @hongjunChoi.\r\n", "Hello Michael, \r\n\r\nThank you for reporting. As for errors regarding sample size not being divisible by batch size, could you try converting numpy arrays to tf.data.Dataset?\r\n\r\nCurrently, partial batch size for final batch is supported for dataset inputs.  ", "Hi @hongjunChoi,\r\n\r\nSorry for the late reply, I have very little time these days to work on this. I tried with `tf.data.Dataset`, it looks like it's approximately working...\r\n\r\nFor example I can find a workaround for the `validation_split` parameter by using `validation_data` instead, however it seems the `model.predict` returns different predictions depending on the batch size (using `.batch()` on `tf.data.Dataset`).\r\n\r\nI hadn't time to investigate but I'm quite surprised predictions may be the same for different samples of random generated data (however it also may be explained by the fact that the model is not really able to train...)\r\n\r\nPlease find a new gist [here](https://colab.research.google.com/drive/1IM3t4BF3iypclJ2fO9VCGg-6TdgORd-R) where I tried `tf.data.Dataset`, I hope it'll help you to investigate.", "Hello Michael,\r\n\r\nApologies for the late update. We are currently looking into this issue (more specifically on different predict result values) and will update you once the bug is fixed. \r\n\r\nThank you!", "Updating this thread as the bug of different predict results has been fixed.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36718\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36718\">No</a>\n"]}, {"number": 42784, "title": "Tensorflow installation document Korean translated page outdated", "body": "## URL(s) with the issue \r\n\r\nhttps://www.tensorflow.org/install?hl=ko\r\n\r\n## Description of issue (what needs changing)\r\n\r\nOn installation document/windows build from source page, it tells that \"TensorFlow\ub97c \ucef4\ud30c\uc77c\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ube4c\ub4dc \ub3c4\uad6c\uc778 Bazel 0.23.0\uc744 \uc124\uce58\ud569\ub2c8\ub2e4. C++\ub97c \ube4c\ub4dc\ud558\ub3c4\ub85d Bazel\uc744 \uc124\uc815\ud569\ub2c8\ub2e4.\" which means install Bazel 0.23.0 to compile Tensorflow. However, most recent version of Tensorflow which is r2.1, doesn't support Bazel 0.23.0. Instead, it uses 0.27.0~0.29.0. I checked English document and it tells me the version of Bazel that is needed. So, I think the Korean page should be renewed like this.\r\n\r\n## Bazel \uc124\uce58\r\n\r\nTensorFlow\ub97c \ucef4\ud30c\uc77c\ud558\ub294 \ub370 \uc0ac\uc6a9\ub418\ub294 \ube4c\ub4dc \ub3c4\uad6c\uc778 [Bazel](https://docs.bazel.build/versions/master/install.html)\uc744 \uc124\uce58\ud569\ub2c8\ub2e4. tensorflow/configure.py\uc5d0 \uba85\uc2dc\ub41c _TF_MIN_BAZEL_VERSION\uacfc _TF_MAX_BAZEL_VERSION \uc0ac\uc774\uc758 \uc9c0\uc6d0\ub418\ub294 \ubc84\uc804\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\r\n\r\nBazel \uc2e4\ud589 \ud30c\uc77c\uc758 \uc704\uce58\ub97c %PATH% \ud658\uacbd \ubcc0\uc218\uc5d0 \ucd94\uac00\ud569\ub2c8\ub2e4.", "comments": ["Hi @jungmin-lim,\r\nUnfortunately, I think It's not part of community translations. :(\r\n", "Sorry. Then where should I report this? I reported this issue at Tensorflow/Tensorflow but they told me to report here.\r\nhttps://github.com/tensorflow/tensorflow/issues/36678", "Hi @lamberta ,\r\nThis issue is for translation of https://github.com/tensorflow/docs/blob/master/site/en/install/source_windows.md\r\nYou know, Installation markdown is not part of community translations.\r\nHow can we fix the error? :)", "Reporting here is fine, thanks.\r\nI need to run another internal translation job but hitting a bug :/\r\nWill keep this open", "Sent off a new job for the /install section. tensorflow.org should be updated in a few weeks, but times can vary."]}, {"number": 36717, "title": "MKL no longer works with tensorflow 1.15", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.15.2\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.24.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc-6 (devtoolset-6 on centos 7)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```bash\r\n bazel build -c opt --copt=-msse4.2 --copt=-mavx \\\r\n       --copt=-O3 --config=mkl --linkopt -ldl \\\r\n       --copt=-march=x86-64 \\\r\n       //tensorflow/tools/pip_package:build_pip_package \\\r\n       //tensorflow/tools/lib_package:libtensorflow_jni \\\r\n       //tensorflow/tools/lib_package:libtensorflow \\\r\n       //tensorflow/tools/lib_package:libtensorflow_proto \\\r\n```\r\n\r\n### Describe the problem\r\nlibtensorflow_framework.so build this way does not have any symbols from MKL. When trying to import tensorflow from ~~java~~ scala, it fails with symbol not found for `tensorflow::DisableMKL()`\r\n\r\nThe number of MKL symbols found in `libtensorflow_framework.so` for 1.15 are also significantly lower than those found in 1.14.\r\n\r\n### Source code / logs\r\n\r\nCode used to import tensorflow in scala\r\n\r\n```scala\r\nimport org.tensorflow.Tensorflow\r\n```\r\nNote: We have to ensure `libiomp5.so` and `libmklml_intel.so` are available on library load path. \r\n\r\nThe simplest solution we found was to load the libraries manually in order. The code snippet can be seen here:\r\nhttps://gist.github.com/pavanky/ea6e71e3e7e52c013db844b715723be0\r\n \r\n\r\nError\r\n\r\n```\r\nlibtensorflow_jni.so: undefined symbol: _ZN10tensorflow10DisableMKLEv\r\n```\r\n\r\nLooking at the number of symbols related to MKL:\r\n\r\n```\t\r\n $ nm -D org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1 | grep -i mkl | wc -l\r\n1\r\n $ nm -D org/tensorflow/native/linux-x86_64/libtensorflow_jni.so | grep -i mkl | wc -l\r\n9\r\n $ nm -D org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1 | grep -i mkl\r\n0000000000e127b0 T _ZN10tensorflow12IsMklEnabledEv\r\n```\r\n\r\nFor reference, 1.14 had a lot more\r\n\r\n```\r\n $ nm -D org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1 | grep -i mkl | wc -l\r\n11388\r\n $ nm -D org/tensorflow/native/linux-x86_64/libtensorflow_jni.so | grep -i mkl | wc -l\r\n8\r\n```\r\n\r\n---------\r\nMKL is also not available in the wheel built by the command mentioned above.\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.python.pywrap_tensorflow.IsMklEnabled()\r\nFalse\r\n```", "comments": ["tried to build with this patch, all it did was get rid of the error. The performance regression tests we have in house indicate MKL is not being used (i.e. they are on par with no-mkl).\r\n\r\n```\r\niff --git a/tensorflow/core/util/util.cc b/tensorflow/core/util/util.cc\r\nindex 489999d..5f93530 100644\r\n--- a/tensorflow/core/util/util.cc\r\n+++ b/tensorflow/core/util/util.cc\r\n@@ -120,7 +120,7 @@ string SliceDebugString(const TensorShape& shape, const int64 flat) {\r\n   return result;\r\n }\r\n \r\n-#ifdef INTEL_MKL\r\n+//#ifdef INTEL_MKL\r\n bool DisableMKL() {\r\n   enum MklStatus { MKL_DEFAULT = 0, MKL_ON = 1, MKL_OFF = 2 };\r\n   static MklStatus status = MKL_DEFAULT;\r\n@@ -135,5 +135,5 @@ bool DisableMKL() {\r\n   }\r\n   return status == MKL_OFF ? true : false;\r\n }\r\n-#endif  // INTEL_MKL\r\n+//#endif  // INTEL_MKL\r\n }  // namespace tensorflow\r\ndiff --git a/tensorflow/core/util/util.h b/tensorflow/core/util/util.h\r\nindex 4aa47aa..c829707 100644\r\n--- a/tensorflow/core/util/util.h\r\n+++ b/tensorflow/core/util/util.h\r\n@@ -57,9 +57,9 @@ string PrintMemory(const char* ptr, size_t n);\r\n string SliceDebugString(const TensorShape& shape, const int64 flat);\r\n \r\n // disable MKL in runtime\r\n-#ifdef INTEL_MKL\r\n+//#ifdef INTEL_MKL\r\n bool DisableMKL();\r\n-#endif  // INTEL_MKL\r\n+//#endif  // INTEL_MKL\r\n \r\n }  // namespace tensorflow\r\n```", "@pavanky Could you please update the original post to provide the commands you used to import tensorflow from java as well? Thank you very much!", "@ashraf-bhuiyan @claynerobison ", "@penpornk I update the original description with more information. We technically use scala and not java, but that shouldnt be too different.", "@penpornk Also verified from python (wheel) that MKL is not available.", "@pavanky Thank you very much for the additional details! \r\nI tried this on Ubuntu 16.04, gcc 5.4.0, and python2.7. (TF 1.15.2.) The issue didn't occur to me.\r\n```\r\n$ nm -D  ${TF_ROOT}/bazel-out/k8-opt/bin/tensorflow/libtensorflow_framework.so.1 | grep -i mkl | wc -l\r\n15711\r\n$ nm -D ${TF_ROOT}/bazel-out/k8-opt/bin/tensorflow/java/libtensorflow_jni.so | grep -i mkl | wc -l\r\n9\r\n$ nm -D ${TF_ROOT}/bazel-out/k8-opt/bin/tensorflow/java/libtensorflow_jni.so | grep -i mkl\r\n                 U MKL_Comatcopy\r\n                 U MKL_Domatcopy\r\n                 U MKL_Somatcopy\r\n                 U MKL_Zomatcopy\r\n                 U _ZN10tensorflow10DisableMKLEv\r\n0000000000000658 b _ZN6mkldnn4impl10mxcsr_saveE\r\n0000000000000650 b _ZN6mkldnn4impl19global_scratchpad_t11scratchpad_E\r\n0000000000000640 b _ZN6mkldnn4impl19global_scratchpad_t16reference_count_E\r\n0000000000000648 b _ZN6mkldnn4impl19global_scratchpad_t5size_E\r\n```\r\nThe pip wheel:\r\n```\r\n$ python2.7\r\n>>> import tensorflow as tf\r\n>>> tf.python.pywrap_tensorflow.IsMklEnabled()\r\nTrue\r\n```\r\n\r\nI'll try CentOS 7 next.", "@penpornk I'm checking this out now...", "@claynerobison Thank you very much!", "One additional question: are there changes in behavior between 1.15.0 and 1.15.2?", "@penpornk did you use the same build instructions or did you try something different?", "@pavanky Yes, I used the instruction you provided:\r\n```\r\n bazel build -c opt --copt=-msse4.2 --copt=-mavx \\\r\n       --copt=-O3 --config=mkl --linkopt -ldl \\\r\n       --copt=-march=x86-64 \\\r\n       //tensorflow/tools/pip_package:build_pip_package \\\r\n       //tensorflow/tools/lib_package:libtensorflow_jni \\\r\n       //tensorflow/tools/lib_package:libtensorflow \\\r\n       //tensorflow/tools/lib_package:libtensorflow_proto\r\n```\r\n\r\nI tried building it on CentOS but got this JDK header error. (I'm using OpenJDK version 1.8.0_242, the same version I used for Ubuntu.) Any suggestions?\r\n```\r\nERROR: <bazel_cache_directory>/1cd7327d9a236f0d3fa30a23be6b1071/external/bazel_tools/tools/jdk/BUILD:96:1: Executing genrule @bazel_tools//tools/jdk:gen_include/linux/jni_md.h failed (Exit 1)\r\ncp: cannot stat 'external/local_jdk/include/linux/jni_md.h': No such file or directory\r\n```", "@penpornk this is the setup I am using (minus the Twitter specific stuff): https://gist.github.com/pavanky/5a45b29ed3ca56b7e1c747b9c0c88503\r\n\r\nAll the files need to be in the same directory when running the following command:\r\n`make IMAGE_TYPE=cpu USE_MKL=1`", "I could reproduce the results of @penpornk on Centos 7 along with all other sw installs mentioned in the original thread. Additionally I used openJDK-devel-1.8.0. \r\n```\r\nnm -D /home/centos/.cache/bazel/_bazel_centos/ee95bf24ea43f832d9db095baebc124d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/java/libtensorflow_jni.so | grep -i mkl\r\n                 U MKL_Comatcopy\r\n                 U MKL_Domatcopy\r\n                 U MKL_Somatcopy\r\n                 U MKL_Zomatcopy\r\n                 U _ZN10tensorflow10DisableMKLEv\r\n00000000000005c0 b _ZN6mkldnn4impl10mxcsr_saveE\r\n00000000000005b8 b _ZN6mkldnn4impl19global_scratchpad_t11scratchpad_E\r\n00000000000005a8 b _ZN6mkldnn4impl19global_scratchpad_t16reference_count_E\r\n00000000000005b0 b _ZN6mkldnn4impl19global_scratchpad_t5size_E\r\n```", "> One additional question: are there changes in behavior between 1.15.0 and 1.15.2?\r\n\r\n@mihaimaruseac  There should be no new features or new functions. All amendments should be all about bug fixes or security issue fixes.", "@preethivenkatesh are you using the same bazel version? Can you also check for symbols in `libtensorflow_framework.so`", "@penpornk are you using the same version of bazel ? 0.24.1?", "@pavanky,\r\n[libtf_symbols.txt](https://github.com/tensorflow/tensorflow/files/4220398/libtf_symbols.txt)\r\n", "@jingxu10 Yes, I know that. But I'm asking because the build process changed between the 1.15.0 and 1.15.2 and wanted to make sure this issue is not because of that.", "> @jingxu10 Yes, I know that. But I'm asking because the build process changed between the 1.15.0 and 1.15.2 and wanted to make sure this issue is not because of that.\r\n\r\nAre the changes in the build process in the r1.15 branch?", "The build scripts are in the branch now. They weren't before we started the patch process release.", "so , does this mean you are able to build it with the latest r11.5?", "@pavanky Yes, I used bazel 0.24.1. I haven't had a chance to try your docker file yet, sorry!\r\n\r\nWould love to know if you had problems with 1.15.0 too.", "@preethivenkatesh Would you mind trying @pavanky's dockerfile as well? Maybe the issue lies there. \r\n\r\n> @penpornk this is the setup I am using (minus the Twitter specific stuff): https://gist.github.com/pavanky/5a45b29ed3ca56b7e1c747b9c0c88503\r\n> \r\n> All the files need to be in the same directory when running the following command:\r\n> `make IMAGE_TYPE=cpu USE_MKL=1`", "yeah I'll try to work on it today", "we are able to reproduce the issue in the dockerfile. we'll continue to troubleshoot", "Thank you for the update!", "@preethivenkatesh please let me know if I can do anything to help", "@preethivenkatesh any updates?", "@pavanky this is assigned to me now.\r\nStay tuned please and I keep you posted.\r\nI was able to reproduce the issue using your `Dockerfile`.\r\nOne thing to keep in mind though both `Python2.7` and `CentOS 7` are considered unsupported configurations starting `TensorFlow v1.15` but I'm not going to make a final call on that just yet \ud83d\ude42 ", "@ashahba python2 support has been dropped internally, if you can get reproduce with python3 that is OK as well. you can run the python3 build using `make IMAGE_TYPE=cpu USE_MKL=1 PY_VER=36`", "@ashahba Even I thought this issue could be because on 2.7, but I was able to reproduce this issue on py36 when I tired. ", "Hi @pavanky and @preethivenkatesh I have put some docker and scripts together to help you with building `TensorFlow with MKL` on `CentOS 7` here: https://github.com/ashahba/centos7-tf\r\n\r\nI tried:\r\n```\r\ndocker build --build-arg TF_BRANCH=v1.15.2 --build-arg PY_VER=3.6 --build-arg CONFIG_VER=v2 -f Dockerfile . -t centos-tf-3.6-v2\r\n```\r\nand the `bazel` options where reported as follows:\r\n```\r\nWriting build flags: build --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-O3 --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-znoexecstack --linkopt=-zrelro --linkopt=-znow --linkopt=-fstack-protector --copt=-mmmx --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-mssse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mpopcnt --copt=-mavx --copt=-maes --copt=-mpclmul --config=mkl --config=v2\r\n```\r\n\r\nand of course the wheels correctly report `MKL` support:\r\n```\r\npython3\r\nPython 3.6.8 (default, Aug  7 2019, 17:28:10) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n>>> import tensorflow_core as tf_core\r\n>>> tf_core.python.pywrap_tensorflow.IsMklEnabled()\r\nTrue\r\n```\r\nand also:\r\n```\r\nnm -D  /tensorflow_src/bazel-out/k8-opt/bin/tensorflow/libtensorflow_framework.so.1 | grep -i mkl | wc -l\r\n15718\r\n```\r\n\r\nI tried this for both `Python 2.7` and `Python 3.6` and on both `v1.15.0` and `v1.15.2` as described in the repo and they all seem to work fine.\r\n\r\nAlso if you have new bazel flags or targets that need to be built, you can just modify the file `build_tf_whl.sh` around this line:\r\nhttps://github.com/ashahba/centos7-tf/blob/master/build_tf_whl.sh#L46\r\n\r\n\r\nGood luck and please let me know if that solves the issue you are seeing.\r\n\r\nThanks.", "One think to note, is that most of the work is just reusing scripts that are already in `TensorFlow` repo.", "@ashahba Thank you very much!", "@ashahab thanks trying!", "Thanks the new dockerfile works!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36717\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36717\">No</a>\n"]}, {"number": 36716, "title": "TFLu: Port SetCancellationFunction() from TFL and add test cases.", "body": "@petewarden : First draft of porting the SetCancellationFunction() from TFL.\r\nFunctionality should be fine - but need to check style in unit test. ", "comments": ["@suphoff Can you please resolve conflicts? Thanks!", "@suphoff  Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@suphoff  Can you please resolve conflicts? Thanks!", "@suphoff Gentle ping to resolve conflicts? Thanks!", "@gbaned : Sorry - I don't have the bandwidth to keep resolving conflicts right now. "]}, {"number": 36715, "title": "BUG: tf.random.normal() has a fixed value in eager mode (TF2.0)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- **Python version**: 3.6.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nIn Tf2.0 eager mode, tf.random.normal() will give the same value over and over again. This happens whether you use a Keras Model or just call the tf.random.normal() tensor repeatedly:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nid = np.ones(shape=(32,10))\r\ni = tf.keras.layers.Input(shape=(10,), batch_size=32, dtype=tf.float64)\r\ny = tf.random.normal(shape=(32,10), name=\"noise\", dtype=tf.float64)\r\no = tf.add(i, y)\r\nmodel = tf.keras.Model(inputs=i, outputs=o)\r\n\r\n# same value every time?\r\nmodel.predict(id)\r\nmodel.predict(id)\r\nmodel.predict(id)\r\n```\r\n\r\nThis occurs without Keras as well:\r\n\r\n```\r\nx = tf.constant(value=np.ones(shape=(32,10)), dtype=tf.float64)\r\ny = tf.random.normal(shape=(32,10), name=\"noise\", dtype=tf.float64)\r\nz = tf.add(x, y)\r\nprint(z)\r\nprint(z)\r\nprint(z)\r\n```\r\n\r\nIf you disable eager mode with tf.compat.v1.disable_eager_execution(), the Keras Model will generate new values each time it's called (as it should).\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/b5dfe95b4ce08d000c0cef3b05bb66d4/36715_eager.ipynb). Thanks!", "`tf.random.normal` op seed is set to a particular value when you execute script. So if you are calling same variable multiple times in same script you will see the same result.\r\nHowever if you run the same script multiple times you can see different results due to different seeds of `tf.random.normal` op\r\nTry executing following script multiple times (#avoid calling same variable multiple times in same script)\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nid = np.ones(shape=(2,2))\r\ni = tf.keras.layers.Input(shape=(2,),dtype=tf.float64)\r\ny = tf.random.normal(shape=(2,2), name=\"noise\", dtype=tf.float64)\r\no = tf.add(i, y)\r\nmodel = tf.keras.Model(inputs=i, outputs=o)\r\nmodel.predict(id)\r\n```\r\nSimilarly try;\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nx = tf.constant(value=np.ones(shape=(2,2)), dtype=tf.float64)\r\ny = tf.random.normal(shape=(2,2), name=\"noise\", dtype=tf.float64)\r\nz = tf.add(x, y)\r\nprint(z)\r\n```", "This is not related to the seed- the issue is that the same code behaves differently in graph vs eager mode. The same code run in graph mode will return new values each time the tensor is evaluated, but in eager mode it stores those values and returns them repeatedly. \r\n\r\nThis means that for example everyone using VAEs built in TF1.0 has code that is silently broken in TF2.0 (now that eager mode is the default)- instead of random values on each training step they're getting a fixed matrix that is instantiated once and evaluated over and over. \r\n\r\nAdditionally there is no way as far as I can tell to get a tensor with new random values on each evaluation in eager mode. \r\n\r\nThis same bug actually does exist in eager mode in TF1.0.\r\n\r\nEDIT: To be clear, to see the different behavior run the code in the original post twice: once as it is, and once with `tf.compat.v1.disable_eager_execution()`.", "Well, to me the behaviour is exactly what I expect of the eager mode. When you have\r\n```python\r\nimport tensorflow as tf\r\ny = tf.random.normal(shape=(2,2), name=\"noise\", dtype=tf.float64)\r\nprint(y)\r\nprint(y)\r\n```\r\nit should write the same value twice -- it was generated once during `tf.random.normal` call. So in your second example, I would expect three same values.\r\n\r\nSimilarly in your Keras code, `y` is generated once and never recomputed -- why should it? It is not a function generating random values, it was generated once and then stored in the model. If you want to generate fresh random values, you must change `y` to be a function of the input. For example, you could write\r\n```python\r\ny = tf.keras.layers.Lambda(lambda _: tf.random.normal(shape=(32,10), name=\"noise\", dtype=tf.float64), dtype=tf.float64)(i)\r\n```\r\nI assume there is a better way to do it in Keras though.", "It's extremely unexpected for a stochastic tensor to become non-stochastic in eager mode. It breaks working code without warning the user, and it does it in a very nasty hard to detect way. Your output will still be normally distributed. It will still change on every initialization (if you are not using a seed). I am 100% sure there are people out there atm working on VAEs that are actually AEs.\r\n\r\nFor people with working VAEs as well as beginners this is highly misleading, not least because [every](https://github.com/hwalsuklee/tensorflow-mnist-VAE/blob/master/vae.py#L74) - [existing](https://github.com/y0ast/VAE-TensorFlow/blob/master/main.py#L49) - [variational](https://github.com/ChengBinJin/VAE-Tensorflow/blob/master/src/vae.py#L51) - [autoencoder](https://github.com/hardmaru/cppn-gan-vae-tensorflow/blob/master/model.py#L94) uses `tf.random_normal()` [now `tf.random.normal()` in TF2.0] directly to sample from a normal distribution. \r\n\r\nUsing a lambda layer may work as a workaround but it doesn't solve the problem that this tensor silently changes its behavior between modes, and it shouldn't.", "I do not think this is a silent change. In eager mode, `tf.random.normal()` is a function returning a TF tensor. When executed, it generated random number, and you get back a piece of memory containing these numbers -- there is no magic for the tensor to be regenerated each time you use it (which is how it worked in a graph mode, where `tf.random_normal()` returned a node in a graph, which was reevaluated every time you ran the graph).\r\n\r\nSimilarly, for `y = np.random.normal()`, you would not expect `y` to change.", "I agree with @foxik, the current behaviour is strictly logical.\r\nAs to the proper way to add noise in TF2, as far as I know, it is by using a dedicated layer, _e.g._ the [GaussianNoise](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GaussianNoise) one.\r\n\r\nSimilarly, the examples linked are a bit different from your initial example; if you write _within your layer's `call` method_ that it should generate a value from random normal and add it to the input, it will indeed vary on each call. This is typically what would happen if you rewrote the first example's function as the call method of a custom layer.", "I hear what you guys are saying. But I think this is a big change from the way things worked in graph mode with no warning and no easy way to see the change. All four of the examples I linked used tf.random_normal the same way I did in the example- none of them wrap it in a layer.call. This has been the standard way to create VAEs for years now- all those people were following the same tutorials and if they've updated to TF 2.0, they've all got broken code now and don't know it. They could be publishing papers with that code and will need to issue retractions. \r\n\r\nIf this change (which is not a change, but is nevertheless a change) is intentional it has _got_ to be documented, and imo there should be a warning when running in eager mode. There should also be a proper way to repeatedly sample from a normal distribution, because right now in eager mode there is no way to do this very common operation other than the workarounds you've posted (thanks for those btw).", "> If this change (which is not a change, but is nevertheless a change) is intentional it has got to be documented, and imo there should be a warning when running in eager mode.\r\n\r\nIn my humble opinion, raising a warning whenever using `tf.random.normal` in eager mode would do more harm than good in that it would be over-triggered; yet I hear your point about the risk for incorrect code being ignorantly run, and I hope someone in the TF team will come up with a reasonable way to inform users. Perhaps tf1-to-tf2 conversion tools and tutorials already include checks on these cases, and if not it may be worth to enhance them in that direction?\r\n\r\n> There should also be a proper way to repeatedly sample from a normal distribution, because right now in eager mode there is no way to do this very common operation other than the workarounds you've posted (thanks for those btw).\r\n\r\nThe simplest way is to run `tf.random.normal` iteratively, _i.e._ each time you want to use a new value. For example, something in the vein of the following would work:\r\n```python\r\nfor sample in my_dataset:\r\n    noise = tf.random.normal()\r\n    output = model(sample + noise)\r\n```\r\nAnd when you want this operation to happen within a model, you similarly sample within the `call` method of a layer, because this is the code that is executed each time a sample is being processed. In TF1, the syntax is different, but the idea is similar : on each run, a symbolic tensor is given a random value sampled following the same generation procedure.", "For posterity, here's a minimal example of how to correctly run the code in the OP with a custom Keras layer:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass Sampler(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super().__init__()\r\n    def call(self, inputs):\r\n        y = tf.random.normal(shape=inputs.shape, name=\"noise\", dtype=inputs.dtype)\r\n        z = tf.add(inputs, y)\r\n        return z\r\n\r\n# with Model\r\nid = np.ones(shape=(32,10))\r\ni = tf.keras.layers.Input(shape=(10,), batch_size=32, dtype=tf.float64)\r\n# y = tf.random.normal(shape=(10,), name=\"noise\", dtype=tf.float64)\r\n# o = tf.add(i, y)\r\no = Sampler()(i)\r\nmodel = tf.keras.Model(inputs=i, outputs=o)\r\n\r\n# random is random now\r\nmodel.predict(id)\r\nmodel.predict(id)\r\nmodel.predict(id)\r\n```\r\n\r\nThat wasn't too bad. Alright, you guys convinced me- put something in the documentation and the update scripts (and maybe on the [migration page](https://www.tensorflow.org/guide/migrate) ) and I'll go away :) ", "@foxik's two comments are exactly right, and I also agree with @markemus that this is a big change. Unfortunately `tf.random.normal` is very broken in TF2 (its semantics was designed for graph mode, and its statefulness makes it very hard to behave the same in TF1 and TF2). Please see https://www.tensorflow.org/guide/random_numbers for the recommended ways to generate random numbers in TF2. We haven't migrated Keras to the new RNGs so we can't deprecate the old RNGs yet.  ", "@wangpengmit Thank you for pointing the guide out. The (new ?) `Generator` class and its associated functions appear to still be in `tf.random.experimental` as of TF 2.1.0, something that the guide does not mention; to be clear, do you advise users to move to it now, or is it better (from a user standpoint) to wait for a future release to migrate home-written code?", "@pandrey-fr It will be `tf.random.Generator` in TF 2.2. I strongly encourage you to use it if you can. Some reasons you can't use it: (1) you need to create a `Generator` in a Distribution Strategy scope which is not allowed (we are working on allowing it, and there are workarounds as described in the guide); (2) you use Keras which doesn't use `Generator` yet.", "Great, thank you for your the clarification!", "@markemus,\r\nCould you please check @wangpengmit's comment above and let us know if your query is resolved. Thanks!", "@amahendrakar the Generator class does seem like it a good replacement that would solve this issue. I've been using it myself per @wangpengmit 's suggestion. However I don't think this is resolved until tf.random.normal() is deprecated or there is a clear way for people to understand its different behavior in the different tensorflow modes. I'm still encountering subtly broken code in the wild.", "Was able to reproduce the issue with TF v2.3 and the latest TF-nightly i.e. v2.4.0-dev20200805. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dae30f344fcb72eff2140acab5270e7c/36715-2-3.ipynb). Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/34aa4b4ef47e4a09ebc3edfcf809a5cf/35650.ipynb). Thanks!", "There is already a warning message mentioned in the Random number generation document [here](https://www.tensorflow.org/guide/random_numbers). Which says `Warning: The old RNGs from TF 1.x such as tf.random.uniform and tf.random.normal are not yet deprecated but strongly discouraged.` \r\nHope this clarifies your issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36715\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36715\">No</a>\n"]}, {"number": 36714, "title": "Tensorflow Serving | Input to reshape is a tensor with 512 values, but the requested shape has 1", "body": "### System information\r\nUbuntu 18.04.4 LTS\r\nTF version 2.1.0\r\nPython 3.6.9\r\n\r\nWhile serving a tensorflow model I keep getting the same error related to tensor shape, it seems that tensors lose 1 dimension while in serving. Everything works fine before hosting.\r\n\r\nModel definition:\r\n```import tensorflow as tf\r\nimport numpy as np\r\nimport requests\r\nimport json\r\n\r\nx1_ = tf.keras.Input(shape=(64,256,))\r\nx2_ = tf.keras.Input(shape=(512,))\r\n\r\n\r\nW1 = tf.keras.layers.Dense(512)\r\nW2 = tf.keras.layers.Dense(512)\r\n\r\nx1 = W1(x1_)\r\nx2 = tf.expand_dims(x2_, 1)\r\nx2 = W2(x2)\r\ny = tf.math.add(x1, x2)\r\n\r\nModel = tf.keras.Model([x1_, x2_], [y])\r\nmodel = Model\r\n\r\ntf.saved_model.save(model, \"/tmp/models/model/1/\")\r\n```\r\nDocker command to launch the container:\r\n```\r\ndocker run --rm -p 8504:8501 --name tfserving_test3 \\\r\n--mount type=bind,source=\"/tmp/models/model\",target=/models/tensorflow_model \\\r\n-e MODEL_NAME=tensorflow_model -t tensorflow/serving\r\n```\r\n\r\n\r\nSend a request to the API:\r\n```\r\nx1 = np.random.normal(size=(1,64,256))\r\nx2 = np.zeros((1, 512), dtype=np.float32)\r\n\r\nvalues = [x1.tolist(), x2.tolist()]\r\ninputs = {t.name[:-2]:t for t in model.inputs}\r\n\r\nd = dict(zip(inputs, values))\r\ndata = {\"instances\": [d]}\r\ndata = json.dumps(data)\r\n\r\nr = requests.post('http://localhost:8504/v1/models/tensorflow_model:predict', data=data)\r\nprint(r.content.decode('utf-8'))\r\n```\r\n\r\nIt seems that while serving one of the tensors lose a dimension. I tried to expand dims and then reshape to make sure that the model still works fine in training but it had no impact while serving.\r\n\r\n", "comments": ["Can I see the output of `saved_model_cli`  and also what is `units` in the model?", "Sorry, I've edited the number of units in the original post (512).\r\n\r\nOutput of the `saved_model_cli show --dir /tmp/models/model/1 --tag_set serve --signature_def serving_default` commeand:\r\n\r\n```\r\nThe given SavedModel SignatureDef contains the following input(s):\r\n  inputs['input_3'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (-1, 64, 256)\r\n      name: serving_default_input_3:0\r\n  inputs['input_4'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (-1, 512)\r\n      name: serving_default_input_4:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\n  outputs['tf_op_layer_Add'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (-1, 64, 512)\r\n      name: StatefulPartitionedCall:0\r\nMethod name is: tensorflow/serving/predict\r\n\r\n```", "Please paste the output of the below command as I am unable to see the serving_default signature\r\n`saved_model_cli show --dir {export_path} --all`\r\n\r\nThanks!", "Yes, of course. Please find below:\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_1'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 64, 256)\r\n        name: serving_default_input_1:0\r\n    inputs['input_2'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 512)\r\n        name: serving_default_input_2:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['tf_op_layer_Add'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 64, 512)\r\n        name: StatefulPartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\n\r\n```", "@eubergene Its not a bug in tensorflow serving. Its the way you are reshaping the data the data to serving is causing the problem. Please take a look at the example [here](https://colab.sandbox.google.com/gist/gowthamkpr/e5b8e6d489ecf6db1921ce35e2e88fc1/final_flowers_serving.ipynb) which should help you in resolving the issue.\r\n\r\nIf you have more questions please post them in stack overflow. Thanks!", "You may find this helpful. https://stackoverflow.com/a/61386690/7699859 "]}]