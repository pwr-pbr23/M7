[{"number": 34483, "title": "Limit GPU memory", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 34482, "title": "Error with Placeholder_1", "body": "I tried to run el script https://github.com/tensorflow/magenta-demos/blob/master/jupyter-notebooks/Sketch_RNN.ipynb but with diferente dataset (pig.npz) and with the checkpoints of that and i have the next problem : \r\n![download](https://user-images.githubusercontent.com/38189240/69332857-d8f75f80-0c57-11ea-813c-22fe4db6b636.png)\r\nThnak you :)\r\n", "comments": ["@Pauladds ,\r\nHi, I see that TF version-1.8.0 is being used, please try using the latest version 1.15 and let us know the results. Also please provide a minimal standard code to reproduce the issue reported here if it exists in latest version. Thanks!", "Hi, I have TF 1.15 and my program still doesn't work. Here is the code (https://github.com/Pauladds/Rnn_sketch) , i am running \"blob_sketch_RNN-bread\" but i don't know my problem... \r\nmy version of TF is:\r\n(magenta) paula@oceanus:~$ pip show tensorflow-gpu\r\nName: tensorflow-gpu\r\nVersion: 1.15.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/paula/miniconda3/envs/magenta/lib/python3.7/site-packages\r\nRequires: tensorflow-estimator, tensorboard, keras-preprocessing, google-pasta, numpy, termcolor, wrapt, gast, keras-applications, absl-py, protobuf, grpcio, astor, opt-einsum, wheel, six\r\n\r\nThank you in advance :) \r\n\r\n\r\n\r\n\r\n", "@Pauladds ,\r\nHi, link provided https://github.com/Pauladds/Rnn_sketch doesnt seem to work, can you provide the working link?Thanks!", "@Pauladds ,\r\nany update on the issue ?Thanks!", "Sorry! I found the problem and I solve it!\r\nThank you :)"]}, {"number": 34481, "title": "Summaries producing warnings", "body": "The example code states \r\n\r\n```\r\nwriter = tf.summary.create_file_writer(\"/tmp/mylogs/tf_function\")\r\n\r\n@tf.function\r\ndef my_func(step):\r\n  with writer.as_default():\r\n    # other model code would go here\r\n    tf.summary.scalar(\"my_metric\", 0.5, step=step)\r\n\r\nfor step in range(100):\r\n  my_func(step)\r\n  writer.flush()\r\n```\r\n\r\nBut on multiple systems and operating systems I am seeing warnings using code like this of the form \r\n\r\n> \r\nWARNING:tensorflow:5 out of the last 5 calls to <function my_func at 0x7fc4f0197ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n", "comments": ["The example should be \r\n\r\n```\r\n    for step in range(100):\r\n        step = tf.convert_to_tensor(step, dtype=tf.int64)\r\n        my_func(step)\r\n        writer.flush() \r\n\r\n\r\nor \r\n\r\n```\r\n    for step in tf.range(100):\r\n        step = tf.cast(step, tf.int64)\r\n        my_func(step)\r\n        writer.flush()", "Could reproduce the issue with TF 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/74b3582f02348373e348f075afbf4edd/untitled266.ipynb). Thanks!", "Thanks for pointing this out, we'll fix it.  @plooney Just to confirm, is the example code you're referring to the code shown in https://www.tensorflow.org/tensorboard/migrate?", "That is correct\n\nOn Tue, 3 Dec 2019, 22:33 Nick Felt, <notifications@github.com> wrote:\n\n> Thanks for pointing this out, we'll fix it. @plooney\n> <https://github.com/plooney> Just to confirm, is the example code you're\n> referring to the code shown in\n> https://www.tensorflow.org/tensorboard/migrate?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34481?email_source=notifications&email_token=ABKFSATQE3YS5PPQ7TE3I5TQW3NEBA5CNFSM4JP7URMKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEF3BSLA#issuecomment-561387820>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABKFSATCSBCNX7QKNSHWYH3QW3NEBANCNFSM4JP7URMA>\n> .\n>\n", "Was able to reproduce the issue with TF v2.2 and TF-nightly (i.e. v2.3.0-dev20200623). Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7f288214bf0c5a7575ad5313805e0124/34481.ipynb#scrollTo=ZG-bSPiwcmEa). Thanks!", "@plooney Thanks for raising this issue. This was updated in the TF website as shown below and it works without any error.\r\n\r\n```\r\nfor step in tf.range(100, dtype=tf.int64):\r\n  my_func(step)\r\n  writer.flush()\r\n```\r\n\r\nI am closing this issue as this was already resolved in the current [migration guide](https://www.tensorflow.org/tensorboard/migrate). Please feel free to reopen if I am mistaken. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34481\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34481\">No</a>\n"]}, {"number": 34480, "title": "Resolve issue #34479", "body": "#34479", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34480) for more info**.\n\n<!-- need_author_cla -->", "Still blocked by the CLA stuff... \ud83d\ude29", "@gekowa thank you for your contribution, please sign CLA.", "Please use a more descriptive title for the PR and put issue number in the description field (which shows up as the first comment). There, it becomes clickable, in the title it does not link to the issue.", "I seems that I've included some changes that committed on my corporate computer, please let me use my own computer to commit these changes, closing this one for now."]}, {"number": 34479, "title": "Got \"ValueError: Unable to create group (name already exists)\" when saving a convolutional model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI'm currently building a YOLOv3 model, the training was good, but when I try to save the model to h5 format, it throws out the \"ValueError: Unable to create group (name already exists)\".\r\n\r\n**Describe the expected behavior**\r\nIt should finish the save process successfully.\r\n\r\n**Code to reproduce the issue**\r\n```\r\ntf.keras.models.save_model(model, \"model.h5\", save_format=\"h5\")\r\n```\r\n\r\n**Other info / logs**\r\nThe log:\r\n```\r\nTraceback (most recent call last):\r\n  File \"image_demo.py\", line 58, in <module>\r\n    tf.keras.models.save_model(model, \"model.h5\", save_format=\"h5\")\r\n  File \"/Users/xhguo/Workspace/TensorFlow2.0-Examples/4-Object_Detection/YOLOV3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 112, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/Users/xhguo/Workspace/TensorFlow2.0-Examples/4-Object_Detection/YOLOV3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 110, in save_model_to_hdf5\r\n    save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n  File \"/Users/xhguo/Workspace/TensorFlow2.0-Examples/4-Object_Detection/YOLOV3/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 627, in save_weights_to_hdf5_group\r\n    g = f.create_group(layer.name)\r\n  File \"/Users/xhguo/Workspace/TensorFlow2.0-Examples/4-Object_Detection/YOLOV3/lib/python3.7/site-packages/h5py/_hl/group.py\", line 61, in create_group\r\n    gid = h5g.create(self.id, name, lcpl=lcpl, gcpl=gcpl)\r\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\r\n  File \"h5py/h5g.pyx\", line 161, in h5py.h5g.create\r\nValueError: Unable to create group (name already exists)\r\n```", "comments": ["@gekowa, Please provide the complete standalone code to reproduce the reported issue.Thanks!", "@gekowa, Similar issue [#32672](https://github.com/tensorflow/tensorflow/issues/32672). Thanks", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "try to use tf.keras  `Lambda` if you do some op on Inputs. "]}, {"number": 34478, "title": "Gradient computation return None", "body": "hello, i write an example to test **tf.while_loop** and **tf.TensorArray**,  it return None when compute gradients, it seem that there is no connection from input to output, i don't know why?\r\n\r\ntensorflow version:  1.10.1\r\ncode:  https://github.com/dingevin/example/blob/master/example.py\r\n\r\nlog:\r\n<tf.Variable 'forward/dense/kernel:0' shape=(1024, 512) dtype=float32_ref>\r\n<tf.Variable 'forward/dense/bias:0' shape=(1024,) dtype=float32_ref>\r\n<tf.Variable 'output/dense/kernel:0' shape=(10, 21504) dtype=float32_ref>\r\n<tf.Variable 'output/dense/bias:0' shape=(10,) dtype=float32_ref>\r\n**[(None, <tf.Variable 'forward/dense/kernel:0' shape=(1024, 512) dtype=float32_ref>), (None, <tf.Variable 'forward/dense/bias:0' shape=(1024,) dtype=float32_ref>), (<tf.Tensor 'gradients/output/dense/dense/MatMul_grad/tuple/control_dependency_1:0' shape=(10, 21504) dtype=float32>, <tf.Variable 'output/dense/kernel:0' shape=(10, 21504) dtype=float32_ref>), (<tf.Tensor 'gradients/output/dense/dense/add_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'output/dense/bias:0' shape=(10,) dtype=float32_ref>)]**\r\nTraceback (most recent call last):\r\n  File \"1.py\", line 203, in <module>\r\n    tf.app.run(main)\r\n  File \"/home/dingzhenyou/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"1.py\", line 188, in main\r\n    model.build_train_model()\r\n  File \"1.py\", line 81, in build_train_model\r\n    grads_and_vars = self.average_gradients(gv_list)\r\n  File \"1.py\", line 100, in average_gradients\r\n    expanded_g = tf.expand_dims(g, 0)\r\n  File \"/home/dingzhenyou/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/dingzhenyou/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 136, in expand_dims\r\n    return gen_array_ops.expand_dims(input, axis, name)\r\n  File \"/home/dingzhenyou/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2020, in expand_dims\r\n    \"ExpandDims\", input=input, dim=axis, name=name)\r\n  File \"/home/dingzhenyou/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 528, in _apply_op_helper\r\n    (input_name, err))\r\n**ValueError: Tried to convert 'input' to a tensor and failed. Error: None values not supported.**\r\n\r\n", "comments": ["@dingevin, Can you try with Tf latest 1.15 version and check still error persists or not. And also if possible provide the minimum code snippet to localize the reported issue. Thanks!", "@gadagashwini  I have written the minimum code snippet as flow. other Tf version (1.14, 1.13) have been tried except 1.15, still error.  the variables in tf.while_loop has no gradients.\r\ncode:  [https://github.com/dingevin/example/blob/master/example.py](url)\r\nlog:\r\n`<tf.Variable 'dense/kernel:0' shape=(512, 128) dtype=float32_ref>`\r\n`<tf.Variable 'dense/bias:0' shape=(128,) dtype=float32_ref>`\r\n`<tf.Variable 'output/output/kernel:0' shape=(128, 10) dtype=float32_ref>`\r\n`<tf.Variable 'output/output/bias:0' shape=(10,) dtype=float32_ref>`\r\n`(None, <tf.Variable 'dense/kernel:0' shape=(512, 128) dtype=float32_ref>)`\r\n`(None, <tf.Variable 'dense/bias:0' shape=(128,) dtype=float32_ref>)`\r\n`(<tf.Tensor 'gradients/output/output/MatMul_grad/tuple/control_dependency_1:0' shape=(128, 10) dtype=float32>, <tf.Variable 'output/output/kernel:0' shape=(128, 10) dtype=float32_ref>)`\r\n`(<tf.Tensor 'gradients/output/output/add_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'output/output/bias:0' shape=(10,) dtype=float32_ref>)`\r\n\r\n", "@dingevin, Tried reproducing the reported issue, Looks like some entities are not defined.\r\nPlease see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/67f894fcd027eefdcf8a86e1fbba58c6/untitled277.ipynb) and provide the more info. Thanks!", "i am sorry. that's complete code. https://github.com/dingevin/example/blob/1f3800fecd4251800cd59a41028b51402bae9e29/example.py#L21", "@dingevin, Tried replicate the reported issue but got different error. \r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/037682e77154d98c5b53f69e0c69a312/untitled277.ipynb). Thanks!", "@gadagashwini  that is my asked question.\r\n`(None, <tf.Variable 'dense/kernel:0' shape=(512, 128) dtype=float32_ref>)`\r\n`(None, <tf.Variable 'dense/bias:0' shape=(128,) dtype=float32_ref>)`\r\n`(<tf.Tensor 'gradients/output/output/MatMul_grad/tuple/control_dependency_1:0' shape=(128, 10) dtype=float32>, <tf.Variable 'output/output/kernel:0' shape=(128, 10) dtype=float32_ref>)`\r\n`(<tf.Tensor 'gradients/output/output/add_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'output/output/bias:0' shape=(10,) dtype=float32_ref>)`\r\n\r\nparameters outside the loop has gradients but inside return none. that confused me.", "@ymodak  Any progress?", "I known it, the finial output to be a function of gather_x, not x. ", "Closing this issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34478\">No</a>\n"]}, {"number": 34477, "title": "Fix nested function inside XLA context.", "body": "", "comments": []}, {"number": 34476, "title": "Fix error message in python/training/supervisor.py", "body": "", "comments": ["Fix a tiny log error in python/training/supervisor.py."]}, {"number": 34475, "title": "WHere is staging package in tf2?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHi there, i want to use the StagingArea structure by using 'from tensorflow.contrib.staging import StagingArea' in old version. How could i use it in tf2, and i see the definition is here :tensorflow/python/ops/data_flow_ops.py\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@YunchuZhang `tensorflow.contrib.staging` uses `StagingArea class` as shown [here](https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/contrib/staging/__init__.py#L20) and also yo can find staging area class [here ](https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/ops/data_flow_ops.py#L1770)\r\n\r\nIn Tensorflow 2.0 the you can import StagingArea as follows\r\n`from tensorflow.python.ops.data_flow_ops import StagingArea`\r\nYou can refer to the following [line](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/data_flow_ops.py#L1769) for more understanding. Thanks!", "Thank you so much!!!\n\n\n\n\nOn Fri, Nov 22, 2019 at 6:28 PM gowthamkpr <notifications@github.com> wrote:\n\n> @YunchuZhang <https://github.com/YunchuZhang> tensorflow.contrib.staging\n> uses StagingArea class as shown here\n> <https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/contrib/staging/__init__.py#L20>\n> and also yo can find staging area class here\n> <https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/ops/data_flow_ops.py#L1770>\n>\n> In Tensorflow 2.0 the you can import StagingArea as follows\n> from tensorflow.python.ops.data_flow_ops import StagingArea\n> You can refer to the following line\n> <https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/data_flow_ops.py#L1769>\n> for more understanding. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34475?email_source=notifications&email_token=AIAVGNNQ7SW6SQDTGILXWLDQVBTLFA5CNFSM4JP5KE3KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE7FCBY#issuecomment-557732103>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIAVGNICVJ7KRR57PHBMKF3QVBTLFANCNFSM4JP5KE3A>\n> .\n>\n", "Hi there,\ndo you know which function or class could be used to change the StagingArea\nin tf2?\n\nCurrently, i want to use tf.Variable in tf2 not placeholder to be put in\nstagingarea like this:\n\nself.staging_tf = StagingArea(\ndtypes=[tf.float32 for _ in self.stage_shapes.keys()],\nshapes=list(self.stage_shapes.values()))\n\nself.buffer_ph_tf = [tf.Variable(tf.ones(shape=shape)) for shape in\nself.stage_shapes.values()\n# tf.compat.v1.placeholder(tf.float32, shape=shape) for shape in\nself.stage_shapes.values()\n]\nself.stage_op = self.staging_tf.put(self.buffer_ph_tf)\n\nbatch = self.staging_tf.get()\n\nit comes error:\n  File \"/home/robertmu/baselines/baselines/her/ddpg.py\", line 345, in\n_create_network\n    batch = self.staging_tf.get()\n  File\n\"/home/robertmu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/data_flow_ops.py\",\nline 1924, in get\n    return self.__internal_get(fn, name)\n  File\n\"/home/robertmu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/data_flow_ops.py\",\nline 1890, in __internal_get\n    return self._get_return_value(ret, indices)\n  File\n\"/home/robertmu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/data_flow_ops.py\",\nline 1740, in _get_return_value\n    tensors = self._create_device_transfers(tensors)\n  File\n\"/home/robertmu/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/data_flow_ops.py\",\nline 1717, in _create_device_transfers\n    curr_device_scope = control_flow_ops.no_op().device\nAttributeError: 'NoneType' object has no attribute 'device\n\nOn Fri, Nov 22, 2019 at 9:19 PM YUNCHU ZHANG <yunchu@ucla.edu> wrote:\n\n> Thank you so much!!!\n>\n>\n>\n>\n> On Fri, Nov 22, 2019 at 6:28 PM gowthamkpr <notifications@github.com>\n> wrote:\n>\n>> @YunchuZhang <https://github.com/YunchuZhang> tensorflow.contrib.staging\n>> uses StagingArea class as shown here\n>> <https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/contrib/staging/__init__.py#L20>\n>> and also yo can find staging area class here\n>> <https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/ops/data_flow_ops.py#L1770>\n>>\n>> In Tensorflow 2.0 the you can import StagingArea as follows\n>> from tensorflow.python.ops.data_flow_ops import StagingArea\n>> You can refer to the following line\n>> <https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/data_flow_ops.py#L1769>\n>> for more understanding. Thanks!\n>>\n>> \u2014\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/34475?email_source=notifications&email_token=AIAVGNNQ7SW6SQDTGILXWLDQVBTLFA5CNFSM4JP5KE3KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE7FCBY#issuecomment-557732103>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AIAVGNICVJ7KRR57PHBMKF3QVBTLFANCNFSM4JP5KE3A>\n>> .\n>>\n>\n", "@YunchuZhang Please post this question in stack overflow as github is only meant for issues related to bug/performance, build/install, feature requests and bugs. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 34474, "title": "Get a confused error when using tf.keras.layers.Conv2D to create Gan model", "body": "Hello, I tried to convert my tensorflow 1.x code to tensorflow 2.0, but met with some problems here:\r\n\r\nthe original 1.x code is:\r\ndef generator(inputs_real, is_train=True, alpha=0.01, name=\"generator\"):\r\n    # 256*256*3\r\n    with tf.variable_scope(name, reuse=(not is_train)):\r\n        # 128*128*64\r\n        conv1 = tf.layers.conv2d(inputs_real, 64, (3,3), padding='same')\r\n        conv1 = tf.nn.relu(conv1)\r\n        conv1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\r\n        # 64*64*128\r\n        conv2 = tf.layers.conv2d(conv1, 128, (3,3), padding='same')\r\n        conv2 = tf.layers.conv2d(conv1, 128, (3,3), padding='same')\r\n        conv2 = tf.nn.relu(conv2)\r\n        conv2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\r\n        # 32*32*256\r\n        conv3 = tf.layers.conv2d(conv2, 256, (3,3), padding='same')\r\n        conv3 = tf.layers.conv2d(conv2, 256, (3,3), padding='same')\r\n        conv3 = tf.nn.relu(conv3)\r\n        conv3 = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\r\n        # 16*16*512\r\n        conv4 = tf.layers.conv2d(conv3, 512, (3,3), padding='same')\r\n        conv4 = tf.layers.conv2d(conv3, 512, (3,3), padding='same')\r\n        conv4 = tf.nn.relu(conv4)\r\n        conv4 = tf.layers.max_pooling2d(conv4, (2,2), (2,2), padding='same')\r\n        # 8*8*512\r\n        conv5 = tf.layers.conv2d(conv4, 512, (3,3), padding='same')\r\n        conv5 = tf.layers.conv2d(conv4, 512, (3,3), padding='same')\r\n        conv5 = tf.nn.relu(conv5)\r\n        conv5 = tf.layers.max_pooling2d(conv5, (2,2), (2,2), padding='same')\r\n        # 4*4*512\r\n        conv6 = tf.layers.conv2d(conv5, 512, (3,3), padding='same')\r\n        conv6 = tf.layers.conv2d(conv5, 512, (3,3), padding='same')\r\n        conv6 = tf.nn.relu(conv6)\r\n        conv6 = tf.layers.max_pooling2d(conv6, (2,2), (2,2), padding='same')\r\n\r\neverything worked fine, but when I try to convert to:\r\n\r\nclass Generator(tf.keras.Model):\r\n  def __init__(self, is_train):\r\n    super(Generator, self).__init__()\r\n    self.is_train = is_train\r\n    self.conv64 = tf.keras.layers.Conv2D(64, (3, 3), input_shape=(256, 256, 3), data_format='channels_last', padding='same')\r\n    self.conv128 = tf.keras.layers.Conv2D(128, (3, 3), data_format='channels_last', padding='same')\r\n    self.conv256 = tf.keras.layers.Conv2D(256, (3, 3), data_format='channels_last', padding='same')\r\n    self.conv512 = tf.keras.layers.Conv2D(512, (3, 3), data_format='channels_last', padding='same')\r\n    self.conv_transpose1 = tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=(2, 2), data_format='channels_last', padding='same')\r\n    self.conv_transpose64 = tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), data_format='channels_last', padding='same')\r\n    self.conv_transpose128 = tf.keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), data_format='channels_last', padding='same')\r\n    self.conv_transpose256 = tf.keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), data_format='channels_last', padding='same')\r\n    self.conv_transpose512 = tf.keras.layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), data_format='channels_last', padding='same')\r\n    self.MaxPool2D = tf.keras.layers.MaxPool2D((2,2), (2,2), padding='same')\r\n    self.BatchNormalization = tf.keras.layers.BatchNormalization(trainable=is_train)\r\n\r\n  def call(self, inputs_real):\r\n    conv1 = self.conv64(inputs_real)\r\n    conv1 = layers.ReLU()(conv1)\r\n    conv1 = self.MaxPool2D(conv1)\r\n\r\n    # 64*64*128\r\n    conv2 = self.conv128(conv1)\r\n    conv2 = layers.ReLU()(conv2)\r\n    conv2 = self.MaxPool2D(conv2)\r\n\r\n    # 32*32*256\r\n    conv3 = self.conv256(conv2)\r\n    conv3 = layers.ReLU()(conv3)\r\n    conv3 = self.MaxPool2D(conv3)\r\n    print(conv3.shape)\r\n\r\n    # 16*16*512\r\n    conv4 = self.conv512(conv3)\r\n    conv4 = layers.ReLU()(conv4)\r\n    conv4 = self.MaxPool2D(conv4)\r\n    print(conv4.shape)\r\n\r\n    # 8*8*512\r\n    conv5 = self.conv512(conv4)\r\n    conv5 = layers.ReLU()(conv5)\r\n    conv5 = self.MaxPool2D(conv5)\r\n        \r\n    # 4*4*512\r\n    conv6 = self.conv512(conv5)\r\n    conv6 = layers.ReLU()(conv6)\r\n    conv6 = self.MaxPool2D(conv6)\r\n\r\nThe error is:\r\nTraceback (most recent call last):\r\n  File \"gan_2.0.py\", line 269, in <module>\r\n    train()\r\n  File \"gan_2.0.py\", line 266, in train\r\n    loss_dict = train_step(image_batch, cartoon_batch)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    gan_2.0.py:228 train_step  *\r\n        fake_cartoons = generatorc(image_batch)\r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:847 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    gan_2.0.py:59 call  *\r\n        conv6 = self.conv512(conv5)\r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:812 __call__\r\n        self.name)\r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py:213 assert_input_compatibility\r\n        ' but received input with shape ' + str(shape))\r\n\r\n    ValueError: Input 0 of layer conv2d_3 is incompatible with the layer: expected axis -1 of input shape to have value 256 but received input with shape [1, 8, 8, 512]\r\n\r\nIt seems that conv5 = self.conv512(conv4) only takes the input of shape[, , , 256], but after conv4 = self.conv512(conv3), the last shape is 512 after the con2d's kernel size is 512.  Which means the kernel size should be different between stacked layers?\r\nI'm really confused here...", "comments": ["@Panshangka \r\n\r\nCan you please share the code with proper indentation or colab link, so it will be easy to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> @Panshangka\r\n> \r\n> Can you please share the code with proper indentation or colab link, so it will be easy to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n\r\nThanks for your reply, This is my colab link:https://colab.research.google.com/drive/1i3gB11KmRjLWESslUaUMWSD6ldJ5SamU", "> It seems that conv5 = self.conv512(conv4) only takes the input of shape[, , , 256], but after conv4 = self.conv512(conv3), the last shape is 512 after the con2d's kernel size is 512. Which means the kernel size should be different between stacked layers?\r\nI'm really confused here...\r\n\r\nTF - Keras defaults to channel last representation. So the problem here is:\r\n\r\n`self.conv512` is first build with input channels 256 when calling `conv4 = self.conv512(conv3)`. However, when calling `conv5 = self.conv512(conv4)`, the input channels become 512.\r\n\r\n```python\r\nconv3 = self.conv256(conv2)\r\nconv3 = layers.ReLU()(conv3)\r\nconv3 = self.MaxPool2D(conv3)\r\nprint(conv3.shape) # here the output channels is 256\r\n\r\nconv4 = self.conv512(conv3) # so the input channels here is 256\r\nconv4 = layers.ReLU()(conv4)\r\nconv4 = self.MaxPool2D(conv4)\r\nprint(conv4.shape) # here the output channels is 512\r\n\r\n# 8*8*512\r\n# errors occurred because the `self.conv512` is built with input channels 256 but received 512 instead.\r\nconv5 = self.conv512(conv4) \r\nconv5 = layers.ReLU()(conv5)\r\nconv5 = self.MaxPool2D(conv5)\r\n```", "> ```python\r\n> conv512\r\n> ```\r\n\r\nThanks! But how do I exactly make this the same as tf.layers? I searched the documentation but got no clue. Maybe I should use another API in tf 2.0 cause I can't change the model structure that I am going to build ", "> > ```python\r\n> > conv512\r\n> > ```\r\n> \r\n> Thanks! But how do I exactly make this the same as tf.layers? I searched the documentation but got no clue. Maybe I should use another API in tf 2.0 cause I can't change the model structure that I am going to build\r\n\r\nCould you say more about the implementation of `tf.layers` so that I could help you? Maybe give a link of reference codes in `tf.layers`. Thanks!", "> > > ```python\r\n> > > conv512\r\n> > > ```\r\n> > \r\n> > \r\n> > Thanks! But how do I exactly make this the same as tf.layers? I searched the documentation but got no clue. Maybe I should use another API in tf 2.0 cause I can't change the model structure that I am going to build\r\n> \r\n> Could you say more about the implementation of `tf.layers` so that I could help you? Maybe give a link of reference codes in `tf.layers`. Thanks!\r\n\r\nThe same notebook, I have pasted the implementation of tf.layers in the second cell. It seems that tf.layers.conv2d does not defaults to channel last representation as tf.keras.layers.con2d do\r\nhttps://colab.research.google.com/drive/1i3gB11KmRjLWESslUaUMWSD6ldJ5SamU", "> The same notebook, I have pasted the implementation of tf.layers in the second cell. It seems that tf.layers.conv2d does not defaults to channel last representation as tf.keras.layers.con2d do\r\n> https://colab.research.google.com/drive/1i3gB11KmRjLWESslUaUMWSD6ldJ5SamU\r\n\r\nIt does actually. Each time you call `tf.layers.conv2d`, it will generate a convolution kernel/weight. For example, in your generator, it generates 14 kernels/weights (each of them might have different shape depending on input and parameters). However, your `tf.keras.layers.*` implementation only builds 4 convolution layers (and I see you try to reuse it if the input/output channels are the same). You could check this by printing number of parameters in each implementation. There will be a large difference between them. To fix this, you have to instantiate 14 convolution layers in `__init__`, and call it carefully in `call` .", "FYI, https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/layers/convolutional.py#L317-L424\r\n\r\nYou could see a new layer will be created (if `reuse` is False) and applied on inputs.", "> FYI, https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/layers/convolutional.py#L317-L424\r\n> \r\n> You could see a new layer will be created (if `reuse` is False) and applied on inputs.\r\n\r\n\u8c22\u8c22\uff0cI should read more about the source code", "No worries! It's a hard transition from 1.x to 2.0. Please let me know if you have further questions. Thank you :-)", "> No worries! It's a hard transition from 1.x to 2.0. Please let me know if you have further questions. Thank you :-)\r\n\r\nHello again! I successfully print out the loss during training, but when I save the model to find out the size of model only 20KB. Something went wrong, and I visualized the structure looks weird.\r\nhere is my colab link:https://colab.research.google.com/drive/1i3gB11KmRjLWESslUaUMWSD6ldJ5SamU\r\nThanks!", "It's because you *instantiate the layer in `call` and do not set it as attribute* (like `self.conv = xxxx`), the weights won't be trackable for either `Generator` or `Discriminator`.\r\n\r\nIMO, the following pattern would be better for you:\r\nhttps://colab.research.google.com/drive/1U2Sy5A3ciwPqVeSV9zFDcxoiA3kDnMCl\r\n\r\nAlso, note that some layers such as batch normalization or dropout have different behavior in training and testing phase, so you *must* pass `training` when calling the layer to differentiate different phase.\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout", "> It's because you _instantiate the layer in `call` and do not set it as attribute_ (like `self.conv = xxxx`), the weights won't be trackable for either `Generator` or `Discriminator`.\r\n> \r\n> IMO, the following pattern would be better for you:\r\n> https://colab.research.google.com/drive/1U2Sy5A3ciwPqVeSV9zFDcxoiA3kDnMCl\r\n> \r\n> Also, note that some layers such as batch normalization or dropout have different behavior in training and testing phase, so you _must_ pass `training` when calling the layer to differentiate different phase.\r\n> https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\r\n> https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\r\n\r\nThank you so much! my problem has been solved!  Now the model file is 386MB"]}, {"number": 34473, "title": "fit_generator with use_multiprocessing=True causes runtime error when initializing worker pool", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): This -> https://cloud.google.com/ml-engine/docs/runtime-version-list#1.14\r\n- TensorFlow installed from (source or binary): Binary.\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\n\r\nTrain a model with:\r\n\r\n```\r\nimport multiprocessing as mp\r\n\r\nif __name__ == '__main__':\r\n    mp.set_start_method('spawn') # or 'forkserver'\r\n\r\n    ...\r\n    ...\r\n    model.fit_generator(\r\n        generator=train_seq,\r\n        validation_data=val_seq,\r\n        callbacks=callbacks,\r\n        shuffle=True,\r\n        initial_epoch=0,\r\n        epochs=1000,\r\n        max_queue_size=16,\r\n        workers=4,\r\n        use_multiprocessing=True,\r\n        verbose=1,\r\n    )\r\n```\r\n\r\nThe following runtime error is usually raised in the middle of training (a few epochs has elapsed), near the end of an epoch, and training hanged:\r\n\r\n```\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\tException in thread Thread-33:\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\tTraceback (most recent call last):\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    self.run()\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/threading.py\", line 862, in run\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    self._target(*self._args, **self._kwargs)\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 748, in _run\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 727, in pool_fn\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    initargs=(seqs, None, get_worker_id_queue()))\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/context.py\", line 118, in Pool\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    context=self.get_context())\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 168, in __init__\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    self._repopulate_pool()\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 233, in _repopulate_pool\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    w.start()\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 105, in start\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    self._popen = self._Popen(self)\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/context.py\", line 274, in _Popen\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    return Popen(process_obj)\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/popen_spawn_posix.py\", line 33, in __init__\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    super().__init__(process_obj)\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/popen_fork.py\", line 20, in __init__\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    self._launch(process_obj)\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/popen_spawn_posix.py\", line 48, in _launch\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    reduction.dump(process_obj, fp)\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 59, in dump\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\t    ForkingPickler(file, protocol).dump(obj)\r\nERROR\t2019-11-21 10:20:18 +0800\tmaster-replica-0\t\tRuntimeError: dictionary changed size during iteration\r\n```\r\n\r\nSimilarly when using ```forkserver``` as the start method:\r\n\r\n```\r\n2019-11-20T16:54:34.389836549Z master-replica-0 Exception in thread Thread-33: E  master-replica-0\r\n2019-11-20T16:54:34.390263319Z master-replica-0 Traceback (most recent call last): E  master-replica-0\r\n2019-11-20T16:54:34.391031265Z master-replica-0   File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner E  master-replica-0\r\n2019-11-20T16:54:34.391372919Z master-replica-0     self.run() E  master-replica-0\r\n2019-11-20T16:54:34.391650676Z master-replica-0   File \"/usr/lib/python3.5/threading.py\", line 862, in run E  master-replica-0\r\n2019-11-20T16:54:34.391963958Z master-replica-0     self._target(*self._args, **self._kwargs) E  master-replica-0\r\n2019-11-20T16:54:34.392265796Z master-replica-0   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 748, in _run E  master-replica-0\r\n2019-11-20T16:54:34.392584800Z master-replica-0     with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor: E  master-replica-0\r\n2019-11-20T16:54:34.392927646Z master-replica-0   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/data_utils.py\", line 727, in pool_fn E  master-replica-0\r\n2019-11-20T16:54:34.393241405Z master-replica-0     initargs=(seqs, None, get_worker_id_queue())) E  master-replica-0\r\n2019-11-20T16:54:34.393555641Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/context.py\", line 118, in Pool E  master-replica-0\r\n2019-11-20T16:54:34.394298315Z master-replica-0     context=self.get_context()) E  master-replica-0\r\n2019-11-20T16:54:34.394565820Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 168, in __init__ E  master-replica-0\r\n2019-11-20T16:54:34.394963264Z master-replica-0     self._repopulate_pool() E  master-replica-0\r\n2019-11-20T16:54:34.395295143Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 233, in _repopulate_pool E  master-replica-0\r\n2019-11-20T16:54:34.395615577Z master-replica-0     w.start() E  master-replica-0\r\n2019-11-20T16:54:34.395916461Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/process.py\", line 105, in start E  master-replica-0\r\n2019-11-20T16:54:34.396240472Z master-replica-0     self._popen = self._Popen(self) E  master-replica-0\r\n2019-11-20T16:54:34.396524190Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/context.py\", line 281, in _Popen E  master-replica-0\r\n2019-11-20T16:54:34.396835565Z master-replica-0     return Popen(process_obj) E  master-replica-0\r\n2019-11-20T16:54:34.397135734Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/popen_forkserver.py\", line 36, in __init__ E  master-replica-0\r\n2019-11-20T16:54:34.397473096Z master-replica-0     super().__init__(process_obj) E  master-replica-0\r\n2019-11-20T16:54:34.397750616Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/popen_fork.py\", line 20, in __init__ E  master-replica-0\r\n2019-11-20T16:54:34.398015022Z master-replica-0     self._launch(process_obj) E  master-replica-0\r\n2019-11-20T16:54:34.398270130Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/popen_forkserver.py\", line 48, in _launch E  master-replica-0\r\n2019-11-20T16:54:34.398548603Z master-replica-0     reduction.dump(process_obj, buf) E  master-replica-0\r\n2019-11-20T16:54:34.398846387Z master-replica-0   File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 59, in dump E  master-replica-0\r\n2019-11-20T16:54:34.399111270Z master-replica-0     ForkingPickler(file, protocol).dump(obj) E  master-replica-0\r\n2019-11-20T16:54:34.399379014Z master-replica-0 RuntimeError: dictionary changed size during iteration E  master-replica-0\r\n```", "comments": ["@tonysung, Please provide the complete standalone code to reproduce the reported issue. Thanks!", "@tonysung, Please post complete code snippet to analyze the issue. Thanks!", "Sorry, we wouldn't be able to provide a reduced test case. Please close.", "Closing the issue. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34472, "title": "Allow building two pip cpu packages on macos.", "body": "Properly build both packages instead renaming.\r\n\r\nPiperOrigin-RevId: 281635032\r\nChange-Id: I3577d1f4f213e1d1daf4caa757506be48d63972c", "comments": []}, {"number": 34471, "title": "Fix bug in NCCL broadcast wrapper", "body": "The recvbuf passed to ncclBroadcast must not be nullptr, else it causes CUDA_ERROR_ILLEGAL_ADDRESS. This issue arises if the source node of the broadcast has no output.\r\n\r\nattn @dubey \r\ncc @nluehr ", "comments": []}, {"number": 34470, "title": "tf.meshgrid bug: Duplicate node name in graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.0, cudnn 7\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**.\r\nWhat I am trying to do is to warp a feature map (shape (N, H, W, C)) using an offset (shape (N, H, W, 2)). However, it doesn't compile. It seems there's some problem on name scopes inside tf.meshgrid.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kingo/Documents/projects/SunRelight/code/sun_relight/network_training/test.py\", line 25, in <module>\r\n    out = warp(x, offset)\r\n  File \"/home/kingo/Documents/projects/SunRelight/code/sun_relight/network_training/test.py\", line 14, in warp\r\n    base_coord2 = tf.stack(tf.meshgrid(tf.range(W), tf.range(H))[::-1], axis=-1)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2954, in meshgrid\r\n    mult_fact = ones(shapes, output_dtype)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2583, in ones\r\n    output = fill(shape, constant(one, dtype=dtype), name=name)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 171, in fill\r\n    result = gen_array_ops.fill(dims, value, name=name)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 3602, in fill\r\n    \"Fill\", dims=dims, value=value, name=name)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1773, in __init__\r\n    control_input_ops)\r\n  File \"/home/kingo/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1613, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Duplicate node name in graph: 'ones'\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nNo error should happen.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.python.keras import backend as K\r\n\r\ndef warp(inputs, offset):\r\n  tensor_input_shape = K.shape(inputs)\r\n  batch_size = tensor_input_shape[0]\r\n  H = tensor_input_shape[1]\r\n  W = tensor_input_shape[2]\r\n\r\n  base_coord = tf.stack(tf.meshgrid(tf.range(W), tf.range(H))[::-1], axis=-1)\r\n  base_coord2 = tf.stack(tf.meshgrid(tf.range(W), tf.range(H))[::-1], axis=-1) # Only to show the bug\r\n  # base_coord = tf.tile(base_coord[tf.newaxis, :, :, :], [batch_size, 1, 1, 1])\r\n  # target_coord = tf.cast(tf.cast(base_coord, tf.float32) + offset, tf.int32)\r\n  # target_coord = tf.clip_by_value(\r\n  #     target_coord, 0,\r\n  #     tf.stack([H-1, W-1])[tf.newaxis, tf.newaxis, tf.newaxis, :])\r\n\r\n  return tf.gather_nd(inputs, base_coord2, batch_dims=1)\r\n\r\nx = keras.Input(shape=(None, None, 3))\r\noffset = keras.Input(shape=(None, None, 2))\r\nout = warp(x, offset)\r\nnet = keras.Model(inputs=[x, offset], outputs=out)\r\n```\r\n\r\n", "comments": ["Can you try running the code in latest -tf-nightly `(!pip install tf-nightly-gpu==2.1.0dev20191121`).Issue seemed to be fixed, kindly find the [gist](https://colab.sandbox.google.com/gist/ravikyram/68cba0317e31a6aa835523edabc03d4b/untitled393.ipynb) of colab for the same.Thanks!", "@kevinkingo \r\nCan you please confirm whether your issue got resolved with nightly versions.Please,confirm whether we can close this thread if the issue was already resolved. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@ravikyram\r\nI found there still have the same error in the stable vesion `tensorflow==2.1.0` (Jan 9, 2020) ...\r\n```bash\r\nValueError: Duplicate node name in graph: 'ones'\r\n```\r\nbut the `tf-nightly-gpu==2.1.0dev20191121` above is alright."]}, {"number": 34469, "title": "ValueError with tf.data.Dataset and model.fit and tf.distribute", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker image\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not mobile\r\n- TensorFlow installed from (source or binary): Docker image: tensorflow/tensorflow:nightly-gpu-py3\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.6\r\n- GPU model and memory: NVIDIA V100\r\n\r\n**Describe the current behaviour**\r\nI receive the following stack trace\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 571, in <module>\r\n    verbose=2,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 792, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 264, in fit\r\n    training_dataset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 677, in experimental_distribute_dataset\r\n    return self._extended._experimental_distribute_dataset(dataset)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 574, in _experimental_distribute_dataset\r\n    split_batch_by=self._num_replicas_in_sync)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/input_lib.py\", line 89, in get_distributed_dataset\r\n    input_context=input_context)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/input_lib.py\", line 509, in __init__\r\n    dataset = distribute._RebatchDataset(dataset, split_batch_by)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/experimental/ops/distribute.py\", line 110, in __init__\r\n    rebatch, dataset_ops.get_structure(input_dataset))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/nest.py\", line 245, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/nest.py\", line 245, in <listcomp>\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/experimental/ops/distribute.py\", line 105, in rebatch\r\n    batch_size = recalculate_batch_size(type_spec._to_legacy_output_shapes())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/experimental/ops/distribute.py\", line 90, in recalculate_batch_size\r\n    if len(output_shapes) < 1:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 823, in __len__\r\n    raise ValueError(\"Cannot take the length of shape with unknown rank.\")\r\nValueError: Cannot take the length of shape with unknown rank.\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect training to proceed without issue.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThis is an excerpt of my code, hopefully it is sufficient to demonstrate what I am doing.\r\n```\r\ndef parse_entry(entry):\r\n    # Create a dictionary describing the features\r\n    feature_description = {\r\n        \"num\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\r\n        \"width\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\r\n        \"height\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\r\n        \"image_l\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\r\n        \"image_r\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\r\n        \"disparity_l\": tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.float32, allow_missing=True),\r\n        \"disparity_r\": tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.float32, allow_missing=True),\r\n    }\r\n    example = tf.io.parse_single_example(entry, feature_description)\r\n\r\n    # Need to decode the image data\r\n    example[\"image_l\"] = tf.io.decode_image(example[\"image_l\"], channels=0, dtype=tf.uint8)\r\n    example[\"image_r\"] = tf.io.decode_image(example[\"image_r\"], channels=0, dtype=tf.uint8)\r\n\r\n    # Disparities were flattened, need to recover their actual shape\r\n    # Assuming that channels will be the last axis\r\n    example[\"disparity_l\"] = tf.reshape(example[\"disparity_l\"], (example[\"width\"], example[\"height\"], 1))\r\n    example[\"disparity_r\"] = tf.reshape(example[\"disparity_r\"], (example[\"width\"], example[\"height\"], 1))\r\n\r\n    return (\r\n        (example[\"num\"], example[\"width\"], example[\"height\"], example[\"image_l\"], example[\"image_r\"],),\r\n        (example[\"num\"], example[\"width\"], example[\"height\"], example[\"disparity_l\"], example[\"disparity_r\"],),\r\n    )\r\n\r\ndef create_dataset(path, args):\r\n    # Create a stats aggregator for the dataset\r\n    aggregator = tf.data.experimental.StatsAggregator()\r\n\r\n    # Load in the dataset\r\n    dataset = tf.data.TFRecordDataset(path)\r\n\r\n    # Shuffle the elements\r\n    if args.shuffle_size > 1:\r\n        dataset = dataset.shuffle(args.shuffle_size, reshuffle_each_iteration=False)\r\n\r\n    # Convert dataset to training format\r\n    dataset = dataset.map(parse_entry)\r\n\r\n    # Set up batch size\r\n    dataset = dataset.batch(batch_size=args.batch_size, drop_remainder=False)\r\n\r\n    # Set output shapes, types, and classes\r\n    dataset.output_classes = (\r\n        (tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor),\r\n        (tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor),\r\n    )\r\n    dataset.output_types = (\r\n        (tf.dtypes.string, tf.dtypes.int64, tf.dtypes.int64, tf.dtypes.uint8, tf.dtypes.uint8,),\r\n        (tf.dtypes.string, tf.dtypes.int64, tf.dtypes.int64, tf.dtypes.float32, tf.dtypes.float32,),\r\n    )\r\n    dataset.output_shapes = (\r\n        ([], [], [], tf.TensorShape([None, None, 3]), tf.TensorShape([None, None, 3]),),\r\n        ([], [], [], tf.TensorShape([None, None, 1]), tf.TensorShape([None, None, 1]),),\r\n    )\r\n\r\n    # Extract the first element so we can retrieve image shapes\r\n    for entry in tf.data.TFRecordDataset(path).take(1):\r\n        element = parse_entry(entry)\r\n        # shape = (element[0][\"width\"], element[0][\"height\"])\r\n        shape = (element[0][2], element[0][1])\r\n\r\n    return dataset, shape\r\n\r\ntrain_dataset, input_resolution = create_dataset(args.train_dataset, args)\r\nvalid_dataset, _ = create_dataset(args.valid_dataset, args)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    # Create the model\r\n    model = CustomKerasModel()\r\n\r\n    # Compile the model, set optimiser, loss function, and metrics to use\r\n    model.compile(optimizer=optimiser, loss=loss, metrics=metrics)\r\n\r\n    # Train\r\n    model.fit(\r\n        x=train_dataset,\r\n        validation_data=valid_dataset,\r\n        validation_steps=None,\r\n        validation_freq=1,\r\n        callbacks=callbacks,\r\n        epochs=args.num_epochs,\r\n        verbose=2,\r\n    )\r\n\r\n    model.save(\r\n        output_path,\r\n        overwrite=True,\r\n        include_optimizer=True,\r\n        save_format=\"h5\",\r\n    )\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI think this problem is similar to #24520 but with `tf.distribute.MirroredStrategy` mixed in. I am using a TFRecordDataset and I am setting `output_classes`, `output_shapes`, and `output_types`, so I am not sure which tensor it is unable to get the shape of. Is there anything I can do to narrow down what is causing this bug? Keep in mind, I am running in a docker container, so code changes are a little difficult to achieve.\r\n\r\nThis may only happen when using `tf.distribute.MirriredStrategy` with a single GPU.\r\n", "comments": ["The issue here seems to be that your dataset has some elements with (completely) unknown shape, and we fail to handle that case correctly. Thanks for flagging this, I have a fix in review. \r\n\r\n\r\nSide note: I don't expect the following code to work:\r\n```\r\n    # Set output shapes, types, and classes\r\n    dataset.output_classes = (\r\n        (tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor),\r\n        (tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor),\r\n    )\r\n    dataset.output_types = (\r\n        (tf.dtypes.string, tf.dtypes.int64, tf.dtypes.int64, tf.dtypes.uint8, tf.dtypes.uint8,),\r\n        (tf.dtypes.string, tf.dtypes.int64, tf.dtypes.int64, tf.dtypes.float32, tf.dtypes.float32,),\r\n    )\r\n    dataset.output_shapes = (\r\n        ([], [], [], tf.TensorShape([None, None, 3]), tf.TensorShape([None, None, 3]),),\r\n        ([], [], [], tf.TensorShape([None, None, 1]), tf.TensorShape([None, None, 1]),),\r\n    )\r\n```\r\nYou'd get an error if you try to set dataset output shapes/types explicitly... unless you meant this as pseudocode?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34469\">No</a>\n"]}, {"number": 34468, "title": "Make test tag filters behave as intended", "body": "`configure.py` adds multiple instances of the `--test_tag_filters` and `--build_tag_filters` arguments to the bottom of `.tf_configure.bazelrc`. For example, on a Mac, the configuration file contains the lines:\r\n```\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu,-nomac,-no_mac\r\ntest --build_tag_filters=-gpu,-nomac,-no_mac\r\n```\r\nThese lines are intended to disable tests with the tags `benchmark-test`, `no_oss`, `oss_serial`, `gpu`, `nomac`, and `no_mac`. Unfortunately, Bazel's `--test_tag_filters` and `--build_tag_filters` commands replace their respective lists of filters instead of adding to them. So only the last set of filters (`-gpu,-nomac,-no_mac` in this case) ends up getting applied. As a result the default Bazel configuration will run tests that shouldn't run.\r\n\r\nThis PR modifies `configure.py` so that the script generates a single value for the `--test_tag_filters` and `--build_tag_filters` arguments. After this change, the lines of `.tf_configure.bazelrc` described above turn into:\r\n```\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac\r\n```", "comments": []}, {"number": 34467, "title": "Fix: typo in tensorflow/compiler/xla/g3doc/shapes.md", "body": "", "comments": ["@mihaimaruseac can we take this in ?", "@nuka137 in the future, if you see a typo in a file, please try to fix all of the typos there. This is because we are running hours of CI infrastructure and would make sense to batch easy fixes together to amortize this cost."]}, {"number": 34466, "title": "TF 1.15 to TF 2.0 breaking changes even with tf.compat.v1 for saving models for use with tf serving", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian dockerized\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): docker\r\n- TensorFlow version (use command below): 1.15 and 2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nCurrently trying to export a `tf.keras` model in v `2.0.0` using `tf.compat.v1` functions (which will be removed in the future) does not work. Using the same code in `1.15.0` works.  Currently there is not a `tf.compat.v2` function which is concerning for those using this functionality for compatibility with tf serving.\r\n\r\n\r\n**Describe the expected behavior**\r\nThat code with `tf.compat.v1` works in `2.0` as it does in `1.15`\r\n\r\n**Code to reproduce the issue**\r\nThis repo: https://gitlab.com/SumNeuron/docker-nf\r\nIs my dockerized TF MWE. It contains the code necessary to reproduce the issue as well as subsequent steps (e.g. spin up a dockerized web app with the served model).\r\n\r\nIn order to reproduce the bug there are two files of relevance:\r\n\r\n1. `Dockerfile.ai`\r\n2. `notebooks/Make Toy Model.ipynb`\r\n\r\nThe first two lines of `Dockerfile.ai`:\r\n```\r\nFROM tensorflow/tensorflow:1.15.0rc2-gpu-py3-jupyter\r\n# FROM tensorflow/tensorflow:2.0.0-gpu-py3-jupyter\r\n```\r\nare for convenience of toggling between TF versions.\r\n\r\nTo spin up the service:\r\n\r\n```\r\ndocker-compose -f docker-compose.ai.development.yml build\r\ndocker-compose -f docker-compose.ai.development.yml up\r\n```\r\n\r\nThen navigate to the mounted notebook.\r\nAfter running the toy model there is a code cell with markdown content:\r\n\r\n>Simple Save for Serving\r\n\r\nunder which has the following code (which is more or less copy-paste from a tf official documentation example).\r\n\r\n```\r\nMODEL_DIR = '../models/serving/toy'\r\nversion = 1\r\nexport_path = os.path.join(MODEL_DIR, str(version))\r\nif not os.path.isdir(export_path):\r\n    os.makedirs(export_path)\r\n    \r\ntf.compat.v1.saved_model.simple_save(\r\n    tf.compat.v1.keras.backend.get_session(),\r\n    export_path,\r\n    inputs={'input_tensor': model.input},\r\n    outputs={'output_tensor': model.outputs[0]})\r\n```\r\nThis will work when using tf version `1.15`. It will fail with tf version `2.0`\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@SumNeuron , the notebook you have shared saves the model successfully. Please share a notebook in which you face the issue. ", "@nikochiko did you change the `FROM` statement in `Dockerfile.ai` from \r\n\r\n```dockerfile\r\nFROM tensorflow/tensorflow:1.15.0rc2-gpu-py3-jupyter\r\n# FROM tensorflow/tensorflow:2.0.0-gpu-py3-jupyter\r\n```\r\n\r\nto \r\n\r\n```dockerfile\r\n# FROM tensorflow/tensorflow:1.15.0rc2-gpu-py3-jupyter\r\nFROM tensorflow/tensorflow:2.0.0-gpu-py3-jupyter\r\n```\r\n", "@SumNeuron What error are you getting? You have provided too little information to help. \r\nAnd to make TF1.X code compatible with TF2.X, you need to disable eager execution with `tf.compat.v1.disable_eager_execution()`.\r\nWorks fine for me. See gist: https://colab.research.google.com/gist/nikochiko/cc43b4e4e57918669c217c9fe91270b1/34466.ipynb", "@oanush @gowthamkpr , I think this issue is wrongly marked as a bug. Please see.", "@nikochiko ah so it was \r\n```\r\ntf.compat.v1.disable_eager_execution()\r\n``` \r\nthat was the missing line?\r\n", "@SumNeuron Yes. See the gist.", "@nikochiko Then for now I recommend perhaps updating the error message.\r\nSet `enable_eager = True` and run with 2.0 and get the error:\r\n\r\n\r\n```\r\nRuntimeError: build_tensor_info is not supported in Eager mode.\r\n```\r\n\r\nto include a statement pointing to `tf.compat.v1.disable_eager_execution()`", "@nikochiko to see the error message. When eager was rolled out there was emphasis on enabling it (auto-on for 2.0), but i have yet to encounter that it could or may need to be disabled for v1 compatibility, so adding a note to the error message may be useful to other users as well", "No, actually you can avoid errors altogether with this syntax instead: \r\n```python\r\nif tf.executing_eagerly():\r\n  tf.compat.v1.disable_eager_execution()\r\n```", "@nikochiko the point I was trying to make is that the need for\r\n```\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\nis not made explicitly clear, especially as many `tf.compat.v1` functions also work in eagerly. \r\n\r\n\r\nSemirelated, do you know the v2 version of the code (e.g. how to achieve the same `.pb` file without `v1` functions?", "@SumNeuron , if I understand correctly, you are trying to make your code compatible with both `TensorFlow 1.X` and `TensorFlow 2.X`. In that case, you should first decide whether you want to have eager execution enabled or not, and then you can make your code consistent with that. As you mentioned that some v1 compat functions are compatible with eager execution, but it will be weird to have your code execute differently in 1.X and 2.X. \r\n\r\nFor the TensorFlow 2.X version of the code, you can use the [`tf_upgrade_v2`](https://www.tensorflow.org/guide/upgrade) utility. It will convert all v1 symbols to their v2 equivalents. ", "@nikochiko Not quite. I am happy to transfer to v2 and live in the new TF paradigm. \r\n\r\nYou might have noticed in your colab (copy from the repo I provided), the header for the code which breaks without disabling eager, is \"Simple Save for Serving\". \r\n\r\nThe only thing I want to be compatible in V2 is saving a trained keras model for later use with `tf-serving`", "@SumNeuron , if you are ready to move to TF2, you only need to apply `tf_upgrade_v2`.\r\n@gowthamkpr , please take this on from here.", "@SumNeuron I am gonna close this issue for now as its not related to bug/performance, build/install, feature or docs related issues. Please post any more questions that you have on stackoverflow as there is a wider community to respond. Thanks @nikochiko for answering."]}, {"number": 34465, "title": "[r2.1:Cherrypick]Support /d2ReducedOptimizeHugeFunctions on Windows when TF_VC_VERSION=16.4", "body": "See https://groups.google.com/a/tensorflow.org/g/build/c/SsW98Eo7l3o\n\nPiperOrigin-RevId: 281412015\nChange-Id: Icdea77fe0677ad6b0e1c5cf8f053a81a14cb402e", "comments": []}, {"number": 34464, "title": "TFLite Micro adding simple api for easier integration with c libraries", "body": "- Goal of this commit is to be able to create a simpler interface to the code that can be generated as part of a libtfmicrolib.a so that the only header file that is needed by other code is the tf_micro_simple.h\r\n\r\n- Adds fPIC to the makefile which was causing problems with creating a shared library if not included  ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34464) for more info**.\n\n<!-- need_sender_cla -->", "I'm not expecting this to be approved as is, more of a work in progress. Wanted to just sharemy perspective working on a majority c code base. A simple api like this would make it easier to incorporate  tf_micro as a library. The thought process would be to \r\n\r\n-> build libtfmicro.a for your specific platform \r\n-> the user specific code then only needs to include tf_micro_simple.h and the two api's to be able to setup and invoke the model. There won't be a need to include other headers from. If you look at the main_function.cc changes in hello world the idea would be to separate out the tf_micro code as much as possible (at least for the most common scenarios)\r\n", "@googlebot I signed it!", "Thanks for this PR @cdknorow. This is useful feedback for us.", "@sensiml  thank you for your contribution, please sign CLA.", "@googlebot  I signed it!", "@sensiml Please sign CLA and resolve the conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "@rthadur @advaitjain  Hey, thanks I wasn't able to work on it for a while. Also I wasn't sure if the tflite team was interested in the sort of change to decouple the tflite library from the example applications so I wasn't going to put too much effort into it without that feedback.", "We did sign the CLA aggreements thought, but it never seemed to register with the system", "@petewarden any thoughts on above comments from @sensiml ?"]}, {"number": 34463, "title": "Unify the APIs on a tf.keras.Model when trained vs when it is saved and loaded.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nExplaining the issue using inference as an example API:\r\nRunning inference on a loaded model is different than running inference on model you just trained and this causes confusion. For a trained model we use: `model.predict(input)` but for a loaded saved model we need to directly invoke the model by running `model(input)` as explained below:\r\n\r\na) Train a Model\r\n```\r\nmodel = tf.keras.Model(...)\r\nmodel.compile(...)\r\nmodel.fit(...)\r\n```\r\n\r\n**Method 1:**\r\nb) Run Predictions\r\n`model.predict(input)`\r\n\r\n**Method 2:**\r\nb) Save a Model\r\n`tf.saved_model.save(model, '/tmp/model')`\r\nc) Load a Model\r\n`tf.saved_model.load(model, '/tmp/model')`\r\nd) Run predictions on loaded model\r\n`model(input)`\r\n\r\nNote: There are many other model APIs that also differ this way. It would be better to have a consistent format.\r\n\r\n**Will this change the current api? How?**\r\nYes, it would change the current API. You'd have to add additional functions to the current loaded model format or ensure it is loaded in the same way as a model trained in a jupyter notebook.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone using tensorflow 2.0\r\n\r\n**Any Other info.**\r\n-\r\n", "comments": ["You should use `tf.keras.models.load_model` to use the Keras APIs (`predict`, `fit`, etc.) on the loaded model.", "Thank you! "]}, {"number": 34462, "title": "Please don't implement a feature suggested in a categorical cross-entropy TODO", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): v2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis is actually an anti-request. I'm requesting that the developers not implement suggested future behavior.\r\n\r\nThere is a TODO comment in [tensorflow/python/ops/nn_ops.py line 3289](https://github.com/tensorflow/tensorflow/blob/b646d41f69f8a1cf2f5c76d934adbf8c067e4444/tensorflow/python/ops/nn_ops.py#L3289) that states an intention to change the categorical cross-entropy loss algorithm so that it will raise an error if the sum of any label vector is not equal to 1. The comment states:\r\n```\r\n  # TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\r\n  # could break users who call this with bad labels, but disregard the bad\r\n  # results.\r\n```\r\n\r\nI make use of label vectors with all elements set to zero in order to perform spatial masking of data in my 2D convolutional neural network. I have regions in some of my images where there is no label available, and the regions change from image to image. These regions make no contribution to the loss if the corresponding label vectors are set to all zero, effectively removing them from gradient calculation and back-propagation. (The regions in the images have valid input data, but a label cannot be assigned.) My suggestion is that this TODO not be implemented or that the implementation would allow for all-zero label vectors.\r\n\r\n**Will this change the current api? How?**\r\nThis will not change the current API.\r\n\r\n**Who will benefit with this feature?**\r\nPeople (like me) who use the current way the algorithm is implemented to do per-image spatial masking.", "comments": ["@JimBiardCics,\r\nIs this still an issue? \r\n\r\nI see that the implementation has not been changed from [TF v2.0](https://github.com/tensorflow/tensorflow/blob/b646d41f69f8a1cf2f5c76d934adbf8c067e4444/tensorflow/python/ops/nn_ops.py#L3289) to [TF v2.4](https://github.com/tensorflow/tensorflow/blob/86c79e9779a2d2653fddbb0e0391e2ea8121e81c/tensorflow/python/ops/nn_ops.py#L3920). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34461, "title": "Can't load libcublas 10.0 on nightly build 2.1.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: tf-nightly-gpu==2.1.0.dev20191120\r\n- Python version: 3.6.8 or 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 - 7.6\r\n- GPU model and memory: RTX 2080 8Go\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI'm trying to run the 2.1.0 nightly build on an RTX 2080. It fails to load all libcublas, libcudart, etc..\r\nIn a tensorflow-gpu==2.0.0 environment, those files load perfectly.\r\n\r\nRunning a simple script triggers the error:\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.random.uniform((2, 2)))\r\n```\r\n\r\nOn the 2.0.0 environment:\r\n![image](https://user-images.githubusercontent.com/12402673/69258553-13f88500-0bbd-11ea-8145-323c27c22e3f.png)\r\n\r\nOn the nightly 2.1.0 environment:\r\n![image](https://user-images.githubusercontent.com/12402673/69258616-2d013600-0bbd-11ea-94af-ef9e665ea14d.png)\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n2.1.0 trace\r\n```\r\n2019-11-20 17:45:31.698108: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2019-11-20 17:45:31.698128: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2019-11-20 17:45:32.396003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-20 17:45:32.422048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:45:32.422487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5\r\ncoreClock: 1.785GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\r\n2019-11-20 17:45:32.422551: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2019-11-20 17:45:32.422591: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2019-11-20 17:45:32.422629: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2019-11-20 17:45:32.422664: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2019-11-20 17:45:32.422700: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2019-11-20 17:45:32.422734: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2019-11-20 17:45:32.425284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-20 17:45:32.425298: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2019-11-20 17:45:32.425514: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-20 17:45:32.430540: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-11-20 17:45:32.430970: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fefaa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-20 17:45:32.430984: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-20 17:45:32.528493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:45:32.529025: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ff19e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-11-20 17:45:32.529091: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5\r\n2019-11-20 17:45:32.529149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-20 17:45:32.529156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      \r\ntf.Tensor(\r\n[[0.8163065  0.6548419 ]\r\n [0.81401074 0.1661377 ]], shape=(2, 2), dtype=float32)\r\n\r\n```\r\n\r\n2.0.0 trace\r\n```\r\n2019-11-20 17:44:00.771398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-20 17:44:00.795944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.796375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.785\r\npciBusID: 0000:01:00.0\r\n2019-11-20 17:44:00.796524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-20 17:44:00.797505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-20 17:44:00.798254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-20 17:44:00.798438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-20 17:44:00.799488: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-20 17:44:00.800282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-20 17:44:00.802750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-20 17:44:00.802822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.803281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.803677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-20 17:44:00.803901: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-20 17:44:00.808387: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-11-20 17:44:00.808663: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4eb4be0 executing computations on platform Host. Devices:\r\n2019-11-20 17:44:00.808725: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-20 17:44:00.905964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.906455: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4eb6a40 executing computations on platform CUDA. Devices:\r\n2019-11-20 17:44:00.906471: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5\r\n2019-11-20 17:44:00.906570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.906972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.785\r\npciBusID: 0000:01:00.0\r\n2019-11-20 17:44:00.906997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-20 17:44:00.907006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-11-20 17:44:00.907014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-11-20 17:44:00.907022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-11-20 17:44:00.907030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-11-20 17:44:00.907038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-11-20 17:44:00.907046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-20 17:44:00.907078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.907480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.907858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-20 17:44:00.907882: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-20 17:44:00.908473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-20 17:44:00.908484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-11-20 17:44:00.908489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-11-20 17:44:00.908554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.909022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-20 17:44:00.909423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7466 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\ntf.Tensor(\r\n[[0.36159635 0.30549836]\r\n [0.09631395 0.7153827 ]], shape=(2, 2), dtype=float32)\r\n```\r\n", "comments": ["The nightly builds, which use the master branch, and is for the release after 2.1, have switched to CUDA 10.1.\r\n\r\nThis was done with commit: https://github.com/tensorflow/tensorflow/commit/7ed383001f1e70a995b2da25db17cb676745f9c5#diff-ade1d3e4b7c35655f854151d899df62b", "@wdirons Thanks for the info, closing this!"]}, {"number": 34460, "title": "404 link in documentation", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nNon functioning link in the documentation:\r\n\r\n```\r\n### Custom Training Data\r\nBy default the script will download the [Speech Commands\r\ndataset](https://download.tensorflow.org/data/speech_commands_v0.01.tgz)\r\n```\r\n\r\nhttps://download.tensorflow.org/data/speech_commands_v0.01.tgz leads to a non working page. ", "comments": ["It's showing that the particular page is \"unsafe\". Going to the advanced tab, we can download the dataset.  ", "Closing this issue since the associated PR has merged and dataset download link is fixed. Thanks!"]}, {"number": 34459, "title": "Modifications to BUILD to Use Target libtensorflowlite and make tar.gz", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: tag v2.0.0\r\n- Python version: Python 3.6.8\r\n- Installed using virtualenv? pip? conda?: from GitHub repository\r\n- Bazel version (if compiling from source): Build label: 0.26.0\r\n- GCC/Compiler version (if compiling from source): gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI modified `tensorflow/tensorflow/tools/lib_package/BUILD` (enclosed). I am trying to build Tensorflow Lite in the same way the Tensorflow C library is built.\r\n\r\n```bash\r\nbazel build --jobs=1 --local_ram_resources=1024 --config=opt --config=v2 --config=noaws //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\nFinal output:\r\n\r\n```\r\nTarget //tensorflow/tools/lib_package:libtensorflow up-to-date:\r\n  bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz\r\n```\r\nI'd like to do the same for Tensorflow Lite.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Compiled Tensorflow C package using `bazel build --jobs=1 --local_ram_resources=1024 --config=opt --config=v2 --config=noaws //tensorflow/tools/lib_package:libtensorflow`\r\n2. Modified `tensorflow/tensorflow/tools/lib_package/BUILD`.\r\n3. Compiled Tensorflow Lite using `bazel build --jobs=1 --local_ram_resources=1024 --config=opt --config=v2 --config=noaws //tensorflow/lite:libtensorflowlite.so`.\r\n4. The archive `libtensorflowlite.tar.gz` is not produced.\r\n5. Tried `bazel build --jobs=1 --local_ram_resources=1024 --config=opt --config=v2 --config=noaws //tensorflow/lite:libtensorflowlite.so` but got the error\r\n\r\n```\r\nERROR: Skipping '//tensorflow/lite:libtensorflowlite': no such target '//tensorflow/lite:libtensorflowlite': target 'libtensorflowlite' not declared in package 'tensorflow/lite' (did you mean 'libtensorflowlite.so'?) defined by /mnt/drive/stuff/src/tensorflow/tensorflow/lite/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target '//tensorflow/lite:libtensorflowlite': target 'libtensorflowlite' not declared in package 'tensorflow/lite' (did you mean 'libtensorflowlite.so'?) defined by /mnt/drive/stuff/src/tensorflow/tensorflow/lite/BUILD\r\nINFO: Elapsed time: 0.279s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 pac\\\r\nkages loaded)\r\n```\r\n\r\nI am happy to turn this into a PR.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nN/A\r\n\r\n[BUILD.zip](https://github.com/tensorflow/tensorflow/files/3870140/BUILD.zip)\r\n\r\n", "comments": ["Apologies! My change worked. I found `libtensorflowlite.tar.gz` in `bazel-bin/tensorflow/tools/lib_package/`. I'll fork, push, and make a PR.", "> Apologies! My change worked. I found `libtensorflowlite.tar.gz` in `bazel-bin/tensorflow/tools/lib_package/`.\r\n\r\n@dtsmith2001,\r\nAny updates regarding this? Please feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar Sorry, I got diverted. Let me look into this today.", "No time to work on this. Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34459\">No</a>\n"]}, {"number": 34458, "title": "Using skip and ignore_errors cause training hang", "body": "Using skip on a dataset that contains corrupted data and then applying ignore_errors() causes fit method to hang before the validation step. The fit method uses dataset for training and validation.\r\n\r\nThis behavior was reported using\r\nUbuntu 18.04, Tensorflow 2.0.0 installed using pip, python 3.7.3\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\ndataset = tf.data.Dataset.from_tensor_slices([1., 0., 2., 1.,])\r\ndataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"error\"))\r\n\r\ntrain = tf.data.Dataset.zip((dataset, dataset)).batch(1).repeat().apply(tf.data.experimental.ignore_errors())\r\nval = tf.data.Dataset.zip((dataset, dataset)).skip(2).batch(1).repeat().apply(tf.data.experimental.ignore_errors())\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(32, activation='relu', input_shape=(1,)),\r\n])\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.fit(train, epochs=10, steps_per_epoch=4, validation_data=val, validation_steps=2 )\r\n```\r\n\r\n", "comments": ["Issue is replicating with Tf 2.0.\r\nPlease take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/db3cd6c2d13e3b16b6cc3fedfac2b1ae/untitled265.ipynb). Thanks!", "@grzjab This bug has been fixed in TF-nightly. Please find my github gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/3beef897724021a730c2e8b0086068c7/copy-of-untitled265.ipynb).", "Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34458\">No</a>\n"]}, {"number": 34457, "title": "Model.fit displays wrong total count in progress bar when using validation data with undefined length", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS\r\n- TensorFlow installed from (source or binary): binary (pip install tensorflow)\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `model.fit` method with specified `validation_data`, the progress bar reports the validation (instead of training) dataset size as the total count in the progress bar. After the end of the epoch the values are correctly displayed.\r\n\r\nThe bug occurs when using tf.data datasets with unknown number of elements (for example when using generators).\r\n\r\nExample output:\r\n\r\nEpoch 1/10\r\n1024/1024 [==============================] - 7s 7ms/step - loss: 23984083.2295 - accuracy: 0.1007 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 2/10\r\n1024/1024 [==============================] - 5s 5ms/step - loss: 25074923.1717 - accuracy: 0.0985 - val_loss: 30138559.6406 - val_accuracy: 0.1052\r\nEpoch 3/10\r\n 78/128 [=================>............] - ETA: 0s - loss: 28146315.7692 - accuracy: 0.1126\r\n\r\n**Describe the expected behavior**\r\n\r\nThe progress bar should report the total number of elements infered from the training dataset, not from the validation dataset. The last line in the example output should be 78/1024, not 78/128.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\n# Model from tf Keras overview\r\nmodel = tf.keras.Sequential([\r\n    layers.Dense(64, activation='relu', input_shape=(32,)),\r\n    layers.Dense(64, activation='relu'),\r\n    layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n\r\n# Data generator\r\ndef get_dataset(batch_size, num_batches):\r\n    \"\"\" Get random tf.data dataset with given batch size and number of batches. \"\"\"\r\n\r\n    def _generator():\r\n        for i in range(num_batches):\r\n            data = np.random.random((batch_size, 32))\r\n            labels = np.random.random((batch_size, 10))\r\n\r\n            yield data, labels\r\n            \r\n    dataset = tf.data.Dataset.from_generator(\r\n        _generator,\r\n        (tf.float32, tf.float32),\r\n        ((batch_size, 32), (batch_size, 10))\r\n    )\r\n    \r\n    return dataset\r\n\r\n# Create datasets\r\ndataset = get_dataset(batch_size=32, num_batches=1024)\r\nval_dataset = get_dataset(batch_size=32, num_batches=128)\r\n\r\n# Train\r\nmodel.fit(dataset, epochs=10,\r\n          validation_data=val_dataset)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe problem still occurs if `steps_per_epoch` is specified in the fit method. If `validation_steps` is provided, then the behaviour is correct. \r\n\r\nSeems like the computation of number of elements in the validation dataset somehow overrides the computed number from the training dataset.", "comments": ["I have tried on colab with TF version 2.0. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/54fe6ec0fbb9a278f553acd31491b977/untitled395.ipynb). Is this the expected behavior?. Thanks!", "Yeah this is the current behaviour, but not what should be happening. When you run the first example, you can see that first the dataset size is unknown (as it should be). After the first epoch is completed (1024 steps), evaluation is performed (128 steps). \r\n\r\nThen for other epochs the size of the evaluation dataset (128) is incorrectly used instead of the the training dataset (1024) as the total counter in the progress bar (it reads x/128). Only after the epoch si complete the progress bar corrects to 1024/1024. That's why after the training is done you can't really see anything wrong with it.\r\n\r\nThis is better shown in the second example, where you used 2 steps for training. Then it incorrectly shows 2/128 for every epoch instead of 2/2.\r\n\r\nWhen you use `validation_step` like you did in the third example, then the behaviour is correct.", "I observe the same behaviour when using `tf.data.Dataset.from_generator`. However it seems to be fixed in the nighlty build. Try replacing the first line in the notebook with:\r\n\r\n```!pip install tf-nightly```", "Can confirm. Using the nightly build (2.1.0-dev20191122) I get the expected behaviour.", "@lojzezust \r\nThis issue is resolved with recent nightly builds. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/71f7ae7f777f7a21b0b91a6c8a105aaa/untitled408.ipynb).Please, let us know we can close this issue?. Thanks!", "Yeah, the issue is resoved in the nightly builds. Thanks. I will close the issue.", "Unfortunately it is not possible for me to use a nightly build. Does anyone know whether this issue is only a display problem or if the datasets really get mixed up during training? I can live with the wrong progress bar but the datasets getting mixed up would be a problem.\r\n\r\nExact package versions:\r\n\r\npython=3.7.5\r\ncudatoolkit=10.0.130=0\r\ncudnn=7.6.4=cuda10.0_0\r\nkeras-applications=1.0.8=py_0\r\nkeras-preprocessing=1.1.0=py_1\r\ntensorboard=2.0.0=pyhb38c66f_1\r\ntensorflow=2.0.0=gpu_py37h57d29ca_0\r\ntensorflow-base=2.0.0=gpu_py37h390e234_0\r\ntensorflow-estimator=2.0.0=pyh2649769_0\r\ntensorflow-gpu=2.0.0=h0d30ee6_0\r\ntensorflow-probability=0.8.0=py_0"]}, {"number": 34456, "title": "AsyncResult hangs in unexpected cases in fit_generator", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.0.0b1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: V10.0.130\r\n- GPU model and memory: Quadro P5000 (16GB)\r\n\r\n**Describe the current behavior**\r\nI have a very complicated model solving an image-to-image problem. I also use a custom callback which at some point generates some noise using `numpy`.\r\nWhen I use `fit_generator` on this model, it manages to do the first epoch, then on the second, third or fourth it hangs at the beginning of the epoch. I managed to see where the problem was happening, and it happens here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/data_utils.py#L875\r\nBasically, if I put a timeout on the second `get` it times out after a few successful epochs (sometimes just one). There is no error thrown out so I don't know why it hangs. Furthermore, if I debug at that point in code, I can just execute the function synchronously and everything will work just fine.\r\n\r\n**Describe the expected behavior**\r\nI would like `fit_generator` to complete even when I use my custom callback.\r\n\r\n**Code to reproduce the issue**\r\nI didn't manage to get a minimal example using `fit_generator` (basically it relies too much on me using my model which is complex). However, I have a minimal example which reproduces the bug when I mimic the [`model_iteration`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_generator.py#L41) function.\r\nYou need to install the following to make it work: `pip install tensorflow-gpu==2.0.0b1 numpy tqdm`\r\n```python\r\n# imports\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import callbacks as cbks\r\nfrom tensorflow.keras.callbacks import Callback\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.engine import training_utils\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.python.keras.utils import data_utils\r\nfrom tensorflow.python.keras.utils import generic_utils\r\nfrom tqdm import tqdm_notebook\r\n\r\n# helper function (taken from https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_generator.py#L500)\r\ndef _make_enqueued_generator(generator,\r\n                             workers=1,\r\n                              use_multiprocessing=False,\r\n                             max_queue_size=10,\r\n                             shuffle=False):    \r\n    enqueuer = data_utils.OrderedEnqueuer(\r\n        generator, use_multiprocessing=use_multiprocessing, shuffle=shuffle)\r\n    enqueuer.start(workers=workers, max_queue_size=max_queue_size)\r\n    output_generator = enqueuer.get()\r\n    return output_generator, enqueuer\r\n\r\n# My silly callback\r\nclass Noise(Callback):\r\n     def on_batch_end(self, batch, logs={}):\r\n        image_shape = [1, 2**7, 2**7, 1]\r\n        noise = np.random.normal(scale=1.0, size=image_shape)\r\n\r\n# My data\r\nbatch_size = 8\r\nn_samples_train = 720\r\nx = np.random.rand(n_samples_train, 256, 256, 1)\r\nim_gen_train = ImageDataGenerator().flow(x, batch_size=batch_size)\r\n\r\n\r\n# My training set up (to mimic https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/training_generator.py#L41)\r\ndata = im_gen_train\r\nsteps_per_epoch = int(n_samples_train / batch_size)\r\nepochs = 20\r\nmax_queue_size=35\r\nworkers=35\r\nuse_multiprocessing=True\r\nshuffle=False\r\ninitial_epoch=0\r\nmode=1\r\nsteps_name='steps'\r\nnoise_cb = Noise()\r\nnoise_cb.on_train_batch_end = noise_cb.on_batch_end\r\ncallbacks=[noise_cb]\r\n\r\ngenerator, enqueuer = _make_enqueued_generator(\r\n    im_gen_train,\r\n    workers=workers,\r\n    use_multiprocessing=use_multiprocessing,\r\n    max_queue_size=max_queue_size,\r\n    shuffle=shuffle)\r\n\r\ncallbacks = cbks.configure_callbacks(\r\n    callbacks,\r\n    Model(),\r\n    do_validation=False,\r\n    epochs=epochs,\r\n    steps_per_epoch=steps_per_epoch,\r\n    batch_size=batch_size,\r\n    samples=n_samples_train,\r\n    verbose=0,  # Handle ProgBar as part of Callbacks once hooks are ready.\r\n    mode=mode,\r\n)\r\ncallbacks._call_begin_hook(mode)\r\n\r\nfor epoch in tqdm_notebook(range(initial_epoch, epochs)):\r\n    callbacks.on_epoch_begin(epoch, {})\r\n\r\n    for step in tqdm_notebook(range(steps_per_epoch), leave=False):\r\n        callbacks._call_batch_hook('train', 'begin', step, {})\r\n        batch_data = next(generator)\r\n        \r\n        # I don't actually train a model, so I just sleep for this time, this would be the backprop\r\n        time.sleep(0.1)\r\n        callbacks._call_batch_hook('train', 'end', step, {})\r\n```\r\n\r\nIf you leave it as such, it will hang after about 1, 2, 3, or 4 iterations.\r\nYou can comment out the `noise = np.random.normal(scale=1.0, size=image_shape)` line and see that it doesn't hang.\r\n\r\nYou can also modify tensorflow's source code and timeout [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/data_utils.py#L875) in the second `get` so you can debug.\r\n\r\nNote also that if the sleeping time is not high enough, hanging doesn't appear.\r\n\r\nI am still working on a minimal example involving `fit_generator` directly, but to me this example is enough to understand what's happening.\r\n", "comments": ["I finally managed to create an example involving `fit_generator`:\r\n\r\n```python\r\n# imports\r\nimport time\r\n\r\nfrom keras_tqdm import TQDMNotebookCallback\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.callbacks import Callback\r\nfrom tensorflow.keras.layers import Input, Conv2D, Lambda, concatenate\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.utils import  Sequence\r\n\r\n\r\n# My silly callback\r\nclass Noise(Callback):\r\n     def on_batch_end(self, batch, logs={}):\r\n        image_shape = [1, 2**7, 2**7, 1]\r\n        noise = np.random.normal(scale=1.0, size=image_shape)\r\n        \r\n# my metrics\r\ndef keras_psnr(y_true, y_pred):\r\n    max_pixel = tf.math.reduce_max(y_true)\r\n    min_pixel = tf.math.reduce_min(y_true)\r\n    return tf.image.psnr(y_true, y_pred, max_pixel - min_pixel)\r\n\r\ndef keras_ssim(y_true, y_pred):\r\n    max_pixel = tf.math.reduce_max(y_true)\r\n    min_pixel = tf.math.reduce_min(y_true)\r\n    return tf.image.ssim(y_true, y_pred, max_pixel - min_pixel)\r\n\r\n# My data\r\nclass MergedGenerators(Sequence):\r\n    def __init__(self, *generators):\r\n        self.generators = generators\r\n        # TODO add a check to verify that all generators have the same length\r\n\r\n    def __len__(self):\r\n        return len(self.generators[0])\r\n\r\n    def __getitem__(self, index):\r\n        return tuple([generator[index] for generator in self.generators])\r\n\r\nbatch_size = 8\r\nn_samples_train = 720\r\nsize = 256\r\nx = np.random.rand(n_samples_train, size, size, 1)\r\nim_gen_train_1 = ImageDataGenerator().flow(x, batch_size=batch_size, seed=0)\r\nim_gen_train_2 = ImageDataGenerator().flow(x, batch_size=batch_size, seed=0)\r\nim_gen_train = MergedGenerators(im_gen_train_1, im_gen_train_2)\r\n\r\n# my fake model\r\nim = Input((None, None, 1))\r\nconv = Conv2D(256, 3, padding='same')(im)\r\nconv = Conv2D(256, 3, padding='same')(conv)\r\nconv = Conv2D(1, 3, padding='same')(conv)\r\nident = Lambda(lambda x: x)(conv)\r\nmodel = Model(im, ident)\r\nmodel.compile(loss='mse', optimizer='adam', metrics=[keras_psnr, keras_ssim])\r\nprint(model.summary(line_length=150))\r\n\r\n# My training set up\r\nnoise_cb = Noise()\r\nnoise_cb.on_train_batch_end = noise_cb.on_batch_end\r\ntqdm_cb = TQDMNotebookCallback(metric_format=\"{name}: {value:e}\")\r\ntqdm_cb.on_train_batch_begin = tqdm_cb.on_batch_begin\r\ntqdm_cb.on_train_batch_end = tqdm_cb.on_batch_end\r\nmodel.fit_generator(\r\n    im_gen_train,\r\n    steps_per_epoch=int(n_samples_train / batch_size), \r\n    epochs=20,\r\n    max_queue_size=35,\r\n    workers=35,\r\n    use_multiprocessing=True,\r\n    shuffle=False,\r\n    callbacks=[noise_cb, tqdm_cb],\r\n    verbose=0,\r\n)\r\n```\r\n\r\nIt's not bare yet, but at least it reproduces the error, and you just need to install `keras-tqdm` to get it working (= hanging).\r\n\r\nNote that I needed the lambda layer and the metrics to get it to hang.\r\n\r\nEDIT\r\n-----\r\nI also posted all of this issue on [SO](https://stackoverflow.com/questions/58957519/asyncresult-hangs-in-unexpected-cases-in-fit-generator-of-tensorflows-keras). ", "Maybe related: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/data_utils.py#L522", "I have tried on colab with TF version 2.0 beta,2.0,2.1.0-dev20191121 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/3b2d9e8d159ca9ca38870cd2650b10ae/untitled396.ipynb).Thanks!", "Thanks to discussions with [@tomMoral](https://github.com/tomMoral), we now think that this is actually caused by https://github.com/numpy/numpy/issues/9248. At least, the lock is coming from `numpy`'s random module.", "I think the issue is resolved in version 2.1.\r\n\r\nAnother fix would be to use the [new random number generation API of `numpy`](https://docs.scipy.org/doc/numpy/reference/random/generator.html) as advised [here](https://github.com/numpy/numpy/issues/9248#issuecomment-522340824). That changes the line `noise = np.random.normal(scale=1.0, size=image_shape)` to `noise = np.random.default_rng().normal(scale=1.0, size=image_shape)`. This fix works even in version 2.0.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34456\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34456\">No</a>\n"]}, {"number": 34455, "title": "tf.ragged.stack breaks with rank-1 regular tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary pip \r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version: CUDA 10.1 cuDNN 7.6.2.24\r\n- GPU model and memory: Quadro P2000\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a ragged tensor by using `tf.ragged.stack` on several regular tensors on the 0-th axis, the function crashes when the rank of the input tensors is 1.\r\n\r\n**Describe the expected behavior**\r\n\r\nAccording to the [documentation](https://www.tensorflow.org/api_docs/python/tf/ragged/stack): `Given a list of tensors or ragged tensors with the same rank R (R >= axis) [...]`. Here, R=1 and  axis=0, so the preconditions should be fulfilled.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\ntf.ragged.stack([[1, 2, 3], [1, 2]], axis=0)\r\n```\r\n\r\n**Other info / logs**\r\nCalling the 2 lines above results in:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-14-488f6e6430bd>\", line 1, in <module>\r\n    tf.ragged.stack([[1, 2, 3], [1, 2]], axis=0)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/ops/ragged/ragged_concat_ops.py\", line 113, in stack\r\n    return _ragged_stack_concat_helper(values, axis, stack_values=True)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/ops/ragged/ragged_concat_ops.py\", line 167, in _ragged_stack_concat_helper\r\n    return array_ops.stack(rt_inputs, axis)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\", line 1154, in stack\r\n    return ops.convert_to_tensor(values, name=name)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\", line 1278, in _autopacking_conversion_function\r\n    return _autopacking_helper(v, dtype, name or \"packed\")\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\", line 1184, in _autopacking_helper\r\n    return gen_array_ops.pack(list_or_tuple, name=name)\r\n  File \"/home/veith/Projects/venvs/anontf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6293, in pack\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [3] != values[1].shape = [2] [Op:Pack] name: stack\r\n```\r\n\r\n**Workaround**\r\n\r\nExpanding the rank-1 tensors to rank-2 tensors followed by squeezing the redundant dimension seems to work:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.ragged.stack([[[1, 2, 3]], [[1, 2]]], axis=0)\r\nprint(x.bounding_shape())  # <tf.Tensor: id=929, shape=(3,), dtype=int64, numpy=array([2, 1, 3])>\r\nx = tf.squeeze(x, axis=1) \r\nprint(x.bounding_shape())  # <tf.Tensor: id=1109, shape=(2,), dtype=int64, numpy=array([2, 3])>\r\n```\r\n\r\n**Estimated cause**\r\n\r\nI did not check the tensorflow source code for this, but the [general ragged documentation](https://www.tensorflow.org/guide/ragged_tensor#ragged_tensors_definitions) states:\r\n![image](https://user-images.githubusercontent.com/12949211/69247798-544f0780-0bab-11ea-8ff0-3f6626a8e2f8.png)\r\nI assume the issue is that there is no uniform dimension in the regular tensors before stacking them. Intuitively I would have assumed that I can use `tf.ragged.stack` to create this uniform dimension to form a ragged tensor from several non-ragged different-dimension tensors as above, similar to the regulat `tf.stack`, which creates a new dimension.\r\nI am not sure whether this is considered a bug or an error in the documentation of `tf.ragged.stack`, but it feels like a bug from a user perspective.", "comments": ["This is not a bug @RunOrVeith As the uniform dimension is missing before the ragged dimensions, its throwing an error. Thanks!", "Yes, I assumed that it is not a bug, but rather more unexpected behavior/missing documentation.\r\nMaybe this could also be labeled as a feature request for `tf.ragged.stack`.\r\nBut it's not super urgent as the workaround with adding a dimension and then squeezing it works well enough.", "The code snippet works with TF 2.4.1\r\nSee [gist](https://colab.research.google.com/gist/ymodak/be4a1037e6b00dff8f159640d6613c2c/github_issue34455.ipynb) for reference. Thanks!"]}, {"number": 34454, "title": "Layer model is not connected, no input to return.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos mohave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: Python 3.7.4\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nRaises exception.\r\nWhen loading saved model through the tf.keras.Model we get (tf=2.0.0):\r\n    \r\n    AttributeError: Layer model is not connected, no input to return.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should pass. It pass when saving and loading model from .h5 format.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (224, 224, 3)\r\n\r\n# functional model\r\nbase_model2 = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape)\r\ninputs = tf.keras.Input(shape=shape, name=\"input\")\r\nx = base_model2(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dense(256, activation=\"relu\", name=\"embeddings\")(x)\r\noutputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\")(x)\r\nmodel2 = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n\r\ntf.keras.models.save_model(model2, \"model\")\r\nmodel_l2 = tf.keras.models.load_model(\"model\")\r\n\r\n# this raises exception\r\nmodel_loaded = tf.keras.Model(\r\n    inputs=model_l2.input, outputs=[model_l2.get_layer(layer_name).output for layer_name in [\"probs\", \"embeddings\"]]\r\n)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n2019-11-20 15:24:41.862516: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-20 15:24:41.873895: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa773aa38b0 executing computations on platform Host. Devices:\r\n2019-11-20 15:24:41.873913: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-20 15:24:55.102446: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /Users/userx/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTraceback (most recent call last):\r\n  File \"save2.py\", line 20, in <module>\r\n    inputs=model_l2.input, outputs=[model_l2.get_layer(layer_name).output for layer_name in [\"probs\", \"embeddings\"]]\r\n  File \"/Users/userx/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1557, in input\r\n    ' is not connected, no input to return.')\r\nAttributeError: Layer model is not connected, no input to return.\r\n```", "comments": ["Issue replicating for given code in TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/edd8e140df86794bc17324b0b7cb2333/34454.ipynb) of colab.Thanks!", "I am not sure if this is working in 2.1rc, because right now there is another bug #33870 which should be solved before.", "@k-w-w Any update on this?\r\n", "> @k-w-w Any update on this?\r\n\r\nsave your model in keras .h5 format instead of tensorflow checkpoint format seems to work. ", "Tested again, this issue can be solved by using .h5 format to save the model. It looks like checkpoint format isn't integrated with Keras well and some information is missing in the checkpoint format. ", "It's working in tf-nightly-2.2...!  Thank you @k-w-w .\r\n\r\nYou can close this!", "yes this issue is fixed with latest tf-nightly. please see the updated [gist](https://colab.sandbox.google.com/gist/goldiegadde/9c8c6b3e324ca6fbe16e8e3c446fb9c0/34454.ipynb)\r\n\r\nUpdating your code to below should work.\r\nmodel_loaded = tf.keras.Model(\r\n    inputs=model_l2.input, outputs=[model_l2.get_layer(layer_name).output for layer_name in [\"probs\", \"embeddings\"]]\r\n)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34454\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34454\">No</a>\n", "I installed the latest tf-nightly and I still have this issue. Here is my code that returns the error\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nimport tensorflow.keras as keras\r\n\r\nmodel3 = Sequential()\r\nmodel3.add(keras.layers.Dense(100))\r\nmodel3.add(keras.layers.Dense(1))\r\nmodel3.layers[1].input\r\n```", "@guillefix  Since you are using Sequential API you may want to specify `input_shape` argument in your first layer.\r\n```python\r\nmodel3.add(keras.layers.Dense(100,input_shape=(500,)))\r\n```\r\n If want to avoid setting `input_shape` then other option will be to build the first by `model.compile` and `model.fit`", "Not sure if it is precisely the same issue, but I got around my specific problem by overriding the `get_config` method in my custom layer, taking care to serialize the initializer arguments in particular"]}]