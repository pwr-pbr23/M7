[{"number": 23053, "title": "Autograph example crashes", "body": "The autograph example from the documentation crashes.\r\nhttps://www.tensorflow.org/guide/autograph#automatically_convert_python_control_flow\r\n\r\nI'm running in docker. The same issues occur with tag latest and nightly.\r\n\r\nCode:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import autograph\r\n\r\n@autograph.convert()\r\ndef square_if_positive(x):\r\n  if x > 0:\r\n    x = x * x\r\n  else:\r\n    x = 0.0\r\n  return x\r\n\r\nprint(square_if_positive(tf.constant(10)))\r\n```\r\n\r\nExecution:\r\n```bash\r\nroot@ab48c4411db3:/notebooks# python tmp.py\r\nTraceback (most recent call last):\r\n  File \"tmp.py\", line 12, in <module>\r\n    print(square_if_positive(tf.constant(10)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/impl/api.py\", line 68, in wrapper\r\n    return converted_call(f, recursive, verbose, True, {}, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/impl/api.py\", line 211, in converted_call\r\n    return converted_f(*effective_args, **kwargs)\r\n  File \"/tmp/tmpaCFtJC.py\", line 21, in square_if_positive\r\n    ag__.rewrite_graph_construction_error(ag_source_map__)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/core/errors.py\", line 137, in rewrite_graph_construction_error\r\n    raise new_error\r\ntensorflow.contrib.autograph.core.errors.GraphConstructionError: Traceback (most recent call last):\r\n  File \"tmp.py\", line 7, in square_if_positive\r\n    if x > 0:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/autograph/utils/multiple_dispatch.py\", line 50, in run_cond\r\n    return control_flow_ops.cond(condition, true_fn, false_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2126, in cond\r\n    (val_x.dtype.name, val_y.dtype.name))\r\n\r\nOutputs of true_fn and false_fn must have the same type: int32, float32\r\n```", "comments": []}, {"number": 23052, "title": "Tensorflow not Compatible with Python 3.7 ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS 10.13.4\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source (whole package)\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\n\r\n- **Exact command to reproduce**: \r\n\r\nrun the following below:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow \r\n\r\nfrom tensorflow import keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Embedding\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(5, 2, input_length=5))\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nSo it appears Tensorflow doesn't play nicely with Python 3.7 - there is a source issue when I run ANY tensorflow or keras command using python 3.7 (but runs perfectly fine with earlier versions of python - reverted back and everything ran perfectly fine). \r\n\r\nHere are all the relevant files with environment (it actually errors out when running in p3.7) \r\n\r\nfiles using 3.7:\r\n\r\n[broken-old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488938/broken-old-tf_env.txt)\r\n[broken-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488939/broken-tf_env.txt)\r\n\r\nnormal env:\r\n\r\n[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488954/old-tf_env.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2488958/tf_env.txt)\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nHere is the trace error message - this exact message occurs when attempting to run ANY keras or tf command using python 3.7:\r\n\r\n(for test case, honestly even the most simple lines will cause the break - try;\r\n\r\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\r\n\r\nOr for keras:\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(10000, 50, input_length=3334))\r\n\r\n\r\nError message:\r\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-2-e17e94177b7f> in <module>()\r\n      6 embedding_size = 100\r\n      7 \r\n----> 8 embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\r\n      9 nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\r\n     10 nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\r\n\r\n~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)\r\n    233   with ops.name_scope(name, \"random_uniform\", [shape, minval, maxval]) as name:\r\n    234     shape = _ShapeTensor(shape)\r\n--> 235     minval = ops.convert_to_tensor(minval, dtype=dtype, name=\"min\")\r\n    236     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=\"max\")\r\n    237     seed1, seed2 = random_seed.get_seed(seed)\r\n\r\n~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    996       name=name,\r\n    997       preferred_dtype=preferred_dtype,\r\n--> 998       as_ref=False)\r\n    999 \r\n   1000 \r\n\r\n~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1092 \r\n   1093     if ret is None:\r\n-> 1094       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1095 \r\n   1096     if ret is NotImplemented:\r\n\r\n~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    215                                          as_ref=False):\r\n    216   _ = as_ref\r\n--> 217   return constant(v, dtype=dtype, name=name)\r\n    218 \r\n    219 \r\n\r\n~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    194   tensor_value.tensor.CopyFrom(\r\n    195       tensor_util.make_tensor_proto(\r\n--> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    198   const_tensor = g.create_op(\r\n\r\n~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    534     raise TypeError(\r\n    535         \"Element type not supported in TensorProto: %s\" % numpy_dtype.name)\r\n--> 536   append_fn(tensor_proto, proto_values)\r\n    537 \r\n    538   return tensor_proto\r\n\r\ntensorflow/python/framework/fast_tensor_util.pyx in tensorflow.python.framework.fast_tensor_util.AppendFloat32ArrayToTensorProto()\r\n\r\n~/.local/share/virtualenvs/embedded-feature-extraction-o8pNKOHv/lib/python3.7/site-packages/google/protobuf/internal/containers.py in append(***failed resolving arguments***)\r\n    249   def append(self, value):\r\n    250     \"\"\"Appends an item to the list. Similar to list.append().\"\"\"\r\n--> 251     self._values.append(self._type_checker.CheckValue(value))\r\n    252     if not self._message_listener.dirty:\r\n    253       self._message_listener.Modified()\r\n\r\nUnboundLocalError: local variable 'self' referenced before assignment\r\n", "comments": ["@ivaylogb  -  Hi, currently Tensorflow tested build configurations Python versions are **2.7, 3.3-3.6.** We do not say that Tensorflow is not compatible with Python 3.7 but the above python versions are the tested ones.\r\nYou can also find this information [here](https://www.tensorflow.org/install/source#tested_build_configurations). Please take a look if you haven't already.\r\n", "Tensorflow __is not__ compatible with Python 3.7 for now because it depends on a version of protobuf which does not support Python 3.7.\r\nprotobuf does not have a stable release that supports Python 3.7.\r\nThere are already many duplicated issues about 3.7 support: e.g. https://github.com/tensorflow/tensorflow/issues/20444#issuecomment-428332353, https://github.com/tensorflow/tensorflow/issues/20950", "Closing this issue. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@ppwwyyxx \r\n\r\nprotobuf now support python 3.7 and still I can't compile tensorflow with python 3.7"]}, {"number": 23051, "title": "[Experiment] Reformat issue template. <DO NOT MERGE>", "body": "", "comments": ["Nagging Assignee @yifeif: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23050, "title": "Batch Normalization with virtual_batch_size not equal to None not implemented correctly for inference time", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n- **Python version**: Python 3.6.2\r\n- **Exact command to reproduce**: python main.py\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Mobile device**: N/A\r\n- **Bazel version**: N/A\r\n\r\n### Describe the problem\r\nThe batch normalization implementation respects the virtual_batch_size parameter in both train and inference modes. As such you are unable to do inference with batch sizes that are not multiples of the virtual_batch_size. Algorithm 1 in the [ghost batch norm paper](https://arxiv.org/pdf/1705.08741.pdf) makes it clear that virtual_batch_size should only be respected in train mode.\r\n\r\nRelevant tf source code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nvirtual_batch_size = 32\r\n\r\ntraining = tf.placeholder(tf.bool, shape=[])\r\nx = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\r\nx_norm = tf.layers.batch_normalization(x, training=training, virtual_batch_size=virtual_batch_size)\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\r\nsess = tf.Session()\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\n\r\nfor _ in range(10):\r\n    sess.run([update_ops, x_norm], feed_dict={x : np.random.random(size=[4*virtual_batch_size, 28, 28, 1]), training : True})\r\n\r\nsess.run(x_norm, feed_dict={x : np.random.random(size=[virtual_batch_size, 28, 28, 1]), training : False})\r\n\r\n# fails after this line\r\nsess.run(x_norm, feed_dict={x : np.random.random(size=[1, 28, 28, 1]), training : False})    \r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "Added ", "In the above code snippet you are performing Ghost Batch Normalization since virtual_batch_size is not None. This creates virtual sub-batches that are normalized separately with shared gamma, beta, and moving average. \r\n\r\nThe main benefit of acquiring the statistics on small virtual batches instead of the real large batch is to reduce the generalization error. It is noted in the paper the full batch statistics need to be used for inference. This is the reason that you need to use multiples of virtual_batch_size for inference.\r\n\r\n", "Please look at the test phase of algorithm 1 in the [ghost norm paper](https://arxiv.org/pdf/1705.08741.pdf). The test phase requires the learned gamma and beta, whose shape are independent of the batch size. The moving statistics mu_run, and sigma_run are calculated \"on the side\" from the full batches during train time, and are otherwise not used during train time. Their size/shape is independent of the batch size. Only mu_run, sigma_run, gamma, and beta are used at test/inference phase, and are not dependent on the test/inference training examples. Test/inference should be able to happen for any batch_size.\r\n\r\nYour suggestion implies that test inputs would not be treated independently, i.e. that it matters what they are batched with, which is not fundamentally sound. ", "Good that you dig deeper in the paper, guess I was not explicit in my explanation. \r\n\r\n> The test phase requires the learned gamma and beta, whose shape are independent of the batch size.\r\n\r\nFor normalization at inference time population mean and variance are used which were calculated during training using moving mean and variance. They are averaged over virtual batches. From Algorithm 1 in Hoffer's paper, I wouldn't say that alpha and gamma are independent of batch size. You can also find more details on Algorithm 1&2 in the paper from [Ioffe & Szegedy](https://arxiv.org/pdf/1502.03167.pdf).\r\n\r\n> Test/inference should be able to happen for any batch_size.\r\n\r\nThis is true in general, but your last sess.run call is made on x_norm, i.e. tf.layers.batch_normalization of x during training where virtual_batch_size is used. ", "> but your last sess.run call is made on x_norm, i.e. tf.layers.batch_normalization of x during training where virtual_batch_size is used.\r\n\r\nThe last `sess.run` call has `training=False`.", "@wt-huang I'm specifically saying their **shape** is independent of the batch size - of course their actual values depend on the batch size during training.\r\n\r\n@ppwwyyxx +1, Exactly I'm claiming that virtual_batch_size is (or ought to be in this implementation) irrelevant when `training=False`", "@ccurro  gamma and beta are parameters.\r\n\r\n> > but your last sess.run call is made on x_norm, i.e. tf.layers.batch_normalization of x during training where virtual_batch_size is used.\r\n> The last sess.run call has training=False.\r\n\r\nYes, but if you change virtual_batch in x_norm as below, sess.run would not incur error.\r\n```\r\nx_norm = tf.layers.batch_normalization(x, training=training, virtual_batch_size=1)\r\n```", "I am aware that gamma and beta are parameters - nothing I have said indicates otherwise. \r\n\r\nYour statement is trivial and reductive. Obviously if I make `virtual_batch_size=1` then yes, at inference time, as the algorithm is currently implemented, I will not encounter a runtime error - that does not however make any statements about the correctness of the implementation. Further, `virtual_batch_size=1` is more than likely completely useless, since the variance for any virtual batch would be zero. \r\n\r\nYour statement does nothing to refute the utility, fundamental soundness, and clear expositional foundation (from the original paper) of training with a given batch size and evaluating with another, possibly singleton, batch size. \r\n\r\nForward batch statistics are not computed during inference. There should be no requirement for an inference batch. All batch statistics are computed during training which are then used at inference time. \r\n\r\nIf you will not change your position, please provide a sound argument or reassign this issue to another Googler/contributor. ", "@ccurro Just run into the same problem and I completely agree with you that virtual_batch_size should only be respected in train mode. By now have you found any solutions to this yet?", "No solutions found", "It seems a long time has passed and the issue is still there (with TF2.0), any plan? ", "@jhseu, can you handle this? I am unfamiliar with ghost batch norm.", "@jhseu Is there any update regarding this issue?\r\nThanks in advance!", "@ccurro @hckiang @Xiadalei @jhseu \r\nI think I solved the bug, would appreciate you confirming.\r\nI think if you replace in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization.py#L712), the line\r\n`expanded_shape = [self.virtual_batch_size, -1] + original_shape[1:]`\r\nwith\r\n`expanded_shape = control_flow_ops.cond(training,lambda:[self.virtual_batch_size, -1] + original_shape[1:], lambda:[1, -1] + original_shape[1:])`\r\nIt will behave as expected\r\n\r\n(Need to add `from tensorflow.python.ops import control_flow_ops`).", "@netanel-s \r\n\r\nAt a glance, your conditional looks like it would produce the correct behavior. Happy testing :smile: \r\n\r\nThanks for checking it out :slightly_smiling_face: ", "In general, can we make the behaviour of this option a bit more forgiving for uneven bactch sizes?\r\nMaybe the following behaviour is more in accordance with the user's expectations:\r\n\r\nIn  **training** mode:\r\n- If the batch size is a multiple of virtual_batch_size -> use virtual batch size.\r\n- If the batch size is not a multiple -> use the whole batch size and raise a warning the first time this occurs.\r\n\r\nIn **testing** mode: No reshaping is needed as the running statistics are used.\r\n\r\nCurrently, tf raises an exception instead of a warning.\r\nThis change would allow users to use the model.fit() calls without worrying about the last batch being the right size or defaulting to a custom training loop.\r\n\r\n\r\n", "Is there any update on this? I attempted the fix from @netanel-s on TF2.3.1 but it hasn't fixed the issue for me. It's clear that virtual_batch_size should have no effect on tensor shapes for either training or inference..", "I'm marking as contributions welcome because no one currently has the bandwidth to implement this. If someone creates a PR for this fix, please CC me.", "Just wanted to say once more that @ccurro 's points are all entirely valid 3 years later.", "In the meantime, as a worksround, I implemented a GhostBatchNorm class in Ludwig that user BatchNorm internally. Posting it here as it may be useful: https://github.com/ludwig-ai/ludwig/blob/master/ludwig/modules/normalization_modules.py#L6-L38", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}, {"number": 23049, "title": "Ghost Batch Normalization performance", "body": "GBN seems to be at least twice as slow as regular BN. This is the case even when using the full batch for normalization in GBN mode (`virtual_batch_size=batch_size`).\r\n\r\nYou can reproduce that with tensorflow/models cifar10 example by running the script below, also available at (https://gist.github.com/MustafaMustafa/a12485746e4c877620a818d227982e1c):\r\n\r\n```bash\r\n\r\ngit clone git@github.com:tensorflow/models.git slow_GBN_issue\r\ncd slow_GBN_issue/tutorials/image/cifar10_estimator\r\n\r\n# use tf.layer.batch_normalization\r\nsed -i  's/tf.contrib.layers.batch_norm/tf.layers.batch_normalization/' model_base.py\r\nsed -i  's/decay=/momentum=/' model_base.py\r\nsed -i  's/is_training=/training=/' model_base.py\r\nsed -i  '/data_format=data_format)/c\\axis=-1 if data_format==\"NHWC\" else 1)' model_base.py\r\n\r\n# make val dataset a multiple of batchsize (multiple of 128)\r\nsed -i  's/return 1000/return 9984/' cifar10.py\r\n\r\npython generate_cifar10_tfrecords.py --data-dir=${PWD}/cifar-10-data\r\nif [ -d ./cifar10_logs ]\r\nthen\r\n  rm -rf ./cifar10_logs\r\nfi\r\npython3 cifar10_main.py --data-dir=${PWD}/cifar-10-data --job-dir=./cifar10_logs --num-gpus=1 --eval-batch-size 128 --train-steps=500 2>&1 | tee bn.log\r\n\r\n# use virtual_batch_size\r\nsed -i '/fused/a virtual_batch_size=128,' model_base.py\r\nif [ -d ./cifar10_logs ]\r\nthen\r\n  rm -rf ./cifar10_logs\r\nfi\r\npython3 cifar10_main.py --data-dir=${PWD}/cifar-10-data --job-dir=./cifar10_logs --num-gpus=1 --eval-batch-size 128 --train-steps=500 2>&1 | tee gbn.log\r\n\r\n# show samples of timing prints\r\necho \"BN:\"\r\ngrep \"sec)\" bn.log | tail -1\r\necho \"GBN:\"\r\ngrep \"sec)\" gbn.log | tail -1\r\n```\r\n\r\nOutput:\r\n```\r\nBN:\r\nINFO:tensorflow:learning_rate = 0.1, loss = 2.09511 (4.351 sec)\r\nGBN:\r\nINFO:tensorflow:learning_rate = 0.1, loss = 2.37345 (9.071 sec)\r\n```\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: v1.12.0-rc0-0-g1a6dea3 1.12.0-rc0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.1.0\r\n- **CUDA/cuDNN version**: 9.2/7.1.4\r\n- **GPU model and memory**: Titan X Pascal 12GB\r\n- **Have I written custom code**: No\r\n- **TensorFlow installed from**: Compiled from source\r\n- **Exact command to reproduce**: bash script for how to reproduce provided in the description.\r\n- **Mobile device**: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nExact command to reproduce\nMobile device", "This is still an issue in TF 1.12 as well. I am not sure if there is any reason for the entire network to be 2x slower with virtual batches. This is the case even when using `virtual batchsize = batchsize`.", "@joel-shor Can you PTAL? Thanks", "@MustafaMustafa \r\nThis is fixed in tf 2.x, could you please verify and let us know.\r\nYou may also refer to :https://github.com/ludwig-ai/ludwig/blob/master/ludwig/modules/normalization_modules.py#L6-L38", "Moving this to closed status due to lack of activity."]}, {"number": 23048, "title": "Bug in function: MutableGraphView::ReplaceInput(..) ?", "body": "------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.11\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.16\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609\r\n- **CUDA/cuDNN version**:  N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI am modifying the Tensorflow source to add a new `tensorflow::grappler::CustomGraphOptimizer` pass, and I am having a hard time of using the fuction: `MutableGraphView::ReplaceInput(..)`. I think there is a bug inside this function, (the master branch version is the same as the r1.11 version), in that the new input is not specified with the `output_port_id`:\r\n\r\n### Source code / logs\r\nThe description of the function seems pretty clear in the header file:\r\n```c++\r\n  // Replaces the input for the output nodes of 'old_input' with a port\r\n  // `output_port_id` with 'new_input'.\r\n  //\r\n  // E.g: We have 2 nodes that use 'bar' node outputs as inputs:\r\n  // foo(bar:0, bar:1),  foo2(other:0, bar:0)\r\n  // Calling ReplaceInput(bar, new, 0) changes every occurrence of bar:0 for\r\n  // new:0.  Result:\r\n  // foo(new:0, bar:1),  foo2(other:0, new:0)\r\n  void ReplaceInput(const NodeDef& old_input, const NodeDef& new_input,\r\n                    int output_port_id = 0);\r\n\r\n```\r\nHowever in the .cc code of the function body, I don't see where we set the output port index to the new_input:\r\n```c++\r\nvoid MutableGraphView::ReplaceInput(const NodeDef& old_input,\r\n                                    const NodeDef& new_input,\r\n                                    const int output_port_id) {\r\n  GraphView::OutputPort output_port =\r\n      GetOutputPort(old_input.name(), output_port_id);\r\n  auto fanout = GetFanout(output_port);\r\n  for (auto& input_port : fanout) {\r\n    // Isn't that here we should have something like: strings::StrCat(new_input.name(), \":\", output_port_id); \r\n   // instead of just put the new_input.name() below!?\r\n    input_port.node->set_input(input_port.port_id, new_input.name());  \r\n    AddFanouts(input_port.node);\r\n  }\r\n}\r\n```\r\n\r\nAlso I have a related question, if the output port indexes are different between the new and old input, how can I properly replace the input? e.g. something like\r\n```c++\r\n  // foo(bar:0, bar:1),  foo2(other:0, bar:0)\r\n  // Calling ReplaceInput(bar, 0, new, 2) changes every occurrence of bar:0 for\r\n  // new:2.  Result:\r\n  // foo(new:2, bar:1),  foo2(other:0, new:2)\r\n```\r\n\r\nThanks a lot!\r\n\r\n", "comments": ["Also I am not sure why we need the `const` keword for the `output_port_id` parm in the .cc file, it is NOT in the header file declaration.  The `int` parm is passed-by-value anyway, no!?\r\n\r\n```c++\r\nvoid MutableGraphView::ReplaceInput(const NodeDef& old_input,\r\n                                    const NodeDef& new_input,\r\n                                    const int output_port_id)  // why const here?\r\n```", "@shivaniag Hi, any update on this ?", "Below is my own `fix` for the issue and also the solution to the different index question:\r\n\r\nIn `mutable_graph_view.h`:\r\n\r\n```c++\r\nvoid ReplaceInput(const NodeDef& old_input, const NodeDef& new_input,\r\n                    int output_port_id = 0, int new_output_port_id = 0);\r\n```\r\n\r\nIn `mutable_graph_view.cc`:\r\n\r\n```c++\r\nvoid MutableGraphView::ReplaceInput(const NodeDef& old_input,\r\n                                    const NodeDef& new_input,\r\n                                    const int output_port_id,\r\n                                    int new_output_port_id) {\r\n  GraphView::OutputPort output_port =\r\n      GetOutputPort(old_input.name(), output_port_id);\r\n \r\n  auto fanout = GetFanout(output_port);\r\n  for (auto& input_port : fanout) {\r\n    VLOG(0) << \"input_port.port_id22 is: \" << input_port.port_id;\r\n    string new_input_name = strings::StrCat(new_input.name(), \":\", new_output_port_id);\r\n    input_port.node->set_input(input_port.port_id, new_input_name);\r\n    AddFanouts(input_port.node);\r\n  }\r\n}\r\n```\r\n\r\nJust for your reference.", "Hi @yehenrytian!\r\nIt seems you are using 1.x versions of Tensorflow. We recommend that you upgrade your code base to 2.x  versions as Many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23048\">No</a>\n"]}, {"number": 23047, "title": "Adam implement details about tf.train.Adamoptimizer", "body": "Hello\uff0crecent days I want to implement a Adam optimizer to compare with the tf.train.Adamoptimizer ,I use the formula mentioned in A DAM : A M ETHOD FOR S TOCHASTIC O PTIMIZATION,(the end of the section 2), but when I use the optimizer to train language model, I found the update speed of trainable variables using original Adamoptimer is more fast than mine. I just wonder the implement details about the tf.train.Adamoptimizer , is there any improvement , or any other revised manners ? If you have some advice ,please !!!!", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23046, "title": "Java API Performance & Multi-Threaded Latency", "body": "Question removed as answer was found...", "comments": []}, {"number": 23045, "title": "tf.contrib.layers.layer_norm fails in tf.estimator.train_and_evaluate() if drop_remainder==False", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux 7.5, \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: unknown\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5.4+\r\n- **Bazel version (if compiling from source)**: 0.15.2- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.1\r\n- **CUDA/cuDNN version**: V9.2.148\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n\r\n### Describe the problem\r\nI have a model using  `tf.contrib.layers.layer_norm` that runs fine throughout training but fails as soon as evaluation starts with the error displayed below (the input to  `tf.contrib.layers.layer_norm` has undefined rank). I suspect this to arise as soon as tensorflow uses a dynamic batch size during evaluation, at least when I set `drop_remainder==True` the problem disappears. The exact line of code causing the error is:\r\n`output = tf.contrib.layers.layer_norm(\r\n            tf.add(first_sublayer, pos_layer_2)\r\n        )`\r\nAny help/hints/ideas?\r\n\r\n### Source code / logs\r\n`  tf.add(first_sublayer, pos_layer_2)\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.8.0_openmpi-3.1.2/lib64/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/hpc_modules/at10.0-based/deeplearning/tensorflow/tensorflow-v1.8.0_openmpi-3.1.2/lib64/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 2135, in layer_norm\r\n    raise ValueError('Inputs %s has undefined rank.' % inputs.name)\r\nValueError: Inputs smiles_attention/Add_1:0 has undefined rank.\r\n`\r\n", "comments": ["@dopexxx Is there any chance you could provide a minimal reproducible code snippet that trigger the issue? It is very hard to debug with the limited information.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23044, "title": "Failed to allocate tensors.", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 6s\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 7.0\r\n- GPU model and memory: None\r\n- Exact command to reproduce: None\r\n\r\nI get this error - \"tensorflow/contrib/lite/kernels/conv.cc:201 filter->type != data_type (3 != 1)\r\nNode 1 failed to prepare.\"\r\n\r\nin this part of code:\r\n```\r\n    if (interpreter->AllocateTensors() != kTfLiteOk) {\r\n        NSLog(@\"Failed to allocate tensors.\");\r\n        exit(-1);\r\n    }\r\n```\r\n\r\nFor float model everything work correctly, but for quant I get this issue.\r\n\r\nHow could I Fix it?", "comments": ["@GlebGer : Can you attach the model and the conversion command.", "> @GlebGer : Can you attach the model and the conversion command.\r\nThanks for your answer!\r\n\r\n![image](https://user-images.githubusercontent.com/26429942/47206922-90091380-d392-11e8-9219-5ca28c2e8e63.png)\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/2495010/model.zip)\r\n", "@GlebGer : I was able to run model without problems, can you also paste the code before allocating tensors. Also trying running on latest TFLite if possible.\r\nAlso perhaps related #22995.", "> @GlebGer : I was able to run model without problems, can you also paste the code before allocating tensors. Also trying running on latest TFLite if possible.\r\n> Also perhaps related #22995.\r\n\r\nThanks a lot for your help!\r\nDid you run it in \"camera\" example? Did you run it on iOS?\r\n\r\nCan you please share me your code?", "@GlebGer Is it still happening, can you checkout latest (or new release branch) and retry. Since #22995 had a fix submitted.\r\n\r\nThanks ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23043, "title": "[Bug or Not?] Variable and VarHandleOp has different initial value in creating slots in ExponentialMovingAverage ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**:  master\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:  None\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nVariable and ResourceVariable has different initial value here. Is it a bug or expect action?\r\n```\r\n        # For variables: to lower communication bandwidth across devices we keep\r\n        # the moving averages on the same device as the variables. For other\r\n        # tensors, we rely on the existing device allocation mechanism.\r\n        with ops.init_scope():\r\n          if isinstance(var, variables.Variable):\r\n            avg = slot_creator.create_slot(var,\r\n                                           var.initialized_value(),\r\n                                           self.name,\r\n                                           colocate_with_primary=True)\r\n            # NOTE(mrry): We only add `tf.Variable` objects to the\r\n            # `MOVING_AVERAGE_VARIABLES` collection.\r\n            ops.add_to_collection(ops.GraphKeys.MOVING_AVERAGE_VARIABLES, var)\r\n          else:\r\n            avg = slot_creator.create_zeros_slot(\r\n                var,\r\n                self.name,\r\n                colocate_with_primary=(var.op.type in [\"Variable\",\r\n                                                       \"VariableV2\",\r\n                                                       \"VarHandleOp\"]))\r\n            if self._zero_debias:\r\n              zero_debias_true.add(avg)\r\n        self._averages[var] = avg\r\n```\r\n \r\n", "comments": ["Hi, @alextp\r\n  Sorry to bother you. This code seems have different initial value here. Is there any other consideration\uff1f\r\nThanks.", "They will have different initial values if they are randomly initialized. TF requires you to specify an op seed when creating a random initializer if you want determinism; otherwise any change in the graph structure can change the internal seed used for the initializer leading to different values.", "Sorry\uff0clet me clarify this question. I mean why using create_slot in Variable branch but using create_zero_slot in VarHandleOp branch\uff1f", "@alextp I am afraid that you mis-understand our questions. What siyu tried to point out here is that for the same Variable stuff(in the if branch it is a normal Variable op, while in the else branch it is a ResourceVariableOp), in our understanding, either Variable op or ResourceVariable op should  have the same initialization value, otherwise ,that is a inconsistency. Could please kindly take a further look at our questions before closing it?\r\n\r\nThanks", "Both ResourceVariable and RefVariable inherit from variables.Variable so they should both go through the initialized value mechanism.\r\n\r\nOnly a non-variable Tensor will trigger the zeros case.", "@alextp Actually `DistributedValues` such as `MirroredVariable` will trigger the zeros case, is that as expected?", "Why treats Variables and Non-Variables different? Are they equivalent in mathematical ?", "Variables are not a mathematical concept, so there's no mathematical justification for the difference. I don't know why this was implemented this way. I don't expect it to ever matter since it's an exponential moving average which is very independent of the initial value if you update it more than a few times.\r\n\r\nWe should probably make MirroredVariable and friends inherit from ResourceVariable not just for this, though. Please file an issue.", "I see. Thanks for your answer"]}, {"number": 23042, "title": "[Intel MKL] Update README.md links to point to 1.11 whls", "body": "", "comments": []}, {"number": 23041, "title": "[Feature Request] tf.keras expose align_corners", "body": "----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nN/A\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nN/A\r\n- **TensorFlow version (use command below)**:\r\nN/A\r\n- **Python version**:\r\nN/A\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nWe need to expose `align_corners` in this Keras upsample\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py#L1935\r\n\r\nSee also https://github.com/keras-team/keras/pull/10994#issuecomment-430305648\r\n", "comments": ["I can take a look at this, in the meantime you could always use a Lambda layer:\r\n\r\n```python\r\nLambda(\r\n  lambda x: tf.image.resize_bilinear(x, (H, W), align_corners=True), output_shape=(H, W, C))(data))\r\n```", "/cc @Shathe", "@omalleyt12 Do you want to pass trough upstream keras or directly in tf.keras?", "@ewilderj A classical opensource upstream downstream issue.\r\nWe are at risk that users and devs start to duplicate bugs against tf.keras interfaces and on the upstream Keras github repo. ", "cc @martinwicke @fchollet for visibility. This is a reasonable point, have we seen it happen much?", "@ewilderj I've saw this behavior but I've not annotated the tickets. \r\nAlso in theory general Keras API could not be modified troughs direct PR against tf.keras right? \r\nSo I suppose that it is at high risk to become a chicken or eggs problem especially for FR where newcomers start to be exposed to `tf.keras` directly instead of coming from a Keras experience and I supposed this hypothesis it is not so much remote with the landing of TF 2.0 right?.   ", "In the meantime probably could you create a specific `tf.keras` [template](https://github.com/pytorch/pytorch/tree/master/.github)?", "@dksb do you think a separate keras template makes sense to simplify triage?", "@bhack It is definitely possible to modify Keras via PR to tf.keras. We may ask for a mirror PR to keras-team/keras, but we are doing regular syncs, and contributions can move both ways.", "@martinwicke Probably this topic needs to be better investigated cause we could have a tf.keras api with only 1 backend that breaks compatibility with Keras API with its own tensorflow backend. More and more users will be exposed to Keras trough tf.keras directly. ", "@martinwicke @ewilderj This is just another bidirectional case example: https://github.com/tensorflow/tensorflow/issues/24009 VS https://github.com/keras-team/keras/pull/9898#issuecomment-380590161.\r\nCould  we have divergent opinions?\r\n"]}, {"number": 23040, "title": "Can tflite model implementation on FPGA ?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I want to quantize the ssd-mobilenet model , then implementation on FPGA\uff0c\r\nnow i use the ssd_mobilenet_v1_quantized_coco model [http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz](url)\r\n,then convert .pb file to .tflite file ,then can i use the tflite model on FPGA?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23039, "title": "Broken links to API docs", "body": "### Describe the problem\r\nThis is a docs bug.\r\nSeveral (maybe all?) links to API docs are broken.\r\neg. on the page [summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) the links to the summary operations go to https://www.tensorflow.org/api_guides/python/summary, but I presume they should point to https://www.tensorflow.org/api_docs/python/tf/summary\r\nThe same pattern happens across other pages that link to the Python API too.\r\n\r\nI wanted to submit a PR but I couldn't figure out the schema for URLs of pages in [site/en/api_docs](https://github.com/tensorflow/docs/tree/master/site/en/api_docs). \r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS High Sierra 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nn/a\r\n- **TensorFlow installed from (source or binary)**:\r\n`pip`\r\n- **TensorFlow version (use command below)**:\r\n1.10\r\n- **Python version**:\r\n3\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **GCC/Compiler version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\nhttps://www.tensorflow.org/guide/faq the link 'See also the API documentation on building graphs.'\r\nor:\r\nhttps://www.tensorflow.org/guide/summaries_and_tensorboard the links to 'summary operations'.\r\n\r\n\r\n", "comments": ["Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for the report. \r\n\r\nIt looks like the problem was on our end. Republishing this page seems to have fixed the problem.\r\n\r\nUsually anything in the python api, wrapped in back-quotes `tf.symbol` will be converted to a link when we build the site."]}, {"number": 23038, "title": "AssertAllCloseToAccordingToType()  Error", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nno\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.08\r\n- **Python version**:\r\n3.0\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\nCPU\r\n\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nComparing two results from division (Array-a / Array-b), if I use self.AssertAllEqual(a,b), it works perfectly, however, if I use self.AssertAllCloseAccordingToType(a , b), it gives me an error, saying:\r\nnot close where =  (array([], dtype=int64),)\r\nnot close lhs =  []\r\nnot close rhs =  []\r\nnot close dif =  []\r\nnot close tol =  []\r\n\r\nI figured that there is a NaN in the array, but AssertAllEqual handles the NaN perfectly. \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23037, "title": "How to save model for Tensorflow Serving while using Tensorflow distributed training?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23036, "title": "[Tflite Model]The tflite model inception_resnet_v2.tflite(2018.04.27) lacks softmax operation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI use [intel/webml-polyfill](https://github.com/intel/webml-polyfill) to parse [this model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md#image-classification-float-models) and do inference, but get the strange output result as the following:\r\n\r\norder | Label | Probability\r\n-- | -- | --\r\n1 | bee eater | 1093.54%\r\n2 | jay | 369.31%\r\n3 | brambling | 363.70%\r\n\r\nAfter add a softmax layer, the output result seems more normal:\r\n\r\norder | Label | Probability\r\n-- | -- | --\r\n1 | bee eater | 96.99%\r\n2 | jay | 0.07%\r\n3 | brambling | 0.07%\r\n\r\nSo if the model need another \"softmax\" operation after \"fullyconnect\" for the image classification task? \r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["In neural networks the last layer outputs the final activation i.e final result of our model. Therefore its meaningful to interpret this result as a probability number ranging from (0-1).This squashing of final activation into desired probability value is performed by the softmax activation function of output layer. Therefore it makes sense to add a softmax layer as an output layer to your neural network model.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23035, "title": "Completed DCGAN tutorial", "body": "I have completed the DCGAN tutorial with detailed explanations, a diagram and learning resources. Please review and approve the pull request. \r\n\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "I somehow can't get github's notebook visualizer to show me this file. It seems to be a single line of code as opposed to the many lines before your edit; do you know if there's a way to reexport the ipynb?", "\r\n@alextp, I was able to show the notebook through the github notebook visualizer although at times it doesn't seem to work. I have reexported the notebook to better reflect the code diff. Looking forward to your review and feedback.\r\n", "Thanks @alextp for your review and feedback! I made updates accordingly.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Thank you @margaretmz! I added some edits to the text (it was easier to commit the source of my Jupyter Notebook rather than copy / pasting them in, I hope I didn't clobber anything!)", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "Thank you @asimshankar for suggesting we use the Sequential model instead of model subclassing for the Generator and Discriminator classes. The current code is using  tf.contrib.eager.defun to boost performance. Can we still use defun with the Sequential model? My understanding is that defun is typically applied in the forward computation, backward computation for the gradients and applications of gradients to variables.", "Thanks @random-forests for your edits to make the tutorial more concise! I verified that the edited notebook looks good.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@margaretmz defun and sequential should work well together, so this should be fine.", "@asimshankar, thanks for the suggestion on sequential. I added a note about that in the notebook. In this case, I'd prefer to keep subclassing (even though its more verbose), just so we have more examples on tf.org in this style - in case it'll help folks modify this for other projects down the road.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "Silly CLA bot.", "@margaretmz : Yup, it should work, see https://github.com/tensorflow/tensorflow/commit/96c7397e08a8fde1bdf069b4e7dd74b5a8d79a31 :)\r\n\r\n@random-forests : I have a slight preference for using `Sequential` here since I think we do want to encourage that style, not only because of brevity, but also because it provides a bunch of other benefits in terms of serialization, introspection (e.g., iterating over the layer structure of a network) etc. Till a couple of months ago there were a bunch of pain points with `Sequential` in eager that have since been addressed. Long story short: I think using `Sequential` here will be good for users and instructive. But if you still feel strongly, I won't object.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I've made updates for your requested changes, @alextp.\r\n\r\nThanks @alextp and @asimshankar for confirming that Sequential and defun work well together. I will update the generator and discriminator models to use Sequential.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@asimshankar, @alextp and @random-forests - I updated the generator and discriminator models to use Sequential, and updated the text accordingly. I modified the generator model during the conversion and the network now generates resembling digits after only 50 epochs. Please review and provide your feedback. Thanks.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@asimshankar, @alextp and @random-forests, I have made updates per all your review feedback. Looking forward to your approval and PR merge. Thanks.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 23034, "title": "TensorFlow doesn't match  cuda9.1( libcublas 9.1 or later version) ?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@dugusiqing Hi, sorry to say that we didn't understand your question correctly and request you to draft your problem clearly. If you are looking for Tensorflow supported CUDA versions, please find below.\r\n\r\n[Linux/macOS](https://www.tensorflow.org/install/source#tested_build_configurations)\r\n[Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23033, "title": "Cannot calculate tf.gradients wrt embedding_matrix", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 8.1\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  8/6.0.21\r\n- **GPU model and memory**: Titan Xp\r\n- **Exact command to reproduce**: \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI used tensorflow to implement an end-to-end lambdaRank retrieval model. There are 3 modules: rep_module, inter_module, and L2R_module. The embeddings (emb_mat) was defined in L2R_module, and pass as params to rep_module and inter_module: So the aggregating L2R module will have:\r\n\r\nself.emb_mat = tf.get_variable(\"emb_mat\",\r\n        shape=[self.vocab_size, self.emb_dim], dtype=tf.float32)\r\nself.rep_mod = RepModule(...., emb_mat=self.emb_mat)\r\nself.inter_mod = InterModule(..., emb_mat=self.emb_mat)\r\nThe goal is to let the emb_mat shared by both rep and inter modules, and learn it jointly with those 2 modules. This L2R module will output a batch of scores: score=(Batch_size, 1)\r\n\r\nThen I have a another higher-level lambdaRank module to calculate the gradients by hand (I cannot use built-in off the self optimizer, since I have to get the grads and multiply that with the things in lambda rank). I have a _jacobian(y, x) function as follows:\r\n\r\n  def _jacobian(self, y_flat, x):\r\n    \"\"\"\r\n    https://github.com/tensorflow/tensorflow/issues/675\r\n    for ranknet and lambdarank\r\n    \"\"\"\r\n\r\n    loop_vars = [\r\n        tf.constant(0, tf.int32),\r\n        tf.TensorArray(tf.float32, size=self.batch_size),\r\n    ]\r\n\r\n    _, jacobian = tf.while_loop(\r\n        lambda j, _: j < self.batch_size,\r\n        lambda j, result: (j + 1, result.write(j, tf.gradients(y_flat[j], x))),\r\n        loop_vars)\r\n\r\n    return jacobian.stack()  \r\nwhich will calculate the grad of each element of y wrt x, and reassemble them together. I can get Jacobians of all score wrt all other variables(model parameters, if I use a fixed embedding, so emb_mat is no longer in trainable_variables()) except the emb_mat. My other variables are like tf.layers.. variables like\r\n\r\ntf.Variable 'conv1/conv1d/kernel:0' shape=(3, 300, 256) dtype=float32_ref,\r\nbut it cannot calculate the gradients wrt the emb_mat. It returned something like : TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [<tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>] and TypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>\r\nThe whole traceback is attached in logs section\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nClass L2R_Model(object):\r\n    .....\r\n    self.emb_mat = tf.get_variable(\"emb_mat\",\r\n            shape=[self.vocab_size, self.emb_dim], dtype=tf.float32,\r\n            initializer=tf.orthogonal_initializer(1.0))\r\n    self.rep_mod = RepModule(...., emb_mat=self.emb_mat)\r\n    self.inter_mod = InterModule(..., emb_mat=self.emb_mat)\r\n\r\ndef _jacobian(self, y_flat, x):\r\n        \"\"\"\r\n        https://github.com/tensorflow/tensorflow/issues/675\r\n        for ranknet and lambdarank\r\n        \"\"\"\r\n\r\n        loop_vars = [\r\n            tf.constant(0, tf.int32),\r\n            tf.TensorArray(tf.float32, size=self.batch_size),\r\n        ]\r\n\r\n        _, jacobian = tf.while_loop(\r\n            lambda j, _: j < self.batch_size,\r\n            lambda j, result: (j + 1, result.write(j, tf.gradients(y_flat[j], x))),\r\n            loop_vars)\r\n\r\n        return jacobian.stack()\r\n\r\nTraceback (most recent call last):\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 460, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 460, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 65, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got <tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main_lambda.py\", line 234, in <module>\r\n    debug()\r\n  File \"main_lambda.py\", line 230, in debug\r\n    inter_param_dict=inter_param_dict, resume=r_flag)\r\n  File \"/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py\", line 25, in __init__\r\n    self.loss, self.num_pairs, self.score, self.train_op = self._build_model()\r\n  File \"/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py\", line 215, in _build_model\r\n    grads = [self._get_derivative(score, Wk, lambda_ij) for Wk in vars]\r\n  File \"/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py\", line 215, in <listcomp>\r\n    grads = [self._get_derivative(score, Wk, lambda_ij) for Wk in vars]\r\n  File \"/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py\", line 112, in _get_derivative\r\n    dsi_dWk = self._jacobian(score, Wk)  # (BS, )\r\n  File \"/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py\", line 98, in _jacobian\r\n    loop_vars)\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2775, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2604, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2554, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/u/nieyifan/projects/L2R_RM/L2R_LambdaRank.py\", line 97, in <lambda>\r\n    lambda j, result: (j + 1, result.write(j, tf.gradients(y_flat[j], x))),\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 175, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 302, in write\r\n    value = ops.convert_to_tensor(value, name=\"value\")\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 611, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 676, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 121, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/u/nieyifan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 464, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [<tensorflow.python.framework.ops.IndexedSlices object at 0x7f025e161f28>]. Consider casting elements to a supported type.", "comments": ["@yifannieudem You can probably use `tf.gradients(loss, embeddings)` for your case which will give a `tf.IndexedSlices` object corresponding to the gradients of embeddings. You can also use `optimizer.apply_gradients` to aggregate gradients for repeating word units.\r\n\r\nManually calculation using jacobian is another way of doing it, make sure that all the types match.\r\n\r\n\r\n"]}, {"number": 23032, "title": "Multiple gpus (1080Ti) do not speed up training, test on cifar10_estimator code", "body": "I was trying to test the performance of multi GPUs version of [cifar10_estimator][1] on 2 or 3 1080Ti, but received no speed-up.\r\n\r\nI find some useful information about hardware [here][2], but still confused how to solve it.\r\n\r\nMy environment:\r\n - docker image tensorflow/tensorflow:latest-gpu-py3\r\n - Ubuntu VERSION=16.04.5 LTS\r\n - Python3\r\n - CUDA_VERSION=9.0.176\r\n - tensorflow-gpu=1.11.0\r\n\r\n\r\nGPU info:\r\n\r\n    nvidia-smi topo -m\r\n    \r\n    \tGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\r\n    GPU0\t X \tPIX\tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\t0-7\r\n    GPU1\tPIX\t X \tPHB\tPHB\tSYS\tSYS\tSYS\tSYS\t0-7\r\n    GPU2\tPHB\tPHB\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t0-7\r\n    GPU3\tPHB\tPHB\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t0-7\r\n    GPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPHB\tPHB\t8-15\r\n    GPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPHB\tPHB\t8-15\r\n    GPU6\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\t X \tPIX\t8-15\r\n    GPU7\tSYS\tSYS\tSYS\tSYS\tPHB\tPHB\tPIX\t X \t8-15\r\n\r\n    Legend:\r\n    \r\n      X    = Self\r\n      SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n      NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n      PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n      PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\r\n      PIX  = Connection traversing a single PCIe switch\r\n      NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n\r\n1 gpu bach_size=128\r\n\r\n    INFO:tensorflow:loss = 2.2576141, step = 200 (3.729 sec)\r\n    INFO:tensorflow:learning_rate = 0.1, loss = 2.2576141 (3.729 sec)\r\n    INFO:tensorflow:Average examples/sec: 2821.06 (2858.65), step = 200\r\n    INFO:tensorflow:Average examples/sec: 2847.23 (3496.06), step = 210\r\n    INFO:tensorflow:Average examples/sec: 2857.91 (3102.29), step = 220\r\n    INFO:tensorflow:Average examples/sec: 2867.04 (3083.62), step = 230\r\n    INFO:tensorflow:Average examples/sec: 2889.21 (3514.15), step = 240\r\n    INFO:tensorflow:Average examples/sec: 2913.15 (3636.28), step = 250\r\n    INFO:tensorflow:Average examples/sec: 2915.99 (2988.94), step = 260\r\n    INFO:tensorflow:Average examples/sec: 2901.94 (2578.95), step = 270\r\n    INFO:tensorflow:Average examples/sec: 2888.87 (2575.46), step = 280\r\n    INFO:tensorflow:Average examples/sec: 2892.13 (2986.66), step = 290\r\n    INFO:tensorflow:global_step/sec: 24.25\r\n\r\n2 gpu bach_size=256 \r\n\r\n    INFO:tensorflow:loss = 2.4630964, step = 200 (5.971 sec)\r\n    INFO:tensorflow:learning_rate = 0.1, loss = 2.4630964 (5.971 sec)\r\n    INFO:tensorflow:Average examples/sec: 3255.68 (4296.71), step = 200\r\n    INFO:tensorflow:Average examples/sec: 3297.51 (4437.93), step = 210\r\n    INFO:tensorflow:Average examples/sec: 3332.15 (4275.33), step = 220\r\n    INFO:tensorflow:Average examples/sec: 3363.86 (4254.65), step = 230\r\n    INFO:tensorflow:Average examples/sec: 3395.09 (4316.94), step = 240\r\n    INFO:tensorflow:Average examples/sec: 3418.44 (4094.23), step = 250\r\n    INFO:tensorflow:Average examples/sec: 3447.17 (4364.24), step = 260\r\n    INFO:tensorflow:Average examples/sec: 3474.56 (4379.02), step = 270\r\n    INFO:tensorflow:Average examples/sec: 3492.73 (4067.13), step = 280\r\n    INFO:tensorflow:Average examples/sec: 3514.19 (4244.23), step = 290\r\n    INFO:tensorflow:global_step/sec: 16.6026\r\n\r\n3 gpu bach_size=384\r\n\r\n    INFO:tensorflow:loss = 2.0980535, step = 200 (9.329 sec)\r\n    INFO:tensorflow:learning_rate = 0.1, loss = 2.0980535 (9.329 sec)\r\n    INFO:tensorflow:Average examples/sec: 3214.65 (4165.7), step = 200\r\n    INFO:tensorflow:Average examples/sec: 3272.85 (5130.99), step = 210\r\n    INFO:tensorflow:Average examples/sec: 3324.15 (4955.13), step = 220\r\n    INFO:tensorflow:Average examples/sec: 3376.65 (5174.76), step = 230\r\n    INFO:tensorflow:Average examples/sec: 3425.48 (5132.15), step = 240\r\n    INFO:tensorflow:Average examples/sec: 3468.29 (4954.35), step = 250\r\n    INFO:tensorflow:Average examples/sec: 3509.91 (5014.23), step = 260\r\n    INFO:tensorflow:Average examples/sec: 3544.29 (4755.56), step = 270\r\n    INFO:tensorflow:Average examples/sec: 3579.69 (4901.39), step = 280\r\n    INFO:tensorflow:Average examples/sec: 3617.84 (5156.66), step = 290\r\n    INFO:tensorflow:global_step/sec: 13.1009\r\n\r\n\r\n[![enter image description here][3]][3]\r\n\r\n\r\n  [1]: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator\r\n  [2]: https://github.com/rossumai/keras-multi-gpu/blob/master/blog/docs/hardware.md\r\n  [3]: https://i.stack.imgur.com/yqIZX.png", "comments": ["Many of the Tensorflow's official examples are not optimally written for performance. I think what you saw is quite normal. You can check out https://github.com/tensorflow/benchmarks/ which is written for performance.", "@ppwwyyxx Thanks!\r\nI try [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) using synthetic data and docker image tensorflow/tensorflow:nightly-gpu-py3, it works pretty well!\r\n\r\n 1 gpu batch_size=32\r\n```\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 198.0 +/- 0.0 (jitter = 0.0)\t8.458\r\n10\timages/sec: 198.1 +/- 0.2 (jitter = 0.3)\t7.997\r\n20\timages/sec: 197.9 +/- 0.3 (jitter = 0.6)\t8.259\r\n30\timages/sec: 198.1 +/- 0.2 (jitter = 0.5)\t8.340\r\n40\timages/sec: 198.0 +/- 0.2 (jitter = 0.6)\t8.188\r\n50\timages/sec: 198.1 +/- 0.2 (jitter = 0.6)\t7.754\r\n60\timages/sec: 198.2 +/- 0.1 (jitter = 0.6)\t8.080\r\n70\timages/sec: 198.1 +/- 0.1 (jitter = 0.6)\t8.472\r\n80\timages/sec: 198.2 +/- 0.1 (jitter = 0.6)\t8.290\r\n90\timages/sec: 198.2 +/- 0.1 (jitter = 0.6)\t8.041\r\n100\timages/sec: 198.1 +/- 0.1 (jitter = 0.6)\t7.993\r\n----------------------------------------------------------------\r\ntotal images/sec: 197.98\r\n```\r\n2 gpus batch_size=32\r\n```\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 373.2 +/- 0.0 (jitter = 0.0)\t8.453\r\n10\timages/sec: 375.8 +/- 0.4 (jitter = 0.4)\t8.209\r\n20\timages/sec: 375.5 +/- 0.5 (jitter = 2.5)\t8.120\r\n30\timages/sec: 375.4 +/- 0.4 (jitter = 2.4)\t8.235\r\n40\timages/sec: 375.6 +/- 0.4 (jitter = 2.3)\t8.108\r\n50\timages/sec: 375.9 +/- 0.4 (jitter = 2.4)\t8.027\r\n60\timages/sec: 375.8 +/- 0.3 (jitter = 2.6)\t8.239\r\n70\timages/sec: 375.7 +/- 0.3 (jitter = 2.4)\t8.373\r\n80\timages/sec: 375.4 +/- 0.3 (jitter = 2.5)\t8.191\r\n90\timages/sec: 375.5 +/- 0.3 (jitter = 2.5)\t7.976\r\n100\timages/sec: 375.6 +/- 0.3 (jitter = 2.8)\t8.157\r\n----------------------------------------------------------------\r\ntotal images/sec: 375.40\r\n```\r\n3 gpus batch_size=32\r\n```\r\nStep\tImg/sec\ttotal_loss\r\n1\timages/sec: 524.8 +/- 0.0 (jitter = 0.0)\t8.394\r\n10\timages/sec: 525.4 +/- 3.4 (jitter = 12.1)\t8.143\r\n20\timages/sec: 527.6 +/- 2.4 (jitter = 10.1)\t8.066\r\n30\timages/sec: 529.1 +/- 1.9 (jitter = 9.2)\t8.191\r\n40\timages/sec: 529.0 +/- 1.5 (jitter = 8.9)\t8.230\r\n50\timages/sec: 530.1 +/- 1.4 (jitter = 9.2)\t8.049\r\n60\timages/sec: 530.5 +/- 1.2 (jitter = 9.2)\t8.292\r\n70\timages/sec: 529.9 +/- 1.2 (jitter = 9.5)\t8.247\r\n80\timages/sec: 530.4 +/- 1.1 (jitter = 9.2)\t8.152\r\n90\timages/sec: 530.6 +/- 1.1 (jitter = 9.1)\t8.019\r\n100\timages/sec: 530.5 +/- 1.0 (jitter = 8.6)\t8.212\r\n----------------------------------------------------------------\r\ntotal images/sec: 530.16\r\n```", "@zihuaweng\r\n\r\n I have tested this benchmark on ImageNet-2012. The training speed is similar to your previous work. However, It's perfect on the synthetic data."]}, {"number": 23031, "title": "Bazel Build fails - \"ERROR: missing input file '//tensorflow/core/api_def:python_api/api_def_DatasetToSingleElement.pbtxt'\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 WSL on Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:0.10.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI am trying to build tensorflow from source. I am getting the following error when I run bazel build command.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nERROR: missing input file '//tensorflow/core/api_def:python_api/api_def_DatasetToSingleElement.pbtxt':/mnt/c/workspace/tensorflow/tensorflow/core/api_def/python_api/api_def_DatasetToSingleElement.pbtxt (No such file or directory)\r\nERROR: /mnt/c/workspace/tensorflow/tensorflow/python/BUILD:1594:1: //tensorflow/python:user_ops_pygenrule: missing input file '//tensorflow/core/api_def:python_api/api_def_DatasetToSingleElement.pbtxt'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /mnt/c/workspace/tensorflow/tensorflow/python/BUILD:1594:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 26.125s, Critical Path: 0.03s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nThe file do exist and I am not sure why it is complaining that api_def_DatasetToSingleElement.pbtx file is missing. \r\n\r\n![screen shot 2018-10-16 at 6 08 56 pm](https://user-images.githubusercontent.com/12464203/47052588-a898e400-d16e-11e8-85ac-3cb4746ac3b5.png)", "comments": ["@srivatsankrishnan Hi, try if Bazel clean would solve this issue. ", "Hi @harshini-gadige,\r\nI am still seeing the same issue. This time its a different file in the same directory:\r\n```\r\nERROR: /mnt/c/workspace/tensorflow/tensorflow/python/BUILD:1564:1: Executing genrule //tensorflow/python:sdca_ops_pygenrule failed (Aborted): bash failed: error executing command /bin/bash bazel-out/host/genfiles/tensorflow/python/sdca_ops_pygenrule.genrule_script.sh\r\n2018-10-17 14:31:41.779121: F tensorflow/python/framework/python_op_gen_main.cc:123] Non-OK-status: api_def_map.LoadFileList(env, api_files) status: Not found: tensorflow/core/api_def/python_api/api_def_StringSplit.pbtxt; No such file or directory\r\n```\r\n", "@harshini-gadige Any update on this yet?", "@annarev  - Hi, any update on this ?", "This error looks very strange. Just before the line where you see the error, we get matching files from file system and then try to get one of these files. So, it is strange that it can't find it.\r\nSo, I don't have any idea right now, but I will look more at it.", "@annarev Thanks for looking into this. I agree this does look strange because I had built versions of Tensorflow on different machines and even on RasPi.\r\n\r\nIs this issue because of Ubuntu 16.04 WSL running on Windows 10? Are you able to reproduce this issue at your end?", "Nagging Assignees @annarev, @harshini-gadige: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry it has been a while since my last reply.\r\n@srivatsankrishnan are you still hitting the issue? If yes, can you check permissions on the missing file? Could it be non-readable?", "@annarev I have moved on to a native Ubuntu 16.04 machine instead of running it on Ubuntu 16.04 WSL on Windows 10. I am pretty sure the Ubuntu 16.04 WSL has root access. So I think it is not a permission issue to read those files.\r\n\r\nI am not facing this issue anymore, so it can be closed. People can re-open if they encounter this problem."]}, {"number": 23030, "title": "Error using kernel_regularizer with tf.layers.Dense and tf.contrib.distribute.MirroredStrategy TF v1.12.0rc0 regression", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.12.0-rc0-0-g1a6dea36de', '1.12.0-rc0')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0.176/7.2.1.38\r\n- **GPU model and memory**: Nvidia V100 16GB\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nAttempting to use a `tf.contrib.layers.l2_regularizer` for the `kernel_regularizer` param of `tf.layers.Dense` causes an `AssertionError` when using with `tf.contrib.distribute.MirroredStrategy` under Tensorflow v1.12.0rc0, but not under Tensorflow v1.11.0. This also occurs with several other layers including `tf.layers.dense` and `tf.layers.conv2d`.\r\n\r\n### Source code / logs\r\nSource of test.py:\r\n```python\r\nimport tensorflow as tf\r\n\r\nDO_L2_REG = False\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef model_fn(features, labels, mode):\r\n    regularizer = tf.contrib.layers.l2_regularizer(0.001) if DO_L2_REG else None\r\n    layer = tf.layers.Dense(1, kernel_regularizer=regularizer)\r\n    logits = layer(features)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\"logits\": logits}\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.mean_squared_error(\r\n        labels=labels, predictions=tf.reshape(logits, []))\r\n\r\n    loss += tf.losses.get_regularization_loss()\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss, global_step=tf.train.get_global_step())#loss_fn())\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\ndef input_fn():\r\n    features = tf.data.Dataset.from_tensors([[1.]]).repeat()\r\n    labels = tf.data.Dataset.from_tensors(1.).repeat()\r\n    return tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nclassifier = tf.estimator.Estimator(model_fn=model_fn, config=config, model_dir='out_test')\r\nprint('********* start train **********')\r\nclassifier.train(input_fn=input_fn, steps=1000)\r\nprint('********* end train/start eval **********')\r\nclassifier.evaluate(input_fn=input_fn, steps=1000)\r\nprint('********* end eval **********')\r\n```\r\n\r\n### Results of testing this code with different version of Tensorflow and `DO_L2_REG` values:\r\n* Calling this with Tensorflow v1.11.0 with `DO_L2_REG` set to either `True` or `False` works as expected, both training and evaluation run successfully.\r\n* Calling this with Tensorflow v1.12.0rc0 with `DO_L2_REG` set to `False` works as expected, both training and evaluation run successfully.\r\n* Calling this with Tensorflow v1.12.0rc0 with `DO_L2_REG` set to `True` results in the following error when the `classifier.train` is called:\r\n```\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Error reported to Coordinator:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"test.py\", line 10, in model_fn\r\n    logits = layer(features)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 746, in __call__\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py\", line 944, in build\r\n    trainable=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 293, in add_weight\r\n    self._handle_weight_regularization(name, variable, regularizer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 638, in _handle_weight_regularization\r\n    self.add_loss(functools.partial(_loss_for_variable, variable))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 142, in add_loss\r\n    loss_tensor = regularizer()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 434, in _tag_unconditional\r\n    loss = loss()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 629, in _loss_for_variable\r\n    with ops.colocate_with(v):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 35, in <module>\r\n    classifier.train(input_fn=input_fn, steps=1000)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1316, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/distribute.py\", line 721, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 556, in _call_for_each_tower\r\n    return _call_for_each_tower(self, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 183, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"test.py\", line 10, in model_fn\r\n    logits = layer(features)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 746, in __call__\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py\", line 944, in build\r\n    trainable=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 293, in add_weight\r\n    self._handle_weight_regularization(name, variable, regularizer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 638, in _handle_weight_regularization\r\n    self.add_loss(functools.partial(_loss_for_variable, variable))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 142, in add_loss\r\n    loss_tensor = regularizer()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 434, in _tag_unconditional\r\n    loss = loss()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 629, in _loss_for_variable\r\n    with ops.colocate_with(v):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n```\r\n", "comments": ["It also does not work when applying the l2 regularization with convolution layers: \r\n```Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 129, in get_model_fn\r\n    self.configure_network(input_tensor=features, output_tensor=labels, mode=mode)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 86, in configure_network\r\n    self.network.construct_network(input_tensor=input_tensor, output_tensor=output_tensor, mode=mode)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 91, in construct_network\r\n    self.construct_architecture(input_tensor=input_tensor,mode=mode)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 54, in construct_architecture\r\n    conv1, pool1 = self.conv_conv_pool(input_tensor, [filter_size, filter_size], training, l2, name=1)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 130, in conv_conv_pool\r\n    name=\"conv_{}\".format(i + 1))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 746, in __call__\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 165, in build\r\n    dtype=self.dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 293, in add_weight\r\n    self._handle_weight_regularization(name, variable, regularizer)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 638, in _handle_weight_regularization\r\n    self.add_loss(functools.partial(_loss_for_variable, variable))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 142, in add_loss\r\n    loss_tensor = regularizer()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 434, in _tag_unconditional\r\n    loss = loss()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 629, in _loss_for_variable\r\n    with ops.colocate_with(v):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 1546790882788519247\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 10996947278692380426\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 9829494766080756018\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 15568473293\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n    link {\r\n      device_id: 1\r\n      type: \"StreamExecutor\"\r\n      strength: 1\r\n    }\r\n  }\r\n}\r\nincarnation: 10718015977797192246\r\nphysical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:62:00.0, compute capability: 7.0\"\r\n, name: \"/device:GPU:1\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 15568473293\r\nlocality {\r\n  bus_id: 2\r\n  numa_node: 1\r\n  links {\r\n    link {\r\n      type: \"StreamExecutor\"\r\n      strength: 1\r\n    }\r\n  }\r\n}\r\nincarnation: 12456162273957908465\r\nphysical_device_desc: \"device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:89:00.0, compute capability: 7.0\"\r\n]\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 37, in <module>\r\n    experimenter.run_training_experiment(config)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 40, in run_training_experiment\r\n    tf.estimator.train_and_evaluate(self.trainer, self.training_specs, self.eval_specs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 610, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 711, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1205, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1316, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 721, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 556, in _call_for_each_tower\r\n    return _call_for_each_tower(self, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 183, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 795, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 129, in get_model_fn\r\n    self.configure_network(input_tensor=features, output_tensor=labels, mode=mode)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 86, in configure_network\r\n    self.network.construct_network(input_tensor=input_tensor, output_tensor=output_tensor, mode=mode)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 91, in construct_network\r\n    self.construct_architecture(input_tensor=input_tensor,mode=mode)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 54, in construct_architecture\r\n    conv1, pool1 = self.conv_conv_pool(input_tensor, [filter_size, filter_size], training, l2, name=1)\r\n  File \"/media/local/myfiles/mynetwork.py\", line 130, in conv_conv_pool\r\n    name=\"conv_{}\".format(i + 1))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/convolutional.py\", line 417, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 817, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 374, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 746, in __call__\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 165, in build\r\n    dtype=self.dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 293, in add_weight\r\n    self._handle_weight_regularization(name, variable, regularizer)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 638, in _handle_weight_regularization\r\n    self.add_loss(functools.partial(_loss_for_variable, variable))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 142, in add_loss\r\n    loss_tensor = regularizer()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 434, in _tag_unconditional\r\n    loss = loss()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 629, in _loss_for_variable\r\n    with ops.colocate_with(v):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 439, in _tensor_conversion_mirrored\r\n    assert not as_ref\r\nAssertionError```", "Experiencing same problem with both conv layers and dense. TF 1.12/CUDA 10/CuDNN 7.4.1", "Hmmm... b08c981d1d5556fb384cbbadcdc0d7c373324300 _seems_ to address this, but this issue is not linked?", "@berendo I can confirm, it's working", "I have encountered the same problem.   \r\nBoth `tf.contrib.layers.l2_regularizer` and `tf.keras.regularizers.l2` not working with `tf.layers.dense` layer, using MirroredStratedy.  \r\ntf 1.12 CUDA 9.0", "@raofengyun I have encountered the same problem. Both tf.contrib.layers.l2_regularizer and tf.keras.regularizers.l2 not working with tf.layers.dense or conv2d layer, using MirroredStratedy. Have you solved this problem? Thanks a lot.\r\ntf 1.12 CUDA 9.0", "@Matt-Hicks-Bose,\r\nYour code could be executed without any error in **`Tensorflow Version 1.15.2`**. Please find the [Gist](https://colab.research.google.com/gist/rmothukuru/aa03a95650e5f5463bd5160f0acb854a/gh_23030.ipynb) of the working code with **`DO_L2_REG = True`**. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 23029, "title": "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "body": "_Originally posted by @tensorflowbutler in https://github.com/tensorflow/tensorflow/issues/17336#issuecomment-374545205_\r\n\r\nYes please. Can you guys add the axis feature to the scatter_update. I need to mutate certain columns of weight matrices at each layer and then substitute them back. I can make the changes using tf.gather but have no idea how to substitute them back. I tried to work around by taking a a transpose of variable but apparently taking a transpose of a variable effects its mutability. tf.scatter_update gives an error.\r\n\r\nTypeError: 'ScatterUpdate' Op requires that input 'ref' be a mutable tensor (e.g.: a tf.Variable) ", "comments": ["`tf.scatter_nd` can do what you need.", "tf.scatter_nd updates into a NEW tensor according to indices. I want to substitute it back to the original tensor. tf.scatter_updates works for rows but not for columns.", "Oops. I mean `tf.scatter_nd_update`.", "It does not. I am unable to replace/change column slices.", "Then either you're using it wrong or there is a bug in TensorFlow. To report bugs, it's better to provide a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve).", "I posted the same issue on Stackoverflow and got a response eventually solving the issue I was having. Scatter_nd_update can be used to mutate columns if all the indices are provided. I still think that the axis feature is required. \r\n\r\nHere is the link to the Question that I posted:\r\n\r\nhttps://stackoverflow.com/questions/52872239/can-tf-scatter-update-or-tf-scatter-nd-update-be-used-to-update-column-slices-of\r\n\r\nLemme know if you feel this could have been solved in a different manner.", "Yes that's the correct usage of `scatter_nd_update`.\r\n`axis=` is a reasonable feature to add on `tf.scatter_update` (not `scatter_nd_update`), given there is already an `axis=` option in `tf.gather`.", "@jaweriaamjad  Thanks for sharing the solution. "]}, {"number": 23028, "title": "Update version information in preparation for 1.12.0-rc1", "body": "", "comments": []}, {"number": 23027, "title": "rtx2080ti  python3.6 tensorflow1.10 cuda9.0+cudnn7.0  Blas xGEMMBatched launch failed", "body": "I have try tensorflow1.8-1.11  but all failed!\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\ntf.Session(config=config).close()\r\n\r\ndef calc():\r\n    N = 15 # works for N <= 14\r\n    a = 16\r\n    b = 8\r\n    X = np.random.rand(N, 11520, b, 1).astype(np.float32)\r\n    print(X.nbytes*1e-6, \"MB\")\r\n    W = np.random.rand(N, 11520, a, b).astype(np.float32)\r\n    print(W.nbytes*1e-6, \"MB\")\r\n    X_ = tf.constant(X, name=\"X-constant\", dtype=tf.float32)\r\n    W_ = tf.constant(W, name=\"W-constant\", dtype=tf.float32)\r\n    return W_ @ X_\r\n\r\ntf.reset_default_graph()\r\na = calc()\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nb = sess.run(a)\r\nsess.close()\r\nprint(b.shape)\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "windows10\r\ntensorflow installed from pip/anaconda(both try)\r\ntensorflow version 1.8-1.11(all try)\r\ncuda9.0  cudnn7.0\r\ngpu rtx2080ti  11g\r\n", ">           Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n> Have I written custom code\r\n> OS Platform and Distribution\r\n> TensorFlow installed from\r\n> TensorFlow version\r\n> Bazel version\r\n> CUDA/cuDNN version\r\n> GPU model and memory\r\n> Exact command to reproduce\r\n> Mobile device\r\n\r\nwindows10\r\ntensorflow installed from pip/anaconda(both try)\r\ntensorflow version 1.8-1.11(all try)\r\ncuda9.0 cudnn7.0\r\ngpu rtx2080ti 11g", "@13718413797 Hi, we understand that your code is running into error and would appreciate if you could describe the problem clearly. Also please provide the error message you are getting along with the log. Thank you !", "@harshini-gadige \r\nInternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\r\n     [[Node: matmul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]", "@harshini-gadige my English is poor  sorry\r\nsame version(tensorflow,cuda,cudnn,python)  gtx965  can run not failed", "@13718413797 Please check [this](https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250) to find any possible solutions. Also request you to post any support related questions/queries in [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow).", "@harshini-gadige I've seen this page , he had issues with CUDA 9.1, try 9.0, that fixed things  ,but I CUDA is 9.0", "This is usually one of two things: either cuda is improperly installed somehow (say, wrong cuda version, or wrong driver version, etc) or your GPU doesn't have enough RAM to run the  model you want.\r\n\r\nFirst update your drivers if possible, then looking at the output of nvidia-smi can help debug this (to see if your GPU has indeed enough free memory before tf runs to actually run this computation).\r\n\r\nThat said, this in neither case is a bug in tf, so this kind of question is best asked in stackoverflow.", "@alextp   cuda is  is properly installed  other Program can run , RAM is enough  my Another Computer 4G RAM can run this Program , divers is lasted version.  ", "Tensorflow is very finnicky about what driver version and cuda version and\ncudnn version you're using, which is why I think you need to look at the\nnvidia-smi output as well as update both cuda and your drivers.\n\nOn Thu, Oct 18, 2018 at 6:20 PM \u4f3d\u5170\u5723\u7ea6 <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> cuda is is properly installed other\n> Program can run , RAM is enough my Another Computer 4G RAM can run this\n> Program , divers is lasted version.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23027#issuecomment-431213258>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxR7kAQztDVH58fUAQvppp8hGCVWxks5umSjlgaJpZM4XeqqJ>\n> .\n>\n\n\n-- \n - Alex\n", "Due to a version conflict in CUDA9.0 and tensorflow 1.12.0\r\n\r\nuse  result = tf.py_func(np_matmul, [x, y], tf.float32)  instead.\r\n\r\nuse a numpy function wrap in tensorflow."]}, {"number": 23026, "title": "Installation failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  failed at import tensorflow\r\n- **OS Platform and Distribution: Ubuntu 18.04.1 LTS\r\n- **TensorFlow installed from : Anaconda\r\n- **TensorFlow version (use command below)**:  1.11.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: not using\r\n- **GCC/Compiler version (if compiling from source)**: [GCC 7.3.0] on linux\r\n- **CUDA/cuDNN version**: 9.1.85 /  not sure was automatically download\r\n- **GPU model and memory**: GeForce 840m\r\n\r\n\r\n### Describe the problem\r\nbasically it doesn't work when i run \"import tensorflow\"\r\n\r\n### Source code / logs\r\nPython 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/herooooooooo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["It is CUDA error. Tensorflow is not able to find cuda. How did you install tensorflow?  ", "> \r\n> \r\n> It is CUDA error. Tensorflow is not able to find cuda. How did you install tensorflow?\r\n\r\nI simply used     conda install -c anaconda tensorflow-gpu ", "I just checked there's no libcuda.so.1, i did find libcuda.so.9", "Try creating an alias of libcuda.so.9 as libcuda.so.1", "I'm really sorry it's libcudart.so.9.2 and does it change if it is in a environment? basically i have to write\r\nconda activate tf_gpu\r\nto start it"]}, {"number": 23024, "title": "Add bullet points so Reduction values are clearer", "body": "Currently it's these items are all in one line in https://www.tensorflow.org/api_docs/python/tf/losses/Reduction, which is hard to visualize.", "comments": ["@ymodak Could you or anyone else merge this? This has been here for a month and I have to resolve conflicts every now and then. ", "@case540 Could you re-approve this? I had to rebase and resolve conflicts. Thank you!", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23023, "title": "Hyperspectral image classification using tensorflow", "body": "\r\nDelivery Status Notification (Failure)Application toolbar\r\nMessage Body\r\nHello sandeep.ladi@gitam.edu,\r\n\r\nWe're writing to let you know that the group you tried to contact (discuss) may not exist, or you may not have permission to post messages to the group. A few more details on why you weren't able to post:\r\n\r\n * You might have spelled or formatted the group name incorrectly.\r\n * The owner of the group may have removed this group.\r\n * You may need to join the group before receiving permission to post.\r\n * This group may not be open to posting.\r\n\r\nIf you have questions related to this or any other Google Group, visit the Help Center at https://support.google.com/a/tensorflow.org/bin/topic.py?topic=25838.\r\n\r\nThanks,\r\n\r\ntensorflow.org admins\r\n\r\n\r\n\r\n----- Original message -----\r\n\r\nX-Google-Smtp-Source: ACcGV625ngDxZGFv6zxaJTNRD5PlUDFyJNQJK/RowOuC79uaJbT7E4TVqkiThwkQIV8Yc/WPsFZ0\r\nX-Received: by 2002:a5d:44ce:: with SMTP id z14-v6mr19096788wrr.286.1539685981354;\r\n        Tue, 16 Oct 2018 03:33:01 -0700 (PDT)\r\nARC-Seal: i=1; a=rsa-sha256; t=1539685981; cv=none;\r\n        d=google.com; s=arc-20160816;\r\n        b=E/S3lH6dW8jBpv80GEP3mZyz1KyIDntxI9nZGVOdAbeJS3yOgQ3nq4bp7hre3N/HU8\r\n         9HqA9XkD3dHukXpz06r+2R5F3i2Rbio9TsIfnjdh9b454Ps75DUM7gm1nuxEGojtfeZd\r\n         2RKUTWWKH3a/1dOWecoz0fRibWJd0ByhBNTYW2F+p9hMdg9TAP3m9nuxpLl/bF/r/VAw\r\n         62gIT4ydaIgySfEH6uXHh5dYAab8+96sPnw5wZiAHzO1LpUt766K85dv/6asxCHoMMOR\r\n         M5lzAq1FK+XOfca7bJqn7lt2Vu2YIU7wJbIJs2vQKtLwZcl7SYf20rkFeVGrNQ4zmonY\r\n         zuwg==\r\nARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;\r\n        h=user-agent:message-id:disposition-notification-to:return-receipt-to\r\n         :subject:to:from:date:mime-version;\r\n        bh=OsFzSe3J2uxfPu0m4Xtt3CiGAfo46dWIPdy+i82VhII=;\r\n        b=ePJSnFaP1g6RcKwXYOLK4F+lXEJ8Ul1ycwocH3iAPT6Ga19fNe5HJXhaFcbexJywfr\r\n         G+86clKs6GOA2baW3nqnAl/4rAVGMjQyWIUMBPFHdei28lIjvMo3gyXX3ocxM0XMPP2z\r\n         2vQ2qCJZQBHZegkhMO/z+d1/8a/bjqScVAH+gZOXS0hbQIp23fsx9CsPeM6iX2boAlDB\r\n         q8zh8LkYwMUxA+p0EDdmrflNA2hs9mqqiYaCJJQ2oD7VvqHpUsqdMeczcmck75SvB0xO\r\n         uvPdhrFHdp26RDN2JKj36FInwYHZTnTEKw69UFchUoLBSgzOCdCnhBQlIWyEyDX6LGYQ\r\n         8fUg==\r\nARC-Authentication-Results: i=1; mx.google.com;\r\n       spf=pass (google.com: domain of sandeep.ladi@gitam.edu designates 103.23.29.142 as permitted sender) smtp.mailfrom=sandeep.ladi@gitam.edu\r\nReturn-Path: <sandeep.ladi@gitam.edu>\r\nReceived: from mail.gitam.edu (mail.gitam.edu. [103.23.29.142])\r\n        by mx.google.com with ESMTP id s1-v6si12238610wrf.2.2018.10.16.03.33.00\r\n        for <discuss@tensorflow.org>;\r\n        Tue, 16 Oct 2018 03:33:00 -0700 (PDT)\r\nReceived-SPF: pass (google.com: domain of sandeep.ladi@gitam.edu designates 103.23.29.142 as permitted sender) client-ip=103.23.29.142;\r\nAuthentication-Results: mx.google.com;\r\n       spf=pass (google.com: domain of sandeep.ladi@gitam.edu designates 103.23.29.142 as permitted sender) smtp.mailfrom=sandeep.ladi@gitam.edu\r\nReceived: from mail.gitam.edu (webmail.gitam.edu [192.168.23.28])\r\n    by mail.gitam.edu (Postfix) with ESMTPA id B765722BCC5\r\n    for <discuss@tensorflow.org>; Tue, 16 Oct 2018 16:02:46 +0530 (IST)\r\nMIME-Version: 1.0\r\nContent-Type: multipart/alternative;\r\n boundary=\"=_8b103ec7c4f10c51b775e6343c7573ca\"\r\nDate: Tue, 16 Oct 2018 16:02:46 +0530\r\nFrom: sandeep.ladi@gitam.edu\r\nTo: discuss@tensorflow.org\r\nSubject: Command to Input Hyperspectral Image dataset to TensorFlow.\r\nReturn-Receipt-To: sandeep.ladi@gitam.edu\r\nDisposition-Notification-To: sandeep.ladi@gitam.edu\r\nMessage-ID: <e885520003a6c5c626388950aeb89319@gitam.edu>\r\nX-Sender: sandeep.ladi@gitam.edu\r\nUser-Agent: GITAM Webmail/1.2.2\r\nX-gitam-MailScanner-Information: Please contact the ISP for more information\r\nX-gitam-MailScanner-ID: B765722BCC5.A15A0\r\nX-gitam-MailScanner: Found to be clean\r\nX-gitam-MailScanner-SpamScore: s\r\nX-gitam-MailScanner-From: sandeep.ladi@gitam.edu\r\nX-Spam-Status: No\r\n\r\n--=_8b103ec7c4f10c51b775e6343c7573ca\r\nContent-Transfer-Encoding: 7bit\r\nContent-Type: text/plain; charset=US-ASCII\r\n\r\nDear sir/Madam, \r\n\r\nGreetings of the day. \r\n\r\nI am Mr.Ladi Sandeep Kumar,working as an Assistant Professor in GITAM\r\nUniversity,Visakhapatnam,India.I and my project students are working on\r\na Project based on Hyperspectral image classification which is an\r\nemerging research topic.The opensource Hyperspectral image datasets are\r\nin .mat files containing reflectance values.We are going to use CNN\r\ntechnique for image classification using Hyperspectral image dataset of \r\nIndian pines which is a dataset of 145x145x200 where 145x145 are height\r\nand width of image and 200 is number of channels.There are lot of\r\nresources and guide for using tensorflow CNN toolbox for color image.Can\r\nyou please tell me the way and send the code to input the Indian pines\r\ndataset of 145x145 x200 as input to the CNN. \r\n\r\nI will use the tensorFlow toolbox in my  research as well as projects \r\nto teach my students and quote your website in my publications wherever\r\nI apply tensorflow module.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code -N/A\nOS Platform and Distribution-Ubuntu 14,google collab application,python 3\nnotebook\nTensorFlow installed from-google collab,python 3 notebook,TPU\nTensorFlow version-N/A\nBazel version-N/A\nCUDA/cuDNN versionN/A\nGPU model and memory-TPU available at Google collab\nExact command to reproduce-pixel wise Classification of hyperspectral image\nusing tensor flow.\nMobile device-Acer Laptop.\n\nOn Wed 17 Oct, 2018, 7:02 AM Alfred Sorten Wolf, <notifications@github.com>\nwrote:\n\n> Thank you for your post. We noticed you have not filled out the following\n> field in the issue template. Could you update them if they are relevant in\n> your case, or leave them as N/A? Thanks.\n> Have I written custom code\n> OS Platform and Distribution\n> TensorFlow installed from\n> TensorFlow version\n> Bazel version\n> CUDA/cuDNN version\n> GPU model and memory\n> Exact command to reproduce\n> Mobile device\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23023#issuecomment-430455829>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APWlDEZYV-gGxl9Xgl1W3Ppim5f11STcks5uloidgaJpZM4XeSe->\n> .\n>\n", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}]