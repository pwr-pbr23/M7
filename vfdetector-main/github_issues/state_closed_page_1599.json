[{"number": 4942, "title": "Make TensorFlow pip package build on Windows", "body": "@mrry @dslomov @damienmg \n", "comments": ["Can one of the admins verify this patch?\n", "@meteorcloudy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @davidzchen to be potential reviewers.\n", "Change updated, please review again!\n\nOne thing I forgot to mention, the pip package build now only works on Windows with Bazel at HEAD and  Visual Studio Update 2. I had some changes to make Bazel work with Python3, but for Visual Studio, we still need /WHOLEARCHIVE to link `_pywrap_tensorflow.so`(In fact, it's a dll). The reason is we need[ the global whole archive feature](https://github.com/bazelbuild/bazel/commit/ee311f4e23f728b1451785434c286d66aa236be9) to force link all its  dependencies no matter whether the dependency has `alwaylink = 1` or not. Without `/WHOLEARCHIVE`, I haven't figured out a simple solution to get all the object files from all of its dependencies.\n\nSo, unfortunately if users want to build the pip package on Windows, they have to install VS 2015 update 2 or newer version.  Is it going to be a serious problem or not?\n", "@tensorflow-jenkins test this please.\n", "I know why tests are failing.. Because my fix for example trainer was reverted at be3bc472a52571a83f048479d6a4fa528b5a495e .. :(\nLet's wait until we solve that problem..\n", "@tensorflow-jenkins test this please.\n", "I fixed the sanity checks failures, please test again :)\n", "@tensorflow-jenkins test this please.\n", "Linux GPU looks like flaky, rerun it?\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "(Third time's a charm!)\n", "Fourth time? \ud83d\ude02\n", "@tensorflow-jenkins test this please.\n", "We bumped up the gradients_test size #5062, it has to work this time :-)\n", "Could you pull rebase and push again? There's a change in tensorflow/python/BUILD. Thanks!\n", "@drpngx I've rebased my branch, please test again. :)\n", "Wait... It's now broken because of `//tensorflow/core/kernels:quantized_ops`\n", "OK, it's fixed.\n", "@tensorflow-jenkins test this please.\n", "Hooray!\n", "You might want to squash the commits when merging....commit history is a bit untraceable now\n", "I installed TensorFlow using pip on Python 2.7.12. When I import TensorFlow I get this error message:\n\n```\nIn [1]: import tensorflow\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-1-a649b509054f> in <module>()\n----> 1 import tensorflow\n\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\n     21 from __future__ import print_function\n     22\n---> 23 from tensorflow.python import *\n     24\n     25\n\nC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\n     45 # when importing numpy (gh-2034).\n     46 import numpy as np\n---> 47 _default_dlopen_flags = sys.getdlopenflags()\n     48 sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\n     49 from tensorflow.python import pywrap_tensorflow\n\nAttributeError: 'module' object has no attribute 'getdlopenflags'\n```\n\n`sys` module has `getdlopenflags` on Python 3.x.\nWhat can be done?\n", "This PR breaks pip package building in Mac.\nIt added \"cp --parents\" which does not seem to be available on macos.\n"]}, {"number": 4941, "title": "FIXBUG: convolution use error GFile API", "body": "This place should use Size() method.\n", "comments": ["Can one of the admins verify this patch?\n", "@pooorman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @rohan100jain, @vrv and @keveman to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I had signed a CLA and add email address in my github. How to pass the authorization. Thanks\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py#L83\n\ndefines a size() and not Size() method. \n"]}, {"number": 4940, "title": "Tensorflow and emscripten", "body": "Any plans to add the build chain for emscripten?\nAt this time there is this attempt https://github.com/tomasreimers/tensorflow-emscripten, but a official supported would be awesome!\n", "comments": ["Do you have a design doc?\n\nIn order to merge a feature like this, it would ideally be configured using Bazel, possible using [cc_toolchain()](https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/rules/cpp/CcToolchain.java). Our Makefile build system is community supported and intended for platforms that aren't able to run Bazel.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@jart this issue was closed for inactivity, by the way any recent progress or contribute in that direction for the build chain (I guess it has changed a bit...)? Thanks."]}, {"number": 4939, "title": "System hangs while training", "body": "Hi everyone,\n\nI am trying to train model for handwritten digits. My data consists of images, each of size 80X80. So I get 6400 features as inputs. \nWhile trying to train model as per code used in https://www.kaggle.com/kakauandme/digit-recognizer/tensorflow-deep-nn, my systems hangs after 200 iterations with training accuracy 0.06.\n\nWhy is this happening? I don't get any errors. My system just freezes.\nAlso, how do I set pooling and convolution layers parameters? \n\nPS: I'm not using GPU. \n", "comments": ["It's hard to determine if issues like this are bugs without more information. We recommend posting on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) for community support.\n"]}, {"number": 4938, "title": "A bug on embedding_attention_seq2seq when output_projection is None?", "body": "When I use embedding_attention_seq2seq without giving an `output_projection` argument, I experienced that the program crashes by a memory allocation error, even when the model ran fine in other libraries or my own implementation of attention seq2seq.\nI didn't suffer this problem when I give an `output_projection` argument to the function explicitly.\n\nI suspect it occurs by the following fact: when `output_projection` is None, it wraps the `cell` with `OutputProjectionWrapper`) in `embedding_attention_seq2seq`. This wrapped `cell` emits output whose dimension matches the number of decoder symbols.\nThen the wrapped `cell` variable is passed to `embedding_attention_decoder`, and to `attention_decoder`.\nHere, in the `attention_decoder` function, the awkward memory allocation happens.\n1) Since the `output_size` argument is None, it is set to `cell.output_size`, which is identical to the number of decoder symbols. (line 576)\n2) `cell_output` of line 650 has dimensions proportional to the number of decoder symbols.\n3) Thus, in line 660, it creates a very large matrix whose size is proportional to the square of the number of decoder symbols.\n\nAccording to the paper the implementation is referencing, the attention mechanism should not depend on the number of decoder symbols.\nSo I think the implementation is somewhat wrong and should be corrected by passing the `cell` without wrapping an `OutputProjectionWrapper` and then performing projection afterwards.\n\nIf it is truly a bug and no one is working now, I will submit a pull request since it can be easily fixed, IMO.\n", "comments": ["Thanks for reporting this issue and taking the time to really dig into what the code is doing. @lukaszkaiser would be the best person to comment on this.\n", "This is indeed a bug, it's exactly as you say and it shouldn't be like this. The problem is that there doesn't seem to be a backward-compatible way of correcting it. If we change the default, we'll break every model trained with the current function without output_projection (because the change of attention variables will change and old checkpoints won't load any more).\n\nThat's why, unless you have something in mind to make it backwards-compatible, I'd rather avoid correcting this. The current seq2seq module will be deprecated anyway, because it does static graph construction (we have while_loop now) and we're also moving away from the list-based API to a single-tensor one (time being the first or second dimension). So it looks to me like correcting this is more troube than it's worth.\n\nBy the way - the work on the new, dynamic seq2seq is happening in contrib.seq2seq and it's happening on github, lead by alrojo -- see, for example, issue #4686. There is no attention decoder there yet, though ethancaballero has asked for it in #4761. So maybe you could sync and work with them to make a new, bug-free attention decoder for the new contrib.seq2seq?\n\nLet me know what you think, thanks for catching this!\n", "@lukaszkaiser maybe we could write a one line PR adding a comment to the code that links to this issue, so if anyone gets confused by this behavior in the future, they'll know it's been addressed?\n", "Doing now.\n", "The documentation change is now approved internally. It will be synced to GitHub soon, at which point this issue will be updated. It's always sad when bugs have to become features. I'm reminded of how the version number for TeX will become \u03c0 when Knuth dies. But at least future users will be able to avoid this problem.\n", "Thanks for the clear answer!\nAnd it is a very good news to me that TF will support a variable-length sequences in the future seq2seq module.\nI will look for it to find whether there exist parts I can contribute.\n"]}, {"number": 4937, "title": "Build iOS sample failed", "body": "After I run the tensorflow/contrib/makefile/build_all_ios.sh command,\nI got the messages bellow:\n\necho 'protoc not found at \n~/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc. Build it first.'\nprotoc not found at ~/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc. Build it first.\n\nI check the protobuf-host directory. This folder contains nothing, how can I solve this?\nDo I miss any steps?\nThanks!\n", "comments": ["Assigning to @petewarden who maintains this Makefile.\n", "When you run build_all_ios.sh make sure there aren't any hidden messages. Search your ~/tensorflow directory for protoc i.e.\n\nfind ~/tensorflow -name 'protoc'\n", "@aselle \nI tried find ~/tensorflow -name 'protoc', \nNo protoc folder found...\n\nHow should I do next? \nThanks.\n", "run $ build_all_ios.sh >& build_log\nand attach it (don't inline it)\n", "@aselle \nOK, I run $ build_all_ios.sh >& build_log\n\nPlease help check the attachment\nThank you.\n[build_log.txt](https://github.com/tensorflow/tensorflow/files/532353/build_log.txt)\n", "It looks like you didn't follow the instructions (or they didn't work). See\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#before-you-start-all-platforms\nSpecifically the download prerequisites.\n", "@aselle \nIt seems that running \"tensorflow/contrib/makefile/download_dependencies.sh\" is going well.\n\n> downloading http://bitbucket.org/eigen/eigen/get/97c1ebe6ccc2.tar.gz\n> downloading http://github.com/google/gemmlowp/archive/8b20dd2ce142115857220bd6a35e8a081b3e0829.tar.gz\n> downloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz\n> downloading http://github.com/google/protobuf/archive/v3.1.0.tar.gz\n> downloading http://github.com/google/re2/archive/7bab3dc83df6a838cc004cc7a7f51d5fe1a427d5.tar.gz\n> download_dependencies.sh completed successfully.\n\nAnd I also download example graph, it's in  ~/graphs.\nThanks.\n", "I tried running the tensorflow/contrib/makefile/build_all_ios.sh command again\nBut I still got the same issue.\n\nShould I provide some other information? \n", "Can you run and attach this log?\n\n```\ntensorflow/contrib/makefile/compile_ios_protobuf.sh 4 >& protobuf.compile.log\n```\n\n(you can look at the log and try to diagnose any errors there)\n", "Here's the log, and I found that protobuf-host is a new folder crated by script.\nI have these in  tensorflow/contrib/makefile/downloads:\n        eigen gemmlowp  googletest  protobuf      re2\n\nMay I miss something ?\n[protobuf.compile.log.txt](https://github.com/tensorflow/tensorflow/files/547149/protobuf.compile.log.txt)\n", "Did you ever do this?\n\n```\nbrew install automake\n```\n", "Also what version of mac os?\n", "Yes, I did brew install automake. It only says automake-1.15 is already installed.\n\nThe version of my mac OS is El Capitan 10.11.6\n", "This issue is quite old and hasn't had recent activity. Be sure to try the latest build instructions (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)\r\non the latest version of TensorFlow, and please create a new bug if it is still not working. Thanks."]}, {"number": 4936, "title": "fix comment error in mnist_softmax.py", "body": "should be tf.nn.softmax\n", "comments": ["Can one of the admins verify this patch?\n", "@MrQianJinSi, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv and @keveman to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it! please check.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4935, "title": "Does Distributed TensorFlow support Infiniband?", "body": "Based on stackoverflow, the answer is no.\n\nIf it is no, does TF have plans to support Infiniband?\n\nhttp://stackoverflow.com/questions/39184496/does-distributed-tensorflow-support-infiniband-interconnections-out-of-the-box\n", "comments": ["TensorFlow uses gRPC. We're going to support whatever gRPC supports. So I would encourage you to ask your question to the gRPC folks: https://github.com/grpc/grpc\n", "@jart Although TensorFlow uses gRPC for cross-node communication, there are flags that are intentionally reserved for out-of-band(OOB) transfers like RDMA. \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker.proto#L223\n\nEven though gRPC could use a RDMA transport, it requires messages be serialized in protobuf, which requires memory copying from GPU to CPU. With OOB transfers like RDMA, it is possible to transfer tensors directly from one GPU to another.\n\nI believe the OP is asking for the OOB RDMA transport for tensors. Maybe some implementation around rdma_cm might be helpful.\n", "There's related discussion on https://github.com/tensorflow/tensorflow/issues/2916\n", "I really appreciate you guys helping out with support @thinxer and @yaroslavvb. I was not aware of this fact.\n\nI would still like to keep this issue closed since #2916 is requesting RDMA in general. AFAIK Infiniband is just a specific implementation of it. So Infiniband discussion is probably more appropriate there for the time being.\n"]}, {"number": 4934, "title": "How to get word alignment in attention_seq2seq in Translate.py?", "body": "Hi, \nI'm wondering if i get the word alignment probability in the seq2seq model for language translation.\n\nThanks a lot! \nStephen\n", "comments": ["@wsnooker : Apologies for the late reply. This question is probably best asked on [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) as the github issues try to remain focused on bugs and feature requests.\n"]}, {"number": 4933, "title": "I want to write a while_loop in while_loop. Does anyone know how write it correctly or have some examples?", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Please check out [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) for community support. We try to keep this issue tracker focused on bugs and feature requests.\n"]}, {"number": 4932, "title": "Distributed inception stuck at tf.initialize_all_variables()", "body": "I'm running distributed training of Inception but its behavior is unpredictable and time to time the _**chief**_ node stuck at (I found out the line through adding logs for every line):\n\n```\ntensorflow/tensorflow/python/training/session_manager.py:233\nsess.run(init_op, feed_dict=init_feed_dict)\n```\n\nThe `init_op` is assigned to `tf.initialize_all_variables()` in Inception distributed training. As I mentioned, the behavior is random and it only happens time to time.\n\nHas anyone else has this issues?\n\nI'm running the latest version of TF compiled from source + cuda 7.5 + cudnn 5.1.3\n", "comments": ["Hey @mrry what are good steps for people to troubleshoot intermittent hanging issues when distributed?\n", "@mbz what if you run it with less threads? (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1). Sometimes it's useful to get a stack trace under `gdb` using `where` or `thread apply all backtrace`. Running it with \"-c dbg\" compiled version of TensorFlow will give line numbers\n", "This sounds like a connectivity issue. The `init_op` itself should be non-blocking, so it's most likely that the chief is having difficulty connecting to one of your PS tasks. (For example, it pings all of the PS tasks at session creation, and will block until it gets a response.)\n\nAs Yaroslav suggests, attaching `gdb` and getting a stack trace is probably the best way to get to the bottom of this.\n", "Thanks for the reply. I tried setting `intra_op_parallelism_threads=1, inter_op_parallelism_threads=1` but the same issue still happened. Now, I'm working on building a debug build to get gdb running. \n\nJust to make sure what Derek is saying: so session creation is async and when it get to `sess.run` it blocks till it gets a response back from all the PSs? It would be great if you point me to this part of the code since I only have one PS so this should be easy to trace.\n", "@mbz Here's where it would block (on the first `sess.run()` call) in Python: https://github.com/tensorflow/tensorflow/blob/f794cd393b1e7821fcc3cdcee9b6a4400f2540bf/tensorflow/python/client/session.py#L998. Here's where it would block in the C++ runtime: https://github.com/tensorflow/tensorflow/blob/f794cd393b1e7821fcc3cdcee9b6a4400f2540bf/tensorflow/core/distributed_runtime/master.cc#L168.\n\nIt's also possible that the chief has made it past these points, and some connection has dropped. We disable fail_fast on the RPCs so gRPC will retry until it gets a response... this means that if a process fails we assume something (e.g. a cluster manager like Kubernetes) will restart the process for us. These might be a little harder to see though, because almost all of the Master->Worker RPCs are asynchronous, but there are various places where a stack trace would be informative.\n", "Thanks for the info @mrry. I put some traces here and there and you are exactly correct about the line which the code is waiting. Communication issues can also justify the random behavior of the system.\n\nI also checked the `fail_fast` flag you mentioned. It seems to be False, hardcoded (am I right?). Therefore, the clients should try to reconnect, don't they? Is it possible to log the connection errors? That should help figuring out the issue a little bit.\n", "Yes, `fail_fast` should be false, and clients should try to reconnect, so this sounds like a permanent failure. I haven't played much with gRPC logging, but it [appears](https://github.com/grpc/grpc/blob/1a62ec83b7af9c4437164d10fa23197c64542382/src/core/lib/support/log.c#L81) that setting the environment variable `GRPC_VERBOSITY=DEBUG` (or `INFO`) should cause it to print more logs.\n\n[EDIT: Just confirmed that that environment variable does cause additional logging, e.g.\n\n```\n$ GRPC_VERBOSITY=DEBUG python\n>>> sess = tf.Session(\"grpc://localhost:2229\")\n>>> sess.run(tf.matmul([[42]], [[1]]))\nI1019 14:05:57.205346638   21169 subchannel.c:642]           Connect failed: {\"created\":\"@1476911157.205294638\",\"description\":\"Failed to connect to remote host\",\"errno\":11,\"file\":\"external/grpc/src/core/lib/iomgr/error.c\",\"file_line\":256,\"os_error\":\"Connection refused\",\"target_address\":\"ipv6:[::1]:2229\"}\nI1019 14:05:57.205373050   21169 subchannel.c:647]           Retry in 0.999859463 seconds\nI1019 14:05:57.205478940   21169 subchannel.c:642]           Connect failed: {\"created\":\"@1476911157.205455254\",\"description\":\"Failed to connect to remote host\",\"errno\":115,\"file\":\"external/grpc/src/core/lib/iomgr/error.c\",\"file_line\":256,\"os_error\":\"Connection refused\",\"target_address\":\"ipv4:127.0.0.1:2229\"}\nI1019 14:05:57.205494995   21169 subchannel.c:647]           Retry in 0.999914674 seconds\nI1019 14:05:58.205463826   21251 subchannel.c:609]           Failed to connect to channel, retrying\nI1019 14:05:58.205669074   21251 subchannel.c:609]           Failed to connect to channel, retrying\nI1019 14:05:58.205731798   21169 subchannel.c:642]           Connect failed: {\"created\":\"@1476911158.205675847\",\"description\":\"Failed to connect to remote host\",\"errno\":115,\"file\":\"external/grpc/src/core/lib/iomgr/error.c\",\"file_line\":256,\"os_error\":\"Connection refused\",\"target_address\":\"ipv6:[::1]:2229\"}\nI1019 14:05:58.205762949   21169 subchannel.c:647]           Retry in 1.913824142 seconds\nI1019 14:05:58.205855762   21169 subchannel.c:642]           Connect failed: {\"created\":\"@1476911158.205836202\",\"description\":\"Failed to connect to remote host\",\"errno\":115,\"file\":\"external/grpc/src/core/lib/iomgr/error.c\",\"file_line\":256,\"os_error\":\"Connection refused\",\"target_address\":\"ipv4:127.0.0.1:2229\"}\nI1019 14:05:58.205863899   21169 subchannel.c:647]           Retry in 1.385827127 seconds\n...\n```\n\n...so hopefully that helps!]\n", "@mbz, do you have any updates? What's your current status?\n", "I'm still working on this. Please keep the bug the open for a few more\ndays, I will come back with more details and feedbacks.\n\nOn Wed, Nov 2, 2016 at 11:57 AM, Andrew Selle notifications@github.com\nwrote:\n\n> @mbz https://github.com/mbz, do you have any updates? What's your\n> current status?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4932#issuecomment-257927112,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABEfT54i9XheGXyoU_obiNJopWt0uSFeks5q6MD3gaJpZM4KVYvU\n> .\n", "Closing due to lack of activity.", "Wanted to update (and close) this thread.\r\nIt turned out to be a DNS issue on our network.\r\n\r\nThank you for great help :)"]}, {"number": 4931, "title": "Couldn't open CUDA library libcuda.so.1 in when building using nvidia docker cuda8 image", "body": "We built tensorflow on nvidia-docker image nvidia/cuda:8.0-cudnn5-devel\n\"\"\"\nHowever, when using the package we built, we run into this problem:\nI tensorflow/stream_executor/dso_loader.cc:105] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n\"\"\"\nIn this special Dockerfile, we are using whatever LD_LIBRARY_PATH defined by nvidia docker.\n\nIt looks like we are looking specificly for the file libcuda.so.1\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L80\n\nAnd it looks like we switched to doing this due to:\nhttps://github.com/tensorflow/tensorflow/issues/2865\n\nHowever, libcuda.so.1 is not linked/made available by nvidia-docker in the docker image.\n\nSide note: it seems to be not available in our docker images based on 7.5, but that one somehow works.\nNo idea how.\n\n@flx42 could you help us shed some light into this?\n", "comments": ["@yifeif is also working on this\n", "Is this related to #4397?\n", "> However, libcuda.so.1 is not linked/made available by nvidia-docker in the docker image.\n\nThis file is only available at runtime when using `nvidia-docker`, if you don't see it, you have a problem.\n", "@jart: no, I believe this is different. In #4397, the issue is that the install path of cuDNN varies depending on how it was installed (deb or tarball).\n", "@flx42 When running nvidia-docker, I can verify that the following command does not return anything:\n$ find / -name libcuda.so.1\n\nHowever, in the host system I can verify that it is available.\n", "@gunan please file a bug against `nvidia-docker`, and include the output of `ls -R /var/lib/nvidia-docker/volumes/nvidia_driver/`\n\nThis is what I have:\n\n```\n$ nvidia-docker run -ti nvidia/cuda sh -c 'find / -name libcuda.so.1'\n/usr/local/nvidia/lib64/libcuda.so.1\n/usr/local/nvidia/lib/libcuda.so.1\n```\n", "I got one machine working!\nWas trying many different combinations, but finally using ubuntu 16.04, 361 driver from ubuntu repo, runfile installer to install just cuda toolkit, and nvidia cuda and cuda-devel image, and mucking with our Dockerfile, I got everything working.\n\nI got rid of the old VM before this report, but I will create the issue once I can re-reproduce the failure.\nThanks for your input, and very quick response Felix!\n"]}, {"number": 4930, "title": "Example MNIST_RNN not working", "body": "Hello everyone,\n\nI use the very last docker container, GPU Enabled (nvidia-docker). \nEverything works fine for what I could have tested so far.\n\nHost : Ubuntu 14.04 Server with CUDA and NVIDIA drivers up to date\n\nExcept, this example is not working anymore :\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist_rnn.py\n\n**AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'**\n\n![screen shot 2016-10-13 at 01 41 19](https://cloud.githubusercontent.com/assets/10923599/19331714/2fac0baa-90e6-11e6-8027-5f68dd10b160.png)\n", "comments": ["Is it possible that this issue is related to #3056?\n", "It seems like I am able to run the failing code :\n\n![capture_](https://cloud.githubusercontent.com/assets/10923599/19341349/5ba6d3ce-912d-11e6-8724-f0e620045106.PNG)\n\nSo I would say, not related. If you need me to run any code. Do not hesitate. You will find my running configuration in this Issue : #4929 \n\nThanks for the help !\n", "Just to clarify your recent comment, this is still broken for you, correct? I think the root cause with this issue and #4929 might be that there doesn't appear to be any tests for these examples. @baoblackcoal contributed this example, so maybe he/she can help.\n", "Hello, yes it is still broken. I was just referencing that I can't reproduce the bug you referenced in the issue #3056.\n", "Assigning to @ilblackdragon who is the skflow czar.\n", "@DEKHTIARJonathan Honestly, I had not tested mnist_rnn.py with GPU(I have not GPU environment :(). mnist_rnn.py work fine on my CPU version of Ubuntu14.4 and TF version is V0.10.0.  the following is log of python mnist_rnn.py\n\n> Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n> Extracting MNIST-data/train-images-idx3-ubyte.gz\n> Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n> Extracting MNIST-data/train-labels-idx1-ubyte.gz\n> Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n> Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n> Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n> Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n> WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpbmrvcx\n> WARNING:tensorflow:Using default config.\n> WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(784)]), is_sparse=False)\n> WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(10)]), is_sparse=False)\n> WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f3bc48e1e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n> WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f3b7e8d52d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n> Accuracy: 0.984300\n> Process finished with exit code 0\n", "@baoblackcoal is thereanything I can change to switch the script to CPU only and not using the GPU ?\nOr do I need to launch a CPU only Container ?\n", "AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape' .\n\nThe issue above that may need the help of @ilblackdragon .\n", "Sorry for late reply. We actually deleted this examples, as they are deprecated and untested. New examples will be `examples/learn`.\nNow in this case, what happen is that `rnn` function call now returns `StateTuple` instead of jsut value. If you want to fix this in your version of example, feel free to change this lines:\n\n```\n   # Define a GRU cell with tensorflow      \n   lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)       \n   # Get lstm cell output       \n   _, encoding = tf.nn.rnn(lstm_cell, X, dtype=tf.float32)      \n\n   return learn.models.logistic_regression(encoding, y)\n```\n\nto\n\n```\n   # Define a GRU cell with tensorflow      \n   lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)       \n   # Get lstm cell output       \n   _, state_tuple = tf.nn.rnn(lstm_cell, X, dtype=tf.float32)\n   encoding = tf.concat(1, [state_tuple.c, state_tuple.h])\n\n   return learn.models.logistic_regression(encoding, y)\n```\n\nHope this helps!\n"]}, {"number": 4929, "title": "Example Digits Not working", "body": "Hello everyone,\n\nI use the very last docker container, GPU Enabled (nvidia-docker). \nEverything works fine for what I could have tested so far.\n\nHost : Ubuntu 14.04 Server with CUDA and NVIDIA drivers up to date\n\nExcept, this example is not working anymore : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/digits.py\n\nCan't manage to make it work and understand the error : InvalidArgumentError: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered kernels:\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_HALF]\n\nI have uploaded a screenshot of the error:\n\n![screen shot 2016-10-13 at 01 36 23](https://cloud.githubusercontent.com/assets/10923599/19331543/83dea378-90e5-11e6-9a7b-27920d47f013.png)\n", "comments": ["What version of CUDA are you using?\n", "Here is my version of CUDA on the host server:\n\n``` bash\n[11:42 jonathan@gpu-linux ~] > apt-show-versions cuda\n**cuda:amd64/unknown 7.5-18 uptodate**\n```\n\nI installed it the following way : \n\n``` bash\nsudo apt-get install -y gcc g++ gfortran build-essential \\\n  git wget linux-image-generic libopenblas-dev python-dev \\\n  python-pip python-nose python-numpy python-scipy\n\n# downloading the (currently) most recent version of CUDA 7.5\nsudo wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64/cuda-repo-ubuntu1404_7.5-18_amd64.deb\n\n# installing CUDA\nsudo dpkg -i cuda-repo-ubuntu1404_7.5-18_amd64.deb\n\nsudo apt-get update\nsudo apt-get install cuda\n\n# setting the environment variables so CUDA will be found\necho -e \"\\nexport PATH=/usr/local/cuda/bin:$PATH\" >> .bashrc\necho -e \"\\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64\" >> .bashrc\n```\n\nI am running with the official NVIDIA-Docker image, GPU Enable:\n\n![capture_](https://cloud.githubusercontent.com/assets/10923599/19341014/f976f27a-912b-11e6-8ae6-cb63f070f22b.PNG)\n\nVery last Docker Images available on the GCR and Docker Registry\n\nThe output given by \"nvidia-smi\":\n\n``` bash\nroot@d69960a36aa5:/notebooks# nvidia-smi\nThu Oct 13 09:50:08 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.99     Driver Version: 352.99         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |\n| 22%   38C    P8    15W / 250W |     23MiB / 12284MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   31C    P8    14W / 250W |    135MiB / 12284MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n```\n\nDriver Version on the nvidia-docker machine : \n\n``` bash\nroot@d69960a36aa5:/notebooks# cat /proc/driver/nvidia/version\nNVRM version: NVIDIA UNIX x86_64 Kernel Module  352.99  Mon Jul  4 23:52:14 PDT 2016\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04.3)\n```\n\nI hope everything will be clear enough \n", "Maybe it is related to the error I have here to #4930 \n", "@theweiho since this example uses contrib.learn, could you help us fix it? And write a regression test for it?\n", "@jart I will try to test by the end of the week, as many ipynb examples as I can. Many of them seems to be not working. \nDo you prefer that I open a different issue, for each failing Notebook Example or I create a main Issue and I list all the failing Notebooks inside it ?\n\nThanks for your help.\n", "It depends on how many examples are broken. We can probably figure out the triaging as we go along.\n", "I think it's complaining that there's no OpKernel registered for Conv2D that supports double on GPU. \n\n@benoitsteiner you reviewed the last change that touched https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L639 - is there any reason why double isn't supported?\n\nIf this is something that used to work but doesn't anymore, then maybe something got flipped from float to double recently.\n\n@zhangyaobit you touched tf.contrib.layers.conv2d() in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L659 recently - any idea if something changed from float to double?\n", "Closing this issue now, as digits.py seems to have been deprecated/removed.\n"]}, {"number": 4928, "title": "When swig is not available, ./configure exits silently", "body": "### Environment info\n\nOperating System:\nUbuntu 16.04 \n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\ncuda 8.0, cudnn does not matter, installation issue\n\nIf installed from binary pip package, provide:\nInstalling from sources\n1. The commit hash (`git rev-parse HEAD`) 55bd9ccdb21a41b7c4046a1c1f2115066ed1f9bc\n2. The output of `bazel version` 0.3.2\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\ngit clone github.com/tensorflow/tensorflow\n./configure\n\nexits before full config  is complete silently.\n\n@caisq  @davidzchen any ideas?\n", "comments": ["The problem is that if swig isn't found, we fail immediately and the following error handling code never executes. Further, since we are redirecting stderr (so that we can print a more helpful error message), if the command fails, it fails silently:\n\n``` bash\nSWIG_PATH=`type -p swig 2> /dev/null`\n```\n\nOpened #4948 to fix this.\n"]}, {"number": 4927, "title": "Branch 135971493", "body": "", "comments": ["@rohan100jain, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @dsmilkov and @keveman to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "@martinwicke Any reason why we are removing `DNNSampledSoftmaxClassifier` from open-source version? https://github.com/tensorflow/tensorflow/pull/4927/commits/b309148f5e9a6766bfc6f68972138a52d095cd5f\n", "We found out that to actually work properly we would have had to use some tooling that we haven't done the work to opensource yet. We will still do that.\n", "I see. Thanks!\n\nOn Thursday, October 13, 2016, Martin Wicke notifications@github.com\nwrote:\n\n> We found out that to actually work properly we would have had to use some\n> tooling that we haven't done the work to opensource yet. We will still do\n> that.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4927#issuecomment-253630362,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AEEnSsI1WxknYb-e55pc_yp58pmftLo0ks5qzpXLgaJpZM4KVUDp\n> .\n"]}, {"number": 4926, "title": "unable to build tensorflow for GPU", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n..\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nWARNING: /home/ammalik/TensorFlow/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nINFO: Found 1 target...\nERROR: /home/ammalik/.cache/bazel/_bazel_ammalik/27908a9a0ff0347c2aebe63a8fa99002/external/zlib_archive/BUILD:7:1: undeclared inclusion(s) in rule '@zlib_archive//:zlib':\nthis rule is missing dependency declarations for the following files included by 'external/zlib_archive/zlib-1.2.8/compress.c':\n  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include-fixed/limits.h'\n  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include-fixed/syslimits.h'\n  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include/stddef.h'\n  '/opt/local/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0/include/stdarg.h'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 962.954s, Critical Path: 0.69s\n", "comments": ["You'll get more sympathy if you actually use the template instead of just quoting it, but I believe your problem is related to/helped by https://github.com/tensorflow/tensorflow/issues/3431#issuecomment-234131699. \n\nIt that doesn't work: your gcc is a custom install, right? You'll probably have it easier using your distributions standard builchain.\n", "Are you using CUDA 8.0? Right now we only support 7.0 - 7.5. But we're actively working on supporting 8.0. See #2559. \n\n@davidzchen is it possible setting the CUDA toolchain is what caused the \"undeclared inclusion\" error with zlib? Possible duplicate of #2109.\n", "When building natively, I can build using cuda8 without any problems.\nBut this issue report is missing too much information.\n\nIs it possible reporter is missing either the prerequisites:\n$ sudo apt-get install python-numpy swig python-dev python-wheel\n\nOr forgot running ./configure before building?\n", "Closing due to inactivity. Feel free to reopen if this is still an issue (and please add more information as requested in the comments above).\n"]}, {"number": 4925, "title": "Minor copyright year update", "body": "Just came across this. Not sure if the change is necessary though.\n", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @davidsoergel and @tensorflower-gardener to be potential reviewers.\n"]}, {"number": 4924, "title": "Using xavier_initializer causes pyglet to fail to display", "body": "Using tf.contrib.layers.xavier_initializer causes pyglet GLX initialization (and therefore gym) to fail. The stack trace is attached. It is not at all clear how calling xavier_initializer can cause GLX initialization to fail.\n\n```\n$ python test.py # See test.py below.\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n[2016-10-12 14:09:54,402] Making new env: Pong-v0\nTraceback (most recent call last):\n  File \"test.py\", line 11, in <module>\n    env.render()\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/gym/core.py\", line 192, in render\n    return self._render(mode=mode, close=close)\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/gym/envs/atari/atari_env.py\", line 119, in _render\n    from gym.envs.classic_control import rendering\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/gym/envs/classic_control/rendering.py\", line 23, in <module>\n    from pyglet.gl import *\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/__init__.py\", line 236, in <module>\n    import pyglet.window\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/window/__init__.py\", line 1816, in <module>\n    gl._create_shadow_window()\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/__init__.py\", line 205, in _create_shadow_window\n    _shadow_window = Window(width=1, height=1, visible=False)\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/window/xlib/__init__.py\", line 163, in __init__\n    super(XlibWindow, self).__init__(*args, **kwargs)\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/window/__init__.py\", line 504, in __init__\n    config = screen.get_best_config(template_config)\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/canvas/base.py\", line 161, in get_best_config\n    configs = self.get_matching_configs(template)\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/canvas/xlib.py\", line 179, in get_matching_configs\n    configs = template.match(canvas)\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/xlib.py\", line 29, in match\n    have_13 = info.have_version(1, 3)\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/glx_info.py\", line 86, in have_version\n    client_version = self.get_client_version().split()[0]\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/gl/glx_info.py\", line 118, in get_client_version\n    return asstr(glXGetClientString(self.display, GLX_VERSION))\n  File \"/home/jay/.virtualenvs/dqn-tf/lib/python3.4/site-packages/pyglet/compat.py\", line 88, in asstr\n    return s.decode(\"utf-8\")\nAttributeError: 'NoneType' object has no attribute 'decode'\n```\n\nHere is my environment information.\n\nOperating system: Ubuntu 14.04.5\nCUDA 7.5 and CuDNN 5.1.3 See [cuda-versions.txt](https://github.com/tensorflow/tensorflow/files/525655/cuda-versions.txt) for details.\nPython 3.4, Gym 0.4.2, Pyglet 1.2.4\nTensorFlow 0.11.0rc0 installed from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl.\n\nHere is some minimal code that exhibits the problem (this is the test.py that causes the stack trace above.) Everything is fine until the random call tries to set up GLX.\n\n```\nimport gym\nimport random\nimport tensorflow as tf\n\nxavier_init = tf.contrib.layers.xavier_initializer()\n\nenv = gym.make('Pong-v0')\nenv.reset()\nfor _ in range(90):\n    env.step(random.randint(0, env.action_space.n - 1))\n    env.render()\n```\n\nCommenting the xavier_init line fixes the problem.\n", "comments": ["Unfortunately, we don't have the bandwidth to support code in the contib packages, so you might want to ask about this on stack overflow. Looking at `xavier_initializer()`, I can't immediately see why it affects `gym` or `pyglet` as it does not depend on them.\n\nSince we don't have the bandwidth to diagnose interactions with `contrib` code, I'm going to close this issue.\n\nThat said, looking at the stack trace you printed, it seems you're using python 3.4 and the exception was raised in [`pyglet/compat.py:88`](https://bitbucket.org/pyglet/pyglet/src/62e91545b98ecd0d087c5d01ed061ed52ccf9d97/pyglet/compat.py?at=default&fileviewer=file-view-default#compat.py-88). Looking at pyglet sources, this suggests that whatever version of pyglet you're using is missing a fix made to work with python3 (see [this diff](https://bitbucket.org/pyglet/pyglet/diff/pyglet/compat.py?diff2=1ed43761e826&at=default) and [commit description](https://bitbucket.org/pyglet/pyglet/commits/1ed43761e826b545b89207787b7afa21c7198c32)).\n\nSo I would suggest that you try updating pyglet.\n\nHope that helps and best of luck!\n"]}, {"number": 4923, "title": "Tutorial on contrib (load_csv) no longer works, please suggest solution", "body": "I try to follow the turotial here:\n\nhttps://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html\n\nBut I get the error:\n\n221:tensorflow_tutorial ME$ python contrib_learn_quick_origin.py \nTraceback (most recent call last):\n  File \"contrib_learn_quick_origin.py\", line 13, in <module>\n    training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,\nAttributeError: 'module' object has no attribute 'load_csv'\n\nPlease suggests solutions and edit the online tutorial.\n", "comments": ["Thank you for reporting this. I wrote up a change which will get approved soon fixing this. Once it gets pushed to GitHub this issue will be closed.\n", "Hi, \n\nim afraid it still doesnt work with the changes. The new method takes 3 arguments instead of 2. This is how i made it work:\n`training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n       filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)\n test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n       filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)`\n\nEDIT: Ive tried to change it according to your commit.\n", "Also, the example files don't exist, so even changing the code will fail -- the files `iris_test.csv` and  `iris_train.csv` need to be committed.\n", "Hey,\n\nYou can see the changed file and the .csv files here at the [TensorFlow github page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/monitors):\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/monitors\n\nDownload the files to a folder and change:\n\n```\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\n\nto\n\n# Data sets\nIRIS_TRAINING = \"<path_to_file>/iris_training.csv\"\nIRIS_TEST = \"<path_to_file>/iris_test.csv\"\n```\n", "If you read through the tutorial, it provides the datasets and says to download into the same file as your python script."]}, {"number": 4922, "title": "Android: read model file from arbitrary directory rather than from assets folder", "body": "Currently, TensorFlowInferenceInterface's method initializeTensorFlow() and its underlying C implementation (in tensorflow_inference_jni.cc and jni_utils.cc) require the usage of Android's AssetManager in order to read model files stored in the assets folder. \n\nHow can we modify TF's code to provide an alternative way to run the classification using a model file that's not stored in the assets folder (i.e. a location described by a File object, or by a String path pointing to another location outside the assets folder).\n\nIs there branch where someone might be working on this? I have poor C skills so as to make such change by myself, but maybe I can work on something with some guidance.\n\nThanks.\n\nBruno.\n", "comments": ["If you remove the `file:///android_asset/` prefix from the model file in [TensorFlowImageListener.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageListener.java), it should just work. There already exists code that can load from non-assets in the native C++ layer. If that prefix is not present it will attempt to treat the location as a regular filepath.\n\nThe label file in the example is a different matter, but this is not difficult to fix in [TensorFlowImageClassifier.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java#L89). I have a TODO open for this already, and will update the code soon to be more flexible.\n", "Hi Andrew. It works pretty well using the instructions above. Thanks for the help!\n\nBruno.\n", "Sir I am trying the same thing as above but could not program it. \r\nCould you please share your code\r\n@andrewharp ", "> Sir I am trying the same thing as above but could not program it.\r\n> Could you please share your code\r\n> @andrewharp\r\n\r\nInstead of using the AssetFileDescriptor use it like this, pass the full file path in load model method\r\n\r\n```\r\nprivate MappedByteBuffer loadModelFile(String modelPath) throws IOException {\r\ntry (FileInputStream fileInputStream = new FileInputStream(\r\n                new File(modelPath))) {\r\n            FileChannel fileChannel = fileInputStream.getChannel();\r\n            return fileChannel.map(MapMode.READ_ONLY, 0, fileChannel.size());\r\n        }\r\n    }\r\n```"]}, {"number": 4921, "title": "FileSystem::GetMatchingPaths doesn't work for simple pattern", "body": "With latest tensorflow.\n\n``` bash\n$ ls\ntest\n```\n\n``` cpp\nstd::vector<string> results;\nauto ret = tensorflow::Env::Default()->GetMatchingPaths(\"te*\", &results);\ncout << results.size() << endl;  // 0\nret = tensorflow::Env::Default()->GetMatchingPaths(\"./te*\", &results);\ncout << results.size() << endl;  // 1\n```\n\nReason:\nIn the implementation of `GetMatchingPaths` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.cc#L94), when the pattern doesn't contain any directory, the prefix `.` is prepended to all elements in `all_files`, and then `MatchPath(\"./test\", \"te*\")` evals to false.\n", "comments": ["Maybe there's something about the design we're not considering, but that seems counter-intuitive enough that maybe @rohan100jain could help clarify.\n", "Coming from the linked issue #4913 I wonder if https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.cc#L104 leads to the reported problem with `/tmp`. More specifically, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.cc#L118 possibly adds non-readable directories below `/tmp` which then should throw the `PermissionDeniedError`. \n", "Fixed in 3e6c638727c3274908a7c9c6bbf4474c014511fe half a year ago."]}, {"number": 4920, "title": "Better support for initializing variables that depend on each other", "body": "Currently there's no easy way to properly initialize variables when some variables initial values depend on other variables, and initialization has to be split over several `.run` calls. This kind of initialization happens with data-dependent parameter init.\n\nThis could be solved if there were something like `var.initialized_value()`, but which only runs initializer if the variable hasn't been initialized already.\n\nExample:\n\n```\na = tf.get_variable(\"a\", shape=())\nb = tf.get_variable(\"b\", initializer=a.initialized_value())\nc = tf.placeholder(tf.float32, shape=())\nd = tf.get_variable(\"d\", initializer=a.initialized_value()+c)\n\nsess.run([a.initializer, b.initializer])\nsess.run([d.initializer], feed_dict={c: 0})\nsess.run([a, b, d])\n\nOut[]: [0.30858743, -1.2756943, 0.30858743]\n```\n\nHere, `a` and `b` end up with different values because initializer for `a` has been run twice, which is counter-intuitive, the user expects `a,b,d` to have same values\n", "comments": ["BTW, here's a [work-around](https://gist.github.com/yaroslavvb/d592394c0cedd32513f8fbb87ca05938) using @purpledog graph editor + `tf.is_variable_initialized()` to make `initialize_all_variables` work the same way whether or not `initialized_value` was used, and which works when initialization is split over several session runs. It works by adding temporary dependency on undocumented \"variable caching node\" `<varname>/read`. A more robust approach might be to add `initializer_once` to variables, which is like `initializer` but is only has effect when variable is not initialized, and add `tf.initialize_all_variables_once` that will initialize all uninitialized variables in correct order\n", "@yaroslavvb Would you mind me fixing the bug? I'm working around the`initialization` this day. @drpngx \n", "I'm not on TensorFlow team, but since it's \"Contributions Welcome\" it sounds like they'd be open to a fix. Maybe a way to start would be with `initializer_once` construct that checks if variable is initialized before running\n", "@yaroslavvb  You said initializer of `a` has been run twice, do you mean `b` and `d` both call `a.initialized_value()`? Then it makes `d == a` in the second call, while `b` equals the first call of `a`, isn't it?\n", "Thanks for your advise on `initilzier_once`.\nWhile I find a interesting output of your example by using `tf.initialize_all_variables()`, it works well.\n\n```\na = tf.get_variable(\"a\", shape=())\nb = tf.get_variable(\"b\", initializer=a.initialized_value())\nc = tf.placeholder(tf.float32, shape=())\nd = tf.get_variable(\"d\", initializer=a.initialized_value()+c)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables(), feed_dict={c: 0})\nsess.run([a, b, d])\n\nOut[1]:  [1.1102008, 1.1102008, 1.1102008]\n```\n", "Which version?\nI tried in 0.10 and 0.11rc1 with same result:\n\nimport tensorflow as tf\na = tf.get_variable(\"a\", shape=())\nb = tf.get_variable(\"b\", initializer=a.initialized_value())\nc = tf.placeholder(tf.float32, shape=())\nd = tf.get_variable(\"d\", initializer=a.initialized_value()+c)\nsess = tf.Session()\nsess.run([a.initializer, b.initializer])\nsess.run([d.initializer], feed_dict={c: 0})\nsess.run([a, b, d])\n\nOut[1]: [1.5315183, -0.65960622, 1.5315183]\n", "Do you tried the `sess.run(tf.initialize_all_variables(), feed_dict={c: 0})`?\nIf you initialize them by `tf.initialize_all_variables()` at once, then you'll get the right outputs.\n", "Correct, this issue only happens when when initialization has to be split over several .run calls. Such use case arises when you need to run some data through parts of your graph in order to determine initial value for variables in another part (ie, in data-dependent initializations)\n", "But I think TF should support separate initializations for every `variable`. At least, it shouldn't give the wrong `initialized_value` as @yaroslavvb mentioned. What do you think, @drpngx ? Do we need to open a PR to support that?  \n", "Yes, I think it would help.\n", "Are there any PRs associated with this?\r\n\r\nI've written code several times that does something like\r\n```python\r\nx = tf.placeholder(tf.float32, [100])\r\nv0 = tf.get_variable('v0', initializer=tf.zeros_like(x))\r\nz = x * v0\r\nv1 = tf.get_variable('v1', initializer=tf.zeros_like(z))\r\n```\r\nThis cannot be initialized with ``tf.initialize_all_variables()`` because ``v1``'s initializer depends on ``v0`` (but only the shape). I've hacked around this by writing a ``static_zeros_like`` function that only uses shape info from graph definition time (``tensor.get_shape().as_list()``).\r\n\r\nIt seems like the clean solution is to do a toposort on the graph consisting of all of the predecessors of any initializer. Executing the \"initializer graph\" nodes in this toposorted order will guarantee safe initialization.", "HI @yaroslavvb \r\n\r\nI'm using the smart_initialize, but I found it re-do the initialization each time you call the session run.\r\n\r\nSeem the edge or something like is not removed. I checked that the control_inputs are actually cleared, but there should be something else. I'm not very familiar with tensor-flow. Do you have any idea what is going on?\r\n\r\nThanks.\r\n", "Can you provide a reproducible example? I'm assuming you are referring to smart_initialize from the gist: https://gist.github.com/yaroslavvb/d592394c0cedd32513f8fbb87ca05938", "Hi, here is a example:\r\n\r\n```\r\n# testing variable order init\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef initialize_all_variables(sess=None):\r\n    \"\"\"Initializes all uninitialized variables in correct order. Initializers\r\n    are only run for uninitialized variables, so it's safe to run this multiple\r\n    times.\r\n    Args:\r\n        sess: session to use. Use default session if None.\r\n    \"\"\"\r\n\r\n    from tensorflow.contrib import graph_editor as ge\r\n    def make_initializer(var):\r\n        def f():\r\n            return tf.assign(var, var.initial_value).op\r\n\r\n        return f\r\n\r\n    def make_noop():\r\n        return tf.no_op()\r\n\r\n    def make_safe_initializer(var):\r\n        \"\"\"Returns initializer op that only runs for uninitialized ops.\"\"\"\r\n        return tf.cond(tf.is_variable_initialized(var), make_noop, make_initializer(var), name=\"safe_init_\" + var.op.name).op\r\n\r\n    if not sess:\r\n        sess = tf.get_default_session()\r\n    g = tf.get_default_graph()\r\n\r\n    safe_initializers = {}\r\n    for v in tf.all_variables():\r\n        safe_initializers[v.op.name] = make_safe_initializer(v)\r\n\r\n    # initializers access variable vaue through read-only value cached in\r\n    # <varname>/read, so add control dependency to trigger safe_initializer\r\n    # on read access\r\n    for v in tf.all_variables():\r\n        var_name = v.op.name\r\n        var_cache = g.get_operation_by_name(var_name + \"/read\")\r\n        ge.reroute.add_control_inputs(var_cache, [safe_initializers[var_name]])\r\n\r\n    sess.run(tf.group(*safe_initializers.values()))\r\n\r\n    # remove initializer dependencies to avoid slowing down future variable reads\r\n    for v in tf.all_variables():\r\n        var_name = v.op.name\r\n        var_cache = g.get_operation_by_name(var_name + \"/read\")\r\n        ge.reroute.remove_control_inputs(var_cache, [safe_initializers[var_name]])\r\n\r\n\r\n################################################################################\r\n# Tests\r\n################################################################################\r\n\r\ndef my_test():\r\n\r\n    def linear_wn(input, output_size, stddev=0.05):\r\n\r\n        def linear_wn_initializer_g(v, init_scale=1.0):\r\n            v_norm = tf.nn.l2_normalize(v, [0])\r\n            x_init = tf.matmul(input, v_norm)\r\n            m_init, v_init = tf.nn.moments(x_init, [0])\r\n            scale_init = init_scale / tf.sqrt(v_init + 1e-10)\r\n            scale_init = tf.Print(scale_init, [0], 'linear ini running')\r\n            return scale_init\r\n\r\n        def linear_wn_initializer_b(v, init_scale=1.0):\r\n            v_norm = tf.nn.l2_normalize(v, [0])\r\n            x_init = tf.matmul(input, v_norm)\r\n            m_init, v_init = tf.nn.moments(x_init, [0])\r\n            scale_init = init_scale / tf.sqrt(v_init + 1e-10)\r\n            return -m_init * scale_init\r\n\r\n        if len(input.get_shape()) > 2:\r\n            input = tf.reshape(input, [input.get_shape().as_list()[0], -1])\r\n\r\n        v = tf.get_variable('v', [input.get_shape().as_list()[1], output_size], initializer=tf.truncated_normal_initializer(stddev=stddev), trainable=True)\r\n        g = tf.get_variable('g', dtype=tf.float32, initializer=linear_wn_initializer_g(v.initialized_value()), trainable=True)\r\n        b = tf.get_variable('b', dtype=tf.float32, initializer=linear_wn_initializer_b(v.initialized_value()), trainable=True)\r\n\r\n        x = tf.matmul(input, v)\r\n        scaler = g / tf.sqrt(tf.reduce_sum(tf.square(v), [0]))\r\n        x = tf.reshape(scaler, [1, output_size]) * x + tf.reshape(b, [1, output_size])\r\n\r\n        return x\r\n\r\n    input = tf.get_variable('input', [1, 100], initializer=tf.truncated_normal_initializer(stddev=1.0), trainable=True)\r\n    output = linear_wn(input, 10)\r\n\r\n    sess = tf.InteractiveSession()\r\n    initialize_all_variables(sess)\r\n\r\n    opt = tf.train.GradientDescentOptimizer(learning_rate=0.0001).minimize(output)\r\n\r\n    while True:\r\n        sess.run([opt])\r\n\r\nif __name__ == '__main__':\r\n    my_test()\r\n\r\nprint(\"Tests passed\")\r\n```\r\n\r\nIt will output:\r\n\r\n```\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] linear ini running[0]\r\n................\r\n```", "@ZhimingZhou thanks for the easy to reproduce test case. It seems the semantics of variables changed. I would add dependency on \"variable/read\" node to force conditional initialization subgraph to run when \"variable\" is being read. But now this no longer works. I tried adding dependency on `var.read_value()`, but that creates new nodes on each evaluation.\r\n\r\n@alextp is there some way to force some op to run when variable is being read?", "Use an identity node with control dependencies instead of the value of the\nvariable itself.\n\nOn Jan 17, 2017 22:58, \"Yaroslav Bulatov\" <notifications@github.com> wrote:\n\n> @ZhimingZhou <https://github.com/ZhimingZhou> thanks for the easy to\n> reproduce test case. It seems the semantics of variables changed. I would\n> add dependency on \"variable/read\" node to force conditional initialization\n> subgraph to run when \"variable\" is being read. But now this no longer\n> works. I tried adding dependency on var.read_value(), but that creates\n> new nodes on each evaluation.\n>\n> @alextp <https://github.com/alextp> is there some way to force some op to\n> run when variable is being read?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273398951>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxT8nbwuEu_hucID6iTDSMbj_LNDOks5rTbgrgaJpZM4KVHy9>\n> .\n>\n", "@ZhimingZhou here's a fixed version using @alextp  suggestion -- http://pastebin.com/5zqcc5Q9\r\n\r\nThis needs a bit more thought though, I don't like the idea of users needing to keep separate tensors for variable write and variable read. Ideally one would be able to do `a=tf.Variable(b+c)` and have this work, running regardless of whether `b,c` is a tensor or a variable, with initialization order being handled automatically", "@yaroslavvb I think the nicest solution here is having a toposorted list of the variables, and then making a ``Session.run`` call for each variables initializer. It might be possible to avoid the overhead of a bunch of ``Session.run`` calls by making a chain of control dependencies between initializers, but I doubt it since it appears data dependencies aren't obeyed during initialization. \r\n\r\nI was starting to write a toposort routine, but I realized the graph is constructed with ancestors first and the TF collections are appended to, which means ``tf.global_variables`` is already toposorted. This is a little bit of hack since it's not specified that TF collections are appended to, but I think it will always work to do something like\r\n```\r\nfor v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\r\n    sess.run(v.initializer)\r\n```\r\n\r\nI'm running TensorFlow 0.10, but here's a non-trivial dependent initialization that works with this approach:\r\n```\r\nx = tf.Variable(np.random.randn(5), name='x')\r\ny = tf.Variable(x, name='y')\r\nz = tf.Variable(tf.zeros_like(x) + y, name='z')\r\nw = tf.Variable(2 * z, name='w')\r\n\r\nwith tf.Session() as sess:\r\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES):\r\n        sess.run(v.initializer)\r\n```\r\n\r\nIt seems that the larger problem (why these workarounds are needed) is that ``Session`` treats initializers (or maybe just running Ops instead of Tensors) differently and doesn't obey data or control dependencies.", "If you want variables' initializations to depend on each other you can use\nvar1.initialized_value in var2's initializer and this should work with the\nnormal tf.global_variables_initializer\n\nOn Wed, Jan 18, 2017 at 9:50 AM, Eric Martin <notifications@github.com>\nwrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> I think the nicest solution\n> here is having a toposorted list of the variables, and then making a\n> Session.run call for each variables initializer. It might be possible to\n> avoid the overhead of a bunch of Session.run calls by making a chain of\n> control dependencies between initializers, but I doubt it since it appears\n> data dependencies aren't obeyed during initialization.\n>\n> I was starting to write a toposort routine, but I realized the graph is\n> constructed with ancestors first and the TF collections are appended to,\n> which means tf.global_variables is already toposorted. This is a little\n> bit of hack since it's not specified that TF collections are appended to,\n> but I think it will always work to do something like\n>\n> for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n>     sess.run(v.initializer)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273548257>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQQrxYc_4IBYyapNwnNr2hpxA0ajks5rTlDugaJpZM4KVHy9>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp the problem with `initialized_value` is that it reruns initializers. This causes inconsistent results when your variable initialization has to be split over several `session.run` calls.\r\n\r\nA different snag is this situation -- your code does calculations based on Tensor `a`. Later you decide use `Variable` for `a` instead of `Tensor`. Now you have to go through your code and replace some instances of `a` with `a.initialized_value()`\r\n\r\n@eamartin -- I think  that's a reasonable replacement. First of all, overhead of `session.run` is something like 100 usec, and you probably won't have more than 1000 layers in your topological sort. Secondly, I think you could collapse it into a single `.run` call by using `tf.while` loop", "@alextp \r\nThat breaks the (useful) abstraction of not having to worry about whether a tensor is a variable or just a usual tensor.\r\n\r\nUsing ``initialized_value`` isn't very nice for cases like\r\n```\r\nx = tf.Variable(5.0)\r\ny = x + 1.0\r\nz = tf.Variable(tf.zeros_like(y))\r\n```\r\nBeyond this, you can imagine dependent initialization spread out much further across a graph. I can't set ``y = x.initialized_value() + 1.0`` because I want ``y = x + 1.0`` on later graph runs (after I mutate ``x``). Using ``initialized_value`` would require separate code paths (/ subgraphs) for initializing and running the graph.\r\n\r\nI think I understand the problem with the default behavior. In my case, ``z``'s initializer has a data dependency that goes back to ``x``. However, ``x`` is still uninitialized at this time, which causes an error. I attempted to work around this by feeding in the ``initialized_value`` for ``variable.value()``, but this isn't a workaround because you can't always evaluate ``initialized_value`` if the initializer depends on other non-initialized variables. If there was a mode of ``Session.run`` which would default to initial variable values, then this strategy could work. Sadly, this cannot be done with ``tf.select`` and an \"init_mode\" placeholder because ``tf.select(true, v.initialized_value(), v)`` fails for uninitialized ``v``.", "You can also use variable.initial_value, it returns the tensor which will\nbe used to initialize the variable.\n\nOn Wed, Jan 18, 2017 at 10:44 AM, Eric Martin <notifications@github.com>\nwrote:\n\n> That breaks the (useful) abstraction of not having to worry about whether\n> a tensor is a variable or just a usual tensor.\n>\n> Using initialized_value isn't very nice for cases like\n>\n> x = tf.Variable(5.0)\n> y = x + 1.0\n> z = tf.Variable(tf.zeros_like(y))\n>\n> Beyond this, you can imagine dependent initialization spread out much\n> further across a graph. I can't set y = x.initialized_value() + 1.0\n> because I want y = x + 1.0 on later graph runs (after I mutate x). Using\n> initialized_value would require separate code paths (/ subgraphs) for\n> initializing and running the graph.\n>\n> I think I understand the problem with the default behavior. In my case, z's\n> initializer has a data dependency that goes back to x. However, x is\n> still uninitialized at this time, which causes an error. I attempted to\n> work around this by feeding in the initialized_value for variable.value(),\n> but this isn't a workaround because you can't always evaluated\n> initialized_value if the initializer depends on other non-initialized\n> variables. If there was a mode of Session.run which would default to\n> initial variable values, then this strategy could work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-273562777>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYySDqYcYsqJb5uZFmaIGDTuRdhJks5rTl11gaJpZM4KVHy9>\n> .\n>\n\n\n\n-- \n - Alex\n", "@eamartin actually, maybe easier solution than toposort with multiple session.run calls is to have a wrapper around variable that recovers previous behavior (separate variable/read op which you can use to trigger initialization on read). Then you could do var = wrap_variable(var), and that would give you an op that runs variable initialization first time it's read using `tf.cond(var.is_initialized)`", "@yaroslavvb Thank for the new version. But I found it still re-do the initialization. There seems still have some modifications to the graph.  \r\n\r\n`BTW. The assert on 'unique tf.identity' seems not hold. In my case, some node have more than one identity. And adam optimizer's moving mean and variance parameter have zero consumer. [Strange, but it should be the case, according to the debug info]`\r\n\r\n@alextp @eamartin @yaroslavvb \r\nI still have not found a efficient way to do the initialization, when it depends input data, and involving date flow. For example:\r\n```\r\n@eamartin \r\nx = tf.Variable(5.0)\r\ny = x + 1.0\r\nz = tf.Variable(tf.zeros_like(y))\r\n```\r\ninitial_value or initialized_value() seems not suitable when involving data manipulation outside variable initializer. But this is essential when people want to do data dependent initialization.\r\n\r\nThe following works, but it could be too slow. It rerun the graph for each variable.\r\n```\r\n@eamartin \r\nfor v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\r\n    sess.run(v.initializer)\r\n```\r\n\r\nAs you mentioned, @eamartin @yaroslavvb, I guess a simple workaround solution is run the variable initialization in order with one sess.run(). I tried but failed to finish it. For example, I tried adding all the variable in order, but it seems not work. Could you help? Thanks.", "I tried something like that strategy using tf.select (on a bool placeholder\nindicating whether or not to initialize). This failed because the select op\nwasn't happy with a non initialized variable as input.\n\nCan tf.cond handle a non-initialized variable input?\n\nI like this approach (assuming it works) as it uses a single sess.run call.\nThe downside is that it requires a custom variable creation function and\ninitialization is implicit rather than explicit.\n\n\nOn Jan 20, 2017 20:23, \"Yaroslav Bulatov\" <notifications@github.com> wrote:\n\n@eamartin <https://github.com/eamartin> actually, maybe easier solution\nthan toposort with multiple session.run calls is to have a wrapper around\nvariable that recovers previous behavior (separate variable/read op which\nyou can use to trigger initialization on read). Then you could do var =\nwrap_variable(var), and that would give you an op that runs variable\ninitialization first time it's read using tf.cond(var.is_initialized)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-274230277>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AARh4IH3LDmts0sFXwzUALYRBwRZInuMks5rUXo2gaJpZM4KVHy9>\n.\n", "Please take a look at the new behavior of Variable.initialized_value (introduced in https://github.com/tensorflow/tensorflow/commit/b25d1c7d3e9f30925aa132ba62e79d281191a3dc ). Now it doesn't force initialization, so it can be safely used when initializing variables from other variables.\r\n\r\nThe only overhead of using this instead of the raw Variable is that it will trigger recomputing the initializer in the beginning of the step even if the variable has already been initialized (this is mildly annoying to fix but it's doable).\r\n\r\nSo it should be kosher to use both in a single and in multiple session.run calls to initialize stuff.", "Progress!\r\n\r\nBTW, we use Variables that depend on each other, and some are initialized from placeholders. Recomputing the initializer means you have to feed placeholder values that aren't getting used.\r\n\r\nAs a work-around I've been recommending the pattern below which uses graph_editor + Switch + Merge to rewrite the graph to make initializer execution lazy.\r\n\r\nhttps://gist.github.com/yaroslavvb/d67410e240369736fc4ba0267250ef27", "Interesting. FWIW, replacing placeholder with constant should allow you to\nnot rerun initialization (at the expense of abandoning the error message).\n\nOn Thu, Mar 23, 2017 at 2:08 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> Progress!\n>\n> BTW, we use Variables that depend on each other, and some are initialized\n> from placeholders. Recomputing the initializer means you have to feed\n> placeholder values that aren't getting used.\n>\n> As a work-around I've been recommending the pattern below which uses\n> graph_editor + Switch + Merge to rewrite the graph to make initializer\n> execution lazy.\n>\n> https://gist.github.com/yaroslavvb/d67410e240369736fc4ba0267250ef27\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4920#issuecomment-288860068>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdsYnQs4lBYK1rnTdpKTum3BrECPks5rot9agaJpZM4KVHy9>\n> .\n>\n\n\n\n-- \n - Alex\n", "@yaroslavvb Is this resolved as of https://github.com/tensorflow/tensorflow/commit/b25d1c7d3e9f30925aa132ba62e79d281191a3dc?", "Closing for now since it looks like yes, but happy to reopen if not.", "I think this should be reopened. Initializing variables from other variables that depend on placeholders is still really difficult. It would be ideal if it were achievable in a single run call, without depending on the graph editor method that @yaroslavvb provided. Although the behavior of initialized_value changed so that it uses tf.cond to decide whether or not to run the initializer, this is still not ideal for cases such as below:\r\n\r\n```\r\n    sess = tf.InteractiveSession()\r\n\r\n    x = tf.placeholder(tf.int32, (), name='X')\r\n    a = tf.Variable(x, name='A', trainable=False)  # First cached variable (depends on placeholder)\r\n    b = a + 1  # Pretend this is a huge amount of code\r\n    c = tf.Variable(b+1, name='C', trainable=False)  # Second cached variable\r\n    d = b + 3  # Pretend this is a bunch more code\r\n    final_output = d + c\r\n\r\n    is_init = tf.is_variable_initialized\r\n    with tf.control_dependencies([a.initializer]):\r\n        is_a = tf.Print(is_init(a), [is_init(a)], \"A: \")\r\n        with tf.control_dependencies([is_a]):\r\n            with tf.control_dependencies([c.initializer]):\r\n                is_c = tf.Print(is_init(c), [is_init(c)], \"C: \")\r\n                init_op = is_c.op\r\n\r\n    init_op.run(feed_dict={x: 1})\r\n\r\n    print(a.eval())\r\n    print(c.eval())\r\n```\r\n\r\nThat code, seemingly randomly, sometimes fails and sometimes works. Using initial_value doesn't help because then the placeholder would have to always be filled in subsequent runs, and using initialized_value doesn't work for the same reason (also because while its ok to do the tf.cond call once for initialization, repeatedly calling it for each training step for a more complex model would be very slow). Or is there a new, better way that I am not aware of?"]}, {"number": 4919, "title": "Fixed fused_batch_norm gradients", "body": "Fixed issues with fused_batch_norm as discussed in #4899, added support to `2D` tensor for fused_batch_norm and added `data_format` option for `bias_add`. \n\nAlso the `nn.fused_batch_norm` op should be able to take minimum `epsilon=1e-5`. This value is set as  mininum value in `cudnn` and has been used in several models in `slim.nets`. At the moment `nn.fused_batch_norm` throw `CUDNN_STATUS_BAD_PARAM` when `epsilon=1e-5`, probably due to the numerical/casting error. This pull added `epsilon=epsilon+1e-12` in `nn.fused_batch_norm` (`1e-5 + 1e-12` works but `1e-5 + 1e-13` doesn't).\n", "comments": ["Can one of the admins verify this patch?\n", "@thuyen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @zhangyaobit, @jhseu and @tensorflower-gardener to be potential reviewers.\n", "@zhangyaobit Could you have a look? :)\n", "Thanks a lot Yao. Just added all your suggestions!\n", "@zhangyaobit I corrected the shape handling for `bias_add`. Could you have a look? :)\n", "@tensorflow-jenkins: test this please\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 4918, "title": "fixing go get and var name", "body": "go get complained about the github.com path, splitting them out fixes it\n", "comments": ["@barnjamin, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar to be a potential reviewer.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I've signed the CLA\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Mr Jenkins, test this please\n", "Just using Bash on Ubuntu 16.04\n\n```\nben@thor:~/go/src/github.com/tensorflow/tensorflow/tensorflow/go$ echo $0\nbash\nben@thor:~/go/src/github.com/tensorflow/tensorflow/tensorflow/go$ go generate github.com/tensorflow/tensorflow/tensorflow/go/op\npackage github.com/golang/protobuf/{proto,protoc-gen-go}: invalid github.com/ import path \"github.com/golang/protobuf/{proto,protoc-gen-go}\"\n../genop/main.go:15: running \"sh\": exit status 1\nop/generate.go:15: running \"go\": exit status 1\nben@thor:~/go/src/github.com/tensorflow/tensorflow/tensorflow/go$ go version\ngo version go1.7.1 linux/amd64\n```\n"]}, {"number": 4917, "title": "QueueRunner deadlock when using all CPUs", "body": "I'm building an input pipeline following the guidelines [here](https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html).  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using `tf.py_func`), and return the processed results to an output queue.  I'd like to use `QueueRunner`'s ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:\n\n``` python\nimport numpy as np\nimport multiprocessing\nimport tensorflow as tf\n\nn_cpus = multiprocessing.cpu_count()\n\nsess = tf.Session()\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nmult = tf.mul(a, b)\n\ndef python_op(x):\n    print \"python_op called with {}\".format(x)\n    # In my real function, the np.cos and np.sin calls are replaced by\n    # python calculations I can't do in tensorflow\n    y = np.cos(x)\n    z = sess.run(mult, feed_dict={a: y, b: x})\n    print \"intermediate result is {}\".format(z)\n    return np.sin(z)\n\nn_inputs = n_cpus\ninput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nload_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))\n\noutput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nget_result = output_queue.dequeue_many(n_inputs)\n\ndef processing_pipeline():\n    input_value = input_queue.dequeue()\n    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))\n\n# Here's the problem: If we use all CPUs here, the program will deadlock.\n# If we change cpus to (cpus-1), it works as expected.\nrunner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))\n\ncoord = tf.train.Coordinator()\nrunner.create_threads(sess, coord=coord, start=True)\n\nprint \"Loading input\"\nsess.run(load_input)\nsess.run(input_queue.close())\n\ntry:\n    print \"waiting for result\"\n    result = sess.run(get_result)\n    print \"RESULT: {}\".format(result)\nexcept tf.errors.OutOfRangeError:\n    print \"Input exhausted\"\n\ncoord.request_stop()\ncoord.join()\nprint \"Done\"\n```\n\nThe program above deadlocks waiting for `sess.run` to complete in `python_op`:\n\n```\n$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n```\n\nThis is running on an 8-core machine; you can see that 8 `python_op`s are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing `(n_cpus)` to `(n_cpus-1)` in the line that creates the `tf.train.QueueRunner`, then the program runs to completion:\n\n```\n$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n```\n\nThe program also completes successfully if we pass in fewer examples than CPUs in the input queue.\n\nI realize it's somewhat awkward for `python_op` to call back into the tensorflow session.  However, the [threading and queues](https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html#threading-and-queues) section of the manual states:\n\n\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"\n\nSo, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?\n\nAs a side note, one option to work around my problems would be to break `python_op` into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since `python_op`'s real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.\n\nOS: Linux\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)\n", "comments": ["`py_func` is a bit special -- it uses the main Python interpreter process to run its Python code, so it can not run while any other Python threads are running because of Global Interpreter Lock. On other hand `session.run` is supposed to release Global Interpreter Lock. I've never seen anyone use `session.run` inside `py_func`, I'm surprised this works at all\n\n@zffchen78 do you see any problems in having py_func call `session.run`?\n", "@bb4242 btw, \"works when fewer threads than CPUs\" vaguely suggests this could be affected by number of ops that are allowed to run in parallel (defaults to number of cores), ie, try `session = tf.Session(tf.Config(operation_timeout_in_ms=2000, inter_op_parallelism_threads=<large number>, intra_op_parallelism_threads=1))`\n", "@yaroslavvb: Thanks, your suggestion was very illuminating. :)  I can confirm that setting `inter_op_parallelism_threads` to a number greater than the number of cores, with `intra_op_parallelism_threads=1`, solves the deadlock.  I think I understand what's going on better now and why my example deadlocks:\n\n1) In the default configuration, tensorflow creates a thread pool with N threads, where N = number of cores.  These threads are shared across all ops that want to execute.\n2) The pipeline I was attempting to run creates N parallel requests to run `python_op`.  These requests get scheduled onto the thread pool, so that every worker thread is occupied.\n3) The `session.run`s inside of each `python_op` require at least one free thread in order to execute (the required number might be as large as `intra_op_parallelism_threads`).  Since there aren't any free threads, deadlock occurs.\n4) Increasing the size of the thread pool (by setting `inter_op_parallelism_threads` to a larger value) ensures that there are threads available even after N `python_op`s are running, thus eliminating the deadlock.\n\nDoes that sound right?  If so, I'd say this is working as expected and I'll close the issue.\n", "Yes, that makes sense. There was a similar issue in core TensorFlow when you scheduled k ops that are waiting for `dequeue` op to run, which deadlocked when `inter_op_parallelism_threads` is exceeded. That was solved by adding special threads to run `dequeue` ops that don't respect `inter_op_parallelism_threads` limit.\n", "Cool, thanks again.  I have a better understanding of what's going on in the tensorflow runtime now. :)\n"]}, {"number": 4916, "title": "Updated the links in docs to match the markdown sytnax.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@haosdent, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @nfiedel to be potential reviewers.\n", "Really sorry that use `\\]\\s\\([#h]` to search code which should use `\\]\\s+\\([#h]`. Hopefully this is the last time.  cc @rohan100jain @terrytangyuan @yifeif \n", "@tensorflow-jenkins test this please\n"]}, {"number": 4915, "title": "Unable to build label_image example, master branch", "body": "I am on OSX 10.11.12 (El Capitan)\nInstalled from source, on master branch \ngit rev-parse HEAD: a566a7701381a5cf7f70fce397759483764e482\nbazel version: 0.3.2-homebrew\n\nI am getting a series of errors when trying to build the label_image example from a freshly downloaded and configured (all default options, no CUDA, GPU, or Hadoop support) tensorflow source. \n\nbazel build tensorflow/examples/label_image ...\n\noutput:\n\nERROR: /Users/corey/Documents/school/hpc/tensorflow/tensorflow/core/BUILD:353:1: C++ compilation of rule '//tensorflow/core:string_ops_op_lib' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer '-std=c++0x' -MD -MF ... (remaining 92 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n\ntensorflow/core/ops/string_ops.cc:186:11: error: return type 'const ::tensorflow::Status' must match previous return type 'tensorflow::Status' when lambda expression has unspecified explicit return type\n          TF_RETURN_IF_ERROR(c->Merge(out, c->input(i), &out));\n          ^\n./tensorflow/core/lib/core/errors.h:43:42: note: expanded from macro 'TF_RETURN_IF_ERROR'\n    if (TF_PREDICT_FALSE(!_status.ok())) return _status; \\\n                                         ^\ntensorflow/core/ops/string_ops.cc:216:7: error: return type 'tensorflow::Status' must match previous return type 'const ::tensorflow::Status' when lambda expression has unspecified explicit return type\n      return Status::OK();\n      ^\ntensorflow/core/ops/string_ops.cc:208:17: error: no viable conversion from 'tensorflow::(lambda at tensorflow/core/ops/string_ops.cc:208:17)' to 'tensorflow::Status (_)(shape_inference::InferenceContext *)'\n    .SetShapeFn([](InferenceContext* c) {\n                ^~~~~~~~~~~~~~~~~~~~~~~~~\ntensorflow/core/ops/string_ops.cc:208:17: note: candidate function\n./tensorflow/core/framework/op.h:247:16: note: passing argument to parameter 'fn' here\n      Status (_fn)(shape_inference::InferenceContext*)) {\n               ^\n3 errors generated.\n\nThanks for any advice or help!\n", "comments": ["Try updating your xcode. See also: tensorflow/models#130\n", "@ctames Just in case it helps, your command works with the current master:\n\n```\nf794cd393b1e7821fcc3cdcee9b6a4400f2540bf refs/remotes/origin/HEAD\n```\n\nOne thing, though, is that I am still on Bazel 0.3.1. No upgrade to Xcode at all (still 7.3).\n"]}, {"number": 4914, "title": "Keeping gradient of sqrt(x) stable for x = 0", "body": "I'm minimizing a function that contains a few `tf.sqrt(c * x)` terms. The `x` is a `tf.Variable` and `c` is a `tf.constant` that is sometimes zero. A `NaN` inevitably presents itself. In my case, the gradient is to `c`, which is `x * 0.5/sqrt(c * x)` and which equals `0 * inf = NaN` when `c` is `0`.\n\nWhen such a `sqrt` is deeply buried in your function, it can be quite an effort to dig out where the `NaN` is coming from. I can understand and appreciate the fact that there is no check for zero in the `sqrt_grad` operator. However, I feel that debugging could be easier for ops that are known to be unstable in some numerical range.\n\nTwo possible fixes would be:\n1. Add exceptions to the documentation of these ops. Right now this is not even indicated for `tf.div`, for instance. Since the use-cases of TensorFlow almost always mean that gradients will be involved, the allowed range should also be mentioned for the gradient, if different from that of the op itself.\n2. Add debug-mode versions of the ops. These could include `NaN` and `inf` checks.\n\nBy the way, I was using the `tf.contrib.opt.ScipyOptimizerInterface` for the minimization, which does not support manually changing the gradients by using `compute_gradients` and `apply_gradients`. That's beside the point, though.\n\nBelow some example code for completeness' sake. The differences in outcome only add to the confusion.\n\n``` python\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\nc = tf.Variable(0.0)\n\nsqrt_grad = tf.gradients(tf.sqrt(c), c)\n\n# another possibility is when another factor in the argument is zero\nx = tf.Variable(1.)\nsqrt_x_grad = tf.gradients(tf.sqrt(x * c), x)\n\n# try to use select to filter out the NaN\nselsqrt_grad = tf.gradients(tf.select(c > 0, tf.sqrt(c), 0), c)\n\n# try clipping of the sqrt\nclipsqrt_grad = tf.gradients(tf.clip_by_value(tf.sqrt(c), 1e-10, 1), c)\n\n# clip the argument of the sqrt --> only numerically stable option\nclipargsqrt_grad = tf.gradients(tf.sqrt(tf.clip_by_value(c, 1e-10, 1)), c)\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n\n    print(sess.run([sqrt_grad, sqrt_x_grad, selsqrt_grad,\n                    clipsqrt_grad, clipargsqrt_grad]))\n    # [[inf], [nan], [nan], [nan], [0.0]]\n```\n", "comments": ["The `selsqrt_grad` was quite interesting (unexpected result).\n\nAnother interesting case:\n\n```\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\ns = tf.Variable(0.0)\nx = tf.square(tf.square(s))\nf = tf.sqrt(x)\ninit_op = tf.initialize_all_variables()\nsess.run(init_op)\ndfds, = tf.gradients(f, [s])\n```\n\ndfds==0 analytically but we get a NaN because 0 \\* Inf = NaN.\n", "@girving could you please take a look at this one?\n", "I think @egpbos exposed a bug in `tf.select` and how it computes gradients. \n\n```\nIn [15]: x = tf.Variable(0.0)\nIn [16]: f = tf.select(x > 0, -x*tf.log(x), 0.0)\nIn [17]: g, = tf.gradients(f, [x])\nIn [18]: sess.run(x.initializer)\nIn [19]: g.eval()\nOut[19]: nan\n```\n", "@jart I'm on leave at the moment.\n", "@hholst80 btw, the issue with select grad is that it seems to do \"0*branch1+branch2\", which has unwanted effect of turning into `nan` when `branch1` is `inf`. Here's a simpler way to show this problem -- the result is `NaN` even though problematic branch is not selected.\n\n@yuanbyu may know if there's any potential way to remedy this problem. Maybe if `select` gradient used `select` instead of multiplication by 0?\n\n```\nx = tf.Variable(0.0)\nf = tf.select(False, tf.log(x), 0)\ng, = tf.gradients(f, [x])\nsess = tf.InteractiveSession()\nx.initializer.run()\ng.eval()\nOut[0]: nan\n```\n", "I have run into similar problems with `tf.select`. It would be a good idea to have `tf.select` gradient use `tf.select` instead of multiplication by zero. It would produce much more intuitive behaviour when working with sequences that may include `NaN`.\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "This is still happening with TF 1.2.1. The updated snippet, which outputs the same results, is:\r\n```Python\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\n\r\nc = tf.Variable(0.0)\r\n\r\nsqrt_grad = tf.gradients(tf.sqrt(c), c)\r\n\r\n# another possibility is when another factor in the argument is zero\r\nx = tf.Variable(1.)\r\nsqrt_x_grad = tf.gradients(tf.sqrt(x * c), x)\r\n\r\n# try to use select to filter out the NaN\r\nselsqrt_grad = tf.gradients(tf.where(c > 0, tf.sqrt(c), 0), c)\r\n\r\n# try clipping of the sqrt\r\nclipsqrt_grad = tf.gradients(tf.clip_by_value(tf.sqrt(c), 1e-10, 1), c)\r\n\r\n# clip the argument of the sqrt --> only numerically stable option\r\nclipargsqrt_grad = tf.gradients(tf.sqrt(tf.clip_by_value(c, 1e-10, 1)), c)\r\n\r\ninit_op = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\n\r\n    print(sess.run([sqrt_grad, sqrt_x_grad, selsqrt_grad,\r\n                    clipsqrt_grad, clipargsqrt_grad]))\r\n```", "Reopening is as contributions welcome. There are two examples above which demonstrate the problem. A potential solution would be to use `tf.select(...,a, b)` for gradient instead of current `a*0+1*b`", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "This can cause `NaN`s to pop-up with `tensorflow_probability.layers.DenseLocalReparameterization` since requires passing gradients through a sqrt of a sum of squares. It can't be avoided with `tf.where` in TF 1.13.0-rc1, just like the `selsqrt_grad` examples at the start of this thread.", "selsqrt_grad is very interesting\r\n", "Curious here, what behaviors do you exactly want near singularities?  Having the software perform analytic continuation, which is what I am reading here, isn't going to be an easy problem.\r\n\r\nI will add a potential second option that will work in some (but not all) cases.  There is a theorem called the Cauchy Integral Theorem which changes the function from an evaluation near zero to a complex integral which can be set far away from zero where the function is stable.  It does allow you to ignore the instabilities near zero at the cost of a much more complex (and slower) procedure. \r\n\r\n", "IMHO you want to be able to select a specific gradient for certain values. Use the algebraic gradient for all values except at those given (singularites) and there use the provided gradient.\r\n\r\nArtificial example f=x^2/x works fine for all values of x except at x=0 where calculus is needed to provide the limit value. For x=0 use the gradient 1, which is the gradient of f (for all x, including x=0). ", "Alternatively, you can side-step the singularity issue because 0.0 in machine precision (or any other floating point number), corresponds to an interval on the real line, so you can avoid pointwise singularities by disambiguating to another member of the interval. Related example is gradient of ReLU --- it is nan at 0 but 0 at 0.0", "floating point 0.0 is exactly equal to 0. check the specs. \r\n\r\nTensorflow *defines* the gradient of ReLU(x) at x=0 to be equal 0. It's really that simple.\r\n> \r\n> \r\n> Alternatively, you can side-step the singularity issue because 0.0 in machine precision (or any other floating point number), corresponds to an interval on the real line, so you can avoid pointwise singularities by disambiguating to another member of the interval. Related example is gradient of ReLU --- it is nan at 0 but 0 at 0.0\r\n\r\n", "Specs say that any real number in range (-[ulp](https://en.wikipedia.org/wiki/Unit_in_the_last_place)/2, +ulp/2) corresponds to 0.0 in machine precision.\r\n\r\nSince -ulp/4 maps to 0.0 and has derivative 0, it makes sense to define Relu(0.0)'=0. It would also make sense to define it at 1 because of ulp/4. Defining it as 2 would not make sense. You can use this approach to figure out values around singularities without having to do limits", "Hi @egpbos! we are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 4913, "title": "Saver returning paths that cause GetMatchingPaths to go digging around in parent dirs", "body": "Note: This works without problems in 0.10.0rc0 (default pip install-ed according to website).\n### Environment info\n\nOperating System:\nUbuntu 15.10\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 189170 Jan  1  2016 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan  1  2016 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jan  1  2016 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jan  1  2016 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jan  1  2016 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed. Tried with two packages:\n   -  `https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl\n     `\n   -  `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl`\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so locally\n0.11.0rc0\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\nRelevant part of error (the file `/tmp/6d7079c109e5_0-saver_853` is supposed to be loaded by `self.saver.restore` (tf.Saver instance), it exists).\n\n```\nParameters according to early stopping: /tmp/6d7079c109e5_0-saver_853\nTraining finished after 854/854 iterations.\nTraceback (most recent call last):\n  File \"run.py\", line 297, in <module>\n    run(config)\n  File \"step.py\", line 109, in run\n    step(config=config)\n  File \"step.py\", line 237, in  step\n    model.fit(dset, config)\n  File \"model.py\", line 210, in fit\n    self.saver.restore(self.sess, save_path=early_stop_name)\n  File \"/anaconda/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1360, in restore\n    if not file_io.get_matching_files(file_path):\n  File \"/anaconda/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 254, in get_matching_files\n    compat.as_bytes(filename), status)]\n  File \"/anaconda/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.PermissionDeniedError: /tmp/systemd-private-a9da6f279d7b4869b7f687b66db5c7dd-rtkit-daemon.service-xozhXc\n```\n\nAfter restarting the computer and re-running the above, the `PermessionDeniedError` now points at `tensorflow.python.framework.errors.PermissionDeniedError: /tmp/systemd-private-bc171111db824721b4f4cec0e0a363ec-systemd-timesyncd.service-MARv8I`\n", "comments": ["Is there any chance you might be running into the same issue as #4852?\n", "@jart thanks for pointing to the updated #4852. However, I'm not sure if it is related to `numpy` in my case? I see in the log of #4852 that something with `numpy` happens. This is not the case in my code. The model I want to save was stored by TF with `early_stop_name = self.saver.save(self.sess, save_path=_path)`. No numpy involved inbetween this saving and restoring a few lines later. Also, it seems that according to the error message above it wants to access some `systemd` temp file, which it should not try to access.\n", "A quick experiment confirmed: If the above `save_path` is something like `~/tmp/` (i.e. in my local home), the problem does not exist. So the global `/tmp` causes problems. `tensorboard` logs into `/tmp` without problems, as additional information.\n", "Reading https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py#L252, it says `errors.OpError: If there are filesystem / directory listing errors.` So the above error is supposed to happen, if I interpret this correctly. However, it should not, as the `listing error` is concerned with a directory `tensorflow` has no business in dealing with.\n", "@jart @rohan100jain Going a bit through the history, this seems to be related to a larger change introduced some time ago here: https://github.com/tensorflow/tensorflow/commit/02adcaeec5f541870750e46ef5b1663bd9b61246#diff-1cc1b8538a5abd0cad0209d2aa34b116R134\n", "Your most recent comment leads me to suspect this might be related to #4921. Could you please take a look at that?\n", "I took a closer look. What is the `save_path` you're passing to `self.saver.restore`? If it's a value like `/tmp/foo` then `FileSystem::GetMatchingPaths` is going to go digging around in your /tmp directory. It needs to be a value like `/tmp/foo/bar-?????-of-nnnnn` which is returned by `Saver.save()`. So the `save_path` parameter really should be called `save_pattern`.\n", "It is `/tmp/6d7079c109e5_0-saver_853`, returned by `Saver.save()` (though I set this name there, too). So you are saying that it needs to be in a separate directory in `/tmp`, e.g. like `/tmp/6d7079c109e5/` (away from computer, can't check)?\n", "@rohan100jain This one sounds like it could be a file i/o related bug. I'm not sure if it's necessarily related to #4921. Although there does appear to be overlap.\n\n@osdf I'm stating the painfully obvious here, but sudo might be a workaround for the time being.\n", "I think for a short term fix, can you make it sub dir? I need to change GetMatchingFiles to ignore some of these permission failures and not throw an error. \n\nThe way get matching files works is that it finds the top level directory that doesn't have a wildcard which is /tmp and then recursively looks into it to find matches. So making a subdir will not only fix this but make this go faster too. \n\nBut I agree that we need to refine the saver / GetMatchingFiles behavior to not do so badly on such cases.\n", "Thanks @rohan100jain and @jart for taking a look !\n", "I made https://github.com/tensorflow/tensorflow/commit/c15bb7b6f64fbc4bfd19aeccfd8b8df99012b74c \n\nThis should help a bit I think. Hopefully this would mean that the Permission denied error directories aren't being looked at. I'll fix that too now though.\n", "Thanks @rohan100jain for your effort!! I resolved the issue now by having a subdirectory with-in `/tmp`, which then is used to write saver objects. Actually, this is much cleaner and easier to handle (just move the whole subdirectory out of `tmp`). Should I close this issue?\n", "Actually I would like to fix this entirely (i.e. even without the sub-dir). Could you try that out and let me know if there are still issues? \n", "I think this should be fixed now. Closing issue now. Please reopen if you still face problems."]}]