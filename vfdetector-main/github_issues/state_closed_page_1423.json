[{"number": 10286, "title": "Not able to run fusedgraph test on Hexagon", "body": "OS: Ubuntu 16.04 64bits\r\nAndroid Version: 7.1 (Nougat)\r\nNDK Version: android-ndk-r12b\r\nHEXAGON SDK: 3.1\r\nnnlib source: https://source.codeaurora.org/quic/hexagon_nn/nnlib\r\n\r\nAble to run runtime tf graph on hexagon **but not able to run fused graph** .\r\nI got below error.\r\n\r\nWARNING: linker: Warning: unable to normalize \"\"\r\nRunning main() from test_main.cc\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from GraphTransferer\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithFusedGraph\r\nnative : hexagon_graph_execution_test.cc:474 Run inception v3 with fused graph\r\nGetSocControllerVersion\r\nnative : hexagon_graph_execution_test.cc:72 Hexagon controller version is 90\r\nnative : hexagon_graph_execution_test.cc:122 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:128 header size = 54\r\nnative : hexagon_graph_execution_test.cc:130 image size = 40\r\nnative : hexagon_graph_execution_test.cc:132 width = 299\r\nnative : hexagon_graph_execution_test.cc:134 height = -299\r\nnative : hexagon_graph_execution_test.cc:262 loading image finished.\r\nnative : hexagon_graph_execution_test.cc:170 loading image finished.\r\nnative : hexagon_graph_execution_test.cc:174 Copy data to tensor.\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : hexagon_graph_execution_test.cc:284 Run graph\r\ntensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:288: Failure\r\nValue of: status.ok()\r\n  Actual: false\r\nExpected: true\r\n[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithFusedGraph (111 ms)\r\n[----------] 1 test from GraphTransferer (111 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (111 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithFusedGraph\r\n\r\n 1 FAILED TEST\r\n  YOU HAVE 4 DISABLED TESTS\r\n\r\n\r\nWhere to get fused graph for inceptionv3q to run on hexagon?\r\nwhat is the difference between TF runtime and TF fused graph(pointers? - operation wise)\r\n\r\n", "comments": ["@satok16 , request you to give pointers on this. thanks", "I'm working on refactoring this part to make it much stable, and that should be done in a week.  Sorry.", "@satok16 , Hi again, \r\nAlso, Need some clarity/understanding on what is getting called in session->run \r\nIn file, tensorflow/core/kernel/hexagon_graph_execution_test.cc +258(RunFusedGraph),\r\n\r\nonce session->run gets called as below:\r\n\r\n---snip ----\r\nLOG(INFO) << \"Run graph\";\r\n  // Run inference with all node as output\r\n  TF_ASSERT_OK(**session->Run(**run_options, input_tensors, output_node_names, {},\r\n                            &output_tensors, &run_metadata));\r\n  ASSERT_EQ(1, output_tensors.size());\r\n  const Tensor& output_tensor = output_tensors.at(0);\r\n  LOG(INFO) <\r\n---snip ----\r\n\r\n\r\nHow does , hexagon_control_wrapper->Init, SetupGraph and ExecuteGraph gets called.\r\n\r\nWhich function this run is actually pointing at? \r\nand from there control is going to hexagon_control_wrapper.\r\n\r\n\r\nIm making changes to work on different models (diff input/output node)\r\n\r\nthanks,\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing due to lack of activity, but please reopen if it needs attention."]}, {"number": 10285, "title": "Tensorflow Windows Visual Studio Help", "body": "Hi all,\r\nI'm VietNamese, my English is not good.\r\nI'm programming mobile robot with Aria ,C++ ,OpenCv and Tensorflow\r\nI'm using Windows and Visual Studio 2012. I can't setup Tensorflow C++.\r\nhow to setup Tensorflow C++ and connect to VS2012\r\nI hope everyone will train and help me. \r\nThank you.\r\n\r\n\r\n ", "comments": ["TensorFlow on windows needs at least Visual Studio 2015, due to required C++11 support.\r\nYou will need to upgrade visual studio otherwise you wont be able to compile TF on your system.\r\n\r\nYou can find some more information here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md", "Tks Gunna so much,\r\nI'll update VS and setup TF, OpenCV and Aria. \r\nI'll ask more when I need help from u.\r\nuhmm... Windows version 7 or 10? i use W7 ^^\r\nTks!\r\n\r\n"]}, {"number": 10284, "title": "Using fixed_unigram_candidate_sampler + nce_loss with reserved_ids emits NaN outputs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: From Source,\r\n- **TensorFlow version (use command below)**: v1.1.0-rc2-773-g7fa0cf3\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 5\r\n\r\n### Describe the problem\r\nUsing `tf.nn.nce_loss` (to be precise, `_compute_sampled_logits` function with argument `subtract_log_q=True`) with `tf.nn.fixed_unigram_candidate_sampler(num_reserved_ids>0)` + inputs with reserved ids gives NaN/inf logit output.\r\n\r\nThe cause for this NaN seems to be the `true_expected_count` return value of `tf.nn.fixed_unigram_candidate_sampler` for ids in range `[0, num_reserved_ids)`, since sampler returns expected count 0.0 for these ids. When the `subtract_log_q` argument of `_compute_sampled_logits` is zero, log value of expected count for these ids become inf or NaN. I used reserved ids for UNK and PAD (since `nce_loss` does not support variable number of target classes yet), using these ids in input was inevitable.\r\n\r\nPossible solution would be adding/cliiping log input by small epsilon. Will there be any better solution?\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_size = 3\r\nnum_true = 4\r\nnum_classes = 5\r\nnum_sampled = 5\r\nembed_dim = 5\r\n\r\ntrue_classes = tf.constant(\r\n    np.array(\r\n        [[3, 1, 2, 0],\r\n         [2, 0, 0, 0],\r\n         [2, 4, 3, 0]]),\r\n    dtype=tf.int64)\r\n\r\nsampled_values = tf.nn.fixed_unigram_candidate_sampler(\r\n    true_classes=true_classes,\r\n    num_true=num_true,\r\n    num_sampled=num_sampled,\r\n    unique=False,\r\n    range_max=num_classes,\r\n    num_reserved_ids=1,\r\n    unigrams=[10, 10, 10, 10]\r\n)\r\nsampled_ids, true_expected_count, sampled_expected_count = sampled_values\r\n\r\nloss = tf.reduce_mean(\r\n    tf.nn.nce_loss(\r\n        weights=tf.ones([num_classes, embed_dim], dtype=tf.float32),\r\n        biases=tf.zeros([num_classes], dtype=tf.float32),\r\n        labels=true_classes,\r\n        inputs=tf.ones([batch_size, embed_dim], dtype=tf.float32),\r\n        num_sampled=num_sampled,\r\n        num_classes=num_classes,\r\n        num_true=num_true,\r\n        sampled_values=sampled_values\r\n    )\r\n)\r\n\r\nwith tf.Session() as sess:\r\n    loss_value, true_count = sess.run([loss, true_expected_count])\r\n    print('Loss: {:.4f}'.format(loss_value))\r\n    print('Min True Count: {:.4f}'.format(np.amin(true_count)))\r\n\r\n>>>>>>>>\r\nLoss: nan\r\nMin True Count: 0.0000\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "same problem"]}, {"number": 10283, "title": "ImportError:_pywrap_tensorflow_internal.so: __sprintf_chk: symbol  not found", "body": "# OS\r\nHost: Windows 10 Professional  64bit\r\nDocker container :  Alpine\r\n`/ # cat /etc/issue`\r\nWelcome to Alpine Linux 3.6\r\nKernel \\r on an \\m (\\l)\r\n\r\n`/ # uname -a`\r\nLinux 3b851449cb60 4.9.27-moby #1 SMP Thu May 11 04:01:18 UTC 2017 x86_64 Linux\r\n\r\n# Installation\r\n- Part of my dockerfile\r\n\r\n`FROM frolvlad/alpine-glibc`\r\n`RUN apk update && apk add --no-cache \\\r\n        wget ca-certificates unzip vim git \\\r\n        gcc g++ python python-dev py-numpy-dev && \\\r\n    apk add --no-cache --virtual=build-dependencies \\\r\n        libffi-dev libressl-dev zlib-dev jpeg-dev freetype-dev libpng-dev `\r\n\r\n`RUN wget https://bootstrap.pypa.io/get-pip.py && \\\r\n    python get-pip.py && rm get-pip.py && \\\r\n    ln -s /usr/include/locale.h /usr/include/xlocale.h && \\\r\n    pip --no-cache-dir install requests[security] ipykernel jupyter matplotlib scipy scikit-learn pandas seaborn \\\r\n    https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.0rc1-cp27-none-linux_x86_64.whl && \\\r\n    python -m ipykernel.kernelspec `\r\n\r\nThe dockerfile was built successfully. But when `import tensorflow` in a container , the following happened\r\n\r\n>Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: Error relocating /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: __sprintf_chk: symbol\r\n not found\r\nFailed to load the native TensorFlow runtime.", "comments": ["Are you in the tensorflow source root directory? Can you try and ```cd``` into another folder and running the same?", "From a quick search and from https://github.com/gliderlabs/docker-alpine/issues/149#issuecomment-194435282 and #103, I believe Alpine uses a different C library than glibc, which unfortunately isn't supported.\r\n\r\nYou might want to try to install glibc and/or build from source. However, since this isn't a supported configuration, I'm afraid we won't be able to guide you through the process here.\r\n"]}, {"number": 10282, "title": "Conflict between Defun and py_func", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS 10.12.5\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A (run on CPU)\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import function\r\n\r\ndef f(x):\r\n    return x\r\n\r\n@function.Defun(tf.float32, func_name='f')\r\ndef f1(x): \r\n    return tf.py_func(f, [x], tf.float32)\r\n\r\nwith tf.Session() as sess:\r\n    x = tf.constant(1.)\r\n    print(sess.run(f1(x)))\r\n```\r\n\r\n### Describe the problem\r\nThe decorator `Defun` does not work with `py_func`, and generates the KeyError when attempting to call with the token `pyfunc_#`.  If one comments out the decorator `@function.Defun(...)`, or if one redefines `def f1(x): return x`, the error will disappear.\r\n\r\n### Source code / logs\r\nError traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py in __call__(self, token, args)\r\n     77   def __call__(self, token, args):\r\n     78     \"\"\"Calls the registered function for `token` with args.\"\"\"\r\n---> 79     func = self._funcs[token]\r\n     80     if func is None:\r\n     81       raise ValueError(\"callback %s is not found\" % token)\r\n\r\nKeyError: 'pyfunc_0'\r\n\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1038     try:\r\n-> 1039       return fn(*args)\r\n   1040     except errors.OpError as e:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1020                                  feed_dict, fetch_list, target_list,\r\n-> 1021                                  status, run_metadata)\r\n   1022 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nInternalError: Failed to run py callback pyfunc_0: see error log.\r\n\t [[Node: n1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_FLOAT], token=\"pyfunc_0\"](n0)]]\r\n\t [[Node: f = f[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-1-0ed0f802b342> in <module>()\r\n      8 with tf.Session() as sess:\r\n      9     x = tf.constant(1.)\r\n---> 10     print(sess.run(f1(x)))\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    776     try:\r\n    777       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 778                          run_metadata_ptr)\r\n    779       if run_metadata:\r\n    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    980     if final_fetches or final_targets:\r\n    981       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 982                              feed_dict_string, options, run_metadata)\r\n    983     else:\r\n    984       results = []\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1030     if handle is None:\r\n   1031       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1032                            target_list, options, run_metadata)\r\n   1033     else:\r\n   1034       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1050         except KeyError:\r\n   1051           pass\r\n-> 1052       raise type(e)(node_def, op, message)\r\n   1053 \r\n   1054   def _extend_graph(self):\r\n\r\nInternalError: Failed to run py callback pyfunc_0: see error log.\r\n\t [[Node: n1 = PyFunc[Tin=[DT_FLOAT], Tout=[DT_FLOAT], token=\"pyfunc_0\"](n0)]]\r\n\t [[Node: f = f[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const)]]\r\n```", "comments": ["@EverettYou : A  quick heads up: as per the [TensorFlow version semantics](https://www.tensorflow.org/programmers_guide/version_semantics), the `Defun` decorator is not part of the public API at this time (it is not directly accessible from the `tf` namespace and is not included in documentation, e.g., on the website). \r\n\r\nFYI: @ali01 who is looking into making that public and defining the limitations/guarantees provided by it.\r\n\r\nI'm tempted to close this issue out since it currently relates to an unsupported/undocumented feature."]}, {"number": 10281, "title": "//tensorflow/python:function_test is passing without GPU support but failing with GPU support", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n     Installed from source (v1.0.1)\r\n- **TensorFlow version (use command below)**:\r\n    ('v1.0.1-0-ge895d5c-dirty', '1.0.1')\r\n- **Bazel version (if compiling from source)**:\r\n    0.4.4-2017-05-26 (@80a07b5)\r\n- **CUDA/cuDNN version**:\r\n     CUDA = 8.0 and cuDNN = 5.1\r\n- **GPU model and memory**:  \r\n     GPU 0: Tesla P100-SXM2-16GB \r\n     GPU 1: Tesla P100-SXM2-16GB\r\n- **Exact command to reproduce**:\r\n      bazel test --config=opt --config=cuda  //tensorflow/python:function_test\r\n\r\n### Describe the problem\r\nIf we run this test without CUDA then its passing, however with CUDA getting failure i.e.  \"array mismatch 0.64697265625%\" error , see below for details : \r\n\r\n### Source code / logs\r\n```\r\n$ bazel test --config=opt --config=cuda //tensorflow/python:function_test\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n-----------------------------------------------------------------------------\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0002:01:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.61GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x10002b8ea90\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties:\r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0006:01:00.0\r\nTotal memory: 15.89GiB\r\nFree memory: 15.61GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 160 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 2 visible devices\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 160 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform CUDA. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): Tesla P100-SXM2-16GB, Compute Capability 6.0\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (1): Tesla P100-SXM2-16GB, Compute Capability 6.0\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcupti.so.8.0 locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\n..I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\n..I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\n.\r\n.\r\n.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\nF.I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0002:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\n..\r\n======================================================================\r\nFAIL: testUnrollLSTMGrad (__main__.UnrollLSTMTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/c2069970ba4ea955300413b19a88640d/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/function_test.runfiles/org_tensorflow/tensorflow/python/framework/function_test.py\", line 832, in testUnrollLSTMGrad\r\n    self.assertAllClose(d0, d1, rtol=1e-4)\r\n  File \"/home/amit/.cache/bazel/_bazel_amit/c2069970ba4ea955300413b19a88640d/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/function_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 485, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.0001, atol=1e-06\r\n\r\n(mismatch 0.64697265625%)\r\n x: array([[[ 4.276238,  0.15792 ,  0.614052, ...,  2.567241,  0.883315,\r\n          0.162565],\r\n        [ 3.256324,  0.069458,  1.132122, ...,  2.211422,  0.491486,...\r\n y: array([[[ 4.276276,  0.15792 ,  0.614058, ...,  2.567239,  0.883316,\r\n          0.162566],\r\n        [ 3.256359,  0.069458,  1.132127, ...,  2.211414,  0.491488,...\r\n\r\n----------------------------------------------------------------------\r\nRan 41 tests in 14.361s\r\n\r\nFAILED (failures=1)\r\n255.971 13.0395\r\n255.971 13.0395\r\nnot close where =  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0]), array([ 0,  1,  4,  6,  6,  8,  8,  9, 11, 12, 12, 13, 13, 15, 18, 19, 26,\r\n       26, 26, 28, 28, 29, 29, 30, 30, 31, 31, 32, 33, 34, 34, 34, 39, 41,\r\n       41, 42, 42, 43, 45, 47, 48, 48, 50, 50, 50, 52, 55, 55, 55, 57, 59,\r\n       61, 62]), array([110,  49,  28, 110, 117,  24,  45,  24, 117,  45,  85,  28, 110,\r\n       104,  20,  13,  21,  28, 109,  24, 104, 110, 117,  45, 117, 104,\r\n       110, 106,  23,   0,  77,  87,  13,  28,  49,  53,  89, 110,  49,\r\n        28,  49, 122,  21,  23,  87,  45,  45,  87, 112, 124,  28,   7,  36]))\r\nnot close lhs =  [ 0.00964716  0.0108541  -0.02481478  0.01410314 -0.01004539 -0.00598446\r\n -0.06178713 -0.00234316  0.00250508  0.13677871  0.01347511  0.03004426\r\n -0.02344483  0.03969495 -0.00342371  0.06425443  0.04404789  0.01697282\r\n  0.00173055  0.00629981 -0.07690701  0.01800087  0.00182568  0.10605431\r\n -0.00270458  0.01780143  0.00939897 -0.01084067 -0.00755548  0.00419208\r\n -0.00418841 -0.00287345  0.05488337 -0.03028315  0.00747772 -0.00952677\r\n  0.01471797 -0.00804311  0.01273583  0.00947097  0.00700331 -0.00019016\r\n  0.03132032  0.14848644 -0.00288739 -0.01046085  0.0042536   0.04803397\r\n  0.0140109  -0.03605841 -0.00635294  0.16805029 -0.01475533]\r\nnot close rhs =  [ 0.0096446   0.01085714 -0.02482039  0.01410037 -0.01004928 -0.00598631\r\n -0.06180775 -0.00234506  0.00250115  0.13676071  0.01347163  0.03003997\r\n -0.02344918  0.03968659 -0.00342231  0.06426588  0.04405451  0.0169698\r\n  0.00172922  0.00629799 -0.07691711  0.01799804  0.00182213  0.1060307\r\n -0.0027072   0.01779342  0.00939631 -0.01084429 -0.00755821  0.00418651\r\n -0.00418626 -0.00287483  0.05489143 -0.03027856  0.0074747  -0.00952475\r\n  0.01471417 -0.00804621  0.01273203  0.00946853  0.00700712 -0.00019156\r\n  0.03132723  0.14846958 -0.00288068 -0.01046896  0.00425826  0.04802619\r\n  0.01401426 -0.03605241 -0.00635018  0.16807139 -0.01475281]\r\nnot close dif =  [  2.56299973e-06   3.04728746e-06   5.60283661e-06   2.77161598e-06\r\n   3.88920307e-06   1.84774399e-06   2.06232071e-05   1.89989805e-06\r\n   3.93390656e-06   1.80006027e-05   3.47942114e-06   4.29153442e-06\r\n  4.35113907e-06   8.35955143e-06   1.40070915e-06   1.14440918e-05\r\n   6.61611557e-06   3.02493572e-06   1.32620335e-06   1.81794167e-06\r\n   1.01029873e-05   2.83122063e-06   3.54647636e-06   2.36034393e-05\r\n   2.62260437e-06   8.01682472e-06   2.65240669e-06   3.62098217e-06\r\n   2.72691250e-06   5.57303429e-06   2.15321779e-06   1.37463212e-06\r\n   8.05780292e-06   4.58955765e-06   3.01748514e-06   2.02655792e-06\r\n   3.79979610e-06   3.09944153e-06   3.79979610e-06   2.44379044e-06\r\n   3.81469727e-06   1.40070915e-06   6.91413879e-06   1.68532133e-05\r\n   6.71669841e-06   8.10623169e-06   4.66406345e-06   7.78585672e-06\r\n   3.36393714e-06   5.99771738e-06   2.75671482e-06   2.11000443e-05\r\n   2.51457095e-06]\r\nnot close tol =  [  1.96445990e-06   2.08571419e-06   3.48203866e-06   2.41003727e-06\r\n   2.00492832e-06   1.59863112e-06   7.18077490e-06   1.23450559e-06\r\n   1.25011445e-06   1.46760713e-05   2.34716254e-06   4.00399631e-06\r\n   3.34491824e-06   4.96865869e-06   1.34223126e-06   7.42658722e-06\r\n   5.40545079e-06   2.69698012e-06   1.17292200e-06   1.62979904e-06\r\n   8.69171163e-06   2.79980395e-06   1.18221283e-06   1.16030706e-05\r\n   1.27071985e-06   2.77934168e-06   1.93963137e-06   2.08442907e-06\r\n   1.75582113e-06   1.41865110e-06   1.41862574e-06   1.28748252e-06\r\n   6.48914238e-06   4.02785645e-06   1.74746981e-06   1.95247480e-06\r\n   2.47141656e-06   1.80462098e-06   2.27320288e-06   1.94685254e-06\r\n   1.70071212e-06   1.01915577e-06   4.13272301e-06   1.58469575e-05\r\n   1.28806778e-06   2.04689604e-06   1.42582599e-06   5.80261849e-06\r\n   2.40142572e-06   4.60524143e-06   1.63501818e-06   1.78071386e-05\r\n   2.47528124e-06]\r\ndtype = float32, shape = (1, 64, 128)\r\n```\r\nAny comments/suggestions ?", "comments": ["Thanks for the report. I haven't traced down how, but it seems this was fixed in a later release (I'm unable to reproduce the failure in TensorFlow 1.1 or the release candidate for the next version - 1.2.0-rc0).\r\n\r\nIs it possible for you to upgrade to a later version (1.1.0 for example)?", "Hi @asimshankar, I have upgraded TF version from 1.0.1 to 1.1.0 , and now this test is passing \r\n\r\nThanks for your suggestion!"]}, {"number": 10280, "title": "Compare base_dtype instead of dtype in piecewise_constant", "body": "Compare base_dtype instead of dtype in piecewise_constant. Fix #10086 ", "comments": ["Can one of the admins verify this patch?", "Thanks for the PR!  Can you add a test to learning_rate_decay_test like in your repro: https://github.com/tensorflow/tensorflow/issues/10086 to validate that it fixes the problem?", "@tensorflow-jenkins test this please", "Failure looks unrelated. @jhseu ready to merge!"]}, {"number": 10279, "title": "Windows GPU Nightly Build Failures", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n* Windows 10\r\n* With GPU\r\n\r\n\r\n### Describe the problem\r\n\r\nThe last stable nightly build for TF on Windows with GPU is currently # 149 and 1+ month old.\r\nIs there a specific blocking issue, why a stable nightly build for this platform is not available for such an extended period?\r\n\r\nThanks for all the hard work!!! As always.\r\n\r\n### Source code / logs\r\n\r\nn/a\r\n", "comments": ["Thanks for the report, FYI @gunan @av8ramit \r\n\r\nThat said though, nightly builds are provided on a best-effort basis. We continue to provide release binaries for supported platforms, so 1.2.0-rc1 for Windows/GPU is available.\r\n"]}, {"number": 10278, "title": "Pool Allocator Problem (re-allocation after every batch!)", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.1\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Tesla K80 16GB\r\n\r\nHi there,\r\nI am running a 2 layer bidirectional LSTM with 128 nodes in each layer, as well as some fully connected layers after. I am training in batch mode (16 inputs/batch) and before training every single batch I see this in the console. It seems to me that pool allocation for fixed batch size and fixed input dimension models should only have to happen once. Instead, as I said, it is happening every single batch. Here is the log: \r\n\r\n2017-05-29 20:46:08.759331: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 15978 get requests, put_count=32386 evicted_count=8000 eviction_rate=0.24702 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:08.963459: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24574 get requests, put_count=50982 evicted_count=18000 eviction_rate=0.353066 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:09.234499: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 34004 get requests, put_count=70412 evicted_count=28000 eviction_rate=0.397659 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:09.458269: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 43726 get requests, put_count=90134 evicted_count=38000 eviction_rate=0.421595 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:09.684101: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 53450 get requests, put_count=109858 evicted_count=48000 eviction_rate=0.436928 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:09.911030: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63171 get requests, put_count=129579 evicted_count=58000 eviction_rate=0.447603 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:10.138285: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 72892 get requests, put_count=149300 evicted_count=68000 eviction_rate=0.455459 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:10.365756: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 82614 get requests, put_count=169022 evicted_count=78000 eviction_rate=0.461478 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:10.593120: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 92388 get requests, put_count=188796 evicted_count=88000 eviction_rate=0.466112 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:10.819558: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 102115 get requests, put_count=208523 evicted_count=98000 eviction_rate=0.469972 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:11.045245: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 111836 get requests, put_count=228244 evicted_count=108000 eviction_rate=0.473178 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:11.274408: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 121562 get requests, put_count=247970 evicted_count=118000 eviction_rate=0.475864 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:11.496428: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 130910 get requests, put_count=267318 evicted_count=128000 eviction_rate=0.47883 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:18.634003: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 15314 get requests, put_count=31563 evicted_count=7000 eviction_rate=0.221779 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:18.895088: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24076 get requests, put_count=50325 evicted_count=17000 eviction_rate=0.337804 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:19.119133: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 33852 get requests, put_count=70101 evicted_count=27000 eviction_rate=0.385159 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:19.343135: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 43576 get requests, put_count=89825 evicted_count=37000 eviction_rate=0.411912 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:19.567368: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 53300 get requests, put_count=109549 evicted_count=47000 eviction_rate=0.429032 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:19.790310: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 63026 get requests, put_count=129275 evicted_count=57000 eviction_rate=0.440921 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:20.012524: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 72746 get requests, put_count=148995 evicted_count=67000 eviction_rate=0.44968 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:20.235646: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 82468 get requests, put_count=168717 evicted_count=77000 eviction_rate=0.456386 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:20.459472: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 92188 get requests, put_count=188437 evicted_count=87000 eviction_rate=0.461693 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:20.685801: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 101917 get requests, put_count=208166 evicted_count=97000 eviction_rate=0.465974 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:20.908996: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 111685 get requests, put_count=227934 evicted_count=107000 eviction_rate=0.469434 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:21.133457: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 121412 get requests, put_count=247661 evicted_count=117000 eviction_rate=0.47242 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:28.415509: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 14283 get requests, put_count=30457 evicted_count=6000 eviction_rate=0.196999 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:28.639884: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24059 get requests, put_count=50233 evicted_count=16000 eviction_rate=0.318516 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:28.864077: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 33783 get requests, put_count=69957 evicted_count=26000 eviction_rate=0.371657 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:29.089434: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 43507 get requests, put_count=89681 evicted_count=36000 eviction_rate=0.401423 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:29.318965: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 53233 get requests, put_count=109407 evicted_count=46000 eviction_rate=0.420448 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:29.545553: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 62953 get requests, put_count=129127 evicted_count=56000 eviction_rate=0.433682 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:29.772076: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 72675 get requests, put_count=148849 evicted_count=66000 eviction_rate=0.443402 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:29.998109: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 82395 get requests, put_count=168569 evicted_count=76000 eviction_rate=0.450854 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:30.223748: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 92118 get requests, put_count=188292 evicted_count=86000 eviction_rate=0.456737 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:30.451240: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 101892 get requests, put_count=208066 evicted_count=96000 eviction_rate=0.461392 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:30.678667: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 111619 get requests, put_count=227793 evicted_count=106000 eviction_rate=0.465335 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:38.292491: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 15761 get requests, put_count=31952 evicted_count=5000 eviction_rate=0.156485 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:38.517312: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 25488 get requests, put_count=51679 evicted_count=15000 eviction_rate=0.290253 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:38.741779: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 35210 get requests, put_count=71401 evicted_count=25000 eviction_rate=0.350135 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:38.969213: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 44934 get requests, put_count=91125 evicted_count=35000 eviction_rate=0.384088 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:39.199590: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 54655 get requests, put_count=110846 evicted_count=45000 eviction_rate=0.405969 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:39.429868: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 64376 get requests, put_count=130567 evicted_count=55000 eviction_rate=0.42124 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:39.660914: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 74097 get requests, put_count=150288 evicted_count=65000 eviction_rate=0.432503 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:39.895695: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 83872 get requests, put_count=170063 evicted_count=75000 eviction_rate=0.441013 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:40.121575: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 93597 get requests, put_count=189788 evicted_count=85000 eviction_rate=0.447868 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:40.350568: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 103320 get requests, put_count=209511 evicted_count=95000 eviction_rate=0.453437 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:48.175862: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 17797 get requests, put_count=36107 evicted_count=6000 eviction_rate=0.166173 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:48.402920: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 27518 get requests, put_count=55828 evicted_count=16000 eviction_rate=0.286595 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:48.636189: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 37239 get requests, put_count=75549 evicted_count=26000 eviction_rate=0.344148 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:48.868928: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 46961 get requests, put_count=95271 evicted_count=36000 eviction_rate=0.377869 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:49.100911: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 56736 get requests, put_count=115046 evicted_count=46000 eviction_rate=0.39984 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:49.332567: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 66462 get requests, put_count=134772 evicted_count=56000 eviction_rate=0.415517 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:49.563835: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 76185 get requests, put_count=154495 evicted_count=66000 eviction_rate=0.427198 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:49.794439: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 85909 get requests, put_count=174219 evicted_count=76000 eviction_rate=0.436233 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:50.005278: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 94453 get requests, put_count=192763 evicted_count=86000 eviction_rate=0.446144 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:58.209250: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 21897 get requests, put_count=44438 evicted_count=9000 eviction_rate=0.202529 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:58.422662: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 31619 get requests, put_count=64160 evicted_count=19000 eviction_rate=0.296135 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:58.653616: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 41394 get requests, put_count=83935 evicted_count=29000 eviction_rate=0.345505 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:58.886744: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 51121 get requests, put_count=103662 evicted_count=39000 eviction_rate=0.376223 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:59.117713: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 60843 get requests, put_count=123384 evicted_count=49000 eviction_rate=0.397134 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:59.353056: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 70567 get requests, put_count=143108 evicted_count=59000 eviction_rate=0.412276 and unsatisfied allocation rate=0\r\n2017-05-29 20:46:59.582799: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 80287 get requests, put_count=162828 evicted_count=69000 eviction_rate=0.42376 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:07.939562: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 19361 get requests, put_count=39256 evicted_count=5000 eviction_rate=0.127369 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:08.175305: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 29088 get requests, put_count=58983 evicted_count=15000 eviction_rate=0.254311 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:08.408822: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 38810 get requests, put_count=78705 evicted_count=25000 eviction_rate=0.317642 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:08.641838: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 48534 get requests, put_count=98429 evicted_count=35000 eviction_rate=0.355586 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:08.874378: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 58255 get requests, put_count=118150 evicted_count=45000 eviction_rate=0.380872 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:09.102372: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 67976 get requests, put_count=137871 evicted_count=55000 eviction_rate=0.398924 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:18.179298: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 21753 get requests, put_count=44138 evicted_count=6000 eviction_rate=0.135937 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:18.407809: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 31473 get requests, put_count=63858 evicted_count=16000 eviction_rate=0.250556 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:18.642316: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 41196 get requests, put_count=83581 evicted_count=26000 eviction_rate=0.311075 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:18.873651: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 50970 get requests, put_count=103355 evicted_count=36000 eviction_rate=0.348314 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:28.081378: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 20448 get requests, put_count=41471 evicted_count=3000 eviction_rate=0.0723397 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:28.314172: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 30170 get requests, put_count=61193 evicted_count=13000 eviction_rate=0.212443 and unsatisfied allocation rate=0\r\n2017-05-29 20:47:28.517176: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 38482 get requests, put_count=79505 evicted_count=23000 eviction_rate=0.28929 and unsatisfied allocation rate=0\r\n---------------------------------------\r\n\r\nAs a result the model training is painfully slow. \r\n\r\nAs an additional note: the GPU is only being utilized about 60% during training. There doesn't seem to be a bottleneck on CPU because we have 4 cores and they are 70% idle.\r\n\r\nThank you so much for your help.\r\n\r\nBest,\r\nDylan\r\n", "comments": ["Hi guys, I think this issue is resolved. I don't think it was the PoolAllocator. Sorry for the mistake.", "Hey Dylan,\r\n\r\nWhat was the error ? Was it because of the batch size ?", "Hey @Chandrahas1991 \r\n\r\nThere was no error. The simple issue was that I was truncating my sequences not small enough, and the computation time blew up (~O(n^4)). Cutting the length into 1/3 resulted in getting approximately what I expect in terms of computation time. \r\n\r\nCheers,\r\nDylan", "@dylanrandle Hey Dylan,\r\n\r\nI met the same problem as yours. I'm running a sequential labeling task with LSTM. Since the lengths of sequences are not the same so I padded them with zero in every batch (but different batches have different lengths). Then I got tons of `PoolAllocator` and the speed is so slow.\r\n\r\nI saw your explanation that the sequences are not short enough so the computation time blew up. \r\n\r\nMy sequences are very long (each is a document with roughly 2k-4k words, batch_num=64). But \"truncate\"? I thought the most common way to process the dynamic batch is to \"padding zero\" to the max length? Truncating the sequences manually will destroy the annotations inside...\r\n\r\nDo you have any evidence to support your theory about sequence length and computation time (~O(n^4))? Thank you !!", "Hi,\r\n\r\nSo basically this is what I was trying to say:\r\n\r\n- There was no problem with the Pool Allocator. It seemed to be a problem, but really it's just that my long sequences (~2000 before) were just taking super long! I say O(n^4) super loosely, just because I used two stacked bidirectional LSTMs, meaning that there are 4 passes of the sequence required. \r\n\r\nMy solution was simply to reduce the sequence length. There was no problem with the pool allocator, it was just the problem I was trying to solve was too computationally intensive. \r\n\r\nThanks,\r\nDylan", "Hi, @dylanrandle \r\nHere is the explanation I found on [StackOverflow: How to interpret Poolallocator messages in tensorflow?](https://stackoverflow.com/questions/35151207/how-to-interpret-poolallocator-messages-in-tensorflow/35166985#35166985)\r\nYes it first says:\r\n> These messages should only be a cause for concern if you run out of memory. In such a case the log messages may help diagnose the problem. \r\n\r\nBut it also says:\r\n> Note also that peak execution speed may only be attained after the memory pools have grown to the proper size.\r\n\r\nSo personally I think the speed will keep low if the `PoolAllocator` still pop up. What do you think?\r\nBTW, the `PoolAllocator` disappeared after you reduced your sequence length? Or, it still pops up but the speed became high? Thanks!", "I'm facing the same issue. The message keeps popping up and no training progress. But when i run the code in jupyter notebook everything works as expected. Anyone gained some insight on this?\r\nEdit: only happens when on GPU, cpu works fine."]}, {"number": 10277, "title": "Android demo app crash on x86 device(libavcodec.so text relocations)", "body": "The app start then,  complain about libavcodec.so text relocations\r\n![screenshot_20170529-122626](https://cloud.githubusercontent.com/assets/4120796/26559275/f5468b90-446b-11e7-85bb-fb1eadcbf691.png)\r\nhere is the log:\r\n\r\n`05-29 12:33:36.787 12014-12014/org.tensorflow.demo E/WindowManager: android.view.WindowLeaked: Activity org.tensorflow.demo.ClassifierActivity has leaked window DecorView@34ca6d3[] that was originally added here\r\n                                                                        at android.view.ViewRootImpl.<init>(ViewRootImpl.java:418)\r\n                                                                        at android.view.WindowManagerGlobal.addView(WindowManagerGlobal.java:331)\r\n                                                                        at android.view.WindowManagerImpl.addView(WindowManagerImpl.java:94)\r\n                                                                        at android.app.Dialog.show(Dialog.java:329)\r\n                                                                        at android.app.AlertDialog$Builder.show(AlertDialog.java:1112)\r\n                                                                        at android.app.Activity.performStart(Activity.java:6723)\r\n                                                                        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2662)\r\n                                                                        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2766)\r\n                                                                        at android.app.ActivityThread.-wrap12(ActivityThread.java)\r\n                                                                        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1507)\r\n                                                                        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n                                                                        at android.os.Looper.loop(Looper.java:154)\r\n                                                                        at android.app.ActivityThread.main(ActivityThread.java:6236)\r\n                                                                        at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:891)\r\n                                                                        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:781)\r\n05-29 12:33:36.864 12014-12014/org.tensorflow.demo E/Surface: dequeueBuffer failed (No such device)\r\n05-29 12:33:36.865 12014-12014/org.tensorflow.demo E/ViewRootImpl[ClassifierActivity]: Could not lock surface\r\n                                                                                       java.lang.IllegalArgumentException\r\n                                                                                           at android.view.Surface.nativeLockCanvas(Native Method)\r\n                                                                                           at android.view.Surface.lockCanvas(Surface.java:310)\r\n                                                                                           at android.view.ViewRootImpl.drawSoftware(ViewRootImpl.java:2853)\r\n                                                                                           at android.view.ViewRootImpl.draw(ViewRootImpl.java:2827)\r\n                                                                                           at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2608)\r\n                                                                                           at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2215)\r\n                                                                                           at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1254)\r\n                                                                                           at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6344)\r\n                                                                                           at android.view.Choreographer$CallbackRecord.run(Choreographer.java:874)\r\n                                                                                           at android.view.Choreographer.doCallbacks(Choreographer.java:686)\r\n                                                                                           at android.view.Choreographer.doFrame(Choreographer.java:621)\r\n                                                                                           at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:860)\r\n                                                                                           at android.os.Handler.handleCallback(Handler.java:751)\r\n                                                                                           at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                                           at android.os.Looper.loop(Looper.java:154)\r\n                                                                                           at android.app.ActivityThread.main(ActivityThread.java:6236)\r\n                                                                                           at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                                           at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:891)\r\n                                                                                           at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:781)\r\n`\r\n\r\nThe device is an Zenfone 2 with an intel atom processor, is it supported?", "comments": ["Haven't seen that one. The demo definitely should work on x86, so long as you have a build x86 native libs in it (though libavcodec isn't one of those). Where did you get the demo from? If built it yourself, does the [prebuilt APK](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow_demo.apk), which does contain x86 libs, work?\r\n\r\nThere may be an error farther up in the log that is the direct cause. Window leaks tend to happen after something else has broken. Can you take a look and post that if possible?", "The apk is built by myself, however the prebuilt apk also crash \r\nbut it throws a different stack trace:\r\n`05-30 08:50:18.841 12116-12611/org.tensorflow.demo W/LegacyRequestMapper: convertRequestMetadata - control.awbRegions setting is not supported, ignoring value\r\n05-30 08:50:18.841 12116-12611/org.tensorflow.demo W/LegacyRequestMapper: Only received metering rectangles with weight 0.\r\n05-30 08:50:18.841 12116-12611/org.tensorflow.demo W/LegacyRequestMapper: Only received metering rectangles with weight 0.\r\n05-30 08:50:19.113 12116-12612/org.tensorflow.demo I/CameraDeviceState: Legacy camera service transitioning to state CAPTURING\r\n05-30 08:50:19.227 12116-12612/org.tensorflow.demo E/BufferQueueProducer: [ImageReader-640x480f23m2-12116-1] dequeueBuffer: createGraphicBuffer failed\r\n05-30 08:50:19.227 12116-12612/org.tensorflow.demo E/Legacy-CameraDevice-JNI: LegacyCameraDevice_nativeProduceFrame: Error while producing frame Out of memory (-12).\r\n05-30 08:50:19.228 12116-12612/org.tensorflow.demo E/CameraDeviceGLThread-0: Received exception on GL render thread: \r\n                                                                             java.lang.UnsupportedOperationException: Unknown error -12\r\n                                                                                 at android.hardware.camera2.legacy.LegacyExceptionUtils.throwOnError(LegacyExceptionUtils.java:77)\r\n                                                                                 at android.hardware.camera2.legacy.LegacyCameraDevice.produceFrame(LegacyCameraDevice.java:683)\r\n                                                                                 at android.hardware.camera2.legacy.SurfaceTextureRenderer.drawIntoSurfaces(SurfaceTextureRenderer.java:775)\r\n                                                                                 at android.hardware.camera2.legacy.GLThreadManager$1.handleMessage(GLThreadManager.java:105)\r\n                                                                                 at android.os.Handler.dispatchMessage(Handler.java:98)\r\n                                                                                 at android.os.Looper.loop(Looper.java:154)\r\n                                                                                 at android.os.HandlerThread.run(HandlerThread.java:61)\r\n05-30 08:50:19.228 12116-12612/org.tensorflow.demo I/CameraDeviceState: Legacy camera service transitioning to state ERROR\r\n05-30 08:50:22.972 12116-12611/org.tensorflow.demo E/RequestThread-0: Timed out while waiting for request to complete.\r\n05-30 08:50:22.972 12116-12611/org.tensorflow.demo E/CameraDeviceState: Cannot receive result while in state: 0\r\n05-30 08:50:22.972 12116-12611/org.tensorflow.demo E/CameraDeviceState: Cannot receive result while in state: 0\r\n05-30 08:50:23.003 12116-12611/org.tensorflow.demo E/CameraDeviceState: Cannot receive result while in state: 0\r\n05-30 08:50:23.030 12116-12612/org.tensorflow.demo W/MessageQueue: Handler (android.os.Handler) {f15b46} sending message to a Handler on a dead thread\r\n                                                                   java.lang.IllegalStateException: Handler (android.os.Handler) {f15b46} sending message to a Handler on a dead thread\r\n                                                                       at android.os.MessageQueue.enqueueMessage(MessageQueue.java:543)\r\n                                                                       at android.os.Handler.enqueueMessage(Handler.java:643)\r\n                                                                       at android.os.Handler.sendMessageAtTime(Handler.java:612)\r\n                                                                       at android.os.Handler.sendMessageDelayed(Handler.java:582)\r\n                                                                       at android.os.Handler.sendMessage(Handler.java:519)\r\n                                                                       at android.hardware.camera2.legacy.GLThreadManager.queueNewFrame(GLThreadManager.java:197)\r\n                                                                       at android.hardware.camera2.legacy.RequestThreadManager$4.onFrameAvailable(RequestThreadManager.java:266)\r\n                                                                       at android.graphics.SurfaceTexture$1.handleMessage(SurfaceTexture.java:207)\r\n                                                                       at android.os.Handler.dispatchMessage(Handler.java:102)\r\n                                                                       at android.os.Looper.loop(Looper.java:154)\r\n                                                                       at android.os.HandlerThread.run(HandlerThread.java:61)\r\n05-30 08:50:23.078 12116-12129/org.tensorflow.demo E/BufferQueueProducer: [SurfaceTexture-1-12116-3] dequeueBuffer: BufferQueue has been abandoned\r\n05-30 08:50:23.078 12116-12128/org.tensorflow.demo E/BufferQueueProducer: [SurfaceTexture-1-12116-3] dequeueBuffer: BufferQueue has been abandoned\r\n05-30 08:50:23.079 12116-12140/org.tensorflow.demo E/BufferQueueProducer: [SurfaceTexture-1-12116-3] dequeueBuffer: BufferQueue has been abandoned\r\n05-30 08:50:24.050 12116-12116/org.tensorflow.demo D/tensorflow: CameraActivity: onPause org.tensorflow.demo.ClassifierActivity@8d34417\r\n05-30 08:50:24.689 12116-12116/org.tensorflow.demo D/tensorflow: CameraActivity: onStop org.tensorflow.demo.ClassifierActivity@8d34417\r\n05-30 08:50:24.691 12116-12116/org.tensorflow.demo D/tensorflow: CameraActivity: onDestroy org.tensorflow.demo.ClassifierActivity@8d34417\r\n`", "This may be a camera2 API problem. It wouldn't surprise me if an x86 device like the Xenfone 2 has issues with it.\r\n\r\nThere's an outstanding PR #8736 to add a fallback to the original camera 1 API on problematic devices -- you might try syncing to that branch and see if it addresses your problem. If it does then we'd like to get it merged, there's just a little bit more refactoring that needs to be done if you have time to fix it up.", "I gonna give it a shot!! thank you very much !!", "is this issue solved? I am willing to look into it if it is still not fixed.", "is at review #10771 ", "Closing as this is resolved"]}, {"number": 10276, "title": "Support partial gets in MapStagingArea", "body": "- size from `small` to `medium`\r\n- introduce 2 shards", "comments": ["Can one of the admins verify this patch?", "/cc @ekelsen with respect to comments in #9686.", "@ekelsen I've updated this PR to support partial gets in the MapStagingArea. It was a use case I'd forgotten and it now ends up being nicely symmetrical with the partial puts.", "I ran `stage_op_test.py` 1000 times and saw no failures. One thing that immediately came to mind was that [test_util.test_session](https://github.com/tensorflow/tensorflow/blob/2eac53d5ea540b0b09326ba9330b6051d742532d/tensorflow/python/framework/test_util.py#L368-L377) states that test sessions are re-used unless an explicit graph is provided:\r\n\r\n```python\r\nReturns a TensorFlow Session for use in executing tests.\r\n    This method should be used for all functional tests.\r\n    This method behaves different than session.Session: for performance reasons\r\n    `test_session` will by default (if `graph` is None) reuse the same session\r\n    across tests. This means you may want to either call the function\r\n    `reset_default_graph()` before tests, or if creating an explicit new graph,\r\n    pass it here (simply setting it with `as_default()` won't do it), which will\r\n    trigger the creation of a new session.\r\n```\r\n\r\nI've:\r\n\r\n1. modified all the tests cases to explicitly create and finalize the compute graph (and hence recreate the Session in each case).\r\n2. Separated some test cases that involved multiple graphs and sessions into separate tests.\r\n\r\nI'll do some more test runs overnight (GMT +2) and see if I can reproduce any hangs.", "```\r\n$ bazel test --test_output=all --jobs=1 --runs_per_test=1000 --test_sharding_strategy=disabled -c opt --config=cuda //tensorflow/python/kernel_tests:map_stage_op_test //tensorflow/python/kernel_tests:stage_op_test\r\n\r\n...\r\n\r\n================================================================================\r\nINFO: Elapsed time: 3483.632s, Critical Path: 10.51s\r\n//tensorflow/python/kernel_tests:map_stage_op_test                       PASSED in 2.1s\r\n  Stats over 1000 runs: max = 2.1s, min = 1.6s, avg = 1.8s, dev = 0.1s\r\n//tensorflow/python/kernel_tests:stage_op_test                           PASSED in 3.0s\r\n  Stats over 1000 runs: max = 3.0s, min = 1.5s, avg = 1.7s, dev = 0.1s\r\n```\r\n\r\nI don't see anything in 1000 runs on either `stage_op_test` or `map_stage_op_test`. With what frequency are you seeing hangs?", "The stage_op_test hangs at least 10% of the time on the jenkins test server (map_stage_op_test seems fine).", "@ekelsen 5000 runs on `stage_op_test` have not revealed any problems thus far. `map_stage_op_test` is on 572/5000 runs now. Note this includes the changes I made to the Graph construction in https://github.com/tensorflow/tensorflow/pull/10276/commits/62948388bb350d0a136e333f93370e5e1cb1ae4b", "Thanks, I've attached the test output if you wish to double check: [bazel_test_output.zip](https://github.com/tensorflow/tensorflow/files/1056440/bazel_test_output.zip).\r\n\r\nI've grepped on `fail` and `error` and seen nothing.\r\n", "Jenkins, test this please.", "Jenkins, test this please", "I put in a fix for the python 2/3 range list/class change. Its very late here, I'll check back in in the morning.", "map_stage_op_test seems to time out. Trying again.\r\n\r\nJenkins, test this please", "Chatted with @ekelsen, and this timeout is reproducible with:\r\n`CUDA_CACHE_DISABLE=1 bazel test --test_env=CUDA_CACHE_DISABLE --nocache_test_results --config=cuda //tensorflow/python/kernel_tests:map_stage_op_test`\r\n\r\nSetting the test size to large and merging for now. If you have cycles, might be worth reducing the JIT compilation time in the test.", "Jenkins, test this please", "Actually, the test times should be fine. They're ~60s on my desktop with CUDA_CACHE_DISABLE set.\r\n\r\nJenkins, test this please"]}, {"number": 10275, "title": "TF 1.2.0-rc1: Build with MKL failed", "body": "Hi!\r\nI just tried building TF 1.2.0-rc1 with MKL, using:\r\n\r\n    bazel build --config=opt --config=cuda --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nWhere, in the `'./configure` step I said \"yes\" to both using MKL and to downloading it from the web. I get the following error:\r\n\r\n    ERROR: /dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/BUILD:1544:1: undeclared inclusion(s) in rule '//tensorflow/core:core_cpu_base':\r\n    this rule is missing dependency declarations for the following files included by 'tensorflow/core/graph/mkl_tfconversion_pass.cc':\r\n      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/function.h'\r\n      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/device_mgr.h'\r\n      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/optimization_registry.h'\r\n      '/dataSSD/untom/tmp/tensorflow-1.2.0-rc1/tensorflow/core/common_runtime/device_set.h'.\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n    Use --verbose_failures to see the command lines of failed build steps.\r\n\r\n\r\nEDIT: I used bazel 0.4.5 and Python 3.6.1\r\n", "comments": ["Not sure about the right fix. \r\nTake the changes in the below file to the file tensorflow/core/BUILD and try\r\n[BUILD.txt](https://github.com/tensorflow/tensorflow/files/1038430/BUILD.txt)\r\n\r\n", "@untom This is a duplicate of #9979 \r\nIt has been fixed in the master. Looks like the fix - PR #9986 - is not in 1.2.0-rc1.", "@av8ramit : Should the PR references above be included in 1.2.0-rc2 (or 1.2.0 final)?", "@asimshankar I'll add them into rc2.", "Can confirm that the problem is gone with rc2 "]}, {"number": 10274, "title": "Windows: Remove session_test from bazel_test_lib.sh", "body": "It was disabled in 49b17146d2e4f04192d16ed67574142de167f3a1\r\n@gunan ", "comments": ["http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/17/\r\n\r\nRunning bazel presubmit.", "bazel build is clean, and the cpu failure is an unrelated flake.\r\nMerging."]}, {"number": 10273, "title": "Android build error: tensorflow/core/framework/op_gen_lib.h:22:59: fatal error", "body": "Hello,\r\n\r\nI previously built (some earlier releases of tensorflow) succesfully for android platform (aarch64). However, I get this  following error \"./tensorflow/core/framework/op_gen_lib.h:22:59: fatal error: tensorflow/core/framework/op_gen_overrides.pb.h: No such file or directory\r\n #include \"tensorflow/core/framework/op_gen_overrides.pb.h\"\r\n\" incase of the latest release.\r\n\r\nI did run \"download_dependencies.sh\", \"compile_android_protobuf.sh\" prior to the command \"make -f ./tensorflow/contrib/makefile/Makefile TARGET=ANDROID\"\r\n\r\nThere is no file called \"op_gen_overrides.pb.h\"!", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\nIn particular, things like what version of the code you're building (is it the release branch or some specific commit? etc.)\r\n\r\nFYI @andrewharp "]}, {"number": 10272, "title": "Fixed fixed_unigram_candidate_sampler docs", "body": "Implementation of `tf.nn.fixed_unigram_candidate_sampler` (https://github.com/waleedka/tensorflow/blob/master/tensorflow/core/kernels/range_sampler.cc#L273) seems that the range of reserved ids is `[0, num_reserved_ids)`.\r\nFixed the docstring about `num_reserved_ids`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10271, "title": "Remove r in docstrings that do not have backslashes", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for the pull request! Not sure it hurts to have r there, since backslashes might easily be added later. Closing."]}, {"number": 10270, "title": "Performance degradation with large lookup tables - optimizer._apply_sparse_duplicate_indices  (TF V1.0.1)", "body": "Hi,\r\n\r\nI ran into this performance issue while trying to upgrade tensorflow from version 0.12.1  to 1.X.\r\n\r\nWe ran a network with large embedding lookup tables:\r\n- 100K X 32 (for example, word embedding -  with 100K unique words)\r\n- 300K X 128 (for example, categorical feature with cardinality of 300K unique items)\r\n\r\n After upgrading TF version to 1.0.1,  GPU usage dropped in from 60% to 30%.\r\nTraining time went up in 50%-200% (depends on how big is the embedding lookup table). \r\n\r\n\r\nThis is the commit that caused the performance degradation:\r\nhttps://github.com/tensorflow/tensorflow/commit/f9f56f9dc7fe41ef1128290a77ac88e889ea5229\r\n\r\nThe handling of unique indexes is very slow and does not run in parallel with others operations. \r\nPlease note the big unique blocks in the middle.\r\n![trace_unique](https://cloud.githubusercontent.com/assets/8734262/26542969/ab0f3740-4464-11e7-9dcb-f3ccd58dfc8a.png)\r\n\r\nHere is a work around (not handling unique indexes ):\r\n```\r\nclass MyOptimizer(tf.train.AdamOptimizer):\r\n        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\r\n               use_locking=False, name=\"Adam\"):\r\n                super(MyOptimizer,self).__init__(learning_rate,beta1, beta2, epsilon, use_locking,name)\r\n\r\n        def _apply_sparse_duplicate_indices(self, grad, var):\r\n                return self._apply_sparse(grad, var)\r\n```\r\n\r\n\r\nThanks,\r\nErez\r\n", "comments": ["Forgot to mention - this is the GPU info:\r\n\r\n```\r\n$nvidia-smi\r\nMon May 29 08:50:48 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla M40 24GB      Off  | 0000:03:00.0     Off |                    0 |\r\n| N/A   23C    P8    16W / 250W |      0MiB / 22939MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla M40 24GB      Off  | 0000:82:00.0     Off |                    0 |\r\n| N/A   21C    P8    17W / 250W |      0MiB / 22939MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```", "I using docker base image:\r\n\r\n`nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04\r\n`\r\n```\r\nnvidia-docker --version\r\nDocker version 1.12.5, build 7392c3b\r\n```", "This is how a good trace looks like:  (after applying downgrading to version 12.0.1 or applying the workaround) \r\n![good_trace_no_unique](https://cloud.githubusercontent.com/assets/8734262/26543479/ca4810b2-4466-11e7-9e6c-64cbb09f6ab4.png)\r\n\r\nNote that there is higher parallelism. \r\nNo big 'unique block' in the middle.  \r\n", "Interesting timelines, and sorry you're running into this. Some/most of this time is likely copying to host memory: we don't actually have a GPU kernel for unique, but one needed to be [registered](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/unique_op.cc#L102) to avoid interfering with op placements. The computation happens on the CPU.\r\n\r\nSo this could be sped up by implementing a real GPU kernel for Unique. That's likely preferable to fusing Adam's sparse updates into a GPU kernel, although it is another possibility.\r\n\r\nSo any interest in writing a GPU kernel for unique? I don't know of anyone who is working on one right now.", "@zhangyaobit @yzhwang since we discussed this.\r\n\r\n@KashiErez: One useful clarification would be how many indices are going into the UniqueOp, which should be equal to the number of embeddings that are accessed in each iteration (e.g. sentence length).", "Also @ekelsen, who has been thinking about using CUB as a way to implement these kinds of ops on the GPU.", "Yeah, hopefully CUB will be usable from TF soon.  In that case, unique can be done by sorting and then doing run-length-encoding.", "Regarding Question: @KashiErez: One useful clarification would be how many indices are going into the UniqueOp, which should be equal to the number of embeddings that are accessed in each iteration (e.g. sentence length).\r\n\r\nAnswer:\r\n\r\nThe batch size is 1024. \r\n\r\nRegarding words:\r\nThe sentence length is 21. \r\nNumber if unique words ~ 100K\r\n\r\nBut We have another categorical feature that has only one value in each iteration.\r\nAnd the number of unique values is ~ 300K.\r\n\r\nFrom the trace you can see that this categorical feature 'unique op' is running much slower then the word embedding 'unique op'.\r\n \r\nSo I think that the parameter you should look at first is 'number of unique values' (== lookup table size).", "In that case, could you add a print node to get the exact shape of the Tensor going into unique?\r\n\r\nThe whole idea of this code path is that the gradients are sparse; the IndexedSlices from the gradient of the embedding lookup has a number of indices equal to the number of embeddings which were actually accessed (which it sounds like should be ~21504 and ~1024?), independent of the size of the embedding lookup table.", "Hi,\r\n\r\nNot sure I understand where to add the print node.\r\n'_apply_sparse_duplicate_indices' gets two parameters: grad, var\r\nShould I print this tensor shapes?\r\n", "Hi,\r\n\r\nThis is the optimizer I used to print the indices shape:\r\n(I copied the code from optimizer.Optimizer class, and injected a tf.Print statement).\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\n\r\n\r\ndef _my_deduplicate_indexed_slices(values, indices, grad):\r\n    \"\"\"Sums `values` associated with any non-unique `indices`.\r\n    Args:\r\n      values: A `Tensor` with rank >= 1.\r\n      indices: A one-dimensional integer `Tensor`, indexing into the first\r\n        dimension of `values` (as in an IndexedSlices object).\r\n    Returns:\r\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\r\n      de-duplicated version of `indices` and `summed_values` contains the sum of\r\n      `values` slices associated with each unique index.\r\n    \"\"\"\r\n\r\n    indices = tf.Print(indices,\r\n                       data=[grad.name, tf.shape(indices)],\r\n                       message='$$$$$$$$ indices.shape   ----- ',\r\n                       first_n=1)\r\n\r\n    unique_indices, new_index_positions = array_ops.unique(indices)\r\n    summed_values = math_ops.unsorted_segment_sum(\r\n        values, new_index_positions,\r\n        array_ops.shape(unique_indices)[0])\r\n    return (summed_values, unique_indices)\r\n\r\n\r\nclass MyAdamOptimizer(tf.train.AdamOptimizer):\r\n    \"\"\"\r\n    This Optimizer is a workaround for this issue:\r\n    https://github.com/tensorflow/tensorflow/issues/10270\r\n    \"\"\"\r\n    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\r\n                 use_locking=False, name=\"MyAdam\"):\r\n        super(MyAdamOptimizer, self).__init__(learning_rate, beta1, beta2, epsilon, use_locking, name)\r\n\r\n    def _apply_sparse_duplicate_indices(self, grad, var):\r\n        \"\"\"\r\n        overriding method to avoid 'unique indexes' logic.\r\n        'unique indexes' logic produce performance degradation in large lookup tables\r\n        \"\"\"\r\n\r\n\r\n        summed_values, unique_indices = _my_deduplicate_indexed_slices(\r\n            values=grad.values, indices=grad.indices, grad=grad)\r\n\r\n        gradient_no_duplicate_indices = ops.IndexedSlices(\r\n            indices=unique_indices,\r\n            values=summed_values,\r\n            dense_shape=grad.dense_shape)\r\n        return self._apply_sparse(gradient_no_duplicate_indices, var)\r\n\r\n```\r\n\r\nI ran it on 3 models, here are the results:\r\n\r\nModel 1 - few small categorical features (no more then 10K each):\r\n\r\n```\r\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][10266]\r\n2017-06-05 12:49:30.234021: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1058]\r\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8687]\r\n2017-06-05 12:49:30.233421: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\r\n2017-06-05 12:49:30.237197: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\r\n```\r\n\r\nModel 2 - small categorical features (no more then 10K each) + words (50K unique values):\r\n\r\n```\r\n2017-06-05 11:39:45.333000: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_8:0][10266]\r\n2017-06-05 11:39:45.332994: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_4:0][1058]\r\n2017-06-05 11:39:45.332984: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_2:0][1036]\r\n2017-06-05 11:39:45.333465: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat:0][8687]\r\n2017-06-05 11:39:45.333727: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_6:0][1033]\r\n2017-06-05 11:39:45.333756: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [optimizer/gradients/concat_10:0][70681]\r\n```\r\n\r\n\r\nModel 3 - small categorical features (no more then 10K each) + words (50K unique values) + big categorical feature (300K unique values):\r\n\r\n```\r\n2017-06-05 12:41:13.841337: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_6:0][1033]\r\n2017-06-05 12:41:13.842412: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_4:0][1057]\r\n2017-06-05 12:41:13.840662: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_8:0][9131]\r\n2017-06-05 12:41:13.841332: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_12:0][40896]\r\n2017-06-05 12:41:13.841334: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat:0][8632]\r\n2017-06-05 12:41:13.841974: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_10:0][315805]\r\n2017-06-05 12:41:13.844600: I tensorflow/core/kernels/logging_ops.cc:79] $$$$$$$$ indices.shape   ----- [my_optimizer_0/gradients/concat_2:0][1036]\r\n\r\n```\r\n\r\nSumming up indices sizes:\r\nModel 1 -   22080  (small categorical features)\r\nModel 2 -   92761  (small categorical features + 50k feature)\r\nModel 3 - 377590 (small categorical features + 50k feature + 300K feature)\r\n\r\nSumming up, feature cardinality size and indices size are correlated.\r\n\r\n \r\nSmall Note, regarding the gradient scopes - my_optimizer_0/gradients/concat*  :\r\n\r\nmy_optimizer_0  - this is my code.\r\ngradients/concat* - I think it comes from here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_impl.py\r\n\r\n\r\n", "Well, that explains why the transfer times are in miliseconds: 3 megabytes of indices at ~1 gigabyte/sec, then take a round trip (even if they're all duplicate, the second result of unique() has the same size as its input).\r\n\r\nThere's a separate question of why that many embeddings are accessed at each iteration (all of them?). It's definitely unusual for a language model. Regardless, clearly it would be nice to have a GPU kernel for unique.", "I met this problem too, so have it been solved?", "@ningyuwhut I'd suggest using the workaround @KashiErez mentioned for now if you don't care about deduplicating sparse gradients and want the previous behavior. This was a bug fix, so we can't just go back to the old behavior by default.\r\n\r\nAFAIK nobody is working on a GPU kernel for UniqueOp, but that still seems like the resolution here if you're interested in taking the bug.", "Hi @KashiErez ! Sorry for the late response. \r\nYou are using older versions(1.x versions) of Tensorflow which is not supported any more. Could you check in  [latest versions ](https://github.com/tensorflow/tensorflow/releases) (TF 2.7/2.8 )? A lot of bug fixes has been done and features has been added in latest version.   Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "\u60a8\u7684\u90ae\u4ef6\u5df2\u7ecf\u6536\u5230--\u963f\u6770", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 10269, "title": "Applying dropout in Tensorflow C++ API", "body": "I have retrained Inception-v3 model using (Tensorflow) Python API and saved a standalone Graph in .pb form. I have also used a dropout layer before the final layer. I can successfully run inference on the graph in python. The code to generate predictions in python is as follows:\r\n\r\n    softmax_tensor = sess.graph.get_tensor_by_name('final_layer/final_result/Softmax:0')\r\n    predictions = sess.run(softmax_tensor, { 'DecodeJpeg/contents:0': image_data, 'final_layer/dropout/Placeholder:0': 1.})\r\n\r\nThe C++ counterpart of the python code is as follows:\r\n\r\n    string input_layer = \"Mul\"; \r\n    string output_layer = \"final_layer/dropout/Placeholder:0\";\r\n    Status run_status = session->Run({{input_layer, resized_tensor}}, {output_layer}, {}, &outputs);\r\n\r\nThe C++ code ends up with the following error message:\r\n\r\n`Running model failed: Invalid argument: You must feed a value for placeholder tensor 'final_layer/dropout/Placeholder'`\r\n\r\nWhat should I change in the above C++ code to remove this error? In other words, how do I change a placeholder value in the C++ code as in python code.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10268, "title": "sparse_softmax_cross_entropy_with_logits gives NaN instead of error when using non-existent labels", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.0.0-65-g4763edf-dirty\r\n1.0.1\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n\r\n- **CUDA/cuDNN version**:\r\nV8.0.61\r\n\r\n- **GPU model and memory**:\r\nNVIDIA GFORCE GTX 760 2GB\r\n\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf\r\nsess = tf.Session(0\r\nsess.run(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=[ 100 ], logits=[[ 0.0, 1.0 ]]))\r\n>>> array([ nan], dtype=float32)\r\n\r\n### Describe the problem\r\nRunning above code gives NaN instead of an error when on Ubuntu but when I run the same code on Windows I correctly get an InvalidArgumentError error.\r\n\r\n### Source code / logs\r\nSee code above.", "comments": ["This behavior is documented and working as intended:\r\n\r\n```\r\n    labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of\r\n      `labels` and result) and dtype `int32` or `int64`. Each entry in `labels`\r\n      must be an index in `[0, num_classes)`. Other values will raise an\r\n      exception when this op is run on CPU, and return `NaN` for corresponding\r\n      loss and gradient rows on GPU.\r\n```\r\n\r\nIt's probably not a behavior we could change, due to our [API stability promise](https://www.tensorflow.org/programmers_guide/version_semantics)."]}, {"number": 10267, "title": "Unnecessary label division at tf.nn.nce_loss?", "body": "As far as I know, NCE Loss is a sampling-based loss that converts large-scale multiclass loss into a sum of binary loss for sampled classes. Each binary classification infers a probability where the given context and word is from the proxy corpus (real distribution) or noise distribution. Therefore, I guess each binary classification of word should be hard binary classification with label 0.0 or 1.0.\r\n\r\nHowever, the documentation and the implementation of `tf.nn.nce_loss` (https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L1044) indicates that *the target probability is assigned to `1 / num_true` to make target probabilities sum to 1*. Implementation of `tf.nn.nce_loss` uses the helper function `_compute_sampled_logits` in same file, which always returns true label divided by the number of true examples (Line 1044).\r\n\r\nIs this division necessary for NCE Loss? Isn't the label of `<1.0` for positive examples generates the unnecessary opposite-direction loss `(1-y)log(1-y')`? Is there any other reason that I missed for this label division?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10266, "title": "QuantizedConv2D (inference) on iOS", "body": "Hello,\r\n\r\nI had some question and it would be nice to get quick help/reply.\r\n\r\nI know, that accelerate framework (cblass_sgemm) is used for Conv operation (gemm_functors.h) incase of iOS platform. However, that is for float datatype. \r\n\r\n1) What happens incase of quantized op of inception5h graph. For example, which library is used for QuantizedConv2D operation incase of iOS mobile platform?\r\n2) Is Accelerate framework is used also for QuantizedConv2D operation? If so, then is the comptuation actually happens for float datatype and not 8bit mult and 32bit accumulation?\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10265, "title": "Possible Bug: tensorflow.cholesky_solve", "body": "I wrote a script to compare the output of a very simple linear system with simple matrix inversion a la [tensorflow.matrix_inverse](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#matrix_inverse), the non-cholesky based matrix equation solver [tensorflow.matrix_solve](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#matrix_solve), and [tensorflow.cholesky_solve](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#cholesky_solve).\r\n\r\nAccording to my understanding of the docs I've linked, these three cases should all yield a solution of the `I/2`, but this is not the case for `tensorflow.cholesky_solve`. Perhaps I'm misunderstanding the docs? \r\n\r\n    import tensorflow as tf\r\n    \r\n    I = tf.eye(2, dtype=tf.float32)\r\n    X = 2 * tf.eye(2, dtype=tf.float32)\r\n    X_inv = tf.matrix_inverse(X)\r\n    X_solve = tf.matrix_solve(X, I)\r\n    X_chol_solve = tf.cholesky_solve(tf.cholesky(X), I)\r\n    \r\n    with tf.Session() as sess:\r\n        for x in [X_inv, X_solve, X_chol_solve]:\r\n            print('{}:\\n{}'.format(x.name, sess.run(x)))\r\n            print\r\n\r\nyielding output:\r\n\r\n    MatrixInverse:0:\r\n    [[ 0.5  0. ]\r\n     [ 0.   0.5]]\r\n    \r\n    MatrixSolve:0:\r\n    [[ 0.5  0. ]\r\n     [ 0.   0.5]]\r\n    \r\n    cholesky_solve/MatrixTriangularSolve_1:0:\r\n    [[ 1.  0.]\r\n     [ 0.  1.]]    \r\n    \r\n    Process finished with exit code 0\r\n\r\n### System information\r\n- **OS: Ubuntu 16.04 xenial**\r\n- **Kernel: x86_64 Linux 4.8.0-52-generic**\r\n- **TensorFlow installed from binary**\r\n- **TensorFlow version ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')**\r\n- **Cuda compilation tools, release 8.0, V8.0.61**\r\n- **GTX 1070 8GB**\r\n\r\n\r\n\r\n", "comments": ["@langmore @rmlarsen : Mind taking a look?", "Hello @jim24 .  Your understanding is correct, and on my system all three methods give the same result.  Could you try running again on CPU, by using:\r\n\r\n`with tf.Session() as sess, tf.device('/cpu:0'):`", "Hi Ian.\r\n\r\nI ran the script again, with your modifications, on a different machine (info posted at the bottom) as I do not have access the machine posted with earlier at the moment.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nI = tf.eye(2, dtype=tf.float32)\r\nX = 2 * tf.eye(2, dtype=tf.float32)\r\nX_inv = tf.matrix_inverse(X)\r\nX_solve = tf.matrix_solve(X, I)\r\nX_chol_solve = tf.cholesky_solve(tf.cholesky(X), I)\r\n\r\nfor device in ['/cpu:0', '/gpu:0']:\r\n    with tf.Session() as sess, tf.device(device):\r\n        for x in [X_inv, X_solve, X_chol_solve]:\r\n            print('{}\\t{}:\\n\\t{}'.format(device, x.name, sess.run(x)))\r\n            print\r\n```\r\n\r\nwhich now gives output:\r\n```\r\n\r\n/cpu:0\tMatrixInverse:0:\r\n[[ 0.5  0. ]\r\n [ 0.   0.5]]\r\n/cpu:0\tMatrixSolve:0:\r\n[[ 0.5  0. ]\r\n [ 0.   0.5]]\r\n/cpu:0\tcholesky_solve/MatrixTriangularSolve_1:0:\r\n[[ 1.  0.]\r\n [ 0.  1.]]\r\n/gpu:0\tMatrixInverse:0:\r\n[[ 0.5  0. ]\r\n [ 0.   0.5]]\r\n/gpu:0\tMatrixSolve:0:\r\n[[ 0.5  0. ]\r\n [ 0.   0.5]]\r\n/gpu:0\tcholesky_solve/MatrixTriangularSolve_1:0:\r\n[[ 1.  0.]\r\n [ 0.  1.]]\r\n```\r\n\r\nSo it seems the problem persists for me on this machine too.\r\n\r\n### System 2 information\r\n- **Windows 7 SP1**\r\n- **TensorFlow installed from binary**\r\n- **TensorFlow version: b'unknown' 1.1.0**\r\n- **Cuda compilation tools, release 8.0, V8.0.60**\r\n- **gtx 660 1.5GB (x2)**\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "I can confirm odd behaviour on CPU. I get:\r\n\r\n```\r\nMatrixInverse:0:\r\n[[ 0.5  0. ]\r\n [ 0.   0.5]]\r\nMatrixSolve:0:\r\n[[ 0.5  0. ]\r\n [ 0.   0.5]]\r\ncholesky_solve/MatrixTriangularSolve_1:0:\r\n[[ 0.49999997  0.        ]\r\n [ 0.          0.49999997]]\r\n```\r\nAlso,\r\n```\r\nprint(\"scipy.linalg.cho_solve\")\r\nprint(cho_solve((np.linalg.cholesky(np.eye(2)*2),True),np.eye(2)))\r\n```\r\ngives,\r\n```\r\nscipy.linalg.cho_solve\r\n[[ 0.5  0. ]\r\n [ 0.   0.5]]\r\n```", "Hmmm, well I've upgraded tensorflow to 1.2.1 since this issue occurred and now I get your result for `cholesky_solve/MatrixTriangularSolve_1:0`. I'll close my issue because the only difference seems to be floating point imprecisions."]}, {"number": 10264, "title": "add missing import for `signal` package", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "I guess we need to change the cmakelists as well.", "@drpngx Unfortunately, I don't know enough about CMake or the code organization to do this myself yet.", "You'll need to add the module:\r\nhttps://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake#L467", "@drpngx Thanks for the hint!\r\n\r\nI noticed that the entry `session_bundle/testdata/saved_model_half_plus_two` doesn't exist (anymore).\r\nThis seems to be some kind of a consistency issue.\r\n\r\nDo you think these entries could be put into a file separate from the code?\r\nAnd would it be possible to programatically check them for validity?", "Jenkins, test this please.\n\nOn May 30, 2017 7:19 AM, \"Androbin\" <notifications@github.com> wrote:\n\n> @drpngx <https://github.com/drpngx> Thanks for the hint!\n>\n> I noticed that the entry session_bundle/testdata/saved_model_half_plus_two\n> doesn't exist (anymore).\n> This seems to be some kind of a consistency issue.\n>\n> Do you think these entries could be put into a file separate from the code?\n> And would it be possible to programatically check them for validity?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10264#issuecomment-304892187>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbeqqfMRTnIl4CHLaF1J_eJEUx33gks5r_CVXgaJpZM4Notkp>\n> .\n>\n", "Could you send a separate PR fixing the saved model issue?\r\n\r\n/CC: @gunan @yifeif for the consistency check. @Androbin I don't have a great answer to that, but if you can propose something I'm all ears. Right now, CMake has been adding support piecemeal as needed, but as you found out, it's not perfect.", "Thanks for the PR!", "I agree with @drpngx 's comment that cmake support is not perfect. It is also under contrib, so any contributions there are welcome.\r\nIm not sure about separating those out of the cmake file though. Especially contrib is under very heavy development, and as things are added and removed people just forget updating all references to those.", "@drpngx @gunan I get your point. It's just that even documentation changes are run through the entire build process and all the unit tests. And although it's `contrib`, installing proper testing allows for a more dynamic work flow. They should not be treated as second-class citizens when it comes to testing.", "For multiple reasons, bazel will stay as our primary build system.\r\ncmake wont be promoted to core.\r\nI am not against the test itself, I am happy to accept a test as you described as a contribution, and run it under contrib/cmake. But it is not a priority for us."]}, {"number": 10263, "title": "Looks like GPU Mac builds are no longer building", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n404 error on Mac GPU download \r\n\r\nLink: https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.0rc0-py3-none-any.whl\r\n\r\nError:\r\nHTTP ERROR 404\r\n\r\nProblem accessing /view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.2.0rc0-py3-none-any.whl. Reason:\r\n\r\nNot Found\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["We dropped support for GPU Mac support with 1.2 release.\r\nYou can find the related announcement here:\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v1.1.0"]}, {"number": 10262, "title": "tensorflow read tfrecord not synchronize", "body": "i would like to read tfrecords with two feats, but when i read it from tfrecords, it not synchronize. my data is like\r\n\r\n    a a_1\r\n    b b_1\r\n    c c_1\r\n    d d_1\r\n    e e_1\r\n    f f_1\r\n    g g_1\r\nmy code write this file to tfrecord is like this:\r\n\r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    import sys,os\r\n    \r\n    def _int64_feature(value):\r\n      return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n    \r\n    def _bytes_feature(value):\r\n      return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n    def float32_feature(value):\r\n      return tf.train.Feature(float_list=tf.train.FloatList(value=value))\r\n    \r\n    def _parse_line(line, writer):\r\n      l = line.rstrip().split()\r\n      feat1=l[0].strip()\r\n      feat2=l[1].strip()\r\n    \r\n      example = tf.train.Example(features=tf.train.Features(feature={\r\n          'feat1': _bytes_feature(feat1),\r\n          'feat2': _bytes_feature(feat2)\r\n        }))\r\n      writer.write(example.SerializeToString())\r\n    \r\n    \r\n    def convert_to(feat_file,output_file):\r\n        f = open(feat_file).readlines()\r\n        writer = tf.python_io.TFRecordWriter(output_file)\r\n        for line in f:\r\n            _parse_line(line, writer)\r\n        return\r\n    \r\n    def main(argv):\r\n        convert_to(sys.argv[1],sys.argv[2])\r\n        \r\n    \r\n    if __name__ == '__main__':\r\n        tf.app.run()\r\n        pass\r\n\r\nmy code is train.py is like this:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    import sys,os\r\n    def read_and_decode(filename_queue):\r\n      reader = tf.TFRecordReader()\r\n      _, serialized_example = reader.read(filename_queue)\r\n      features = tf.parse_single_example(\r\n          serialized_example,\r\n          features={\r\n              'feat1': tf.FixedLenFeature([], tf.string),\r\n              'feat2': tf.FixedLenFeature([], tf.string)\r\n          })\r\n      feat1=features['feat1']\r\n      feat2=features['feat2']\r\n      return feat1,feat2\r\n    \r\n    def batch_inputs():\r\n        tf_record_pattern = os.path.join('./', '%s*' % 'record')\r\n        data_files = tf.gfile.Glob(tf_record_pattern)\r\n        print data_files\r\n        filename_queue = tf.train.string_input_producer(data_files, num_epochs=1,shuffle=True)\r\n        feat1,feat2 = read_and_decode(filename_queue)\r\n        feats1,feats2 = tf.train.shuffle_batch([feat1,feat2],batch_size=1, num_threads=1,capacity=1090,min_after_dequeue=1000)\r\n        return feats1,feats2\r\n    with tf.Session() as sess:\r\n        feat1,feat2=batch_inputs()\r\n        init = tf.group(tf.global_variables_initializer(),\r\n    \t       tf.local_variables_initializer())\r\n        sess.run(init)\r\n        coord = tf.train.Coordinator()  \r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        print sess.run(feat1)\r\n        print sess.run(feat2)\r\n\r\nwhen i run this train.py. it outputs \r\n['b']\r\n['d_1']\r\nwhich i suppose  it should output,the feat1 is always corresponding feat2 like\r\n['b']\r\n['b_1']", "comments": ["print sess.run([feat1,feat2])\r\n"]}, {"number": 10261, "title": "Fix typos", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins test this please."]}, {"number": 10260, "title": "Image and label decode from TFRecord are synchronized.", "body": "The data I read from TFRecords has a problem, the image and label are synchronized. In other words, what we want is [a---1, b---2, c---3, d---4, e---5], but what I got is [a---2, b---3, c---4, d---5, e---1]. The version of TensorFlow I used is 1.1.0. Can someone help me? thanks a lot. Here is my code:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimages = ['a', 'b', 'c', 'd', 'e']\r\nlabels = [1, 2, 3, 4, 5]\r\n\r\n\r\ndef convert_to_tfrecords(_images, _labels):\r\n    path = 'test.tfrecords'\r\n    writer = tf.python_io.TFRecordWriter(path)\r\n    for [i, l] in zip(_images, _labels):\r\n        example = tf.train.Example(features=tf.train.Features(feature={\r\n            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(i)])),\r\n            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[l]))\r\n        }))\r\n        writer.write(example.SerializeToString())\r\n    writer.close()\r\n\r\n\r\ndef read_and_decode(filename_queue):\r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n    features = tf.parse_single_example(serialized_example, features={\r\n        'image': tf.FixedLenFeature([], tf.string),\r\n        'label': tf.FixedLenFeature([], tf.int64)\r\n    })\r\n    _image = tf.cast(features['image'], tf.string)\r\n    _label = tf.cast(features['label'], tf.int64)\r\n    return _image, _label\r\n\r\n\r\ndef main():\r\n    convert_to_tfrecords(images, labels)\r\n    filename = 'test.tfrecords'\r\n    filename_queue = tf.train.string_input_producer([filename])\r\n    img, lab = read_and_decode(filename_queue)\r\n    with tf.Session() as sess:\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        for _ in range(5):\r\n            print(sess.run(img), '---', sess.run(lab))\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "comments": ["You should use `sess.run([img, lab])`", "@g21589 Thank you very much for your answer. It is okay now."]}, {"number": 10259, "title": "Retrain.py try download remote archive ", "body": "I want manually set input directory, but retrain.py downloading remote archive. Why so?\r\n\r\n`python retrain.py --image_dir=D:\\123\\flower_photos\\`", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 10258, "title": "Compilation error: image_ops_gpu.cu.pic.o was not created.", "body": "The issue is similar to https://stackoverflow.com/questions/44116381/error-when-install-tensorflow-from-source\r\n/cc @drpngx \r\n```\r\n1 error detected in the compilation of \"/tmp/tmpxft_00004364_00000000-7_image_ops_gpu.cu.cpp1.ii\".\r\n ERROR:PATH/tensorflow_cuda75/tensorflow/contrib/image/BUILD:20:1: output 'tensorflow/contrib/image/_objs/python/ops/_image_ops_gpu/tensorflow/contrib/image/kernels/image_ops_gpu.cu.pic.o' was not created.\r\n```\r\n\r\n```\r\nERROR: PATH/tensorflow_cuda75/tensorflow/contrib/image/BUILD:20:1: \r\nnot all outputs were created or valid.`\r\n```\r\nSelective logs before error:\r\n```\r\n ./tensorflow/contrib/image/kernels/image_ops.h(69): error: Within a __device__/__global__ function, only __shared__ variables may be marked \"static\"\r\n          detected during:                                                                                                                                                 instantiation of \"Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, ArgType>, Device>::CoeffReturnType Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, ArgType>, Device>::coeff(Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, ArgType>, Device>::Index) const [with Generator=tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, tensorflow::uint8>, ArgType=const Eigen::TensorMap<Eigen::Tensor<cons\r\nt tensorflow::uint8, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, Device=Eigen::GpuDevice]\"\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): here                                                                                        instantiation of \"void Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LeftArgType, RightArgType>, Device>::evalScalar(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LeftArgType, RightArgType>, Device>::Index) [with LeftArgType=Eigen::TensorMap<Eigen::Tensor<tensorflow::uint8, 4, 1, Eigen::DenseInd\r\nex>, 16, Eigen::MakePointer>, RightArgType=const Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, tensorflow\r\n::uint8>, const Eigen::TensorMap<Eigen::Tensor<const tensorflow::uint8, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>>, Device=Eigen::GpuDevice]\"\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h(210): here\r\n\r\n\r\n...... omit....\r\n\r\n           instantiation of \"void Eigen::internal::EigenMetaKernelEval<Evaluator, Index, Vectorizable>::run(Evaluator &, Index, Index, Index) [with Evaluator=Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, const Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, double>, c\r\nonst Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>>>, E\r\nigen::GpuDevice>, Index=Eigen::DenseIndex, Vectorizable=false]\"                                         external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h(243): here                               instantiation of \"void Eigen::internal::EigenMetaKernel(Evaluator, Index) [with Evaluator=Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, const Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, double>, const Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, E\r\nigen::DenseIndex>, 16, Eigen::MakePointer>>>, Eigen::GpuDevice>, Index=Eigen::DenseIndex]\"\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h(260): here                               instantiation of \"void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression &, const Eigen::GpuDevice &) [with Expression=const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, const Eigen::Te\r\nnsorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::GPUDevice, double>, cons\r\nt Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>>>, Vect\r\norizable=false]\"\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h(35): here                                  instantiation of \"Eigen::TensorDevice<ExpressionType, DeviceType> &Eigen::TensorDevice<ExpressionType, DeviceType>::operator=(const OtherDerived &) [with ExpressionType=Eigen::TensorMap<Eigen::Ten\r\nsor<double, 4, 1, Eigen::DenseIndex>, 16, Eigen::MakePointer>, DeviceType=tensorflow::functor::GPUDevice, OtherDerived=Eigen::TensorGeneratorOp<tensorflow::generator::ProjectiveGenerator<tensorflow::functor::\r\nGPUDevice, double>, const Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, Eigen::DenseIndex>, 16, Eig\r\nen::MakePointer>>]\"\r\n(156): here\r\n            instantiation of \"void tensorflow::functor::FillProjectiveTransform<Device, T>::operator()(c\r\nonst Device &, tensorflow::functor::FillProjectiveTransform<Device, T>::OutputType *, const tensorflow::\r\nfunctor::FillProjectiveTransform<Device, T>::InputType &, const tensorflow::functor::FillProjectiveTrans\r\nform<Device, T>::TransformsType &) const [with Device=tensorflow::functor::GPUDevice, T=double]\"\r\ntensorflow/contrib/image/kernels/image_ops_gpu.cu.cc(36): here\r\n``` \r\n\r\n\r\nEnvironment: centos 6.8  devtoolset-3 cuda 7.5 cudnn v4. gcc-4.9 bazel 4.5 TF r1.2\r\n\r\n\r\nAny advice and suggestions will be appreciated!", "comments": ["same issue..", "It is possible the build with cuda 7.5 is now broken.\r\nYou can try upgrading to cuda 8.0 and rebuilding.", "Thanks for the prompt response!\r\nTF r.1.2 can successfully be built with cuda 8.0. But, on the workstation, I don't have the privilege to upgrade the current cuda driver 352.93, which didn't support cuda 8.0.\r\n\r\nPS. Commit [a4b352b](https://github.com/tensorflow/tensorflow/commit/a4b352) works fine with cuda 7.5 and also included the updating RNNCell objects changes. ", "As we moved our support to cuda 8.0, the support for 7.5 is best effort.\r\nI cannot promise we will get to this in a timely manner, so I am marking this contributions welcome.\r\nI am happy to accept a PR to fix this on CUDA 7.5 and also does not break CUDA 8.0", "Hi @gunan, I created a PR that fixes this problem.", "Hello,\r\n\r\nI see that the PR was merged into master. Which release and when will the patch be merged into? Has it already been merged into a release?", "I found the commit on r1.3\r\n\r\n@zuxfoucault @jpuigcerver Did you try building from source with the PR merged? It's still not working for me.", "I would recommend upgrading to cuda 8. CUDA 9 is out as an RC now, and once that is released we will swiftly move to CUDA 9, which will make CUDA 7.5 even more obsolete.", "@gunan I have cuda driver 352.93 installed for the geforce titan x with cuda toolkit 7.5\r\n\r\nWill the hardware work with cuda tookit 8?", "The driver will be a problem in your case, too. \r\nIt is possible a new issue was introduced.\r\nAs we do not run continuous tests using older CUDA versions, we would not have caught it.\r\nAgain, we will welcome any contributions to fix any issues.", "@gunan I pulled down v1.3.0 and was able to successfully install tensorflow from built source", "Great, then it looks like this was resolved."]}, {"number": 10257, "title": "Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs", "body": "I got this error message when I ran the app. It crash just a few seconds after app launched.\r\n\r\n```\r\n05-28 09:05:29.791 15453-15472/my.intellij.androidtensorflowbirdexample A/native: tensorflow_jni.cc:304 Error during inference: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs\r\n                                                                                  \t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\r\n05-28 09:05:29.791 15453-15472/my.intellij.androidtensorflowbirdexample A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 15472 (InferenceThread)\r\n                                                                                \r\n                                                                                [ 05-28 09:05:29.796 15453:15503 E/         ]\r\n                                                                                [android_ws] Format: 5, Width: 1080, Height: 1620\r\n                                                                                \r\n                                                                                \r\n                                                                                [ 05-28 09:05:29.796 15453:15503 E/         ]\r\n                                                                                [android_ws] Format: 5, Width: 1080, Height: 1620\r\n```\r\n\r\nPlease advice. Thank you.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@StarRain-L actually I got the problem solved. but since they said need to refer stackoverflow, better you ask there and I will answer it.", "@datomnurdin I have the same problem how did you solve it\r\n"]}]