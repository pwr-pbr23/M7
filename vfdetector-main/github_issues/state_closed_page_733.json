[{"number": 31585, "title": "[ROCm] enable roll op on ROCm.", "body": "", "comments": []}, {"number": 31584, "title": "Keras Optimizer Documentation Fix", "body": "The documentation of Keras Optimizer mention the following:\r\n```python\r\nIf global_step  was not `None`, that operation also increments global_step.\r\n```\r\n\r\nYou can also find it here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Optimizer#apply_gradients\r\n\r\nKeras Optimizer do not use the concept of global_step. It's misleading and shall be removed.\r\n\r\n**Note:** As it impacts directly the TF2.0 documentation. It would be nice if this PR could be cherrypicked in the next beta/official release for TF2.0", "comments": ["Local step not global step. Especially that global step refers to the concept of TF 1.x which doesn't exist anymore in TF 2.x\n\nIt's confusing especially that no argument allow any control on this. \n\n_Sent from my Galaxy S9+ using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_", "Is that a response, or a comment?", "A response to your comment \ud83d\ude0a", "Hmmm...my comment is to add \"increase local step xxx\"....", "Oh I misunderstood your point then.\nI think the optimizer local step needs to be explained better than this. Giving a reference to an attribute not referenced at any other point around is nothing but confusing.\nNonetheless I agree would be nice to mention that it increases an internal counter.\nMaybe you wish to suggest a direction? Want me to add you as a contributor to my fork? ", "> Oh I misunderstood your point then.\r\n> I think the optimizer local step needs to be explained better than this. Giving a reference to an attribute not referenced at any other point around is nothing but confusing.\r\n> Nonetheless I agree would be nice to mention that it increases an internal counter.\r\n> Maybe you wish to suggest a direction? Want me to add you as a contributor to my fork?\r\n\r\nI think it's fine to mention it in this PR (where I suggested). Optimizer has both setter and getter for iteration so it's not a hidden concept.\r\nThough you could name it to be `iterations` instead of `local step`.", "Can one of the admins verify this patch?", "There is another request to fix this. So I made a code change for it instead of waiting here. @DEKHTIARJonathan ", "I suppose you talk about commit 97fb325e3b8499d375251359fd69abd2fa96ee39.\r\nGood with me \ud83d\udc4d "]}, {"number": 31583, "title": "From object detection using Tensorflow", "body": "from utils import label_map_util\r\n\r\nfrom utils import visualization_utils as vis_util \r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-956de605e8fe> in <module>()\r\n----> 1 from utils import label_map_util\r\n      2 \r\n      3 from utils import visualization_utils as vis_util\r\n\r\nC:\\Users\\hp\\Downloads\\models\\research\\object_detection\\utils\\label_map_util.py in <module>()\r\n     24 import tensorflow as tf\r\n     25 from google.protobuf import text_format\r\n---> 26 from object_detection.protos import string_int_label_map_pb2\r\n     27 \r\n     28 \r\n\r\nImportError: cannot import name 'string_int_label_map_pb2'\r\n", "comments": ["Please refer to the following [solution](https://github.com/tensorflow/models/issues/1595#issuecomment-309234210) which will help you in resolving this problem. Also please make sure to post questions like these on [Tensorflow/Models](https://github.com/tensorflow/models/issues) as this issue is related to Models. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as this has been inactive for more than 14 days. Please add additional comments and we can open the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31583\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31583\">No</a>\n"]}, {"number": 31582, "title": "[TF2] Unhashable variables breaks ExponentialMovingAverage", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tf-nightly-2.0-preview\r\n\r\n**Describe the current behavior**\r\nAs described in https://github.com/tensorflow/tensorflow/commit/2e1214094b6a78ab72d39051c7fd6e86c682ddf4#diff-ae1a8f7b66539f000615a4ab7e4b2151 \r\n\r\nVariables are no longer hashable in TF2. This causes the dictionary tracking of variables to break:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L371\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L448\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/moving_averages.py#L462\r\n\r\n**Describe the expected behavior**\r\nWe can likely just keep the variable names as the dictionary keys.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nfoo = tf.Variable(3.0)\r\nema = tf.train.ExponentialMovingAverage(0.1)\r\ndecayed_foo = ema.apply([foo])\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"break.py\", line 5, in <module>\r\n    decayed_a = ema.apply([a])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/moving_averages.py\", line 425, in apply\r\n    if var not in self._averages:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 1085, in __hash__\r\n    raise TypeError(\"Variable is unhashable if Tensor equality is enabled. \"\r\nTypeError: Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as t\r\n```\r\n", "comments": ["Since I added batchnorm to model, MAE assign_average_vars()  broke with this error.\r\n\r\n```\r\n File \"/home/dmitry/.vscode/extensions/ms-python.python-2019.8.30787/pythonFiles/lib/python/ptvsd/__main__.py\", line 316, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/dmitry/PythonProjects/mae_test/mae_test.py\", line 65, in <module>\r\n    optimizer.assign_average_vars(model.variables)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow_addons/optimizers/moving_average.py\", line 119, in assign_average_vars\r\n    assign = tf.group([v.assign(self._ema.average(v)) for v in var_list])\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow_addons/optimizers/moving_average.py\", line 119, in <listcomp>\r\n    assign = tf.group([v.assign(self._ema.average(v)) for v in var_list])\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1144, in assign\r\n    value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1100, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1158, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1237, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 305, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 246, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 254, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/dmitry/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 115, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, handle, device, dtype)\r\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n```\r\n\r\n\r\nCode to reproduce error:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nimport numpy as np\r\n\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = np.expand_dims(x_train, -1) / 255.0, np.expand_dims(x_test, -1) / 255.0\r\n\r\n\r\nmodel = tf.keras.models.Sequential(\r\n    [\r\n        tf.keras.layers.Conv2D(\r\n            16, (3, 3),\r\n            kernel_initializer=tf.keras.initializers.he_uniform(),\r\n            activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Conv2D(\r\n            32, (3, 3),\r\n            (2, 2),\r\n            kernel_initializer=tf.keras.initializers.he_uniform(),\r\n            activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Conv2D(\r\n            32, (3, 3),\r\n            kernel_initializer=tf.keras.initializers.he_uniform(),\r\n            activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Conv2D(\r\n            64, (3, 3),\r\n            (2, 2),\r\n            kernel_initializer=tf.keras.initializers.he_uniform(),\r\n            activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Conv2D(\r\n            64, (3, 3),\r\n            kernel_initializer=tf.keras.initializers.he_uniform(),\r\n            activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.GlobalAvgPool2D(),\r\n        tf.keras.layers.Dense(\r\n            64, kernel_initializer=tf.keras.initializers.he_uniform(),\r\n            activation='relu'),\r\n        tf.keras.layers.Dropout(0.2),\r\n        tf.keras.layers.Dense(\r\n            10, kernel_initializer=tf.keras.initializers.he_uniform(),\r\n            activation='softmax')\r\n    ]\r\n)\r\n\r\noptimizer = tfa.optimizers.LazyAdam(decay=1e-4)\r\noptimizer = tfa.optimizers.MovingAverage(optimizer)\r\n\r\nmodel.compile(optimizer=optimizer,\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(x_train, y_train, batch_size=64, epochs=10)\r\nprint(\"Test metrics before mae:\")\r\nmodel.evaluate(x_test, y_test)\r\n\r\noptimizer.assign_average_vars(model.variables)\r\nprint(\"Test metrics after mae:\")\r\nmodel.evaluate(x_test, y_test)\r\nmodel.save('./model.h5')\r\n\r\n```", "Sean, I think the bug has been fixed by dc3534c ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31582\">No</a>\n", "I have the same error still (tried fresh install of tfp 0.7 and even tfp 0.6), in my case it occurs at \r\n\r\n`tfd.MultivariateNormalDiag(loc, scale).sample()`\r\n\r\n\r\n>     /home/pycharm_project/VAE/vae_tf2.py:636 compute_loss  *\r\n>         latent_code = posterior.sample()\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/distributions/distribution.py:840 sample\r\n>         return self._call_sample_n(sample_shape, seed, name, **kwargs)\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/distributions/transformed_distribution.py:391 _call_sample_n\r\n>         y = self.bijector.forward(x, **bijector_kwargs)\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py:933 forward\r\n>         return self._call_forward(x, name, **kwargs)\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py:904 _call_forward\r\n>         mapping = self._lookup(x=x, kwargs=kwargs)\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py:1343 _lookup\r\n>         mapping = self._from_x[x].get(subkey, mapping).merge(x=x)\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py:151 __getitem__\r\n>         return super(WeakKeyDefaultDict, self).__getitem__(weak_key)\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/bijector.py:181 __hash__\r\n>         return hash(x)\r\n>     /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py:713 __hash__\r\n>         raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\r\n> \r\n>     TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n> \r\n"]}, {"number": 31581, "title": "r2.0-cp: Override eigen strong inline to reduce windows build times for debugg\u2026", "body": "\u2026ing the\r\n\r\nfailures.\r\n\r\nPiperOrigin-RevId: 262857756", "comments": []}, {"number": 31580, "title": "Fix sign mismatch in NEON mean", "body": "Fixes native compilation on aarch64 Ubuntu 18.04. \r\n\r\nSigned-off-by: Matthew Bentham <Matthew.Bentham@arm.com>", "comments": ["@abattery, as you recently added this file, could you help w/ this review? Thx!"]}, {"number": 31578, "title": "Keras Loss Scale Optimizer - Bug Fixes", "body": "Hi,\r\n\r\nI noticed a bug using automatic mixed precision and this optimizer to automatically scale the loss:\r\n\r\n`tf.keras.mixed_precision.experimental.LossScaleOptimizer` does not call __init__ from inherited class. This has some disruptive effects in my work directly. And in addition the following methods fails consequently to not calling: `super(LossScaleOptimizer, self).__init__(*args, **kwargs)`\r\n\r\n- `optimizer.variables()`\r\n- `optimizer.get_weights()`\r\n- `optimizer.weights`\r\n\r\n```\r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 735, in variables\r\n    return self._weights\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 551, in __getattribute__\r\n    raise e\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 541, in __getattribute__\r\n    return super(OptimizerV2, self).__getattribute__(name)\r\nAttributeError: 'LossScaleOptimizer' object has no attribute '_weights'\r\n```\r\n\r\n**Note:** This change is important for AMP in TF2.0, therefore if it could be cherrypicked that would be terrific.\r\n\r\n@reedwm @nluehr  FYI\r\n\r\nThanks", "comments": ["Thank you for the PR. We intentionally do not call `super(LossScaleOptimizer, self).__init__(*args, **kwargs)`, as we do not want to initialize any OptimizerV2 state, but instead delegate to the wrapped optimizer.\r\n\r\nFor the methods you mentioned that currently don't work, that is a bug, and the LossScaleOptimizer should delegate to the wrapped optimizer. For this PR, can you instead directly call `self._optimizer`'s methods for the methods that do not currently work?\r\n\r\nAlso, you mentioned \"This has some disruptive effects in my work directly\". Can you elaborate? If there are issues with the current LossScaleOptimizer design, I want to make sure we can resolve them. Thank you.", "Why preventing the optimizer from initializing any OptimizerV2 state ?\r\n\r\nAs TF2.X removes `tf.global_variables`, the only way we found to access the current loss scale tensor is by keeping a list of the defined optimizers. And you can easily do this by monkey patching the `__init__` method of Keras Optimizers.\r\n\r\nI can't pass a python object linking to the Loss Scale optimizer. And I'm not using Keras Callbacks therefore can't access directly the optimizer => Using GradientTape. If you have any other idea ?\r\n\r\nMoreover, a subclass having a different behavior from all the other subclasses seems a very dangerous move.\r\n\r\nIf your objective is to forward calls to the base optimizer and prevent any internal behavior of LossScaleOptimizer aside of the loss scaling, the implementation by Horovod for distributed training seems a much better design: https://github.com/horovod/horovod/blob/master/horovod/_keras/__init__.py#L22-L70\r\n\r\nWould you be opened to a modification in this direction ? Would probably be the best compromise \n\n_Sent from my Galaxy S9+ using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_", "> Why preventing the optimizer from initializing any OptimizerV2 state ?\r\n\r\nThe reason is that LossScaleOptimizer warps another optimizer. The other optimizer holds all the state, so LossScaleOptimizer should not try to recreate the state on it's own.\r\n\r\n> the only way we found to access the current loss scale tensor is by keeping a list of the defined optimizers\r\n\r\nCan you clarify? You can access the current loss scale tensor with the `LossScaleOptimizer.loss_scale` property.\r\n\r\n> Moreover, a subclass having a different behavior from all the other subclasses seems a very dangerous move.\r\n> \r\n> If youc objective is to forward calls to the base optimizer and prevent any internal behavior of LossScaleOptimizer, the implementation by Horovod for distributed training seems a much better design: https://github.com/horovod/horovod/blob/master/horovod/_keras/__init__.py#L22-L70\r\n> \r\n> Would you be opened to a modification in this direction ? Would probably be the best compromise\r\n\r\nAgreed the current design of LossScaleOptimizer is somewhat dangerous. Horovod's approach of dynamically creating a subclass with the `type` constructor is clever.\r\n\r\n@fchollet @alextp, thoughts on using Python's [type()](https://docs.python.org/3/library/functions.html#type) constructor to dynamically create a LossScaleOptimizer that inherits from the appropriate subclass? Currently, LossScaleOptimizer inherits from OptimizerV2 and wraps another optimizer. This means if a user passes an SGD optimizer, the returned LossScaleOptimizer will not be an instance of SGD, but just an instance of OptimizerV2.\r\n\r\nWith the type() approach, if a user passes an SGD, we return a LossScaleOptimizer that subclasses from SGD. And, instead of wrapping the SGD, we will create a new LossScaleOptimizer that inherits from SGD, copying the configuration from the old SGD. The big advantage is this means we can completely emulate the interface of SGD, with the addition of adding loss scaling. The disadvantage is using the type() constructor adds a significant complexity cost.", "> Can you clarify? You can access the current loss scale tensor with the LossScaleOptimizer.loss_scale property.\r\n\r\nAs said above I can't pass around a reference to the optimizer. Mainly because I don't have access to the user's code. As I can't get variable by name anymore like with TF 1.x, I need to find another way.\r\nWith Keras Callbacks it's easy because you can go with `self.model.optimizer`. But if someone use GradientTape, I still need a way to access the Optimizer. Patchting `__init__` is a non intrusive easy way to perform this task. And honestly I have no other idea (aside of checking each Python object not garbage collected).\r\n\r\n> The big advantage is this means we can completely emulate the interface of SGD, with the addition of adding loss scaling. The disadvantage is using the type() constructor adds a significant complexity cost.\r\n\r\nIndeed that increases the complexity, the beauty of this approach is that it somehow feels natural. SGD Optimizer stays an SGD Optimizer. Only the way gradients are computed / applied changes. And in essence that would fit perfectly your requirements, namely those about OptimizerV2 State", "I think it's worth prototyping this extra magic, but the need for this is\npart of the reason why I don't like the loss scale optimizer inheriting\nfrom existing optimizers and would prefer it to be a wrapper or something\nlike that.\n\nOn Tue, Aug 13, 2019 at 12:08 PM Reed <notifications@github.com> wrote:\n\n> Why preventing the optimizer from initializing any OptimizerV2 state ?\n>\n> The reason is that LossScaleOptimizer warps another optimizer. The other\n> optimizer holds all the state, so LossScaleOptimizer should not try to\n> recreate the state on it's own.\n>\n> the only way we found to access the current loss scale tensor is by\n> keeping a list of the defined optimizers\n>\n> Can you clarify? You can access the current loss scale tensor with the\n> LossScaleOptimizer.loss_scale property.\n>\n> Moreover, a subclass having a different behavior from all the other\n> subclasses seems a very dangerous move.\n>\n> If youc objective is to forward calls to the base optimizer and prevent\n> any internal behavior of LossScaleOptimizer, the implementation by Horovod\n> for distributed training seems a much better design:\n> https://github.com/horovod/horovod/blob/master/horovod/_keras/__init__.py#L22-L70\n>\n> Would you be opened to a modification in this direction ? Would probably\n> be the best compromise\n>\n> Agreed the current design of LossScaleOptimizer is somewhat dangerous.\n> Horovod's approach of dynamically creating a subclass with the type\n> constructor is clever.\n>\n> @fchollet <https://github.com/fchollet> @alextp\n> <https://github.com/alextp>, thoughts on using Python's type()\n> <https://docs.python.org/3/library/functions.html#type> constructor to\n> dynamically create a LossScaleOptimizer that inherits from the appropriate\n> subclass? Currently, LossScaleOptimizer inherits from OptimizerV2 and wraps\n> another optimizer. This means if a user passes an SGD optimizer, the\n> returned LossScaleOptimizer will not be an instance of SGD, but just an\n> instance of OptimizerV2.\n>\n> With the type() approach, if a user passes an SGD, we return a\n> LossScaleOptimizer that subclasses from SGD. And, instead of wrapping the\n> SGD, we will create a new LossScaleOptimizer that inherits from SGD,\n> copying the configuration from the old SGD. The big advantage is this means\n> we can completely emulate the interface of SGD, with the addition of adding\n> loss scaling. The disadvantage is using the type() constructor adds a\n> significant complexity cost.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31578?email_source=notifications&email_token=AAABHRIPMLU4S7YAJGREHBTQEMBBPA5CNFSM4ILI6ZY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4GUY6A#issuecomment-520965240>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLKWH7LM3AL7LELIJTQEMBBPANCNFSM4ILI6ZYQ>\n> .\n>\n\n\n-- \n - Alex\n", "@reedwm how do you want to proceed? ", "@DEKHTIARJonathan, you said you want to get a reference to the loss scale tensor, and that monkey patching `__init__` allows you to do this. How does improving the interface of LossScaleOptimizer help with this? Even if we go with the Horovod approach and the LossScaleOptimizer completely emulates the interface of the passed optimizer, it doesn't seem to solve the problem of getting a reference to the loss scale.\r\n\r\nI agree we need to improve the LossScaleOptimizer interface, but I'm not sure how it solves your problem.", "What I need is that the Optimizer keeping the property loss_scale to call init method. If you dynamically generate this class based from any other optimizer its still good for me \ud83d\udc4d\n\nI will try tomorrow if I can use `__new__` instead, might be easier on your side.\n\nAnyway let me know how I can help. Having an optimizer which fails for 3 important methods is not an ideal scenario.\n\nBTW. `get_config` is also failing. And this one is even more dramatic because its required to serialize the optimizer", "Yeah, I will try dynamically generating the class. I will also fix `get_config`. LossScaleOptimizer is being worked on and still has many issues.", "Can one of the admins verify this patch?", "Hi @reedwm,\r\n\r\n@nluehr and myself have prototyped the following solution. I'll try to give you a rapid overview of the changes.\r\n\r\n### How to test\r\n\r\nI have setup a demo repository, should take <5 mins to test the changes.\r\nhttps://github.com/DEKHTIARJonathan/KerasLossScaleOptimizer_Demo\r\n\r\n### Feature Lists:\r\n\r\n- [x] fully fix all the missing properties\r\n- [x] fully fix optimizer serialization/deserialization\r\n- [x] support for DynamicLossScaling and FixedLossScaling save and restore current state\r\n- [x] does not change the original user API => no need to update any model\r\n- [x] does not recreate any additional optimizer (as requested above)\r\n- [x] does not create an additional OptimizerV2 state (as requested above)\r\n- [x] we can apply the same design pattern to Horovod Keras DistributedOptimizer => Suffer from the same issues\r\n\r\nBasically we tried to propose a solution which does not re-instantiate an OptimizerV2 wrapping the previous one. Thus leading to only one OptimizerV2 defined with the proposed solution.\r\n\r\nAfter some investigation, we did not follow the implementation highlighted above implemented by Horovod because it essentially dynamically recreates a new optimizer inheriting from the base opt (and could/should delete the old one). Aside of recreating an additional OptimizerV2, Horovod solution does not support serialization/deserialization.\r\n\r\n## Proposed Solution\r\n\r\n- Take the original OptimizerV2 defined by the user\r\n- Monkey patch the instance to add the loss scaling mechanic directly\r\n- Make sure we still support the original OptimizerV2 API: properties/from_config/get_config\r\n- Make sure the user API is unchanged => No need to update any user code. It's plug'n'play.\r\n\r\n## Known Issue\r\n\r\n- TF does not allow to restore an optimizer not defined in optimizer_v2 module: we had to add this line. If fixed this can be removed: https://github.com/tensorflow/tensorflow/pull/31578/files#diff-237d1c954fd5432666f5e731878e2598R378\r\n\r\n- We had to define a metaclass which can be useful to modify any OptimizerV2: https://github.com/tensorflow/tensorflow/pull/31578/files#diff-237d1c954fd5432666f5e731878e2598R50 I believe we should move this one in the `optimizer_v2.py` file. It can be useful in many frameworks (e.g. Horovod)\r\n\r\nWaiting for your feedbacks", "Hi @DEKHTIARJonathan,\r\n\r\nThank you for the prototype! Monkey patching attributes on the optimizer is an interesting idea I haven't considered until now. I also have a prototype that uses the Horovod approach that is under review which also supports serialization.\r\n\r\nMonkey patching is convenient, because it does not create new optimizer state. However, changing an instance by monkey patching can be confusing for a user, as their instance's behavior will fundamentally change. Additionally, changing `opt.__class__` is hacky, and I imagine will be controversial. However, it is also confusing to create a brand-new optimizer and ignore the original optimizer, only copying some of it's configuration, which is what my prototype does. I am undecided on which approach to go with.\r\n\r\nI'm pretty sure you can modify the monkey-patching approach to not change `opt.__class__`. [This link](https://filippo.io/instance-monkey-patching-in-python/) specifies a method to monkey patch individual methods. However, we may also want to be able to pass an `isinstance(opt, LossScaleOptimizer)` check, which is only possible by modifying `opt.__class__` with the monkey-patching approach. You currently fail that check, but it would be possible to add as long as you continue to monkey patch `opt.__class__`.\r\n\r\nReading your implementation, one thing I am confused about is the need for BaseOptimizer. Why is this class needed? You can always use `super` to call a base method.\r\n\r\nIn any case, I recommend not making any changes until we determine what approach we want to take here, to avoid doing any additionally work that might not be used. Unfortunately, my primary reviewer is OOO until next week, but once he is back, we can discuss with him.", "Hi \r\n\r\nThanks for your message @reedwm. We actually tried to implement the fix using Horovod approach.\r\n\r\n**You can find in the demo repository two different approaches to solve the problem:**\r\n- by inheritance (Horovod approach): https://github.com/DEKHTIARJonathan/KerasLossScaleOptimizer_Demo/tree/by_inheritance\r\n- by monkey patch: https://github.com/DEKHTIARJonathan/KerasLossScaleOptimizer_Demo/tree/by_monkey_patch\r\n\r\n**We ended up not taking the Horovod for a few reasons:**\r\n- This approach initializes a new optimizer and OptimizerV2 state => You were explicitly against this.\r\n- The original optimizer (Adam, SGD, etc.) is not used and should be garbage collected\r\n- Save and restore is quite convoluted ... Doable but not ideal (I implemented it in the link above).\r\n\r\n**Regarding the monkey patch:**\r\n- Monkey patch is usually not considered \"clean\" or \"safe\". However, here it actually makes a lot of sense. We want to \"plug\" in the Adam/SGD/etc. original optimizer and modify the original behavior. I tried my best to implement it in the cleanest fashion as I could. If you have any feedback to improve on this matter, feel free to directly comment the diff.\r\n\r\n- We have to monkey patch the object `__class__` due to the use of properties and classmethods (namely `from_config`). If we don't patch the `__class__` attribute, you would directly modify the original class => Adam/SGD/etc. Which you absolutely don't want. Hence the only safe strategy (at least that I know) is to create a copy of the class dynamically. This also allow to address the `isinstance()` concern as pointed out.\r\n\r\n- This approach introduces a metaclass called: `OptimizerV2MetaFactory` which, if merged in `optimizer_v2.py`, can become a standard approach / design pattern to interact with the Keras Optimizer (I believe that Horovod is likely to adopt the approach if made standard). Can be very interesting/useful in research when you try to modify how the gradient is used / computed / distributed.\r\n\r\n---------------------\r\n\r\n> changing an instance by monkey patching can be confusing for a user\r\n\r\nI don't really agree. Very few users will in fact open the code inside the TF codebase. And even if they do it, monkey patching is quite a common/usual practice in python. Nonetheless, true we should write some developer documentation. Let me know what kind of information would make you feel more comfortable.\r\n\r\n> I'm pretty sure you can modify the monkey-patching approach to not change `opt.__class__`\r\n\r\nI tried and honestly that would tremendously increase the code complexity (see above why). And not even sure it's doable without impacting the base class (Adam/SGD/etc.) due to classmethods/properties.\r\n\r\n> we may also want to be able to pass an `isinstance(opt, LossScaleOptimizer)`\r\n\r\nThis one is definitely doable. I'll try to update my PR this week fixing this issue. I'll try to implementing the following:\r\n```python\r\nisinstance(opt, LossScaleOptimizer)  # True\r\nisinstance(opt, Adam)  # True\r\n```\r\n\r\n> BaseOptimizer. Why is this class needed? You can always use super to call a base method.\r\n\r\nTrue I could use `super()`. However, it makes the code more understandable that way and I wanted to ease code review. I can actually remove it ;) However, you raised concerns about the approach being confusing, this approach could be a way to make the code more \"readable\" => In case of multiple inheritance, `super()` MRO is not always crystal clear to lambda users. This approach has the advantage of making it obvious which base method is called. It's your call, at the end it really depends on who is supposed to read this code. \r\n\r\nI hope it helped to address your concerns, please let me know if I can be of any further help\r\n\r\n@nluehr FYI", "@DEKHTIARJonathan Could you please resolve the conflicts? Thanks!", "You might want to hold off resolving conflicts until we discuss the correct approach that should be take for loss scaling.\r\n\r\n(@DEKHTIARJonathan and I discussed offline, and we still need to decide how the LossScaleOptimizer should be handled)", "Please do not close. Currently OOO. Planning to keep on working this soon", "Will restart work early november", "Feel free to ignore the bot. If it closes this PR, I will reopen it.", "@reedwm Any update on this PR, please. "]}, {"number": 31577, "title": "TensorFlow Lite BroadcastTo", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\nTensorFlow installed from (source or binary):source\r\nTensorFlow version (use command below): tf-nightly\r\nPython version: Python 3.6\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: NVIDIA GeForce GTX 1050 Ti\r\n\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DIV, FULLY_CONNECTED, MAXIMUM, MAX_POOL_2D, MEAN, PAD, RESHAPE, SOFTMAX, SPLIT_V, SQRT, SQUARE, STRIDED_SLICE, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BroadcastTo.\r\n```\r\n\r\n[frozen_graph.pb](https://drive.google.com/open?id=1z-TPW-n0OziQ6-5ix1sCMzBMT5qmlbv-)\r\n[full_log.txt](https://github.com/tensorflow/tensorflow/files/3495827/full_log.txt)\r\n\r\n\r\n**Any other info / logs**\r\nI am trying to convert a person re-identification model to tflite to run on Android. However it seems that the operator BroadcastTo is unsupported. I'm not sure where this operator is even used, because searching on TensorBoard returns nothing. I am aware that it is possible to create [custom operators](https://www.tensorflow.org/lite/guide/ops_custom), however I instantly got lost at the C++ code, I don't even know where to place the new operator if I manage to make it. And even then I'd have to compile a new AAR and use JNI in order to use it on Android.\r\n\r\nThe command I used is:\r\ntoco --graph_def_file=output/frozen_graph.pb --output_file=output/model.tflite --input_shapes=2,1,160,60,3 --input_arrays=images --output_arrays=Softmax --output_format=TFLITE --inference_type=FLOAT --input_data_type=FLOAT\r\n", "comments": ["After a ton of debugging, the problem is two calls to tf.subtract that are causing the broadcast error. However, the [documentation](https://www.tensorflow.org/lite/guide/ops_compatibility) says \"and broadcasting is only support in a limited number of ops (tf.add, tf.mul, tf.sub, and tf.div)\".\r\n\r\nIs anyone aware of a workaround or something?", "@srjoglekar246 can you help investigate to see if BroadcastTo is truly necessary here? Or can be pruned away?", "And after looking into it further, the problem is that I'm using 6 dimensional tensors. Tensorflow lite seems to only support a maximum of 4. This problem has been solved with Tensorflow 2.0 but it's quite a pain to have to upgrade all the code, especially when I didn't write it.", "### Correction\r\n**Tensorflow 2.0 does not solve the issue**,\r\n\r\nthe reason I thought it was solved with 2.0 is because I tried a minimal example to try to reproduce the error, and it worked after compiling with 2.0. However, I just spent the last 3 days upgrading my entire model to 2.0 only to find the same problem with BroadcastTo not being supported.", "To me it seems that armeabi-v7a has problems with broadcast operations.\r\nAlthough, according to unit tests it must be supported. In practice, trying to multiply 4D array of input data with a constant of 0.1 (same type as the array) cause a crash of an application. No reasonable error message pops up. I'm not sure about arm64-v8a, but armeabi-v7a has errors.", "@nartes  can you be more specific? Do you have a minimal repro case?", "@jdduke I needed tensorflow.keras.layers.LeakuReLU layer. But you do support only tensorflow.nn.relu and tensorfluw.nn.relu6 according to my experiments.\r\n```python\r\ndef like_constant(x, c):\r\n    return tensorflow.tile(\r\n            tensorflow.reshape(\r\n                c,\r\n                tensorflow.ones(\r\n                    (\r\n                        tensorflow.shape(\r\n                            tensorflow.shape(x)\r\n                        )[0],\r\n                    ),\r\n                    dtype=numpy.int32\r\n                )\r\n            ),\r\n            tensorflow.shape(x)\r\n        )\r\n\r\n\r\ndef adhoc_relu(x, alpha=None):\r\n    if alpha is None:\r\n        alpha = 0.1\r\n\r\n    return -tensorflow.nn.relu(-x) * like_constant(x, alpha) + \\\r\n        tensorflow.nn.relu(x)\r\n\r\ndef adhoc_relu_layer(**kwargs):\r\n    return tensorflow.keras.layers.Lambda(\r\n        functools.partial(\r\n            adhoc_relu,\r\n            **kwargs\r\n        )\r\n    )\r\n```\r\n\r\nThe code above works well on armeabi-v7a. Although I've a problem. 18MiB of neural network weights are coupled with 60MiB of 0.1 constants.\r\nIf I do replace like_constant(x, alpha) with 0.1, a tensorflowlite jni library crashes upon graph initialization on armeabi-v7a. I'm not talking about an android emulator though. Since in there, it seems to work.\r\n\r\nI've been using 0.0.0-nightly prebuilt artifacts.\r\n```gradle\r\n    repositories {\r\n        google()\r\n        jcenter()\r\n        mavenLocal()\r\n    }\r\n\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n```\r\n\r\nAlso, I've checked that you do have unit tests that verify broadcasting rules. But still, it doesn't work on armeabi-v7a.", "Would you mind sharing the .tflite model you're using? And also can you share what device you're testing with that is generating the error? Thanks.", "The problem was that I can't use the add or subtract operations on tensors with more than 4 dimensions.", "@renjie-liu can you take a look to see which operators would need to be updated to support this model (w/ 5/6D tensors)? Thanks.", "Sure, will take a look.\n\nOn Fri, Dec 27, 2019 at 12:40 AM Jared Duke <notifications@github.com>\nwrote:\n\n> @renjie-liu <https://github.com/renjie-liu> can you take a look to see\n> which operators would need to be updated to support this model (w/ 5/6D\n> tensors)? Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31577?email_source=notifications&email_token=AIURNGMBO5TUXNXAFO2T453Q2TM7ZA5CNFSM4ILIC3SKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHVZM4A#issuecomment-569087600>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIURNGMHMIOWVAKKWGLIRX3Q2TM7ZANCNFSM4ILIC3SA>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "Hi, wonder can you explain your usage a little bit? is 5D/6D necessary?\r\n\r\nThanks!", "FYI, we have fixed regarding broadcast to operation support up to 8 dim. Please try your conversion script with the tomorrow's nightly version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31577\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31577\">No</a>\n"]}, {"number": 31576, "title": "AttributeError: module 'tensorflow' has no attribute '__version__'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI installed tensorflow 1.10, and it worked fine. After I installed tensorflow-serving-api 1.10, error occured.   \r\n```\r\nOriginal exception was:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n\r\n```\r\nAnd the result of dir(tf) is :\r\n```\r\n>>> dir(tf)\r\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\r\n```\r\nIt seems that the \"__init__.py\" under tensorflow has been cleaned.   \r\nBefore installation of tf-serving-api: \r\n```\r\n[root@eac5952e1443 /]# cat /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py \r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n\r\n...\r\n_names_with_underscore = ['__version__', '__git_version__', '__compiler_version__', '__cxx11_abi_flag__', '__monolithic_build__']\r\n__all__ = [_s for _s in dir() if not _s.startswith('_')]\r\n__all__.extend([_s for _s in _names_with_underscore])\r\n__all__.remove('print_function')\r\n\r\n\r\nfrom tensorflow.python.util.lazy_loader import LazyLoader  # pylint: disable=g-import-not-at-top\r\ncontrib = LazyLoader('contrib', globals(), 'tensorflow.contrib')\r\ndel LazyLoader\r\n\r\nfrom tensorflow.python.platform import flags  # pylint: disable=g-import-not-at-top\r\napp.flags = flags  # pylint: disable=undefined-variable\r\n\r\ndel absolute_import\r\ndel division\r\ndel print_function\r\n\r\n# These symbols appear because we import the python package which\r\n# in turn imports from tensorflow.core and tensorflow.python. They\r\n# must come from this module. So python adds these symbols for the\r\n# resolution to succeed.\r\n# pylint: disable=undefined-variable\r\ndel python\r\ndel core\r\n# pylint: enable=undefined-variable\r\n```\r\nAfter installation of tf-serving-api:\r\n```\r\n[root@eac5952e1443 /]# cat /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py \r\n[root@eac5952e1443 /]# \r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\n>>> print(tf.__version__)\r\n1.10.0\r\n```\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\npip install tensorflow==1.10\r\npip install tensorflow-serving-api==1.10\r\npython\r\n>> import tensorflow as tf\r\n>> print(tf.__version__)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nOf course I know that it would work fine if I reinstall tensorflow after tf-serving-api installation. But I need to install all of these into a docker image from Dockerfile. Reinstall seems not to be a good choice. Anyone has any suggestions? Thanks.", "comments": ["TensorFlow 1.10 is long outside of the support window as it has been released more than a year ago.\r\n\r\nPlease try using a newer version and let us know if issue persists.", "@mihaimaruseac Using TF 1.14 with TF Serving 1.14, doesn't result in any error. Please find the colab notebook [here](https://colab.sandbox.google.com/gist/gowthamkpr/b915a82d399c2e1a91f035574e87c93c/untitled91.ipynb)", "@elvys-zhang Can I close the issue as it has been resolved. Thanks!", "@gowthamkpr Sure. I found that this issue has been fixed in tensorflow serving 1.10.1. Thanks"]}, {"number": 31575, "title": "Add CancellationManager for PrefetchDataset", "body": "This PR replaces the cancellation logic in `PrefetchDataset` with a CancellationManager.", "comments": ["@jsimsa Thanks for the prompt review! The comments are addressed [here](https://github.com/tensorflow/tensorflow/pull/31575/commits/6bb73d674dfb0441c89cc664dfb1fe20fb383c70). Please take another look when you get a chance!", "This PR is rebased to resolve the conflicts with https://github.com/tensorflow/tensorflow/commit/98ae57bef336f099986783e0500693b31c03dd56. @jsimsa, could you please take another look?"]}, {"number": 31574, "title": "Fix \"TypeError\" when loading keras h5 model", "body": "h5 model used by keras was saved using json.dump function\u3002 json.dump function silently converts int keys to string , [silently converts int keys to string](https://bugs.python.org/issue34972).  The keys in \"constant\" dict are INT type, but they convert to STRING in the saved model. \r\nFor the above reasons\uff0cI got \"TypeError: 'str' object cannot be interpreted as an integer\" ERROR, when loading my h5 model.\r\nConverting \"index\" variable to int type fixes the problem.", "comments": []}, {"number": 31573, "title": "use estimator got InvalidArgumentError Cannot assign a device for operation and allow_soft_placement: true doesn't work", "body": "I create estimator and preditor based on tensor2tensor following code\r\n\r\n```\r\nhp = create_hparams()\r\ndecode_hp = create_decode_hparams()\r\nrun_conf = t2t_trainer.create_run_config(hp)\r\nestimator = trainer_lib.create_estimator(\r\n    FLAGS.model,\r\n    hp,\r\n    run_conf,\r\n    decode_hparams=decode_hp,\r\n    use_tpu=FLAGS.use_tpu)\r\npredictor=tf.contrib.predictor.from_estimator(estimator, input_fn)\r\n```\r\n\r\nthen got\r\n\r\n> InvalidArgumentError: Cannot assign a device for operation transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n> Colocation Debug Info:\r\n> Colocation group had the following types and supported devices: \r\n> Root Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]\r\n> ImageSummary: CPU \r\n> \r\n> Colocation members, user-requested devices, and framework assigned devices, if any:\r\n>   transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention (ImageSummary) /device:GPU:0\r\n> \r\n> Op: ImageSummary\r\n> Node attrs: max_images=1, T=DT_FLOAT, bad_color=Tensor<type: uint8 shape: [4] values: 255 0 0...>\r\n> Registered kernels:\r\n>   device='CPU'\r\n> \t [[{{node transformer/body/parallel_0/body/encoder/layer_0/self_attention/multihead_attention/dot_product_attention/attention}}]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n\r\nwhen I print session_config\r\n`print(run_conf.session_config)`\r\n\r\nI got \r\n\r\n\r\n> gpu_options {\r\n>   per_process_gpu_memory_fraction: 0.95\r\n> }\r\n> **allow_soft_placement: true**\r\n> graph_options {\r\n>   optimizer_options {\r\n>     global_jit_level: OFF\r\n>   }\r\n> }\r\n> isolate_session_state: true\r\n\r\n\r\nstill can't solve this problem", "comments": ["I solved it. Explained in this [issue](https://github.com/tensorflow/tensorflow/issues/30782)"]}, {"number": 31572, "title": "batch inference is as slow as single image inference in tensorflow c++", "body": "OS:Ubuntu 16.04\r\nversion:Tensorflow c++ 2.0-beta1 (compiled with all optimization flag:AVX AVX2 SSE4.1 SSE4.2 FMA XLA)\r\nIDE:eclipse\r\nWith CUDA:No (just CPU in my prediction)\r\nI have test the time of single image inference with tensorflow c++ api is 0.02 seconds which is so long that i just can not believe with my own eyes because i have compiled tensorflow c++ shared library with all the optimizations such as AVX/AVX2/FMA/SSE4.1/SSE4.2/FMA. However,i have to find the solution to decrease the cost time in prediction.Someone tells me the time can hugely decrease if i use batch inference instead of single image inference.Unfortunately,the time is 0.7 seconds in batch inference when the batch size is 32.In another word,0.7/32=0.02,it is as slow as single image inference.\r\nThis issue is related to tensorflow c++ performance so i ask it here and any help will be much appreciated.I have also put detailed information here [stackoverflow](https://stackoverflow.com/questions/57460782/batch-inference-is-as-slow-as-single-image-inference-in-tensorflow-c) .", "comments": ["Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I meet the same problem, I used tensoflow-gpu = 1.10.0, 1.12.0 and 1.14.0, the time of inference with batch_size = 1 is 80ms, and time of inference with batch_size=2 is 155ms. So in my opinion\uff0cbatch inference is so fast compared to single image inference,Is that right? "]}, {"number": 31571, "title": "Add big-endian support for LMDB-related tests", "body": "The test cases `//tensorflow/python/kernel_tests:reader_ops_test` and `//tensorflow/contrib/data/python/kernel_tests:lmdb_dataset_op_test` have been failing in our s390x (System Z) builds due to a byte order issue.\r\n\r\nThe LMDB database library supports both big- and little-endian architectures, but the library uses different on-disk formats depending on the byte order of the host machine. I took a look through the [source code](https://github.com/LMDB/lmdb/blob/mdb.master/libraries/liblmdb/mdb.c), and this difference appears to be deliberate.\r\n\r\nThis pull request adds a big-endian version of the binary input file used by both failing test cases. To generate this file, I first wrote a Python script that generates a byte-for-byte identical version of the original file when run on an x86_64 machine. Then I ran this script on an s390x machine.\r\n\r\nI also updated the comments in `tensorflow/core/BUILD` with a more complete description of the process that produced `lib/lmdb/testdata/data.mdb`.\r\n\r\nI have added code in both test cases that chooses an appropriate input file based on the byte order of the host machine.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 31570, "title": " tensorflow estimator doesn't need to depend on tensorflow", "body": "Tensorflow estimator (https://github.com/tensorflow/estimator) installs only a set of python files, and doesn't really require Tensorflow to build. However, its build looks for Tensorflow for some reason.\r\n\r\nPlease remove the build-time Tensorflow dependency in Tensorflow estimator.\r\n\r\nEstimator should actually be either a part of Tensorflow itself, or Tensorflow should depend on it because the Tensorflow's python code imports it.\r\n\r\n(Created a bug here because the estimator project doesn't allow issues for some reason.)", "comments": ["The Estimator package is pulled into TF proper, such that installing tensorflow includes tensorflow_estimator. The separate estimator package is not designed to be used independently per se, as it is part of the main TF package.", "For slightly more context: Estimator is separated from main TensorFlow for separation of concerns, to modularize code (see also https://github.com/tensorflow/community/pull/77). This allows for faster development and can make it function like a plugin.\r\n\r\nHowever, Estimator is not designed to be used individually or to just do `import tensorflow_estimator` directly.\r\n\r\nWe are currently working on making this a better situation but the above context is our end goal", "I committed the FreeBSD port for TensorFlow but then I had to create a separate port for estimator because it was missing."]}, {"number": 31569, "title": "[INTEL MKL] Fix for Batchmatmul regression.", "body": "BatchMatmul now uses the BatchMatMulV2 operator, however TF+MKL hadn't been updated accordingly.  Fixed this issue and also added missing unit tests. Since V2 is now the default, I replaced MklBatchMatMul with V2 operator rather than adding a new one.", "comments": ["Temporarily closing this. I will reopen shortly."]}, {"number": 31568, "title": "Tensorflow 1.14.0 with C++17 Custom Op Has Binary Incompatibility", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, I have a custom op written in CUDA.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.5.6\r\n- Bazel version (if compiling from source): 0.25.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Nvidia GTX 1080\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I ran ```tf.load_library(\"my_ops.so\")```\r\nIt will complain about:\r\n```\r\nNotFoundError: my_ops.so: undefined symbol: _ZN10tensorflow6StatusC1ENS_5error4CodeESt17basic_string_viewIcSt11char_traitsIcEE\r\n```\r\n\r\n**Describe the expected behavior**\r\nSuccessfully `load_library`\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nTensorflow 1.14 was built with `bazel build --config=opt --config=cuda  --copt=-march=native --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package`\r\nMy custom op is built with Bazel.\r\n", "comments": ["@golden0080 \r\n\r\nCan you please provide detailed reproduction instructions to reproduce it on our environment.Thanks!", "@ravikyram Sorry I was busy. I'm trying to get you a reproducible example for my issue.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31568\">No</a>\n", "I ran into the same issue using tensorflow 1.13.1 and gcc 7.4, and fixed it by patching `tensorflow/include/absl/base/config.h` to not define `ABSL_HAVE_STD_STRING_VIEW` even when the C++17 `string_view` is detected (by commenting line 432).", "@pmh47 I tested in my code base, using bazel to patch absl, and it worked! I think the TF were not using `C++ string_view` correctly thus resulted in missing symbols."]}, {"number": 31567, "title": "TensorFlow for C doesn't compile - cannot open source file \"tensorflow/c/tf_attrtype.h\"", "body": "My team is requesting that we use the C API for tensorflow.\r\n\r\nI followed the instructions outlined under \"Install TensorFlow for C\" (https://www.tensorflow.org/install/lang_c). However, when I attempt to compile it fails with the message:\r\n\r\n> cannot open source file \"tensorflow/c/tf_attrtype.h\"\r\n\r\nI am using Windows 10 with Visual Studio 2017. The steps used for installation/compilation are as follows:\r\n\r\n1.  Download Windows CPU zip file\r\n2. Extract to desktop folder tf-demo\r\n   `C:\\Users\\....\\Desktop\\tf-demo`\r\n3. Start Visual Studio 2017\r\n4. Create a Console application project\r\n5. Incorporate Example program in Visual Studio's 'main' file/routine.\r\n6. Update Project's Additional Include Directories\r\n   6.1 Right click on project\r\n   6.2 Select Properties\r\n   6.3 Expand Configuration\r\n   6.4 Expand C/C++\r\n   6.5 Select General\r\n   6.6 Select Additional Include Directories\r\n   6.7 Enter the path to the include files\r\n       `C:\\Users\\....\\Desktop\\tf-demo\\include`\r\n   6.8 [Ok]\r\n   6.9 [Ok]\r\n7. Disable precompiled Headers\r\n   7.1 Right click on project\r\n   7.2 Select Properties\r\n   7.3 Expand Cnfiguration\r\n   7.4 Expand C/C++\r\n   7.5 Select Precompiled Headers\r\n   7.6 Change \"Precompiled Headers\" to \"Not using....\"\r\n   7.7 [OK]\r\n\r\nAfter this I compile and it fails with the messages\r\n\r\n`\r\nSeverity\tCode\tDescription\tProject\tFile\tLine\tSuppression State\r\nError (active)\tE1696\tcannot open source file \"tensorflow/c/tf_attrtype.h\"\tDemoProg\r\n\r\nc:\\Users\\....\\Desktop\\tf-demo\\include\\tensorflow\\c\\c_api.h\t22\t\r\nError\tC1083\tCannot open include file: 'tensorflow/c/tf_attrtype.h': No such file or directory\tDemoProg\tc:\\users\\....\\desktop\\tf-demo\\include\\tensorflow\\c\\c_api.h\t22\t\r\n`\r\n\r\nI reviewed the contents of the tf-demo directory and there is no tf_attrtype.h anywhere in the install package.\r\n`\r\nC:\\Users\\....\\Desktop\\tf-demo>dir /b /s\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\include\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\lib\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\include\\tensorflow\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\include\\tensorflow\\c\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\include\\tensorflow\\c\\c_api.h\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\include\\tensorflow\\c\\eager\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\include\\tensorflow\\c\\LICENSE\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\include\\tensorflow\\c\\eager\\c_api.h\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\lib\\tensorflow.dll\r\n\r\nC:\\Users\\....\\Desktop\\tf-demo\\lib\\tensorflow.lib\r\n`\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nIf you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\nVisual Studio 2017\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNot applicable\r\n\r\n- TensorFlow installed from (source or binary):\r\nAssuming it's binary. The GIT hub page did (https://www.tensorflow.org/install/lang_c) not specify. Moreover, the contents of the installed material (see earlier post, did not have any source files but did have lib and dll)\r\n\r\n- TensorFlow version:\r\nI'm not sure about the version of tensor flow. The installer has the following name:\r\nlibtensorflow-cpu-windows-x86_64-1.14.0.zip\r\n\r\nI'm guessing it is 1.14.0?\r\n\r\nThe installer was downloaded from the github page at https://www.tensorflow.org/install/lang_c that provides the instructions for installing and compiling the demo program.\r\n\r\n- Python version:\r\nNot applicable. I am using the C version\r\n\r\n- Installed using virtualenv? pip? conda?:\r\nWindows Zip file\r\n\r\n- Bazel version (if compiling from source):\r\nN/A\r\n\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n\r\n- CUDA/cuDNN version:\r\nN/A\r\n\r\n- GPU model and memory:\r\nN/A this is a compile problem not a run time problem.\r\n\r\n\r\n\r\n**Describe the problem**\r\nDoesn't compile. Complains about missing header files. See original problem description for details.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSee original problem description for details", "Can you please try with [MSVC 2015](https://visualstudio.microsoft.com/vs/older-downloads/)?\r\nThanks!\r\n", "Hello @ymodak ,\r\n\r\nI get the same result with Visual Studio 2015.\r\n\r\nI think the root cause is that the windows installation zip file does not have the file\r\n`tensorflow/c/tf_attrtype.h`\r\n\r\nThe failure is that this file cannot be found, and the only header files present after the install are:\r\n\r\nC:\\Users\\xxxx\\Desktop\\tf-demo>dir /b /s include\r\nC:\\Users\\xxxx\\Desktop\\tf-demo\\include\\tensorflow\r\nC:\\Users\\xxxx\\Desktop\\tf-demo\\include\\tensorflow\\c\r\nC:\\Users\\xxxx\\Desktop\\tf-demo\\include\\tensorflow\\c\\c_api.h\r\nC:\\Users\\xxxx\\Desktop\\tf-demo\\include\\tensorflow\\c\\eager\r\nC:\\Users\\xxxx\\Desktop\\tf-demo\\include\\tensorflow\\c\\LICENSE\r\nC:\\Users\\xxxx\\Desktop\\tf-demo\\include\\tensorflow\\c\\eager\\c_api.h", "Yes you are right ```tensorflow/c/tf_attrtype.h``` is missing in windows package. ", "Download linux package, inside it copy `tensorflow/c/tf_attrtype.h`, then paste inside windows package. It works for me!", "Thank you @cesarsalgado. This is a great workaround that we will use. I also look forward to the Windows installer being fixed so that it is complete.", "@RochaStratovan please confirm if your issue is resolved.", "@Saduf2019, I can confirm that I no longer have the error when I work with 1.15.0\r\n\r\nThank you.", "Moving this issue to resolved status as per confirmation", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31567\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31567\">No</a>\n", "@yifeif @av8ramit looks like some headers are missing on the windows package.", "I have solved the problem.  The windows package is complete. You have just to specify the path in the include like this \r\n#include<tensorflow/c/c_api.h>\r\nAnd do not include the complete path in your project properties.  just stop your path in the include folder. \r\nLike this : $(SolutionDir)/tensorflow/include/"]}, {"number": 31566, "title": "Fixing build break of compiler/xla/tests", "body": "Fixing build breaks for missing dependencies mostly due to discrepancy between google internal build system and the OSS build system.\r\n\r\nSee commit logs for details.\r\n", "comments": []}, {"number": 31565, "title": "[tflite] support broadcasting in Metal Add kernel", "body": "Re-adds support for broadcasted addition in the `Add` kernel on Metal after it was removed in https://github.com/tensorflow/tensorflow/commit/b4f842384a7f63f513a62bbe3efa41bf1ff3f9ad#diff-54815b5e93d252c9f48c9036228a4fa1. Due to the same issue as the one described in #31468, it's necessary to implement this as an unlinkable kernel, to properly specify a threadgrouping.\r\n\r\ncloses #31526", "comments": ["@LK can you please add test cases ?", "@rthadur see comment here: https://github.com/tensorflow/tensorflow/pull/31564#issuecomment-520618964", "Can one of the admins verify this patch?", "@NikolayChirkov Can you please take a look on this PR? Thanks!", "@LK Can you please resolve conflicts? Thanks!", "@LK can you please add test cases as well, thank you.", "This was implemented in https://github.com/tensorflow/tensorflow/commit/cb98da75f249cf7e4a0e8185b620cf0b3f19c020#diff-54815b5e93d252c9f48c9036228a4fa1."]}, {"number": 31564, "title": "[tflite] make Metal Mul kernel unlinkable", "body": "As highlighted in #31468, the Mul operator causes an error when run on larger inputs. As far as I could tell from my digging, this is caused by the fact that:\r\n\r\n * the Mul kernel is linked with other compute tasks, and as a result does not specify a `resize_function`\r\n * when the Mul task is the first in a `FusionChain`, the `NonLinkableStub`'s resize function is used, which assigns the entire input to a single threadgroup\r\n\r\nIt seems that this then triggers a timeout on the GPU for larger inputs because there are not enough threads associated to this group. This workaround, which makes the Mul kernel unlinkable and specifies a proper thread grouping for this kernel, enables #31468 to work as expected.\r\n\r\nI would imagine this change does incur some performance penalty, but I'm not sure how else to specify proper threading grouping for a linked operator.\r\n\r\ncloses #31468", "comments": ["@LK can you please add test cases ?", "@rthadur I was trying to figure out how to run the tests for the GPU kernels (as they need to run on device), but I'm not sure how to do that outside of Google (I think this is what I'm missing? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/special_rules.bzl#L3). Any suggestions?", "Can one of the admins verify this patch?", "This would be a really useful fix for us - we're hitting it with some of our larger u-net models. I've merged in the pull request to our copy and it resolves the problem nicely.", "Making an operation unlinkable increases kernels count that leads tto lower performance. The operation should remain linkable.\r\nIt is better to fix NonLinkableStub operation work group size calculation (larger/smaller workgroup size?) or to extend NonLinkableStub to manipulate on more elements in the shader", "@NikolayChirkov would it be correct to just change the default threadgroup sizing of `NonLinkableStub` to what I specified in the `resize_function` in this PR? (in other words is it true that none of the linkable kernels now rely on the current threadgrouping?)", "@LK Could you please resolve the conflicts? Thanks!", "@NikolayChirkov Can you please take a look on this PR? Thanks!", "@impjdi can you review?", "@LK \r\n\r\nI forwarded your PR to an engineer who's been working on Metal performance.  Before we proceed with the workaround you created, he wanted you to check the latest version whether it still fails.\r\n\r\n", "@impjdi Any update on this PR? Please. Thanks!", "@gbaned We're actually blocked on @LK trying out the latest version.  There were some changes that might make this work without this change.", "@LK Any update on this PR? Please. Thanks!", "I'm having trouble compiling the TFLite GPU library from the master branch. I believe the installation instructions that I had followed last August are no longer published, and trying to run the `tensorflow/lite/tools/make/download_dependencies.sh` script fails. If someone could point me to build instructions for the TFLite GPU library I can try it out; otherwise, here is a TFLite model that was crashing for me: \r\n[model-broken.tflite (1).zip](https://github.com/tensorflow/tensorflow/files/4622919/model-broken.tflite.1.zip)\r\n\r\nIf this model runs now, then the issue is resolved.\r\n", "@LK Can you please resolve conflicts? Thanks!", "@impjdi Could you comment on what changes you were referring to and whether this PR is still needed or not?\r\n\r\n@LK For building Metal delegate at HEAD, you should run:\r\n\r\n```sh\r\nbazel build -c opt --config=ios_fat \\\r\n    //tensorflow/lite/experimental/ios:TensorFlowLiteCMetal_framework\r\n```\r\n\r\nafter following the initial bazel setup at https://www.tensorflow.org/lite/guide/build_ios#building_locally.", "@yyoon sorry, I don't have details.  that's what I got from an engineer who worked on Metal a while ago.", "@LK Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31563, "title": "Correctly convert const int8 weights to uint8 for NNAPI", "body": "This is required for using the new post-training quantization models with Android Q and NNAPI.", "comments": ["Unfortunately had to pull in https://github.com/tensorflow/tensorflow/pull/31563/commits/3ee2cb4b28303bb17b36835162a892dcf518b75d, but it was mostly a refactor."]}, {"number": 31562, "title": "decode_png returns shape of 3 dimensions of question marks", "body": "I am trying to read an image and convert it into tensor using the following code\r\n\r\n```\r\nimg_path = <image directory>\r\nimg_raw = tf.io.read_file(img_path)\r\nimg_tensor = tf.image.decode_png(img_raw)\r\n```\r\n\r\n\r\nand when I try to print its shape \r\n`print(img_tensor.shape)`\r\n\r\nit gives me 3 question marks instead of values:\r\n`(?, ?, ?)\r\n`\r\n\r\nwhy it doesn't work and how to fix this??", "comments": ["I guess you are using TF 1.X.  You can try couple of things :\r\n- Try using [```tf.Session```](https://www.tensorflow.org/api_docs/python/tf/Session) to print value of ```img_tensor.shape``` \r\n See [run](https://www.tensorflow.org/api_docs/python/tf/Session#run) method\r\n-  Simply enable eager execution and run code snippet \r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nimg_path = <image directory>\r\nimg_raw = tf.io.read_file(img_path)\r\nimg_tensor = tf.image.decode_png(img_raw)\r\nprint(img_tensor.shape)\r\n```", "yeah the tf.Session solved my problem, thanks", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 31561, "title": "TensorFlow Lite conversion", "body": "**System information**\r\n- TensorFlow running in Google Colab\r\n\r\nText Output from TFLite convert\r\n\r\n```\r\nTensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, EXPAND_DIMS, FILL, FULLY_CONNECTED, GATHER, LESS, MAXIMUM, MINIMUM, MUL, NOT_EQUAL, PACK, RANGE, RESHAPE, SELECT, SHAPE, SOFTMAX, SPLIT, STRIDED_SLICE, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Hey,\nI encountered this issue on Google Colab. I was running Python3 notebook. I\ndon't know what Architecture or OS is Google Colab using. And I install\ntf-nightly via pip.\n\nHope that helps\n\nOn Tue, Aug 13, 2019 at 12:28 PM ravikyram <notifications@github.com> wrote:\n\n> Please provide details about what platform you are using (operating\n> system, architecture). Also include your TensorFlow version. Also, did you\n> compile from source or install a binary?\n>\n> Make sure you also include the exact command if possible to produce the\n> output included in your test case. If you are unclear what to include see\n> the issue template displayed in the Github new issue template\n> <https://github.com/tensorflow/tensorflow/issues/new/choose>.\n>\n> We ask for this in the issue submission template, because it is really\n> difficult to help without that information. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31561?email_source=notifications&email_token=ADMTONBKJ3YCI3NACE23S6TQEJLP3A5CNFSM4ILEPWB2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4EW2CA#issuecomment-520711432>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADMTONAH22VALJP6VYFWJGTQEJLP3ANCNFSM4ILEPWBQ>\n> .\n>\n\n\n-- \nRegards\nApoorv Agarwal\nGithub <https://github.com/defineapoorv> | LinkedIn\n<https://www.linkedin.com/in/apoorvagarwal00/> | Devpost\n<https://devpost.com/defineapoorv>\n", "https://github.com/tensorflow/tensorflow/issues/30185#issuecomment-506833929", "@ymodak  Have it not soved yet? \r\nAnd when will it be available?\r\nLooking forward your replace,thx\r\n"]}, {"number": 31560, "title": "[XLA:GPU][ROCm] Fix XLA runtime issue on custom PTX check.", "body": "Commit c52f26a538d498e616e8a454964591612d15dd5b is erroneous and doesn't apply on ROCm. The check is only applicable on CUDA.", "comments": []}, {"number": 31559, "title": "[TFLite] Use error_message() instead of message() api", "body": "", "comments": ["@tomergafner thank you for your contribution, can you please add some description for this change."]}, {"number": 31558, "title": "add tensorflow in practice specialization in more info", "body": "I have added the TensorFlow in practice Specialization by Laurence Moroney and Andrew Ng on Corusera in the  For more information section, or click here to see it: https://github.com/aaronhma/tensorflow-pull-request-v2jl#for-more-information . Thanks TensorFlow team! This will be my first PR!", "comments": ["Thanks! Added a follow-up commit as a general README cleanup", "Thanks, everyone!"]}, {"number": 31557, "title": "Use fused_batch_norm in LayerNormalization", "body": "Current LayerNormalization uses nn.batch_normalization, which might be not as efficient as nn.fused_batch_norm.\r\n\r\nThis PR applies some reshaping on the inputs and uses nn.fused_batch_norm for better performance.\r\n\r\nfyi @nluehr", "comments": ["Hi @qlzh727  Thanks for your comments. I found a logic error in previous commits. This new commit fixes it. I also made some changes and added a new test based on your comments. Please check.\r\n\r\nBasically, this PR tries to collapse the given axis params (the dims that is gonna be normalized over) to (HW) in NCHW and other axis to C (because fused_batch_norm will not normalize the C axis).\r\n\r\nAlso, the current PR only supports the case that given axis can be collapsed to the last axis, meaning we can put the collapsed axis to (HW) in NCHW without doing a transpose op.", "Can one of the admins verify this patch?", "@qlzh727 Any updates on this?", "Hi @reedwm Thanks for your comments. I made some additional changes according to your suggestion. Please check.", "@reedwm Thx. I have made the changes accordingly. Please have a look.", "@reedwm Done.", "Hi @reedwm There are still four tests failing, but I feel the failure is not related to this PR. Could you please help check?", "Looking into it. This should be merged soon.", "Hi @houtoms,\r\n\r\nAs discussed offline, there is some reluctance to expose the \"fused\" parameter. Ideally, the implementation would use the fused version if possible, assuming there are no significant numeric differences.\r\n\r\nUnfortunately, as you mentioned, FusedBatchNorm does not support NCHW on the CPU, which means using fused if possible is difficult. There are two options\r\n  * Supporting NCHW on FusedBatchNorm on the CPU\r\n  * Use the implementation_selector to have a separate implementation on the CPU and GPU. See [this function](https://github.com/tensorflow/tensorflow/blob/e560a277795a3261744e0873f39fb90ad639412c/tensorflow/python/keras/layers/recurrent_v2.py#L569) for an example of this being done in GRU, where we use the cuDNN op only on GPUs. Currently, there is a bug preventing implementation_selector from working with mixed precision in all cases, but this will be fixed.\r\n\r\nI recommend the former approach. The fact FusedBatchNorm does not support NCHW on the CPU is a usability issue of the BatchNormalization error. For example, this raises an error, as BatchNormalization chooses the fused implementation even though it is not supported on the CPU:\r\n\r\n```\r\nwith tf.device('/CPU:0'):\r\n  x = tf.random.normal((2, 3, 4, 5))\r\n  layer = tf.keras.layers.BatchNormalization(axis=1)\r\n  y= layer(x)\r\n```\r\n\r\n", "@reedwm , I added the NCHW support to CPU impl. and removed the `fused` param. Please chek.", "Thank you for the FusedBatchNorm fix! Can you split the FusedBatchNorm changes into a separate PR? It's easier to review and will make rollbacks easier if something goes wrong.", "Sure. The split PR is https://github.com/tensorflow/tensorflow/pull/32567. Please chek.", "Should I take a look again now the batch norm PR is merged?", "Yes, I just solved some conflicts. This PR is much simpler now. Please take a look at it again. Thx.", "Thx for the suggestion. The changes are committed.", "Can we make fusion as an option? We see numeric differences.", "I discussed with the API owners. There originally was a `fused` parameter but we decided to remove it", "@houtoms sorry for the delay, this is failing some internal tests. I'll have an update soon.", "Sure. Thanks for the update.\r\n", "Sorry for the delay. I added some tests in d38aec73faf44e347662ef7639c1920f11901b02 that fail with this PR. I think if the PR passes those tests, it will also pass the internal tests that were previously failing. Can you please fix the PR? Free feel to increase the rtol/atol slightly if necessary, but right now they need to be significantly increased.\r\n\r\nMy guess is that this is failing due to the fact [fused_batch_norm sets a minimum value of epsilon here](https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/ops/nn_impl.py#L1300). Try only using the fused version if epsilon is at least 1.001e-5. I plan on removing the requirement that epsilon is at least 1.001e-5, as cuDNN removed the requirement, but I have to run extensive internal tests first so it might take a while.\r\n\r\n", "Also, when you make the changes, please add a comment saying \"PTAL\". Then I'll get an email notification so I know I should continue the review.", "@reedwm PTAL"]}, {"number": 31556, "title": "[INTEL MKL] Integrated MKL to TF conversion op with MKL-DNN v1.x", "body": "", "comments": ["@penpornk I have addressed your review comments. Please take a look. Thank you!"]}, {"number": 31555, "title": "[INTEL MKL] Integrated MKL input conversion op with MKL-DNN v1.x.", "body": "", "comments": ["@penpornk I have addressed your review comments. Please take a look. Thanks!", "@penpornk Looks like some changes didn't get added when I copied them as a `diff` from my internal branch. I've added them now. I'm so sorry for the inconvenience.", "@penpornk The one in `mkl_tfconv_op.h` takes additional parameters compared to the one in `mkl_util.h`. Do you want me to merge the two and have just one function in `mkl_util.h`?\r\n\r\nRegarding `ConvertMklToTF()` in `mkl_quantized_conv_ops_test.cc`, do you want me to move that to `mkl_util.h` as well?", "@bhavani-subramanian I see. So there are multiple `ConvertMklToTf()` in different files. We should unify them, or at least put all of them in the same place (`mkl_util.h`). I think we should have a separate PR for this. Could you please add a TODO? For this PR, I just want to make sure that the changes won't break the build.", "@penpornk I finished making the changes you requested. Thanks again for reviewing the PR :)", "Can one of the admins verify this patch?", "@penpornk Just checking if there's anything else needed from my end w.r.t this PR?", "@bhavani-subramanian Sorry for the delay! This PR was pulled in and was being merged automatically. I didn't notice that the merge was unsuccessful.\r\n\r\nThere is a merge conflict internally (with `mkl_util.h`). Could you please try syncing the PR and resolve the conflict? (If you can't see the conflict, let me know and I'll resolve this internally.)", "@penpornk No worries! I just tried to merge this branch with master and I don't see any merge conflicts. ", "@bhavani-subramanian Thanks for checking! I'll do it then. :)"]}]