[{"number": 26443, "title": "R1.13", "body": "", "comments": []}, {"number": 26442, "title": "TFLite LSTM example produces different results for Lite and for standard Tensorflow for variable length input.", "body": "I have extended the LSTM example for TFLite `unidirectional_sequence_lstm_test.py` in `tensorflow/tensorflow/lite/experimental/examples/lstm` and it returns different results for standard Tensorflow and Tensorflow Lite.\r\n\r\nThe example project is available here: [https://github.com/MiloslavPalaxo/tflite-lstm-text](https://github.com/MiloslavPalaxo/tflite-lstm-text)\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-9708-gafab5b3 1.14.1-dev20190306\r\n- Python version: 3.7\r\nsary to generate the problem.\r\n\r\n**Other info / logs**\r\n\r\n2019-03-07 14:12:33.453487: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400115000 Hz\r\n2019-03-07 14:12:33.454111: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x2644ff0 executing computations on platform Host. Devices:\r\n2019-03-07 14:12:33.454139: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0307 14:12:33.472815 139655812351808 deprecation.py:506] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0307 14:12:33.495823 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/lite/experimental/examples/lstm/rnn.py:218: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0307 14:12:33.645129 139655812351808 deprecation.py:506] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py:883: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0307 14:12:34.316469 139655812351808 deprecation.py:323] From /home/XXX/ideaprojects/tflite-lstm-text/sentiment-lite3.py:151: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\n\r\nW0307 14:13:07.583182 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nW0307 14:13:07.947111 139655812351808 deprecation.py:323] From /home/XXX/ideaprojects/tflite-lstm-text/sentiment-lite3.py:191: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0307 14:13:07.947545 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:247: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nW0307 14:13:08.154012 139655812351808 deprecation.py:323] From /home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.remove_training_nodes`\r\n[[0.5784946  0.17042728 0.25107813]\r\n [0.2503695  0.35664326 0.3929873 ]\r\n [0.26069233 0.3423219  0.39698583]\r\n [0.5784946  0.17042728 0.25107813]\r\n [0.5784946  0.17042728 0.25107813]]\r\n[[0.25950563 0.34442514 0.39606926]\r\n [0.25036934 0.3566433  0.3929873 ]\r\n [0.2606922  0.34232193 0.39698586]\r\n [0.25918585 0.34475493 0.3960592 ]\r\n [0.25936535 0.34485146 0.39578322]]\r\n\r\nFailure\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.7/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.7/unittest/case.py\", line 615, in run\r\n    testMethod()\r\n  File \"/home/XXX/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py\", line 428, in wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/XXX/ideaprojects/tflite-lstm-text/sentiment-lite3.py\", line 271, in testDynamicRnnMultiRnnCell\r\n    self.assertTrue(np.allclose(expected_output, result, rtol=1e-6, atol=1e-2))\r\n  File \"/usr/lib/python3.7/unittest/case.py\", line 692, in assertTrue\r\n    raise self.failureException(msg)\r\nAssertionError: False is not true\r\n\r\n", "comments": ["Thanks for reporting the issue, I am the owner of TF RNN API, but the lite part of that was worked by another engineer. I have forwarded this issue to him, and probably he will reply here. \r\n\r\nAlso its unclear to me that what modification has been make to the original example and cause the test to fail. Can u make it more clear to us so that we can do the troubleshoot?\r\n\r\nOne of the difference I can think of between keras LSTMCell and TF cell is that keras was using \"hard_sigmoid\" as activation function for historical reasons. You can try use \"activation='sigmoid'\" when you construct keras.layers.LSTMCell. In 2.0, the default activation for LSTM will be changed to sigmoid.", "Hi, can you sync pass https://github.com/tensorflow/tensorflow/commit/dc3900a96964a5174a591fdcf67242d62ba05947#diff-24bb9d73def4fa712275769dcffb18b8\r\n\r\nand try again?\r\n\r\nThanks a lot!", "Hi Airlrj,\r\n\r\nThank you for your advice. I am actually using a later commit: afab5b322f780c235c61038b49593e34b523d400\r\n\r\nSo all the suggested changes are already implemented in my test.", "Hi Miloslav,\r\n\r\nI think the issue is seq_length.\r\n\r\nFor tflite, seq_length will be dropped but the \"timestamp\" is actually dynamically inferred, so it does not work very well for a \"ragged batch\".\r\n\r\nIf you export a graph with no seq length & export to tflite, and infer one by one. The result will probably be the same. (no need to pad)", "Hi renjie-liu,\r\n\r\nThank you. I have suspected the same.  Now, I have verified that by setting the seq length to the maximum by the command `batch_lengths.fill(max_seq_len)`. Now, the test **passes**.\r\n\r\nHowever, I believe that this is still a bug and the support for seq length should be added to the TF lite. Firstly, the parameter is there and it is should be supported. Secondly, seq length support is crucial for text data.\r\n", "> Also its unclear to me that what modification has been make to the original example and cause the test to fail. Can u make it more clear to us so that we can do the troubleshoot?\r\n>\r\n\r\nI have made two minimal changes to support text data:\r\n* I have added the embedding layer.\r\n* I have added sequences lengths.", "I have found an additional problem. If I choose to use `encoder_state` instead I get the conversion error:\r\n\r\n`prediction = tf.matmul(tf.add(encoder_state[-1][0], encoder_state[-1][1]), out_weights) + out_bias`\r\n\r\nFailure\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.7/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"/usr/lib/python3.7/unittest/case.py\", line 615, in run\r\n    testMethod()\r\n  File \"/home/peak/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py\", line 428, in wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/peak/ideaprojects/tftest/sentiment-lite3.py\", line 276, in testDynamicRnnMultiRnnCell\r\n    result = self.tfliteInvoke(frozen_graph, test_inputs, output_class)\r\n  File \"/home/peak/ideaprojects/tftest/sentiment-lite3.py\", line 213, in tfliteInvoke\r\n    curr = convert_op_hints_to_stubs(graph_def=curr)\r\n  File \"/home/peak/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/lite/python/op_hint.py\", line 1277, in convert_op_hints_to_stubs\r\n    return _convert_op_hints_to_stubs_helper(graph_def, write_callback)\r\n  File \"/home/peak/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/lite/python/op_hint.py\", line 1172, in _convert_op_hints_to_stubs_helper\r\n    assert (len(children_hints) > 0)  #  pylint: disable=g-explicit-length-test\r\nAssertionError", "Right, currently final state is not supported. Really sorry about that.\n\nOn Sun, Mar 10, 2019 at 3:14 AM MiloslavPalaxo <notifications@github.com>\nwrote:\n\n> I have found an additional problem. If I choose to use encoder_state\n> instead I get the conversion error:\n>\n> prediction = tf.matmul(tf.add(encoder_state[-1][0], encoder_state[-1][1]),\n> out_weights) + out_bias\n>\n> Failure\n> Traceback (most recent call last):\n> File \"/usr/lib/python3.7/unittest/case.py\", line 59, in testPartExecutor\n> yield\n> File \"/usr/lib/python3.7/unittest/case.py\", line 615, in run\n> testMethod()\n> File\n> \"/home/peak/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py\",\n> line 428, in wrapper\n> return fn(*args, **kwargs)\n> File \"/home/peak/ideaprojects/tftest/sentiment-lite3.py\", line 276, in\n> testDynamicRnnMultiRnnCell\n> result = self.tfliteInvoke(frozen_graph, test_inputs, output_class)\n> File \"/home/peak/ideaprojects/tftest/sentiment-lite3.py\", line 213, in\n> tfliteInvoke\n> curr = convert_op_hints_to_stubs(graph_def=curr)\n> File\n> \"/home/peak/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/lite/python/op_hint.py\",\n> line 1277, in convert_op_hints_to_stubs\n> return _convert_op_hints_to_stubs_helper(graph_def, write_callback)\n> File\n> \"/home/peak/.virtualenvs/tftest/lib/python3.7/site-packages/tensorflow/lite/python/op_hint.py\",\n> line 1172, in _convert_op_hints_to_stubs_helper\n> assert (len(children_hints) > 0) # pylint: disable=g-explicit-length-test\n> AssertionError\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26442#issuecomment-471274306>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AikWmbb5hIS8viUQVd2vEXEDvK-k8fPkks5vVNsggaJpZM4bjKJ_>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "I have managed to find a solution (or rather a workaround) to the issue. I do not pass the lengths to the rnn encoder and I select the correct output manually with tf.gather_nd() instead. See line 136 in my [sentiment-lite3.py](https://github.com/MiloslavPalaxo/tflite-lstm-text/blob/master/sentiment-lite3.py).\r\n\r\nThe test passes with this solution, however, I believe that the lengths should be supported in the encoder itself (`tf.lite.experimental.nn.dynamic_rnn`).\r\n\r\nTo be completely honest, it took me rather long to find the solution because something really strange is happening on line 134. I must use a different variable name otherwise the code does not work!\r\n\r\nThe code has been tested with: `1.14.1-dev20190306`", "agree, looks like supporting seq_length can make use cases like yours much easier.", "@aselle  @yenchenlin", "@renjie-liu When the lite will support RNN friendly or be stable?", "Hi Hui, we will support control flow shortly, which allows you to convert all rnn* to the unfused version. however, if you need to convert to the fused version, please refer to the examples under lite/experimental/examples/lstm", "@renjie-liu Great, look forward to it.", "Hi There,\r\n\r\nWe are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26442\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26442\">No</a>\n"]}, {"number": 26441, "title": "Tensorboard not opening in jupyter notebook", "body": "I am trying to open tensorboard inside jupyter notebook with the command \r\n`% tensorboard --logdir keras`.\r\nBut I am getting \r\n**localhost refused to connect.**\r\nSpecifications:\r\n\r\n- Jupyter notebook= 5.7.5\r\n- Python = 3.6.7\r\n- Windows =10\r\n-  tensorflow = 2.0.0-alpha0\r\n\r\n**Code**\r\n```\r\nfrom kaggle_data import load_data, preprocess_data, preprocess_labels\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nX_train, labels = load_data('../data/kaggle_ottogroup/train.csv', train=True)\r\nX_train, scaler = preprocess_data(X_train)\r\nY_train, encoder = preprocess_labels(labels)\r\n\r\nX_test, ids = load_data('../data/kaggle_ottogroup/test.csv', train=False)\r\nX_test, _ = preprocess_data(X_test, scaler)\r\n\r\nnb_classes = Y_train.shape[1]\r\nprint(nb_classes, 'classes')\r\n\r\ndims = X_train.shape[1]\r\nprint(dims, 'dims')\r\n\r\nimport tensorflow as tf\r\nfrom keras.layers import Dense, Activation\r\n\r\ndims = X_train.shape[1]\r\nprint(dims, 'dims')\r\nprint(\"Building model...\")\r\nnb_classes = Y_train.shape[1]\r\nprint(nb_classes, 'classes')\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(nb_classes, input_shape=(dims,), activation='sigmoid'))\r\nmodel.compile(optimizer = 'sgd', loss='categorical_crossentropy')\r\nmodel.fit(X_train, Y_train,epochs=1,callbacks=[tf.keras.callbacks.TensorBoard('keras')] )\r\n\r\n%load_ext tensorboard.notebook\r\n%tensorboard --logdir keras\r\n```", "comments": ["Hi @Sagarsharma4244, It's working absolutely fine on my system.\r\nMaybe a host based firewall has been set to block all TCP ports or block IP 127.0.0.1 localhost. Maybe, You need to reboot the machine to clear O/S issues. Try resetting your network adapters too.\r\nLet me know how it goes. Thanks :)", "Initially when I  installed 2.0 and tensorboard extension, it did work once.  I did try to reboot the system but that didn't affect the result.", "What are the port numbers, etc.? (e.g. `tensorboard.notebook.list()`) Are you sure that the tensorboard is running on \"localhost\", not on a remote machine?", "@Sagarsharma4244 Please post tensorboard related issues to Tensorboard group [here](https://github.com/tensorflow/tensorboard/issues). It's very active team who will support your questions faster. Thanks!"]}, {"number": 26440, "title": "Update README.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26440) for more info**.\n\n<!-- need_sender_cla -->", "@MashallHe could you please elaborate what this change about ?", "Closing it, irrelevant change."]}, {"number": 26439, "title": "TF Lite profiling module warning fix", "body": "profiling module warning fix", "comments": ["Closing as this code is no longer hidden by a build flag."]}, {"number": 26438, "title": "TF Lite toco/tflite warning fix", "body": "toco/tflite module warning fix", "comments": ["@jdduke \r\n\r\nSorry for delay and closing this PR due to write perssion denied, same changes will be track with new PR #28543."]}, {"number": 26437, "title": "using HParams with TPU v2 PODs broken", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- CUDA/cuDNN version: 9.0 / 7\r\n- GPU model and memory: T4, 16 GB\r\n- TPU type: v2-32\r\n- TPU TensorFlow version: 1.12.0\r\n\r\n**Describe the current behavior**\r\nThe distribution to the pod fails with\r\n1. `AttributeError: 'TPUContext' object has no attribute 'is_input_sharded_per_core'` if passing a user-provided `tf.contrib.training.HParams` instance to the `TPUEstimator`. Use `--use_hparams` argument in the demo script.\r\n2. `AttributeError: 'dict' object has no attribute 'batch_size'` if no `HParams` are provided. Use `--nouse_hparams` argument in the demo script.\r\n\r\n**Describe the expected behavior**\r\nI expect `params` in the `model_fn` and `input_fn` to be an instance of `tf.contrib.training.HParams`, hence its parameters should be accessible as attributes. However in 2.) `params` is a python `dict`.\r\n\r\nI expect to be able to use the `TPUEstimator` with my own `HParams`.\r\n\r\n**Code to reproduce the issue**\r\nSee https://gist.github.com/ceaed6f7c2ed027da35fe5fbbe0056ae\r\nHelp:\r\n```bash\r\nUSAGE: tpu_hparams.py [flags]\r\nflags:\r\n\r\ntpu_hparams.py:\r\n  --num_cores: The number of TPU cores in the pod.\r\n    (default: '32')\r\n    (an integer)\r\n  --project: The project name, the TPU is instantiated in.\r\n    (default: '')\r\n  --tpu: The TPU name.\r\n    (default: '')\r\n  --[no]use_hparams: To avoid the bug.\r\n    (default: 'true')\r\n  --zone: The zone name, the TPU is instantiated in.\r\n    (default: '')\r\n```\r\n\r\n**Other info / logs**\r\nWhat causes the issues?\r\n1. If a user provides `params`, then the `TPUEstimator` adds the `TPUContext` to `params` by calling\r\nhttps://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py#L1528-L1531\r\nThis triggers the `add_param` or `set_param` methods. If `set_param` is used because the key `context` was not found in `params`, then `_cast_to_type_if_compatible` is invoked before setting the parameter. Unfortunately this triggers a casting / copy operation:\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/contrib/training/python/training/hparam.py#L187\r\nBecause the syntax of the `TPUContext` only requires one positional argument\r\nhttps://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/tpu/python/tpu/tpu_context.py#L43-L55\r\na new `TPUContext` is initialized, where the `_internal_ctx` is set to the `TPUContext` instance, which itself actually contains a instance of `_InternalTPUContext`.\r\n\r\nHow could this be solved?\r\n* create a clone / copy constructor for `HParams`\r\n* use deep cloning? In case of standard types, this would instantiate a new one as well, if I am not mistaken.\r\n* fix the problem in the `TPUEstimator` by always using instances of `HParams` and setting `context` before, so we add the `context` by calling `add_hparam`, which does not use the type casting, instead of `set_hparam`.\r\n\r\n2. `hparams` suddenly is a `dict`, not a instance of `HParams`.\r\n", "comments": ["a hotfix would be to pass `hparams` as a `dict` like so\r\n```python\r\nestimator = tf.contrib.tpu.TPUEstimator(\r\n    use_tpu=True,\r\n    model_fn=model_fn,\r\n    config=config,\r\n    params=params.__dict__,\r\n    train_batch_size=128\r\n)\r\n```", "btw I am motivated to implement a fix as well. At the moment though I am not sure which way is the best. Especially because I am not aware of any reasoning behind the way it is implemented now.", "Thanks for the bug report and apologies for the slow reply.\r\n\r\nThis issue has been fixed (by fixing the behavior of `HParams.set_param`). It should show up in the tf-nightly build today, and will be available with the next release of TensorFlow.\r\n\r\nIn the meantime, as you mention you can use a dictionary instead of the HParams object as a workaround. Sorry for the inconvenience!", "cool great to hear that \ud83d\udc4d "]}, {"number": 26436, "title": "tf.contrib.training.HParams domain interval support", "body": "Hi, this is my first issue/feature request. I hope it's relevant :)\r\n\r\nI am currently working on hyperparameter optimisation and would like to use the HParams API but in my eyes there is a feature missing. Instead of having a discrete list of values for a parameter, I would like to represent a feasible interval, from which I can take values.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, the HParams class holds a set of hyperparameters as name-value pairs, that is a discrete set of values for each of the hyperparameters. I would like to be able to specify also the feasible interval from which the values are coming. Also the TensorBoard api for reporting experiment summaries provides the possibility to report the domain interval and I think it would be handy to have this information in the HParams class, too.\r\n\r\nThe TensorBoard API let's you specify the `domain_discrete` but you could also provide `domain_interval`.\r\n```\r\napi_pb2.HParamInfo(name='num_units',  \r\n                             display_name='Number of units',  \r\n                             type=api_pb2.DATA_TYPE_FLOAT64,  \r\n                             domain_discrete=num_units_list_val),\r\n```\r\n\r\n**Will this change the current api? How?**\r\nYes, this would require changing the API such that the User can also specify the interval of a parameter, for example:\r\n\r\n```\r\nadd_hparam(\r\n    name,\r\n    value,\r\n    start,\r\n    end\r\n)\r\n```\r\n\r\nFurthermore, the data structure would need to be changed to save the interval.\r\n[Tensor2Tensor has something similar](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_hparams.py) with RangedHParams.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone working with intervals for hyperparameters compared to a fixed set of discrete values.\r\n\r\n**System information**\r\n- TensorFlow version (you are using): r1.12\r\n- Are you willing to contribute it (Yes/No): Yes, I am happy to help, but this would be my first contribution and I would need guidance", "comments": ["Hi @moritzmeister !\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "Yes, this is outdated, will close."]}, {"number": 26435, "title": "[Tflite] Unable to convert model to tflite with uneven input dimension", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nHi I have a fully convolutional CNN model (PoseNet to be precise), which is therefore able to process input of any dimension.\r\n\r\nConverting the model to tflite works perfectly with even Input dimensions like 224x224.\r\nBut I need it to get converted with uneven Input Dimensions something like 225x225 or 289x289.\r\n\r\nSetting these uneven input dimension in toco/ the tflite converter results in the following error:\r\n```\r\nRuntimeError: TOCO failed see console for info.\r\nb'2019-03-07 10:20:29.334193: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] \r\nBefore Removing unused ops: 213 operators, 318 arrays (0 quantized)\\n2019-03-07 10:20:29.340266: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] \r\nBefore general graph transformations: 213 operators, 318 arrays (0 quantized)\\n2019-03-07 10:20:29.348270: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991] \r\nCheck failed: height_with_paddings % block_height == 0 (1 vs. 0)\\n'\r\n```\r\n\r\nQuestions:\r\n\r\n1. Why does tflite need fixed input dimensions? (I suppose this reduces necessary computations and therefore allow faster inference)\r\n2. Why is it not possible to convert a model to tflite with uneven Input Dimensions even though the model is fully convolutional and can therefore generally handle those dimensions?\r\n3. Is there a workaround to fix my problem?\r\n4. Additional: Can someone who knows posenet explain to me why it needs this +1 padding to even input dimensions: (line 109 in: https://github.com/tensorflow/tfjs-models/blob/master/posenet/src/util.ts)\r\n\r\nThank you very much in advance!\r\n", "comments": ["I am facing the same problem, I think tflite Interpreter needs a fixed input dimension to feed data for faster runtime. \r\none walk around might be fixe the input size to target size. but fully conv operation usually comes with pyramid, multiple input size could be fixed.\r\nBut, this means multiple .tflite file. I don't think this is worth trying.\r\nSome better way of fully conv runs with tflite is really needed.\r\n\r\ncurrently, I am using the old style .pb like to run a fully conv net on mobile, which is ok though", "@gustavz This is a stale issue. \r\n\r\nMany bugs have been fixed in the latest TF version. Could you please execute your code using Latest Version 2.6 (and `tf-nightly`) and let us know if the issue still persists? If it persists with latest TF versions, can you please share a simple standalone code to reproduce the issue?\r\n\r\nAlso, please note that we are not supporting TF1.x. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26435\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26435\">No</a>\n"]}, {"number": 26434, "title": "Removed the Reference to Depricated function", "body": "Removed TfLiteQuantizationParams reference from the file", "comments": ["Nagging Reviewer @suharshs: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@suharshs , thanks for review comments, i think you made perfect sense, I am closing this PR as per your suggestion. \r\n\r\nI have one more request, there are some of my important PRs on Quantization under your name, can you please have a look at those as well.\r\n\r\nRegards\r\nAmit "]}, {"number": 26433, "title": "replace extend_with_weight_decay examples with extend_with_decoupled_weight_decay examples", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26433) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "Please send this PR to tensorflow/addons instead; contrib is deprecated and stated to be removed so we're not making new changes to it.", "@alextp This is a response to https://github.com/tensorflow/tensorflow/pull/26433\r\nwhich is inherently connected to the code in contrib.\r\nThere is no new code, only a correction to the existing misleading documentation. I cannot find the corresponding functions in tensorflow/addons https://github.com/tensorflow/addons/tree/master/tensorflow_addons/optimizers/python, could you redirect me to the appropriate place to add this PR?", "THe issue is that we're no longer releasing new versions of contrib. So if this code hasn't moved to addons it currently has no home."]}, {"number": 26432, "title": "ImportError: librt.so.1: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: `pip`\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: 16GB RAM\r\n\r\n\r\n\r\n**Describe the problem**\r\n1. If I run python with `$ python3`, then `import tensorflow`, I get an `ImporError: librt.so.1: cannot open shared object file: No such file or directory`\r\n2. If I run python with `$ LD_LIBRARY_PATH=/home/kmonisit/.linuxbrew/lib:$LD_LIBRARY_PATH python3`, then `import tensorflow`, it succeeds.\r\n3. I will be using `tensorflow` from within C code, i.e. C code calls Python code calls tensorflow. I want to find a way where I can use `tensorflow` without having to specify `LD_LIBRARY_PATH`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ brew install binutils linux-headers\r\n$ brew install glibc --force-bottle\r\n$ brew install gcc --force-bottle\r\n$ brew install python3\r\n$ pip3 install tensorflow\r\n```\r\n\r\nNow I have `glibc` in `$HOME/.linuxbrew/lib`. Next step is:\r\n\r\n```\r\n$ python3\r\n>>> import tensorflow\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: librt.so.1: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/kmonisit/.linuxbrew/Cellar/python/3.7.2_2/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: librt.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Successful case**\r\nIf I do:\r\n```\r\n$ LD_LIBRARY_PATH=/home/kmonisit/.linuxbrew/lib:$LD_LIBRARY_PATH python3\r\n>>> import tensorflow\r\n>>> # import success!\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFull traceback already included above.\r\n", "comments": ["@kitmonisit  Can you please let me know the output of following:\r\n$ LD_LIBRARY_PATH=/home/kmonisit/.linuxbrew/lib:$LD_LIBRARY_PATH python3 -V", "```\r\n$ LD_LIBRARY_PATH=/home/kmonisit/.linuxbrew/lib:$LD_LIBRARY_PATH python3 -V\r\nPython 3.7.2\r\n```", "Just came back to the office and the compilation I did last night completed successfully. To compile Tensorflow, here's what I did:\r\n\r\n#### Download and compile bazel\r\n- LD_LIBRARY_PATH should be clean (i.e. does not have `$HOME/.linuxbrew/lib`)\r\n```\r\n$ cd ~\r\n$ mkdir build-bazel\r\n$ cd build-bazel\r\n$ wget https://github.com/bazelbuild/bazel/releases/download/0.20.0/bazel-0.20.0-dist.zip\r\n$ unzip bazel-0.20.0-dist.zip\r\n$ env EXTRA_BAZEL_ARGS=\"--host_javabase=@local_jdk//:jdk\" bash ./compile.sh\r\n```\r\nAt this point I now have bazel without JDK. That's sufficient to compile Tensorflow.\r\n\r\n#### Let's get started with Tensorflow\r\n```\r\n$ cd ~\r\n$ mkdir build-tensorflow\r\n$ cd build-tensorflow\r\n$ wget https://codeload.github.com/tensorflow/tensorflow/zip/v1.13.1 -O tensorflow-1.13.1.zip\r\n$ unzip tensorflow-1.13.1.zip\r\n$ cd tensorflow-1.13.1\r\n$ PATH=$HOME/build-bazel/output:$PATH ./configure\r\n$ PATH=$HOME/build-bazel/output:$PATH bazel build --config=monolithic //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n#### Compilation will take a long time. I let this run overnight.\r\n\r\n```\r\n$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOME/tmp/tensorflow_pkg\r\n$ cd $HOME/tmp/tensorflow_pkg\r\n$ pip3 install tensorflow-1.13.1-cp37-cp37m-linux_x86_64.whl\r\n$ python3\r\n>>> import tensorflow\r\n>>> :)\r\n```\r\n\r\n#### \\* Success Kid \\*", "Thanks for sharing the installation steps. I will close this issue now since its resolved. Thanks!"]}, {"number": 26431, "title": "[doc] dead link in \"Build and load a SavedModel\"", "body": "**System information**\r\n- TensorFlow version: master\r\n- Doc Link: https://www.tensorflow.org/guide/saved_model\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe link <https://www.tensorflow.org/tfx/guide/serving/serving_basic> for \"TensorFlow serving\" is no long available in the doc [Build and load a SavedModel](https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel)\r\n", "comments": ["@csukuangfj Correct two links are not working.\r\n1. First \"Tensorflow Serving\" in this [section](https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel) . -- this is what you found.\r\n2. \"instructions\" in this [section](https://www.tensorflow.org/guide/saved_model#load_and_serve_a_savedmodel_in_tensorflow_serving). \r\nThanks for pointing them. We will take care of them. Thanks!", "Both fixes are in the source files, there will be some delay to push to the site."]}, {"number": 26430, "title": "Keras model saving is not working when graph is finalized", "body": "- TensorFlow installed from (source or binary): from pip\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n\r\nTLDR) A Keras model used in a static-graph and session mode cannot save its weight when the graph is finalized.\r\n\r\n```python\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Dense(256)\r\n])\r\n\r\nx = tf.zeros([10, 3])   # dummy input\r\nmodel(x)\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n# finalize the graph (or it's done automatically in MonitoredSession)\r\ntf.get_default_graph().finalize()\r\n\r\n# Error!\r\nmodel.save('/tmp/keras-test')\r\n```\r\n\r\nThe error is `RuntimeError: Graph is finalized and cannot be modified.` Stacktrace:\r\n\r\n```\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1415, in save_weights\r\n    saving.save_weights_to_hdf5_group(f, self.layers)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\", line 742, in save_weights_to_hdf5_group\r\n    weight_values = K.batch_get_value(symbolic_weights)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 2819, in batch_get_value\r\n    return get_session().run(tensors)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 482, in get_session\r\n    _initialize_variables(session)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 758, in _initialize_variables\r\n    [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 758, in <listcomp>\r\n    [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 2924, in is_variable_initialized\r\n    return state_ops.is_variable_initialized(variable)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 133, in is_variable_initialized\r\n    return ref.is_initialized(name=name)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 833, in is_initialized\r\n    return gen_resource_variable_ops.var_is_initialized_op(self.handle, name)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 1334, in var_is_initialized_op\r\n    \"VarIsInitializedOp\", resource=resource, name=name)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    self._check_not_finalized()\r\n  File \"$PREFIX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2945, in _check_not_finalized\r\n    raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\nRuntimeError: Graph is finalized and cannot be modified.\r\n```\r\n\r\nThe operation being created here is:\r\n\r\n```\r\n <tf.Operation 'VarIsInitializedOp_1' type=VarIsInitializedOp>,\r\n <tf.Operation 'VarIsInitializedOp_2' type=VarIsInitializedOp>]\r\n```\r\n\r\nFrom [`keras/backend.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L761):\r\n\r\n```python\r\n is_initialized = session.run(\r\n     [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n```\r\n\r\nWell, I think we should not create and call this \"new op\" to check whether a variable is initialized. Why is it implemented in this way? Not only it doesn't work, ~but also it will result in op leak as well even though it would work without finalizing the graph?~ (UPD: it seems that the result is cached through `_keras_initialized` so it won't be the case) There is another way to check whether the variable is initialized or not without creating a new operation.", "comments": ["A workaround I'm using currently is to call `model.save` once before the MonitoredSession starts, so that those VarIsInitializedOp's can be created before finalizing the graph.", "@wookayin Can you clarify why you need to finalize the graph before saving? Typically, you would save before finalizing the graph, because saving requires graph + var access.", "@karmel My use case was as follows:\r\n\r\n* Define graphs and build models as usual\r\n* Create `MonitoredSession` (which is a wrapper of standard Session) -- upon entering the context scope, the graph gets finalized\r\n* Run training steps (say, 1000 iterations)\r\n* Save the graph (and the graph has been finalized) <--- error!\r\n\r\nThough it might not be known very widely, indeed saving graph requires creating operations, hence should be before graph finalization --- but in order to save variables, variables need to have been initialized after session creation. I see a cyclic dependency here. (please look at my workaround above as well)\r\n\r\n", "Same problem when using Estimator + Keras model. I using tf1.14.", "```\r\nI0723 15:56:05.430358 140354566588224 basic_session_run_hooks.py:606] Saving checkpoints for 2550 into exp/emo/keras-lstm/ckpt/model.ckpt.\r\n316 I0723 15:56:05.641069 140354566588224 estimator.py:1145] Calling model_fn.\r\n317 Traceback (most recent call last):\r\n318   File \"/home/luban//nlu-ml/delta/main.py\", line 114, in <module>\r\n319     app.run(main)\r\n320   File \"/home/luban/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n321     _run_main(main, args)\r\n322   File \"/home/luban/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n323     sys.exit(main(argv))\r\n324   File \"/home/luban//nlu-ml/delta/main.py\", line 82, in main\r\n325     solver.train_and_eval()\r\n326   File \"/home/luban//nlu-ml/delta/utils/solver/estimator_solver.py\", line 441, in train_and_eval\r\n327     self.train_and_eval_one_epoch(nn, train_spec, eval_spec)\r\n328   File \"/home/luban//nlu-ml/delta/utils/solver/estimator_solver.py\", line 397, in train_and_eval_one_epoch\r\n329     nn, train_spec, eval_spec)\r\n330   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n331     return executor.run()\r\n332   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n333     return self.run_local()\r\n334   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n335     saving_listeners=saving_listeners)\r\n336   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n337     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n338   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1156, in _train_model\r\n339     return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n340   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1219, in _train_model_distributed\r\n341     self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n342   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1329, in _actual_train_model_distributed\r\n343     saving_listeners)\r\n344   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\r\n345     _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n346   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n347     run_metadata=run_metadata)\r\n348   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n349     run_metadata=run_metadata)\r\n350   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\r\n351     raise six.reraise(*original_exc_info)\r\n352   File \"/home/luban/.local/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n353     raise value\r\n354   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n355     return self._sess.run(*args, **kwargs)\r\n356   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1419, in run\r\n357     run_metadata=run_metadata))\r\n358   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 594, in after_run\r\n359     if self._save(run_context.session, global_step):\r\n360   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 619, in _save\r\n361     if l.after_save(session, step):\r\n362   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n363     self._evaluate(global_step_value)  # updates self.eval_result\r\n364   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n365     self._evaluator.evaluate_and_export())\r\n366   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\r\n367     hooks=self._eval_spec.hooks)\r\n368   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n369     name=name)\r\n370   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\r\n     return _evaluate()\r\n372   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 501, in _evaluate\r\n373     self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\r\n374   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1501, in _evaluate_build_graph\r\n375     self._call_model_fn_eval(input_fn, self.config))\r\n376   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1537, in _call_model_fn_eval\r\n377     features, labels, ModeKeys.EVAL, config)\r\n378   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1146, in _call_model_fn\r\n379     model_fn_results = self._model_fn(features=features, **kwargs)\r\n380   File \"/home/luban//nlu-ml/delta/utils/solver/estimator_solver.py\", line 80, in _model_fn\r\n381     model_outputs = self.model_class(features, training=is_train)\r\n382   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n383     outputs = call_fn(inputs, *args, **kwargs)\r\n384   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 149, in wrapper\r\n385     raise e.ag_error_metadata.to_exception(type(e))\r\n386 RuntimeError: in converted code:\r\n387     relative to /home/luban/:\r\n388 \r\n389     nlu-ml/delta/models/speech_cls_model.py:49 call *\r\n390         x = self.lstm2(x)\r\n391     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py:619 __call__\r\n392         return super(RNN, self).__call__(inputs, **kwargs)\r\n393     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:634 __call__\r\n394         outputs = call_fn(inputs, *args, **kwargs)\r\n395     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py:2533 call\r\n396         inputs, mask=mask, training=training, initial_state=initial_state)\r\n397     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py:743 call\r\n398         zero_output_for_mask=self.zero_output_for_mask)\r\n399     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3732 rnn\r\n400         input_time_zero, tuple(initial_states) + tuple(constants))\r\n401     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py:728 step\r\n402         output, new_states = self.cell.call(inputs, states, **kwargs)\r\n403     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py:2215 call\r\n404         self.kernel, num_or_size_splits=4, axis=1)\r\n405     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1563 split\r\n406         axis=axis, num_split=num_or_size_splits, value=value, name=name)\r\n407     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py:9496 split\r\n408         \"Split\", split_dim=axis, value=value, num_split=num_split, name=name)\r\n409     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:527 _apply_op_helper\r\n410         preferred_dtype=default_dtype)\r\n411     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1224 internal_convert_to_tensor\r\n412         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n413     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:305 _constant_tensor_conversion_function\r\n414         return constant(v, dtype=dtype, name=name)\r\n415     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:246 constant\r\n416         allow_broadcast=True)\r\n417     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:290 _constant_impl\r\n418         name=name).outputs[0]\r\n419     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507 new_func\r\n420         return func(*args, **kwargs)\r\n421     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3588 create_op\r\n422         self._check_not_finalized()\r\n423     env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3225 _check_not_finalized\r\n424         raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\n425 \r\n426     RuntimeError: Graph is finalized and cannot be modified.\r\n427 \r\n428 E0723 15:56:06.049019 140354566588224 tf_should_use.py:71] ==================================\r\n429 Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\r\n430 <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fa5fc348f98>      \r\nIf you want to mark it as used call its \"mark_used()\" method.\r\n432 It was originally created here:\r\n433   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3722, in <genexpr>\r\n434     for ta, input_ in zip(input_ta, flatted_inputs))  File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n435     return _add_should_use_warning(fn(*args, **kwargs))\r\n436 ==================================\r\n437 E0723 15:56:06.049994 140354566588224 tf_should_use.py:71] ==================================\r\n438 Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\r\n439 <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fa5fc37a240>\r\n440 If you want to mark it as used call its \"mark_used()\" method.\r\n441 It was originally created here:\r\n442   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3722, in <genexpr>\r\n443     for ta, input_ in zip(input_ta, flatted_inputs))  File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n444     return _add_should_use_warning(fn(*args, **kwargs))  File \"/home/luban/\r\n/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1256, in unstack\r\n445     return self._implementation.unstack(value, name=name)  File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n446     return _add_should_use_warning(fn(*args, **kwargs))\r\n447 ==================================\r\n448 E0723 15:56:06.050341 140354566588224 tf_should_use.py:71] ==================================\r\n449 Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\r\n450 <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fa5fc37ac88>\r\n451 If you want to mark it as used call its \"mark_used()\" method.\r\n452 It was originally created here:\r\n453   File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3722, in <genexpr>\r\n454     for ta, input_ in zip(input_ta, flatted_inputs))  File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n455     return _add_should_use_warning(fn(*args, **kwargs))  File \"/home/luban//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1256, in unstack\r\n456     return self._implementation.unstack(value, name=name)  File \"/home//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n457     return _add_should_use_warning(fn(*args, **kwargs))  File \"env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 351, in unstack\r\n458     indices=math_ops.range(0, num_elements), value=value, name=name)  File \"//env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 193, in wrapped\r\n459     return _add_should_use_warning(fn(*args, **kwargs))\r\n460 ==================================                                                                                                                                                       \r\n```\r\n\r\nhow to fix this ?", "Any updates on this?", "@zh794390558 A dirty but easiest workaround for this would be:\r\n```python\r\n# g : tf.Graph (e.g. tf.get_default_graph())\r\n\r\ng._finalized = False       # de-finalized for a while\r\n# ... (add more ops) ...\r\ng.finalize()              # finalize again\r\n\r\n```\r\n", "@wookayin, can this work with `tf.estimator.train_and_evaluate`?\r\n\r\n**UPDATE**: no, it fails with `AttributeError: can't set attribute`.", "This is working as intended -- saving when the Graph is finalized won't work because the Saver needs to add saving/restore ops to properly save the variables. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26430\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26430\">No</a>\n", "@k-w-w, any ideas why [training the tf.estimator](https://github.com/sdll/psenet/blob/8ecf0631051b877a8bfb8ca5ed3e7d6c92bf8428/psenet/train.py#L97) with a Keras model under the hood can fail with this error?", "@k-w-w I don't think this can be closed. Nor any documentation of the behavior we have.\r\n\r\nI think the problem is really when one use `MonitoredSession`, `tf.{estimator,train}.train_and_evalute` (or any similar wrappers), `tf.Estimator` because the graph gets automatically finalized inside and we don't have much control of (keras) model to ensure the creation of saving/restore ops.", "@sdll @wookayin How is the model being exported to SavedModel? Are you using `estimator.export_saved_model`?", "@k-w-w, the problem occurs when [using `tf.estimator.train_and_evaluate`](https://github.com/sdll/psenet/blob/8ecf0631051b877a8bfb8ca5ed3e7d6c92bf8428/psenet/train.py#L217) as @wookayin describes. The training step goes well, but then the program crashes on eval. Might this be because we need to convert the keras model to the estimator, instead of using keras in `model_fn`?"]}, {"number": 26429, "title": "Cannot import tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution  Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary): pip3 install -U tf-nightly-gpu-2.0-preview\r\n- TensorFlow version: cannot be imported\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nImport Error : \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~/anaconda3/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~/anaconda3/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-1ceb9a7c68bc> in <module>()\r\n      2 import time\r\n      3 \r\n----> 4 import tensorflow as tf\r\n      5 from tensorflow.python.ops import lookup_ops\r\n      6 \r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow/__init__.py in <module>()\r\n     25 import sys as _sys\r\n     26 \r\n---> 27 from tensorflow._api.v2 import audio\r\n     28 from tensorflow._api.v2 import autograph\r\n     29 from tensorflow._api.v2 import bitwise\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow/_api/v2/audio/__init__.py in <module>()\r\n      6 from __future__ import print_function as _print_function\r\n      7 \r\n----> 8 from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n      9 from tensorflow.python.ops.gen_audio_ops import encode_wav\r\n     10 \r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n**Any other info / logs**\r\nInstalled Anaconda using : wget https://repo.anaconda.com/archive/Anaconda3-5.0.0-Linux-x86_64.sh\r\npython3 --version gives python 3.6.2\r\nInstalled tensorflow 2 using cmd : pip3 install -U tf-nightly-gpu-2.0-preview\r\n\r\nOpened Jupyter notebook and doing : import tensorflow as tf\r\n", "comments": ["You should wrap the code in a fenced codeblocks using three backticks ( ``` ).\r\n\r\nThe error message says: `ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory` --- you should have CUDA 10 installed.\r\n", "Hi,\nI was using : pip3 install -U tf-nightly-gpu-2.0-preview , which is a gpu\nversion, and I was doing in AWS EC2 t2large CPU instance.\nNow I have installed : tensorflow==2.0.0-alpha0,\nGetting a diff issue: its not finding this in jupyter :ModuleNotFoundError:\nNo module named 'tensorflow'\n\nPls see log here , I am in virtualenv, and also able to print Tensorflow\nversion from cmd line. But the same does not work in jupyter.\n\n (venv) ubuntu@ip-172-31-22-38:~$ python3 -c \"import tensorflow as tf;\nprint(tf.__version__);\"\n\n2.0.0-alpha0\n\n Followed the steps : https://www.tensorflow.org/install/pip\n\n\nPlease help.\n\nThanks\n\nOn Thu, Mar 7, 2019 at 1:41 PM Jongwook Choi <notifications@github.com>\nwrote:\n\n> You should wrap the code in a fenced codeblocks using three backticks (\n> ``` ).\n>\n> The error message says: ImportError: libcublas.so.10.0: cannot open\n> shared object file: No such file or directory --- you should have CUDA 10\n> installed.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26429#issuecomment-470426654>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ANFvt0owdG38R82E7R5GLF0CDNYcNXwJks5vUMnJgaJpZM4bijmX>\n> .\n>\n", "@anirbankonar123 Did you install tensorflow from source? Are you running jupyter from the source directory? \r\nYou need to run Jupyter notebook from source directory. Or if you are using a virtual environment(venv), you need to install Tensorflow in that too in order for Jupyter notebook to import it.\r\nLet me know if this resolves your issue. Thanks :)", "Pls see here the steps:\nI am activating venv, and the fact its activated is proven by the\ntensorflow version which i can print out from cmdline:\nubuntu@ip:~$ source activate venv\n(venv) ubuntu@ip:~$ python3 -c \"import tensorflow as tf;\nprint(tf.__version__); \"\n2.0.0-alpha0\n(venv) ubuntu@ip:~$ jupyter notebook\n\nbut then the same error msg : ModuleNotFoundError: No module named\n'tensorflow'\n\nCant believe my eyes :) Maybe will reinstall evthing.\n\n\n\n\nOn Thu, Mar 7, 2019 at 4:37 PM Ayush Agrawal <notifications@github.com>\nwrote:\n\n> @anirbankonar123 <https://github.com/anirbankonar123> Did you install\n> tensorflow from source? Are you running jupyter from the source directory?\n> You need to run Jupyter notebook from source directory. Or if you are\n> using a virtual environment(venv), you need to install Tensorflow in that\n> too in order for Jupiter notebook to import it.\n> Let me know if this resolves your issue. Thanks :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26429#issuecomment-470484566>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ANFvt6021lOI1GMm2qiG9WUuqAqKxL_rks5vUPL4gaJpZM4bijmX>\n> .\n>\n", "@anirbankonar123 If you really want to reinstall, I would recommend you to install via Anaconda. It'll be much easier to manage your packages through that.\r\nDo let us know about the progress or issues you face. Thanks", "@anirbankonar123 I don't see any issue. check the image [here](https://screenshot.googleplex.com/d9OAKP6x4E8). \r\n\r\nBy default, when you enter like (venv) ubuntu@ip:~$ jupyter notebook, a jupyter dashboard opens up like [this](https://screenshot.googleplex.com/beEJn01vtqg) \r\nthen go to your venv folder like [this](https://screenshot.googleplex.com/5HVwQCUf69A). Then run the commands. It will work. Please close once you verify it. \r\n\r\nIn future, please post support questions in stackoverflow. We should this GitHub uncluttered for the open source developers. Thanks!", "Thanks, actually I was doing evthing fine, had a small issue . Had to start up a kernel properly before starting up Jupyter : \r\ncmd : ipython kernel install --user --name=.venv  (was doing this but with name=venv, missed the . (dot) !).\r\nThanks for the responses, appreciate it.\r\nClosing the issue.\r\nThanks"]}, {"number": 26428, "title": "Fix the wrong container name in AddResourceInput", "body": "When `container == \"\"`, this function puts the resource in container `default_container()`, but sets the handle to the wrong container `\"\"`.\r\n\r\nThis PR fixes the behavior.\r\n\r\n// Originally found by Wendy Huang. fix #26454", "comments": []}, {"number": 26427, "title": "[Intel MKL] Parallelize UnsortedSegmentSum on CPU deivce.", "body": "UnsortedSegmentSum is a single thread op on CPU, here use the Eigen device\r\n to do parallelization.\r\n\r\nmodified:\r\n- tensorflow/core/kernels/segment_reduction_ops.cc\r\n- tensorflow/core/kernels/segment_reduction_ops_test.cc\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["It can be speed up 3x times above. Several models in MLPerf model zoo can be benefited from this optimization:\r\n- For NCF, execution time of this op has changed from ```12.89ms``` to ```2.37ms``` per step;\r\n- For Transformer_LT, execution time of this op has changed from ```16.58ms``` to ```4.29ms``` per step;\r\n\r\nI added a benchmark to test the improvement:\r\n\r\nOrigin:\r\n===\r\nBM_UnsortedSegmentSum_4096_128_1_          300315       2279     6983.2MB/s\r\nBM_UnsortedSegmentSum_4096_128_128_        306408       2268     6844.3MB/s\r\n\r\nOptimized:\r\n===\r\nBM_UnsortedSegmentSum_4096_128_1_          302769       2268     6926.6MB/s\r\nBM_UnsortedSegmentSum_4096_128_128_        116179       6112     18051.1MB/s\r\n\r\nThis patch tries to parallel UnsortSegmentReduction with `num_segments`, so the performance has no change if `num_segments` is 1, otherwise it has ~3X improvement in the benchmark.\r\n", "@rthadur per https://github.com/tensorflow/tensorflow/pull/26457#issuecomment-470700742 I'm not the right person to review this non-XLA change.", "@rthadur I would prefer to give this to someone else, I already have a list of PRs to push through. Sorry, and thanks for your understanding.", "There're some comments need to be normalized, temporarily close this PR now. I'll reopen it once I finish this work, thanks.", "![image](https://user-images.githubusercontent.com/38638514/56342254-125f7c80-61ea-11e9-9e31-856b50995d77.png)\r\n\r\nHere's a simple introduce for the optimization:\r\n1. `row_counter` stores how many rows will be reduce to each output row, `num_reductions` is the real `num_segments` which excluded 0 count rows, it also decides the degree of parallelism.\r\n2. Loop `row_counter` to pick each task to `block_range`. `block_range` records task index for shard function.\r\n ", "pinging @ezhulenev for review. Thanks.", "Thank you, @ezhulenev! I'm removing my review request then.", "They seem to be unrelated to changes in this PR, should be pulled in on monday..", "I'll be submitting roll back of this PR., it fails with SIGFPE:\r\n\r\n```E0502 20:13:20.231303    4313 process_state.cc:1210] *** SIGFPE (@0x5624fcd7d9b9), see go/stacktraces#s15 received by PID 4192 (TID 4313); stack trace: ***\r\nE0502 20:13:20.253758    4313 process_state.cc:1213] PC: @     0x5624fcd7d9b9  (unknown)  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n    @     0x562503d2359f       1648  FailureSignalHandler()\r\n    @     0x7f5ffe08d9a0    6178112  (unknown)\r\n    @     0x5624fcd7d670        192  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n    @     0x56250078c627       2528  tensorflow::(anonymous namespace)::ExecutorState::Process()\r\n    @     0x5625007950fc         48  std::__g::_Function_handler<>::_M_invoke()\r\n    @     0x562500bd0870        192  Eigen::ThreadPoolTempl<>::WorkerLoop()\r\n    @     0x562500bcff81         48  std::__g::_Function_handler<>::_M_invoke()\r\n    @     0x562502573cc1        208  Thread::ThreadBody()\r\n    @     0x7f5ffe0854e8        176  start_thread\r\n    @     0x7f5ffdebc22d  (unknown)  clone\r\nW0502 20:13:20.253819    4313 process_state.cc:1221] --- CPU registers: ---\r\n   r8=7f5fe5537e40  r9=0 r10=52e4 r11=0 r12=6 r13=1 r14=fffffffffffffffd r15=0\r\n  rdi=1e rsi=7f5fb4daef20 rbp=7f5fe55378b0 rbx=6 rdx=0 rax=5 rcx=5\r\n  rsp=7f5fe5537700 rip=5624fcd7d9b9 efl=10217 cgf=33 err=0 trp=0\r\n  msk=fffffffe10000000 cr2=0```\r\n\r\nWill try to find why exactly it fails tomorrow.", "I have a fix already, `num_reductions` can be 0. Will re-submit this PR tomorrow.", "@ezhulenev Could you help to add a corresponding case to `./tensorflow/python/kernel_tests/segment_reduction_ops_test.py` when you re-submit this PR? \r\n```\r\ndiff --git a/tensorflow/python/kernel_tests/segment_reduction_ops_test.py b/tensorflow/python/kernel_tests/segment_reduction_ops_test.py\r\nindex 8af1b47e83..1280979b1b 100644\r\n--- a/tensorflow/python/kernel_tests/segment_reduction_ops_test.py\r\n+++ b/tensorflow/python/kernel_tests/segment_reduction_ops_test.py\r\n@@ -462,6 +462,12 @@ class UnsortedSegmentTest(SegmentReductionHelper):\r\n         self.assertAllClose(np_ans, tf_ans)\r\n         self.assertShapeEqual(np_ans, s)\r\n \r\n+  def testAllNegatives(self):\r\n+    with self.session(use_gpu=False):\r\n+      data = np.ones((2, 1), dtype=np.float32)\r\n+      segment_ids = np.array([-1, -1], dtype=np.int32)\r\n+      unsorted = math_ops.unsorted_segment_sum(data, segment_ids, 2)\r\n+      self.assertAllClose(unsorted.eval(), np.zeros((2, 1), dtype=np.float32))\r\n```\r\nThis new corner case could help to detect similar error.\r\n", "PR was rolled back one more time, now with this error:\r\n\r\n```\r\n @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n    @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n    @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n    @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n    @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n    @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n    @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n    @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n    @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n    @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n    @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n    @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n    @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n    @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n    @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n    @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n    @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n    @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n    @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n    @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n    @     0x55bcb94d8225  tensorflow::Device::Compute()\r\n    @     0x55bcb94d8225  tensorflow::Device::Compute()\r\n\r\nterminate called after throwing an instance of 'std::__g::out_of_range'\r\n  what():  vector::_M_range_check: __n (which is 2) >= this->size() (which is 2)\r\n*** SIGABRT received by PID 9955 (TID 10144) from PID 9955; ***\r\n```", "> PR was rolled back one more time, now with this error:\r\n> \r\n> ```\r\n>  @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n>     @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n>     @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n>     @     0x55bcc6efeea0  std::__g::__throw_out_of_range_fmt()\r\n>     @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n>     @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n>     @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n>     @     0x55bca31faf2b  std::__g::vector<>::_M_range_check()\r\n>     @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n>     @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n>     @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n>     @     0x55bca321af84  std::__g::vector<>::operator[]()\r\n>     @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n>     @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n>     @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n>     @     0x55bcbc70702b  tensorflow::functor::UnsortedSegmentFunctor<>::operator()()\r\n>     @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n>     @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n>     @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n>     @     0x55bcbc7069e8  tensorflow::UnsortedSegmentReductionOp<>::Compute()\r\n>     @     0x55bcb94d8225  tensorflow::Device::Compute()\r\n>     @     0x55bcb94d8225  tensorflow::Device::Compute()\r\n> \r\n> terminate called after throwing an instance of 'std::__g::out_of_range'\r\n>   what():  vector::_M_range_check: __n (which is 2) >= this->size() (which is 2)\r\n> *** SIGABRT received by PID 9955 (TID 10144) from PID 9955; ***\r\n> ```\r\n\r\nCould you share the failed UT? I'll refine the code and verify again before re-submit."]}, {"number": 26426, "title": "[Intel Mkl] Parallel BiasAddGrad op with eigen intra thread pool", "body": "The BiasAddGrad op is running on single thread, which badly influences the training performance of corresponding models.\r\n\r\nWe provide a optimized parallel implementation of BiasAddGrad op.\r\n\r\nThe following is the time cost for original and optimized BiasAddGrad op of some corresponding models. All run on skx-8180 single socket:\r\nYoloV2 (batch size 16) : 263.76ms -> 12.06ms\r\nTrans-LT (batch size 1024) : 68.61ms -> 3.09ms\r\nInception-ResV2 (batch size 64) : 334.96ms -> 73.18ms\r\nNCF (batch size 1024) : 1.34ms -> 0.243ms\r\nNCF-mlperf (batch size 1024) : 0.867ms -> 0.341ms\r\nMaskRCNN (batch size 1) : 370.57ms -> 15.23ms\r\nDCGAN (batch size 32) : 10.46ms -> 0.512ms", "comments": ["I did run benchmarks from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/bias_op_test.cc and got pretty good results already, feel free to add more benchmarks, probably you want to test NCHW as well.\r\n\r\n```name                                              old time/op             new time/op             delta\r\nBM_BiasAddGradNHWC_32_32_32_128_cpu               4.84ms \u00b1 1%             2.30ms \u00b1 1%  -52.46%        (p=0.000 n=10+10)\r\nBM_BiasAddGradNHWC_32_32_32_256_cpu               9.66ms \u00b1 2%             4.15ms \u00b1 1%  -57.08%        (p=0.000 n=10+10)\r\nBM_BiasAddGradNHWC_32_32_32_512_cpu               80.1ms \u00b113%             18.4ms \u00b1 4%  -76.97%        (p=0.000 n=10+10)\r\nBM_BiasAddGradNHWC_32_32_32_1024_cpu               242ms \u00b1 2%               41ms \u00b117%  -83.17%          (p=0.000 n=9+9)\r\nBM_BiasAddGradNHWC_32_64_64_128_cpu               58.3ms \u00b1 3%             18.5ms \u00b1 5%  -68.16%        (p=0.000 n=10+10)\r\nBM_BiasAddGradNHWC_32_64_64_256_cpu                177ms \u00b1 5%               44ms \u00b112%  -75.48%         (p=0.000 n=9+10)\r\nBM_BiasAddGradNHWC_32_64_64_512_cpu                452ms \u00b1 6%               92ms \u00b1 8%  -79.75%        (p=0.000 n=10+10)\r\nBM_BiasAddGradNHWC_32_64_64_1024_cpu               1.19s \u00b1 5%              0.23s \u00b1 7%  -80.93%        (p=0.000 n=10+10)\r\n\r\nname                                              old allocs/op           new allocs/op           delta\r\nBM_BiasAddGradNHWC_32_32_32_128_cpu                  106 \u00b1 0%                 90 \u00b1 0%  -15.09%         (p=0.000 n=10+9)\r\nBM_BiasAddGradNHWC_32_32_32_256_cpu                  170 \u00b1 0%                 90 \u00b1 0%  -47.06%         (p=0.000 n=10+9)\r\nBM_BiasAddGradNHWC_32_32_32_512_cpu                  170 \u00b1 0%                 90 \u00b1 1%  -47.35%         (p=0.000 n=8+10)\r\nBM_BiasAddGradNHWC_32_32_32_1024_cpu                 170 \u00b1 0%                 90 \u00b1 1%  -47.33%        (p=0.000 n=10+10)\r\nBM_BiasAddGradNHWC_32_64_64_128_cpu                  107 \u00b1 1%                 90 \u00b1 0%  -15.57%         (p=0.000 n=10+9)\r\nBM_BiasAddGradNHWC_32_64_64_256_cpu                  171 \u00b1 0%                 90 \u00b1 0%  -47.37%          (p=0.001 n=8+9)\r\nBM_BiasAddGradNHWC_32_64_64_512_cpu                  170 \u00b1 0%                 90 \u00b1 1%  -47.33%        (p=0.000 n=10+10)\r\nBM_BiasAddGradNHWC_32_64_64_1024_cpu                 170 \u00b1 0%                 90 \u00b1 1%  -47.48%        (p=0.000 n=10+10)\r\n```", "Based on your PR I've submitted very similar change in https://github.com/tensorflow/tensorflow/commit/d5f6595171bb21e19da1c7dc75d02f3df85cf12b, apparently this \"reduce all outer dimension\" is pretty common in other gradient kernels (e.g. in FusedBatchNorm https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L215). Can you please move the code for NCHW case to `redux_functors.h`, and call it similar way. Not sure though what's a good name for it.\r\n\r\nCurrently it takes Tensor as an input, but it could be any Eigen expression, I'll work on it after this PR will be merged.", "Hi @ezhulenev ,\r\nI have read the codes in ReduceOuterDimensions.\r\nSo, you want me to add a functor for situations like:\r\n    (32, 11, 256, 256) -> (11);\r\nor maybe more general like:\r\n    (D1, D2, ... , DN) -> (DM) (where M belong to set [1,N])\r\nand then ref the new functor to do the reduce?", "Yeah, that\u2019s the idea. I think you can skip the general case, and just add\nsupport for (d1, ... , dn) -> dm where m is in [2, n-1] range. Reducing\ninner most dimensions are already fast in Eigen, I added optimized case for\nreducing outer dimensions. We just need one more optimized version for\nreducing all-except \u201cmiddle\u201d dimension.\nOn Mon, Mar 11, 2019 at 11:57 PM Letian <notifications@github.com> wrote:\n\n> Hi @ezhulenev <https://github.com/ezhulenev> ,\n> I have read the codes in ReduceOuterDimensions.\n> So, you want me to add a functor for situations like:\n> (32, 11, 256, 256) -> (11);\n> or maybe more general like:\n> (D1, D2, ... , DN) -> (DM) (where M belong to set [1,N])\n> and then ref the new functor to do the reduce?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26426#issuecomment-471876004>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABHrajX_gC0cA2ojKyuJ4NRtE2P_La-zks5vV0_ugaJpZM4bifIc>\n> .\n>\n", "@ezhulenev  OK", "@ezhulenev \r\nCould you please provide the command to run the test in bias_op_test.cc?", "`bazel run .... -- --benchmarks=all` I think something like that, but I'm not 100% sure, it might be some difference between internal/external tooling. Teng Lu (@Zantares) from Intel made it work in https://github.com/tensorflow/tensorflow/pull/26424#issuecomment-472965743, I guess he definitely has the answer :)", "This fails internally with a error:\r\n\r\n```\r\n\r\nInvalidArgumentError: var and grad do not have the same shape[1] [3,1]\r\n\t [[node adagrad/slot_11/update_matmul3_b/ApplyAdagrad (defined at {INTERNAL}/training_graph_generator.py:1749) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node adagrad/slot_11/update_matmul3_b/ApplyAdagrad:\r\n gradients/matmul3/BiasAdd_grad/BiasAddGrad (defined at {INTERNAL}//training_graph_generator.py:1306)\t\r\n matmul3_b (defined at {INTERNAL})\r\n\r\n```\r\n\r\ndidn't have time to debug it yet", "@ezhulenev \r\nOK, how can I reproduce the error? I can not find the file \"training_graph_generator.py\" anywhere...", "@ezhulenev is there anything I can do?", "I'll try to prepare reproducible test, right now it's a part of a large model and it's hard to tell whats going on.", "@ezhulenev Thx. Just tell me if there is anything to do. We need this pr merged ASAP, cause several model is depending on this one."]}, {"number": 26425, "title": "How to predict images using trained mobilenet model?", "body": "Hi ,I somehow managed to train my mobile net model but i want to know with the model trained, how can i  use it to make predictions about images?\r\nKindly Suggest it will be a great help!\r\n\r\n\r\n", "comments": ["@harshita-pal Can you please try going through this [Link](https://www.tensorflow.org/hub/tutorials/image_retraining). It contains step-by-step procedure and instructions to manage the model.\r\nLet us know if you still need help \ud83d\ude04 ", "@harshita-pal In future, Please post this kind of support questions in stackoverflow. We would like to keep GitHub less cluttered for open source developers. Close the issue as soon as possible. Thanks!", "I am implementing MobileNet Modal but it is not giveing me a relevent result see : https://prnt.sc/mwuz3r\r\nthis is a Iphone and it is saying Remote.\r\ncan I trained my own modal ?", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26424, "title": "[Intel MKL] Use Shard function instead of Eigen device to parallelize Adam kernel.", "body": "This could reduce the memory access and get good cache locality for CPU.\r\n\r\nmodified:\r\n- tensorflow/core/kernels/training_ops.cc\r\n- tensorflow/core/kernels/training_ops.h\r\n- tensorflow/core/kernels/training_ops_gpu.cu.cc\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["Eigen device expression can only update 1 variable once, but Adam needs to update 3 variables and uses 3 expression which would impact the cache locality of CPU. Here use Shard function to replace Eigen device expression.\r\n\r\nThis patch is tested on NCF model which is in MLPerf 0.5 submission.\r\nIt can speed up Adam kernel 15%~30%, and improve 10%~20% overall for the model.", "pinging @ezhulenev  for a review. Thanks.", "> Could you please also add a benchmark similar to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/bias_op_test.cc, I'll need it to run performance testing internally.\r\n\r\nHi, @ezhulenev , I've refined the code and added a benchmark https://github.com/tensorflow/tensorflow/pull/26424/files#diff-0b9bd0c5daec98f25d2e15c9b8c0370cR200.\r\n\r\nMy test result is:\r\n\r\noriginal:\r\nBenchmark             Time(ns) Iterations\r\n-----------------------------------------\r\nBM_SGD/131072            58816      10000        8914.0MB/s 2228.5M items/s\r\nBM_SGD/262144           107837       6335        9723.7MB/s 2430.9M items/s\r\nBM_Adagrad/131072       136092       5126        3852.5MB/s 963.1M items/s\r\nBM_Adagrad/262144       216470       3093        4844.0MB/s 1211.0M items/s\r\nBM_Momentum/131072      126981       5114        4128.9MB/s 1032.2M items/s\r\nBM_Momentum/262144      206378       3434        5080.9MB/s 1270.2M items/s\r\nBM_Adam/131072/0        194416       3452        2696.7MB/s 674.2M items/s\r\nBM_Adam/262144/0        334504       2110        3134.7MB/s 783.7M items/s\r\n**BM_Adam/16777216/1     9864090        100        6803.4MB/s 1700.8M items/s**\r\nBM_RMSProp/131072       187562       3545        2795.3MB/s 698.8M items/s\r\nBM_RMSProp/262144       334770       2180        3132.2MB/s 783.1M items/s\r\nBM_AddSign/131072       512574       1449        1022.9MB/s 255.7M items/s\r\nBM_AddSign/262144       922186        727        1137.1MB/s 284.3M items/s\r\nBM_PowerSign/131072    2179936        311        240.5MB/s 60.1M items/s\r\nBM_PowerSign/262144    3963514        177        264.6MB/s 66.1M items/s\r\n\r\noptimized:\r\nBenchmark             Time(ns) Iterations\r\n-----------------------------------------\r\nBM_SGD/131072            69680       9636        7524.2MB/s 1881.0M items/s\r\nBM_SGD/262144            95855       5395        10939.2MB/s 2734.8M items/s\r\nBM_Adagrad/131072       158376       5181        3310.4MB/s 827.6M items/s\r\nBM_Adagrad/262144       234968       2831        4462.6MB/s 1115.7M items/s\r\nBM_Momentum/131072      118026       5495        4442.1MB/s 1110.5M items/s\r\nBM_Momentum/262144      215430       3169        4867.4MB/s 1216.8M items/s\r\nBM_Adam/131072/0        202429       3486        2590.0MB/s 647.5M items/s\r\nBM_Adam/262144/0        328765       1965        3189.4MB/s 797.4M items/s\r\n**BM_Adam/16777216/1     7393820        100        9076.3MB/s 2269.1M items/s**\r\nBM_RMSProp/131072       187683       3003        2793.5MB/s 698.4M items/s\r\nBM_RMSProp/262144       372268       2006        2816.7MB/s 704.2M items/s\r\nBM_AddSign/131072       648737       1000        808.2MB/s 202.0M items/s\r\nBM_AddSign/262144       876228        764        1196.7MB/s 299.2M items/s\r\nBM_PowerSign/131072    2178264        322        240.7MB/s 60.2M items/s\r\nBM_PowerSign/262144    3908483        174        268.3MB/s 67.1M items/s\r\n\r\n---\r\n\r\nThere may be some data variance between different execution, but you can find the optimized is always better than the original.", "BTW, the Tensor vectorization form has similar performance with the loop version, I guess maybe Eigen can generate same ASM internally, but I didn't dig too deep.", "I guess after inlining it all might have been fused into a single loop by compiler. Anyway it's great that there is no performance difference and we can keep simpler code.", "When I try to use `block_align` to align shard size, I found the performance was decreased in real model, then I captured the param size from model and made a small benchmark: https://github.com/tensorflow/tensorflow/pull/26424/commits/e4dae32cdae1220a19e431e76520b361643e866b#diff-0b9bd0c5daec98f25d2e15c9b8c0370cR200.\r\n\r\n**env:** Intel Xeon skylake-8180, 56 cores\r\n**cmd:** numactl -N 0 -l bazel run --config=mkl --copt=-mavx2 --copt=-mfma --copt=-march=broadwell --copt=-O2 --copt=-L$HOME/code/1/gcc6/gcc6.3/lib64/  -- //tensorflow/core/kernels:training_ops_test -- --benchmarks=..\r\n\r\ncurrent implementation:\r\n---\r\nBM_Adam/8192/1           82133       8295        **399.0MB/s 99.7M items/s**\r\nBM_Adam/16777216/1     8501990        100        7893.3MB/s 1973.3M items/s\r\n\r\nwith `block_align`:\r\n---\r\nBM_Adam/8192/1           88312       7707        **371.0MB/s 92.8M items/s**\r\nBM_Adam/16777216/1     8462090        100        7930.5MB/s 1982.6M items/s\r\n\r\n---\r\nWith \"manual vectorization\", the small benchmark will get better performance(+10%). It's really confused me, maybe Eigen efficiency model https://bitbucket.org/eigen/eigen/src/4b28c8008901c6d760f48f26ee2e3423fd8a2b40/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h?fileviewer=file-view-default#TensorDeviceThreadPool.h-188 can't handle small size with block_align well?\r\n\r\n---\r\nHow I use `block_align`\r\n```\r\n+    // Set a function to align block size to packet size, which can get more\r\n+    // chance to vectorize.\r\n+    auto block_align = [packet_size](Index block_size) -> Index {\r\n+      return Eigen::divup(block_size, packet_size) * packet_size;\r\n+    };\r\n+    d.parallelFor(length, cost, block_align, shard)\r\n```\r\n`block_align` will get the `block_size` computed by Eigen efficiency model, it allows us round up the size with own rule and return it as the new block size. I must increase the block size in `block_align` , or it will trap in an infinity loop.\r\nBase on the result, I prefer to use \"manual vectorization\" version, how do you think about this situation?\r\n\r\n\r\n\r\n", "That's strange. I'll try to reproduce it internally after it will be merged.", "I think the problem is in incorrectly computed cost, and Eigen sharding too much or too less.", "> I think the problem is in incorrectly computed cost, and Eigen sharding too much or too less.\r\n\r\nHi @ezhulenev ,please take a look at the new commit, I fixed an error of computing cost - need to multiply `length` to `compute_cycles`. I also checked the CostModel in Eigen, it already had some estimation on cache so I added the store cost. This should be the last commit if no more review suggestion.\r\n\r\nThe 'manual vectorization' is still better than `block_align`, I guess because compiler could get more static info from 'manual vectorization' while `block_align` may generate a tail that can't be divided in run-time.\r\n\r\n"]}, {"number": 26423, "title": "[TF 2.0 API docs] Creating the tf.keras.losses.poisson documentation", "body": "This addresses issue #25973.\r\n\r\nThis is my first PR to TensorFlow, so please let me know if something is not up to standard. \ud83d\ude42", "comments": []}, {"number": 26422, "title": "using TFlite: ModuleNotFoundError: No module named '_tensorflow_wrap_interpreter_wrapper'", "body": "**System information**\r\n- Windows 10:\r\n- TensorFlow installed from pip:\r\n- TensorFlow version 1.12.0:\r\n- Python 3.6.8:\r\n\r\nFailed when trying to run a mobilenet_v1_1.0_224.tflite model using tflite. So I copy the interpreter_test.py and run it. End up with 10 fails in 12 tests, all with the same error of '_tensorflow_wrap_interpreter_wrapper'.\r\n\r\nAlso, my D:\\Python 36\\Lib\\site-packages\\tensorflow\\contrib\\lite\\python\\interpreter_wrapper folder only has : \r\n/__pycache__ , tensorflow_wrap_interpreter_wrapper.py , __init__py\r\n\r\nI rename the tensorflow_wrap_interpreter_wrapper.py and uploaded it to see if some one can help me.\r\n[tensorflow_wrap_interpreter_wrapper.txt](https://github.com/tensorflow/tensorflow/files/2939561/tensorflow_wrap_interpreter_wrapper.txt)\r\n\r\n**Error log**\r\nTraceback (most recent call last):\r\n  File \"D:\\Python 36\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\interpreter_wrapper\\tensorflow\r\n_wrap_interpreter_wrapper.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_tensorflow_wrap_interpreter_wrapper', [dirname(__f\r\nile__)])\r\n  File \"D:\\Python 36\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_tensorflow_wrap_interpreter_wrapper'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_tflite.py\", line 52, in <module>\r\n    tflite_output = run(model_file, image_data)\r\n  File \"run_tflite.py\", line 16, in run\r\n    interpreter = interpreter_wrapper.Interpreter(model_path=model_file)\r\n  File \"D:\\Python 36\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\interpreter.py\", line 52, in _\r\n_init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"D:\\Python 36\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\", line 53, in __getattr\r\n__\r\n    module = self._load()\r\n  File \"D:\\Python 36\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"D:\\Python 36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"D:\\Python 36\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\interpreter_wrapper\\tensorflow\r\n_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"D:\\Python 36\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\interpreter_wrapper\\tensorflow\r\n_wrap_interpreter_wrapper.py\", line 20, in swig_import_helper\r\n    import _tensorflow_wrap_interpreter_wrapper\r\nModuleNotFoundError: No module named '_tensorflow_wrap_interpreter_wrapper'\r\n\r\n", "comments": ["I faced the same error.\r\nYou can try `pip install tf-nightly`.\r\nIt solved the issue in my case.\r\nYou may also need to change `tf.contrib.lite.Interpreter` to `tf.lite.Interpreter`\r\n\r\n", "> I faced the same error.\r\n> You can try `pip install tf-nightly`.\r\n> It solved the issue in my case.\r\n> You may also need to change `tf.contrib.lite.Interpreter` to `tf.lite.Interpreter`\r\n\r\nIt works in my case too ! Thanks a lot !", "I think it was resolved. I am closing the issue. Please open a new ticket if you see similar issue again. Thanks!", "So you need to install tf to use tfLite ? \r\n\r\nI have followed the instructions from here to install tfLIte on a RBPi https://www.tensorflow.org/lite/guide/build_rpi \r\n\r\n\r\nBut, when I try to run some example code I get an \"ImportError: No module named 'tensorflow' error  \" ", "I'm not sure this is related, but when I installed tensorflow lite using https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package\r\nI needed to add the following files to Extension(sources=[...]):\r\n```\r\n'interpreter_wrapper/numpy.cc',\r\n'interpreter_wrapper/python_error_reporter.cc',\r\n'interpreter_wrapper/python_utils.cc',\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/setup.py#L125\r\n", "> I faced the same error.\r\n> You can try `pip install tf-nightly`.\r\n> It solved the issue in my case.\r\n> You may also need to change `tf.contrib.lite.Interpreter` to `tf.lite.Interpreter`\r\n\r\nThis works! You saved me. Thank you."]}, {"number": 26421, "title": "Syntax Error in Udacity example code", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: nVIDIA GTX 1050 Ti\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nA syntax error is raised.\r\n\r\n**Describe the expected behavior**\r\nThe example code should work without modifications.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nThe first code block on:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb\r\n\r\ngives an error when it tries to execute:\r\n```python\r\n# Config the matplotlib backend as plotting inline in IPython\r\n%matplotlib inline\r\n```\r\n`SyntaxError: invalid syntax`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@CJDennis If you are not using Jupyter IPython notebook, just comment out (or delete) the line, everything will work fine and a separate plot window will be opened if you are running your python script from the console.\r\nIf you want to make use of the line, install Jupyter IPython and run the notebook through that.\r\nLet me know if the issue still exists. Thanks :)", "@Ayush517 Thanks! The tutorial didn't mention Jupyter anywhere that I saw. It also wasn't obvious how to get the .py code running in Jupyter, but I've got it working now!", "@CJDennis If it's resolved, you can close the issue \ud83d\ude04 \r\nJust let us know if you have any more issues. Thanks "]}, {"number": 26420, "title": "/var/lib/jenkins/workspace/19_EMBEDDED_AI_SRID/Source/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':", "body": "System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: 1.13.0rc0\r\nPython version: 3.6.5\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source): 0.21\r\nGCC/Compiler version (if compiling from source):  5.4.0 20160609\r\nCUDA/cuDNN version: 9.0\r\nGPU model and memory: RTX 2070 and 8GB\r\nDescribe the problem\r\n\r\nERROR: /var/lib/jenkins/workspace/19_EMBEDDED_AI_SRID/Source/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarrayobject.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarraytypes.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/numpyconfig.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_numpyconfig.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_endian.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_cpu.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/utils.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__multiarray_api.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_interrupt.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ufuncobject.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_math.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'\r\n  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__ufunc_api.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build", "comments": ["@abhajaswal Could you follow the installation process listed in TF [website](https://www.tensorflow.org/install/source) or Please follow the process listed [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions) but use updated drivers to install TF1.13. Also, keep in mind about the tested build [configurations](https://www.tensorflow.org/install/source#linux). Please let me know how it progresses. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26420\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26420\">No</a>\n"]}, {"number": 26419, "title": "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1711991009] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc", "body": "I tried to run a VAE code in my new desktop. It can run the tensorflow with GPU. However, when I tried some code in github, the error comes.\r\nCode source: https://github.com/hwalsuklee/tensorflow-mnist-VAE\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: source\r\nTensorFlow version: 1.12\r\nPython version: 3.6.8\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source): 0.17\r\nCUDA/cuDNN version: cuda 10.0 cuDNN 7.4.2\r\nGPU model and memory: Nvidia RTX 2080ti\r\n\r\nError:\r\n\r\nG:\\PythonLib\\tensorflow-mnist-VAE-master>python run_main.py --dim_z 2\r\nExtracting data\\train-images-idx3-ubyte.gz\r\nExtracting data\\train-labels-idx1-ubyte.gz\r\nExtracting data\\t10k-images-idx3-ubyte.gz\r\nExtracting data\\t10k-labels-idx1-ubyte.gz\r\n2019-03-06 21:47:07.963506: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-03-06 21:47:08.167281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.635\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.00GiB freeMemory: 9.03GiB\r\n2019-03-06 21:47:08.173938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-03-06 21:47:08.579713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-06 21:47:08.584136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2019-03-06 21:47:08.586602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2019-03-06 21:47:08.588443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8706 MB memory) -> physica\r\nl GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [2] vs. [0]\r\n         [[{{node gradients/Sum_1_grad/floordiv}} = FloorDiv[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/Sum_1_grad/Shape, gradients/Sum_1_grad/Maximum)]]\r\n\r\n         [[{{node Neg_1/_25}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_\r\ndevice_incarnation=1, tensor_name=\"edge_823_Neg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_main.py\", line 323, in <module>\r\n    main(args)\r\n  File \"run_main.py\", line 291, in main\r\n    feed_dict={x_hat: batch_xs_input, x: batch_xs_target, keep_prob : 0.9})\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [2] vs. [0]\r\n         [[node gradients/Sum_1_grad/floordiv (defined at run_main.py:228)  = FloorDiv[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/Sum_1_grad/Shape, gradi\r\nents/Sum_1_grad/Maximum)]]\r\n         [[{{node Neg_1/_25}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_\r\ndevice_incarnation=1, tensor_name=\"edge_823_Neg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'gradients/Sum_1_grad/floordiv', defined at:\r\n  File \"run_main.py\", line 323, in <module>\r\n    main(args)\r\n  File \"run_main.py\", line 228, in main\r\n    train_op = tf.train.AdamOptimizer(learn_rate).minimize(loss)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 400, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 519, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 630, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 814, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 408, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 814, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 82, in _SumGrad\r\n    tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 35, in _safe_shape_div\r\n    return x // math_ops.maximum(y, 1)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 866, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1111, in floordiv\r\n    return gen_math_ops.floor_div(x, y, name=name)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3079, in floor_div\r\n    \"FloorDiv\", x=x, y=y, name=name)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n...which was originally created as op 'Sum_1', defined at:\r\n  File \"run_main.py\", line 323, in <module>\r\n    main(args)\r\n  File \"run_main.py\", line 225, in main\r\n    y, z, loss, neg_marginal_likelihood, KL_divergence = vae.autoencoder(x_hat, x, dim_img, dim_z, n_hidden, keep_prob)\r\n  File \"G:\\PythonLib\\tensorflow-mnist-VAE-master\\vae.py\", line 82, in autoencoder\r\n    KL_divergence = 0.5 * tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(1e-8 + tf.square(sigma)) - 1, 1)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1345, in reduce_sum\r\n    name=name))\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 8389, in _sum\r\n    name=name)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\shica\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Incompatible shapes: [2] vs. [0]\r\n         [[node gradients/Sum_1_grad/floordiv (defined at run_main.py:228)  = FloorDiv[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients/Sum_1_grad/Shape, gradi\r\nents/Sum_1_grad/Maximum)]]\r\n         [[{{node Neg_1/_25}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_\r\ndevice_incarnation=1, tensor_name=\"edge_823_Neg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26418, "title": "How to install tensorflow2.0 in cuda9?", "body": "Any way to specific a cuda version with tensorflow2.0 alpha version? To  reinstall cuda is really risky, cause there are many other projects based on caffe which does not support an advanced cuda", "comments": ["@jinfagang Is there any version of Tensorflow already installed on your system?", "@jinfagang TF2.0 was built with CUDA10.0. But, I think you can use CUDA9.0 with TF2.0. You need to make sure that the CUDA and cuDNN are referenced correctly. Thanks!", "@jvishnuvardhan Sorry, I'm not the correct person to be assigned", "@jinfagang You need to build TF2.0 from source, then you can use CUDA 9.0 which you already have it. Please don't use binaries as they are built with CUDA10.0 and some of the dependencies will be looking for CUDA 10.0 paths. Please let us know how it progresses. Thanks! ", "@jinfagang Please check the resource [here](https://www.tensorflow.org/install/source). Thanks!", "@jviereck Thanks, I never built tensorflow successfully yet, most reason is that it downloads too many dependencies and some using google which blocked in China................ I meant if there any wheel just like pytorch does provide difference version built on cuda 9 with cudnn 7", "@jinfagang Do you have access to https://github.com/tensorflow/tensorflow/tags and https://pypi.org/project/tensorflow-gpu/? You could download from those locations. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26418\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26418\">No</a>\n", "I don't find the tf2.0 package built with cuda9.0 from https://github.com/tensorflow/tensorflow/tags and https://pypi.org/project/tensorflow-gpu/. And it's troublesome for me to install tf by building. Finally, I give up and adopt the shell from https://github.com/phohenecker/switch-cuda to switch cuda version as a temporary solution.", "@jvishnuvardhan you are right.  I have succeeded to build TF2.0 with CUDA9 and cudnn6.7  from source. Now it's working grateful. Thanks.", "> as a temporary solution.\r\n\r\n\r\n\r\n> I don't find the tf2.0 package built with cuda9.0 from https://github.com/tensorflow/tensorflow/tags and https://pypi.org/project/tensorflow-gpu/. And it's troublesome for me to install tf by building. Finally, I give up and adopt the shell from https://github.com/phohenecker/switch-cuda to switch cuda version as a temporary solution.\r\n\r\nTensorflow2.0 officially built with cuda10.0. There are community built binaries are there but not sure whether they have anything you are looking. Please [check some here](https://github.com/tensorflow/tensorflow/issues/28022#issuecomment-485357906). Thanks", "hi @SmileTM will you share the binary for us? thanks.", "> hi @SmileTM will you share the binary for us? thanks.\r\n\r\n@sudonto  \uff0chi\r\nThis binary may not be useful to you , because it was modified based on my configuration.\r\nif you need the binary, you must bazel the  binary  in your configuration.\r\nyou can refer [my method](https://s-tm.cn/2019/09/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91-tf2-gpu%E5%AE%89%E8%A3%85/).", "> > hi @SmileTM will you share the binary for us? thanks.\r\n> \r\n> @sudonto \uff0chi\r\n> This binary may not be useful to you , because it was modified based on my configuration.\r\n> if you need the binary, you must bazel the binary in your configuration.\r\n> you can refer [my method](https://s-tm.cn/2019/09/28/%E9%82%A3%E4%BA%9B%E5%B9%B4%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91-tf2-gpu%E5%AE%89%E8%A3%85/).\r\n\r\nhello,  I meet some probleam by your blog,  can  I add  your . qq .  or can  u share the tensorflow 2.0 which build by cuda9.0?", "> hi @SmileTM will you share the binary for us? thanks.\r\n\r\n@sudonto \r\nThe binary in the here. But i am not sure ,it works well in your machine.\r\nIf it works in your machine,  please reply me.\r\nhttps://github.com/SmileTM/Tensorflow2.0-GPU-CUDA9.0"]}, {"number": 26417, "title": "Missing warning or documentation for upcast in sparse_softmax_cross_entropy_with_logits?", "body": "In the \"sparse_softmax_cross_entropy_with_logits\" function inside tensorflow/python/ops/nn_ops.py, logits that are in fp16 are automatically upcasted to fp32. At the end of the function, the result is cast back down to fp16. \r\n\r\nThis functionality ought to be exposed in documentation and/or a warning. For users doing mixed precision training, the expected behavior would be that the entire loss (softmax + cross entropy) is done in fp16. \r\n\r\nRelevant lines:\r\n```\r\nlogits = ops.convert_to_tensor(logits)\r\nprecise_logits = math_ops.cast(logits, dtypes.float32) if (dtypes.as_dtype(\r\n\tlogits.dtype) == dtypes.float16) else logits\r\n...\r\ncost, _ = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\r\n\tprecise_logits, labels, name=name)\r\n```\r\n\r\nThanks so much.\r\n", "comments": ["@anshulsamar I know this is a stale issue. There were lot of improvements with `mixed_precision`.  Is this still an issue for you? Please check the guide on [`mixed_precision`](https://www.tensorflow.org/guide/mixed_precision). \r\n\r\nIf this is still an issue, can you please share a simple standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 26416, "title": "Installation issues with cuda10.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS  Linux Ubuntu 16.04:\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0\r\n- TensorFlow version:2.0.0-alpha0\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?:pip \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:Titan\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAfter the installation of tensorflow2.0 alpha package and running the python file\r\nwith \r\n*import tensorflow as tf*\r\ngetting below error :\r\n\r\n\"ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\"\r\n\r\n**Any other info / logs**\r\nFile \"/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/_api/v2/audio/__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/deep/tf2.0/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n", "comments": ["Please switch to cuda 10.0 and build again since the pre compiled TF 2.0 binaries support cuda 10. Thanks!", "@ymodak  Switching to cuda 10.0 resolved the issue. Didn't required a new tf 2.0 build.\r\nThanks."]}, {"number": 26415, "title": "Test for RepeatDataset", "body": "This PR adds the test for the kernel of `RepeatDataset`.\r\n\r\ncc @rachellim @jsimsa ", "comments": ["@jsimsa Thanks for your review! The comments are addressed in [this commit](https://github.com/tensorflow/tensorflow/pull/26415/commits/24ca9eaadf33d5f7b7a1ccaa5d1eb8e2396a02a5). Could you have a look when you have time?", "@jsimsa [This commit](https://github.com/tensorflow/tensorflow/pull/26415/commits/4ac1f45709d6c251ec3290cc62b67cc4773947e4) removes the internal member variables, and move the code in the helper class to each test to improve the code readability. Could you have a look at the changes? Any suggestions are very welcome."]}, {"number": 26414, "title": "ResNet50 in Eager mode. Model training set to True and False", "body": "I am using the Resnet50 code maintained under Eager Execution as is. I am using the following code for training: **_prediction = model(x, training=True)_**\r\nWhen I try to validate while training, i replace **training=True** with **training=False**. This however, results in a significant change in the loss value as if the model has not been trained at all. \r\nCan you please explain to me what is going on, and how to correct this issue?  ", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}]