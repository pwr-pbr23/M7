[{"number": 30241, "title": "[INTEL_MKL] Changed 4 unit tests because of the node label changes re\u2026", "body": "\u2026cently introduced by graph rewrite changes", "comments": []}, {"number": 30240, "title": "[INTEL MKL] MKL ML code cleanup -- mkl_util", "body": "Clean up MKL ML code from mkl_util.h and mkl_util_test.cc.   (Remove ~800 lines of unused code)\r\n\r\nMKL ML implementation is no longer supported.", "comments": []}, {"number": 30239, "title": "Add BatchMatMulV2 to auto_mixed_precision whitelist", "body": "(I don't suppose there's a way to get notified when ops get added to core?)\r\n\r\nattn. @reedwm \r\ncc. @nluehr ", "comments": ["> (I don't suppose there's a way to get notified when ops get added to core?)\r\n\r\nI don't think there's an easy way. You can periodically see if any new files are added to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/api_def/base_api, which perhaps can be automated."]}, {"number": 30238, "title": "[XLA:GPU][ROCm] Adding ROCm gpu_executable compatiblity", "body": "Summary of change: created `CheckCompatibilityWithServiceExecutableRunOptions` for compatibility check across `ROCm` and `Cuda` platform.\r\n\r\n@whchung ", "comments": ["@timshen91, would you mind to take this review? ", "+tatianashp\r\n\r\n@timshen91 a gentle ping", "@thomasjoerg a gentle ping", "Sorry for the excessive delay. I was out yesterday and on vacation before that. I guess that @thomasjoerg was only having a pass-by comment, so I'll review this later today.", "@jerryyin could you help work on the changes based on the discussion between @timshen91 and me? it seems I couldn\u2019t amend this PR otherwise CLA check bot would fail.", "@timshen91 Addressed review in the latest commit. Let me know if there's any other feedback."]}, {"number": 30237, "title": "[ROCm] Adding ROCm support for the fused_batch_norm op", "body": "This PR adds ROCm support for the fused_batch_norm op\r\n\r\nThe changes in this PR are trivial. please review and merge. thanks.\r\n\r\n---------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg\r\n", "comments": []}, {"number": 30236, "title": "1.14.1 cherry-pick request: Add contents of wrapped module __dict__ to DeprecationWrapper __dict__.", "body": "Cherrypick of https://github.com/tensorflow/tensorflow/commit/b789a3b37b59e6795f799645a6e8b1a6c70fc346\r\n\r\nAdd contents of wrapped module __dict__ to DeprecationWrapper __dict__.\r\nShould fix the following two issues:\r\nhttps://github.com/tensorflow/tensorflow/issues/30184,\r\nhttps://github.com/tensorflow/tensorflow/issues/30028.\r\n\r\nAlso, while at it, reducing maximum deprecation warnings to 1 per module to reduce amount of spam.", "comments": ["Looks like there are new failures for MacOS Contrib build. I will push what I think is a fix and rerun the builds.", "Build errors of the form `ImportError: cannot import name 'feature_column_v2'` are caused by estimator version mismatch in our builds.\r\n\r\nI changed Mac and Python3 PIP builds to correctly install tensorflow_estimator==1.14.0 and they passed (except MacOS Py2 which failed due to infra issue).\r\nI haven't fixed estimator version pulled in for build on Windows. So, it still fails with `feature_column_v2` error. However, since we know what causes that error, I think this cherrypick is now ok to submit."]}, {"number": 30235, "title": "`xla.compile` + `tf.function` lose information about compile-time constants", "body": "**System information**\r\n- TensorFlow version (use command below): 1.14\r\n\r\n**Describe the current behavior**\r\n\r\nIn Eager mode, when a function is compiled with `xla.compile`, any constant arguments are auto-cast to Tensors, but these Tensors don't appear to reflect the fact that they are constant. Certain ops, like `tf.range` are sensitive to constant inputs due to the need for static shapes.\r\n\r\nIn effect, the following code raises an error:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n@tf.function\r\ndef bad(count):\r\n  return tf.range(count)\r\n\r\nxla.compile(bad, (3,))\r\n```\r\n```\r\nInvalidArgumentError: Argument to function must be a compile-time constant, but unable to resolve argument value to a constant.\r\n\tThis error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\n\t [[{{node cluster}}]] [Op:__inference_xla_compile_wrapper_196]\r\n```\r\n\r\nWhereas this code works:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n@tf.function\r\ndef good():\r\n  return tf.range(3)\r\n\r\nxla.compile(good, ())\r\n```\r\n```\r\n[<tf.Tensor: id=157, shape=(3,), dtype=int32, numpy=array([0, 1, 2], dtype=int32)>]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe two snippets of code above should be equivalent, that is, they should both run successfully.\r\n", "comments": ["@sanjoy Can you please take a look? Thanks!", "CC @alextp @joker-eph \r\n\r\nI think this will be tricky to fix given what little I understand about `tf.function` and python decorators.  Moreover, I'm especially worried that this will lead to a slippery slope where users will expect `xla.compile(bad, (2+1,))`, `xla.compile(bad, (5-x+x,))` etc. to also work.\r\n\r\nI understand that the UX here is not ideal, but is this blocking you in some way?", "Hi There,\r\n\r\nWe are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "It seems stale - closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30235\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30235\">No</a>\n"]}, {"number": 30234, "title": "How to calculate gradients for meta learning loop?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF 2.0.0-dev20190628\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: \r\n\r\n\r\n**Describe the current behavior**\r\nI want to compute the gradients of a loss function with respect to a model (in order to do meta-learning) and use those gradients to define the same model with new weights. I get None as the values of the model when  I use Input layers.\r\n\r\n**Describe the expected behavior**\r\nI was expecting it to work the same whether I define my input layer as an Input layer or as tf.random.uniform() tensor.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, Input\r\n\r\n\r\nwith tf.GradientTape() as t:\r\n    model = tf.keras.models.Sequential(\r\n        (\r\n            Conv2D(filters=64, kernel_size=3, activation='relu'),\r\n            Conv2D(filters=64, kernel_size=3, activation='relu'),\r\n            Conv2D(filters=64, kernel_size=3, activation='relu'),\r\n            Conv2D(filters=64, kernel_size=3, activation='relu'),\r\n            Flatten(),\r\n            Dense(10, activation='softmax'),\r\n        )\r\n    )\r\n\r\n    train_inputs = Input(shape=(28, 28, 1))\r\n    train_labels = Input(shape=(10, ))\r\n\r\n    # train_inputs = tf.random.uniform(shape=(1, 28, 28, 1), dtype=tf.float32)\r\n    # train_labels = tf.random.uniform(shape=(1, 10), dtype=tf.float32)\r\n\r\n    train_outputs = model(train_inputs)\r\n    loss = tf.losses.categorical_crossentropy(train_labels, train_outputs)\r\n\r\nd_weights = t.gradient(loss, model.trainable_weights)\r\nprint(d_weights)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nIf I uncomment those two lines d_weights is calculated and printed. When they are commented I get this error:\r\n\r\n2019-06-28 14:27:30.352043: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-28 14:27:30.386438: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2019-06-28 14:27:30.387394: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f99b30 executing computations on platform Host. Devices:\r\n2019-06-28 14:27:30.387417: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/home/siavash/programming/meta-learning-framework/models.py\", line 27, in <module>\r\n    d_weights = t.gradient(loss, model.trainable_weights)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\", line 1001, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\", line 666, in _ones\r\n    return _fast_fill(value, shape, dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py\", line 621, in _fast_fill\r\n    constant_op.constant(shape, dtype=dtypes.int32),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\", line 246, in constant\r\n    allow_broadcast=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\", line 254, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\", line 115, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, handle, device, dtype)\r\nValueError: TypeError: object of type 'Tensor' has no len()", "comments": ["I am able to reproduce the issue on Colab with Tensorflow 2.0.0-dev20190628. Thanks!", "This is sadly not something we can feasibly fix. Keras is not compatible with meta learning because TF variable assignments, used during the training process, are not differentiable.\r\n\r\nTo do meta learning in TF use the lower level operations (no layers and no optimizers) and write your own training loop; this way you can make your training loop itself differentiable.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30234\">No</a>\n", "@alextp Thank you for your response.\r\nSo the gradient tape should be used when there is real data flowing in the model and it cannot be used when we just define models with keras since there is no computation happened. Is my understanding right?\r\nActually, I am a little confused about gradient tape and why it should not work in this example.", "The issue here is not about the gradient tape; it also happens with tf.gradients. The issue is around tf.variable, and how operations on tf.variables are not differentiable.\r\n\r\nSo for example in the following code snippet\r\n\r\n```python\r\nv  = tf.Variable(1.0)\r\nx = tf.convert_to_tensor(2.0)\r\nwith tf.GradientTape() as t:\r\n  t.watch(x)\r\n  v.assign_add(x)\r\n  loss = v * v\r\nassert t.gradient(loss, x) is None\r\n```\r\n\r\nand the gradient is None because TF doesn't track assignments to tf.Variables to their source operations, because state in variables is stored outside the graph. To get the gradient to be not None you need to replace the tf.Variable with a normal tensor.", "@alextp Thank you for clarifying this. I see what was the reason now."]}, {"number": 30233, "title": "Make saver.restore work with tf.keras.layers when using tf.estimator to save model checkpoints", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.14\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n How do I  convert a saved tensorflow estimator model to a Keras model? (either by using the check pointed weights or the saved model) It is possible to convert a compiled Keras model to an estimator instance but it would be great if there is a clean way to convert an estimator saved model back to a keras model without having to manually copy weights. \r\n\r\n**Will this change the current api? How?**\r\nNot sure\r\n\r\n**Who will benefit with this feature?**\r\nWill help folks using tf estimators to train and save model checkpoints and restore them directly when a tf.keras model is instantiated.\r\n\r\n**Any Other info.**\r\n", "comments": ["The tf.keras layers do not follow the same naming convention used in the estimator model checkpoint files. So, it becomes a mess to restore models. I am not sure if this is a legitimate issue or if I am missing something while restoring.", "Can you please elaborate about the issue & the context.Will it be possible to provide related code.Thanks!\r\n", "Have updated the form", "  There aren't any plans to update `tf.train.Saver`, given that Keras checkpoints use a somewhat different saving format (`tf.train.Checkpoint`). I've created a gist demonstrating how to save/load the weights in Estimator and Keras. The main change to the usual Estimator code is the use of `tf.train.Checkpoint`.\r\n\r\nhttps://gist.github.com/k-w-w/01c58789c2baf2a350c28e5a89566420", "@k-w-w Thanks, I cannot remember exactly now but I think I ran into issues when using a network using tf.keras.layers without using a keras model for training, I had just replaced tf.layers with tf.keras.layers and something broke when I tried to copy the weights into a Keras model(with the same structure as that of the original network) from the saved estimator checkpoint file, but I cannot precisely remember where.", "I believe there are some minor differences between tf.layers and tf.keras.layers which may affect the resulting checkpoint. If you can reproduce the steps, that would be extremely helpful. (and don't forget to re-open the bug)", "@k-w-w I just looked at the gist you made. I did not explicitly create tf.train.Checkpoint and pass it to the scaffold class like you did and since the estimator(I think) uses tf.train.Saver under the hood, it expects the variable names to be the same while restoring. One other difference I noticed(not directly related to this) is that tf.keras.layers will rename layers even if a name is provided to the layer if a layer with the same name exists within a session, and also it does not reuse layers even if a scope is provided.(I forgot about this back then)\r\nSo, since all this is expected behavior, I do not think the issue needs to be reopened. Thanks\r\n\r\n"]}, {"number": 30232, "title": "Tensorflow Lite conversion misshapes bias vector of FullyConnected", "body": "**System information**\r\n- Have I written custom code: tflite converter code is straight from an example script\r\n- OS Platform and Distribution: Mac OS 10.14.5\r\n- TensorFlow installed from: `pip install tf-nightly` and `pip install tensorflow`\r\n- TensorFlow version:\r\ntested on  `v1.12.1-5178-gbafa0371c8 1.15.0-dev20190628` and `v1.13.0-rc2-5-g6612da8951 1.13.1`\r\n- Python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\nTFLite converter incorrectly shapes the bias for FullyConnected operators.\r\nSpecifically in my test case (see attached model below), in the original freeze graph model, `MatMul_6` takes a product of 32x12 matrix and 12x1 vector then `add_7` adds a 32x1 vector to it as a bias. The converted TFLite model puts these two operations together into a FullyConnected op, and somehow its bias `MatMul_6_bias` is incorrectly shaped as a single-element vector. Consequently, the inference result of this TFLite model is incorrect.\r\n\r\n**Describe the expected behavior**\r\nThe bias vectors should be shaped as they were in the original freeze graph model.\r\n\r\n**Code to reproduce the issue**\r\n[tflite_bias_shape_issue.zip](https://github.com/tensorflow/tensorflow/files/3340234/tflite_bias_shape_issue.zip)\r\nThis zip file contains debug.pb (TF freeze graph model) and debug.tflite (TFLite converted model from frozen model). The conversion code is taken straight from the document:\r\n```\r\nimport tensorflow as tf\r\ngraph_def_file = \"debug.pb\"\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"Reshape_1\"]\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"debug.tflite\", \"wb\").write(tflite_model)\r\n```\r\nThe model has extra operators in the beginning (Sub, Div, Gather) just because I did not have time to rebuild the bare minimal test case but I think it is already simple enough.", "comments": ["[fc_op.zip](https://github.com/tensorflow/tensorflow/files/3340595/fc_op.zip)\r\nOK here is the bare minimal case. It's just MatMul then Add, which TFLite converts to FullyConnected with an incorrect bias shape.", "Any words on this issue? This seems like a grave issue without any workaround. I tried many things like adding extra ops or dimensions but the converter stubbornly insists on optimizing everything to a simple FullyConnected op with an incorrect, single-element bias.", "@dukecyto : I have checked into the fc_op model you shared. The output shape is (32, 1), is it not the expected one? Sorry if i misinterpreted your issue.", "@ANSHUMAN87 Please use a visualization tool such as Netron and have a look at both `fc_op.pb` and `fc_op.tflite`. In the FullyConnected op of `fc_op.tflite`, The shape of the \"bias\" input is chopped off to a single-element vector. I do not think this is a visualization problem because the network actually evaluates to an incorrect value.", "@dukecyto : Thanks for your detailed reply. I got your issue clearly now. I will work on it, and share solution for it. Please stay tuned. Thanks!", "@dukecyto : I have found the reason for the issue. I will upload a PR to fix it. In the mean time, i can give you work around for the issue.\r\nCurrently your model has operation like, Weight[32x12] X Input[12, 1], followed by BiasAdd(Add) [32x1].\r\n\r\nI would suggest to change the order as below, Input[1, 12] X Weight[12x32], followed by BiasAdd(Add)[1x32], and then you can add Transpose to fulfill your expected shape for next layer.\r\n\r\nI hope it will solve your blocking issue. Please let me know your results, Thanks!", "@ANSHUMAN87 This sound like great news! I while I wait for your PR to get merged, I will try your suggested workaround.", "@ANSHUMAN87 The suggested workaround is confirmed to work with my complete model, generating expected inference results. All extra transposes are optimized out in the final TFLite compute graph but is messy in the code and is a major pitfall in using TFLite so I still look forward to your PR getting merged.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30232\">No</a>\n", "@gargn er, isn't it more like Tensorflow team updates ME on the progress?", "@ANSHUMAN87 Do you have an update on your PR?", "@dukecyto, @gargn : i am extremely sorry, could not upload the PR yet, please give me some time(may be next week), will inform you about the progress. Thanks!", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30232\">No</a>\n"]}, {"number": 30231, "title": "[ROCm] Add ROCm support for pooling operators", "body": "This is a follow-up PR fo #29185 . Enable pooling operators on ROCm.\r\n\r\nmax pooling for qint8 is disabled on ROCm for now", "comments": []}, {"number": 30230, "title": "XLA - Batch Norm - Bessel's correction generates NaNs in FP16", "body": "For FP16 this means that the factor will be a NaN for sufficiently big inputs.", "comments": ["@rthadur Please see https://github.com/tensorflow/tensorflow/pull/30219#issuecomment-506764404.", "> @rthadur Please see [#30219 (comment)](https://github.com/tensorflow/tensorflow/pull/30219#issuecomment-506764404).\r\n\r\nThanks for letting me know."]}, {"number": 30229, "title": "ReorderAxes in TFLite", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): TF 2.0-beta1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2019-06-28 19:47:44.325579: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-06-28 19:47:44.342432: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 211 operators, 317 arrays (0 quantized)\r\n2019-06-28 19:47:44.345471: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 211 operators, 317 arrays (0 quantized)\r\n2019-06-28 19:47:44.354646: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 153 operators, 279 arrays (0 quantized)\r\n2019-06-28 19:47:44.358004: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 150 operators, 273 arrays (0 quantized)\r\n2019-06-28 19:47:44.361223: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 150 operators, 273 arrays (0 quantized)\r\n2019-06-28 19:47:44.363593: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 150 operators, 273 arrays (0 quantized)\r\n2019-06-28 19:47:44.367975: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 55296128 bytes, theoretical optimal value: 33177600 bytes.\r\n2019-06-28 19:47:44.369169: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. \r\nIf you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). \r\nHere is a list of builtin operators you are using: ABS, ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, DIV, EXPAND_DIMS, FULLY_CONNECTED, GREATER, LEAKY_RELU, LOGICAL_NOT, MAXIMUM, MEAN, MINIMUM, MUL, NEG, PACK, POW, RESHAPE, RESIZE_BILINEAR, SHAPE, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE. \r\nHere is a list of operators for which you will need custom implementations: ReorderAxes.\r\n```\r\n", "comments": ["Current workaround \u2014 use `tf.lite.OpsSet.SELECT_TF_OPS` for `converter.target_spec.supported_ops` like from [this](https://www.tensorflow.org/lite/guide/ops_select) page.\r\n\r\n\r\n", "I don't know why, but my problem with `ReorderAxes` disappeared. It happened when I had fixed my problems from [this](https://github.com/tensorflow/tensorflow/issues/30222) issue.  ", "@Oktai15 Looks like issue resolved. Can you please let us know if you are happy to close if no issue persists.  ", "@gadagashwini yes. Please, look at the issue that I mentioned above", "@gadagashwini finally, I reproduced the problem. It was with this line:\r\n`y = 10 * x[..., 1] - 5`. \r\n\r\nIt means that TF Lite converter can't work with `...`. Need to fix.\r\n\r\nCurrent workaround:\r\nReplace `...` with `:`'s, e.g `x[..., 1]` -> `x[:, :, :, 1]`.  \r\n\r\nI'll reopen issue.\r\n  ", "`ReorderAxes` is supported indirectly in TensorFlow Lite by being converted to other operations. Usually the error message is indicative of another underlying issue. Can you please provide a minimally reproducible example to produce the ReorderAxes unsupported message?", "@gargn any case with `...`", "@Oktai15 Is this still an issue? If this is still an issue, try `TF2.0` and let us know whether it was resolved or not. Please close the issue if it was resolved by `TF2.0`. Otherwise, as mentioned by @gargn \r\n> Can you please provide a minimally reproducible example to produce the ReorderAxes unsupported message?\r\n\r\nThanks!\r\n", "@Oktai15\r\nplease update as per the above comment", "@Oktai15\r\nplease update as per the above comment", "Thanks!"]}, {"number": 30228, "title": "[ROCm] Adding ROCm support for the relu op", "body": "This PR adds ROCm support for the relu op\r\n\r\nThe changes in this PR are trivial. please review and merge. thanks.\r\n\r\n-------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg", "comments": ["@rthadur \r\n\r\nthis is the 4th of 4 PRs that seem to be stuck in the merge pipeline. \r\nPlease let me know if there is anything that needs to be done on my end.\r\n\r\nthanks\r\n\r\ndeven", "@deven-amd thank you for your patience waiting for @chsigg review in order to pull this in.", "@chsigg  gentle ping.", "@chsigg Can you please review this PR?"]}, {"number": 30227, "title": "tf.keras.layers.RNN calls the cell using the first timestep of the timeseries twice", "body": "Trying to develop an internal attention module for Lummetry.AI deep learning team, I experienced a possible bug in working with tf.keras.layers.RNN. The problem comes from calling the first timestep of the given timeseries twice. I'm using as a RNN cell, for debugging scopes, the class defined in the example given at https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.enable_eager_execution()\r\n\r\nclass MinimalRNNCell(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, units, **kwargs):\r\n        self.units = units\r\n        self.state_size = units\r\n        super(MinimalRNNCell, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\r\n                                      initializer='uniform',\r\n                                      name='kernel')\r\n        self.recurrent_kernel = self.add_weight(\r\n            shape=(self.units, self.units),\r\n            initializer='uniform',\r\n            name='recurrent_kernel')\r\n        self.built = True\r\n\r\n    def call(self, inputs, states):\r\n        prev_output = states[0]\r\n        h = K.dot(inputs, self.kernel)\r\n        output = h + K.dot(prev_output, self.recurrent_kernel)\r\n        print(\"Call {}\".format(inputs)) \r\n        return output, [output]\r\n\r\nif __name__ == \"__main__\":\r\n    inp = np.array([2, 3, 4, 1, 2, 3, 0, 1]).reshape(2,2,2).astype(np.float32)\r\n    minimal_cell = MinimalRNNCell(32)\r\n    rnn_lyr = tf.keras.layers.RNN(minimal_cell, return_sequences=True)\r\n    rnn_lyr(tf.Variable(inp)) \r\n    # This call outputs: \r\n    # Call: [[2. 3.]\r\n    #   [2. 3.]]\r\n    # Call: [[2. 3.]\r\n    #   [2. 3.]]\r\n    # Call: [[4. 1.]\r\n    #   [0. 1.]]\r\n```\r\n\r\nCan someone explain if there is something that I am missing, or if it really is a bug?\r\n\r\nThanks!\r\n", "comments": ["Searching deeply, I found that the first timestep is also used to determine the cell output shape and its dtype."]}, {"number": 30226, "title": "tensorflow:libtensorflow_cc.so fails to build for --cpu=armeabi-v7a", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04.3\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: current master branch(28.06.2019)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.24.1\r\n- **GCC/Compiler version (if compiling from source)**:7.4.0 \r\n- **CUDA/cuDNN version**:Not Used\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nbazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a --verbose_failures --cxxopt=\"-DTENSROFLOW_DISABLE_META\" --jobs 8 --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --copt=-DNO_LOCAL_MEM --copt=-DEIGEN_DONT_VECTORIZE_SYCL --copt=-DMDB_USE_ROBUST=0 --cxxopt=\"-fPIC\" --cxxopt=\"-std=c++11\"\r\n\r\n\r\n### Describe the problem\r\nTrying to compile //tensorflow:libtensorflow_cc.so for armeabi-v7a and/or for arm64-v8a, both failing with the output:\r\n**ERROR: ../tensorflow/tensorflow/cc/saved_model/BUILD:40:1: no such target '//tensorflow/core:saved_model_portable_proto': target 'saved_model_portable_proto' not declared in package 'tensorflow/core' defined by ../tensorflow/tensorflow/core/BUILD and referenced by '//tensorflow/cc/saved_model:reader**\r\n", "comments": ["TensorFlow (Mobile) for Android is deprecated. Is there a reason you cannot use TensorFlow Lite?", "@jdduke thanks for replying.\r\nI am fairly new to Tensorflow, and I am using it with C++ API, and since the tensorflow has more ops, I need to use it instead of the Lite. Also the examples for the C++ Api for Tensorflow Lite are extremely poor", "@jdduke do we have any news in this !!! ", "You can use certain whitelisted TensorFlow operators from TensorFlow Lite using [this approach](https://www.tensorflow.org/lite/guide/ops_select). The [label_image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc#L157) example shows how to use the C++ API.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30226\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30226\">No</a>\n"]}, {"number": 30225, "title": "[ROCm] Adding ROCm support for the transpose op", "body": "This PR adds ROCm support for the transpose op\r\n\r\nThe changes in this PR are trivial. please review and merge. thanks.\r\n\r\n-------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg ", "comments": []}, {"number": 30224, "title": "May need more packages in Anaconda 2019.3 in win 10 1903", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 64-bit 1903**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): `.whl`\r\n- TensorFlow version: **1.14.0**\r\n- Python version: **3.7.3 (from Anaconda 2019.3)**\r\n- Installed using virtualenv? pip? conda?: **pip**\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no GPU (Intel\u00ae UHD Graphics 630)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI need to install tensorflow in an offline environment. So I installed Anaconda 2019.3. And downloaded tensorflow .whl file (`tensorflow-1.14.0-cp37-cp37m-win_amd64.whl`). As the `tensorflow/tools/pip_package/setup.py ` file said, other packages were needed. Here I quote,\r\n```\r\nREQUIRED_PACKAGES = [\r\n    'absl-py >= 0.7.0',\r\n    'astor >= 0.6.0',\r\n    'gast >= 0.2.0',\r\n    'google_pasta >= 0.1.6',\r\n    'keras_applications >= 1.0.8',\r\n    'keras_preprocessing >= 1.0.5',\r\n    'numpy >= 1.14.5, < 2.0',\r\n    'opt_einsum >= 2.3.2',\r\n    'six >= 1.10.0',\r\n    'protobuf >= 3.6.1',\r\n    'tensorboard >= 1.14.0, < 1.15.0',\r\n    'tensorflow_estimator >= 1.14.0rc0, < 1.15.0rc0',\r\n    'termcolor >= 1.1.0',\r\n    'wrapt >= 1.11.1',\r\n]\r\n```\r\nBut actually, this list was not enough.\r\n`markdown >= 2.6.8` was required by `tensorboard == 1.14.0`. \r\n`grpcio >= 1.8.6` was required by `tensorflow == 1.14.0`.\r\nAnd, `setuptools >= 41.0.0` was required by `tensorflow == 1.14.0`, since `setuptools == 40.8.0` in Anaconda 2019.3.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you please help us with error log for faster resolution.Thanks!", "Not sure about the error log. Perhaps a windows sandbox can reproduce the process.\r\nThe `pip install <pkg.whl>` command gave the error messages that other packages were required. I downloaded and installed the required packages, then tensorflow and tensorboard could be installed.\r\n\r\nTo make the issue short, the `REQUIRED_PACKAGES` listed might not be enough. `markdown >= 2.6.8`, `grpcio >= 1.8.6` are required.", "Just to verify did you get chance to follow instructions from [Tensorflow Website](https://www.tensorflow.org/install/pip).Thanks!", "Here's my installation steps\r\n1. Install Anaconda 2019.3(python 3). Download all the packages from Tensorflow website and https://pypi.org/\r\n2. `pip install <pkg.whl>` or `cd <pkg>; python setup.py install`  depending on whether the package has wheel installer or not.\r\n3. `pip install tensorflow-1.12.0-cp36-cp36m-win_amd64.whl`", "And, after the latest versions of `markdown`, `grpcio` and `setuptools` installed, tensorflow was installed successfully in my computer. I have ran a simple case using tensorflow.", "@swun90 Can i close this issue as your issue was resolved.Thanks!"]}, {"number": 30223, "title": "training code utilize more cpu memory than gpu memory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (1.13.1):\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10\r\n- GPU model and memory:Nvidia RTx 2080\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am using tensorflow 1.13.1 with GPU RTx 2080. I am using reinforcement learning to train the robot in a gazebo platform using ROS. The issue is that it is running in CPU memory rather than GPU memory. It takes a lot of time to train a robot. I attached a screenshot of the nvidia smi which shows that it utilize 1.7Gb only of GPU while use 16Gb of RAM.\r\n**Describe the expected behavior**\r\n![Screenshot 2019-06-28 20:53:03](https://user-images.githubusercontent.com/45818401/60343521-c188b300-99e6-11e9-8715-2b511e40b023.png)\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@mudassirej Will it be possible us to provide the minimal code snippet which reproduces the reported issue here. Thanks!", "@gadagashwini thank you for your consideration. Here is the snippet\r\nfrom __future__ import print_function\r\nfrom GazeboWorld import GazeboWorld\r\n\r\nimport tensorflow as tf\r\nimport random\r\nimport numpy as np\r\nimport time\r\nimport rospy\r\n\r\nfrom collections import deque\r\n\r\nGAME = 'GazeboWorld'\r\nACTIONS = 7 # number of valid actions\r\nSPEED = 2 # DoF of speed\r\nGAMMA = 0.99 # decay rate of past observations\r\nOBSERVE = 10. # timesteps to observe before training\r\nEXPLORE = 20000. # frames over which to anneal epsilon\r\nFINAL_EPSILON = 0.0001 # final value of epsilon\r\nINITIAL_EPSILON = 0.1 # starting value of epsilon\r\nREPLAY_MEMORY = 10000# number of previous transitions to remember\r\nBATCH = 1 # size of minibatch\r\nMAX_EPISODE = 20000\r\nMAX_T = 200\r\nDEPTH_IMAGE_WIDTH = 160\r\nDEPTH_IMAGE_HEIGHT = 128\r\nRGB_IMAGE_HEIGHT = 228\r\nRGB_IMAGE_WIDTH = 304\r\nCHANNEL = 3\r\nTAU = 0.001 # Rate to update target network toward primary network\r\nH_SIZE = 8*10*64\r\nIMAGE_HIST = 4\r\n\r\n\r\ndef variable_summaries(var):\r\n\t\"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\r\n\twith tf.name_scope('summaries'):\r\n\t\tmean = tf.reduce_mean(var)\r\n\ttf.summary.scalar('mean', mean)\r\n\twith tf.name_scope('stddev'):\r\n\t\tstddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\r\n\ttf.summary.scalar('stddev', stddev)\r\n\ttf.summary.scalar('max', tf.reduce_max(var))\r\n\ttf.summary.scalar('min', tf.reduce_min(var))\r\n\ttf.summary.histogram('histogram', var)\r\n\t\t\r\ndef weight_variable(shape):\r\n\tinitial = tf.truncated_normal(shape, stddev = 0.01)\r\n\treturn tf.Variable(initial, name=\"weights\")\r\n\r\ndef bias_variable(shape):\r\n\tinitial = tf.constant(0., shape = shape)\r\n\treturn tf.Variable(initial, name=\"bias\")\r\n\r\ndef conv2d(x, W, stride_h, stride_w):\r\n\treturn tf.nn.conv2d(x, W, strides = [1, stride_h, stride_w, 1], padding = \"SAME\")\r\n\r\n\r\nclass QNetwork(object):\r\n\t\"\"\"docstring for ClassName\"\"\"\r\n\tdef __init__(self, sess):\r\n\t\t# network weights\r\n\t\t# input 128x160x1\r\n\t\twith tf.name_scope(\"Conv1\"):\r\n\t\t\tW_conv1 = weight_variable([10, 14, IMAGE_HIST, 32])\r\n\t\t\tvariable_summaries(W_conv1)\r\n\t\t\tb_conv1 = bias_variable([32])\r\n\t\t# 16x20x32\r\n\t\twith tf.name_scope(\"Conv2\"):\r\n\t\t\tW_conv2 = weight_variable([4, 4, 32, 64])\r\n\t\t\tvariable_summaries(W_conv2)\r\n\t\t\tb_conv2 = bias_variable([64])\r\n\t\t# 8x10x64\r\n\t\twith tf.name_scope(\"Conv3\"):\r\n\t\t\tW_conv3 = weight_variable([3, 3, 64, 64])\r\n\t\t\tvariable_summaries(W_conv3)\r\n\t\t\tb_conv3 = bias_variable([64])\r\n\t\t# 8x10x64\r\n\t\twith tf.name_scope(\"FCValue\"):\r\n\t\t\tW_value = weight_variable([H_SIZE, 512])\r\n\t\t\tvariable_summaries(W_value)\r\n\t\t\tb_value = bias_variable([512])\r\n\t\t\t# variable_summaries(b_ob_value)\r\n\r\n\t\twith tf.name_scope(\"FCAdv\"):\r\n\t\t\tW_adv = weight_variable([H_SIZE, 512])\r\n\t\t\tvariable_summaries(W_adv)\r\n\t\t\tb_adv = bias_variable([512])\r\n\t\t\t# variable_summaries(b_adv)\r\n\r\n\t\twith tf.name_scope(\"FCValueOut\"):\r\n\t\t\tW_value_out = weight_variable([512, 1])\r\n\t\t\tvariable_summaries(W_value_out)\r\n\t\t\tb_value_out = bias_variable([1])\r\n\t\t\t# variable_summaries(b_ob_value_out)\r\n\r\n\t\twith tf.name_scope(\"FCAdvOut\"):\r\n\t\t\tW_adv_out = weight_variable([512, ACTIONS])\r\n\t\t\tvariable_summaries(W_adv_out)\r\n\t\t\tb_adv_out = bias_variable([ACTIONS])\r\n\t\t\t# variable_summaries(b_ob_adv_out)\t\r\n\r\n\t\t# input layer\r\n\t\tself.state = tf.placeholder(\"float\", [None, DEPTH_IMAGE_HEIGHT, DEPTH_IMAGE_WIDTH, IMAGE_HIST])\r\n\t\t#print(\"State:\", self.state.shape)\r\n\t\t# Conv1 layer\r\n\t\th_conv1 = tf.nn.relu(conv2d(self.state, W_conv1, 8, 8) + b_conv1)\r\n\t\t#print(\"Hidden:\", h_conv1)\r\n\t\t# Conv2 layer\r\n\t\th_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2, 2) + b_conv2)\r\n\t\t# Conv2 layer\r\n\t\th_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1, 1) + b_conv3)\r\n\t\th_conv3_flat = tf.reshape(h_conv3, [-1, H_SIZE])\r\n\r\n\t\t# FC ob value layer\r\n\t\th_fc_value = tf.nn.relu(tf.matmul(h_conv3_flat, W_value) + b_value)\r\n\t\tvalue = tf.matmul(h_fc_value, W_value_out) + b_value_out\r\n\r\n\t\t# FC ob adv layer\r\n\t\th_fc_adv = tf.nn.relu(tf.matmul(h_conv3_flat, W_adv) + b_adv)\t\t\r\n\t\tadvantage = tf.matmul(h_fc_adv, W_adv_out) + b_adv_out\r\n\t\t\r\n\t\t# Q = value + (adv - advAvg)\r\n\t\tadvAvg = tf.expand_dims(tf.reduce_mean(advantage, axis=1), axis=1)\r\n\t\tadvIdentifiable = tf.subtract(advantage, advAvg)\r\n\t\tself.readout = tf.add(value, advIdentifiable)\r\n\t\t#print(\"readout:\", self.readout.shape)\r\n\r\n\t\t# define the ob cost function\r\n\t\tself.a = tf.placeholder(\"float\", [None, ACTIONS])\r\n\t\tself.y = tf.placeholder(\"float\", [None])\r\n\t\tself.readout_action = tf.reduce_sum(tf.multiply(self.readout, self.a), axis=1)\r\n\t\tself.td_error = tf.square(self.y - self.readout_action)\r\n\t\tself.cost = tf.reduce_mean(self.td_error)\r\n\t\tself.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.cost)\r\n\r\ndef updateTargetGraph(tfVars,tau):\r\n\ttotal_vars = len(tfVars)\r\n\top_holder = []\r\n\tfor idx,var in enumerate(tfVars[0:total_vars/2]):\r\n\t\top_holder.append(tfVars[idx+total_vars/2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars/2].value())))\r\n\treturn op_holder\r\n\r\ndef updateTarget(op_holder,sess):\r\n\tfor op in op_holder:\r\n\t\tsess.run(op)\r\n\r\ndef trainNetwork():\r\n\tsess = tf.InteractiveSession()\r\n\twith tf.name_scope(\"OnlineNetwork\"):\r\n\t\tonline_net = QNetwork(sess)\r\n\twith tf.name_scope(\"TargetNetwork\"):\r\n\t\ttarget_net = QNetwork(sess)\r\n\trospy.sleep(1.)\r\n\r\n\treward_var = tf.Variable(0., trainable=False)\r\n\treward_epi = tf.summary.scalar('reward', reward_var)\r\n\t# define summary\r\n\tmerged_summary = tf.summary.merge_all()\r\n\tsummary_writer = tf.summary.FileWriter('./logs1', sess.graph)\r\n\r\n\t# Initialize the World\r\n\tenv = GazeboWorld()\r\n\tprint('Environment initialized')\r\n\r\n\t# Initialize the buffer\r\n\tD = deque()\r\n\r\n\t# get the first state \r\n\t#depth_img_t1 = env.GetDepthImageObservation()\r\n\t#print(\"input image dimensrion:\", depth_img_t1.shape)\r\n\t#depth_imgs_t1 = np.stack((depth_img_t1, depth_img_t1, depth_img_t1, depth_img_t1), axis=2)\r\n\t#print(\"depth images t1:\", depth_imgs_t1.shape)\r\n\t#terminal = False\r\n\t\r\n\t# saving and loading networks\r\n\ttrainables = tf.trainable_variables()\r\n\ttrainable_saver = tf.train.Saver(trainables)\r\n\tsess.run(tf.global_variables_initializer())\r\n\tcheckpoint = tf.train.get_checkpoint_state('saved_networks/Q')\r\n\tprint('checkpoint:', checkpoint)\r\n\tif checkpoint and checkpoint.model_checkpoint_path:\r\n\t\ttrainable_saver.restore(sess, checkpoint.model_checkpoint_path)\r\n\t\tprint(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\r\n\r\n\telse:\r\n\t\tprint(\"Could not find old network weights\")\r\n\t\t\r\n\t# start training\r\n\tepisode = 0\r\n\tepsilon = INITIAL_EPSILON\r\n\tr_epi = 0.\r\n\tt = 0\r\n\tT = 0\r\n\trate = rospy.Rate(5)\r\n\tprint('Number of trainable variables:', len(trainables))\r\n\ttargetOps = updateTargetGraph(trainables,TAU)\r\n\tloop_time = time.time()\r\n\tlast_loop_time = loop_time\r\n\twhile episode < MAX_EPISODE and not rospy.is_shutdown():\r\n\t\tenv.ResetWorld()\r\n\t\tt = 0\r\n\t\tr_epi = 0.\r\n\t\tterminal = False\r\n\t\treset = False\r\n\t\tloop_time_buf = []\r\n\t\taction_index = 0\r\n\t\twhile not reset and not rospy.is_shutdown():\r\n\t\t\tdepth_img_t1 = env.GetDepthImageObservation()\r\n\t\t\tdepth_imgs_t1 = np.stack((depth_img_t1, depth_img_t1, depth_img_t1, depth_img_t1), axis=2)\r\n\t\t\tprint(\"input image dimensrion1:\", depth_img_t1.shape)\r\n\t\t\tdepth_img_t1 = np.reshape(depth_img_t1, (DEPTH_IMAGE_HEIGHT, DEPTH_IMAGE_WIDTH, 1))\r\n\t\t\tprint(\"input image dimensrion after reshape:\", depth_img_t1.shape)\r\n\t\t\tdepth_imgs_t1 = np.append(depth_img_t1, depth_imgs_t1[:, :, :(IMAGE_HIST - 1)], axis=2)\r\n\t\t\tprint(\"input image dimensrion after append1:\",depth_imgs_t1.shape)\r\n\t\t\treward, terminal, reset = env.GetRewardAndTerminate(t)\r\n\t\t\tv_t, theta_t = env.GetSelfOdomeSpeed(t)\r\n\t\t\tif t > 0 :\r\n\t\t\t\tD.append((depth_imgs_t, a_t, reward, depth_imgs_t1, terminal))\r\n\t\t\t\tif len(D) > REPLAY_MEMORY:\r\n\t\t\t\t\tD.popleft()\r\n\t\t\tdepth_imgs_t = depth_imgs_t1\r\n\r\n\t\t\t# choose an action epsilon greedily\r\n\t\t\ta = sess.run(online_net.readout, feed_dict = {online_net.state : [depth_imgs_t1]})\r\n\t\t\tprint(\"action:\", a.shape)\r\n\t\t\treadout_t = a[0]\r\n\t\t\ta_t = np.zeros([ACTIONS])\r\n\t\t\tif episode <= OBSERVE:\r\n\t\t\t\taction_index = random.randrange(ACTIONS)\r\n\t\t\t\ta_t[action_index] = 1\r\n\t\t\telse:\r\n\t\t\t\tif random.random() <= epsilon:\r\n\t\t\t\t\tprint(\"----------Random Action----------\")\r\n\t\t\t\t\taction_index = random.randrange(ACTIONS)\r\n\t\t\t\t\ta_t[action_index] = 1\r\n\t\t\t\telse:\r\n\t\t\t\t\taction_index = np.argmax(readout_t)\r\n\t\t\t\t\ta_t[action_index] = 1\r\n\t\t\t# Control the agent\r\n\t\t\tenv.Control(action_index)\r\n\r\n\t\t\tif episode > OBSERVE :\r\n\t\t\t\t# # sample a minibatch to train on\r\n\t\t\t\tminibatch = random.sample(D, BATCH)\r\n\t\t\t\ty_batch = []\r\n\t\t\t\t# get the batch variables\r\n\t\t\t\tdepth_imgs_t_batch = [d[0] for d in minibatch]\r\n\t\t\t\ta_batch = [d[1] for d in minibatch]\r\n\t\t\t\tr_batch = [d[2] for d in minibatch]\r\n\t\t\t\tdepth_imgs_t1_batch = [d[3] for d in minibatch]\r\n\t\t\t\tQ1 = online_net.readout.eval(feed_dict = {online_net.state : depth_imgs_t1_batch})\r\n\t\t\t\tQ2 = target_net.readout.eval(feed_dict = {target_net.state : depth_imgs_t1_batch})\r\n\t\t\t\tfor i in range(0, len(minibatch)):\r\n\t\t\t\t\tterminal_batch = minibatch[i][4]\r\n\t\t\t\t\t# if terminal, only equals reward\r\n\t\t\t\t\tif terminal_batch:\r\n\t\t\t\t\t\ty_batch.append(r_batch[i])\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\ty_batch.append(r_batch[i] + GAMMA * Q2[i, np.argmax(Q1[i])])\r\n\r\n\t\t\t\t#Update the network with our target values.\r\n\t\t\t\tonline_net.train_step.run(feed_dict={online_net.y : y_batch,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\tonline_net.a : a_batch,\r\n\t\t\t\t\t\t\t\t\t\t\t\t\tonline_net.state : depth_imgs_t_batch })\r\n\t\t\t\tupdateTarget(targetOps, sess) # Set the target network to be equal to the primary network.\r\n\r\n\t\t\tr_epi = r_epi + reward\r\n\t\t\tt += 1\r\n\t\t\tT += 1\r\n\t\t\tlast_loop_time = loop_time\r\n\t\t\tloop_time = time.time()\r\n\t\t\tloop_time_buf.append(loop_time - last_loop_time)\r\n\t\t\trate.sleep()\r\n\r\n\t\t\t# scale down epsilon\r\n\t\t\tif epsilon > FINAL_EPSILON and episode > OBSERVE:\r\n\t\t\t\tepsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\r\n\r\n\t\t#  write summaries\r\n\t\tif episode > OBSERVE:\r\n\t\t\tsummary_str = sess.run(merged_summary, feed_dict={reward_var: r_epi})\r\n\t\t\tsummary_writer.add_summary(summary_str, episode - OBSERVE)\r\n\r\n\t\t# save progress every 500 episodes\r\n\t\tif (episode+1) % 100 == 0 :\r\n\t\t\ttrainable_saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = episode)\r\n\r\n\t\tif len(loop_time_buf) == 0:\r\n\t\t\tprint(\"EPISODE\", episode, \"/ REWARD\", r_epi, \"/ steps \", T, v_t, reward_t)\r\n\t\telse:\r\n\t\t\tprint(\"EPISODE\", episode, \"/ REWARD\", r_epi, \"/ steps \", T,\r\n\t\t\t\t\"/ LoopTime:\", np.mean(loop_time_buf),\"/ velocity \", v_t, \"/ angle \", theta_t, \"/ reward current \", reward)\r\n\r\n\t\tepisode = episode + 1\t\r\n\r\ndef main():\r\n\ttrainNetwork()\r\n\r\nif __name__ == \"__main__\":\r\n\tmain()\r\n", "Apologies for the delay in response. It is difficult to debug lengthy scripts like these. Perhaps you can provide a minimal code snippet from your whole script which reflects the issue reported. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30223\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30223\">No</a>\n", "sorry for the late response. I am attaching the snippet of the code and also the status of my gpu and system memory. I need a solution to this problem as I am not able to train my model.\r\n![Screenshot 2019-07-31 16:51:16](https://user-images.githubusercontent.com/45818401/62258113-74f82380-b43b-11e9-9093-8b0e6e55203e.png)\r\n![Screenshot 2019-08-01 09:03:16](https://user-images.githubusercontent.com/45818401/62258114-7590ba00-b43b-11e9-9765-a7bc30631318.png)\r\n![Screenshot 2019-08-01 09:03:34](https://user-images.githubusercontent.com/45818401/62258115-7590ba00-b43b-11e9-8ca1-382e366f421b.png)\r\n![Screenshot 2019-08-01 09:03:43](https://user-images.githubusercontent.com/45818401/62258116-7590ba00-b43b-11e9-8b8f-029a8ef730a6.png)\r\n\r\n\r\n\r\n"]}, {"number": 30222, "title": "SelectV2 and Reciprocal in TFLite", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): TF 2.0-beta1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2019-06-28 15:22:19.759933: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: SelectV2\r\n2019-06-28 15:22:19.772940: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal\r\n2019-06-28 15:22:19.772994: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: SelectV2\r\n2019-06-28 15:22:19.774404: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 138 operators, 202 arrays (0 quantized)\r\n2019-06-28 15:22:19.776339: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 138 operators, 202 arrays (0 quantized)\r\n2019-06-28 15:22:19.782519: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 94 operators, 172 arrays (0 quantized)\r\n2019-06-28 15:22:19.784430: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 93 operators, 170 arrays (0 quantized)\r\n2019-06-28 15:22:19.786303: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 93 operators, 170 arrays (0 quantized)\r\n2019-06-28 15:22:19.787673: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 93 operators, 170 arrays (0 quantized)\r\n2019-06-28 15:22:19.790494: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 33226752 bytes, theoretical optimal value: 22118464 bytes.\r\n2019-06-28 15:22:19.791230: W tensorflow/lite/toco/tflite/operator.cc:2654] Op SelectV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-06-28 15:22:19.791295: W tensorflow/lite/toco/tflite/operator.cc:2654] Op SelectV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-06-28 15:22:19.791385: W tensorflow/lite/toco/tflite/operator.cc:2654] Op SelectV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-06-28 15:22:19.791436: W tensorflow/lite/toco/tflite/operator.cc:2654] Op SelectV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-06-28 15:22:19.791510: E tensorflow/lite/toco/toco_tooling.cc:462] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. \r\nIf you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). \r\nHere is a list of builtin operators you are using: ABS, ADD, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, FULLY_CONNECTED, GREATER, LEAKY_RELU, MAXIMUM, MEAN, MINIMUM, MUL, NEG, POW, RESHAPE, RESIZE_BILINEAR, SUB, TANH. \r\nHere is a list of operators for which you will need custom implementations: SelectV2.\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nAt the beginning, I tried to convert without this \"magic\" line:\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\nAnd I got almost the same error log, BUT it was additional operation that couldn't be converted. I attached the end of that log:\r\n\r\n```\r\nHere is a list of operators for which you will need custom implementations: Reciprocal, SelectV2.\r\n```\r\n\r\nAs far as understood correctly, `tf.where` doesn't work and it is related to SelectV2, because when I remove line with `tf.where(x > 0, left, right)` everthing is okay. It's strange, because TFLite docs said that `tf.where` should work.\r\n\r\nAlso, I figured out that `Reciprocal` is linked to `1.0 / x` expression. ", "comments": ["Current workaround for `tf.where`:\r\n\r\n```\r\ndef where(bools, first, second):\r\n    first *= tf.cast(bools, tf.float32)\r\n    second *= tf.cast(tf.logical_not(bools), tf.float32)\r\n    return first + second\r\n```\r\n\r\nCurrent workaround for `Reciprocal`:\r\nFind every `1.0 / x` expression and replace it. In my case, I had `(x - y) * (1.0 - (1.0 / (a - b))` and I replace it with `(x  - y) * (a - b - 1.0) / (a - b)`.\r\nI suppose error happened because TF is looking for `1.0 / x` and optimize it in some another way rather than simple `x / y`. Need to fix.", "Hi @Oktai15, we're investigating direct converter support for reciprocal, stay tuned for updates.", "Hello @Oktai15, now the direct support for reciprocal is ready. FYI, https://github.com/tensorflow/tensorflow/commit/df41bbafb50e1ea99404257cf7fa6f60bd063b5f", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30222\">No</a>\n", "SelectV2 op is now available in Tensorflow Lite."]}, {"number": 30221, "title": "Create an internal undeprecated function to check checkpoint exists.", "body": "Related issue [#30119](https://github.com/tensorflow/tensorflow/issues/30119l) for this MR.", "comments": ["Could you please tell me what are the required changes?", "@abdullahselek Can you please check build failures? Thanks!", "I hope should be fine now.", "@abdullahselek Can you please check ubuntu sanity errors? Thanks!", "Sanity errors should have gone with last changes.", "@allenlavoie or @gbaned could you please check my last changes?", "Rebased from original repo and repushed, please check @allenlavoie.", "@allenlavoie I guess failing builds are not related to my changes, what is your suggestion?", "No worries then, it'll get pulled."]}, {"number": 30220, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The following commit broken the `--config=rocm` build when it was merged\r\nhttps://github.com/tensorflow/tensorflow/commit/b7d4805838a64e6a33135b19281345032c400f56#diff-f9ae051d576c85eab133e77af84c1937\r\n\r\nThe \"CheckRedzones\" routine (in file conv_ops.cc) has CUDA specific code, and hence results in a compile error when building TF with ROCm support. That routine was added while the above commit/PR (which enables ROCm support for convolution ops) was being reviewed, and the combination of the two changes leads to the compile error.\r\n\r\nThe \"fix\" is to make the \"CheckRedzone\" routine visible only in the TF build with CUDA support.\r\n\r\n--------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg ", "comments": ["looks like another commit already applied the same fix as this PR\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/b8cb81d24a0fc0d50b1fe2ed8780c4e8b037e506\r\n\r\nm gonna go ahead and close out this PR, as it is now redundant.\r\n"]}, {"number": 30219, "title": "[XLA] Add required header", "body": "For me, the master branch fails to build with this header not explicitly included in the source file.  I'm not sure where the transitive inclusion occurs for the main CI system.  Perhaps it is better to avoid relying on transitive inclusions anyway.\r\n\r\n", "comments": ["@gbaned I'm leaving the project at the end of July, so we need to stop tagging me as the point of contact for all XLA PRs/bugs.  Ideally we'd have an \"XLA CPU/GPU group\" in github, I dunno if that's something you could help us set up.  Otherwise I've updated go/tf-who-do-i-notify with the new point of contact.", "sure thing.  not a problem\r\n", "done", "thanks.  as you can probably guess, i'm trying to get our repo more in line with the public one, so we can transition more easily to making it actually public.\r\n", "> @gbaned I'm leaving the project at the end of July, so we need to stop tagging me as the point of contact for all XLA PRs/bugs. Ideally we'd have an \"XLA CPU/GPU group\" in github, I dunno if that's something you could help us set up. Otherwise I've updated go/tf-who-do-i-notify with the new point of contact.\r\n\r\n@jlebar Thank you for the update. Sure, we will assign XLA PRs/bugs to new point of contact as per go/tf-who-do-i-notify", "H @gbaned I do not think that the MacOS contrib could be failing due to this change.", "Don't worry, we are not holding this because of that.  Our system for\naccepting these changes is just...designed to be as slow and opaque as\npossible.\n\nOn Mon, Jul 1, 2019 at 7:48 AM David Norman <notifications@github.com>\nwrote:\n\n> H @gbaned <https://github.com/gbaned> I do not think that the MacOS\n> contrib could be failing due to this change.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30219?email_source=notifications&email_token=AABEZB3L2MU6EATBGSC3AO3P5IKKLA5CNFSM4H4EO5QKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6K6EQ#issuecomment-507293458>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABEZB3NSWKIH6IOFKH2AD3P5IKKLANCNFSM4H4EO5QA>\n> .\n>\n", "This is submitted internally, it just looks like our to-github updater is stuck, no updates since 3 days ago."]}, {"number": 30218, "title": "Substitute to keras.backend.get_session()", "body": "Hi. \r\n\r\nCurrently, I am exploring the Movidius Neural Compute Stick by Intel along with OpenVINO. For this, I am using `tensorflow` 1.13.1 since it still has `tensorflow.keras.back.get_session()`. Now why `get_session()`. This is because using this function I can easily extract the session from my `tensorflow.keras` model and use that session to generate `.pb` file. This `.pb` file is then supplied to OpenVINO to get the necessary files to run inference on an NCS. \r\n\r\nI am looking for ways to generate the `.pb` file using `tensorflow` 2.0. Following is how I do it using `tensorflow` 1.13.1:\r\n\r\n```python\r\nsess = K.get_session()\r\nfrozen = tf.graph_util.convert_variables_to_constants(sess,\r\n\tsess.graph_def, [model.output.op.name])\r\ngraph_io.write_graph(frozen, os.getcwd(), \"inference_graph.pb\", as_text=False)\r\n```\r\n\r\nwhere K is the alias for `tensorflow.keras.backend`.  ", "comments": ["Hi,\r\nI have no experience with `.pb` files, but if you have a keras Model instance, you probably can just use `my_model.save('some_folder')`, which creates multiple outputs in the pointed folder, including a `.pb` file.\r\nI hope this helps.", "@pandrey-fr \r\n\r\n> Hi,\r\nI have no experience with .pb files, but if you have a keras Model instance, you probably can just use my_model.save('some_folder'), which creates multiple outputs in the pointed folder, including a .pb file.\r\nI hope this helps.\r\n\r\nWill surely try. I did know about `model.save()` but I did not that it creates `.pb` file. Thank you :)", "You're welcome! It used not to, but from the docs it seems that this is becoming the default behavior in TF 2.0."]}, {"number": 30217, "title": "[XLA] Add disable option to 2 XLA CC tests", "body": "Allow backends to disable tests from these 2 test suites using the manifest mechanism.", "comments": ["> @gbaned please check go/tf-who-do-i-notify for the person you should be assigning these PRs to. And please let us know if you can help us set up an \"XLA group\" in github, that would be even better.\r\n\r\n@jlebar Thank you for the update. Sure, we will assign XLA PRs/bugs to new point of contact as per go/tf-who-do-i-notify"]}, {"number": 30216, "title": "[XLA] allow for disabling some XLA CC tests", "body": "Simply prefix the test macros with 'XLA_' to allow specific backends to disable them using a test manifest.\r\n\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30216) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 30215, "title": "Unexpected `tf.cast` behavior between signed and unsigned integers", "body": "It is tested on Google's own platform `https://colab.research.google.com`:\r\n**System information**\r\n- OS Platform and Distribution: Linux version 4.14.79+ (chrome-bot@swarm-cros-634)\r\n- TensorFlow installed from (source or binary): The platform is provided by Google\r\n- TensorFlow version: 1.14.0-rc1  (also tested on my own machine with TF 1.13.1)\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nType casting `tf.cast` from `tf.int32` to `tf.uint32` will make the Tensor become 0.\r\n\r\n**Describe the expected behavior**\r\n`tf.cast` should not change the bit representation of values.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nc = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nd = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nx = tf.cast(c, dtype=tf.uint32)\r\ny = tf.cast(c, dtype=tf.uint32)\r\nwith tf.Session() as sess:\r\n    x_raw, y_raw = sess.run([x, y])\r\n    print(x_raw.dtype, y_raw.dtype)\r\n    print(x_raw)\r\n    print(y_raw)\r\n    \r\nprint(tf.__version__)\r\n```\r\nRunning this code gives the result:\r\n```\r\nuint32 uint32\r\n[0 0 0 0 0 0]\r\n[0 0 0 0 0 0]\r\n1.14.0-rc1\r\n```", "comments": ["This is really odd... @acs6610987, can you explain why are you assigning to `d` but never actually using it?\r\nI've run your example and get the same results:\r\n```\r\nimport tensorflow as tf\r\n\r\nc = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nd = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nx = tf.cast(c, dtype=tf.uint32)\r\ny = tf.cast(c, dtype=tf.uint32)\r\nwith tf.Session() as sess:\r\n    x_raw, y_raw = sess.run([x, y])\r\n    print(x_raw.dtype, y_raw.dtype)\r\n    print(x_raw)\r\n    print(y_raw)\r\n    \r\nprint(tf.__version__)\r\n```\r\nYields:\r\n```\r\n2019-06-28 17:17:28.815499: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-28 17:17:28.842292: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-06-28 17:17:28.843186: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4739440 executing computations on platform Host. Devices:\r\n2019-06-28 17:17:28.843243: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nuint32 uint32\r\n[0 0 0 0 0 0]\r\n[0 0 0 0 0 0]\r\n1.13.1\r\n```\r\nBut if I comment that single line I get the correct results!\r\n```\r\nimport tensorflow as tf\r\n\r\nc = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\n#d = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nx = tf.cast(c, dtype=tf.uint32)\r\ny = tf.cast(c, dtype=tf.uint32)\r\nwith tf.Session() as sess:\r\n    x_raw, y_raw = sess.run([x, y])\r\n    print(x_raw.dtype, y_raw.dtype)\r\n    print(x_raw)\r\n    print(y_raw)\r\n    \r\nprint(tf.__version__)\r\n```\r\nYields:\r\n```\r\n2019-06-28 17:15:10.449152: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-28 17:15:10.470490: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-06-28 17:15:10.471184: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x44bae00 executing computations on platform Host. Devices:\r\n2019-06-28 17:15:10.471227: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nuint32 uint32\r\n[ 5  6  7  8  9 10]\r\n[ 5  6  7  8  9 10]\r\n1.13.1\r\n```\r\nRunning on a docker container with TF 1.13, Python 3.5.2 and \r\n```\r\ntf-docker /root > uname -a\r\nLinux bc59301e4081 4.18.0-21-generic #22~18.04.1-Ubuntu SMP Thu May 16 15:07:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```", "Same happens on TF2.0beta1 using V1 compatibility (commenting the assignment to `d` solves the problem) \r\n\r\nA similar code running on TF2.0 (eager) seems to be correct:\r\n```\r\nimport tensorflow as tf\r\n\r\nc = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nd = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nx = tf.cast(c, dtype=tf.uint32)\r\ny = tf.cast(c, dtype=tf.uint32)\r\n#with tf.Session() as sess:\r\n    #x, y = sess.run([x, y])\r\nprint(\"x,y\")\r\nprint(x.dtype, y.dtype)\r\nprint(x)\r\nprint(y)\r\n\r\n#c, d = sess.run([c, d])\r\nprint(\"c,d\")\r\nprint(c.dtype, d.dtype)\r\nprint(c)\r\nprint(d)\r\n\r\nprint(tf.__version__)\r\n```\r\nGives\r\n```\r\ntf-docker /root > python --version\r\nPython 3.6.7\r\ntf-docker /root > python tmp/test-eager.py \r\n2019-06-28 17:23:06.690077: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-28 17:23:06.728827: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz\r\n2019-06-28 17:23:06.729713: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2465ae0 executing computations on platform Host. Devices:\r\n2019-06-28 17:23:06.729737: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nx,y\r\n<dtype: 'uint32'> <dtype: 'uint32'>\r\ntf.Tensor([ 5  6  7  8  9 10], shape=(6,), dtype=uint32)\r\ntf.Tensor([ 5  6  7  8  9 10], shape=(6,), dtype=uint32)\r\nc,d\r\n<dtype: 'int32'> <dtype: 'int32'>\r\ntf.Tensor([ 5  6  7  8  9 10], shape=(6,), dtype=int32)\r\ntf.Tensor([ 5  6  7  8  9 10], shape=(6,), dtype=int32)\r\n2.0.0-beta0\r\ntf-docker /root > uname -a\r\nLinux ccc7b6654656 4.18.0-21-generic #22~18.04.1-Ubuntu SMP Thu May 16 15:07:19 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n", "@eduardofv , about the `d`, I actually meant to use it for `y`, so I meant to post the following code:\r\n```\r\nimport tensorflow as tf\r\n\r\nc = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nd = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nx = tf.cast(c, dtype=tf.uint32)\r\ny = tf.cast(d, dtype=tf.uint32)\r\nwith tf.Session() as sess:\r\n    x_raw, y_raw = sess.run([x, y])\r\n    print(x_raw.dtype, y_raw.dtype)\r\n    print(x_raw)\r\n    print(y_raw)\r\n    \r\nprint(tf.__version__)\r\n```\r\nThis would also give the wrong result. I made a typo for using `c` but not `d` in the issue description, but this did not change the observed weird behavior...:( There must be something wrong in the TF runtime.", "You are right @acs6610987. I've done several tests and can confirm your findings. This is very odd and, unless there's something I don't understand from the execution model, this must be a bug.\r\n\r\nIn [this colab notebook](https://colab.research.google.com/drive/1FRH7p31Nl2qEo3WCm9dHw_4yZzmyOIzw) I can reproduce your results.\r\n\r\nIn [this one](https://colab.research.google.com/drive/1duKokwg4fMuz_1shg2GpDd6wmxXEvr52) just commenting the creation of the second const the cast operation is correct.\r\n\r\nIn [this other notebook](https://colab.research.google.com/drive/1k_kzUCyUP-qsjjli6Gzvry_0o7LEWxut) I separated the second const definition and cast, and run independently. When only one operation is performed the results are correct. But when the second operation is performed the results are zeroes again as shown below:\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nc = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nx = tf.cast(c, dtype=tf.uint32)\r\nwith tf.Session() as sess:\r\n    x_raw = sess.run([x])\r\n    print(type(x_raw))\r\n    #print(x_raw.dtype)\r\n    print(x_raw)\r\n\r\nd = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\ny = tf.cast(d, dtype=tf.uint32)\r\nwith tf.Session() as sess:\r\n    x_raw, y_raw = sess.run([x, y])\r\n    print(type(x_raw))\r\n    print(x_raw.dtype)\r\n    print(x_raw)\r\n    print(type(y_raw))\r\n    print(y_raw.dtype)\r\n    print(y_raw)\r\n```\r\n```\r\n1.14.0\r\n<class 'list'>\r\n[array([ 5,  6,  7,  8,  9, 10], dtype=uint32)]\r\n<class 'numpy.ndarray'>\r\nuint32\r\n[0 0 0 0 0 0]\r\n<class 'numpy.ndarray'>\r\nuint32\r\n[0 0 0 0 0 0]\r\n```\r\nI went as far as I could, reproducing the tf.cast python operation and debugging [on this notebook](https://colab.research.google.com/drive/1as229hGZ_uK8_YszziRfL4Js99Ew4iRU), but couldn't find any point of failure up to the moment the C++ operations are invoked. I gave a glimpse to the implementation but it's been long since I worked on C++!! I think this issue will have to be revised by someone with deeper knowledge of the TF kernel.\r\n\r\n\r\n", "After some [other](https://colab.research.google.com/drive/13WotBr941mpvUgZjNAMiItRAOt6a_Wr7) [tests](https://colab.research.google.com/drive/1dJ9-iiCKGsjtIiwoEfcB4W6uERPqL5Da), it seems that the error occurs when casting from ints or floats to uint32 or uint64. uint8 and unit16 seems to be OK. I wonder if it may have to do something with this LSBZeroSetter at [cast_op.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cast_op.cc)", "I have tried on colab with TF version 1.14 & 1.13.1 with and without commenting d and was able to reproduce the  issue.Also, i replaced with d, which actually meant to use it for y (by mistake used c) and checked,still i am able to reproduce the issue.Thanks!", "@rmlarsen the problem doesn't happen in eager or if you replace the tf.constant in the original bug with a placeholder_with_default, so I think it's a constant folding issue.\r\n\r\nDo you know who should take a look? It seems bad...", "same issue as: https://github.com/tensorflow/tensorflow/issues/30691\r\nPlease refer to this PR: https://github.com/tensorflow/tensorflow/pull/30707\r\nI try it and fix this issue", "The reason when we comment this line:\r\n#d = tf.constant([5, 6, 7, 8, 9, 10], dtype=tf.int32)\r\nwould fix the issue is that: the nodes numbers need to be greater than 4 then that the code would go through this optimization path(which cause the problem).\r\nPlease refer to this code, if you change the min_graph_nodes larger than 4, the problem wouldn't happen\r\n```\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nrewriteOptions = rewriter_config_pb2.RewriterConfig(min_graph_nodes=10)\r\ngraphOptions = tf.GraphOptions(rewrite_options=rewriteOptions)\r\nconfig = tf.ConfigProto(graph_options=graphOptions)\r\n\r\nsess = tf.Session(config=config)\r\n```\r\n\r\nCould anyone help to have a review of this PR:https://github.com/tensorflow/tensorflow/pull/30707?\r\n", "@zicofish,\r\nYour code could be run successfully and is resulting in `Expected Output`. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/04b83f38b3fe97529fca8f773ab94480/gh_30215.ipynb) of the working code. Thanks!", "@zicofish  I executed the code in TF 2.5 version and  it is returning the output as expected. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/147b32ec410157ead26e53a6bd8aa89d/untitled78.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30215\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30215\">No</a>\n"]}, {"number": 30214, "title": "Fix macos makefile build", "body": "- Adding `CoreFoundation` to `HOST_LDFLAGS` is necessary as the library target uses that variable\r\n- Adding `CoreFoundation` to `LIBS` is necessary as the benchmark target uses that variable", "comments": ["@mihaimaruseac ", "Thanks for your contribution, we will not be accepting PRs which are not against master, so closing this PR\r\nCC @mihaimaruseac\r\n", "Opened new PR against master at #30279"]}, {"number": 30213, "title": "Fix macos makefile", "body": "", "comments": []}, {"number": 30212, "title": "error C2280: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': attempting to reference a deleted function", "body": "**System information**\r\n- OS Platform :   Windows 10 Pro\r\n- TensorFlow version: 2.99\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv: conda\r\n- Bazel version :  0.24.1\r\n- MSVC17\r\n- NO CUDA\r\n\r\nconfiguration : \r\nPlease specify the location of python. [Default is C:\\Users\\zen2_microsoft\\Anaconda3\\envs\\tensorflow-1.13.1-without-mkl\\python.exe]:\r\nFound possible Python library paths:\r\n  C:\\Users\\zen2_microsoft\\Anaconda3\\envs\\tensorflow-1.13.1-without-mkl\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\zen2_microsoft\\Anaconda3\\envs\\tensorflow-1.13.1-without-mkl\\lib\\site-packages]\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: N\r\n\r\n**Describe the problem**\r\nBuild fails with below error .\r\ncompilation error log :\r\n\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.12.25827/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/genfiles/external/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/genfiles/external/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/core/grappler/_objs/grappler_item/grappler_item.obj /c tensorflow/core/grappler/grappler_item.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/com_google_absl\\absl/meta/type_traits.h(141): error C2280: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': attempting to reference a deleted function\r\n.\\tensorflow/core/framework/function.h(334): note: see declaration of 'tensorflow::FunctionLibraryDefinition::operator ='\r\nexternal/com_google_absl\\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsCopyAssignableImpl<tensorflow::FunctionLibraryDefinition>' being compiled\r\nexternal/com_google_absl\\absl/meta/type_traits.h(150): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsCopyAssignableImpl,T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::FunctionLibraryDefinition\r\n        ]\r\nexternal/com_google_absl\\absl/meta/type_traits.h(432): note: see reference to class template instantiation 'absl::is_copy_assignable<T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::FunctionLibraryDefinition\r\n        ]\r\nexternal/com_google_absl\\absl/types/internal/optional.h(173): note: see reference to class template instantiation 'absl::is_trivially_copy_assignable<tensorflow::FunctionLibraryDefinition>' being compiled\r\ntensorflow/core/grappler/grappler_item.cc(118): note: see reference to class template instantiation 'absl::optional<tensorflow::FunctionLibraryDefinition>' being compiled\r\n.\\tensorflow/core/framework/function.h(334): note: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': function was explicitly deleted\r\nexternal/com_google_absl\\absl/meta/type_traits.h(144): error C2280: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': attempting to reference a deleted function\r\n.\\tensorflow/core/framework/function.h(334): note: see declaration of 'tensorflow::FunctionLibraryDefinition::operator ='\r\nexternal/com_google_absl\\absl/meta/type_traits.h(121): note: see reference to alias template instantiation 'IsMoveAssignableImpl<tensorflow::FunctionLibraryDefinition>' being compiled\r\nexternal/com_google_absl\\absl/meta/type_traits.h(155): note: see reference to class template instantiation 'absl::type_traits_internal::is_detected<absl::type_traits_internal::IsMoveAssignableImpl,T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::FunctionLibraryDefinition\r\n        ]\r\nexternal/com_google_absl\\absl/types/internal/optional.h(328): note: see reference to class template instantiation 'absl::is_move_assignable<T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::FunctionLibraryDefinition\r\n        ]\r\nexternal/com_google_absl\\absl/types/optional.h(120): note: see reference to class template instantiation 'absl::optional_internal::assign_copy_traits<T>' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::FunctionLibraryDefinition\r\n        ]\r\n.\\tensorflow/core/framework/function.h(334): note: 'tensorflow::FunctionLibraryDefinition &tensorflow::FunctionLibraryDefinition::operator =(const tensorflow::FunctionLibraryDefinition &)': function was explicitly deleted\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 11024.399s, Critical Path: 6294.48s\r\nINFO: 2398 processes: 2398 local.\r\nFAILED: Build did NOT complete successfully \r\n\r\nPlease help me out .", "comments": ["Just to verify did you get chance to follow instructions from [Tensorflow Website](https://www.tensorflow.org/install/source_windows).Thanks!", "@ravikyram \r\nYes I have followed below instructions:\r\n1) conda create -n tensorflow-1.13.1-without-mkl python=3.6 anaconda\r\n2)conda activate tensorflow-1.13.1-without-mkl\r\n3)Install MSYS2\r\n set environmental variable, under PATH -> C:\\msys64\\usr\\bin\r\n4)Install JDK12 and set environmental variable : JAVA_HOME = C:\\Program Files\\Java\\jdk-12\r\n5)pacman -Syu zip unzip\r\n6)pacman -Syuu --noconfirm patch\r\n7)set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\r\n8) set BAZEL_SH=C:\\msys64\\usr\\bin\\bash.exe\r\n9) git clone https://github.com/tensorflow/tensorflow.git -b r2.99\r\n10) python ./configure.py\r\n11) bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nPlease correct me in case i am wrong. \r\n", "I think 2.99 is just an experimental branch.\r\nIt is possible that it is simply broken.\r\nCould you try 2.0 or 1.4?", "@gunan \r\nIssue with r1.3  #30599 ", "I am also seeing this on `2.0` and current master (commit `sha:1ba28ea0022ce67bd5286cc8c30d7cb5fc7f7bad` ).\r\n\r\nI see it on one windows machine but not another with exactly the same code, bazel version, etc. :-/ \r\n\r\nTrying to pin down what the difference between these two is but, so far, without success.", "I am also getting this error on 1.14 using MSVC 2017 and bazel 0.24.1", "I was able to get around this error by upgrading the Visual Studio 2017 Build Tools and Community IDE to the latest version 15.9.14. Maybe someone can verify if this is a valid fix, for instance @stewart-r ? ", "The same problem (tensorflow 1.14.0, MSVC 2017 (15.19.18), bazel 0.24.1)\r\n\r\nSeems like a problem in `absl::optional` implementation that might require copy assignment operator for the underlying type but it shouldn't.\r\n\r\nI've made a local fast fix with replacement of `absl::optional` to `unique_ptr` [patch](https://github.com/tensorflow/tensorflow/files/3983214/patch_tf_fn_lib_no_copy_assign_2.txt) and it compiles nice.\r\n\r\nBetter fix would be to reuse `std::optional` and I have no idea why tensorflow developers didn't (maybe obsolete code when `std::optional` was not available in C++ stdlib).\r\n", "std::optional wasn't supported in MSVC 2015 as far as I can tell, meaning the absl or boost one had to be used instead of the std library one (https://devblogs.microsoft.com/cppblog/stdoptional-how-when-and-why/). I'm trying your patch on MSVC 2019, and I have a feeling it's going to work. I wonder if we should patch absl, or if tensorflow is going to transition away from MSVC 2015 at some point soon. 2017 is officially supported so something broke here probably recently. :D", "Thank you very much for debugging!\r\nOn our side, TF recently moved the official builds to MSVC 2019, that may be why we missed the issue.\r\n\r\nWe try to use STL as much as possible due to various reasons, some performance, some know issues with interoperability with eigen, etc. So using std::optional may not be OK for us.\r\nWe can look into patching absl, as it looks like MSVC 2017 users are also reporting issues.", "@mayadav1 Could you please try using TF v2.6.0 and refer to [build from source](https://www.tensorflow.org/install/source_windows) ?Please let us know if it helps?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30212\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30212\">No</a>\n"]}]