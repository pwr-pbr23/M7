[{"number": 47830, "title": "Remove uint8 support from conv and depthwise conv.", "body": "See the discussion on https://github.com/tensorflow/tensorflow/pull/47471 and #44912 for more details.\r\n\r\nFixes #42883", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47829, "title": "Conv2D output not compatible with ZeroPadding3D input", "body": "I need to zero pad outputs of multiple CNN layers in order to concatenate them together as input to another layer. \r\n\r\nA simple way to demonstrate this is:\r\n\r\n```\r\nx = Input(shape=(256, 256, 3))\r\nx = Conv2D(32, (2,2), (2,2))(x)     # 1st CNN layer\r\nx = ZeroPadding3D(padding=(2, 2, 2))(x)\r\n```\r\n\r\nThis fails on the output of the CNN layer, irrespective of what values I provide for padding.\r\n\r\nThe error I get is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Mark\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 951, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"C:\\Users\\Mark\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1090, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"C:\\Users\\Mark\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"C:\\Users\\Mark\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 862, in _infer_output_signature\r\n    self._maybe_build(inputs)\r\n  File \"C:\\Users\\Mark\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 2684, in _maybe_build\r\n    input_spec.assert_input_compatibility(\r\n  File \"C:\\Users\\Mark\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\", line 219, in assert_input_compatibility\r\n    raise ValueError('Input ' + str(input_index) + ' of layer ' +\r\nValueError: Input 0 of layer zero_padding3d_4 is incompatible with the layer: expected ndim=5, found ndim=4. Full shape received: (None, 128, 128, 32)\r\n```\r\nI have looked at the example of ZeroPadding2D and this appears to work OK [in the Pix2Pix example](https://www.tensorflow.org/tutorials/generative/pix2pix), but only works on the first 2 dimensions. I need to pad 3 dimensions.\r\n\r\nThe documentation for [ZeroPadding3D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding3D) also does not show how this layer can be used in a network.\r\n\r\n\r\nOutput of `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`:\r\n\r\n`v2.4.0-49-g85c8b2a817f 2.4.1`\r\nPython version = 3.8.5\r\nOS = Windows 10 (Latest developer release)\r\nNVIDIA-SMI 465.51       \r\nDriver Version: 465.51       \r\nCUDA Version: 11.3\r\nGPU = RTX3070 (8GB)\r\nTensorflow installed from binary (pip)\r\n\r\nThe expected behaviour is that I can use ZeroPadding3D as a layer within my CNN to pad the output of a convolutional layer in all 3 dimensions.\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/02297354e15b5036f1b88a48c117ffaf/47829.ipynb). Thanks!", "ZeroPadding3D expects a [5D tensor](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding3D#input_shape) whereas input Conv2D gives [4D tensor](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D#output_shape_2).\r\nSwitching to ZeroPadding2D  for Conv2D works;\r\n```python\r\nx = Input(shape=(256, 256, 3))\r\nx = Conv2D(32, (2,2), (2,2))(x)     # 1st CNN layer\r\nx = ZeroPadding2D(padding=(2,2))(x)\r\n```\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47829\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47829\">No</a>\n"]}, {"number": 47828, "title": "[Intel MKL] Enable Conv2D plus FusedBatchNorm and optional activation fusion", "body": "The fusion for Conv2D + FusedBatchNorm + optional activation was previously disabled for the MKL build. This PR adds the support to enable this optimization for the MKL build. \r\n(This PR has dependency on the OneDNN 2.1 PR 47743 which is already released in base now)", "comments": ["@penpornk - thanks for reviewing the PR and suggested changes. I have incorporated your comments, and also, merged to the latest baseline. Please let me know if any additional comments. Thanks", "@yimeisun123  Can you please check build failures. Thanks!", "@gbaned - I checked failed logs, they are not related to the changes in this PR. There are failures from LLVM.", "Agreed. I'll rerun the tests."]}, {"number": 47827, "title": "ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'   ", "body": "I am using faster_rcnn_inception_v2_coco model and want to convert it into tflite...   \r\nimport tensorflow as tf  \r\n\r\ntflite_model_name=\"TF.lite\"\r\n\r\nsaved_model_dir=\"exported_output_graph/saved_model/saved_model.pb\"  \r\n#Convert the model \r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory \r\ntflite_model = converter.convert()  \r\nwith open(tflite_model_name, 'wb') as f:   \r\nf.write(tflite_model)   \r\nI don't what I am missing here\r\n\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution =Linux \r\n- TensorFlow library =1.14", "comments": ["Could you try the conversion with the recent TF version? TF 1.14 is too old to be supported.", "> Could you try the conversion with the recent TF version? TF 1.14 is too old to be supported.\r\n\r\nI tried the newer version then got this error\r\nKeyError: \"The name 'prefix/FirstStageFeatureExtractor/Assert/Assert:0' refers to a Tensor which does not exist. The operation, 'prefix/FirstStageFeatureExtractor/Assert/Assert', exists but only has 0 outputs.\"\r\n\r\n\r\n", "@Zakria96 It is really impossible to debug your issue without having enough information. Could you create a gist or share your minimal steps to reproduce your problem at our side?", "> @Zakria96 It is really impossible to debug your issue without having enough information. Could you create a gist or share your minimal steps to reproduce your problem at our side?\r\n\r\nI am training my own object detection(rgb) model by using predefined faster_rcnn_inception_v2_coco... and I have successfully completed my training and created my frozen_inference_graph from the checkpoints..now I want to convert my model into TFlite.\r\n\r\nTo find the input , output array name and tensor shape  required  to convert .pb file into .tflite file using python script below.\r\n\r\nimport tensorflow as tf\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    with tf.io.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.compat.v1.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def, name=\"prefix\")\r\n    return graph\r\n\r\n\r\nif __name__ == '__main__':\r\n    graph = load_graph(\"frozen_inference_graph.pb\")\r\n    for op in graph.get_operations():\r\n        abc = graph.get_tensor_by_name(op.name + \":0\")\r\n        print(abc)\r\n\r\nKeyError: \"The name 'prefix/FirstStageFeatureExtractor/Assert/Assert:0' refers to a Tensor which does not exist. The operation, 'prefix/FirstStageFeatureExtractor/Assert/Assert', exists but only has 0 outputs.\"\r\n\r\n", "Is it possible to share your frozen graph file to us? Or can you create a colab to reproduce the issue easily?", "> Is it possible to share your frozen graph file to us? Or can you create a colab to reproduce the issue easily?\r\n\r\nyeah I can....How should I send it...Should I send it here...\r\n", "You can share the file through a google drive link or something else.", "> You can share the file through a google drive link or something else.\r\nYou can find my frozen graph  here: \r\nhttps://drive.google.com/file/d/1jKYaZCnqqkfTR4JDZUSFFe8x485c8kz1/view?usp=sharing ", "Could you clarify the names of the input and output tensors of the given frozen graph?\r\n\r\nThen you can convert your model with the following code snippet.\r\n\r\n```\r\n  converter = tf.compat.v1.lite.TFLiteConverter.frrom_frozen_graph(\".../frozen_inference_graph.pb\", [\"input_tensor_name\"], [\"output_tensor_name\"])\r\n  converter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n  ]\r\n  model = converter.convert()\r\n```", "> Could you clarify the names of the input and output tensors of the given frozen graph?\r\n> \r\n> Then you can convert your model with the following code snippet.\r\n> \r\n> ```\r\n>   converter = tf.compat.v1.lite.TFLiteConverter.frrom_frozen_graph(\".../frozen_inference_graph.pb\", [\"input_tensor_name\"], [\"output_tensor_name\"])\r\n>   converter.target_spec.supported_ops = [\r\n>     tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n>   ]\r\n>   model = converter.convert()\r\n> ```\r\n\r\nI am not able to find the input and output tensors of my frozen_inference_graph.... Can you tell me how to find the input and output tensors of my model... and also show me an example of input and output tensor.... I tried a lot... I would be thankful..", "I could the input and output tensor names from https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/public/faster_rcnn_inception_v2_coco/faster_rcnn_inception_v2_coco.md.\r\n\r\nI could successfully convert the above frozen graph def by using the following code snippet.\r\n\r\n```\r\n  converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\"/tmp/fg.pb\", [\"image_tensor\"], [\"detection_classes\", \"detection_scores\", \"detection_boxes\", \"num_detections\"])\r\n  converter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n  ]\r\n  model = converter.convert()\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47827\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47827\">No</a>\n"]}, {"number": 47826, "title": "Prefer tf.ones and tf.zeros over tf.fill", "body": "This PR removes usage of `tf.fill` in favour of `tf.ones` or `tf.zeros` to simplify the code.", "comments": ["@allenlavoie I had to update the PR to fix a pylint error on CI, do you mind approving it again?"]}, {"number": 47825, "title": "Construct scalar constants directly instead of calling zeros([]) or ones([])", "body": "This PR changes the code to construct scalar constants directly instead of calling `zeros([])` or `ones([])`.\r\nFor scalars in eager mode calling `tf.constant` directly is about 9x faster than calls to `zeros([])` or `ones([])`. In practice this will only matter in edge cases, but since it doesn't reduce readability I think it is good to do in any case.", "comments": ["Looks like these changes didn't work for bool and string types, I updated the PR to fix the CI failures.", "@lgeiger Please submit this PR to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. Thankyou.\r\n@qlzh727", "> Please submit this PR to the github.com/keras-team/keras repository instead. Thankyou.\r\n\r\nThis code also touches code outside of `tensorflow/python/keras` which is not part of keras-team/keras (the PR even predates the move to keras-team/keras). I won't resubmit, feel free to close if it's not possible to merge.", "Acknowledged, sorry about that! As per above, closing because this is unmergable as of the recent Keras repo split."]}, {"number": 47824, "title": "Pip installation issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n![Screenshot 2021-03-16 003933](https://user-images.githubusercontent.com/65313257/111207853-22f41980-85f0-11eb-8800-e72e8382452e.png)\r\n", "comments": ["@shyamdsundar \r\nPlease refer to this resolved issues: #47365, #47140, #44656 and verify your system and pip requirements.\r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47824\">No</a>\n"]}, {"number": 47823, "title": "Fix AddExpandDims() in lite/micro/micro_mutable_op_resolver.h", "body": "PR5 for issue #46258.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47822, "title": "[TFLM] Add support for optimized DEPTHWISE_CONV2D for CEVA-DSP BX1 and SP500", "body": "Relevant github issue: https://github.com/tensorflow/tensorflow/issues/45607\r\n\r\nRemoved support for UINT8 so #ifdef-ed them out from the tests.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 47821, "title": "Fix typo in Normalization layer doc", "body": "", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, if possible please include more such changes in a single PR.Thank you\r\n"]}, {"number": 47820, "title": "OOM when allocating tensor with shape[3,3,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu Jetson Nano\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.4.0+nv21.2\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nOOM when allocating tensor with shape[3,3,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]\r\n![Screenshot from 2021-03-15 18-49-12](https://user-images.githubusercontent.com/67184718/111161346-12787a80-85c1-11eb-9bb0-f7ebe515ac23.png)\r\n![Screenshot from 2021-03-15 19-00-43](https://user-images.githubusercontent.com/67184718/111161388-1dcba600-85c1-11eb-960b-1b71dd26af21.png)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["\r\n/usr/bin/python3.6 /home/swag/Desktop/IGVC_ConeDetectionScript/detect_independent.py\r\n2021-03-15 15:33:26.917255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n2021-03-15 15:33:39.556381: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-15 15:33:39.691466: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-15 15:33:39.771661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:33:39.793576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties: \r\npciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3\r\ncoreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 3.87GiB deviceMemoryBandwidth: 194.55MiB/s\r\n2021-03-15 15:33:39.793916: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n2021-03-15 15:33:39.977313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-03-15 15:33:39.977799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-03-15 15:33:40.055171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-15 15:33:40.164782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-15 15:33:40.301700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-15 15:33:40.377652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-15 15:33:40.394881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-15 15:33:40.395772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:33:40.396638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:33:40.396876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0\r\nGPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2021-03-15 15:33:40.573865: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-15 15:33:40.574405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:33:40.574550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties: \r\npciBusID: 0000:00:00.0 name: NVIDIA Tegra X1 computeCapability: 5.3\r\ncoreClock: 0.9216GHz coreCount: 1 deviceMemorySize: 3.87GiB deviceMemoryBandwidth: 194.55MiB/s\r\n2021-03-15 15:33:40.574750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n2021-03-15 15:33:40.574904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-03-15 15:33:40.575010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-03-15 15:33:40.575096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-15 15:33:40.575171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-15 15:33:40.575246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-15 15:33:40.575320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-15 15:33:40.575396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-15 15:33:40.575675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:33:40.575965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:33:40.576181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0\r\n2021-03-15 15:33:40.589765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n2021-03-15 15:35:04.102739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-15 15:35:04.868559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      0 \r\n2021-03-15 15:35:04.875308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 0:   N \r\n2021-03-15 15:35:07.222793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:35:07.698663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:35:07.747358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-03-15 15:35:08.060367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 67 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X1, pci bus id: 0000:00:00.0, compute capability: 5.3)\r\n2021-03-15 17:16:57.309165: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 18.00MiB (rounded to 18874368)requested by op Mul\r\nIf the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \r\nCurrent allocation summary follows.\r\nCurrent allocation summary follows.\r\n2021-03-15 17:16:57.311009: I tensorflow/core/common_runtime/bfc_allocator.cc:975] BFCAllocator dump for GPU_0_bfc\r\n2021-03-15 17:16:57.311451: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (256): \tTotal Chunks: 64, Chunks in use: 64. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 14.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.311711: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (512): \tTotal Chunks: 84, Chunks in use: 84. 42.0KiB allocated for chunks. 42.0KiB in use in bin. 42.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.311908: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (1024): \tTotal Chunks: 85, Chunks in use: 85. 85.8KiB allocated for chunks. 85.8KiB in use in bin. 85.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.312094: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (2048): \tTotal Chunks: 9, Chunks in use: 9. 19.5KiB allocated for chunks. 19.5KiB in use in bin. 19.4KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.312260: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-03-15 17:16:57.312452: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (8192): \tTotal Chunks: 1, Chunks in use: 1. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.312640: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (16384): \tTotal Chunks: 6, Chunks in use: 6. 104.0KiB allocated for chunks. 104.0KiB in use in bin. 96.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.312826: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (32768): \tTotal Chunks: 4, Chunks in use: 3. 161.0KiB allocated for chunks. 112.0KiB in use in bin. 96.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.313015: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (65536): \tTotal Chunks: 12, Chunks in use: 12. 811.8KiB allocated for chunks. 811.8KiB in use in bin. 784.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.313200: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (131072): \tTotal Chunks: 5, Chunks in use: 4. 736.0KiB allocated for chunks. 544.0KiB in use in bin. 544.0KiB client-requested in use in bin.\r\n2021-03-15 17:16:57.313373: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (262144): \tTotal Chunks: 13, Chunks in use: 11. 3.44MiB allocated for chunks. 2.88MiB in use in bin. 2.78MiB client-requested in use in bin.\r\n2021-03-15 17:16:57.313720: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (524288): \tTotal Chunks: 11, Chunks in use: 10. 6.50MiB allocated for chunks. 5.75MiB in use in bin. 5.50MiB client-requested in use in bin.\r\n2021-03-15 17:16:57.313890: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (1048576): \tTotal Chunks: 2, Chunks in use: 2. 2.12MiB allocated for chunks. 2.12MiB in use in bin. 2.12MiB client-requested in use in bin.\r\n2021-03-15 17:16:57.313982: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (2097152): \tTotal Chunks: 10, Chunks in use: 8. 23.75MiB allocated for chunks. 19.25MiB in use in bin. 18.00MiB client-requested in use in bin.\r\n2021-03-15 17:16:57.314065: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.\r\n2021-03-15 17:16:57.314136: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-03-15 17:16:57.314225: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 20.75MiB allocated for chunks. 20.75MiB in use in bin. 18.00MiB client-requested in use in bin.\r\n2021-03-15 17:16:57.314309: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-03-15 17:16:57.314400: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-03-15 17:16:57.314494: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-03-15 17:16:57.314593: I tensorflow/core/common_runtime/bfc_allocator.cc:982] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2021-03-15 17:16:57.334694: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin for 18.00MiB was 16.00MiB, Chunk State: \r\n2021-03-15 17:16:57.334891: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Next region of size 1048576\r\n2021-03-15 17:16:57.335299: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950000 of size 1280 next 1\r\n2021-03-15 17:16:57.335496: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950500 of size 256 next 7\r\n2021-03-15 17:16:57.335651: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950600 of size 256 next 8\r\n2021-03-15 17:16:57.335798: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950700 of size 256 next 9\r\n2021-03-15 17:16:57.335944: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950800 of size 256 next 10\r\n2021-03-15 17:16:57.336090: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950900 of size 256 next 11\r\n2021-03-15 17:16:57.336234: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950a00 of size 256 next 12\r\n2021-03-15 17:16:57.336379: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950b00 of size 256 next 13\r\n2021-03-15 17:16:57.336622: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950c00 of size 256 next 16\r\n2021-03-15 17:16:57.336774: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950d00 of size 256 next 17\r\n2021-03-15 17:16:57.336923: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950e00 of size 256 next 18\r\n2021-03-15 17:16:57.337071: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00950f00 of size 256 next 19\r\n2021-03-15 17:16:57.337220: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951000 of size 256 next 22\r\n2021-03-15 17:16:57.337367: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951100 of size 256 next 23\r\n2021-03-15 17:16:57.337688: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951200 of size 256 next 2\r\n2021-03-15 17:16:57.337850: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951300 of size 256 next 3\r\n2021-03-15 17:16:57.338003: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951400 of size 256 next 24\r\n2021-03-15 17:16:57.338151: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951500 of size 256 next 26\r\n2021-03-15 17:16:57.338297: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951600 of size 256 next 27\r\n2021-03-15 17:16:57.338447: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951700 of size 256 next 28\r\n2021-03-15 17:16:57.338653: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951800 of size 256 next 29\r\n2021-03-15 17:16:57.338852: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951900 of size 256 next 32\r\n2021-03-15 17:16:57.339011: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951a00 of size 256 next 33\r\n2021-03-15 17:16:57.339158: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951b00 of size 256 next 34\r\n2021-03-15 17:16:57.339304: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951c00 of size 256 next 37\r\n2021-03-15 17:16:57.339450: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951d00 of size 256 next 38\r\n2021-03-15 17:16:57.339595: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951e00 of size 256 next 39\r\n2021-03-15 17:16:57.339741: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00951f00 of size 256 next 40\r\n2021-03-15 17:16:57.339885: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00952000 of size 256 next 43\r\n2021-03-15 17:16:57.340030: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00952100 of size 256 next 4\r\n2021-03-15 17:16:57.340176: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00952200 of size 256 next 5\r\n2021-03-15 17:16:57.340400: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00952300 of size 3584 next 6\r\n2021-03-15 17:16:57.340549: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953100 of size 256 next 79\r\n2021-03-15 17:16:57.340696: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953200 of size 256 next 80\r\n2021-03-15 17:16:57.340844: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953300 of size 256 next 82\r\n2021-03-15 17:16:57.340990: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953400 of size 256 next 83\r\n2021-03-15 17:16:57.341136: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953500 of size 256 next 84\r\n2021-03-15 17:16:57.341283: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953600 of size 256 next 85\r\n2021-03-15 17:16:57.341430: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953700 of size 256 next 87\r\n2021-03-15 17:16:57.341696: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953800 of size 256 next 88\r\n2021-03-15 17:16:57.341825: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953900 of size 256 next 89\r\n2021-03-15 17:16:57.341881: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953a00 of size 256 next 90\r\n2021-03-15 17:16:57.341944: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953b00 of size 512 next 91\r\n2021-03-15 17:16:57.341997: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953d00 of size 512 next 93\r\n2021-03-15 17:16:57.342051: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00953f00 of size 512 next 94\r\n2021-03-15 17:16:57.342102: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00954100 of size 512 next 95\r\n2021-03-15 17:16:57.342157: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00954300 of size 1024 next 99\r\n2021-03-15 17:16:57.342215: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00954700 of size 1024 next 100\r\n2021-03-15 17:16:57.342278: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00954b00 of size 1024 next 101\r\n2021-03-15 17:16:57.342342: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00954f00 of size 1024 next 102\r\n2021-03-15 17:16:57.342408: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00955300 of size 512 next 105\r\n2021-03-15 17:16:57.342477: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00955500 of size 512 next 106\r\n2021-03-15 17:16:57.342549: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00955700 of size 512 next 107\r\n2021-03-15 17:16:57.342623: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00955900 of size 512 next 108\r\n2021-03-15 17:16:57.342697: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00955b00 of size 512 next 110\r\n2021-03-15 17:16:57.342773: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00955d00 of size 512 next 111\r\n2021-03-15 17:16:57.342848: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00955f00 of size 512 next 112\r\n2021-03-15 17:16:57.342923: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956100 of size 512 next 113\r\n2021-03-15 17:16:57.342997: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956300 of size 512 next 115\r\n2021-03-15 17:16:57.343071: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956500 of size 512 next 116\r\n2021-03-15 17:16:57.343145: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956700 of size 512 next 117\r\n2021-03-15 17:16:57.343220: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956900 of size 512 next 118\r\n2021-03-15 17:16:57.343294: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956b00 of size 512 next 121\r\n2021-03-15 17:16:57.343369: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956d00 of size 512 next 122\r\n2021-03-15 17:16:57.343443: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00956f00 of size 512 next 30\r\n2021-03-15 17:16:57.343519: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00957100 of size 8192 next 31\r\n2021-03-15 17:16:57.343594: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959100 of size 256 next 44\r\n2021-03-15 17:16:57.343669: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959200 of size 256 next 45\r\n2021-03-15 17:16:57.343744: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959300 of size 256 next 47\r\n2021-03-15 17:16:57.343819: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959400 of size 256 next 48\r\n2021-03-15 17:16:57.343898: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959500 of size 256 next 49\r\n2021-03-15 17:16:57.343972: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959600 of size 256 next 50\r\n2021-03-15 17:16:57.344047: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959700 of size 512 next 51\r\n2021-03-15 17:16:57.344121: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959900 of size 512 next 54\r\n2021-03-15 17:16:57.344196: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959b00 of size 512 next 55\r\n2021-03-15 17:16:57.344306: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959d00 of size 512 next 56\r\n2021-03-15 17:16:57.344416: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00959f00 of size 256 next 58\r\n2021-03-15 17:16:57.344491: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a000 of size 256 next 59\r\n2021-03-15 17:16:57.344537: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a100 of size 256 next 60\r\n2021-03-15 17:16:57.344578: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a200 of size 256 next 61\r\n2021-03-15 17:16:57.344619: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a300 of size 256 next 63\r\n2021-03-15 17:16:57.344660: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a400 of size 256 next 64\r\n2021-03-15 17:16:57.344697: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a500 of size 256 next 65\r\n2021-03-15 17:16:57.344735: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a600 of size 256 next 66\r\n2021-03-15 17:16:57.344774: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a700 of size 256 next 68\r\n2021-03-15 17:16:57.344815: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a800 of size 256 next 69\r\n2021-03-15 17:16:57.344858: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095a900 of size 256 next 70\r\n2021-03-15 17:16:57.344904: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095aa00 of size 256 next 71\r\n2021-03-15 17:16:57.344953: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095ab00 of size 256 next 72\r\n2021-03-15 17:16:57.345027: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095ac00 of size 256 next 75\r\n2021-03-15 17:16:57.345104: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095ad00 of size 256 next 76\r\n2021-03-15 17:16:57.345165: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095ae00 of size 256 next 77\r\n2021-03-15 17:16:57.345221: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095af00 of size 256 next 78\r\n2021-03-15 17:16:57.345279: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095b000 of size 256 next 20\r\n2021-03-15 17:16:57.345339: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095b100 of size 16384 next 21\r\n2021-03-15 17:16:57.345397: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0095f100 of size 16384 next 25\r\n2021-03-15 17:16:57.345458: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00963100 of size 16384 next 67\r\n2021-03-15 17:16:57.345572: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967100 of size 512 next 123\r\n2021-03-15 17:16:57.345610: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967300 of size 512 next 125\r\n2021-03-15 17:16:57.345642: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967500 of size 512 next 126\r\n2021-03-15 17:16:57.345674: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967700 of size 512 next 127\r\n2021-03-15 17:16:57.345704: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967900 of size 512 next 128\r\n2021-03-15 17:16:57.345734: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967b00 of size 512 next 129\r\n2021-03-15 17:16:57.345763: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967d00 of size 512 next 130\r\n2021-03-15 17:16:57.345791: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00967f00 of size 512 next 131\r\n2021-03-15 17:16:57.345818: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968100 of size 512 next 132\r\n2021-03-15 17:16:57.345846: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968300 of size 512 next 133\r\n2021-03-15 17:16:57.345874: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968500 of size 512 next 134\r\n2021-03-15 17:16:57.345903: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968700 of size 512 next 135\r\n2021-03-15 17:16:57.345933: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968900 of size 512 next 136\r\n2021-03-15 17:16:57.345965: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968b00 of size 512 next 138\r\n2021-03-15 17:16:57.345998: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968d00 of size 512 next 139\r\n2021-03-15 17:16:57.346033: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00968f00 of size 512 next 140\r\n2021-03-15 17:16:57.346090: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969100 of size 512 next 141\r\n2021-03-15 17:16:57.346130: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969300 of size 512 next 143\r\n2021-03-15 17:16:57.346170: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969500 of size 512 next 144\r\n2021-03-15 17:16:57.346209: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969700 of size 512 next 145\r\n2021-03-15 17:16:57.346249: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969900 of size 512 next 146\r\n2021-03-15 17:16:57.346289: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969b00 of size 512 next 148\r\n2021-03-15 17:16:57.346330: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969d00 of size 512 next 149\r\n2021-03-15 17:16:57.346373: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00969f00 of size 512 next 150\r\n2021-03-15 17:16:57.346415: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096a100 of size 512 next 151\r\n2021-03-15 17:16:57.346457: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096a300 of size 512 next 153\r\n2021-03-15 17:16:57.346499: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096a500 of size 512 next 154\r\n2021-03-15 17:16:57.346541: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096a700 of size 512 next 155\r\n2021-03-15 17:16:57.346583: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096a900 of size 512 next 156\r\n2021-03-15 17:16:57.346626: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096ab00 of size 512 next 159\r\n2021-03-15 17:16:57.346669: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096ad00 of size 512 next 160\r\n2021-03-15 17:16:57.346711: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096af00 of size 512 next 46\r\n2021-03-15 17:16:57.346754: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0096b100 of size 49152 next 14\r\n2021-03-15 17:16:57.346797: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00977100 of size 73728 next 15\r\n2021-03-15 17:16:57.346839: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00989100 of size 16384 next 41\r\n2021-03-15 17:16:57.346881: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0098d100 of size 16384 next 42\r\n2021-03-15 17:16:57.346923: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991100 of size 512 next 161\r\n2021-03-15 17:16:57.346965: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991300 of size 512 next 163\r\n2021-03-15 17:16:57.347007: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991500 of size 512 next 164\r\n2021-03-15 17:16:57.347050: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991700 of size 512 next 165\r\n2021-03-15 17:16:57.347092: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991900 of size 512 next 166\r\n2021-03-15 17:16:57.347134: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991b00 of size 512 next 168\r\n2021-03-15 17:16:57.347176: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991d00 of size 512 next 169\r\n2021-03-15 17:16:57.347218: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00991f00 of size 512 next 170\r\n2021-03-15 17:16:57.347260: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992100 of size 512 next 171\r\n2021-03-15 17:16:57.347302: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992300 of size 512 next 173\r\n2021-03-15 17:16:57.347345: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992500 of size 512 next 174\r\n2021-03-15 17:16:57.347389: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992700 of size 512 next 175\r\n2021-03-15 17:16:57.347432: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992900 of size 512 next 176\r\n2021-03-15 17:16:57.347477: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992b00 of size 512 next 178\r\n2021-03-15 17:16:57.347519: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992d00 of size 512 next 179\r\n2021-03-15 17:16:57.347561: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00992f00 of size 512 next 180\r\n2021-03-15 17:16:57.347603: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993100 of size 512 next 181\r\n2021-03-15 17:16:57.347644: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993300 of size 512 next 183\r\n2021-03-15 17:16:57.347687: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993500 of size 512 next 184\r\n2021-03-15 17:16:57.347729: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993700 of size 512 next 185\r\n2021-03-15 17:16:57.347771: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993900 of size 512 next 186\r\n2021-03-15 17:16:57.347813: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993b00 of size 512 next 188\r\n2021-03-15 17:16:57.347855: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993d00 of size 512 next 189\r\n2021-03-15 17:16:57.347896: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00993f00 of size 512 next 190\r\n2021-03-15 17:16:57.347939: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00994100 of size 512 next 191\r\n2021-03-15 17:16:57.347981: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00994300 of size 512 next 193\r\n2021-03-15 17:16:57.348023: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00994500 of size 512 next 194\r\n2021-03-15 17:16:57.348065: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00994700 of size 512 next 195\r\n2021-03-15 17:16:57.348107: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00994900 of size 512 next 196\r\n2021-03-15 17:16:57.348150: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00994b00 of size 1536 next 86\r\n2021-03-15 17:16:57.348299: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00995100 of size 24576 next 35\r\n2021-03-15 17:16:57.348344: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f0099b100 of size 73728 next 36\r\n2021-03-15 17:16:57.348387: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009ad100 of size 32768 next 57\r\n2021-03-15 17:16:57.348429: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009b5100 of size 32768 next 62\r\n2021-03-15 17:16:57.348472: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009bd100 of size 65536 next 124\r\n2021-03-15 17:16:57.348515: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cd100 of size 1024 next 275\r\n2021-03-15 17:16:57.348557: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cd500 of size 1024 next 276\r\n2021-03-15 17:16:57.348599: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cd900 of size 1024 next 277\r\n2021-03-15 17:16:57.348641: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cdd00 of size 1024 next 279\r\n2021-03-15 17:16:57.348684: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009ce100 of size 1024 next 280\r\n2021-03-15 17:16:57.348726: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009ce500 of size 1024 next 281\r\n2021-03-15 17:16:57.348768: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009ce900 of size 1024 next 282\r\n2021-03-15 17:16:57.348810: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009ced00 of size 1024 next 284\r\n2021-03-15 17:16:57.348852: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cf100 of size 1024 next 285\r\n2021-03-15 17:16:57.348893: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cf500 of size 1024 next 286\r\n2021-03-15 17:16:57.348937: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cf900 of size 1024 next 287\r\n2021-03-15 17:16:57.348980: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009cfd00 of size 1024 next 289\r\n2021-03-15 17:16:57.349022: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d0100 of size 1024 next 290\r\n2021-03-15 17:16:57.349064: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d0500 of size 1024 next 291\r\n2021-03-15 17:16:57.349106: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d0900 of size 1024 next 292\r\n2021-03-15 17:16:57.349148: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d0d00 of size 1024 next 294\r\n2021-03-15 17:16:57.349190: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d1100 of size 1024 next 295\r\n2021-03-15 17:16:57.349232: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d1500 of size 1024 next 296\r\n2021-03-15 17:16:57.349274: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d1900 of size 1024 next 297\r\n2021-03-15 17:16:57.349316: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d1d00 of size 1024 next 299\r\n2021-03-15 17:16:57.349358: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d2100 of size 1024 next 300\r\n2021-03-15 17:16:57.349400: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d2500 of size 1024 next 301\r\n2021-03-15 17:16:57.349443: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d2900 of size 1024 next 302\r\n2021-03-15 17:16:57.349528: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d2d00 of size 2048 next 303\r\n2021-03-15 17:16:57.349577: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d3500 of size 2048 next 305\r\n2021-03-15 17:16:57.349612: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d3d00 of size 2048 next 306\r\n2021-03-15 17:16:57.349636: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009d4500 of size 2048 next 307\r\n2021-03-15 17:16:57.349659: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] Free  at f009d4d00 of size 50176 next 81\r\n2021-03-15 17:16:57.349683: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f009e1100 of size 147456 next 73\r\n2021-03-15 17:16:57.349704: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00a05100 of size 147456 next 74\r\n2021-03-15 17:16:57.349725: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00a29100 of size 65536 next 92\r\n2021-03-15 17:16:57.349746: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00a39100 of size 93952 next 18446744073709551615\r\n2021-03-15 17:16:57.349767: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Next region of size 2097152\r\n2021-03-15 17:16:57.349789: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00a50000 of size 294912 next 53\r\n2021-03-15 17:16:57.349810: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00a98000 of size 65536 next 142\r\n2021-03-15 17:16:57.349829: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00aa8000 of size 65536 next 152\r\n2021-03-15 17:16:57.349848: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00ab8000 of size 65536 next 162\r\n2021-03-15 17:16:57.349868: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00ac8000 of size 65536 next 172\r\n2021-03-15 17:16:57.349887: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00ad8000 of size 65536 next 182\r\n2021-03-15 17:16:57.349906: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00ae8000 of size 65536 next 192\r\n2021-03-15 17:16:57.349926: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] Free  at f00af8000 of size 196608 next 119\r\n2021-03-15 17:16:57.349947: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00b28000 of size 589824 next 120\r\n2021-03-15 17:16:57.349981: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00bb8000 of size 262144 next 198\r\n2021-03-15 17:16:57.350005: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00bf8000 of size 360448 next 18446744073709551615\r\n2021-03-15 17:16:57.350027: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Next region of size 4194304\r\n2021-03-15 17:16:57.350051: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c50000 of size 1024 next 197\r\n2021-03-15 17:16:57.350076: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c50400 of size 1024 next 199\r\n2021-03-15 17:16:57.350101: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c50800 of size 1024 next 200\r\n2021-03-15 17:16:57.350126: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c50c00 of size 2048 next 204\r\n2021-03-15 17:16:57.350152: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c51400 of size 2048 next 205\r\n2021-03-15 17:16:57.350179: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c51c00 of size 2048 next 206\r\n2021-03-15 17:16:57.350206: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c52400 of size 2048 next 207\r\n2021-03-15 17:16:57.350233: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c52c00 of size 1024 next 210\r\n2021-03-15 17:16:57.350260: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c53000 of size 1024 next 211\r\n2021-03-15 17:16:57.350288: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c53400 of size 1024 next 212\r\n2021-03-15 17:16:57.350315: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c53800 of size 1024 next 213\r\n2021-03-15 17:16:57.350342: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c53c00 of size 1024 next 215\r\n2021-03-15 17:16:57.350371: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c54000 of size 1024 next 216\r\n2021-03-15 17:16:57.350400: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c54400 of size 1024 next 217\r\n2021-03-15 17:16:57.350428: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c54800 of size 1024 next 218\r\n2021-03-15 17:16:57.350457: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c54c00 of size 1024 next 220\r\n2021-03-15 17:16:57.350486: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c55000 of size 1024 next 221\r\n2021-03-15 17:16:57.350515: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c55400 of size 1024 next 222\r\n2021-03-15 17:16:57.350544: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c55800 of size 1024 next 223\r\n2021-03-15 17:16:57.350573: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c55c00 of size 1024 next 225\r\n2021-03-15 17:16:57.350602: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c56000 of size 1024 next 226\r\n2021-03-15 17:16:57.350630: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c56400 of size 1024 next 227\r\n2021-03-15 17:16:57.350659: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c56800 of size 1024 next 228\r\n2021-03-15 17:16:57.350688: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c56c00 of size 1024 next 230\r\n2021-03-15 17:16:57.350716: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c57000 of size 1024 next 231\r\n2021-03-15 17:16:57.350745: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c57400 of size 1024 next 232\r\n2021-03-15 17:16:57.350774: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c57800 of size 1024 next 233\r\n2021-03-15 17:16:57.350803: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c57c00 of size 1024 next 235\r\n2021-03-15 17:16:57.350832: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c58000 of size 1024 next 236\r\n2021-03-15 17:16:57.350861: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c58400 of size 1024 next 237\r\n2021-03-15 17:16:57.350890: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c58800 of size 1024 next 238\r\n2021-03-15 17:16:57.350919: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c58c00 of size 1024 next 239\r\n2021-03-15 17:16:57.350948: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c59000 of size 1024 next 240\r\n2021-03-15 17:16:57.350976: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c59400 of size 1024 next 241\r\n2021-03-15 17:16:57.351005: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c59800 of size 1024 next 242\r\n2021-03-15 17:16:57.351034: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c59c00 of size 1024 next 244\r\n2021-03-15 17:16:57.351063: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5a000 of size 1024 next 245\r\n2021-03-15 17:16:57.351091: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5a400 of size 1024 next 246\r\n2021-03-15 17:16:57.351120: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5a800 of size 1024 next 247\r\n2021-03-15 17:16:57.351149: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5ac00 of size 1024 next 249\r\n2021-03-15 17:16:57.351178: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5b000 of size 1024 next 250\r\n2021-03-15 17:16:57.351206: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5b400 of size 1024 next 251\r\n2021-03-15 17:16:57.351235: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5b800 of size 1024 next 252\r\n2021-03-15 17:16:57.351264: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5bc00 of size 1024 next 255\r\n2021-03-15 17:16:57.351293: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5c000 of size 1024 next 256\r\n2021-03-15 17:16:57.351322: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5c400 of size 1024 next 257\r\n2021-03-15 17:16:57.351352: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5c800 of size 1024 next 258\r\n2021-03-15 17:16:57.351383: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5cc00 of size 1024 next 260\r\n2021-03-15 17:16:57.351414: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5d000 of size 1024 next 261\r\n2021-03-15 17:16:57.351444: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5d400 of size 1024 next 262\r\n2021-03-15 17:16:57.351474: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5d800 of size 1024 next 263\r\n2021-03-15 17:16:57.351504: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5dc00 of size 1024 next 265\r\n2021-03-15 17:16:57.351533: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5e000 of size 1024 next 266\r\n2021-03-15 17:16:57.351564: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5e400 of size 1024 next 267\r\n2021-03-15 17:16:57.351594: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5e800 of size 1024 next 268\r\n2021-03-15 17:16:57.351624: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5ec00 of size 1024 next 270\r\n2021-03-15 17:16:57.351654: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5f000 of size 1024 next 271\r\n2021-03-15 17:16:57.351683: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5f400 of size 1024 next 272\r\n2021-03-15 17:16:57.351711: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5f800 of size 1024 next 273\r\n2021-03-15 17:16:57.351740: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c5fc00 of size 1024 next 114\r\n2021-03-15 17:16:57.351769: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c60000 of size 65536 next 103\r\n2021-03-15 17:16:57.351799: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c70000 of size 131072 next 104\r\n2021-03-15 17:16:57.351828: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00c90000 of size 131072 next 109\r\n2021-03-15 17:16:57.351857: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00cb0000 of size 786432 next 97\r\n2021-03-15 17:16:57.351887: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00d70000 of size 1179648 next 98\r\n2021-03-15 17:16:57.351916: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00e90000 of size 589824 next 137\r\n2021-03-15 17:16:57.351944: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00f20000 of size 262144 next 229\r\n2021-03-15 17:16:57.351973: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] Free  at f00f60000 of size 327680 next 147\r\n2021-03-15 17:16:57.352002: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f00fb0000 of size 655360 next 18446744073709551615\r\n2021-03-15 17:16:57.352030: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Next region of size 8388608\r\n2021-03-15 17:16:57.352059: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01050000 of size 589824 next 158\r\n2021-03-15 17:16:57.352088: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f010e0000 of size 589824 next 167\r\n2021-03-15 17:16:57.352116: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01170000 of size 589824 next 177\r\n2021-03-15 17:16:57.352145: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01200000 of size 589824 next 187\r\n2021-03-15 17:16:57.352174: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01290000 of size 262144 next 248\r\n2021-03-15 17:16:57.352203: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f012d0000 of size 262144 next 259\r\n2021-03-15 17:16:57.352232: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01310000 of size 262144 next 269\r\n2021-03-15 17:16:57.352260: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01350000 of size 262144 next 278\r\n2021-03-15 17:16:57.352289: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01390000 of size 262144 next 288\r\n2021-03-15 17:16:57.352317: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f013d0000 of size 262144 next 298\r\n2021-03-15 17:16:57.352346: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] Free  at f01410000 of size 786432 next 224\r\n2021-03-15 17:16:57.352376: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f014d0000 of size 3670016 next 18446744073709551615\r\n2021-03-15 17:16:57.352403: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Next region of size 16777216\r\n2021-03-15 17:16:57.352432: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] Free  at f01850000 of size 262144 next 219\r\n2021-03-15 17:16:57.352462: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01890000 of size 262144 next 208\r\n2021-03-15 17:16:57.352492: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f018d0000 of size 524288 next 209\r\n2021-03-15 17:16:57.352672: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01950000 of size 524288 next 214\r\n2021-03-15 17:16:57.352704: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f019d0000 of size 1048576 next 304\r\n2021-03-15 17:16:57.352734: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] Free  at f01ad0000 of size 2097152 next 202\r\n2021-03-15 17:16:57.352763: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f01cd0000 of size 4718592 next 203\r\n2021-03-15 17:16:57.352793: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f02150000 of size 2359296 next 234\r\n2021-03-15 17:16:57.352823: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f02390000 of size 2359296 next 243\r\n2021-03-15 17:16:57.352853: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] Free  at f025d0000 of size 2621440 next 18446744073709551615\r\n2021-03-15 17:16:57.352881: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Next region of size 33554432\r\n2021-03-15 17:16:57.352911: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f02850000 of size 2359296 next 254\r\n2021-03-15 17:16:57.352942: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f02a90000 of size 2359296 next 264\r\n2021-03-15 17:16:57.352971: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f02cd0000 of size 2359296 next 274\r\n2021-03-15 17:16:57.353003: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f02f10000 of size 2359296 next 283\r\n2021-03-15 17:16:57.353033: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f03150000 of size 2359296 next 293\r\n2021-03-15 17:16:57.353065: I tensorflow/core/common_runtime/bfc_allocator.cc:1031] InUse at f03390000 of size 21757952 next 18446744073709551615\r\n2021-03-15 17:16:57.353093: I tensorflow/core/common_runtime/bfc_allocator.cc:1036]      Summary of in-use Chunks by size: \r\n2021-03-15 17:16:57.353132: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 64 Chunks of size 256 totalling 16.0KiB\r\n2021-03-15 17:16:57.353170: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 84 Chunks of size 512 totalling 42.0KiB\r\n2021-03-15 17:16:57.353202: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 83 Chunks of size 1024 totalling 83.0KiB\r\n2021-03-15 17:16:57.353235: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 1280 totalling 1.2KiB\r\n2021-03-15 17:16:57.353266: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 1536 totalling 1.5KiB\r\n2021-03-15 17:16:57.353298: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 8 Chunks of size 2048 totalling 16.0KiB\r\n2021-03-15 17:16:57.353330: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 3584 totalling 3.5KiB\r\n2021-03-15 17:16:57.353361: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 8192 totalling 8.0KiB\r\n2021-03-15 17:16:57.353393: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 5 Chunks of size 16384 totalling 80.0KiB\r\n2021-03-15 17:16:57.353426: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 24576 totalling 24.0KiB\r\n2021-03-15 17:16:57.353459: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 2 Chunks of size 32768 totalling 64.0KiB\r\n2021-03-15 17:16:57.353527: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 49152 totalling 48.0KiB\r\n2021-03-15 17:16:57.353563: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 9 Chunks of size 65536 totalling 576.0KiB\r\n2021-03-15 17:16:57.353600: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 2 Chunks of size 73728 totalling 144.0KiB\r\n2021-03-15 17:16:57.353627: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 93952 totalling 91.8KiB\r\n2021-03-15 17:16:57.353649: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 2 Chunks of size 131072 totalling 256.0KiB\r\n2021-03-15 17:16:57.353670: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 2 Chunks of size 147456 totalling 288.0KiB\r\n2021-03-15 17:16:57.353690: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 9 Chunks of size 262144 totalling 2.25MiB\r\n2021-03-15 17:16:57.353710: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 294912 totalling 288.0KiB\r\n2021-03-15 17:16:57.353730: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 360448 totalling 352.0KiB\r\n2021-03-15 17:16:57.353750: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 2 Chunks of size 524288 totalling 1.00MiB\r\n2021-03-15 17:16:57.353769: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 6 Chunks of size 589824 totalling 3.38MiB\r\n2021-03-15 17:16:57.353788: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 655360 totalling 640.0KiB\r\n2021-03-15 17:16:57.353807: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 786432 totalling 768.0KiB\r\n2021-03-15 17:16:57.353826: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 1048576 totalling 1.00MiB\r\n2021-03-15 17:16:57.353845: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 1179648 totalling 1.12MiB\r\n2021-03-15 17:16:57.353864: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 7 Chunks of size 2359296 totalling 15.75MiB\r\n2021-03-15 17:16:57.353882: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 3670016 totalling 3.50MiB\r\n2021-03-15 17:16:57.353901: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 4718592 totalling 4.50MiB\r\n2021-03-15 17:16:57.353920: I tensorflow/core/common_runtime/bfc_allocator.cc:1039] 1 Chunks of size 21757952 totalling 20.75MiB\r\n2021-03-15 17:16:57.353940: I tensorflow/core/common_runtime/bfc_allocator.cc:1043] Sum Total of in-use chunks: 56.95MiB\r\n2021-03-15 17:16:57.353957: I tensorflow/core/common_runtime/bfc_allocator.cc:1045] total_region_allocated_bytes_: 66060288 memory_limit_: 70307840 available bytes: 4247552 curr_region_allocation_bytes_: 67108864\r\n2021-03-15 17:16:57.394864: I tensorflow/core/common_runtime/bfc_allocator.cc:1051] Stats: \r\nLimit:                        70307840\r\nInUse:                        59718656\r\nMaxInUse:                     59718656\r\nNumAllocs:                         419\r\nMaxAllocSize:                 21757952\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2021-03-15 17:16:57.395156: W tensorflow/core/common_runtime/bfc_allocator.cc:444] **********************xx****__****************___***********************************************xxxx\r\n2021-03-15 17:16:57.584017: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cwise_ops_common.h:128 : Resource exhausted: OOM when allocating tensor with shape[3,3,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"/home/swag/Desktop/IGVC_ConeDetectionScript/detect_independent.py\", line 17, in <module>\r\n    yolo = Load_Yolo_model()\r\n  File \"/home/swag/Desktop/IGVC_ConeDetectionScript/utils.py\", line 41, in Load_Yolo_model\r\n    yolo = Create_Yolo(input_size=YOLO_INPUT_SIZE, CLASSES=\"classes.txt\")\r\n  File \"/home/swag/Desktop/IGVC_ConeDetectionScript/utils.py\", line 18, in Create_Yolo\r\n    conv_tensors = YOLOv4(input_layer, NUM_CLASS)\r\n  File \"/home/swag/Desktop/IGVC_ConeDetectionScript/yolov4.py\", line 289, in YOLOv4\r\n    route_1, route_2, conv = cspdarknet53(input_layer)\r\n  File \"/home/swag/Desktop/IGVC_ConeDetectionScript/yolov4.py\", line 155, in cspdarknet53\r\n    input_data = convolutional(input_data, (3, 3, 512, 1024), downsample=True, activate_type=\"mish\")\r\n  File \"/home/swag/Desktop/IGVC_ConeDetectionScript/yolov4.py\", line 44, in convolutional\r\n    bias_initializer=tf.constant_initializer(0.))(input_layer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 952, in __call__\r\n    input_list)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1091, in _functional_construction_call\r\n    inputs, input_masks, args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 862, in _infer_output_signature\r\n    self._maybe_build(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 2710, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 205, in build\r\n    dtype=self.dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 639, in add_weight\r\n    caching_device=caching_device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\", line 810, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 260, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 221, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 199, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2618, in default_variable_creator\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1585, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1712, in _init_from_args\r\n    initial_value = initial_value()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\", line 420, in __call__\r\n    dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\", line 1073, in random_normal\r\n    shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=self.seed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\", line 96, in random_normal\r\n    mul = rnd * stddev_tensor\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1164, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1496, in _mul_dispatch\r\n    return multiply(x, y, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 518, in multiply\r\n    return gen_math_ops.mul(x, y, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 6068, in mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[3,3,512,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]\r\n\r\nProcess finished with exit code 1", "@AbhisekOmkar,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n- the complete code and the dataset you are using\r\n\r\nAlso try limiting GPU memory growth using any of the methods listed [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it helps. Thanks!", "> @AbhisekOmkar,\r\n> In order to expedite the trouble-shooting process, could you please provide the following information\r\n> \r\n> * CUDA/cuDNN version:\r\n> * GPU model and memory:\r\n> * the complete code and the dataset you are using\r\n> \r\n> Also try limiting GPU memory growth using any of the methods listed [here](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it helps. Thanks!\r\n\r\nGPU: Nvidia Jetson Nano NVIDIA Maxwell architecture with 128 NVIDIA CUDA\u00ae cores\r\nCUDA: 11.2\r\nCODE: https://github.com/Project-SwaG/IGVC_ConeDetectionScript\r\n", "Did you build TF from sources to support cuda 11.2?\r\nAlso have you tried limiting gpu memory growth? Make sure you close all python sessions and exit python interpreter to free up memory and try running script as:\r\n```python\r\n#On top of your script\r\nimport tensorflow as tf\r\ngpu = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpu[0], True) #limits gpu memory\r\n# Rest of your code\r\n```\r\n", "Issues has been resolved (Jetson Nano has 4gb CPU and Tensorflow was using more than 4gb of CPU during Inference. So, we finally Shift from Tensorflow to Tensorflow RT or You can Switch to Higher versions of Jetson).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47820\">No</a>\n"]}, {"number": 47818, "title": "Add Resnet-18 and Resnet-34 in tf.keras.applications.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes, but would probably need help.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently there are multiple resnet variants available (50, 101, 152 + V2 options). \r\n\r\nThose model's are rather big and thus limiting the research and use of computer vision solutions on edge devices.\r\n\r\nAdding requested models would make it easier to research and prototype new solutions in Tensorflow.\r\n\r\n**Will this change the current api? How?**\r\nIt could be possible to use requested models in the same way as the `ResNet50` is called. An example could look like the following:\r\n```python\r\nresnet18 = tf.keras.applications.ResNet18(\r\n    include_top=True, weights='imagenet', input_tensor=None,\r\n    input_shape=None, pooling=None, classes=1000, **kwargs\r\n)\r\n\r\nresnet34 = tf.keras.applications.ResNet34(\r\n    include_top=True, weights='imagenet', input_tensor=None,\r\n    input_shape=None, pooling=None, classes=1000, **kwargs\r\n)\r\n```\r\n\r\n**Who will benefit with this feature?**\r\nAny user's who are working with smaller versions of computer vision models.\r\n\r\n**Any Other info.**\r\nThis issue expands already existing request for Resnet-34:\r\nhttps://github.com/tensorflow/tensorflow/issues/44099 \r\n", "comments": ["Raised a CL to fix this issue. Thanks!", "Sorry for the late response.\r\n\r\nThis is a duplicate. we can track the progress in keras-team/keras repo https://github.com/keras-team/keras/issues/15269\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!\r\n\r\nI will close this issue here and please track this feature development in keras-team/keras repo. Thanks!"]}, {"number": 47817, "title": "HEAD build wants bazel >= 3.7.4 and <4.0.0 which are unavailable", "body": "**System information**\r\n- OS Platform and Distribution: Debian 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 4.0.0\r\n- GCC/Compiler version (if compiling from source): 8.3\r\n\r\n**Describe the problem**\r\n\r\nNightly builds of Tensorflow want Bazel >= 3.7.4 so at Linaro CI we fetched 3.7.4 and built TF. Worked fine.\r\n\r\nUpstream dropped Bazel 3.7.4 release so now 3.7.2 and 4.0.0 are available on their Github releases page. We moved to 4.0.0 and then TF complains that it is too new.\r\n\r\nSo at the moment Tensorflow nightly is not buildable on our CI.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFetch bazel 4.0.0, build and install. Then build Tensorflow.\r\n\r\n**Any other info / logs**\r\n\r\n00:50:24.871     spawn ./configure\r\n00:50:24.871     You have bazel 4.0.0- (@non-git) installed.\r\n00:50:24.871     Please downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n\r\nWhole log: https://ci.linaro.org/job/ldcg-python-tensorflow-nightly/nodes=docker-buster-arm64-ldcg/116/console", "comments": ["Coworker pointed me to 3.7.2 being minimal.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47817\">No</a>\n"]}, {"number": 47814, "title": "micro: port op L2_POOL_2D from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator L2_POOL_2D from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1 (step 1): Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2 (step 2): Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\n\r\nThe next 3 steps are combined into a single PR3 with separate commits:\r\n\r\n(step 3): Copy operator from lite to micro making minimal changes and not including in the build\r\n(step 4): Delete extra code from the micro copy of the operator\r\n(step 5): Port micro copy of operator as necessary and add a corresponding test", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47814\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47814\">No</a>\n"]}, {"number": 47813, "title": "Tensorflow lite lstm/bilstm reset variables why needed ? Can we avoid reset variables. ", "body": "Getting different outputs for multiple runs in tensorflow lite invoke when reset is not called. Why different output ? Is this step causes performance issue in multiple runs because of reset function extra step during every run. why interpreter.reset_all_variables() is needed.\r\n\r\n`# -*- coding: utf-8 -*-\r\n\"\"\"BiLSTMinput_Var_LastMode_layer.ipynb\r\n\r\nAutomatically generated by Colaboratory.\r\n\r\nOriginal file is located at\r\n    https://colab.research.google.com/drive/1xTKC1yQ4S016p_yFFFm91TCJlHj7nnDf\r\n\"\"\"\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nmodel = keras.Sequential()\r\n\r\n# Add a LSTM layer with 128 internal units.\r\nmodel.add(layers.Bidirectional(layers.LSTM(input_shape=(None,20),units= 128, return_sequences = False)))\r\n\r\n# Step 2: Train & Evaluate the model.\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy']);\r\nx_test = np.zeros([3000,4,20], dtype=np.float32);\r\nx_train = np.zeros([3000,4,20], dtype=np.float32);\r\ny_test =  np.zeros([3000, 1,], dtype=np.float32);\r\ny_train = np.zeros([3000, 1,], dtype=np.float32);\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)\r\n\r\nmodel.summary()\r\n\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nSTEPS = 2\r\nINPUT_SIZE = 20\r\n\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n\r\n# model directory.\r\nMODEL_DIR = \"keras_bi_lstm_new\"\r\nmodel.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\ntflite_model = converter.convert()\r\n\r\nwith open('bilstmmodel_1_2_20.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n# Run the model with TensorFlow Lite\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n#interpreter = tf.lite.Interpreter(model_path=\"bilstmmodel_last_1_2_20.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\n\r\noutput_shape = output_details[0]['shape']\r\nprint(input_shape) \r\nprint(output_shape)\r\n\r\nmyinp = np.loadtxt('img.txt')\r\n\r\nmyinp = np.resize(myinp,input_shape);\r\nmyinp = (np.float32(myinp) - 0) / 1\r\nprint(myinp)\r\n\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0][\"index\"], myinp)\r\ninterpreter.invoke()\r\nresult = interpreter.get_tensor(output_details[0][\"index\"])\r\nprint(result)\r\ninterpreter.invoke()\r\nresult = interpreter.get_tensor(output_details[0][\"index\"])\r\nprint(result)\r\n\r\ninterpreter.reset_all_variables()\r\ninterpreter.invoke()\r\nresult = interpreter.get_tensor(output_details[0][\"index\"])\r\nprint(result)\r\nprint(result.shape)`", "comments": ["@pranathibl \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "@renjie-liu could you take a look at this?", "Yes, the reset variable call is expected, the lstm kernel in tflite is stateful (meaning the states will be mutated and stayed there so you don't need to copy and pass in again).", "Is there any way to avoid based on network type. For CNN's we dont need that right ?", "Yes, for CNN network you don't need to do that", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47813\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47813\">No</a>\n"]}, {"number": 47812, "title": "Does anymore know how to install for tensorflow version 1.01 in google colabs ", "body": "", "comments": ["@jackson119 All you have to do in colab is ```'!pip install tensorflow==2.4```\r\nYou may refer to [link](https://colab.research.google.com/notebooks/tensorflow_version.ipynb), [link1](https://medium.com/@tuxnani/install-tensorflow-2-0-in-colab-36f778945a48),.\r\nPlease note tf 1.x version is not supported, please use 2.x, 2.4 is the stable version.\r\n\r\nIf this answers your query, move this issue to closed status.", "so no more tensorflow 1 in google colabs ? okay really appreciate your  helps", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47812\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47812\">No</a>\n", "@jackson119 \r\nYou can pip install tf 1.x, but there is no support fot it, you can use 2.x.\r\nIs there any particular reason to use 1.x.\r\nPlease move this to closed status as you have reopened it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47812\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47812\">No</a>\n"]}, {"number": 47811, "title": "Stabilize the order of definitions in schema_py_generated.py", "body": "This makes it reproducible.\r\n\r\n---\r\n\r\nNote: I have not tested this on Macos and Windows.", "comments": []}, {"number": 47810, "title": "Sort and deduplicate generated _names_with_underscore", "body": "This makes the generated code more readable and deterministic.\r\n\r\n---\r\n\r\nExample before (in `tensorflow/__init__.py`):\r\n```(python)\r\n_names_with_underscore = ['__version__', '__git_version__', '__compiler_version__', '__cxx11_abi_flag__', '__monolithic_build__', '__internal__', '__internal__', '__internal__', '__operators__', '__internal__', '__internal__', '__internal__']\r\n```\r\nafter:\r\n```(python)\r\n_names_with_underscore = ['__compiler_version__', '__cxx11_abi_flag__', '__git_version__', '__internal__', '__monolithic_build__', '__operators__', '__version__']\r\n```", "comments": []}, {"number": 47809, "title": "silently crash", "body": "Folks,\r\n\r\nI have a strange issue that the latest night build never starts training nor I see any stack traces.\r\n\r\n```\r\nBuild\r\n2.5.0-dev20210314\r\n```\r\n\r\nit sees GPU\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\n2021-03-15 08:32:21.311250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n>>> print(device_lib.list_local_devices())\r\n2021-03-15 08:32:23.915414: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the f\r\nollowing CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-15 08:32:23.918372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-15 08:32:23.942584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-03-15 08:32:23.942998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-15 08:32:23.971817: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-15 08:32:23.972051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-15 08:32:23.989838: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-15 08:32:23.995070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-15 08:32:24.000006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-15 08:32:24.024262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-15 08:32:24.025408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-15 08:32:24.025666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0\r\n2021-03-15 08:32:24.527083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-15 08:32:24.527332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1310]      0\r\n2021-03-15 08:32:24.527503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1323] 0:   N\r\n2021-03-15 08:32:24.527805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Created TensorFlow device (/device:GPU:0 with 6629 MB memory) -> physical GPU (device: 0, nam\r\ne: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 17497860804910539641\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 6951272448\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 16814837068753917860\r\nphysical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1\"\r\n]\r\n```\r\n\r\n* System window 10 all update.\r\n\r\n* Visual Studio Code 2019.\r\n\r\n* All Cuda test 11.22 passed.\r\n\r\n```\r\ntest.py\r\n2021-03-15 08:41:55.464473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-15 08:41:57.975692: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-15 08:41:58.002676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-03-15 08:41:58.003092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-15 08:41:58.032297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-15 08:41:58.032535: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-15 08:41:58.050106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-15 08:41:58.055219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-15 08:41:58.059974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-15 08:41:58.079919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-15 08:41:58.080858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-15 08:41:58.081073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n2021-03-15 08:41:58.567984: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the f\r\nollowing CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-15 08:41:58.569479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1779] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-03-15 08:41:58.569775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1917] Adding visible gpu devices: 0\r\n2021-03-15 08:41:59.048119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-15 08:41:59.048383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1310]      0\r\n2021-03-15 08:41:59.048591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1323] 0:   N\r\n2021-03-15 08:41:59.049151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6629 MB memory)\r\n-> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2021-03-15 08:41:59.053009: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-15 08:41:59.332823: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-15 08:41:59.333765: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-15 08:41:59.334101: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-15 08:41:59.334533: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-15 08:41:59.334936: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-15 08:41:59.341584: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-15 08:41:59.342176: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\nCode I run\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\ntf.debugging.set_log_device_placement(True)\r\n\r\ntf.autograph.set_verbosity(5)\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dropout(0.2),\r\n      tf.keras.layers.Dense(10)\r\n            ])\r\n\r\npredictions = model(x_train[:1]).numpy()\r\nprint(predictions)\r\n\r\ntf.nn.softmax(predictions).numpy()\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nloss_fn(y_train[:1], predictions).numpy()\r\n\r\nmodel.compile()\r\nmodel.summary()\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test,  y_test, verbose=2)\r\n\r\nprobability_model = tf.keras.Sequential([\r\n      model, tf.keras.layers.Softmax()\r\n        ])\r\nprobability_model(x_test[:5])\r\n```\r\n", "comments": ["@spyroot,\r\nOn changing `model.compile()` to `model.compile(optimizer='adam',loss=loss_fn,metrics=['accuracy'])`, I was able to run the code without any issues. \r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dc7e6ac5eb3a5391a402e5202941b7e0/47809-tf-nightly.ipynb). Thanks!", "I changed the library to 2.4.1 and I have no issue with the latest CUDA 11.2 etc.  So 2.5 I think has some strange issues.  \r\nNote the example I provided.  I just took the most basic one to make sure it not my code.  So I have a bunch of \r\nother code that behaves exactly the same.\r\n\r\nI have orthogonal question why don't you just ask for a location for Cuda and resolve symbols by loading DLLs.\r\nI'm not a window guy but normally with a shared object, so it trivially to implement and check if all symbols resolve.\r\nit looks strange that you actually searching for an exact match for dll file name, while it looks like Nvidia can't come up with a generic name for dll :))) \r\n\r\ni.e you don't need to have this env option, just pass location for Cuda,  resolve all DLL's for \r\nall required symbols in one recursive pass.  \r\n", "@spyroot Is this still an issue for you? Can you please try  (`TF2.5` and `tf-nightly`) and let us know if this is still an issue for you. I couldn't test on Windows with GPU. \r\n\r\nOn Colab, it throws a user error when `compile()` is used. Replacing `compile()` with `model.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])`, your code runs without any issue. \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/9f9e09b1693fb4f508382d1bbc0afe7e/47809-tf-nightly.ipynb) is a gist for a reference. Thanks!", "As point out by @jvishnuvardhan you need to pass an optimizer and  a loss to the model. The error you get if you don't is:\r\n\r\n```\r\nValueError: No gradients provided for any variable\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47809\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47809\">No</a>\n"]}, {"number": 47808, "title": "Update mobilenet_v3.py", "body": "Hi, I found that normalization operation is included in the definition of mobilenetv3 model and Mobile netv2 doesn't do that.\r\nI don't think include normalization in the model a good idea, it should be outside the model. First of all, it limits the ability to customize. Secondly, I find that the normalization of 255 directly results in the accuracy of the model almost reduced to random after the model is converted to tflite int-8 version. \r\nSo I think it's better to remove it in the model.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47808) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 47807, "title": "The GPU memory allocate logic bug in tensorflow 1.15", "body": "<em>\r\nHello, I tried to upgrade my code from tensorflow 1.12 to tensorflow 1.15. Everything is ok in tf1.12, but I found out some gpu memory allocate logic bug in tf1.15. \r\n\r\nHere are some details: \r\n\r\n1.I fail to use allow_growth=False, tensorflow report \"Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\", but in server of my lab, everything is ok, so I guess that maybe it relates to the gpu memory my local computer occupy, e.g. compiz, Xorg. \r\n\r\n2.When i set the allow_growth=True, the code works in tf 1.15, but the gpu memory-usae is so weired, I think gpu memory-usage is linear function of batch_size, but not directly proportional in tensorflow 1.15 which is different from tensorflow 1.12. In tf1.15, the gpu memory-usage is 10G when batchsize=1, 11G when batchsize=2, 19G when batchsize=3, and 17G wen batch size =4. \r\n\r\nIn conclusion, I guess that maybe some gpu memory allocate bugs exist in tensorflow 1.15. \r\n\r\n</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04)\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):tensorflow 1.15.\r\n- Python version:3.6.13\r\n- Bazel version (if compiling from source):1.0.0\r\n- GCC/Compiler version (if compiling from source):7.5.0\r\n- CUDA/cuDNN version:cuda:10 cudnn 7.6.0\r\n- GPU model and memory: 32G tesla V100 (server)and 12G RTX 2080 (local)\r\n\r\n", "comments": ["@alexyinxuanyu \r\nThere is no support for tf 1.x, please upgrade to 2.x and let us know.\r\n[[System requirement](https://github.com/tensorflow/tensorflow/issues/47720#issuecomment-797249831) for the same #47517]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47807\">No</a>\n"]}, {"number": 47805, "title": "Hello World Example not supported in Himax WE-1 board. ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Himax WE-1\r\n\r\nHi,\r\n\r\nI'm curious why TFLM doesn't support the Hello World example of the Himax WE-1 board. Until January of this year, we have confirmed that TFLM supports the Hello World example of the Himax WE-1 board. I am curious about the reason for not applying at this time. (The Arduino IDE still supports the Hello World example.)\r\n\r\nThanks \r\n\r\n", "comments": ["@SunBeenMoon   This is a micro related issue , Please post this in [micro repository](https://github.com/tensorflow/tflite-micro/issues) & move this issue to closed status.. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47805\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47805\">No</a>\n"]}, {"number": 47804, "title": "Quadratic slowdown on LSTM models in tflite", "body": "**System information**\r\nThis reproduces on both colab and my local machine. I assume colab is more standard and thus more interesting, but my local info for completeness:\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a bit!\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1 (also reproduces on nightly)\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nLSTM models experience quadratic slowdown when converted to tflite.\r\n\r\nThe attached notebook shows the behaviour clearly: when using a standard keras model for predictions, the prediction latency is linear in the length of the sequence provided, as expected. When the same model is converted to tflite, the performance is initially good but scales up quadratically with the input size, and becomes effectively unusable for large sequences.\r\n\r\n**Describe the expected behavior**\r\nI would expect performance to scale linearly with sequence length in tflite, as it does in keras, and as you'd expect from the underlying maths.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1APyFWSL3ln5gZFEotw_5wwUDpaSJrzD-\r\n\r\n**Other info / logs**\r\nThe notebook shows the context pretty clearly, but I'm happy to provide more info if needed!\r\n", "comments": ["@renjie-liu could you take a look at this performance experiments?", "Can you try set fixed input shapes as shown in here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb\r\n\r\nThanks!", "@renjie-liu - I've updated the notebook, and can confirm that with fixed input shapes it is now linear, and quite a bit faster in absolute terms.\r\n\r\nFirst: thanks very much for the pointer! I believe this unblocks my application.\r\n\r\nIs the quadratic behaviour of the unoptimized case expected / is that something that could be fixed?", "That one seems to be the unfused case.\r\n\r\nHaoliang may help take a look? thanks!", "The quadratic time was coming from the underlying tensorlist I believe. Currently a TensorListSetItem is written to two concatenation op, it will have O(n) complexity for one single operation (where n is the sequence length). This will be a problem when the sequence length becomes very long in general. And great that fused LSTM is working in this case!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Would it make sense to keep this open to track the quadratic complexity in the unfused case, or should I let the butler close it?", "Hi, we currently don't have plans to optimize the unfused quadratic complexity issue. Are you able to use fused LSTM in tflite instead?\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/Ub4apUvblN8", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yeah, I'm personally unblocked for now.\r\n\r\nI still think this would be _good_ to fix or document -- the quadratic complexity for this operation is quite surprising, given both the vanilla tf and theoretical behaviour, and it took a while to figure out what was going on -- but I understand that time is limited, and if you don't think you'll get to it it seems fine to close. (And thanks again for the help!)", "Hrm, actually, turns out I am _not_ able to use the fused LSTM... it causes an unacceptable accuracy loss for my problem. (In the example I'm testing out now, the error rate goes from ~0.13 to ~0.30 when I fix the input shape.)\r\n\r\nIs that a known issue? If not, happy to open another issue... seems unrelated to the quadratic complexity.", "You can try flex tensor list op instead with the recent tf nightly version.\r\n\r\n```\r\nconverter._experimental_lower_tensor_list_ops = False\r\n```", "TFLite fused LSTM is stateful, so you will need to call `reset_all_variables`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/interpreter.py#L847-L848\r\n\r\nbetween each innovation.", "> TFLite fused LSTM is stateful, so you will need to call reset_all_variables: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/interpreter.py#L847-L848\r\n> between each innovation.\r\n\r\nYes, can confirm I'm doing this correctly. (Error rate is closer to 0.7-0.8 without it.)\r\n\r\n> You can try flex tensor list op instead with the recent tf nightly version.\r\n\r\nI have not tried this! Will report back.", "Hmm, no appreciable difference unfortunately.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tmp)\r\nconverter._experimental_lower_tensor_list_ops = True\r\ntflite_model = converter.convert()\r\n```\r\n\r\nIs that the right usage?\r\n\r\nIf it matters for the optimization: my actual model is a stack of multiple bidirectional LSTM layers, followed by a softmax and CTC-decode. Can share code if folks are interested!", "I would recommend testing this with disabling lowering tensor list ops like the following example:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(tmp)\r\nconverter._experimental_lower_tensor_list_ops = False\r\ntflite_model = converter.convert()\r\n```", "Ah, apologies, I misunderstood... thanks! Performance is improved in that case but still quadratic. I've updated the notebook:\r\nhttps://colab.research.google.com/drive/1APyFWSL3ln5gZFEotw_5wwUDpaSJrzD-", "The numbers in the new graph looks like reasonable to me. It would be better to understand why this performance improvement is not suitable for your use cases. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue since the performance improvement has been achieved through Flex tensor list ops."]}, {"number": 47803, "title": "Port TFL operator GATHER to TFLM", "body": "PR4 for issue #45196. This PR aims to finish porting GATHER from TFL to TFLM.\r\n\r\nNotes:\r\n1. For input/output tensors, only data types float and int8_t are supported;\r\n2. For position tensors, only data types int32_t and int64_t are supported; (update March 15: int64_t caused TFLite Micro Internal CI build to fail, so it has been removed)\r\n3. The reference implementation in lite/kernels/internal/reference/gather.h was not used, due to the mismatched data type between members of TfLiteGatherParams and of GatherParams;\r\n4. While this PR was under development, the lite kernel Gather added code to handle batch_dims; that change has not been incorporated into the micro kernel (update March 16: the support for batch_dims has been added into the TFLM Gather).\r\n\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "As of April 5th, this PR has passed all checks. It has been approved, but it does not have the \"ready to pull\" label. Is there anything still missing?", "@rsun-bdti  Can you please resolve conflicts? Thanks!", "I don't know why, but this PR just can't get merged into the master for two months. Closing it now to start over.  "]}, {"number": 47802, "title": "module 'gast' has no attribute 'Constant'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code based on https://www.tensorflow.org/tutorials/structured_data/time_series\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windoes 10 pro build 19041.867\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (python pip)\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: v11.2 with cudnn64_8.dll\r\n- GPU model and memory: GeForce GTX 1080 Ti Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9525 MB memory)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nConsole output:\r\n2021-03-12 11:01:01.858750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n\r\n\tI'm currently running Tensorflow version 2.4.1\r\n\r\n2021-03-12 11:01:09.201806: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-12 11:01:09.202893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-12 11:01:09.222367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-03-12 11:01:09.222840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 11:01:09.231298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-12 11:01:09.231559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-12 11:01:09.236389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-12 11:01:09.238217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-12 11:01:09.243569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-12 11:01:09.247807: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-12 11:01:09.248947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-12 11:01:09.249278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-12 11:01:09.249799: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-12 11:01:09.251831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.607GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-03-12 11:01:09.252388: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-12 11:01:09.252646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-12 11:01:09.252901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-12 11:01:09.253149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-12 11:01:09.253374: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-12 11:01:09.253613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-12 11:01:09.253851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-12 11:01:09.254088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-12 11:01:09.254365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-12 11:01:10.247733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-12 11:01:10.247986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-12 11:01:10.248140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-12 11:01:10.248903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9525 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2021-03-12 11:01:10.250531: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 48\r\nInput indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\r\nLabel indices: [47]\r\nLabel column name(s): ['T (degC)']> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Constant'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert", "comments": ["@BrianC42,\r\nYou can safely ignore the warnings and suppress the messages by changing the log level at the start of the program  \r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \r\nimport tensorflow as tf\r\n```\r\n\r\nor you can set the [autograph verbosity](https://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity\r\n) level using the below the below code \r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nos.environ['AUTOGRAPH_VERBOSITY'] = 1\r\n```\r\n\r\nAlso, please take a look at similar issues [#46582](https://github.com/tensorflow/tensorflow/issues/46582#issuecomment-764795885) and [#45956](https://github.com/tensorflow/tensorflow/issues/45956#issuecomment-750906198) for reference.\r\n\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47802\">No</a>\n"]}, {"number": 47801, "title": "[TFLM] Added support for optimized CONV2D for CEVA-DSP BX1 and SP500 platforms", "body": "As described in https://github.com/tensorflow/tensorflow/issues/45607\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 47800, "title": "Tensorflow (Using TensorFlow backend.)", "body": "while running the INVASE code I got this error,\r\n\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Mahsa\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\Mahsa\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ctypes\\__init__.py\", line 364, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n How could I fix it? \r\nI have installed many Tersorflow libraries and check if one works or not, but noone has works!\r\n", "comments": ["@mahsajalali,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nalong with the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47800\">No</a>\n"]}, {"number": 47799, "title": "Process finished with exit code 132 (interrupted by signal 4: SIGILL)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.2.1 Apple M1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nAfter running my script I receive the following error: \"Process finished with exit code 132 (interrupted by signal 4: SIGILL)\"\r\n\r\n**Describe the expected behavior**\r\nI expect the script to run entirely\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\ntraining_dataset = pd.read_csv('/Users/--/Downloads/Archive/AAPL.csv')\r\nprint(training_dataset)\r\ntraining_set = training_dataset.iloc[:,1:-1].values\r\nprint(training_set)\r\nfrom sklearn.preprocessing import MinMaxScaler\r\n\r\nsc = MinMaxScaler(feature_range = (0, 1))\r\ntraining_set_scaled = sc.fit_transform(training_set)\r\nprint(training_set_scaled)\r\n\r\nX_train = []\r\nY_train = []\r\n\r\nfor i in range(252,len(training_set)):\r\n    X_train.append(training_set_scaled[i - 252:i,0])\r\n    Y_train.append(training_set_scaled[i,0])\r\n\r\nX_train, Y_train = np.array(X_train), np.array(Y_train)\r\n\r\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import Dropout\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\r\n\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(model.add(LSTM(units = 50, return_sequences = True)))\r\n\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(model.add(LSTM(units = 50, return_sequences = True)))\r\n\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(model.add(LSTM(units = 50, return_sequences = True)))\r\n\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(Dense(units = 1))\r\n\r\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\r\n\r\nmodel.fit(X_train, Y_train, epochs = 100, batch_size = 32)\r\n\r\n", "comments": ["@eam2000 \r\nI ran the code shared and find some dependencies missing, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/524acfb5edf6f18c9f4c14e4bcd59ca0/untitled561.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47799\">No</a>\n"]}, {"number": 47798, "title": "Cuda Error for CNN", "body": "\r\n**CUDA Error with CNN algorithm, \r\nCUDA Toolkit 11.0 installed \r\nTensorFlow shows GPU and runs GPU very well in other algorithms but not in CNN.**\r\n\r\n**_https://keras.io/examples/audio/speaker_recognition_using_cnn/_** I am trying to run this code in my RTX 3070\r\n\r\n\r\n\r\n 'python' 'c:\\Users\\krish\\.vscode\\extensions\\ms-python.python-2021.2.633441544\\pythonFiles\\lib\\python\\debugpy\\launcher' '50523' '--' 'c:\\Users\\krish\\Desktop\\All_DL\\keras_site\\cnn_test.py'\r\n2021-03-14 21:26:45.065591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nFound 6 files belonging to 2 directories\r\n2021-03-14 21:26:47.470858: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set   \r\n2021-03-14 21:26:47.474642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-14 21:26:47.502462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-14 21:26:47.509399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-14 21:26:47.516038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-14 21:26:47.522950: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-14 21:26:47.530936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-14 21:26:47.542472: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-14 21:26:47.549435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-14 21:26:47.558565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-14 21:26:47.563129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-14 21:26:47.572197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-14 21:26:47.575148: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-14 21:26:47.593140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.77GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-14 21:26:47.608325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-14 21:26:47.611339: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-14 21:26:47.623072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-14 21:26:47.626485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-14 21:26:47.629093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-14 21:26:47.639563: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-14 21:26:47.645983: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-14 21:26:47.655138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-14 21:26:47.658821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-14 21:26:48.096628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-14 21:26:48.099998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-03-14 21:26:48.102166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-03-14 21:26:48.104363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6441 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-03-14 21:26:48.132896: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n6 noise files were split into 354 noise samples where each is 1 sec. long\r\nOur class names: ['Benjamin_Netanyau', 'Jens_Stoltenberg', 'Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela']\r\nProcessing speaker Benjamin_Netanyau\r\nProcessing speaker Jens_Stoltenberg\r\nProcessing speaker Julia_Gillard\r\nProcessing speaker Magaret_Tarcher\r\nProcessing speaker Nelson_Mandela\r\nFound 7501 files belonging to 5 classes.\r\nUsing 6751 files for training.\r\nUsing 750 files for validation.\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput (InputLayer)              [(None, 8000, 1)]    0\r\n__________________________________________________________________________________________________\r\nconv1d_1 (Conv1D)               (None, 8000, 16)     64          input[0][0]\r\n__________________________________________________________________________________________________\r\nactivation (Activation)         (None, 8000, 16)     0           conv1d_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_2 (Conv1D)               (None, 8000, 16)     784         activation[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d (Conv1D)                 (None, 8000, 16)     32          input[0][0]\r\n__________________________________________________________________________________________________\r\nadd (Add)                       (None, 8000, 16)     0           conv1d_2[0][0]\r\n                                                                 conv1d[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_1 (Activation)       (None, 8000, 16)     0           add[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling1d (MaxPooling1D)    (None, 4000, 16)     0           activation_1[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_4 (Conv1D)               (None, 4000, 32)     1568        max_pooling1d[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_2 (Activation)       (None, 4000, 32)     0           conv1d_4[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_5 (Conv1D)               (None, 4000, 32)     3104        activation_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_3 (Conv1D)               (None, 4000, 32)     544         max_pooling1d[0][0]\r\n__________________________________________________________________________________________________\r\nadd_1 (Add)                     (None, 4000, 32)     0           conv1d_5[0][0]\r\n                                                                 conv1d_3[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_3 (Activation)       (None, 4000, 32)     0           add_1[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling1d_1 (MaxPooling1D)  (None, 2000, 32)     0           activation_3[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_7 (Conv1D)               (None, 2000, 64)     6208        max_pooling1d_1[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_4 (Activation)       (None, 2000, 64)     0           conv1d_7[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_8 (Conv1D)               (None, 2000, 64)     12352       activation_4[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_5 (Activation)       (None, 2000, 64)     0           conv1d_8[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_9 (Conv1D)               (None, 2000, 64)     12352       activation_5[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_6 (Conv1D)               (None, 2000, 64)     2112        max_pooling1d_1[0][0]\r\n__________________________________________________________________________________________________\r\nadd_2 (Add)                     (None, 2000, 64)     0           conv1d_9[0][0]\r\n                                                                 conv1d_6[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_6 (Activation)       (None, 2000, 64)     0           add_2[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling1d_2 (MaxPooling1D)  (None, 1000, 64)     0           activation_6[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_11 (Conv1D)              (None, 1000, 128)    24704       max_pooling1d_2[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_7 (Activation)       (None, 1000, 128)    0           conv1d_11[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_12 (Conv1D)              (None, 1000, 128)    49280       activation_7[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_8 (Activation)       (None, 1000, 128)    0           conv1d_12[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_13 (Conv1D)              (None, 1000, 128)    49280       activation_8[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_10 (Conv1D)              (None, 1000, 128)    8320        max_pooling1d_2[0][0]\r\n__________________________________________________________________________________________________\r\nadd_3 (Add)                     (None, 1000, 128)    0           conv1d_13[0][0]\r\n                                                                 conv1d_10[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_9 (Activation)       (None, 1000, 128)    0           add_3[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling1d_3 (MaxPooling1D)  (None, 500, 128)     0           activation_9[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_15 (Conv1D)              (None, 500, 128)     49280       max_pooling1d_3[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_10 (Activation)      (None, 500, 128)     0           conv1d_15[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_16 (Conv1D)              (None, 500, 128)     49280       activation_10[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_11 (Activation)      (None, 500, 128)     0           conv1d_16[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_17 (Conv1D)              (None, 500, 128)     49280       activation_11[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_14 (Conv1D)              (None, 500, 128)     16512       max_pooling1d_3[0][0]\r\n__________________________________________________________________________________________________\r\nadd_4 (Add)                     (None, 500, 128)     0           conv1d_17[0][0]\r\n                                                                 conv1d_14[0][0]\r\n__________________________________________________________________________________________________\r\nactivation_12 (Activation)      (None, 500, 128)     0           add_4[0][0]\r\n__________________________________________________________________________________________________\r\nmax_pooling1d_4 (MaxPooling1D)  (None, 250, 128)     0           activation_12[0][0]\r\n__________________________________________________________________________________________________\r\naverage_pooling1d (AveragePooli (None, 83, 128)      0           max_pooling1d_4[0][0]\r\n__________________________________________________________________________________________________\r\nflatten (Flatten)               (None, 10624)        0           average_pooling1d[0][0]\r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 256)          2720000     flatten[0][0]\r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 128)          32896       dense[0][0]\r\n__________________________________________________________________________________________________\r\noutput (Dense)                  (None, 5)            645         dense_1[0][0]\r\n==================================================================================================\r\nTotal params: 3,088,597\r\nTrainable params: 3,088,597\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n2021-03-14 21:26:50.279589: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/100\r\n2021-03-14 21:26:51.735210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-14 21:26:52.441015: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.443810: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.446376: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.450450: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.453265: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.460886: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.469255: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.478998: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:52.482605: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:53.456269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-14 21:26:54.448825: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:54.452490: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:54.458915: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2021-03-14 21:26:54.462869: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\krish\\Desktop\\All_DL\\keras_site\\cnn_test.py\", line 279, in <module>\r\n    history = model.fit(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node model/conv1d/conv1d (defined at c:\\Users\\krish\\Desktop\\All_DL\\keras_site\\cnn_test.py:279) ]] [Op:__inference_train_function_3382]\r\n\r\nFunction call stack:\r\ntrain_function\r\n", "comments": ["@krishdb38,\r\nPlease try limiting GPU memory growth using any of the methods listed in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it helps. \r\n\r\nAlso, take a look at issues [#47008](https://github.com/tensorflow/tensorflow/issues/47008#issuecomment-780018347) [#47411](https://github.com/tensorflow/tensorflow/issues/47411#issuecomment-786204681) with a similar error log. Thanks!", "I am trying to find a solution.\r\nsearching solution \r\nThank you \r\n@amahendrakar ", "@krishdb38,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I am sorry for late response.\r\ndefault TensorFlow installation exist the problem.\r\n \r\nI solved this with tf-nightly \r\nissue #45285 \r\n\r\nCurrently, I am facing a speed problem with my GPU\r\n\r\n![gpu_RTX](https://user-images.githubusercontent.com/45994360/113222382-e0099580-92c1-11eb-9523-535c8cf5242d.png)\r\n\r\n**My GPU is not running in its full capacity**\r\n\r\nThank you very much for your great work\r\n", "@krishdb38,\r\n> I solved this with tf-nightly\r\n\r\nThank you for the update. \r\n\r\n\r\n\r\n> Currently, I am facing a speed problem with my GPU\r\n\r\nSince the original error is resolved, can we close this issue and create a new one for the GPU issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47798\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47798\">No</a>\n"]}, {"number": 47797, "title": "Length for attr 'output_shapes' of 0 must be at least minimum 1", "body": "I am trying to convert a list of lists of strings into a tf.data.Dataset object, but it throws this error:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Length for attr 'output_shapes' of 0 must be at least minimum 1\r\n\t; NodeDef: {{node ParallelMapDatasetV2}}; Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default=\"default\"; attr=preserve_cardinality:bool,default=false> [Op:ParallelMapDatasetV2]\r\n```\r\nAs you can see, I specified output_shapes and output_types, I even tried converting the list into a ragged tensor first but that also changed nothing. \r\nHere is my code:\r\n```\r\n    tf_train_ds = tf.data.Dataset.from_generator(\r\n        lambda: train_list,\r\n        output_types=(tf.string),\r\n        output_shapes=(178728,)\r\n        #output_shapes=(tf.TensorSpec(shape=(2,), dtype=tf.string), tf.TensorSpec(shape=(2,), dtype=tf.string))\r\n        # output_signature=(\r\n        #     tf.TensorSpec(shape=(2,), dtype=tf.string),\r\n        #     tf.TensorSpec(shape=(2,), dtype=tf.string)\r\n        # )\r\n    )\r\n```\r\n\r\nI am stuck with this error for so so long...", "comments": ["@redmlr,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "@amahendrakar \r\nTF Version: 2.4.0-rc0\r\nDataset:\r\nI have a dataset from common voice (mozilla) containing filenames and corresponding labels. I've loaded the data into json like this: \r\n```\r\n[\r\n    {\r\n        \"audio_path\": \"common_voice_de_22136164.wav\",\r\n        \"label\": \"Diese pyromanen ... Vertrauen.\"\r\n    },\r\n    {\r\n        \"audio_path\": \"common_voice_de_19872706.wav\",\r\n        \"label\": \"Die einzelnen Unterar...\r\n```\r\nand then this into a list of lists, like [['audio_filename', 'corresponding label'], ['...', '...'], ...].\r\nAfter that, I extracted the filenames into a simple python list, structured like this: `['common_voice_de_22136164.wav', common_voice_de_129376192.wav, ...]`\r\n\r\nCode:\r\n```\r\ndef get_waveform_and_label(file_path):\r\n\r\n    # get label\r\n    #label_ = get_label(file_path)\r\n    # get the loaded lists\r\n    train_list, _, _ = load_json_into_lists(TRAIN_DS_PATH, TEST_DS_PATH)\r\n\r\n    for filename in train_list:\r\n        if filename == str(file_path):\r\n            return filename\r\n\r\n    # get waveform\r\n    #waveform_ = get_waveform(file_path)\r\n    for filename in train_list:\r\n        if filename == str(file_path):\r\n            file_to_read = str(\"/de/cv_valid_data/\" + filename)\r\n            audio_binary = tf.io.read_file(file_to_read)\r\n            waveform = decode_audio(audio_binary)\r\n\r\n            return waveform\r\n\r\n\r\n@tf.autograph.experimental.do_not_convert\r\ndef lists_to_tf_ds():\r\n    \r\n    train_list, validation_list, test_list = load_json_into_lists(TRAIN_DS_PATH, TEST_DS_PATH)\r\n\r\n    filenames_train_list = tf.ragged.constant([sample[0] for sample in train_list])\r\n\r\n    print(\"Example file tensor: \", filenames_train_list[0])\r\n\r\n    AUTOTUNE = tf.data.AUTOTUNE\r\n\r\n    files_ds = tf.data.Dataset.from_tensor_slices(filenames_train_list)\r\n\r\n    waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE) \r\n\r\n    return waveform_ds\r\n\r\ntf_waveform_ds = lists_to_tf_ds()\r\n```\r\n\r\nI tried to do something similar to this: \r\nhttps://www.tensorflow.org/tutorials/audio/simple_audio\r\n\r\n", "@redmlr,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'load_json_into_lists' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/73105eb918071d531804c2b0275d1f0b/47797.ipynb).\r\n\r\nCould you please share a minimal code snippet so that we can reproduce the issue on our end? Alternatively, you can also run the code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks!", "@amahendrakar,\r\nSo, just for testing reasons, I'd recommend to grab some (2-4) .wav audio clips and labels (can be just one sentence). You don't need the function the code is missing in your comment, try out my notebook: https://colab.research.google.com/drive/1hVm4fyty92G9YFfQ0TMjIKs0QOouZ8lb?usp=sharing\r\nI never worked in colab before, but you have to do 3 things: \r\n1. Do what was in my first sentence; \r\n2. Upload your .wav files to colab; \r\n3. Read my comments; \r\nThis should work for you:)", "@jvishnuvardhan,\r\nI was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/fa9ed4f606a4ad1f6e0f454df4f0df8d/47797.ipynb#scrollTo=xGopHeBEuWLk) and TF-nightly. Whereas with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/761ea542250be22f05e03b3f95f59f1d/47797-2-3.ipynb), I am facing an error stating `AttributeError: module 'tensorflow._api.v2.data' has no attribute 'AUTOTUNE'`. \r\n\r\nPlease check the linked gist for reference. Thanks!", "@redmlr Sorry, this is a confusing error message that is trying to complain that the `Dataset.map` function `get_waveform_and_label` doesn't always return something. It should fix the issue if you put a `return \"\"` at the end", "@aaudiber Thank you for trying! In fact, I could fixed the error 1 day ago. It's so strange but the solution is the following:\r\n`waveform_ds = files_ds.map(lambda file_path: tf.py_function(get_waveform_and_label, [file_path], [tf.float32, tf.string]), num_parallel_calls=AUTOTUNE) ` \r\n\r\nBtw, I am currently having a new error that I'm totally stuck with in #48368. Sooo, if you want to help me again:) ", "Ah, that should also fix the issue by avoiding tracing altogether. Thanks for sharing the fix! If dataset performance ever becomes an issue down the line, you may want to revisit the use of `py_function`, which could add overhead compared to executing a traced map function. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47797\">No</a>\n"]}]