[{"number": 51870, "title": "[TF SavedModel Export] - 2GB Tensor size constraint ", "body": "Hi,\r\n\r\nCC: @nluehr @WhiteFangBuck\r\n\r\nWe discussed this already a few times with @bixia1 @sanjoy and @pkanwar23.\r\n\r\nWith the recent progress in dynamic shapes support for TF-TRT, we are trying to process larger models (e.g. BERT Large, DLRM, TransformerXL) and some of them have issues to be saved in SavedModel format due to the 2GBs tensor size limit.\r\n\r\nWe have compiled a Google Colab to expose this issue: https://colab.research.google.com/drive/1NUrzK_m8HMBTx44WNQOIZ8baJHO60gDm#scrollTo=dqgB2jkpGDIj\r\n\r\n```bash\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-3-de5a306b1fff> in <module>()\r\n     50 print('\\n3. Converting')\r\n     51 converter = trt.TrtGraphConverterV2(input_saved_model_dir=args.saved_model_path)\r\n---> 52 converter.convert()\r\n     53 \r\n     54 # def input_fn():\r\n\r\n4 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    526     if nparray.size * nparray.itemsize >= (1 << 31):\r\n    527       raise ValueError(\r\n--> 528           \"Cannot create a tensor proto whose content is larger than 2GB.\")\r\n    529     tensor_proto.tensor_content = nparray.tobytes()\r\n    530     return tensor_proto\r\n\r\nValueError: Cannot create a tensor proto whose content is larger than 2GB.\r\n```\r\n\r\n@fang @sanjoy could you please orient us on what should be a way forward to address this situation in the short term and potentially come up with a path forward more robust on the long term. I don't believe only TF-TRT runs into this issue", "comments": ["IMO the ideal solution is:\r\n1. TensorRT lifts the restriction of needing layer weights to be compile time constants.\r\n2. We no longer freeze graphs, which means the SavedModel proto can be small (the variable values are in checkpoint files).\r\n\r\nQuestions:\r\n - Do you see (1) happening soon?\r\n - Can (2) happen independently of (1)?", "The Google Colab provided by Jonathan shows two issues:\r\n(1) a freeze graph exceeds the savedModel 2GB limit.\r\n(2) a single constant exceeds the 2GB limit for \"tensor proto\" (see [DLRM from FB](https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/))\r\n\r\nHere are the concerns for the proposed solution to have TF-TRT work on checkpoints instead of savedModel:\r\n(1) It doesn't address the second problem above.\r\n(2) Common ML practice is to use savedModel for inference. Now we can trying to convince people to use checkpoints for inference, if you want to enable TF-TRT. If SavedModel a more stable interface comparing with checkpoints? ", "@sanjoy as promised here a few additional data points:\r\n\r\n## 1. It's totally doable to save in a SavedModel format >= 2GBs\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.ops import array_ops\r\n\r\nclass MyModule(tf.Module):\r\n  def __init__(self):\r\n    self.v = None\r\n\r\n  @tf.function\r\n  def __call__(self, x):\r\n    if self.v is None:\r\n      # 4 GiB variable\r\n      self.v =  tf.Variable(tf.random.uniform((1024,1024,1024), dtype=tf.dtypes.float32))\r\n    x = tf.math.reduce_sum(x * self.v)\r\n    return array_ops.identity(x, name=\"output_0\")\r\n\r\ninput = np.random.uniform(size=(1024,)).astype(np.float32)\r\nfunc = MyModule()\r\nout = func(input)\r\n\r\ncfunc = func.__call__.get_concrete_function(tf.TensorSpec(input.shape, tf.float32))\r\ntf.saved_model.save(func, 'my_saved_model', signatures=cfunc)\r\n```\r\nWhich produces the following:\r\n```bash\r\n$ tree my_saved_model/\r\nmy_saved_model/\r\n\u251c\u2500\u2500 assets\r\n\u251c\u2500\u2500 saved_model.pb\r\n\u2514\u2500\u2500 variables\r\n    \u251c\u2500\u2500 variables.data-00000-of-00001\r\n    \u2514\u2500\u2500 variables.index\r\n\r\n2 directories, 3 files\r\n\r\n$ ls -lh my_saved_model/variables\r\ntotal 4.1G\r\n-rw-r--r-- 1 root root 4.1G Sep  8 23:58 variables.data-00000-of-00001     # <===== Larger than 2GBs\r\n-rw-r--r-- 1 root root  211 Sep  8 23:58 variables.index\r\n```\r\n\r\nIf we look into the SavedModel documentation: https://www.tensorflow.org/api_docs/python/tf/saved_model/save\r\n\r\n> **Variables and Checkpoints**\r\nVariables must be tracked by assigning them to an attribute of a tracked object or to an attribute of obj directly. TensorFlow objects (e.g. layers from tf.keras.layers, optimizers from tf.train) track their variables automatically. This is the same tracking scheme that tf.train.Checkpoint uses, and an exported Checkpoint object may be restored as a training checkpoint by pointing tf.train.Checkpoint.restore to the SavedModel's \"variables/\" subdirectory.\r\n\r\nSo in consequence, it appears that we don't seem to run in a limitation of the SavedModel itself, rather of the `Const` OP and how it is serialized: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/constant_op.cc#L94-L137\r\n\r\nA few possibilities exists so far:\r\n- Increase the protobuf maxsize => unfortunately this seems very difficult to do\r\n- Implement inside the SavedModel API a \"translation mechanism\":\r\n  - during save: `Const` => `ConstSerializable` (a new OP created that is serialized differently and doesn't have this size limit)\r\n  - during loading:  `ConstSerializable` => `Const` to not break any of existing TF code.\r\n\r\nThe second approach would be fairly easy to implement, totally backward compatible and can be implement as a graph transformation.\r\n\r\n## 2. Why \"lazy graph freezing\" will not work for TF-TRT 100% of the time :\r\n\r\nIndeed it's a fair assumption that we could \"lazily\" freeze the graph just before building the engine, which absolutely would solve the problem (only for TF-TRT) in the graph is saved \"unfrozen\". In addition this change can be made easily backward compatible.\r\n\r\nHowever, this absolutely breaks TF-TRT in the following scenarios:\r\n- TRT Engine built ahead of time => There is no way to build the engine without freezing the graph, so if we build before saving => this can't work\r\n- Ahead of time TRT Engine INT8 calibration, similarly you can't calibrate without building the TRT Engine(s), so same issue here, and we both know customer(s) to whom INT8 calibration is fairly essential.\r\n\r\n## 3. Solutions already discussed that would be less then ideal\r\n- Changing TRT to not enforce graph freezing, even in the very hypothetical world where they would agree to do this and it is feasible. This is a very long time ahead (between time to implement, next release and time for Google to pick it up).\r\n\r\n- Using TF Checkpoints and dropping SavedModel:\r\n  - major API change and would totally break backward compatibility on every TF-TRT code ever written.\r\n  - Would break backward/forward compatibility as TF Checkpoints tends be significantly less stable over time (way more TF  version dependant)", "Hi @Saduf2019  !Could you please look into this issue.Replicating in [TF 2.5](https://colab.research.google.com/gist/mohantym/b1358880ec360b5872c314fbef7218d2/-tf-savedmodel-export-2gb-tensor-size-constraint-in-tf-trt-conversion.ipynb#scrollTo=LRODRnetacZG), [2.6](https://colab.research.google.com/gist/mohantym/b311392185889e21c74c989b1694d4a3/-tf-savedmodel-export-2gb-tensor-size-constraint-in-tf-trt-conversion.ipynb#scrollTo=30ets2JqFwrC) and [nightly(2.7.0dev)](https://colab.research.google.com/gist/mohantym/d42d25f5f5e63572cf6ae9f9c8c19a84/-tf-savedmodel-export-2gb-tensor-size-constraint-in-tf-trt-conversion.ipynb#scrollTo=t0YQOjhPa-HV)", "@mohantym this solution will not work because TF-TRT (behind the hood) relies on graph freezing API and there's no way to use variable partitioner / tf.split() OPs AFAIK"]}, {"number": 51863, "title": "Bus error when TFLite library compiled as \"opt\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Poky (Yocto Project Reference Distro) 3.1.3 (dunfell)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **current (commit id 27b2309)**\r\n- Python version:\r\n- Bazel version (if compiling from source): **bazel 3.7.2**\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nThe bug is pretty well described here: https://stackoverflow.com/questions/68687294/google-cloud-vision-api-object-detection-model-gives-bus-error-on-raspberry-pi\r\n(although I have the same problem on a different platform than raspberry Pi).\r\nSince this summer (I cannot tell exactly), Google changed the models in their Vision AutoML service, from:\r\n```\r\nTensorFlow Lite v3\r\nImage Object Detection Model\r\nTOCO Converted. Model built using AutoML Vision\r\n1.14.0\r\n```\r\nto\r\n```\r\nTensorFlow Lite v3\r\nImage Object Detection Model\r\nMLIR Converted. Model built using AutoML Vision\r\n2.5.0\r\n``` \r\nSince then when I try to allocate tensors from these models, e.g.:\r\n```\r\n// Load model\r\nstd::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(\"model.tflite\");\r\n\r\n// Build the interpreter\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\ntflite::InterpreterBuilder(*model.get(), resolver)(&interpreter);\r\n\r\ninterpreter->AllocateTensors();\r\n```\r\nit immediatelly goes to a bus error. GDB shows:\r\n```\r\nProgram received signal SIGBUS, Bus error.\r\n0xb6d5d5b4 in tflite::ops::builtin::broadcastto::ResizeOutputTensor(TfLiteContext*, tflite::ops::builtin::broadcastto::BroadcastToContext*) () from libtensorflowlite.so\r\n(gdb) bt\r\n#0  0xb6d5d5b4 in tflite::ops::builtin::broadcastto::ResizeOutputTensor(TfLiteContext*, tflite::ops::builtin::broadcastto::BroadcastToContext*) () from libtensorflowlite.so\r\n#1  0xb6d5d978 in tflite::ops::builtin::broadcastto::Prepare(TfLiteContext*, TfLiteNode*) () from libtensorflowlite.so\r\n#2  0xb6f89f64 in tflite::Subgraph::PrepareOpsStartingAt(int, std::vector<int, std::allocator<int> > const&, int*) ()\r\n   from libtensorflowlite.so\r\n#3  0xb6f8a220 in tflite::Subgraph::PrepareOpsAndTensors() () from libtensorflowlite.so\r\n#4  0xb6f8cbc4 in tflite::Subgraph::AllocateTensors() () from libtensorflowlite.so\r\n#5  0x0041a7a2 in allocate_interpreter (interpreter=0xbefffb78, model=..., num_threads=2) at inference/inference.cpp:55\r\n```\r\n\r\nI found out however that when I compile the TFLite library using \"fastbuild\" or \"dbg\", everything works, i.e., \r\nThis produces a library which goes into bus error:\r\n`bazel build --config=elinux_armhf -c opt //tensorflow/lite:libtensorflowlite.so`\r\nbut the library produced like this works just fine (no bus error):\r\n`bazel build --config=elinux_armhf -c dbg //tensorflow/lite:libtensorflowlite.so`\r\n\r\nThis shows that there is some bug in how the optimized code is generated for embedded linux with the new AutoML models (maybe something to do with MLIR?)\r\n\r\n\r\n**Describe the expected behavior**\r\nAlso the optimized libtensorflowlite library should work with the latest AutoML models.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://stackoverflow.com/questions/68687294/google-cloud-vision-api-object-detection-model-gives-bus-error-on-raspberry-pi\r\n\r\n**Other info / logs**\r\nI attached the `model.tflite` which causes the problem (one needs to rename it from .zip to .tflite)\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/7120665/model.zip)", "comments": ["Quite incredibly I managed to \"circumvent\" the problem by inserting a \"printf\" in the get_shape_data in `tensorflow/lite/kernels/broadcast_to.cc` :)\r\n\r\n```\r\n// Check if output shape is broadcastable from input shape.\r\nauto get_shape_data = [op_context](int i) -> int32_t {\r\n   if (op_context->shape->type == kTfLiteInt32) {\r\n     return GetTensorData<int32_t>(op_context->shape)[i];\r\n   } else {\r\n+    printf(\"FIXING TFLITE BUS ERROR\\n\");\r\n     return GetTensorData<int64_t>(op_context->shape)[i];\r\n   }\r\n};\r\n```\r\n\r\nCan it be that there is some memory alignment issue with the input tensor? I am really puzzled by this and would be happy if someone could propose a proper fix.\r\n", "Hey @terryheo since this is memory-related could you take a look?", "Could you check if the following work?\r\n```\r\n  auto get_shape_data = [op_context](int i) -> int32_t {\r\n    if (op_context->shape->type == kTfLiteInt32) {\r\n      return GetTensorData<int32_t>(op_context->shape)[i];\r\n    } else {\r\n      return static_cast<int32_t>(GetTensorData<int64_t>(op_context->shape)[i]);\r\n    }\r\n  };\r\n```", "> Could you check if the following work?\r\n> \r\n> ```\r\n>   auto get_shape_data = [op_context](int i) -> int32_t {\r\n>     if (op_context->shape->type == kTfLiteInt32) {\r\n>       return GetTensorData<int32_t>(op_context->shape)[i];\r\n>     } else {\r\n>       return static_cast<int32_t>(GetTensorData<int64_t>(op_context->shape)[i]);\r\n>     }\r\n>   };\r\n> ```\r\n\r\nUnfortunately this did not help. I am getting the same bus error."]}, {"number": 51859, "title": "'Failed to apply delegate' error occurred when training with tflite using nnapi", "body": "### 1. System information\r\n\r\n#### Converter\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- TensorFlow installation (pip package or built from source): pip package (pip install tf-nightly)\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\n![image](https://user-images.githubusercontent.com/37358784/132276541-58ff757e-6b7d-404f-ad5d-6a3b9151a4b0.png)\r\n\r\n#### Mobile phone\r\n - Model: Samsung galaxy z-flip 3(SM-F711N)\r\n - OS&SDK version: Android11, API30\r\n\r\n### 2. Code\r\n\r\n#### - model & tflite converter\r\n```\r\nimport tensorflow as tf\r\n\r\nIMG_SIZE = 28\r\n\r\nclass Model(tf.Module):\r\n\r\n  def __init__(self):\r\n    self.model = tf.keras.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    self.model.compile(\r\n        optimizer='sgd',\r\n        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n    self._LOSS_FN = tf.keras.losses.CategoricalCrossentropy()\r\n    self._OPTIM = tf.optimizers.SGD()\r\n\r\n  @tf.function(input_signature=[\r\n      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\r\n      tf.TensorSpec([None, 10], tf.float32),\r\n  ])\r\n  def train(self, x, y):\r\n    with tf.GradientTape() as tape:\r\n      prediction = self.model(x)\r\n      loss = self._LOSS_FN(prediction, y)\r\n    gradients = tape.gradient(loss, self.model.trainable_variables)\r\n    self._OPTIM.apply_gradients(\r\n        zip(gradients, self.model.trainable_variables))\r\n    result = {\"loss\": loss}\r\n    for grad in gradients:\r\n      result[grad.name] = grad\r\n    return result\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32)])\r\n  def predict(self, x):\r\n    return {\r\n        \"output\": self.model(x)\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def save(self, checkpoint_path):\r\n    tensor_names = [weight.name for weight in self.model.weights]\r\n    tensors_to_save = [weight.read_value() for weight in self.model.weights]\r\n    tf.raw_ops.Save(\r\n        filename=checkpoint_path, tensor_names=tensor_names,\r\n        data=tensors_to_save, name='save')\r\n    return {\r\n        \"checkpoint_path\": checkpoint_path\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def restore(self, checkpoint_path):\r\n    restored_tensors = {}\r\n    for var in self.model.weights:\r\n      restored = tf.raw_ops.Restore(\r\n          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\r\n          name='restore')\r\n      var.assign(restored)\r\n      restored_tensors[var.name] = restored\r\n    return restored_tensors\r\n\r\nSAVED_MODEL_DIR = \"saved_model\"\r\nm= Model()\r\ntf.saved_model.save(\r\n    m,\r\n    SAVED_MODEL_DIR,\r\n    signatures={\r\n        'train':\r\n            m.train.get_concrete_function(),\r\n        'infer':\r\n            m.predict.get_concrete_function(),\r\n        'save':\r\n            m.save.get_concrete_function(),\r\n        'restore':\r\n            m.restore.get_concrete_function(),\r\n    })\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n]\r\nconverter.experimental_enable_resource_variables = True\r\ntflite_model = converter.convert()\r\nwith open('model.tflite','wb') as f:\r\n    f.write(tflite_model)\r\n# Press the green button in the gutter to run the script.\r\nif __name__ == '__main__':\r\n    print(\"main\")\r\n```\r\n\r\n#### - android code\r\n\r\n```\r\npackage com.tvstorm.tflitetest\r\nimport android.os.Bundle\r\nimport android.util.Log\r\nimport androidx.appcompat.app.AppCompatActivity\r\nimport org.apache.commons.io.FileUtils.copyInputStreamToFile\r\nimport org.apache.commons.io.IOUtils\r\nimport org.tensorflow.lite.Interpreter\r\nimport java.io.File\r\nimport java.io.InputStream\r\nimport java.nio.FloatBuffer\r\nclass MainActivity : AppCompatActivity() {\r\n    override fun onCreate(savedInstanceState: Bundle?) {\r\n        super.onCreate(savedInstanceState)\r\n        setContentView(R.layout.activity_main)\r\n        runTFLite()\r\n    }\r\n    val NUM_EPOCHS = 100\r\n    val NUM_TRAININGS = 60000\r\n    val trainImages = Array(NUM_TRAININGS) { Array(28) { FloatArray(28) } }\r\n    val trainLabels = Array(NUM_TRAININGS) { FloatArray(10) }\r\n    var NUM_TESTS = 10\r\n    var testImages = Array(NUM_TESTS) { Array(28) { FloatArray(28) } }\r\n    var output = Array(NUM_TESTS) { FloatArray(10) }\r\n    fun runTFLite() {\r\n        val options = Interpreter.Options().apply { // use nnapi option\r\n            setUseNNAPI(true)\r\n        }\r\n        val interpreter =\r\n            Interpreter(convertInputStreamToFile(resources.openRawResource(R.raw.model)), options)\r\n        for (i in 0 until NUM_EPOCHS) {\r\n            val inputs: MutableMap<String, Any> = HashMap()\r\n            inputs[\"x\"] = trainImages\r\n            inputs[\"y\"] = trainLabels\r\n            val outputs: MutableMap<String, Any> = HashMap()\r\n            val loss: FloatBuffer = FloatBuffer.allocate(1)\r\n            outputs[\"loss\"] = loss\r\n            interpreter.runSignature(inputs, outputs, \"infer\")\r\n            Log.d(\"LOSS\", \"loss: ${loss[0]}\")\r\n        }\r\n    }\r\n    fun convertInputStreamToFile(inputStream: InputStream): File {\r\n        val tempFile = File.createTempFile(java.lang.String.valueOf(inputStream.hashCode()), \".tmp\")\r\n        tempFile.deleteOnExit()\r\n        copyInputStreamToFile(inputStream, tempFile)\r\n        return tempFile\r\n    }\r\n\r\n}\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\r\n\r\nWe have succeeded in training on android device with tflite according to the above document.\r\n\r\nwe tested using nnapi with the interpreter option, but it raises error with the message 'Failed to apply delegate' that is relevant to static tensor size.\r\n\r\nDoes 'On device training' not support nnapi yet? Is there a loadmap for nnapi?\r\n\r\n### 4. error log\r\n\r\n```\r\n2021-09-07 11:28:23.521 20502-20502/com.tvstorm.tflitetest E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.tvstorm.tflitetest, PID: 20502\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{com.tvstorm.tflitetest/com.tvstorm.tflitetest.MainActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#3 is a dynamic-sized tensor).\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3832)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4008)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2317)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:247)\r\n        at android.app.ActivityThread.main(ActivityThread.java:8618)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:602)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1130)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#3 is a dynamic-sized tensor).\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:487)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:88)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:51)\r\n        at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:40)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:196)\r\n        at com.tvstorm.tflitetest.MainActivity.runTFLite(MainActivity.kt:32)\r\n        at com.tvstorm.tflitetest.MainActivity.onCreate(MainActivity.kt:17)\r\n        at android.app.Activity.performCreate(Activity.java:8215)\r\n        at android.app.Activity.performCreate(Activity.java:8199)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1309)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3805)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:4008) \r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:85) \r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:135) \r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:95) \r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2317) \r\n        at android.os.Handler.dispatchMessage(Handler.java:106) \r\n        at android.os.Looper.loop(Looper.java:247) \r\n        at android.app.ActivityThread.main(ActivityThread.java:8618)\r\n```", "comments": ["NNAPI delegate by default only support models with static shapes. \r\n\r\nQuick question, does the model require dynamic shapes? If not, there might be something needed in the conversion process to force using static shapes. @srjoglekar246 "]}, {"number": 51848, "title": "Cannot load saved tflite model when model contains dropout. However quantization-aware model is fine.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): No \r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.9\r\n- CUDA/cuDNN version:  cuDNN-8.1.1.33. CUDA-11.2.0\r\n\r\n**Describe the current behavior**\r\nIf a model contains dropout layers, if can be converted to tflite, but the tflite model cannot be loaded. The error message is:\r\n```\r\nValueError: Did not get operators or tensors in subgraph 1.\r\n```\r\n\r\nConfusingly, if I make the model quantization-aware, the model can be successfully converted to tflite and loaded. Does QAT automatically remove dropout?\r\n\r\n**Describe the expected behavior**\r\nIf a tflite model that contain dropout is not supported, then the desired behavior are:\r\n\r\n1.  Raise an error during tflite conversion instead of at runtime. Or\r\n2. Remove the dropout operation during tflite conversion.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\n\r\nquantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\nquantize_apply = tfmot.quantization.keras.quantize_apply\r\n\r\n\r\n\r\ndef get_model(quantization_aware: bool = False):\r\n    input_embeddings = tf.keras.Input(shape=(512,), dtype=tf.float32, name=\"input_embeddings\")\r\n    is_training = tf.keras.Input(shape=(), dtype=tf.bool, name=\"is_training\")\r\n\r\n    x = tf.keras.layers.Dropout(rate=0.5)(inputs=input_embeddings, training=is_training)\r\n\r\n    if quantization_aware:  # apply quantization to dense layer and return quantization-aware model\r\n        out = quantize_annotate_layer(\r\n            tf.keras.layers.Dense(units=512,name=\"dense\")\r\n        )(x)\r\n\r\n        model = quantize_apply(\r\n            tf.keras.Model(\r\n                inputs=[input_embeddings, is_training],\r\n                outputs=[x, out],\r\n                name=\"toy_model\",\r\n            )\r\n        )\r\n    else: # return vanilla model\r\n        out = tf.keras.layers.Dense(units=512,name=\"dense\")(x)\r\n        model = tf.keras.Model(\r\n            inputs=[input_embeddings, is_training],\r\n            outputs=[x, out],\r\n            name=\"toy_model\",\r\n        )\r\n\r\n    return model\r\n\r\ndef convert_to_tflite(saved_model_path, output_model_path):\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_path))\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    tflite_quant_model = converter.convert()\r\n\r\n    with open(output_model_path, \"wb\") as fh:\r\n        fh.write(tflite_quant_model)\r\n\r\n\r\n\r\nmodel = get_model(quantization_aware=False)  # set to False will raise the error. Set to True the code runs successfully.\r\ninput_data = np.random.rand(16, 512)\r\nembeddings, out = model([input_data, True])\r\n\r\nmodel.save(\"toy_model\")\r\n\r\nconvert_to_tflite(\"toy_model\", \"toy_model.tflite\")\r\ninterpreter = tf.lite.Interpreter(\"toy_model.tflite\")\r\n```\r\n\r\nI'm wondering how should I deal with dropout layer in tflite? As dropout is essential for training, how can I export a trained model without dropout? This is [a similar issue](https://github.com/tensorflow/tensorflow/issues/44232), and I'm not satisfied by the workaround proposed there.\r\n", "comments": ["Oh I figured out what I did wrong and also the correct way to use dropout. \r\n\r\nBasically I shouldn't explicitly pass the `training` argument to dropout layer, it is handled during model invocation where we can tell the model to be in training mode by `out = model(input_batch, training=True)`. This way tflite can handle dropout correctly.\r\n\r\nHowever, I still don't understand why quantization-aware model doesn't have the dropout issue. "]}, {"number": 51847, "title": "set_visible_device_list causing session create failure", "body": "**System information**\r\n- Standard code (as shown in the \"gpu_device_test.cc\" provided with TF source code)\r\n- Windows 10 64 bit\r\n- Tensorflow built from source\r\n- TF 2.5\r\n- Python 3.8.7\r\n- Bazel 3.7.2\r\n- CUDA 11.2 cudnn 8\r\n- GeForce GTX 1050\r\n\r\nThe session create function fails if I set a specific GPU with the function set_visible_device_list, even if I have a single GPU.\r\nFor instance if I set the GPU like this\r\n```\r\nSessionOptions options;\r\nConfigProto* config = &options.config;\r\n(*config->mutable_device_count())[\"GPU\"] = 1;\r\nconfig->mutable_gpu_options()->set_visible_device_list(\"0\");\r\n```\r\n\r\nThe subsequent piece of code and in the specific the session create fails. If I don't set, it doesn't fail.\r\n\r\n```\r\narchitecture->session.reset(NewSession(options));\r\ngraph_def.reset(new GraphDef());\r\nReadBinaryProto(Env::Default(), (string)architecture_path, graph_def.get());\r\narchitecture->session->Create(*graph_def.get());\r\n```\r\n\r\nThe GPU is working and visbile by Tensorflow\r\n```\r\nauto gpu_factory = DeviceFactory::GetFactory(\"GPU\");\r\ngpu_factory->ListPhysicalDevices(&devs_gpu);\r\n```\r\n\r\nHow can I set a specific GPU?", "comments": ["In addition if I call the CreateDevices after setting the specific GPU, it also fails wit this error\r\n\r\n`DeviceFactory::GetFactory(\"GPU\")->CreateDevices(options, kDeviceNamePrefix, &devices) failed: \"Internal: Failed to get memory allocator for TF GPU 0 with 2914163099 bytes of memory.\"`\r\n\r\n```\r\nstd::vector<std::unique_ptr<Device>> devices;\r\nconst char* kDeviceNamePrefix = \"/job:localhost/replica:0/task:0\";\r\nDeviceFactory::GetFactory(\"GPU\")->CreateDevices(options, kDeviceNamePrefix, &devices);\r\n```\r\n\r\n", "Hi @jvishnuvardhan ,could you please look into this issue?\r\n"]}, {"number": 51832, "title": "Forked tf script deadlocks unless disabling intra op parallelism", "body": "**System information**\r\n- Have I written custom code: **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.2 LTS**\r\n- Mobile device: **No**\r\n- TensorFlow installed from: **pip**\r\n- TensorFlow version (use command below): **v2.6.0-rc2-32-g919f693420e 2.6.0**\r\n- Python version: **3.9.6**\r\n- Bazel/GCC/Compiler version: **Not compiling from source**\r\n- CUDA/cuDNN/GPU version: **Using CPU only**\r\n\r\n**Describe the current behavior**\r\n\r\nMe and the R&D team of our company are trying do a submission for an standardized benchmark, namely the [NIST FRVT 1:1](https://pages.nist.gov/frvt/html/frvt11.html) verification. Their benchmark suite employs this architecture:\r\n- They run a custom submitter `initialize` function, where one can prepare the environment an run expensive functions that loads models and prepare temporary structures;\r\n- They fork the process with unix `fork()` a number of times and they run either a `create_template` or `match` function in the forked children. According to the rules of the benchmark, the elaboration must be performed in **CPU** only. No GPU elaboration is allowed.\r\n\r\nSince we can't modify the architecture because it's not in our control, we are trying to fit tensorflow so it will work according to these rules, but we are finding the children processes to deadlock trying to elaborate some layers. The only workaround we found is setting intra op parallelism to 1 with `tf.config.threading.set_intra_op_parallelism_threads(1)`, which appears to disable parallelism for operations like matrix multiplications. This workaround will not work in all scenarios, though. Running the model loading in the forked children will also workaround the issue but will penalize us in the benchmark since the time needed for the loading will be accounted for the elaboration.\r\n\r\n**Describe the expected behavior**\r\nSince we are enforcing CPU only elaboration and there are no resources that require exclusive access we are expecting tensorflow to correctly fit in this architecture and be able to run correctly in forked processes.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe following minimal python script will mimic the architecture and reproduce the issue. [Model](https://github.com/JonneOkkonen/MachineLearningProjects/blob/main/models/cats_vs_dogs_model_86_83.h5) and [test](https://github.com/JonneOkkonen/MachineLearningProjects/blob/main/testImages/cat.jpg) image used are linked.\r\nIt's available also as a Colab [notebook](https://colab.research.google.com/drive/1tvB9kZQvq6tiS7S4aMr761DQ9-DRcKkY?usp=sharing), which features a very similar behavior as running in a local machine, with the difference that in Colab notebook the `os.waitpid()` call never works but that could be an environment limitation.\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport cv2\r\nimport numpy\r\n\r\ntf.config.set_visible_devices(tf.config.list_physical_devices('CPU'))\r\n#tf.config.threading.set_intra_op_parallelism_threads(1) # Decomment this line and the child will not deadlock\r\n#tf.debugging.set_log_device_placement(True)             # Decomment to see job placements\r\n\r\nmodel = None\r\n\r\ndef initialize():\r\n    global model\r\n    model = tf.keras.models.load_model('cats_vs_dogs_model_86_83.h5')\r\n    print('Model Loaded')\r\n\r\ndef child():\r\n    print('Child spawned')\r\n    imageSize = 128\r\n    testImage = cv2.resize(src=cv2.imread('cat.jpg'), dsize=(imageSize, imageSize), interpolation=cv2.INTER_LINEAR) / 255\r\n    result = model(testImage.reshape(-1, imageSize, imageSize, 3))[0]\r\n    print('Result: Cat: ' + str(result[0]) + '| Dog: ' + str(result[1]))\r\n    print('Child finished')\r\n\r\ndef parent():\r\n    initialize()\r\n    newpid = os.fork()\r\n    if newpid == 0:\r\n        child()\r\n    else:\r\n        pids = (os.getpid(), newpid)\r\n        print(\"Parent: %d, Waiting for child: %d\\n\" % pids)\r\n        os.waitpid(newpid, 0)\r\n\r\nparent()\r\n```\r\n**Other info / logs**\r\nEnabling log device placement will reveal that the elaboration stops elaborating in a convolution layer but with other scenarios the elaboration can deadlock in different operations.\r\n\r\n```\r\nChild spawned\r\n2021-09-04 12:10:03.207086: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-04 12:10:03.209115: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-04 12:10:03.210155: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2021-09-04 12:10:03.211392: I tensorflow/core/common_runtime/eager/execute.cc:1161] Executing op Conv2D in device /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\n\r\nWe are using Numpy version: 1.19.5 as installed automatically from pip. We also tried to enforce updated numpy 1.21.2 as suggested [here](https://github.com/tensorflow/tensorflow/issues/13802#issuecomment-881665082) in an other `fork()` related issue but that didn't help.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n- Do you want to contribute a PR? (yes/no): **No**\r\n", "comments": ["I was able to create a dump of the stuck process with gcore and I inspected it with gdb. From the analysis it shows a single thread stuck on a condition variable doing an Eigen operation. It follows an excerpt, attached the full [backtrace](https://github.com/tensorflow/tensorflow/files/7111317/all-threads-stacks.txt):\r\n\r\n```\r\n(gdb) thread apply all bt\r\n\r\nThread 1 (Thread 0x7fa1751d7740 (LWP 11903)):\r\n#0  futex_wait_cancelable (private=<optimized out>, expected=0, futex_word=0x7ffea44694f8) at ../sysdeps/nptl/futex-internal.h:183\r\n#1  __pthread_cond_wait_common (abstime=0x0, clockid=0, mutex=0x7ffea44694a8, cond=0x7ffea44694d0) at pthread_cond_wait.c:508\r\n#2  __pthread_cond_wait (cond=0x7ffea44694d0, mutex=0x7ffea44694a8) at pthread_cond_wait.c:638\r\n#3  0x00007fa133c8be30 in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007fa1249054b3 in Eigen::Barrier::Wait() () from /home/ceztko/projects/current/ENVision/build-enpython/enpython/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fa1279d6700 in void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::evalProductImpl<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, 0>(float*, Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback) const () from /home/ceztko/projects/current/ENVision/build-enpython/enpython/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fa1279d8caf in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::NoOpOutputKernel const> const> const> const, Eigen::ThreadPoolDevice, true, (Eigen::internal::TiledEvaluation)0>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReshapingOp<Eigen::DSizes<long, 4> const, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::NoOpOutputKernel const> const> const> const&, Eigen::ThreadPoolDevice const&) () from /home/ceztko/projects/current/ENVision/build-enpython/enpython/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fa1279d96fc in void tensorflow::functor::SpatialConvolutionFunc<Eigen::ThreadPoolDevice, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::NoOpOutputKernel>(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer>, int, int, int, int, Eigen::PaddingType const&, Eigen::NoOpOutputKernel const&, int, int, int, int) () from /home/ceztko/projects/current/ENVision/build-enpython/enpython/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fa127af62e1 in tensorflow::(anonymous namespace)::LaunchGeneric<Eigen::ThreadPoolDevice, float>::operator()(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, tensorflow::Padding const&, std::vector<long, std::allocator<long> > const&, tensorflow::Tensor*, tensorflow::TensorFormat) [clone .isra.8556] [clone .constprop.8631] () from /home/ceztko/projects/current/ENVision/build-enpython/enpython/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fa127af6a11 in tensorflow::LaunchConv2DOp<Eigen::ThreadPoolDevice, float>::operator()(tensorflow::OpKernelContext*, bool, bool, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, tensorflow::Padding const&, std::vector<long, std::allocator<long> > const&, tensorflow::Tensor*, tensorflow::TensorFormat) () from /home/ceztko/projects/current/ENVision/build-enpython/enpython/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fa127af786c in tensorflow::Conv2DOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*) () from /home/ceztko/projects/current/ENVision/build-enpython/enpython/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n[...] \r\n```\r\n\r\nThe presence of a singe thread probably means that a resource, possibly a thread pool, that is supposed to exist actually doesn't in the forked children, which is consistent with the fact that unix `fork()` will preserve only a single thread in the children. Another discovery is that by just moving the loading of the model to the child, but keeping all the imports as before as done [here](https://colab.research.google.com/drive/1tvB9kZQvq6tiS7S4aMr761DQ9-DRcKkY#scrollTo=SuUkj5vtTbiJ), will also fix the issue but is not desirable for complex models which takes long to load.\r\n\r\nBecause of this two considerations it appears the problem is either that tensorflow/Eigen can't recreate/reset the needed resources, possibly threads/condition variables, in the forked children (operations like these are usually done by fork handlers with `pthread_atfork`). Or alternatively just loading the model shouldn't cause tensorflow to load threading support in the library, which appear to happen here (I put breakpoints in `tensorflow.python.eager.context.ensure_initialized`).", "@ceztko Could you please have a look at the similar [issue1](https://github.com/huggingface/transformers/issues/5486), [issue2](https://stackoverflow.com/questions/41233635/meaning-of-inter-op-parallelism-threads-and-intra-op-parallelism-threads) and let us know if it helps?Thanks!", "@sushreebarsa thank your for the prompt answer. [Issue1](https://github.com/huggingface/transformers/issues/5486) does not appear to matter here (I tried the TOKENIZERS_PARALLELISM=false, though, and it didn't help). Instead [issue2](https://stackoverflow.com/questions/41233635/meaning-of-inter-op-parallelism-threads-and-intra-op-parallelism-threads) is about the correct use of `set_intra_op_parallelism_threads()` and `set_inter_op_parallelism_threads()`, which I already tried both. The only one that seems to partially workaround the issue is `set_intra_op_parallelism_threads(1)`, which by definition should not enable threading for matrix operations, but that doesn't work always, for example evaluating the tensor with `model.predict()` also results in deadlock in my scenario with both `set_intra_op_parallelism_threads(1)`, `set_inter_op_parallelism_threads(1)` set. Before creating the bug report I searched several [similar](https://github.com/tensorflow/tensorflow/issues/13802) [issues](https://github.com/keras-team/keras/issues/9964) and solutions, but they mostly not apply here since the `fork()` call in my use case is performed outside of tensorflow itself and I can't change or override that call. My latest debugging [here](https://github.com/tensorflow/tensorflow/issues/51832#issuecomment-913122756) shows that the issue seems to be really related to the way tensorflow or Eigen handle (or not handle) the fork call.", "@sanatmpa1 Was able to reproduce the issue on Colab using TF v2.5, 2.6 and tf-nightly,Please find the[ gist](https://colab.research.google.com/gist/sushreebarsa/dad6e56c8fa248ebdc10dccbad66b893/untitled190.ipynb#scrollTo=LUTrz0LeJAXK) for reference .Thanks!", "Eigen and TF's threadpool will not play nicely with unix `fork()`.  Calling fork while multiple threads are using a `std::mutex` will lead to undefined behavior.  We re-use the intra-op threadpool, so once the context is initialized, the threadpool will continue to exist.  It looks like calling `fork()` after this initialization is causing this issue.\r\n\r\nYou can try re-initializing the context in the child process, but that will likely introduce new issues.\r\n\r\nIs there no way to ignore the model load time during benchmarking?", "I understand that, in the general, `fork()` can cause big headaches for all software using thread pools, and not all scenarios can be possibly supported because of unpredictable results if a fork() arises during some thread computations. Please note the following considerations:\r\n- In this scenario we are forking after a model loading, so it should be a safe \"still balls\" moment;\r\n- we really **can't** move the model loading in the child. It's very slow and penalizes us in the benchmark. Even if we move the model loading in a fork handler as a very bad trick, and this is not taken into account in the timings of the prediction operation, that would create harm in the execution of the benchmark anyway, being heavily slowed down while processing thousands and thousands of images;\r\n- we evaluated alternative stacks and and we found that pytorch supports the load + fork execution model. In fact we found fork handlers being set in pythorch source code resetting their thread pool.\r\n\r\nIf you currently don't plan to fully support this scenario with threading enabled by implementing similar fork [handlers](https://man7.org/linux/man-pages/man3/pthread_atfork.3.html) we at least expect tensorflow to play nice when threading is fully disabled. This does not happen all the times. In [this](https://colab.research.google.com/drive/1tvB9kZQvq6tiS7S4aMr761DQ9-DRcKkY#scrollTo=iV9MTC0Rd2cI&line=29&uniqifier=1) script we both set intra/inter operations thread count to 1 and we perform the prediction with `model.predict()` instead. The script still hangs during an inference operation:\r\n```\r\nExecuting op __inference_predict_function_527 in device /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\n\r\nHaving a reliable single thread execution of tensorflow with this load+fork execution model would be enough for us since this benchmark is spawned in multiple processes anyway  and the benchmark also recommends it. Please let us know if you also reproduce the issue.", "Model loading initializes the context, which initializes the threadpool(s).  I don't see an easy way to separate this atm.\r\n\r\nWe will likely never implement fork handlers - that is specific to `pthread`.\r\n\r\nWe don't have a way to fully disable threading entirely - we have at least one thread in each pool (inter/intra).  This is necessary since we have asynchronous ops.\r\n\r\nThe only work-around I can think of is to provide a mechanism to re-initialize the threadpools that you can explicitly call at the start of your child processes.\r\n\r\nWe would be happy to review a pull-request along those lines.  Otherwise, this is a very specific unsupported use-case, so I can't make any guarantees about timelines.", "> Model loading initializes the context, which initializes the threadpool(s). I don't see an easy way to separate this atm.\r\n\r\nYes, we noticed it. It happens all the times model loading is performing some evaluation.\r\n\r\n> We will likely never implement fork handlers - that is specific to `pthread`.\r\n\r\nI don't understand: in all the platforms where `fork()` is available and functional (basically server/desktop unices, including MacOS) we expect `pthread_atfork` to be [functional](https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man3/pthread_atfork.3.html) as [well](https://www.freebsd.org/cgi/man.cgi?query=pthread_atfork&sektion=3&manpath=FreeBSD+6.0-RELEASE), so why fork handlers can't be set up just on these platforms? Also pthread dependency is very likely enforced in TF indirectly by [other](https://github.com/tensorflow/tensorflow/issues/13802) depedendencies.\r\n\r\n> We don't have a way to fully disable threading entirely - we have at least one thread in each pool (inter/intra). This is necessary since we have asynchronous ops.\r\n> \r\n\r\nI understand, even if this sounds more like a design choice (either of Eigen or TF) than a true requirement.\r\n\r\n> The only work-around I can think of is to provide a mechanism to re-initialize the threadpools that you can explicitly call at the start of your child processes. [...]\r\n> We would be happy to review a pull-request along those lines.\r\n\r\nWe unfortunately don't have resources to put on this atm so our choice is just dodge the issue or continue the evaluation of other stacks where this scenario is better supported.\r\n\r\n> Otherwise, this is a very specific unsupported use-case, so I can't make any guarantees about timelines.\r\n\r\nWhile it may definitely be a specific use-case, we spotted several [users](https://stackoverflow.com/questions/54820729/sharing-saved-model-of-tensorflow-among-child-processes#comment122014575_54820729) that were definitely [attempting](https://stackoverflow.com/questions/52310182/how-to-handle-a-tensorflow-session-in-a-multiprocessing-environment-with-fork) to use tensorflow in this exact way. Also, because python has several limitations with threading, properly supporting this scenario may be a quick and elegant solution to create a pool of workers in all the platforms where `fork()` is available and cheap to call, so we recommend TF devs to reconsider their supporting intentions with regard to this use-case."]}, {"number": 51826, "title": "TF-32 Tensor Cores not used", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: RTX A6000, 48GB\r\n\r\n**Describe the current behavior**\r\nI'm training a fairly large convnet, and with long training times, one thing I've noticed is apparent lack of tensor core utilization. I have tried using float16, but my model greatly suffers from the reduced precision. However, since this is an Ampere GPU I expect to be able to use tensor cores in 32 bit as well. Using the tensorboard profiler, I see many of my kernels are tensor core eligible but none are actually using tensor cores. \r\n<img width=\"785\" alt=\"Screen Shot 2021-09-03 at 11 50 49 AM\" src=\"https://user-images.githubusercontent.com/19317207/132033324-c31d0d45-d1cc-4bee-80da-c747028f69b7.png\">\r\n\r\nAll of my convolution layers have multiples of 8 for filters (32, 64, 128, etc.) so dimensions should not be affecting tensor cores. One other thing I though worth noting was lots of memory being taken up by NHWC->NCHW transpose layers. It's my understanding that tensor cores should operate entirely in NHWC, so my model should not be doing any computation in NCHW and yet it seems to be.\r\n\r\n<img width=\"750\" alt=\"Screen Shot 2021-09-03 at 11 54 19 AM\" src=\"https://user-images.githubusercontent.com/19317207/132033733-18ce0744-ed68-467b-9f1f-b802198061dc.png\">\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect Tensor Cores to be used for most convolution and matmul operations provided the multiple-of-8 condition is met. One thing I'm wondering: is it possible that the bug is actually in reporting tensor core usage rather than not using tensor cores in the first place? I.e. is tensorboard only looking for float16 tensor cores and not reporting tf-32 operations as using tensor cores when they actually are? That still doesn't explain the NCHW behavior though.  \r\n\r\n**Standalone code to reproduce the issue**\r\nWorking on producing an example. Colab is not an option due to lack of Ampere GPUs\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nHere is my TensorBoard log file. This includes the profiler data as well as a model checkpoint for inspecting the architecture\r\n[20210902-145514.zip](https://github.com/tensorflow/tensorflow/files/7107228/20210902-145514.zip)", "comments": ["@atyshka Could you please post this issue in [Tensorboard repo](https://github.com/tensorflow/tensorboard/issues) ,you will get the right help there ?Thank you!", "@sushreebarsa I\u2019m unsure though whether this is an issue with tensorboard or tensorflow. Is there a way to check whether layers are using tensor cores without Tensorboard?"]}, {"number": 51825, "title": "Grouped convolutions generate seriously obscure errors on CPU", "body": "Hello there :wave: \r\n\r\nToday I ran into a cumbersome error that only happens when running on CPU instead of GPUs. I tracked the source of the error to grouped convolutions and managed to make a reproducible minimal snippet. I happened to suspect that it was because of grouped convolutions since I ran into some problems a few days ago with those using SavedModels but it's pure luck. \r\n\r\nIt would be good to improve the error message or even get this fixed if possible :pray: \r\n\r\nHappy to help provided some directions!\r\n\r\n**System information**\r\n- Have I written custom code: yes, the code snippet\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from: binary, via pip\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: CUDA 11.4 (cuDNN 8.2.0)\r\n- GPU model and memory: NVIDIA GeForce RTX 2070 with Max-Q Design\r\n\r\n**Describe the current behavior**\r\n\r\nAs of now, running the snippet further down below throws an error on CPU but not on GPU.\r\n\r\n**Describe the expected behavior**\r\n\r\nSimple:\r\n- having a better error (pointing the lack of support of grouped convolutions on CPU)\r\n- or even better, if that could get fixed :) \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nsamples = tf.zeros((1, 256, 256, 3), dtype=tf.float32)\r\nmodel = Sequential([layers.Conv2D(18, padding='same', kernel_size=3, groups=1), layers.GlobalAveragePooling2D(), layers.Dense(1)])\r\ntrouble_model = Sequential([layers.Conv2D(18, padding='same', kernel_size=3, groups=3), layers.GlobalAveragePooling2D(), layers.Dense(1)])\r\n\r\n# Backprop on classic model\r\nwith tf.GradientTape() as tape:\r\n    out = model(samples, training=True)\r\ngrads = tape.gradient(out, model.trainable_weights)\r\n\r\n# Now with grouped conv\r\nwith tf.GradientTape() as tape:\r\n    out = trouble_model(samples, training=True)\r\ngrads = tape.gradient(out, trouble_model.trainable_weights)\r\n```\r\n\r\nwhich runs successfully on GPU but on CPU throws the following:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-e03a8706f9a2> in <module>\r\n     19 with tf.GradientTape() as tape:\r\n     20     out = trouble_model(samples, training=True)\r\n---> 21 grads = tape.gradient(out, trouble_model.trainable_weights)\r\n\r\n~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1072                           for x in nest.flatten(output_gradients)]\r\n   1073 \r\n-> 1074     flat_grad = imperative_grad.imperative_grad(\r\n   1075         self._tape,\r\n   1076         flat_targets,\r\n\r\n~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     69         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\r\n     70 \r\n---> 71   return pywrap_tfe.TFE_Py_TapeGradient(\r\n     72       tape._tape,  # pylint: disable=protected-access\r\n     73       target,\r\n\r\n~/miniconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\r\n    157       gradient_name_scope += forward_pass_name_scope + \"/\"\r\n    158     with ops.name_scope(gradient_name_scope):\r\n--> 159       return grad_fn(mock_op, *out_grads)\r\n    160   else:\r\n    161     return grad_fn(mock_op, *out_grads)\r\n\r\n~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/nn_grad.py in _Conv2DGrad(op, grad)\r\n    579   # in Eager mode.\r\n    580   return [\r\n--> 581       gen_nn_ops.conv2d_backprop_input(\r\n    582           shape_0,\r\n    583           op.inputs[1],\r\n\r\n~/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d_backprop_input(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\r\n   1245       return _result\r\n   1246     except _core._NotOkStatusException as e:\r\n-> 1247       _ops.raise_from_not_ok_status(e, name)\r\n   1248     except _core._FallbackException:\r\n   1249       pass\r\n\r\n~/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6895   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6896   # pylint: disable=protected-access\r\n-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6898   # pylint: enable=protected-access\r\n   6899 \r\n\r\n~/miniconda3/lib/python3.8/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Computed input depth 3 doesn't match filter input depth 1 [Op:Conv2DBackpropInput]\r\n```\r\n\r\n", "comments": ["In TensorFlow, grouped convolution is only implemented on the GPU version. That is to say, in the CPU version of TensorFlow, you cannot achieve grouped convolution by changing the value of the parameter groups in tf.keras.layers.Conv2D().", "@calmisential yes thanks, I understood that during my personal investigation :+1: \r\nMy point is that this is far from being obvious with this error message:\r\n- the forward pass is doing well both on CPU & GPU\r\n- however the backpropagation has some troubles on CPU only\r\n\r\nSo my suggestion is the following:\r\n- If the forward pass is also erroneous, this needs to throw a `NotImplementedError` when doing it\r\n- if that's indeed only the backward pass, which I find quite strange, a more specific error needs to be thrown\r\n\r\nObviously, it would be better to have full support of grouped convolutions but if that's not possible right now, this really needs some user error improvement :sweat_smile: ", "@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.5, v2.6 and tf-nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/723f860cfaf0d3e2f7901e7bafac425c/un51825.ipynb).", "Thanks for reproducing it @tilakrayal :pray: \r\nSo how should we proceed? I doubt we could leave grouped convolutions unsupported on a Deep Learning framework such as Tensorflow :sweat_smile: \r\n@ymodak ", "Any update @ymodak?\r\nConsidering architectures such as MobileNets use this extensively, I think it's rather important to tackle this. Especially considering it was taken care of in other frameworks a long time ago", "https://github.com/tensorflow/tensorflow/commit/87e0abde2e51fa8c4cc10a683a226e378ca95576 should improve the error message a bit. Unfortunately, we don't have the staffing to actually fix this issue - but we're open and welcome external contributions! ", "Another option is to use jit_compile=True. \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfilters = tf.Variable(initial_value=tf.zeros((3, 3, 1, 18)), dtype=tf.float32)\r\n\r\n@tf.function(jit_compile=True)\r\ndef f(samples):\r\n  with tf.device(\"cpu:0\"):\r\n      with tf.GradientTape() as tape:\r\n        out = tf.nn.conv2d(samples, filters, 1, \"SAME\")\r\n\r\n      grads = tape.gradient(out, filters)\r\n      return grads\r\n\r\nsamples = tf.zeros((1, 256, 256, 3), dtype=tf.float32)\r\nf(samples)\r\n```", "> [87e0abd](https://github.com/tensorflow/tensorflow/commit/87e0abde2e51fa8c4cc10a683a226e378ca95576) should improve the error message a bit. Unfortunately, we don't have the staffing to actually fix this issue - but we're open and welcome external contributions!\r\n\r\nCould you point to the files that you expect to need some modifications for this?\r\nIf it's within my reach, I'm happy to help as I believe this is really important for the long-term compatibility of TF!\r\n\r\nAbout the `jit_compile`, you mean that grouped convolutions are supported if we trace them?", "Any update @rohan100jain ? :)", "I'm also experiencing this issue. I am unable to get a model trained using grouped convolutions and loaded via `tf.keras.models.load_model(...)`, to perform inference on the CPU. No issues with the GPU.\r\n\r\nThe error is: \"Fused conv implementation does not support grouped convolutions for now.\"\r\n\r\nI have tried enabling XLA, and I still experience this issue on the CPU."]}, {"number": 51818, "title": "Low performance when using persistent mode GradientTape with LSTM/GRU layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: 11.0/8.2.2\r\n- GPU model and memory: NVIDIA RTX Titan 24GB\r\n\r\n**Describe the current behavior**\r\nThe performance was very low in graph mode when using persistent mode tf.GradientTape or create multi-GradientTape objects in one with block.\r\nThis phenomenon only happens when the model includes a LSTM or GRU layers.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\noptimizer=tf.keras.optimizers.Adam()\r\n\r\n@tf.function\r\ndef train_step_0():\r\n  with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))\r\n  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))\r\n\r\nt0=time.time()\r\nfor i in range(100):\r\n  train_step_0()\r\nprint(time.time()-t0)\r\n```\r\n**output**\r\n```\r\n2021-09-03 12:39:22.262342: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n2021-09-03 12:39:22.287055: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n2021-09-03 12:39:22.300477: W tensorflow/core/common_runtime/process_function_library_runtime.cc:841] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n17.58007049560547\r\n```\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\noptimizer=tf.keras.optimizers.Adam()\r\n\r\n@tf.function\r\ndef train_step_1():\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape.gradient(loss1, model1.trainable_variables)\r\n  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))\r\n  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))\r\n\r\nt0=time.time()\r\nfor i in range(100):\r\n  train_step_1()\r\nprint(time.time()-t0)\r\n```\r\n**output**\r\n```\r\n2021-09-03 12:43:44.947280: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n2021-09-03 12:43:44.972180: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:801] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n2021-09-03 12:43:44.985573: W tensorflow/core/common_runtime/process_function_library_runtime.cc:841] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node zeros_like_1 was passed float from sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n16.632988929748535\r\n```\r\n```python\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel0 = tf.keras.models.Sequential(\r\n    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n)\r\nmodel1 = tf.keras.models.Sequential(\r\n    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(128,))\r\n)\r\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\noptimizer=tf.keras.optimizers.Adam()\r\n\r\n@tf.function\r\ndef train_step_2():\r\n  with tf.GradientTape() as tape0:\r\n    f = model0(tf.random.normal([256, 300, 40]))\r\n    y_p0 = model1(f)\r\n    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n  with tf.GradientTape() as tape1:\r\n    y_p1 = model1(tf.random.normal((256, 128)))\r\n    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n  optimizer.apply_gradients(zip(grad0,model0.trainable_variables))\r\n  optimizer.apply_gradients(zip(grad1,model1.trainable_variables))\r\n\r\nt0=time.time()\r\nfor i in range(100):\r\n  train_step_2()\r\nprint(time.time()-t0)\r\n```\r\n**output**\r\n```\r\n4.321804523468018\r\n```\r\n**Other info**\r\nBoth train_step_0 and train_step_1 show the error, while train_step_2 doesn't. In my GPU, the first 2 approaches take around 17 in doing 100 training steps, while the third one takes 4.3s.\r\nFurthermore, we can only reproduce this performace drop when using GRU/LSTMs in graph mode. Which is, if we remove the tf.function decorator from the train_step functions or if we switch the LSTM by a dense layer, all 3 examples take the same time and none of them outputs any error.\r\nAs an additional info, this problem happens running both in CPU and in GPU\r\n\r\nBy the way, this issue is an updated version of #35928 which addressed a very similar problem", "comments": ["@AlexFuster Thanks for creating this issue. Looks like this is more related to keras-team/keras. So, I moved this issue to keras-team/keras repo for resolving. Thanks!", "Well, Keras team doesn't seem to agree with that ", "@AlexFuster looks like more related to TF core. So, this repo is right place for this issue. Thanks!", "I noticed an interesting code flipping recurrent_v2._use_new_code() to False:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/73b709743a2eba2c912351e8d3334ef25e174c4b\r\n\r\nThis could explain why the performance degraded again.\r\n\r\nI checked if I monkey patch a revert of the commit, the speed drastically improves:\r\n\r\n```\r\nfrom keras.layers import recurrent_v2\r\nrecurrent_v2._use_new_code = lambda : True\r\n```\r\n\r\nThe change is by @yhliang2018 -- any backgrounds on why we have reverted to the old / slower code path?\r\n"]}, {"number": 51813, "title": "tf.io.decode_image will flip the image", "body": "Please look at [colab](https://colab.research.google.com/drive/1gybGlto8ol7lnE82G9LLx0-DhRWtweUd#scrollTo=gecIt24H9fHO)\r\n\r\n![IMG_20210729_194824430_MFNR](https://user-images.githubusercontent.com/33624574/131965649-813d23db-d14d-4c12-8dd5-25c3bf313e90.jpg)\r\n", "comments": ["@AR-fan \r\nIn order to expedite the trouble-shooting process,Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!\r\n", "@sushreebarsa \r\n\r\n- Have I written custom code :  \r\n```  \r\nimg_bytes = open(\"./IMG_20210729_194824430_MFNR.jpg\", 'rb').read()\r\nimg = tf.io.decode_image(img_bytes)\r\nnew_img_bytes = tf.io.encode_jpeg(img)\r\n```  \r\n- OS Platform and Distribution : colab Linux version 5.4.104+   \r\n- TensorFlow version : 2.6.0  \r\n- Python version : Python 3.7.11  \r\n- GCC/Compiler version (if compiling from source) : gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0  \r\n- CUDA/cuDNN version : None  \r\n\r\n**Describe the current behavior**  \r\ntf.io.decode_image op will flip the input image  \r\n\r\n**Describe the expected behavior**  \r\ntf.io.decode_image op not flip the input image  \r\n\r\n- Do you want to contribute a PR? (yes/no):yes", "@AR-fan I tried to run your code on Colab using TF v2.6 and didn't face the issue reported here ,Please find the [gist](https://colab.research.google.com/gist/sushreebarsa/72f61f633b7a0b5bad7e5aacd60561f2/-github_issue_41126_ipynb.ipynb#scrollTo=CfrODBN8a4ul) for reference. Could you please have a look at the [link](https://www.tensorflow.org/api_docs/python/tf/io/decode_image) and similar [issue](https://github.com/tensorflow/tensorflow/issues/41126)? Thank you!", "@sushreebarsa \r\nPlease try this picture:\r\n![IMG_20210729_194824430_MFNR](https://user-images.githubusercontent.com/33624574/132168738-77702ed9-66e4-481a-9489-605554e40931.jpg)\r\n", "@sushreebarsa \r\nI have seen that issue, but not works for me.  \r\nUsing tf op is right in most case, but fail in this case.", "@AR-fan Could you please refer to the [gist ](https://colab.research.google.com/gist/sushreebarsa/74683afc831c6bf99b4c728f1f4fe5f3/untitled433.ipynb) and let us know if it helps? Thank you!", "@sushreebarsa \r\nCould you please have a look at this [gist](https://colab.research.google.com/drive/1gybGlto8ol7lnE82G9LLx0-DhRWtweUd?usp=sharing)?    \r\nI modify your gist and use wget to get picture.  \r\nThe result is still wrong.    \r\nIf you find a solution , please tell me , thanks~\r\nOnz\r\n", "@sanatmpa1 Was able to reproduce the issue on colab using TF 2.6 and tf-nightly ,Please find the gist[ here ](https://colab.research.google.com/gist/sushreebarsa/7d4afd700b35821cf840464ec8eb67c9/-github_issue_41126_ipynb.ipynb#scrollTo=11TT3PoSVUuZ) for reference.Thanks! ", "hi, how is it going?~", "@AR-fan When I used two functions from a TF tutorial https://www.tensorflow.org/tutorials/generative/style_transfer to (load & process) and show image, original image and decoded image show [similarly](https://colab.research.google.com/gist/jvishnuvardhan/32ef152edaebc1fe3faf5d6e5789fcd8/-github_issue_41126_ipynb.ipynb). Can you please verify whether `cv` library changing the order of the image?\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/sushreebarsa/7d4afd700b35821cf840464ec8eb67c9/-github_issue_41126_ipynb.ipynb)", "@jvishnuvardhan \r\nI have collected a lot of pictures. I need pictures not to be reversed. I found that using tf.io will reverse some pictures, but some pictures will not be reversed, but opencv will never reverse.\r\nMaybe I'll use OpenCV to process the picture, because it will be done as I expected."]}, {"number": 51812, "title": "Is there any update on profiler's TensorCores eligible rules?", "body": "@yisitu\r\nI find the rules code to judge whether a kernel uses Tensor Cores is 1~2 years old. I'm not sure whether there should be new updates, along with latest hardward chips or library updates.\r\nBTW, how these rules come from? Is there any doc that lists all kernels which uses Tensor Cores?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/profiler/utils/kernel_stats_utils.cc#L95-L119\r\n", "comments": ["@trisolaran  these rules need to be updated for Ampere and beyond. ", "@yisitu Thanks! But how do you get the rules? Maybe I could contribute if I know the way to get them.", "Hi Teng,\r\n\r\nIt is a heuristic and obtained mainly through observation, empirical data, and research. I agree that it would be great if NVIDIA documentation lists all kernels which uses Tensor Cores. Alternatively, you could read the cuDNN documentation to see the prereqs for using TensorCores and observe what kernels are launched - but this requires significant amount of effort.\r\n\r\nSitu", "Hi yisitu,\r\nThanks a lot for response! \r\nDo you mean trying to run a lot of classic models on different NVIDIA archtectures(such as Turing and Volta), and using [nvprof and tensor_precision_fu_utilization metric](https://developer.nvidia.com/blog/using-nsight-compute-nvprof-mixed-precision-deep-learning-models/) to check each kernel, recording every one with valid value on this metric? This way is also effort consuming and can't cover full kernel set.\r\nYes, I don't find any list provided by NVIDIA. The document is not detail enough to kernel level.", "It is not so much about the models as it is about the resulting cuDNN kernels for each GPU architecture. You can simplify this a little more to cover just the cuDNN TensorCore use cases."]}, {"number": 51810, "title": "RuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (64 != 1)Node number 13 (CONV_2D) failed to prepare.", "body": "Hello I am using semantic segmentation model. The model is suclass custom model. It has been trained and saved successfully. i also convert it into tflite version. But when I tried for inference of tflite model it shows the mentioned error in allocating tensors. i have also tried it using tf-nightly 2.7 and tf version 2.5 but it shows same error while allocation tensor. Any help will be highly appreciated. Thanks\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow version = 2.5:\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option A: Reference colab notebooks\r\n\r\n1)  Reference [TensorFlow Model Colab]\r\nhttps://colab.research.google.com/drive/1v8SvJbMmjTyYVnRBeGwwCGSnRpkpR80p#scrollTo=L_PMowpPmaFx\r\n\r\n\r\n", "comments": ["Sorry for inconvenience. I have change the share setting of the provided colab link. Now anyone can access to it. If anyone can help to resolve this issue it will be highly appreciated."]}, {"number": 51803, "title": "GPU Error: Check failed: work_element_count > 0 (0 vs. 0) ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2/8.1.0.77\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n\r\n\r\nHey everyone,\r\n\r\nI am experiencing a strange issue when trying to train  a 3DConv with Custom data generator.\r\nWhne I call `model.fit(train_data,epochs=10)`  where `train_data` is the generator I get this GPU related error\r\n`2021-09-02 12:27:24.281505: F .\\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)`\r\nThe error seems to disappear when not using GPU \r\nI\r\n\r\nThe generator is this:\r\n\r\n```python\r\nclass Dataset(tf.keras.utils.Sequence):\r\n    def __init__(self, data, batch_size=BATCH_SIZE, shuffle=True):\r\n        self.data = np.array(data)\r\n        self.batch_size = batch_size\r\n        self.shuffle = shuffle\r\n        self.indices = data.index.tolist()\r\n\r\n    # @staticmethod\r\n    def __load_dicom_image(self,path, img_size=IMAGE_SIZE, voi_lut=True, rotate=0):\r\n        dicom = pydicom.read_file(path)\r\n        data = dicom.pixel_array\r\n        if voi_lut:\r\n            data = apply_voi_lut(dicom.pixel_array, dicom)\r\n        else:\r\n            data = dicom.pixel_array\r\n\r\n        if rotate > 0:\r\n            rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\r\n            data = cv2.rotate(data, rot_choices[rotate])\r\n\r\n        data = cv2.resize(data, (img_size, img_size))\r\n        return data\r\n\r\n    \r\n\r\n    def __load_dicom_images_3d(self, scan_id, num_imgs=NUM_IMAGES, img_size=IMAGE_SIZE, mri_type=\"FLAIR\", split=\"train\",\r\n                               rotate=0):\r\n\r\n        files = sorted(glob.glob(f\"{data_directory}/{split}/{scan_id}/{mri_type}/*.dcm\"),\r\n                       key=lambda var: [int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\r\n\r\n        middle = len(files) // 2\r\n        num_imgs2 = num_imgs // 2\r\n        p1 = max(0, middle - num_imgs2)\r\n        p2 = min(len(files), middle + num_imgs2)\r\n        img3d = np.stack([self.__load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]).T\r\n        if img3d.shape[-1] < num_imgs:\r\n            n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\r\n            img3d = np.concatenate((img3d, n_zero), axis=-1)\r\n\r\n        if np.min(img3d) < np.max(img3d):\r\n            img3d = img3d - np.min(img3d)\r\n            img3d = img3d / np.max(img3d)\r\n\r\n        return np.expand_dims(img3d, 0)\r\n\r\n    def __len__(self):\r\n        return len(self.indices) // self.batch_size\r\n\r\n    def __get_data(self, data):\r\n        data = np.array(data)\r\n        images = []\r\n        X = []\r\n        Y = []\r\n        for id in data:\r\n            images.append([self.__load_dicom_images_3d(scan_id=id[0]), id[1]])\r\n        for img in images:\r\n            X.append(img[0])\r\n            Y.append(img[1])\r\n        Y = list(map(int,Y))\r\n        return np.array(X), np.array(Y)\r\n\r\n    def __getitem__(self, index):\r\n        print(index)\r\n        data = self.data[index * self.batch_size:(index + 1) * self.batch_size]\r\n        x, y = self.__get_data(data)\r\n       \r\n        return x, y\r\n```\r\n\r\nand the 3D CovNet is this:\r\n\r\n```python\r\nclass MultiBranchCNN(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MultiBranchCNN,self).__init__()\r\n        # self.inputA = tf.keras.Input(shape=(1,256,256,64))\r\n\r\n        self.conv3d = Conv3D(64, input_shape=(1,256,256,64),kernel_size=(3, 3,3), activation='relu', padding='same')\r\n        self.maxpool3d = MaxPool3D(pool_size=(3,3, 3))\r\n        self.conv3d2 = Conv3D(64, kernel_size=(3,3, 3), activation='relu', padding='same')\r\n        self.maxpool3d2 = MaxPool3D(pool_size=(3,3 ,3))\r\n        self.conv3d3 = Conv3D(64, kernel_size=(3,3, 3), activation='relu', padding='same')\r\n        self.maxpool3d3 = MaxPool3D(pool_size=(3,3, 3))\r\n        self.Flatten = Flatten()\r\n        self.Dense = Dense(512, activation='relu')\r\n        self.Dropout = Dropout(0.1)\r\n        self.Dense2 = Dense(1, activation='sigmoid')\r\n\r\n    def call(self, inputs):\r\n        print(type(inputs))\r\n        # x = self.inputA(inputs)\r\n        x = self.conv3d(inputs)\r\n        x = self.maxpool3d(x)\r\n        x = self.conv3d2(x)\r\n        x = self.maxpool3d2(x)\r\n        x = self.conv3d3(x)\r\n        x = self.maxpool3d3(x)\r\n        x = self.Flatten(x)\r\n        x = self.Dense(x)\r\n        x = self.Dropout(x)\r\n        x = self.Dense2(x)\r\n        return x\r\n\r\n```\r\nThis error occurs in `Epoch 1/10` (no train happens at all)\r\nI've been trying to change the model architecture and Dataset shape but with no luck.\r\n \r\nIs this a bug or I am doing something wrong?\r\nThanks in advance\r\n", "comments": ["@makisgrammenos,\r\n\r\nCan you take a look at this similar issues [link1](https://github.com/keras-team/keras/issues/9870), [link2](https://github.com/matterport/Mask_RCNN/issues/521) and this [SO thread](https://stackoverflow.com/questions/51704365/tensorflow-check-failed-work-element-count-0/53586477)", "Already checked them but nothing helped so far.\r\n I have added some print functions to help debug this problem int `__getitem__`  (data generator) and  in  `__call__` (model)\r\nhere is the updated version\r\n\r\nget item\r\n```python\r\n   def __getitem__(self, index):\r\n       \r\n        data = self.data[index * self.batch_size:(index + 1) * self.batch_size]\r\n        x, y = self.__get_data(data)\r\n        print(x.shape , y.shape , index)\r\n       \r\n        return x, y\r\n```\r\n\r\n\r\ncall function:\r\n```python\r\n\r\ndef call(self, inputs):\r\n        print(type(inputs))\r\n        print(inputs)\r\n        # x = self.inputA(inputs)\r\n        x = self.conv3d(inputs)\r\n        \r\n        x = self.maxpool3d(x)\r\n        \r\n        x = self.conv3d2(x)\r\n        \r\n        x = self.maxpool3d2(x)\r\n      \r\n        x = self.conv3d3(x)\r\n      \r\n        x = self.maxpool3d3(x)\r\n       \r\n        x = self.Flatten(x)\r\n       \r\n        x = self.Dense(x)\r\n       \r\n        x = self.Dropout(x)\r\n       \r\n        return self.Dense2(x)\r\n```\r\n\r\nand here is the output:\r\n```\r\n2021-09-04 17:30:08.821150: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-04 17:30:10.758939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4628 MB memory:  -> device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n(4, 1, 256, 256, 64) (4,) 0\r\n<class 'tensorflow.python.framework.ops.EagerTensor'>\r\ntf.Tensor(\r\n[[[[[0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    ...\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]]\r\n\r\n   [[0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    ...\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]]\r\n\r\n   [[0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    ...\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]]\r\n\r\n   ...\r\n\r\n   [[0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    ...\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]]\r\n\r\n   [[0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    ...\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]]\r\n\r\n   [[0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    ...\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]\r\n    [0.00025782 0.00025896 0.         ... 0.00019592 0.\r\n     0.        ]]]]\r\n\r\n\r\n\r\n [[[[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   ...\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]]]\r\n\r\n\r\n\r\n [[[[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   ...\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]\r\n\r\n   [[0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    ...\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]\r\n    [0.         0.         0.         ... 0.         0.\r\n     0.        ]]]]\r\n\r\n\r\n\r\n [[[[0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    ...\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]]\r\n\r\n   [[0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    ...\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]]\r\n\r\n   [[0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    ...\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]]\r\n\r\n   ...\r\n\r\n   [[0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    ...\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]]\r\n\r\n   [[0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    ...\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]]\r\n\r\n   [[0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    ...\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]\r\n    [0.00016502 0.         0.00016319 ... 0.         0.00019763\r\n     0.        ]]]]], shape=(4, 1, 256, 256, 64), dtype=float32)\r\n2021-09-04 17:30:16.534560: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\r\n2021-09-04 17:30:20.413557: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nEpoch 1/10\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\nTensor(\"IteratorGetNext:0\", shape=(None, None, None, None, None), dtype=float32)\r\n<class 'tensorflow.python.framework.ops.Tensor'>\r\nTensor(\"IteratorGetNext:0\", shape=(None, None, None, None, None), dtype=float32)\r\n(4, 1, 256, 256, 64) (4,) 125\r\n2021-09-04 17:30:25.331454: F .\\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)\r\n\r\n(ai) C:\\Users\\makis\\rsna>\r\n```\r\n\r\nIt seems the tensors with ` shape=(None, None, None, None, None)` are causing the error but I can't figure out why since the data generator returns the batch normaly as it should\r\n\r\nPS: sorry for the long post", "Still no solution found.\r\nAnyone  know how to fix it?\r\n", "After hours of searching it seems that there's a bug in tensorflow when it comes to subclass a model using GPU with this  model architecture.\r\nBy using the functional API and the same model architecture everythign seems to work fine and the model trains as it should"]}, {"number": 51799, "title": "Compiling TF 2.6 in debug mode on Windows env.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r2.6.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.3/8\r\n- GPU model and memory: GTX 1060\r\n\r\n**Describe the current behavior**\r\nI try to compile my example program that utilises TF:\r\n\r\n```\r\n// tensorflow/cc/example/example.cc\r\n\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\nint main() {\r\n  using namespace tensorflow;\r\n  using namespace tensorflow::ops;\r\n  Scope root = Scope::NewRootScope();\r\n  // Matrix A = [3 2; -1 0]\r\n  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });\r\n  // Vector b = [3 5]\r\n  auto b = Const(root, { {3.f, 5.f} });\r\n  // v = Ab^T\r\n  auto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\r\n  std::vector<Tensor> outputs;\r\n  ClientSession session(root);\r\n  // Run and fetch v\r\n  TF_CHECK_OK(session.Run({v}, &outputs));\r\n  // Expect outputs[0] == [19; -3]\r\n  LOG(INFO) << outputs[0].matrix<float>();\r\n  return 0;\r\n}\r\n```\r\n\r\n```\r\nbazel build --local_ram_resources=HOST_RAM*.7 --config=dbg --config=windows --copt=/FS --copt=-nvcc_options=disable-warnings --linkopt=/DEBUG:FULL --strip=never --define=no_tensorflow_py_deps=true -s --verbose_explanations --subcommands=pretty_print //tensorflow/cc/example:example\r\n```\r\nThe compilation fails with exception:\r\n```\r\nLINK : warning LNK4286: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)'\r\nLINK : warning LNK4286: symbol 'TF_DeleteShapeHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)'\r\nLINK : warning LNK4217: symbol 'TF_DeleteDimensionHandle' defined in 'ops.lo.lib(ops.obj)' is imported by 'bitcast_op_lib.lo.lib(bitcast.obj)' in function '\"void __cdecl ComputeNewShape(struct TF_ShapeInferenceContext *,struct TF_ShapeHandle *,enum TF_DataType,enum TF_DataType,struct TF_Status *)\" (?ComputeNewShape@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_ShapeHandle@@W4TF_DataType@@2PEAUTF_Status@@@Z)'\r\nLINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'histogram_summary_op_lib.lo.lib(histogram_summary.obj)' in function '\"void __cdecl histogram_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?histogram_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'\r\nLINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'merge_summary_op_lib.lo.lib(merge_summary.obj)' in function '\"void __cdecl merge_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?merge_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'\r\nLINK : warning LNK4217: symbol 'TF_ShapeInferenceContextScalar' defined in 'ops.lo.lib(ops.obj)' is imported by 'summary_op_lib.lo.lib(summary.obj)' in function '\"void __cdecl scalar_summary_shape_inference_fn(struct TF_ShapeInferenceContext *,struct TF_Status *)\" (?scalar_summary_shape_inference_fn@@YAXPEAUTF_ShapeInferenceContext@@PEAUTF_Status@@@Z)'\r\nLINK : warning LNK4217: symbol 'TF_NumDims' defined in 'tf_tensor.lib(tf_tensor.obj)' is imported by 'tensor_shape_utils.lib(tensor_shape_utils.obj)' in function '\"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::ShapeDebugString(struct TF_Tensor *)\" (?ShapeDebugString@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAUTF_Tensor@@@Z)'\r\nLINK : warning LNK4217: symbol 'TF_Dim' defined in 'tf_tensor.lib(tf_tensor.obj)' is imported by 'tensor_shape_utils.lib(tensor_shape_utils.obj)' in function '\"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::ShapeDebugString(struct TF_Tensor *)\" (?ShapeDebugString@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAUTF_Tensor@@@Z)'\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@E$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBE$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@E$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@E@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::Variant,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,bool,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<double>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<float>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,double,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,float,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<float,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@M$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@M$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,float>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Ubfloat16@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Ubfloat16@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::half,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@Uhalf@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,signed char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned int,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@I$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBI$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@I$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned int>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@I@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,__int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::ThreadPoolDevice,unsigned __int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UThreadPoolDevice@Eigen@@_K$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned __int64>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_K@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::Variant,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,bool,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<double>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,class std::complex<float>,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,double,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,float,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<float,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@M$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBM$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@M$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,float>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@M@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Ubfloat16@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,struct Eigen::bfloat16>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Ubfloat16@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,struct Eigen::half,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::half,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@Uhalf@2@$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Uhalf@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,struct Eigen::half>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Uhalf@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,signed char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned char,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@E$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBE$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@E$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@E@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned short,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned int,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned int,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@I$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBI$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@I$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned int>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@I@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,__int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::ThreadPoolDevice,unsigned __int64,1>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UThreadPoolDevice@Eigen@@_K$00@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_K$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned __int64>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_K@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nbazel-out\\x64_windows-dbg\\bin\\tensorflow\\cc\\example\\example.exe : fatal error LNK1120: 36 unresolved externals\r\nTarget //tensorflow/cc/example:example failed to build\r\nINFO: Elapsed time: 5521.801s, Critical Path: 3434.58s\r\nINFO: 2724 processes: 112 internal, 2612 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Hi @jvishnuvardhan ,Could you please look at this issue!", "The error comes from 2 files:\r\ntensorflow/core/kernels/depthtospace_op.cc and tensorflow/core/kernels/spacetodepth_op.cc\r\n\r\nI think it's related to another bug report https://github.com/tensorflow/tensorflow/issues/41118\r\n\r\n1) depthtospace_op.cc\r\nI added #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\r\nand #endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\r\nbetween lines 115-130, because this code is related only to GPU\r\n\r\n2) Same fix for tensorflow/core/kernels/spacetodepth_op.cc\r\nfor lines 129-149 & 155-156\r\n\r\nThe compilation passed.\r\n\r\nYou should fix it in a repo as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51799\">No</a>\n", "Another reference: https://stackoverflow.com/a/63339256/1100913", "By the way, the issue comes from the fact that I build tensorflow without GPU support."]}, {"number": 51794, "title": "[TFLite] Change rounding mode of the quantized RESIZE_BILINEAR op for coherence with MultiplyByQuantizedMultiplier", "body": "Hello,\r\n\r\nThis PR changes the rounding mode of the `ResizeBilinearInteger` kernel to be more coherent with the rounding of the `MultiplyByQuantizedMultiplier` method (its single-rounding version #50290).\r\n\r\nThibaut", "comments": ["@jianlijianli @thaink could you review this PR?", "@jianlijianli, @thaink  Can you please review this PR ? Thanks!", "@jianlijianli, @thaink Can you please review this PR ? Thanks!", "Thanks Tessil. The change looks fine. Is it possible to add a test case that behaves differently between the two rounding implementations?", "I added two tests with negative values (it seems the resize on negative values wasn't tested). The int16 test would generate a different result with the old rounding implementation."]}, {"number": 51785, "title": "[TFLite] Add INT8 and INT16x8 support to NON_MAX_SUPPRESSION_V4 / V5 operators", "body": "This pull request adds a fully quantized INT8 and INT16x8 implementation for the TensorFlow Lite NON_MAX_SUPPRESSION_V4  and NON_MAX_SUPPRESSION_V5 operators.", "comments": ["Could you share why these ops are needed for your case to understand the contexts around this PR? e.g., a certain model should have int16x8 op support like this one.\r\n\r\nWe are trying to keep the TFLite binary slim if possible if there are no valid use cases. ", "Thanks for sharing the contexts! @jianlijianli @thaink \r\nCould you review this PR?", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/51785\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51785) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51785) for more info**.\n\n<!-- need_author_consent -->", "@arovir01 Can you please resolve conflicts? Thanks!", "> @arovir01 Can you please resolve conflicts? Thanks!\r\n\r\nDone.", "@jianlijianli Can you please review this PR ? Thanks!", "I suspect the versioning needs to be updated from 2.7 to at least 2.8, as 2.7 has already since been released.", "> I suspect the versioning needs to be updated from 2.7 to at least 2.8, as 2.7 has already since been released.\r\n\r\nDone.", "For information here are some quick results of the Yolo v3 416x416 model using the quantized NMS op of this PR with the COCO 2014 dataset. The whole model is quantized except the EXP operator (for which a TABLE op could be used). \r\n\r\nWe used 1000 images from the training set for calibration and 2000 images of the validation set for the tests as we don't have the ground truth values for coco-test (only 2000 images were used for a quick test as some of the int16 kernels are not yet well optimized and the whole set would take a long time). Score threshold of 0.3, max boxes of 100 and an IoU of 0.5 is used for the mAP score.\r\n\r\n| Type | mAP  |\r\n| ----- |-------------:|\r\n| float32 | 0.566 |\r\n| int8   | 0.523 |\r\n| int16 |  0.565 | ", "@arovir01 Can you please resolve conflicts? Thanks!", "Done.\r\n", "Since I have moved on to another team & project, I have also added @Tessil as a collaborator on my repo. This should allow him to fix merge conflicts and push commits whenever required.", "@Tessil  Can you please resolve conflicts? Thank you!"]}, {"number": 51771, "title": "tf.vectorized_map fails on ragged tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 21.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.6.0\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: 2080ti\r\n\r\n**Describe the current behavior**\r\n\r\ntf.vectorized_map fails on RaggedTensor\r\n\r\n**Describe the expected behavior**\r\n\r\ntf.vectorized_map succeeds on RaggedTensor\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.__version__ # 2.6.0\r\n\r\nA1 = tf.constant([[1,3], [1,5]], dtype=tf.float32)\r\n\r\nb1 = tf.constant([3,4], shape = (2,1), dtype=tf.float32)\r\n\r\ntf.linalg.lstsq(A1,b1) # SUCCEEDS, single task\r\n\r\nA2 = tf.constant([[1,3], [1,4], [1,5]], dtype=tf.float32)\r\n\r\nb2 = tf.constant([2,3,4], shape = (3,1), dtype=tf.float32)\r\n\r\ntf.linalg.lstsq(A2,b2) # SUCCEEDS, single task\r\n\r\nA1A1 = tf.stack([A1,A1])\r\n\r\nb1b1 = tf.stack([b1,b1])\r\n\r\ntf.linalg.lstsq(A1A1, b1b1) # SUCCEEDS, parallel task\r\n\r\nA1A1ragged = tf.ragged.stack([A1,A1])\r\n\r\ntf.linalg.lstsq(A1A1ragged, b1b1) # FAILS TypeError: object of type 'RaggedTensor' has no len()\r\n\r\n@tf.function\r\ndef least_squares(task):\r\n    A, b = task\r\n    return tf.linalg.lstsq(A.to_tensor(), b)\r\n\r\ntf.stack([least_squares(task) for task in zip(A1A1ragged, b1b1)]) # SUCCEEDS, serial task\r\n\r\ntf.map_fn(least_squares, (A1A1ragged, b1b1), dtype = tf.float32) # SUCCEEDS, possible but not guaranteed parallel task\r\n\r\ntf.vectorized_map(least_squares, (A1A1ragged, b1b1))  # FAILED, ValueError: Received a shape scalar with unknown static value\r\n\r\nA1A2ragged = tf.ragged.stack([A1,A2])\r\n\r\nb1b2 = tf.ragged.stack([b1,b2])\r\n\r\n@tf.function\r\ndef least_squares_ragged(task):\r\n    A, b = task\r\n    return tf.linalg.lstsq(A.to_tensor(), b.to_tensor())\r\n\r\ntf.ragged.stack([least_squares_ragged(task) for task in zip(A1A2ragged, b1b2)])  # SUCCEED, but serial\r\n\r\ntf.map_fn(least_squares_ragged, (A1A2ragged, b1b2), dtype = tf.float32) # SUCCEED,not guaranteed parallel\r\n\r\ntf.vectorized_map(least_squares_ragged, (A1A2ragged, b1b2)) # FAIL  ValueError: Received a shape scalar with unknown static value\r\n\r\n```\r\n\r\n", "comments": ["Hi @sanatmpa1 ,Could you please look at this issue , providing [gist](https://colab.research.google.com/gist/mohantym/ad7afdd8f8f312b5c84b2c4d7d417696/github_51771.ipynb#scrollTo=Q-WhCzmqRkiw) for reference."]}, {"number": 51770, "title": "TensorFlow master build fails on s390x due to boringssl issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\nReference issues: https://github.com/tensorflow/tensorflow/issues/14039, https://github.com/tensorflow/tensorflow/issues/50351\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.7.0 (from master branch)\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): bazel 3.7.2- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nTensorFlow master build is failing on `s390x` due to `boringssl` issue. It looks like some recent commits in master branch have triggered it. \r\n\r\nI made some tests and it looks like this commit (https://github.com/tensorflow/tensorflow/commit/738e5aa37935f713a2366fc00a26d1d5830cb971) trigged the issue (the previous commit didn't trigger it). This commit has the dependency of `@com_github_grpc_grpc` which needs `boringssl` and the code appears to be in the `experimental` folder.\r\n\r\nSince `boringssl` is still not supported on `s390x`, is there any way to disable this part on `s390x` build? Thank you very much!\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" build  --define=tensorflow_mkldnn_contraction_kernel=0 //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nor\r\n\r\n`bazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" build  --define=tensorflow_mkldnn_contraction_kernel=0 --config=noaws //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```bash\r\nERROR: /home/test/Tensorflow_tmp/_bazel_test/78e38be8987e210d400487db993cf28d/external/com_github_grpc_grpc/BUILD:2013:16: C++ compilation of rule '@com_github_grpc_grpc//:tsi' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 46 argument(s) skipped)\r\nIn file included from external/boringssl/src/include/openssl/ssl.h:145:0,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session.h:29,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session_cache.cc:23:\r\nexternal/boringssl/src/include/openssl/base.h:122:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n  ^~~~~\r\nIn file included from external/boringssl/src/include/openssl/asn1.h:68:0,\r\n                 from external/boringssl/src/include/openssl/x509.h:70,\r\n                 from external/boringssl/src/include/openssl/pem.h:67,\r\n                 from external/boringssl/src/include/openssl/ssl.h:149,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session.h:29,\r\n                 from external/com_github_grpc_grpc/src/core/tsi/ssl/session_cache/ssl_session_cache.cc:23:\r\nexternal/boringssl/src/include/openssl/bn.h:165:2: error: #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n```\r\n```bash\r\nERROR: /home/test/Tensorflow_tmp/_bazel_test/78e38be8987e210d400487db993cf28d/external/boringssl/BUILD:130:11: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 24 argument(s) skipped)\r\nIn file included from external/boringssl/src/include/openssl/digest.h:60:0,\r\n                 from external/boringssl/src/crypto/x509v3/v3_skey.c:61:\r\nexternal/boringssl/src/include/openssl/base.h:122:2: error: #error \"Unknown target CPU\"\r\n #error \"Unknown target CPU\"\r\n  ^~~~~\r\nIn file included from external/boringssl/src/include/openssl/asn1.h:68:0,\r\n                 from external/boringssl/src/include/openssl/x509.h:70,\r\n                 from external/boringssl/src/include/openssl/x509v3.h:60,\r\n                 from external/boringssl/src/crypto/x509v3/v3_skey.c:64:\r\nexternal/boringssl/src/include/openssl/bn.h:165:2: error: #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n```\r\n", "comments": ["@kun-lu20 Could you please refer to the similar issues [link](https://github.com/tensorflow/tensorflow/issues/17587), [link1](https://github.com/tensorflow/tensorflow/issues/20014) and let us know if it helps ?Thank you!", "@sushreebarsa Thanks! I'll look into them and get back to you later.", "Hi @sushreebarsa , these links really help! Issue https://github.com/tensorflow/tensorflow/issues/40049 is similar as well.\r\n\r\nIt looks like the root cause is that the recent commit https://github.com/tensorflow/tensorflow/commit/738e5aa37935f713a2366fc00a26d1d5830cb971 has the dependency of secure variant of `GRPC`, which depends on `boringssl`, while `boringssl` does not support big-endian systems at this moment. \r\n\r\nI tried to replace each `@com_github_grpc_grpc//:grpc++` with `//tensorflow:grpc++` in `tensorflow/distribute/experimental/rpc/kernels/BUILD`, which will conditionally choose secure/unsecure variant of `GRPC` as per the target arch, now the master build works fine.\r\n\r\nCould you please fix this issue at your end so that TF master build won't be broken on s390x? Thank you very much!\r\n\r\n\r\n", "@kun-lu20 Thank you for the update! The recent commit 738e5aa  is in work in progress. We recommend you to use to stable Version.After replacing each`@com_github_grpc_grpc//:grpc++` with `//tensorflow:grpc++ ` in `tensorflow/distribute/experimental/rpc/kernels/BUILD`, which will conditionally choose secure/unsecure variant of `GRPC` as per the target arch, if the the master build works fine, Could you please close the ticket ?We will definitely work on the TF master build so that it won't be broken on s390x.Thank you!", "Closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51770\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51770\">No</a>\n", "Hi @sushreebarsa  , Happy New Year!\r\n\r\nCould you please help me check if this issue has been fixed on TF master build? Thank you!", "Hi @sushreebarsa , hope all is well with you.\r\n\r\nLooks like `@com_github_grpc_grpc//:grpc++` is still being used in `tensorflow/distribute/experimental/rpc/kernels/BUILD` on master branch and version 2.8.0. \r\n\r\nCould you please help me check if there are any updates on this issue? Thank you very much!"]}, {"number": 51766, "title": "Issue about using Nsight System on Tensorflow2.6.0", "body": "**System information**\r\n\r\n\r\n\\- OS Platform Linux Ubuntu 18.04.5\r\n\r\n\\- TensorFlow installed from **docker**(tensorflow/tensorflow:2.6.0rc2-gpu)\r\n\r\n\\- TensorFlow version :**2.6.0rc2-gpu** (default)\r\n\r\n\\- Python version:**3.6.9** (default)\r\n\r\n\\- CUDA version:**11.2** (default)\r\n\r\n\\- cuDNN version:**8.1.0** (default)\r\n\r\n\\- GPU model and memory:**NVIDIA A6000** / 48685MiB\r\n\r\n \r\n\r\n**Describe the current behavior**\r\n\r\nWhen using Nsight System to profile my training, I found that there were only cuDNN and cuBLAS in my profiling file like this, but CUDA API and CUDA HW was missing.\r\n\r\n \r\n![1](https://user-images.githubusercontent.com/89841382/131491418-ef079c92-c7b9-47e8-a557-7bf57950ff0c.png)\r\n\r\n\r\nBut when I change the tensorflow version from 2.6.0rc2 to 2.4.3(with CUDA 11.0), the CUDA API and CUDA HW appeared.\r\n\r\n \r\n![2](https://user-images.githubusercontent.com/89841382/131491433-f980c222-5c22-480c-9661-af95bf80bf4e.jpg)\r\n\r\n\r\nI'd like to know the cause of this problem and what  I can do to make CUDA API and CUDA HW appear on Tensorflow2.6.0.rc2.\r\n\r\nThank you.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe run scripts is like this.\r\n\r\n\r\n```\r\nnsys profile -t cuda,osrt,nvtx,cudnn,cublas \\\r\n       -o $1 \\\r\n       -f true \\\r\n       -w true \\\r\n       --sampling-period 2000000\\\r\n       python train.py\r\n\r\n```\r\n\r\nThe core code of train.py is like this.\r\n\r\n```python\r\n# ---preprocess dataset---\r\n# ---get model and compile\r\n# Add nvtx to nsys\r\nimport ctypes\r\n_cuda_tools_ext = ctypes.CDLL(\"libnvToolsExt.so\")\r\n_cuda_tools_ext.nvtxRangePushA(ctypes.c_char_p(\"start train\".encode('utf-8')))\r\n\r\nmodel.fit(train_db, \r\n\tepochs=1, \r\n\tvalidation_data = test_db,\r\n\tvalidation_freq=1)\r\n_cuda_tools_ext.nvtxRangePop()\r\n\r\n```", "comments": ["@nluehr Do you happen to know if this is an issue with TensorFlow or with Nsight?", "@duck7216 What version of nsys is used for profiling, and is the is the same version used in both cases? "]}, {"number": 51765, "title": "Updated log_poisson_loss() function", "body": "result is updated in the log_poisson_loss(targets, log_input, compute_full_loss=False, name=None) function. For input values too large, error message is displayed instead of returning NaN.", "comments": ["@Aravind-11 \r\nLine 98 \r\n`result = math_ops.exp(log_input) - log_input * target` \r\nIt should be targets instead of target.", "@Aravind-11  Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!\r\n", "@Aravind-11 Any update on this PR? Please. Thanks!", "@Aravind-11 Any update on this PR? Please. Thanks!", "I'm not sure how to add unit test. Can someone help?\r\n", "@Aravind-11 first of could you please raise an issue where this problem was seen or maybe share a notebook or gist, as @mihaimaruseac pointed out? Next for every operations there is a unit test code. All you need to add is a piece of code showing the changes solve the problem. \r\nI guess this helps. Thanks", "@Aravind-11, @mihaimaruseac Any update on this PR? Please. Thanks!", "@Aravind-11, @mihaimaruseac Any update on this PR? Please. Thanks!", "@Aravind-11, @mihaimaruseac Any update on this PR? Please. Thanks!", "@Aravind-11, @mihaimaruseac Any update on this PR? Please. Thanks!", "Error message is the way to go.\n\nOn Tue, 29 Mar 2022 at 7:18 PM, Surya Prakash Mishra <\n***@***.***> wrote:\n\n> ***@***.**** commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/nn_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/51765#discussion_r837499362>\n> :\n>\n> > +    if(!result.is_nan()):\n> +      raise ValueError(\n> +          \"Input contains NaN, infinity or a value too large for dtype('float64') (%s vs %s)\" %\n> +          (log_input.get_shape(), targets.get_shape()))\n>\n> @Aravind-11 <https://github.com/Aravind-11> can you please answer to\n> this. Should it return NaN or an error message is a better way to go?\n> TIA\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51765#discussion_r837499362>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKDCEOVSLAXUBJAXPV6DSSTVCMC2ZANCNFSM5DD3FICQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "Still needs a unit test before we can take it", "Could you provide guidance on how to do that ?\n\nOn Tue, 29 Mar 2022 at 8:31 PM, Mihai Maruseac ***@***.***>\nwrote:\n\n> Still needs a unit test before we can take it\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51765#issuecomment-1081981909>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKDCEOVPYTYS7J4BBFREVITVCMLLJANCNFSM5DD3FICQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_test.py", "Also, please change PR title, https://cbea.ms/git-commit/", "Sure!\n\nOn Wed, 30 Mar 2022 at 1:30 AM, Mihai Maruseac ***@***.***>\nwrote:\n\n> Also, please change PR title, https://cbea.ms/git-commit/\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51765#issuecomment-1082318461>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKDCEOW5G36ITHPRGD3S7JDVCNONZANCNFSM5DD3FICQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "@Aravind-11 Any update on this PR? Please. Thank you!"]}, {"number": 51760, "title": "tf.distribute.MirroredStrategy does not work via srun (SLURM) causes NCCL error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (Ubuntu 20.04.2 LTS)\r\n- TensorFlow installed from container 21.05 (https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_21-05.html#rel_21-05)\r\n- TensorFlow version (2.5):\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.3.0/8.2.0.51\r\n- GPU model and memory: A100\r\n\r\nThe distributed training runs fails when training via slurm (using srun).\r\n\r\nThe code is run inside an enroot container. Due to slurm this container has a number of slurm specific environment variables set.\r\n\r\nSo, using ```MirroredStrategy``` to distribute training fails due to NCCL errors on a simple example.\r\n\r\nNote, a number of other distributed options work as is highlighted in the code.\r\n\r\nNOTE, this code works fine outside of the slurm environment (in the exact same container). The slurm environment variables seem to be creating an issue with NCCL. \r\n\r\nThe srun command looks like\r\n\r\n```\r\nsrun \\\r\n        --container-image $container_path \\\r\n        -N1 \\\r\n        --nodelist=$node_name \\\r\n        --gpus-per-node=$gpus_per_node \\\r\n        --cpus-per-task=$cpus_per_task \\\r\n        --mem=$DEFAULT_RAM \\\r\n```\r\n\r\n```python\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# Can't use tf.distribute.MirroredStrategy in srun (slurm) enviroment\r\n\r\n# Tried with tf 2.5 and tf nightly.\r\n\r\nimport tensorflow as tf\r\n\r\n# Force dynamic memory growth\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\ntf.__version__\r\n\r\n\r\n# op 1 . NCCL error in slurmn enviroment. Works fine inside enroot container (not submitted via srun)\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\n# op 2. Not using NCCL. Works.\r\n#strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n\r\n# op 2. Works in slurmn enviroment. Needs to be optimized\r\n#slurm_resolver = tf.distribute.cluster_resolver.SlurmClusterResolver()\r\n#strategy = tf.distribute.MultiWorkerMirroredStrategy(cluster_resolver=slurm_resolver)\r\n\r\n# op 3 # Works in slurmn enviroment\r\n#strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\n\r\nwith strategy.scope():\r\n\r\n    model = models.Sequential()\r\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\n    model.add(layers.MaxPooling2D((2, 2)))\r\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(layers.MaxPooling2D((2, 2)))\r\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(layers.Flatten())\r\n    model.add(layers.Dense(64, activation='relu'))\r\n    \r\n    model.add(layers.Dense(10))\r\n    # ADD sync bn..\r\n    model.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n\r\n\r\nhistory = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n```\r\n\r\nError\r\n\r\n```\r\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\r\n2021-08-31 15:13:58.031219: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-08-31 15:13:58.050668: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2245715000 Hz\r\nEpoch 1/10\r\n2021-08-31 15:14:03.739139: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-08-31 15:14:04.423163: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:05.254400: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:05.933033: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-08-31 15:14:06.133786: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:07.302772: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:07.895167: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-08-31 15:14:08.600313: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:09.692392: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:10.554939: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:11.510503: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200\r\n2021-08-31 15:14:12.416170: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\nhai-a100-3:3778851:3779455 [6] NCCL INFO Bootstrap : Using enp226s0:10.16.2.21<0>\r\nhai-a100-3:3778851:3779455 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\r\nhai-a100-3:3778851:3779455 [6] NCCL INFO P2P plugin IBext\r\nhai-a100-3:3778851:3779455 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_6:1/IB/SHARP [3]mlx5_8:1/IB/SHARP [4]mlx5_4:1/RoCE [5]mlx5_10:1/RoCE ; OOB enp226s0:10.16.2.21<0>\r\nhai-a100-3:3778851:3779455 [6] NCCL INFO Using network IBext\r\nNCCL version 2.8.3+cudaCUDA_MAJOR.CUDA_MINOR\r\n\r\nhai-a100-3:3778851:3779883 [4] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:3778851:3779883 [4] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n\r\nhai-a100-3:3778851:3779884 [5] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\n\r\nhai-a100-3:3778851:3779885 [6] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:3778851:3779885 [6] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\nhai-a100-3:3778851:3779884 [5] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n\r\nhai-a100-3:3778851:3779880 [1] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed\r\nhai-a100-3:3778851:3779880 [1] NCCL INFO ib_plugin.c:284 -> 2\r\nhai-a100-3:3778851:3779880 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\n\r\nhai-a100-3:3778851:3779886 [7] ibvwrap.c:118 NCCL WARN Call to ibv_create_cq failed\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO ib_plugin.c:174 -> 2\r\nhai-a100-3:3778851:3779880 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\n\r\nhai-a100-3:3778851:3779879 [0] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:3778851:3779880 [1] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n\r\nhai-a100-3:3778851:3779881 [2] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO ib_plugin.c:322 -> 2\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:22 -> 2\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:52 -> 2\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:3778851:3779886 [7] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\nhai-a100-3:3778851:3779881 [2] NCCL INFO ib_plugin.c:284 -> 2\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:3778851:3779881 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:3778851:3779881 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:3778851:3779879 [0] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\nhai-a100-3:3778851:3779881 [2] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n\r\nhai-a100-3:3778851:3779882 [3] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:3778851:3779882 [3] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n2021-08-31 15:14:17.616013: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-08-31 15:14:17.616262: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-08-31 15:14:17.616307: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-08-31 15:14:17.616347: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-08-31 15:14:17.616383: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-08-31 15:14:17.616413: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-08-31 15:14:17.616444: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-08-31 15:14:17.616475: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at nccl_ops.cc:104 : Internal: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\nTraceback (most recent call last):\r\n  File \"basic_distributed_minst_v4.py\", line 102, in <module>\r\n    history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\", line 1183, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 950, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 3023, in __call__\r\n    return graph_function._call_flat(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1960, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 591, in call\r\n    outputs = execute.execute(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InternalError: 8 root error(s) found.\r\n  (0) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n  (1) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n         [[Adam/Adam/group_deps/_291]]\r\n  (2) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n         [[Adam/Adam/group_deps/_295]]\r\n  (3) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n         [[Adam/Adam/group_deps/_299]]\r\n  (4) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n         [[Adam/Adam/group_deps/_303]]\r\n  (5) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n         [[Adam/Adam/group_deps/_307]]\r\n  (6) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n         [[Adam/Adam/group_deps/_311]]\r\n  (7) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce (defined at basic_distributed_minst_v4.py:102) ]]\r\n         [[Adam/Adam/group_deps/_315]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_9445]\r\n```\r\nError when using 21.07 container and tf-nightly\r\n```\r\n Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\r\nEpoch 1/10\r\n2021-09-01 09:42:11.001143: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-01 09:42:12.030493: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-01 09:42:13.145586: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-01 09:42:14.594286: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-01 09:42:15.853778: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-01 09:42:17.049535: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n2021-09-01 09:42:17.237328: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-01 09:42:18.567093: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\n2021-09-01 09:42:19.610511: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\r\nhai-a100-3:376586:376986 [6] NCCL INFO Bootstrap : Using enp226s0:10.16.2.21<0>\r\nhai-a100-3:376586:376986 [6] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\r\nhai-a100-3:376586:376986 [6] NCCL INFO P2P plugin IBext\r\nhai-a100-3:376586:376986 [6] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB/SHARP [1]mlx5_3:1/IB/SHARP [2]mlx5_6:1/IB/SHARP [3]mlx5_8:1/IB/SHARP [4]mlx5_4:1/RoCE [5]mlx5_10:1/RoCE ; OOB enp226s0:10.16.2.21<0>\r\nhai-a100-3:376586:376986 [6] NCCL INFO Using network IBext\r\nNCCL version 2.8.3+cudaCUDA_MAJOR.CUDA_MINOR\r\n\r\nhai-a100-3:376586:377446 [7] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:376586:377446 [7] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:376586:377446 [7] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:376586:377446 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:376586:377446 [7] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\n\r\nhai-a100-3:376586:377440 [1] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed\r\nhai-a100-3:376586:377440 [1] NCCL INFO ib_plugin.c:284 -> 2\r\nhai-a100-3:376586:377440 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:376586:377440 [1] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\n\r\nhai-a100-3:376586:377444 [5] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:376586:377444 [5] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:376586:377444 [5] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:376586:377444 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:376586:377444 [5] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\n\r\nhai-a100-3:376586:377439 [0] ibvwrap.c:106 NCCL WARN Call to ibv_reg_mr failed\r\nhai-a100-3:376586:377439 [0] NCCL INFO ib_plugin.c:284 -> 2\r\nhai-a100-3:376586:377439 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:376586:377439 [0] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377440 [1] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\nhai-a100-3:376586:377446 [7] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\nhai-a100-3:376586:377444 [5] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n\r\nhai-a100-3:376586:377445 [6] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:376586:377445 [6] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:376586:377445 [6] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:376586:377445 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:376586:377445 [6] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:376586:377445 [6] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\nhai-a100-3:376586:377439 [0] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n\r\nhai-a100-3:376586:377443 [4] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\n\r\nhai-a100-3:376586:377442 [3] ibvwrap.c:118 NCCL WARN Call to ibv_create_cq failed\r\nhai-a100-3:376586:377442 [3] NCCL INFO ib_plugin.c:174 -> 2\r\nhai-a100-3:376586:377442 [3] NCCL INFO ib_plugin.c:322 -> 2\r\nhai-a100-3:376586:377442 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:22 -> 2\r\nhai-a100-3:376586:377442 [3] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:52 -> 2\r\nhai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:376586:377443 [4] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:376586:377443 [4] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:376586:377443 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:376586:377443 [4] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:376586:377442 [3] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\nhai-a100-3:376586:377443 [4] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n\r\nhai-a100-3:376586:377441 [2] ibvwrap.c:130 NCCL WARN Call to ibv_create_qp failed\r\nhai-a100-3:376586:377441 [2] NCCL INFO ib_plugin.c:196 -> 2\r\nhai-a100-3:376586:377441 [2] NCCL INFO ib_plugin.c:273 -> 2\r\nhai-a100-3:376586:377441 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:21 -> 2\r\nhai-a100-3:376586:377441 [2] NCCL INFO bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:51 -> 2\r\nhai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/init.cc:310 -> 2\r\nhai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/init.cc:577 -> 2\r\nhai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/init.cc:878 -> 2\r\nhai-a100-3:376586:377441 [2] NCCL INFO external/nccl_archive/src/group.cc:72 -> 2 [Async thread]\r\n2021-09-01 09:42:23.237419: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-09-01 09:42:23.237581: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-09-01 09:42:23.237606: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-09-01 09:42:23.237628: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-09-01 09:42:23.237649: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-09-01 09:42:23.237670: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-09-01 09:42:23.237689: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n2021-09-01 09:42:23.237711: W tensorflow/core/framework/op_kernel.cc:1694] OP_REQUIRES failed at nccl_ops.cc:104 : INTERNAL: NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\nTraceback (most recent call last):\r\n  File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n    history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n  File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 58, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InternalError: 8 root error(s) found.\r\n  (0) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n  (1) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n         [[Adam/Adam/group_deps/_307]]\r\n  (2) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n         [[Adam/Adam/group_deps/_311]]\r\n  (3) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n         [[Adam/Adam/group_deps/_315]]\r\n  (4) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n         [[Adam/Adam/group_deps/_291]]\r\n  (5) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n         [[Adam/Adam/group_deps/_295]]\r\n  (6) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n         [[Adam/Adam/group_deps/_299]]\r\n  (7) INTERNAL:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n         [[node Adam/NcclAllReduce\r\n (defined at /usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py:151)\r\n]]\r\n         [[Adam/Adam/group_deps/_303]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_9449]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nInput Source operations connected to node Adam/NcclAllReduce:\r\nIn[0] Adam/split:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"basic_distributed_minst_v5.py\", line 97, in <module>\r\n>>>     history = model.fit(train_images, train_labels, epochs=10, steps_per_epoch=100)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\r\n>>>     return fn(*args, **kwargs)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1185, in fit\r\n>>>     tmp_logs = self.train_function(iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 851, in train_function\r\n>>>     return step_function(self, iterator)\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 840, in step_function\r\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>>>\r\n>>>   File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/utils.py\", line 151, in _all_reduce_sum_fn\r\n>>>     return distribution.extended.batch_reduce_to(tf.distribute.ReduceOp.SUM,\r\n>>>\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function\r\n\r\n```\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function -> train_function\r\n", "comments": []}, {"number": 51759, "title": "[PluggableDevice] Add TF_IsHostMemoryInput and TF_IsHostMemoryOutput to the C API", "body": "This reduces boilerplate logic in pluggable devices code that decouple operator registration from kernel implementation. This also makes the transition from TF 1.15 to 2.x easier since the same device code in 1.15 would have been able to use `OpKernelContext::input_memory_type`.", "comments": ["@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "I'm sorry I get to this PR so late. Is this required for TF-DirectML for TF 2.9? Asking because we are having an API freeze starting tomorrow.", "@PatriceVignola Could you please help address @rohan100jain's comment? Thank you very much!", "@rohan100jain Thanks for the review! I removed the fatal checks and added a status instead.", "@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 51751, "title": "Tensorflow Lite C API + GPU delegate crash on exit", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master/nightly\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc 8.4.0\r\n- CUDA/cuDNN version: cuda 11.1\r\n- GPU model and memory: 4GB VRAM, 8GM system RAM\r\n\r\n**Describe the current behavior**\r\nUsing the C API of tensorflow lite, with GPU delegate (GPU = NVidia GTX 1650), I get a crash on exit. This only happens when I build the latest mater or nightly branch. **The v2.6.0 tag does not have this problem.** \r\nHere is my code to reproduce the issue:\r\n```\r\n#include \"tensorflow/lite/c/c_api.h\"\r\n#include \"tensorflow/lite/delegates/gpu/delegate.h\"\r\n\r\nint main(void)\r\n{\r\n    TfLiteGpuDelegateOptionsV2 opts = TfLiteGpuDelegateOptionsV2Default();\r\n\r\n    TfLiteDelegate* gpuDelegate = TfLiteGpuDelegateV2Create(&opts);\r\n    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n\r\n    TfLiteInterpreterOptionsAddDelegate(options, gpuDelegate);\r\n\r\n    //This problem should be be reproducible using any convolutional model\r\n    TfLiteModel* model = TfLiteModelCreateFromFile(\"../mymodel.tflite\");\r\n    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n\r\n    TfLiteInterpreterAllocateTensors(interpreter);\r\n\r\n    TfLiteInterpreterInvoke(interpreter);\r\n\r\n    TfLiteInterpreterDelete(interpreter);\r\n    TfLiteGpuDelegateV2Delete(gpuDelegate);\r\n    TfLiteInterpreterOptionsDelete(options);\r\n    TfLiteModelDelete(model);\r\n}\r\n```\r\n\r\nThe stacktrace I get from gdb is:\r\n```\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n\r\nThread 1 \"dlib_test\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffed25d1a0 in ?? ()\r\n   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1\r\n(gdb) bt\r\n#0  0x00007fffed25d1a0 in ?? ()\r\n   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1\r\n#1  0x00007fffed15655c in ?? ()\r\n   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1\r\n#2  0x00007fffed153b08 in ?? ()\r\n   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1\r\n#3  0x00007fffed155c22 in ?? ()\r\n   from /usr/lib/x86_64-linux-gnu/libnvidia-opencl.so.1\r\n#4  0x00007ffff7531a06 in tflite::gpu::cl::Buffer::Release() ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#5  0x00007ffff752fadf in tflite::gpu::cl::InferenceContext::~InferenceContext() ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#6  0x00007ffff7529369 in tflite::gpu::cl::(anonymous namespace)::InferenceRunnerImpl::~InferenceRunnerImpl() ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#7  0x00007ffff75222e2 in tflite::gpu::(anonymous namespace)::DelegatePrepare(TfLiteContext*, TfLiteDelegate*)::{lambda(TfLiteContext*, void*)#2}::_FUN(TfLiteContext*, void*) ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlit---Type <return> to continue, or q <return> to quit---\r\ne_c.so\r\n#8  0x00007ffff751c654 in tflite::Subgraph::CleanupNode(int) ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#9  0x00007ffff751c6bf in tflite::Subgraph::~Subgraph() ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#10 0x00007ffff751c879 in tflite::Subgraph::~Subgraph() ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#11 0x00007ffff7a00697 in tflite::Interpreter::~Interpreter() ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#12 0x00007ffff7516ef1 in TfLiteInterpreterDelete ()\r\n   from /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so\r\n#13 0x0000555555554bf8 in main ()\r\n```\r\nWhen I run my program using valgrind, I get:\r\n```\r\n==19931== Conditional jump or move depends on uninitialised value(s)\r\n==19931==    at 0x4FCAA64: tflite::gpu::ConvPowerVR::GenerateConv[abi:cxx11](tflite::gpu::GpuInfo const&, tflite::gpu::OperationDef const&, bool, tflite::gpu::ConvPowerVR::ConvParams const&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#3}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const [clone .isra.283] (in /home/<user>/Desktop/tensorflow_lite_c_master/lib/x86_64/libtensorflowlite_c.so)\r\n```\r\n", "comments": ["I am not sure but it looks like that this issue may come from not enough memory . At least when I not provide enough memory to my tensorflow program I get funny errors like illegal memory access or uninitialized layer .Can you confirm that you have enough memory? ", "@impjdi could you take a look at this?", "@vulkomilev I have a fairly powerful machine with 8GB of system ram and an NVidia GTX 1650 with 4GB vram. I was able to repro the issue using the code I showed above and a NasnetMobile (FP32) net I download from https://www.tensorflow.org/lite/guide/hosted_models . This is a relatively small model and shouldn't cause memory pressure. Also, as I mentioned in the original post, I don't get this crash when building tflite from v2.6.0 tag. So this feels more like a bug that was recently introduced."]}, {"number": 51728, "title": "More freedom in Dtype of indices of sparse.SparseTensors", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): v2.6.0\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nsparse.SparseTensors  'indices dtype are hardcoded as tf.int64. I would like to allow other integers types (tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.int32)\r\nsee https://github.com/tensorflow/tensorflow/blob/919f693420e35d00c8d0a42100837ae3718f7927/tensorflow/python/framework/sparse_tensor.py#L130\r\n\r\n**Will this change the current api? How?**\r\nneed to be investigated. I am not sure why this is hard-coded.\r\n**Who will benefit with this feature?**\r\nSmaller memory useage of SparseTensors which is needed in a lot of application with limited RAM.\r\n**Any Other info.**\r\n", "comments": []}, {"number": 51719, "title": "TensorRT model outputs/performance keeps changing across conversions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TensorFlow 2.6 , 2.7.0.dev20210825\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 11.0/8.0 and / 11.2/8.1\r\n- TensorRT: LIBNVINFER=7.2.2-1\r\n- GPU model and memory: v100 16GB\r\n\r\n**Describe the current behavior**\r\nI have been trying to convert an object detection model using the TensorRT converter.\r\nI can successfully convert the model and also get the desired boost in latency while still maintaining the same outputs/performance. But more often or so, I see that my model shows degraded performance.\r\n\r\nmAP score for `tf saved_model`: 40.3  (this does not change no matter how many times I run `tf.saved_model.save(...)`)\r\nmAP score for `tf-trt saved_model`:  the results keep changing on each conversion [40.4, 36.0] (detailed metrics below)\r\n\r\n```\r\nconversion run 1 (FP16)\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.360\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.528\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.391\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.175\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.531\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.323\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.488\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.250\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.571\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.713\r\n```\r\n\r\n```\r\nconversion run 2 (FP16)\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\r\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.598\r\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.434\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.220\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.453\r\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.575\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.333\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.522\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.549\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.331\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.610\r\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.732\r\n```\r\nThis happens for both FP16 and FP32. And I have made sure that there is absolutely nothing that is changed/different in my environment across runs.\r\n\r\nI am attaching the conversion logs for both the runs with\r\n`TF_CPP_VMODULE=trt_engine_op=2,convert_nodes=2,convert_graph=2,segment=2,trt_shape_optimization_profiles=2,trt_engine_resource_ops=2`\r\n\r\n**Describe the expected behavior**\r\nConsistent outputs/performance across conversions\r\n\r\n**Standalone code to reproduce the issue**\r\n[`saved_model` download_link](https://github.com/srihari-humbarwadi/retinanet-tensorflow2.x/releases/download/v0.1.0/mscoco-retinanet-resnet50-640x640-30x-256.zip)\r\n\r\ncode to convert \r\n```python\r\nimport os\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_TRT_ALLOW_NMS_TOPK_OVERRIDE'] = '1'\r\nprint(tf.__version__)\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\n[tf.config.experimental.set_memory_growth(x, True)\r\n for x in physical_devices]\r\n\r\nPRECISION = 'FP16'\r\n\r\nparams = tf.experimental.tensorrt.ConversionParams(\r\n    minimum_segment_size=10,\r\n    precision_mode=PRECISION)\r\n\r\n\r\ninput_saved_model_dir = 'mscoco-retinanet-resnet50-640x640-30x-256'\r\noutput_saved_model_dir = 'tf-trt-' + PRECISION\r\nprint('Using saved_model from : ', input_saved_model_dir)\r\n\r\nconverter = tf.experimental.tensorrt.Converter(\r\n    input_saved_model_dir=input_saved_model_dir,\r\n    use_dynamic_shape=False,\r\n    conversion_params=params)\r\n\r\n\r\ndef input_fn(steps=1):\r\n    for i in range(steps):\r\n        yield (\r\n            tf.random.uniform([1, 640, 640, 3]), tf.constant(1, dtype=tf.int32),\r\n            tf.ones([1, 4]))\r\n\r\n\r\nconverter.convert()\r\nconverter.build(input_fn=input_fn)\r\n\r\nprint('Saving converted saved_model to : ', output_saved_model_dir)\r\nconverter.save(output_saved_model_dir)\r\n```\r\n\r\n[36.0_run.log](https://github.com/tensorflow/tensorflow/files/7068995/36.0_run.log)\r\n[40.4_run.log](https://github.com/tensorflow/tensorflow/files/7068996/40.4_run.log)\r\n\r\nI cannot submit a standalone colab notebook because my model uses `tf.image.combined_non_max_suppression` which needs the TensorRT `batchedNMSPlugin` to run and that cannot be loaded in colab since\r\ncolab needs to have tensorrt python client installed to call so that it can load the plugin\r\n```\r\nimport tensorrt as trt\r\ntrt.init_libnvinfer_plugins(None, '')\r\n```\r\nwhich is not possible currently https://github.com/googlecolab/colabtools/issues/1844", "comments": ["Hi @Saduf2019 , Could you please look at this issue. providing gist in [2.5 ](https://colab.research.google.com/gist/mohantym/aa709eed3ee2ff0913f5eda6a01e57a3/github_51719_tf_2-6.ipynb#scrollTo=iYyjwZaCx17R),[2.6](https://colab.research.google.com/gist/mohantym/c7699da28d8c60b24d17e1c75027c3a8/github_51719_tf_2-6.ipynb) and [2.7](https://colab.research.google.com/gist/mohantym/78421807c8baf9165de02f0fcf55f57d/github_51719_tf_2-6.ipynb#scrollTo=5M1IfJiD0hDq) for reference. Session is crashing in TF 2.5."]}, {"number": 51693, "title": "Support resizing images on TPU using `tf.image.resize` when `size` is not a compile-time constant", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nXLA compilation fails when the `size` arg of `tf.image.resize` is not a compile-time constant.\r\n\r\n**Who will benefit with this feature?**\r\nThis will allow users to resize images to dynamic sizes on TPUs\r\n\r\n**Any Other info.**\r\ncode to reproduce the issue\r\n```python\r\nimport tensorflow as tf\r\nprint('TensorFlow:', tf.__version__)\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect('local')\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\n\r\ndef _run(inputs):\r\n    mask, height, width = inputs\r\n    return tf.image.resize(mask, size=[height, width], method='nearest')\r\n\r\n@tf.function\r\ndef _distributed_run(inputs):\r\n    outputs = strategy.run(_run, args=(inputs,))\r\n    return strategy.gather(outputs, axis=0)\r\n\r\nmask = tf.random.normal((1, 100, 100, 3))\r\nheight = 200\r\nwidth = 200\r\ninputs = (mask, height, width)\r\n\r\noutputs = _distributed_run(inputs)\r\nprint(outputs.shape)\r\n```\r\noutput:\r\n```\r\nTensorFlow: 2.6.0\r\n\r\nD0826 06:54:39.426496290   15038 ev_posix.cc:173]            Using polling engine: epollex\r\nD0826 06:54:39.426573151   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"grpclb\"\r\nD0826 06:54:39.426583436   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"priority_experimental\"\r\nD0826 06:54:39.426587482   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"weighted_target_experimental\"\r\nD0826 06:54:39.426591102   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"pick_first\"\r\nD0826 06:54:39.426594552   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"round_robin\"\r\nD0826 06:54:39.426606683   15038 dns_resolver_ares.cc:499]   Using ares dns resolver\r\nD0826 06:54:39.426633671   15038 certificate_provider_registry.cc:33] registering certificate provider factory for \"file_watcher\"\r\nD0826 06:54:39.426644953   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"cds_experimental\"\r\nD0826 06:54:39.426649442   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"xds_cluster_impl_experimental\"\r\nD0826 06:54:39.426653456   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"xds_cluster_resolver_experimental\"\r\nD0826 06:54:39.426658294   15038 lb_policy_registry.cc:42]   registering LB policy factory for \"xds_cluster_manager_experimental\"\r\nI0826 06:54:39.426774965   15038 server_builder.cc:332]      Synchronous server. Num CQs: 1, Min pollers: 1, Max Pollers: 2, CQ timeout (msec): 10000\r\nI0826 06:54:39.426889181   15038 socket_utils_common_posix.cc:353] TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\r\nI0826 06:54:39.472098409   15342 subchannel.cc:1065]         New connected subchannel at 0x62a3ce0 for subchannel 0x393c600\r\nD0826 06:54:43.380396347   15830 init.cc:226]                grpc_shutdown starts clean-up now\r\nD0826 06:54:46.843676713   15830 ev_posix.cc:173]            Using polling engine: epollex\r\nD0826 06:54:46.843732206   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"grpclb\"\r\nD0826 06:54:46.843754461   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"priority_experimental\"\r\nD0826 06:54:46.843758619   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"weighted_target_experimental\"\r\nD0826 06:54:46.843761981   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"pick_first\"\r\nD0826 06:54:46.843768556   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"round_robin\"\r\nD0826 06:54:46.843777491   15830 dns_resolver_ares.cc:499]   Using ares dns resolver\r\nD0826 06:54:46.843795296   15830 certificate_provider_registry.cc:33] registering certificate provider factory for \"file_watcher\"\r\nD0826 06:54:46.843810614   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"cds_experimental\"\r\nD0826 06:54:46.843814307   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"xds_cluster_impl_experimental\"\r\nD0826 06:54:46.843818589   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"xds_cluster_resolver_experimental\"\r\nD0826 06:54:46.843825122   15830 lb_policy_registry.cc:42]   registering LB policy factory for \"xds_cluster_manager_experimental\"\r\nI0826 06:54:46.843933271   15830 server_builder.cc:332]      Synchronous server. Num CQs: 1, Min pollers: 1, Max Pollers: 2, CQ timeout (msec): 10000\r\nI0826 06:54:46.844822787   15631 subchannel.cc:1065]         New connected subchannel at 0x63af920 for subchannel 0x3867c80\r\nF0826 06:54:47.241484   15823 image_resize_ops.cc:42] Check failed: out_size.size() == 2 (0 vs. 2) Invalid argument: Input 1 to node `resize/ResizeNearestNeighbor` with op ResizeNearestNeighbor must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n*** Check failure stack trace: ***\r\n    @     0x7f0a1cc2ad87  (unknown)\r\n    @     0x7f0a1cc29914  (unknown)\r\n    @     0x7f0a1cc292c3  (unknown)\r\n    @     0x7f0a1cc2b709  (unknown)\r\n    @     0x7f0a18de0146  (unknown)\r\n    @     0x7f0a18ddfbcf  (unknown)\r\n    @     0x7f0a193f29da  (unknown)\r\n    @     0x7f0a19c5e92c  (unknown)\r\n    @     0x7f0a193d4295  (unknown)\r\n    @     0x7f0a193e0ba5  (unknown)\r\n    @     0x7f0a193dc526  (unknown)\r\n    @     0x7f0a18e4f8d7  (unknown)\r\n    @     0x7f0a129d5c5e  (unknown)\r\n    @     0x7f0a129d7373  (unknown)\r\n    @     0x7f0a129cecd3  TpuCompile_CompileAndBuild\r\n    @     0x7f0a25aa81a5  tensorflow::tpu::TpuProgramGroup::CompileAndBuild()\r\n    @     0x7f0a25a3bed9  tensorflow::tpu::TpuCompileOpKernelImpl::Compile()\r\n    @     0x7f0a25ab2122  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCache()\r\n    @     0x7f0a25ab27a8  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()::{lambda()#3}::operator()()\r\n    @     0x7f0a25ab287c  std::_Function_handler<>::_M_invoke()\r\n    @     0x7f0a25a6d25a  tensorflow::tpu::TpuCompilationCacheExternal::InitializeEntry()\r\n    @     0x7f0a25abc45a  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsentHelper()\r\n    @     0x7f0a25abcf4a  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsent()\r\n    @     0x7f0a25ab4ca4  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()\r\n    @     0x7f0a25ab608d  tensorflow::tpu::TpuCompileOpKernelCommon::Compute()\r\n    @     0x7f0a2c8fb330  tensorflow::(anonymous namespace)::ExecutorState<>::Process()\r\n    @     0x7f0a2c8eed82  std::_Function_handler<>::_M_invoke()\r\n    @     0x7f0a2cc12e65  Eigen::ThreadPoolTempl<>::WorkerLoop()\r\n    @     0x7f0a2cc10b37  std::_Function_handler<>::_M_invoke()\r\n    @     0x7f0a2cbf41ef  tensorflow::(anonymous namespace)::PThread::ThreadFn()\r\n    @     0x7f0c4f7a7609  start_thread\r\nhttps://symbolize.stripped_domain/r/?trace=7f0a1cc2ad87,7f0a1cc29913,7f0a1cc292c2,7f0a1cc2b708,7f0a18de0145,7f0a18ddfbce,7f0a193f29d9,7f0a19c5e92b,7f0a193d4294,7f0a193e0ba4,7f0a193dc525,7f0a18e4f8d6,7f0a129d5c5d,7f0a129d7372,7f0a129cecd2,7f0a25aa81a4,7f0a25a3bed8,7f0a25ab2121,7f0a25ab27a7,7f0a25ab287b,7f0a25a6d259,7f0a25abc459,7f0a25abcf49,7f0a25ab4ca3,7f0a25ab608c,7f0a2c8fb32f,7f0a2c8eed81,7f0a2cc12e64,7f0a2cc10b36,7f0a2cbf41ee,7f0c4f7a7608&map=b7c22d7954df6b6961e4435041132cf899ee4a5e:7f0a1d725000-7f0a31424270,ca1b7ab241ee28147b3d590cadb5dc1b:7f0a0ff1c000-7f0a1cf4eb20\r\nhttps://symbolize.stripped_domain/r/?trace=7f0c4f80718b,7f0c4f80720f,7f0a1cc2aec7,7f0a1cc29913,7f0a1cc292c2,7f0a1cc2b708,7f0a18de0145,7f0a18ddfbce,7f0a193f29d9,7f0a19c5e92b,7f0a193d4294,7f0a193e0ba4,7f0a193dc525,7f0a18e4f8d6,7f0a129d5c5d,7f0a129d7372,7f0a129cecd2,7f0a25aa81a4,7f0a25a3bed8,7f0a25ab2121,7f0a25ab27a7,7f0a25ab287b,7f0a25a6d259,7f0a25abc459,7f0a25abcf49,7f0a25ab4ca3,7f0a25ab608c,7f0a2c8fb32f,7f0a2c8eed81,7f0a2cc12e64,7f0a2cc10b36,7f0a2cbf41ee,7f0c4f7a7608&map=b7c22d7954df6b6961e4435041132cf899ee4a5e:7f0a1d725000-7f0a31424270,ca1b7ab241ee28147b3d590cadb5dc1b:7f0a0ff1c000-7f0a1cf4eb20\r\n*** SIGABRT received by PID 15038 (TID 15823) on cpu 2 from PID 15038; ***\r\nE0826 06:54:47.512108   15823 coredump_hook.cc:292] RAW: Remote crash data gathering hook invoked.\r\nE0826 06:54:47.512125   15823 coredump_hook.cc:384] RAW: Skipping coredump since rlimit was 0 at process start.\r\nE0826 06:54:47.512134   15823 client.cc:222] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.\r\nE0826 06:54:47.512143   15823 coredump_hook.cc:447] RAW: Sending fingerprint to remote end.\r\nE0826 06:54:47.512149   15823 coredump_socket.cc:124] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket\r\nE0826 06:54:47.512154   15823 coredump_hook.cc:451] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?\r\nE0826 06:54:47.512158   15823 coredump_hook.cc:525] RAW: Discarding core.\r\nF0826 06:54:47.241484   15823 image_resize_ops.cc:42] Check failed: out_size.size() == 2 (0 vs. 2) Invalid argument: Input 1 to node `resize/ResizeNearestNeighbor` with op ResizeNearestNeighbor must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a p\r\nE0826 06:54:47.998539   15823 process_state.cc:771] RAW: Raising signal 6 with default behavior\r\nAborted (core dumped)\r\n```", "comments": ["Hi @srihari-humbarwadi  !  I was getting a different error in last line of code. while try to eradicate the issue , it was found that height,width were not being passed as Integer values in return `tf.image.resize(mask, size=[height, width], method='nearest') `.   Could you please try like below and update.\r\n\r\n```\r\nimport tensorflow as tf\r\nprint('TensorFlow:', tf.__version__)\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\nheight = 100\r\nwidth = 100\r\n\r\ndef _run(mask):\r\n    return tf.image.resize(mask,[height,width], method='nearest')\r\n\r\n@tf.function\r\ndef _distributed_run(mask):\r\n    outputs = strategy.run(_run, args=(mask,))\r\n    return strategy.gather(outputs, axis=1)\r\n\r\nmask = tf.random.normal((1, 100, 100, 3))\r\n#print(mask)\r\n\r\n\r\n\r\noutputs = _distributed_run(mask)\r\nprint(outputs.shape)\r\n```\r\n\r\n\r\nProviding [gist ](https://colab.research.google.com/gist/mohantym/fe5383119f045c8704753ac927cd9d95/github_51693.ipynb)for reference. Thanks!", "You are right. The changes that you made to the snippet will allow it to compile on TPUs and run. But, that is not what I am trying to do. Passing height and width as function arguments is what I am looking for because they aren't fixed and are computed during run time in the use case I am working on. This is currently not supported!", "@sanatmpa1 ,Could you please look into this issue , providing [gist ](https://colab.research.google.com/gist/mohantym/fe5383119f045c8704753ac927cd9d95/github_51693.ipynb#scrollTo=QlC7C0rX2XBn)for reference", "Any update on this? I am trying to do something similar. I want to implement data augmentation in a `tf.data.Dataset` pipeline, but the problem is that the image dimensions are known only at run time. ", "> Any update on this? I am trying to do something similar. I want to implement data augmentation in a `tf.data.Dataset` pipeline, but the problem is that the image dimensions are known only at run time.\r\n\r\n+1 to have resize XLA implementation for the same purposes"]}, {"number": 51688, "title": "Nan gradient when calling tf.gradients() more than 1 time in graph mode for TF Dormand-Prince solver", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n(Please see jupyter notebook in the attached ZIP file)\r\n\r\nI notice that if `tf.gradients() ` is called more than one time inside a `@tf.function`-decorated function `ode_gradient_graph()` (graph mode) when using a Tensorflow Dormand-Prince ODE solver, the later gradients can evaluate incorrectly to have NaN values. Switching the order of the tf.gradients()` calls allows computation of the second gradient but not the first. \r\n\r\nHowever, all gradients are computed normally in eager mode using GradientTape with `persistent=True`. \r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.gradients()` should return a non-NaN answer for the second gradient computed inside the `@tf.function`-decorated function `ode_gradient_graph()`. Additionally, when parameters `ys` and `xs` to `tf.gradients()` are both single tensorflow tensors,` tf.gradients()` should have the same behavior as `tf.GradientTape.gradient()`.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): N/A\r\n\r\n**Standalone code to reproduce the issue**\r\nSee attached ZIP file for notebook\r\n[ode_gradients_bug.ipynb.zip](https://github.com/tensorflow/tensorflow/files/7049692/ode_gradients_bug.ipynb.zip)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached. N/A\r\n", "comments": ["@sanatmpa1 ,\r\nI was able to reproduce the issue in tf [v2.5](https://colab.research.google.com/gist/tilakrayal/eeecfc800762856c56c980f3a7a00d57/2-5.ipynb),[v2.6](https://colab.research.google.com/gist/tilakrayal/1ced80a3b0d64b5ab7b2bb552177e8c4/2-6.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/1c4a01fbcfa1c30f7ba5e6f8cef26c58/nightly.ipynb).Please find the gist here."]}, {"number": 51681, "title": "Inconsistency in Gradients Calculation of reduce_max", "body": "Hi, I found that the gradients calculation of `reduce_max` in tensorflow is different from other deep learning libraries such as theano, CNTK. And I want to know which algorithm is correct?\r\n\r\nHere is an example code using TensorFlow2.6.0:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.GradientTape() as tape:\r\n    x = tf.Variable([[[0.6],\r\n                      [0.6],\r\n                      [0.3]]])\r\n\r\n    y = tf.reduce_max(x)\r\ng = tape.gradient(y, x)\r\n\r\n\r\nprint(\"gradients of max: \", g.numpy())\r\n```\r\nThe result is:\r\n```\r\ngradients of max:  [[[0.5]\r\n                     [0.5]\r\n                     [0. ]]]\r\n```\r\n\r\nAnd this is the code using CNTK2.7:\r\n```python\r\nimport numpy as np\r\nimport cntk as C\r\n\r\nx = C.input_variable(shape=(1, 3, 1), needs_gradient=True)\r\nx_val = np.array([[[0.6],\r\n                   [0.6],\r\n                   [0.3]]])\r\n\r\ny = C.reduce_max(x)\r\ng = y.grad({x: x_val})\r\n\r\n\r\nprint(\"gradients of max: \", g)\r\n``` \r\nThe result is:\r\n```\r\ngradients of max:  [[[[1.]\r\n                      [1.]\r\n                      [0.]]]]\r\n```\r\nThe inconsistency exists when there are multiple max elements.\r\n\r\nAny replies will be appreciated.\r\nThanks.", "comments": ["@River861 ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Thanks!", "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **--**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.6.0**\r\n- Python version: **3.8.11**\r\n- Bazel version (if compiling from source): **--**\r\n- GCC/Compiler version (if compiling from source):  **--**\r\n- CUDA/cuDNN version:  **--** **\uff08cpu only\uff09**\r\n- GPU model and memory:  **--**\r\n\r\n**Describe the current behavior**\r\nthe gradients calculation of `reduce_max` in tensorflow is different from other deep learning libraries such as theano, CNTK.\r\nThe inconsistency exists when there are **multiple max elements**.\r\n\r\n**Describe the expected behavior**\r\nI want to know which algorithm is correct? Is it a bug in TensorFlow?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n\r\n> Hi, I found that the gradients calculation of `reduce_max` in tensorflow is different from other deep learning libraries such as theano, CNTK. And I want to know which algorithm is correct?\r\n> \r\n> Here is an example code using TensorFlow2.6.0:\r\n> \r\n> ```python\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> \r\n> with tf.GradientTape() as tape:\r\n>     x = tf.Variable([[[0.6],\r\n>                       [0.6],\r\n>                       [0.3]]])\r\n> \r\n>     y = tf.reduce_max(x)\r\n> g = tape.gradient(y, x)\r\n> \r\n> \r\n> print(\"gradients of max: \", g.numpy())\r\n> ```\r\n> \r\n> The result is:\r\n> \r\n> ```\r\n> gradients of max:  [[[0.5]\r\n>                      [0.5]\r\n>                      [0. ]]]\r\n> ```\r\n> \r\n> And this is the code using CNTK2.7:\r\n> \r\n> ```python\r\n> import numpy as np\r\n> import cntk as C\r\n> \r\n> x = C.input_variable(shape=(1, 3, 1), needs_gradient=True)\r\n> x_val = np.array([[[0.6],\r\n>                    [0.6],\r\n>                    [0.3]]])\r\n> \r\n> y = C.reduce_max(x)\r\n> g = y.grad({x: x_val})\r\n> \r\n> \r\n> print(\"gradients of max: \", g)\r\n> ```\r\n> \r\n> The result is:\r\n> \r\n> ```\r\n> gradients of max:  [[[[1.]\r\n>                       [1.]\r\n>                       [0.]]]]\r\n> ```\r\n> \r\n> The inconsistency exists when there are multiple max elements.\r\n> \r\n> Any replies will be appreciated.\r\n> Thanks.\r\n\r\n@tilakrayal , Sorry for not writing the template before.\r\nAny replies will be appreciated.\r\nThanks.", "I think it's also good to check PyTorch and JAX before making a decision here.  First of all, the gradient is mathematically not defined in this case (left-limit and right-limit is different, 0 and 1), and for gradient-descent optimizing purpose, I think having a stable gradient norm with respect to floating point round off error is not a bad choice.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51681\">No</a>\n"]}, {"number": 51677, "title": "py_function doesn't work with functions with no return values and inputs as keras.Input", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: GeForce RTX 2080; Memory: 8G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> a = tf.keras.Input((1,), dtype=tf.int32, name='a')\r\n>>> tf.py_function(lambda a: None, [a], [])\r\n```\r\n\r\nRaises error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 512, in eager_py_func\r\n    return _eager_py_func(\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 414, in _eager_py_func\r\n    return _internal_py_func(\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 343, in _internal_py_func\r\n    result = gen_script_ops.eager_py_func(\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 53, in eager_py_func\r\n    return eager_py_func_eager_fallback(\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 96, in eager_py_func_eager_fallback\r\n    _attr_Tin, input = _execute.convert_to_mixed_eager_tensors(input, ctx)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 295, in convert_to_mixed_eager_tensors\r\n    v = [ops.convert_to_tensor(t, ctx=ctx) for t in values]\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 295, in <listcomp>\r\n    v = [ops.convert_to_tensor(t, ctx=ctx) for t in values]\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1566, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 346, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 271, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 283, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 308, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 106, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/engine/keras_tensor.py\", line 244, in __array__\r\n    raise TypeError(\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 210, in wrapper\r\n    result = dispatch(wrapper, args, kwargs)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 126, in dispatch\r\n    result = dispatcher.handle(op, args, kwargs)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/layers/core.py\", line 1473, in handle\r\n    return TFOpLambda(op)(*args, **kwargs)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 976, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1114, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 848, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 893, in _infer_output_signature\r\n    outputs = tf.nest.map_structure(\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 869, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py\", line 869, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/engine/keras_tensor.py\", line 584, in keras_tensor_from_tensor\r\n    out = keras_tensor_cls.from_tensor(tensor)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/keras/engine/keras_tensor.py\", line 172, in from_tensor\r\n    type_spec = tf.type_spec_from_value(tensor)\r\n  File \"/home/kaiyu/tmp/py_env/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py\", line 609, in type_spec_from_value\r\n    raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\r\nTypeError: Could not build a TypeSpec for <tf.Operation 'tf.py_function_1/EagerPyFunc' type=EagerPyFunc> with type Operation\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nShould simply pass.\r\n\r\nIf `a` is a constant instead, it works.\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> a = tf.constant(1)\r\n>>> tf.py_function(lambda a: None, [a], [])\r\n[]\r\n```\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): N/A\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/19qhwT5fPmVX0SB6FgNJYZvaI3kXCSzOG?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Saduf2019 ,\r\nI was able to reproduce the issue in tf v2.5,v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/a57179e3cc8975153aa7d66816a0bfb0/51677.ipynb).", "@kaiyu-pony \r\nYou can use \"tf.compat.v1.disable_eager_execution()\" to avoid the error, please refer to this [gist here](https://colab.research.google.com/gist/Saduf2019/eedfcb6027362d1c0dc5db5ce5aca460/untitled631.ipynb).", "@Saduf2019 I see. But for some reason I need to avoid that in my project.\r\nIs this the expected behavior? If so, could you please make the error message clearer? Otherwise, could you please fix it?"]}, {"number": 51659, "title": "Upgrade TF to CUDA 11.4 and cuDNN 8.2 ", "body": "Upgrade TF to CUDA 11.4 and cuDNN 8.2 ", "comments": ["What are the steps to upgrade TF to CUDA 11.4 and cuDNN 8.2 using docker image \r\n tensorflow/tensorflow:latest-devel-gpu", "We do plan on adding support for cuda 11.4 and cudnn 8.2 in a future release. If you want to add support to your local build, you'll need to build from source. ", "Hi was there ever an update on this?  I've been trying to pip install tensorflow within this image, `docker://nvidia/cuda:11.4.0-cudnn8-runtime-ubuntu20.04`,  but I keep getting these errors: https://github.com/google/jax/issues/7239, https://github.com/google/jax/discussions/6843. It'd be really helpful if you could put out an image with tensorflow and jax already inside!"]}]