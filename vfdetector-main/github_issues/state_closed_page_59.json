[{"number": 53519, "title": "Tensorflow ist not using full gpu memory", "body": "Systeminformation:\r\n\r\nubuntu-server 20.04\r\ngpu: rtx3060ti\r\ntensor-flow: 2.7.0\r\ndriver version: 495.44\r\ncuda: 11.2\r\ncudnn: 8.1.0\r\n\r\nI expect that tensorflow would use nearly the full gpu memory not only 6435MiB from 8GB.\r\n\r\ntest code (jupyter notebook):\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\nimport sys\r\n\r\nprint('TensorFlow:', tf.__version__)\r\nprint('Python:', sys.version)\r\n\r\nprint('\\n***CUDA***\\n')\r\n!nvcc --version\r\n\r\nprint('\\n***CUDNN***\\n')\r\n!ls /usr/local/cuda/lib64/libcudnn.so* -l\r\n\r\n!nvidia-smi\r\ntime.sleep(2)\r\n\r\nmodel = tf.keras.models.Sequential()\r\ntime.sleep(2)\r\n\r\n!nvidia-smi\r\n\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTensorFlow: 2.7.0\r\nPython: 3.8.10 (default, Nov 26 2021, 20:14:08) \r\n[GCC 9.3.0]\r\n\r\n***CUDA***\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_21:12:58_PST_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n\r\n***CUDNN***\r\n\r\nlrwxrwxrwx 1 root root     13 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.8\r\nlrwxrwxrwx 1 root root     17 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so.8 -> libcudnn.so.8.1.0\r\n-rwxr-xr-x 1 root root 158264 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so.8.1.0\r\nWed Dec 22 11:36:00 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| 30%   39C    P0    N/A / 200W |      0MiB /  7972MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n2021-12-22 11:36:03.614764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:03.632056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:03.632548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:03.634278: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-12-22 11:36:03.635303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:03.635783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:03.636194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:04.027651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:04.027831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:04.027964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-22 11:36:04.028084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6137 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\r\n\r\nWed Dec 22 11:36:06 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   39C    P2    41W / 200W |   6437MiB /  7972MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2917      C   ...ensorflow/venv/bin/python     6435MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n```", "comments": ["Wenn I run the code a second time in a new notebook, I get this output:\r\n\r\n```\r\nWed Dec 22 12:28:31 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   45C    P2    42W / 200W |   7748MiB /  7972MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2917      C   ...ensorflow/venv/bin/python     6435MiB |\r\n|    0   N/A  N/A      3067      C   ...ensorflow/venv/bin/python     1311MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "I have to add that after the training is started, more gpu memory is used. When I increase the batch size until I get an oom-error gpu memory usage is 7351MiB /  7972MiB.", "@Doev Could you please check this similar issues link [1](https://stackoverflow.com/questions/36927607/how-can-i-solve-ran-out-of-gpu-memory-in-tensorflow),link [ 2](https://stackoverflow.com/questions/46023775/why-keras-with-tensorflow-is-not-using-all-gpu-memory) ,link [ 3 ](https://stackoverflow.com/questions/46225077/tensorflow-doesnt-allocate-full-gpu-memory), link [4](https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory) and let us know if it helps?Thank you!", "Hello and sorry for my late answer. I tested the links above, but I was not able to get any new information. Most links where just to old to work and the given code was deprecated.", "@Doev I tried to replicate the issue in TF `v2.7.0` and `tf-nightly(2.8.0.dev2021122109)` on Colab .Could you please find the gist [here](https://colab.sandbox.google.com/gist/sushreebarsa/279472e07190f020642385444b33570d/53519.ipynb) and confirm the same?Thank you!", "Hello sushreebarsa,\r\n\r\nits a similar output:\r\n\r\n```\r\nTensorFlow: 2.9.0-dev20211227\r\nPython: 3.8.10 (default, Nov 26 2021, 20:14:08) \r\n[GCC 9.3.0]\r\n\r\n***CUDA***\r\n\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_21:12:58_PST_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n\r\n***CUDNN***\r\n\r\nlrwxrwxrwx 1 root root     13 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.8\r\nlrwxrwxrwx 1 root root     17 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so.8 -> libcudnn.so.8.1.0\r\n-rwxr-xr-x 1 root root 158264 Jul 28 22:31 /usr/local/cuda/lib64/libcudnn.so.8.1.0\r\nWed Dec 29 20:55:43 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   35C    P8    20W / 200W |      0MiB /  7972MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n2021-12-29 20:55:45.316934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n\r\nWed Dec 29 20:55:47 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   37C    P2    42W / 200W |   6437MiB /  7972MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A   2156232      C   ...ow_test/ttest/bin/python3     6435MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n2021-12-29 20:55:45.324117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.324361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.325560: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-12-29 20:55:45.325995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.326213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.326402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.715567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.715730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.715859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-12-29 20:55:45.715975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6141 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\r\n\r\n```", "GPU allocates as much memory as required for the runtime allocations. Initially, it starts allocating small amount of memory and as  the code runs it starts allocating memory for the required/visible process.\r\nAnyways, you can try  with `TF_FORCE_GPU_ALLOW_GROWTH` to `true` in your environmental variable.\r\nAlso, you can try hard setting the visible memory with `tf.config.set_logical_device_configuration(gpus[0],[tf.config.LogicalDeviceConfiguration(memory_limit=1024)])`", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53518, "title": "Update backprop.py", "body": "Renamed `dz_dy` to `dz_dw` for better readability\r\n\r\n#53499 ", "comments": []}, {"number": 53516, "title": "[jax] Minor C++ jit path cleanup", "body": "* Fix up sticky_device types and usage\r\n* Removes unneeded default_pyclient_", "comments": []}, {"number": 53515, "title": "[jax] Combine GlobalJitState and ThreadLocalJitState into single JitState", "body": "This avoids duplication when defining new fields. This change also\r\ncreates standard getters for each field that check if there's\r\nthread-local state defined and make sure a global value is set when\r\nappropriate.", "comments": []}, {"number": 53514, "title": "Update version numbers for TensorFlow 2.8.0-rc0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 8 -> 8\nPatch: 0 -> 0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.8.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:49:2.8.0\ntensorflow/tools/pip_package/setup.py:109:2.8.0\ntensorflow/tools/pip_package/setup.py:111:2.8.0\ntensorflow/tools/pip_package/setup.py:113:2.8.0\ntensorflow/tools/ci_build/release/requirements_common.txt:27:2.8.0\ntensorflow/tools/ci_build/release/requirements_common.txt:28:2.8.0\ntensorflow/tools/ci_build/release/requirements_common.txt:29:2.8.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:106:2.8.0\ntensorflow/tensorflow.bzl:59:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:98:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:160:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:367:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:368:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:369:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:370:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:371:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:372:2.8.0\ntensorflow/lite/g3doc/examples/on_device_training/overview.ipynb:102:2.8.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.8.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:49:2.8.0\ntensorflow/tools/pip_package/setup.py:109:2.8.0\ntensorflow/tools/pip_package/setup.py:111:2.8.0\ntensorflow/tools/pip_package/setup.py:113:2.8.0\ntensorflow/tools/ci_build/release/requirements_common.txt:27:2.8.0\ntensorflow/tools/ci_build/release/requirements_common.txt:28:2.8.0\ntensorflow/tools/ci_build/release/requirements_common.txt:29:2.8.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:106:2.8.0\ntensorflow/tensorflow.bzl:59:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:98:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:160:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:367:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:368:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:369:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:370:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:371:2.8.0\ntensorflow/lite/tools/versioning/runtime_version.cc:372:2.8.0\ntensorflow/lite/g3doc/examples/on_device_training/overview.ipynb:102:2.8.0\n```", "comments": []}, {"number": 53513, "title": "Update release notes for TensorFlow 2.8.0", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.8.0\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 53512, "title": "TF2.7 failed build for rocm 4.5", "body": "**System information**\r\n- OS Platform and Distribution : Gentoo OS\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.7.0\r\n- Python version: 3.9.9\r\n- Installed using : portage\r\n- Bazel version: 4.2.2\r\n- GCC/Compiler version: 11.2.0\r\n- GPU model and memory: AMD Vega Frontier\r\n\r\n**Describe the problem**\r\n```\r\nmlir-tblgen failed: error executing command\r\nCommandLine Error: Option 'd' registered more than once!\r\nLLVM ERROR: inconsistency in registered CommandLine options\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ncd /var/tmp/portage/sci-libs/tensorflow-2.7.0-r3/work/tensorflow-2.7.0-bazel-base/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    HOME=/var/tmp/portage/sci-libs/tensorflow-2.7.0-r3/homedir \\\r\n    KERAS_HOME=/var/tmp/portage/sci-libs/tensorflow-2.7.0-r3/temp/.keras \\\r\n    PATH=/usr/lib/portage/python3.9/ebuild-helpers/xattr:/usr/lib/portage/python3.9/ebuild-helpers:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin:/usr/lib/llvm/12/bin:/usr/lib64/subversion/bin:/opt/cuda/bin:/opt/rocm-4.5.2/hip/bin:/opt/rocm-4.5.2/llvm/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages \\\r\n    ROCBLAS_TENSILE_LIBPATH=/opt/rocm-4.5.2/lib/library \\\r\n    ROCM_PATH=/opt/rocm-4.5.2 \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_SYSTEM_LIBS=astor_archive,astunparse_archive,boringssl,com_github_googlecloudplatform_google_cloud_cpp,curl,cython,dill_archive,double_conversion,enum34_archive,flatbuffers,functools32_archive,gast_archive,gif,hwloc,icu,libjpeg_turbo,lmdb,nasm,nsync,opt_einsum_archive,org_sqlite,pasta,png,pybind11,six_archive,snappy,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt,zlib \\\r\n  bazel-out/k8-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen -gen-op-decls -dialect tfg tensorflow/core/ir/ops.td -I external/llvm-project/mlir/include -I bazel-out/k8-opt/bin/external/llvm-project/mlir/include -I ./tensorflow/core/ir/include -I bazel-out/k8-opt/bin/./tensorflow/core/ir/include -I ./tensorflow/core/ir/types/include -I bazel-out/k8-opt/bin/./tensorflow/core/ir/types/include -I ./ -I bazel-out/k8-opt/bin/./ -I tensorflow/core/ir -I bazel-out/k8-opt/bin/tensorflow/core/ir -o bazel-out/k8-opt/bin/tensorflow/core/ir/ops.h.inc\r\n```\r\n**Any other info / logs**\r\n[strace](https://gist.githubusercontent.com/raw/1b3a9b4f520aa3450f5cf1c5bad47f76)\r\n[bazelrc](https://gist.github.com/raw/346a6a4e32d0da1514d7fe2f0fc0ae4c)\r\n[environment](https://gist.github.com/raw/b53debf6a768aa99032b455d487b0ad8)\r\n[build.log](https://gist.github.com/raw/e4009002bbdd26493b31a63ce6fff26f)\r\n", "comments": ["May be issue in theese lines ?\r\n- https://github.com/tensorflow/tensorflow/blob/9cba88624b952cc9f3d382929e4346b752f6ba06/third_party/gpus/rocm_configure.bzl#L711\r\n- https://github.com/tensorflow/tensorflow/blob/b56e6db5e7cccfffa824bbc1b5e018c6cc413c21/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc#L739\r\n- https://github.com/tensorflow/tensorflow/issues/53475\r\n\r\n- https://github.com/tensorflow/tensorflow/search?q=hcc", "resolved this issue on my side was errors in compile binutils, clang, bazel and tensorflow\r\n\r\n- binutils and clang need rebuild with flag gold\r\n- bazel 4.2.2 need patch file unix_cc_configure.bzl to be same as edge from master branch\r\n- tensorflow need strip system flags with this script:\r\n\r\n```\r\n# do the great cleanup\r\nstrip-flags\r\nfilter-flags '-mcpu=*' '-march=*' '-mtune=*' '-m32' '-m64'\r\nfilter-flags '-fvtable-verify=@(std|preinit)'\r\nstrip-unsupported-flags\r\n\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53512\">No</a>\n"]}, {"number": 53511, "title": "[ROCm] Disabling MKLDNN based Eigen contraction kernels for ROCm build", "body": "On some RCm CI nodes (typically those with higher CPU core counts 128/256), the `//tensorflow/c/eager:c_api_distributed_test_gpu` test fails on an intermitent basis.\r\n\r\nWhen it does fail, the failures manifests as segfault at the end of the test, with the stack dump shown at the end of this commit message. The stack dump points the finger to a routine within the MKLDNN implementation. This is further confirmed by the observation that disabling the MKLDNN based Eigen contraction kernels (for ROCm) seems to make the crash go away.\r\n\r\nThis commit worksaround the bug by disabling  MKLDNN based Eigen contraction kernels for ROCm build\r\n\r\n```\r\nThread 191 \"c_api_distribut\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to thread 191 (Thread 0x7ffc777fe700 (LWP 159004))]\r\n0x00007fff54530000 in ?? ()\r\n(gdb) where\r\n#0  0x00007fff54530000 in ?? ()\r\n#1  0x00007fffd5d15ae4 in dnnl::impl::cpu::x64::avx_gemm_f32::sgemm_nocopy_driver(char const*, char const*, long, long, long, float const*, float const*, long, float const*, long, float const*, float*, long, float const*, float*) ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/libexternal_Smkl_Udnn_Uv1_Slibmkl_Udnn.so\r\n#2  0x00007fffd5d166e1 in dnnl::impl::cpu::x64::jit_avx_gemm_f32(int, char const*, char const*, long const*, long const*, long const*, float const*, float const*, long const*, float const*, long const*, float const*, float*, long const*, float const*) ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/libexternal_Smkl_Udnn_Uv1_Slibmkl_Udnn.so\r\n#3  0x00007fffd5e277ed in dnnl_status_t dnnl::impl::cpu::x64::gemm_driver<float, float, float>(char const*, char const*, char const*, long const*, long const*, long const*, float const*, float const*, long const*, float const*, float const*, long const*, float const*, float const*, float*, long const*, float const*, bool, dnnl::impl::cpu::x64::pack_type, dnnl::impl::cpu::x64::gemm_pack_storage_t*, bool) ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/libexternal_Smkl_Udnn_Uv1_Slibmkl_Udnn.so\r\n#4  0x00007fffd5665056 in dnnl::impl::cpu::extended_sgemm(char const*, char const*, long const*, long const*, long const*, float const*, float const*, long const*, float const*, long const*, float const*, float*, long const*, float const*, bool) ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/libexternal_Smkl_Udnn_Uv1_Slibmkl_Udnn.so\r\n#5  0x00007fffd52fe983 in dnnl_sgemm ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/libexternal_Smkl_Udnn_Uv1_Slibmkl_Udnn.so\r\n#6  0x0000555557187b0b in Eigen::internal::TensorContractionKernel<float, float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0, 1>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer> >::invoke(Eigen::internal::blas_data_mapper<float, long, 0, 0, 1> const&, Eigen::internal::ColMajorBlock<float, long> const&, Eigen::internal::ColMajorBlock<float, long> const&, long, long, long, float, float) ()\r\n#7  0x000055555718dc76 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::kernel(long, long, long, bool) ()\r\n#8  0x000055555718f327 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::signal_kernel(long, long, long, bool, bool) ()\r\n#9  0x00005555571904cb in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::pack_rhs(long, long) ()\r\n#10 0x000055555718fd69 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::enqueue_packing_helper(long, long, long, bool) ()\r\n#11 0x00007ffff6b607a1 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/_U_S_Stensorflow_Sc_Seager_Cc_Uapi_Udistributed_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.2\r\n#12 0x00007ffff6b5de93 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/_U_S_Stensorflow_Sc_Seager_Cc_Uapi_Udistributed_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.2\r\n#13 0x00007ffff6b40107 in tensorflow::(anonymous namespace)::PThread::ThreadFn(void*) ()\r\n   from /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/../../../_solib_local/_U_S_Stensorflow_Sc_Seager_Cc_Uapi_Udistributed_Utest_Ugpu___Utensorflow/libtensorflow_framework.so.2\r\n#14 0x00007fffd1ca86db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#15 0x00007fffd00b471f in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n\r\n-----------------------------\r\n\r\n/cc @chsigg @cheshire ", "comments": []}, {"number": 53510, "title": " AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call'", "body": "Hello.\r\nI am importing the following libraries while running the example that comes as the [basic text classification ](https://www.tensorflow.org/tutorials/keras/text_classification?hl=es-419) in the official Tensorflow documentation. But I get an error message. My version of tensorflow is 2.7.0\r\n\r\nRunning code for which the error occurred:\r\n```\r\nepochs = 10\r\nhistory = model.fit( train_ds, validation_data=val_ds, epochs=epochs)\r\n```\r\n\r\nLibraries loads:\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport re\r\nimport shutil\r\nimport string\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import losses\r\n```\r\nError message:\r\n```\r\nAttributeError: in user code:\r\n\r\n    File \"C:\\Users\\Octavio\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1525, in test_function  *\r\n        return step_function(self, iterator)\r\n    File \"C:\\Users\\Octavio\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1514, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"C:\\Users\\Octavio\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1507, in run_step  **\r\n        outputs = model.test_step(data)\r\n    File \"C:\\Users\\Octavio\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1474, in test_step\r\n        return self.compute_metrics(x, y, y_pred, sample_weight)\r\n    File \"C:\\Users\\Octavio\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 961, in compute_metrics\r\n        result = metric.result()\r\n    File \"C:\\Users\\Octavio\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 124, in decorated\r\n        tf.__internal__.distribute.strategy_supports_no_merge_call()):\r\n```\r\nThank you\r\n", "comments": ["Hi @Fer020707 ! I was able to run above [example ](https://www.tensorflow.org/tutorials/keras/text_classification?hl=es-419) in Colab with 2.7 version too. Could you please share a minimal standalone code along other details through [template](https://github.com/tensorflow/tensorflow/issues/new/choose) to reproduce this issue ?Attaching relevant [thread](https://github.com/tensorflow/tensorflow/issues/27120) for reference. Thanks!", "Of course. This would be the minimum code to reproduce the error. Will it be an installation aspect?\r\n\r\nImported libraries:\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport re\r\nimport shutil\r\nimport string\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import losses\r\n```\r\nData upload\r\n```\r\nurl = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\r\n\r\ndataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\r\n                                    untar=True, cache_dir='.',\r\n                                    cache_subdir='')\r\ndataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\r\ntrain_dir = os.path.join(dataset_dir, 'train')\r\nremove_dir = os.path.join(train_dir, 'unsup')\r\nshutil.rmtree(remove_dir)\r\n```\r\nData work\r\n```\r\nbatch_size = 32\r\nseed = 42\r\nraw_train_ds = tf.keras.utils.text_dataset_from_directory(\r\n    'aclImdb/train', \r\n    batch_size=batch_size, \r\n    validation_split=0.2, \r\n    subset='training', \r\n    seed=seed)\r\nraw_val_ds = tf.keras.utils.text_dataset_from_directory(\r\n    'aclImdb/train', \r\n    batch_size=batch_size, \r\n    validation_split=0.2, \r\n    subset='validation', \r\n    seed=seed)\r\nraw_test_ds = tf.keras.utils.text_dataset_from_directory(\r\n    'aclImdb/test', \r\n    batch_size=batch_size)\r\ndef custom_standardization(input_data):\r\n  lowercase = tf.strings.lower(input_data)\r\n  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\r\n  return tf.strings.regex_replace(stripped_html,\r\n                                  '[%s]' % re.escape(string.punctuation),\r\n                                  '')\r\ndef vectorize_text(text, label):\r\n  text = tf.expand_dims(text, -1)\r\n  return vectorize_layer(text), label\r\n\r\nmax_features = 10000\r\nsequence_length = 250\r\n\r\nvectorize_layer = layers.TextVectorization(\r\n    standardize=custom_standardization,\r\n    max_tokens=max_features,\r\n    output_mode='int',\r\n    output_sequence_length=sequence_length)\r\ntrain_text = raw_train_ds.map(lambda x, y: x)\r\nvectorize_layer.adapt(train_text)\r\ntext_batch, label_batch = next(iter(raw_train_ds))\r\nfirst_review, first_label = text_batch[0], label_batch[0]\r\ntrain_ds = raw_train_ds.map(vectorize_text)\r\nval_ds = raw_val_ds.map(vectorize_text)\r\ntest_ds = raw_test_ds.map(vectorize_text)\r\nAUTOTUNE = tf.data.AUTOTUNE\r\n\r\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\r\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\r\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\r\nembedding_dim = 16\r\nmodel = tf.keras.Sequential([\r\n  layers.Embedding(max_features + 1, embedding_dim),\r\n  layers.Dropout(0.2),\r\n  layers.GlobalAveragePooling1D(),\r\n  layers.Dropout(0.2),\r\n  layers.Dense(1)])\r\n\r\nmodel.summary()\r\nmodel.compile(loss=losses.BinaryCrossentropy(from_logits=True),\r\n              optimizer='adam',\r\n              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\r\nepochs = 10\r\nhistory = model.fit(\r\n    train_ds,\r\n    validation_data=val_ds,\r\n    epochs=epochs)\r\n```", "Ok @Fer020707 !  it might be related to environment/installation perspective. Could not replicate this in Colab again. Attaching [Gist](https://colab.sandbox.google.com/gist/mohantym/c033344bb53aa4929a4c6060a134dd9b/github_53510.ipynb) and relevant [thread](https://stackoverflow.com/questions/67696519/module-tensorflow-compat-v2-internal-has-no-attribute-tf2/67708260#67708260) for reference.\r\n\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues) if it does not get resolved.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks you!", "@Fer020707 I was able to find the source of this issue. When installing `tensorflow 2.7.0`, `keras 2.8.0` was installed. I solved this issue by pinning `keras` to `2.7.0` in my `pyproject.toml`", "Ok @Fer020707 ! Could you please try with above [ solution](https://github.com/tensorflow/tensorflow/issues/53510#issuecomment-1000071177). we already let the team know about it. Thank you! ", "Thank you very much. By correcting that, it worked perfect", "Ok @Fer020707 ! Closing this issue as it seems to be resolved from above comment . Thank you!", "Similar error on MacOS (ARM) with tensorflow-macos. After an pip -U ... I downgraded keras and tensorflow-estimator to 2.7.0.\r\n\r\nWhen fitting the model, the following error occurs:\r\n\r\n```\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\nEpoch 1[/5]\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/Users/wolf/Development/py/5 ml/[1]() prerequisites/07 tensorflow/01_quickstart.ipynb Cell 18' in <module>\r\n----> 1 model.fit(x_train, y_train, epochs=5)\r\n\r\nFile ~/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n         65 except Exception as e:  # pylint: disable=broad-except\r\n         66   filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67   raise e.with_traceback(filtered_tb) from None\r\n         68 finally:\r\n         69   del filtered_tb\r\n\r\nFile ~/Development/py/5 ml/venv/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129, in func_graph_from_py_func.<locals>.autograph_handler(*args, **kwargs)\r\n   1127 except Exception as e:  # pylint:disable=broad-except\r\n   1128   if hasattr(e, \"ag_error_metadata\"):\r\n-> 1129     raise e.ag_error_metadata.to_exception(e)\r\n   1130   else:\r\n   1131     raise\r\n\r\nAttributeError: in user code:\r\n\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function  *\r\n        return step_function(self, iterator)\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function  **\r\n        - A `tf.data.Dataset`.\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step  **\r\n        validation_data: Data on which to evaluate\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 863, in train_step\r\n        model._train_counter.assign_add(1)  # pylint: disable=protected-access\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\r\n        return self.apply_gradients(grads_and_vars, name=name)\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 668, in apply_gradients\r\n        grads_and_vars = self._aggregate_gradients(grads_and_vars)\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 484, in _aggregate_gradients\r\n        return self.gradient_aggregator(grads_and_vars)\r\n    File \"/Users/wolf/Development/py/5 ml/venv/lib/python3.9/site-packages/keras/optimizer_v2/utils.py\", line 33, in all_reduce_sum_gradients\r\n        if strategy_supports_no_merge_call():\r\n\r\n    AttributeError: module 'tensorflow.compat.v2.__internal__.distribute' has no attribute 'strategy_supports_no_merge_call'\r\n```\r\n\r\nThis worked before the update\r\n\r\n"]}, {"number": 53509, "title": "[ROCm] Dropping support for older ROCm releases", "body": "/cc @chsigg @cheshire ", "comments": []}, {"number": 53508, "title": "[TF:TRT] Use IEinsumLayer from TRT 8.2", "body": "This PR refactors the Einsum converter to use IEinsumLayer which is available in TRT 8.2.", "comments": ["I refactored einsum converter but didn't add this feature #53591 . ", "Extended testing, improved validation of equation. This PR depends on #53591, I will rebase it once that is merged.", "@tfeher Can you please resolve conflicts? Thanks!"]}, {"number": 53507, "title": "[TF:TRT] Fix Grappler params to enable layout optimizer for the C++ API", "body": "This PR sets the VirtualCluster argument while calling the Grappler optimization pass for TF-TRT. Setting the cluster arg is necessary to enable layout optimizer (otherwise layout optimizer returns without changing the graph). This change only affects conversions initiated by the C++ API of TF-TRT #52012.\r\n\r\nTagging @bixia1 for review.", "comments": ["@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!", "Added a mutex because we can only provision one cluster per process.", "@tfeher  Can you please address Ubuntu Sanity errors? Thanks!", "The logs says `\"ERROR: An error occurred during the fetch of repository 'rules_proto':\"` This is not related to my changes.", "The changes are already included in master, in commit https://github.com/tensorflow/tensorflow/commit/1d5124678decc63d3b64f2598b91a61169748c3e. Closing the PR."]}, {"number": 53506, "title": "Specify what is ag__ in the output of tf.autograph.to_code", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/autograph/to_code\r\n\r\n## Description of issue (what needs changing):\r\nI tried to look everywhere but I could not find what is `ag__`, how could I use the output of `tf.autograph.to_code` with the python interpreter.\r\nI thought the name `ag__` was `tf.autograph` but this module does not have `FunctionScope` for example.\r\ntf.compat.v1.autograph also did not work.\r\nI did not seem to be able to find online any explanation of this shorthand `ag__`\r\n\r\n### Parameters defined\r\nCould this variable be defined to know how can we interpret the outputted source code?", "comments": ["@ricvo \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "I am not sure.. you mean the following?\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/autograph/to_code\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/autograph/to_code\r\n\r\n## Description of issue (what needs changing):\r\nI tried to look everywhere but I could not find what is ag__, how could I use the output of tf.autograph.to_code with the python interpreter.\r\nI thought the name ag__ was tf.autograph but this module does not have FunctionScope for example.\r\ntf.compat.v1.autograph also did not work.\r\nI did not seem to be able to find online any explanation of this shorthand ag__\r\n\r\n### Clear description\r\nI tried to look everywhere but I could not find what is ag__, how could I use the output of tf.autograph.to_code with the python interpreter.\r\nThis would be really useful for anybody wanting to use the source code output of autograph, to be able to have the python interpreter correctly executing it\r\n\r\n### Correct links\r\nhttps://www.tensorflow.org/api_docs/python/tf/autograph/to_code\r\n\r\nExample of output of the simple tanh loop in the tutotial for tf.function online:\r\n```\r\n@tf.function(autograph=False)\r\ndef tf__tanh_loop(x):\r\n    with ag__.FunctionScope('tanh_loop', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\r\n        do_return = False\r\n        retval_ = ag__.UndefinedReturnValue()\r\n\r\n        def get_state():\r\n            return (x,)\r\n\r\n        def set_state(vars_):\r\n            nonlocal x\r\n            (x,) = vars_\r\n\r\n        def loop_body():\r\n            nonlocal x\r\n            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)\r\n            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)\r\n\r\n        def loop_test():\r\n            return (ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1)\r\n        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})\r\n        try:\r\n            do_return = True\r\n            retval_ = ag__.ld(x)\r\n        except:\r\n            do_return = False\r\n            raise\r\n        return fscope.ret(retval_, do_return)\r\n```\r\n\r\n### Parameters defined\r\nParameters are defined.\r\n\r\n### Returns defined\r\nThe output is somewhat undefined. It should be the source code output of autograph, the problem is that some symbols are not clearly defined and thus the python interpreter doesn't know how to handle them. Example: `ag__`\r\n\r\nThe following did not work:\r\n`ag__ = tf.autograph`\r\n`ag__ = tf.autograph.operators`\r\n`ag__ = tf.compat.v1.autograph`\r\n\r\n\r\n### Raises listed and defined\r\nyes\r\n\r\n### Usage example\r\nyes\r\n\r\n\r\n### Request visuals, if applicable\r\nnot applicable\r\n\r\n### Submit a pull request?\r\nmaybe\r\n", "@ricvo \r\nThis is not a bug or feature request ,could you please refer to the [link1](https://www.tensorflow.org/api_docs/python/tf/autograph/to_code) , [link2](https://www.tensorflow.org/guide/function) and for any further queries you may open this issue in [TF discussion forum ](https://discuss.tensorflow.org/)as there is a larger community there to get you the right help. Thank you!", "@sushreebarsa thanks for the time you are dedicating to this issue.\r\nI did not open this as a bug nor as a feature request. I think I opened this as a Documentation Issue. Indeed it is not specified in the Documentation what is `ag__` (see also in the links you suggest).\r\nWorking on another issue https://github.com/tensorflow/tensorflow/issues/43800 I found out what `ag__` is, but unfortunately it is not something accessible from outside due to the fact that the module is dynamic it seems.\r\nAt the moment I modified my local tensorflow python package code to be able to access the module. The module is accessible internally as\r\n```\r\ntensorflow.python.autograph.impl.api.PyToTF().get_extra_locals()['ag__']\r\n```\r\nas also mentioned in the issue referred above.\r\nI can live with this change, but it's not ideal, I think a more friendly way to access `ag__` should be provided and documented.\r\nI did it locally, seems like a minimal change, I am willing to contribute if needed. Thanks", "@ricvo Thank you for the update!\r\n\r\nCould you please raise a PR if you are willing to contribute to this issue ? Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53505, "title": "tf.random is broken on macOS Monterey 12.1", "body": "On a Mac M1 with Tensorflow-metal 0.3.0 and Tensorflow-2.7.0\r\n\r\nthis code used to generate different sequences but now generate the same sequence : \r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.random.uniform((10,))\r\ny = tf.random.uniform((10,))\r\n\r\ntf.print(x)\r\ntf.print(y)\r\n[0.178906798 0.8810848 0.384304762 ... 0.162458301 0.64780426 0.0123682022]\r\n[0.178906798 0.8810848 0.384304762 ... 0.162458301 0.64780426 0.0123682022]\r\n```\r\n\r\nIt works on CPU, it works on collab, it works on cuda, it used to works on MacOS 12.0.\r\n\r\nI tried many workaround like using a generator, but generator can't be used on M1 GPU, eg  : \r\n```\r\nrandomgen = tf.random.Generator.from_non_deterministic_state()\r\n#%%\r\nfor _ in range(10):\r\n        g2 = tf.random.get_global_generator()\r\n        x = g2.uniform((10,),(1,2))\r\n        y = g2.uniform((10,),(3,4))\r\n        tf.print(x)\r\n        tf.print(y)\r\n```\r\n\r\n```\r\nNotFoundError: No registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}\r\n\t.  Registered:  device='CPU'\r\n [Op:RngReadAndSkip]\r\n```\r\n\r\nother people reported other problems with various code that used to works in 12.0 but the problems are likely to be related to randomness.\r\na broken random is obviously bad in machine learning ^^\r\n\r\nI opened an apple feedback as well as a discussion : https://developer.apple.com/forums/thread/697057?answerId=699100022#699100022\r\n", "comments": ["@ker2x ,\r\nWe don't see any issue while running the given code in colab.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/28dd551b99618132c68dc3f85dfde981/untitled149.ipynb). Since its not a bug from TF end and you also mentioned that the issue is with M1, Can you try posting your question in this [tf-metal ](https://developer.apple.com/forums/tags/tensorflow-metal)developer forum of Apple? Thanks!", "Yes, as mentioned in the bug report, I already posted it on tf-metal dev forum. I wasn't sure if it had its place here, couldn't find any GitHub for Tensorflow-metal. Feel free to close this request if it's off topic. \r\n\r\nThank you for your answer.", "@ker2x ,\r\nCan you please feel free to move this issue to closed status.Thanks!", "okay closing :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53505\">No</a>\n"]}, {"number": 53504, "title": "[ROCm] Explicitly setting the `__launch_bounds__` attribute to 1024 for MLIR generated kernels (for ROCm only)", "body": "The following LLVM commit changes the default `__launch_bounds__` attribute from 1024 to 256 (for the ROCm path).\r\n\r\nhttps://github.com/llvm/llvm-project/commit/e1da62910e140cf45eafec64193c813e79796f05#diff-e0acb49d6ec4a97460a1af5fe8f80abc39f04d172d6c9a08c33da1209115cd4b\r\n\r\nThat results in quite a few TF unit-test failures, because TF side assumes/expects the default to be 1024, and launches GPU kernels at runtime with block-size == 1024. As a consequence, all unit-tests that do this, run into an error similar to the following error\r\n\r\n```\r\n...\r\n:3:hip_module.cpp           :363 : 20798447993994 us: 2453 : [7f51477fe700] ihipModuleLaunchKernel ( 0x0x7f5138273b40, 1024, 1, 1, 1024, 1, 1, 0, stream:0x26b7fb0, 0x7f51477fcb30, char array:<null>, event:0, event:0, 0, 0 )\r\n:1:hip_module.cpp           :256 : 20798447993999 us: Launch params (1024, 1, 1) are larger than launch bounds (256) for kernel Mul_GPU_DT_FLOAT_DT_FLOAT_kernel\r\n...\r\n```\r\n\r\nThe fix for this breakage requires changes on the both the TF side and LLVM side\r\n\r\nOn the LLVM side, we need to add the ability to configure/overwrite the default value (256) of the amdgpu-flat-work-group-size attribute for MLIR generated GPU kernels. This is done via the addition of the rocdl.max_flat_work_group_size attribute, which when set on the mlir::GPUFuncOp will override the default value (256) when mlir::GPUFuncOp gets translated to llvm::FuncOp. See the following LLVM PR for details\r\n\r\nhttps://reviews.llvm.org/D115741\r\n\r\nThe above PR has already been merged, and the TF LLVM ptr has been updated to pick up that change.\r\n\r\nOn the TF side, we need to explicitly specify the rocdl.max_flat_work_group_size attribute with value set to 1024 for MLIR generated kernels...which is what this commit does.\r\n\r\n--------------------------------------\r\n\r\n/cc @chsigg @cheshire ", "comments": ["@sherhut gentle ping...please approve\r\n", "@cheshire @chsigg please approve."]}, {"number": 53503, "title": "Build broken by recent commit updating llvm-raw", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild errors out with\r\n\r\nINFO: Repository 'llvm-raw' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'd68b5fa9b851cc0b8a5f84f7659404380f477efc859f21951bc3554476047245' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/ec0e4545caa18e0b55e4f25db913a10a4782fdc3.tar.gz\r\nIf the definition of 'llvm-raw' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'llvm-raw':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/andrew/src/tensorflow/third_party/repo.bzl\", line 75, column 30, in _tf_http_archive_impl\r\n\t\tctx.patch(patch_file, strip = 1)\r\nError in patch: Error applying patch /home/andrew/src/tensorflow/third_party/llvm/Bazel-update-build-files-for.patch: Incorrect Chunk: the chunk content doesn't match the target\r\n**Original Position**: 840\r\n\r\n**Original Content**:\r\n    copts = llvm_copts,\r\n    deps = [\r\n        \":Core\",\r\n        \":Support\",\r\n        \":config\",\r\n    ],\r\n\r\n**Revised Content**:\r\n    copts = llvm_copts,\r\n    deps = [\r\n        \":Core\",\r\n        \":DebugInfoDWARF\",\r\n        \":Support\",\r\n        \":config\",\r\n    ],\r\nERROR: no such package '@llvm-raw//utils/bazel': Error applying patch /home/andrew/src/tensorflow/third_party/llvm/Bazel-update-build-files-for.patch: Incorrect Chunk: the chunk content doesn't match the target\r\n**Original Position**: 840\r\n\r\n**Original Content**:\r\n    copts = llvm_copts,\r\n    deps = [\r\n        \":Core\",\r\n        \":Support\",\r\n        \":config\",\r\n    ],\r\n\r\n**Revised Content**:\r\n    copts = llvm_copts,\r\n    deps = [\r\n        \":Core\",\r\n        \":DebugInfoDWARF\",\r\n        \":Support\",\r\n        \":config\",\r\n    ],\r\nINFO: Elapsed time: 64.208s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --remote_http_cache=\"\"  --remote_cache_proxy=\"\" --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only --copt=-ffp-contract=off --verbose_failures -- //tensorflow/core/ir/...\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThis commit seems to be implicated\r\nhttps://github.com/tensorflow/tensorflow/commit/403b4b19f9d143b4868d8a822c325acb6fdd8704\r\n", "comments": ["@cfRod @nSircombe ", "Build gets past the failure point at the commit just prior to the one mentioned above.\r\ngit checkout 8ae9fff651a617ed102fe51fbea2cdb81e51e1bd", "Fixed by https://github.com/tensorflow/tensorflow/commit/0bfb4ca53e0ab095b9326ca70fc5ad2a752a7557", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53503\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53503\">No</a>\n"]}, {"number": 53502, "title": "Add missing dialect dependency on xla-legalize-tf-control-flow", "body": "Add missing dialect dependency on xla-legalize-tf-control-flow. This\r\nfixes crashes when using tf-opt to lower tf.while to mhlo.while. The\r\nreason the current test cases for the above pass don't crash is that\r\nthose test cases already have mhlo ops and so the dialect is loaded at\r\nparse time hiding this issue.", "comments": ["Can you add one function in the test that does not have MHLO ops?\r\n(So that it'll trigger the crash)", "> Can you add one function in the test that does not have MHLO ops? (So that it'll trigger the crash)\r\n\r\nAdding a function to the file without mhlo ops won't help here since the file doesn't use `split-input-file` -- so the dialect would get loaded. It would be odd to add this as the first test case instead of functional ones that get complex from top of the file to the bottom. Instead, I freed one of the test cases from mhlo ops and \"dash split\" the file around it. That reproduces the crash.\r\n", "> > Can you add one function in the test that does not have MHLO ops? (So that it'll trigger the crash)\r\n> \r\n> Adding a function to the file without mhlo ops won't help here since the file doesn't use `split-input-file` -- \r\n\r\nSorry: adding --split-input-file if not present (I didn't check) was implicit in my mind :)\r\n\r\n> It would be odd to add this as the first test case instead of functional ones that get complex from top of the file to the bottom. \r\n\r\nThat wouldn't work anyway: the entire file would be parsed and then the pass pipeline invoked, at which point the dialect would be loaded!\r\n\r\nThanks!\r\n"]}, {"number": 53500, "title": "The TensorFlow contrib module will not be included in TensorFlow 2.0.", "body": "WARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nW1221 15:55:51.620390 140392233633600 lazy_loader.py:50] \r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n and i checked that version of tensorflow in my system is 1.15\r\n\r\n", "comments": ["@kusumlata123 ,\r\nPlease take a look at this issue [link](https://github.com/tensorflow/tensorflow/issues/47204) and the doc [link](https://www.tensorflow.org/guide/migrate) which has more info on `tf.contrib`.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53500\">No</a>\n"]}, {"number": 53499, "title": "Doc Improvement", "body": "https://github.com/tensorflow/tensorflow/blob/c256c071bb26e1e13b4666d1b3e229e110bc914a/tensorflow/python/eager/backprop.py#L812\r\n\r\ndz_dy = tape.gradient(z, w)\r\nThe Variable `dz_dy` could be more intuitive by renaming to `dz_dw`.", "comments": ["Hi @chunduriv ! Could you please look at this issue?", "@kaimo455,\r\n\r\nThe issue will move to closed status once the [PR](https://github.com/tensorflow/tensorflow/pull/53518) is merged. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53498, "title": "SparseTensor generator for tensorflow keras model training", "body": "We had large data to fit for the model training. And in the model construction, we used tf.keras.experimental.SequenceFeatures, which required the input should be SpareTensor. I tired using generaotr, but did not work for SparsTensor. \r\nHere is the sample code:\r\n```\r\nfrom models.model_attention import AttentionModel\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\r\n\r\n\r\ninputs = {'f1': tf.keras.layers.Input(name='f1', sparse=True, shape=(40, 1), dtype='float32'),\r\n          'f2': tf.keras.layers.Input(name='f2', sparse=True, shape=(40, 1), dtype='float32')}\r\n\r\nfeatures = [tf.feature_column.sequence_numeric_column('f1', dtype=tf.float32),\r\n            tf.feature_column.sequence_numeric_column('f2', dtype=tf.float32)]\r\n\r\ninput_layer, _ = tf.keras.experimental.SequenceFeatures(features)(inputs)\r\nlstm_out = LSTM(128, return_sequences=False)(input_layer)\r\nlstm_out = Dropout(0.2)(lstm_out)\r\nlstm_out = Dense(1, activation='tanh')(lstm_out)\r\nmodel = tf.keras.models.Model(inputs, lstm_out)\r\nmodel.compile(loss='mse', metrics='mae', optimizer='Adam')\r\n\r\n\r\ndef gen():\r\n    batch = 4\r\n    while True:\r\n        x1 = tf.sparse.from_dense(np.random.random((batch, 40, 1)))\r\n        x2 = tf.sparse.from_dense(np.random.random((batch, 40, 1)))\r\n        x = {'f1': x1, 'f2': x2}\r\n        y = np.random.random((batch, 1))\r\n        yield x, y\r\n\r\n\r\nx, y = gen().__next__()\r\n# x, y yielded from generator works\r\nmodel.fit(x, y, epochs=2, verbose=2)\r\ng = gen()\r\n# TypeError: Input must be a SparseTensor.\r\nmodel.fit(g, steps_per_epoch=2, epochs=2, verbose=2, validation_data=g, validation_steps=2)\r\n```\r\n\r\ngen() function used to work for normal numpy array generator, but did not work for SparseTensor input. Any advices to the issue?", "comments": ["@henghamao \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "Ok, posted to keras-team. \r\nhttps://github.com/keras-team/keras/issues/15823", "@henghamao Thank you for the update! \r\nCould you please move this issue to closed status as we will track the other ticket  in keras-team/keras ? Thanks!\r\n\r\n> Ok, posted to keras-team.\r\nkeras-team/keras#15823", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53498\">No</a>\n"]}, {"number": 53497, "title": "Range of parameters while converting to TFLite models", "body": "Hi,\r\n\r\nWe are trying to apply an INT8 TFLite model to Arm NPU; however, there are some requirements in the operations. For example, convolution must satisfy the (requirements below)[https://github.com/ARM-software/ethos-n-driver-stack/blob/master/SUPPORTED.md#convolution-2d]:\r\n```\r\nI*W/O must be between 2.33e-10 and 1, where:\r\nI is the input quantization scale.\r\nW is the weight quantization scale.\r\nO is the output quantization scale. \r\n```\r\n\r\nAs a result, is it possible to set the values of input, weight, and output quantization scales in convolution and depthwise convolution layers while converting the pb files to INT8 TFLite models?", "comments": ["@Rahn80643 ,\r\nPlease take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/48879) and the [link](https://www.tensorflow.org/lite/performance/quantization_spec) for more information on quantization scales.It helps.Thanks!", "Hi,\r\nThe [issue](https://github.com/tensorflow/tensorflow/issues/48879) mentions how to modify the weight values in tflite models, and the [link](https://www.tensorflow.org/lite/performance/quantization_spec) is about the specification of the scales of operations. \r\n\r\nThe following steps are how we convert pb files to tflite files:\r\n1. load the pb file and set the input and output node name as a tflite converter\r\n2. set the parameter \"optimization\" as \"tf.lite.Optimize.DEFAULT\"\r\n3. set the parameter \"target_spec.supported_ops\" as \"tf.lite.OpsSet.TFLITE_BUILTINS_INT8\"\r\n4. set the parameter \"inference_input_type as \"tf.int8\"\r\n5. set the parameter \"inference_output_type\" as \"tf.int8\"\r\n\r\nBut we want to know is it possible to control the range of the scale values for the input and output of convolution and depthwise convolution layers.", "@Rahn80643 ,\r\nKindly open a tf discussion forum issue for this as it is not a bug or feature request. There is a big community to support and learn from your questions.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53496, "title": "Update `image_ops_impl.py`", "body": "In line 3755\uff0c modified tf.image.non_max_suppression_padded to  tf.image.non_max_suppression_with_scores;\r\nIn line 3760\uff0cmodified tf.image.non_max_suppression_padded to  tf.image.non_max_suppression_with_scores;\r\nFixes #[53453](https://github.com/tensorflow/tensorflow/issues/53453)", "comments": []}, {"number": 53495, "title": "[ROCm] Switch to ROCm 4.5.2", "body": "also a couple of minor changes...see commit messages for details\r\n\r\n---------------------------------\r\n\r\n/cc @chsigg @cheshire ", "comments": []}, {"number": 53494, "title": "Tensorflow `tf.function` fails if function is called with two identical arguments", "body": "In my TF model, my call functions calls an external energy function which is dependent on a function where single parameter is passed twice (see simplified version below):\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef calc_sw3(gamma,gamma2, cutoff_jk):\r\n    E3 = 2.0\r\n    return E3\r\n\r\n@tf.function\r\ndef calc_sw3_noerr( gamma0, cutoff_jk):\r\n    E3 = 2.0\r\n    return E3\r\n\r\n@tf.function # without tf.function this works fine\r\ndef energy(coords, gamma):\r\n    xyz_i = coords[0, 0 : 3]\r\n    xyz_j = coords[0, 3 : 6]\r\n    rij = xyz_j - xyz_i\r\n    norm_rij = (rij[0]**2 + rij[1]**2 + rij[2]**2)**0.5\r\n    E3 = calc_sw3( gamma,gamma,norm_rij)    # repeating gamma gives error\r\n    # E3 = calc_sw3_noerr( gamma, norm_rij) # this gives no error\r\n    return E3\r\n\r\n\r\n\r\nclass SWLayer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.gamma = tf.Variable(2.51412, dtype=tf.float32)\r\n\r\n    def call(self, coords_all):\r\n        total_conf_energy = energy( coords_all, self.gamma)\r\n        return total_conf_energy\r\n# =============================================================================\r\n\r\n\r\nSWL = SWLayer()\r\ncoords2 = tf.constant([[\r\n                        1.9434,  1.0817,  1.0803,  \r\n                        2.6852,  2.7203,  1.0802,  \r\n                        1.3807,  1.3573,  1.3307]])\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(coords2)\r\n    E = SWL( coords2)\r\n```\r\n\r\nHere if gamma is passed only once, or if I do not use `tf.function` decorator. But with `tf.function` and passing same variable twice, I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"temp_tf.py\", line 47, in <module>\r\n    E = SWL( coords2)\r\n  File \"...venv/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"temp_tf.py\", line 34, in call\r\n    total_conf_energy = energy( coords_all, self.gamma)\r\ntensorflow.python.autograph.impl.api.StagingError: Exception encountered when calling layer \"sw_layer\" (type SWLayer).\r\n\r\nin user code:\r\n\r\n    File \"temp_tf.py\", line 22, in energy  *\r\n        E3 = calc_sw3( gamma,gamma,norm_rij)    # repeating gamma gives error\r\n\r\n    IndexError: list index out of range\r\n\r\n\r\nCall arguments received:\r\n  \u2022 coords_all=tf.Tensor(shape=(1, 9), dtype=float32)\r\n```\r\n\r\nIs this expected behaviour?\r\n\r\n**System information**\r\n- MacOS Mojave\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.7.7", "comments": ["Hi @Saduf2019 ! Could you please look at this issue? It's replicating in [2.6](https://colab.sandbox.google.com/gist/mohantym/ae7eb60bf54e07d6fe210b1ebeb0a065/github_53494.ipynb#scrollTo=D8-xWSqcM3pT) and [2.7](https://colab.sandbox.google.com/gist/mohantym/2c2af612a69e31ba45f0e42788c445ae/github_53494.ipynb#scrollTo=D8-xWSqcM3pT) . Thanks!", "Got some hints on StackOverflow, but exact cause and source is not clear: https://stackoverflow.com/questions/70426458/tensorflow-tf-function-fails-if-function-is-called-with-two-identical-argument", "Thank you for bringing this to our attention and sorry about the inconvenience! The bug has been fixed at the head so you can use the nightly version or wait for the next release!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53494\">No</a>\n"]}, {"number": 53493, "title": "Fix floating point issue on RequantizeManyInNewRange", "body": "Hi,\r\nwhile comparing tests `requantize_op_test` and `quantize_down_and_shrink_range_op_test` I noticed that both tests are quite similar, like so\r\n```\r\nTEST_F(QuantizeDownAndShrinkRangeTest, HandCrafted) {\r\n  TF_ASSERT_OK(NodeDefBuilder(\"quantize_down_and_shrink_range_op\",\r\n                              \"QuantizeDownAndShrinkRange\")\r\n                   .Input(FakeInput(DT_QINT32))\r\n                   .Input(FakeInput(DT_FLOAT))\r\n                   .Input(FakeInput(DT_FLOAT))\r\n                   .Attr(\"Tinput\", DataTypeToEnum<qint32>::v())\r\n                   .Attr(\"out_type\", DataTypeToEnum<quint8>::v())\r\n                   .Finalize(node_def()));\r\n  TF_ASSERT_OK(InitOp());\r\n\r\n  // For this test we have an input that has the theoretical range of -256.0f to\r\n  // +256.0f, but the actual values present only span -1.0f to 1.0f. We expect\r\n  // the operator to take advantage of this, and rescale the output to fill up\r\n  // the available range in the lower bit depth, and update to the true min and\r\n  // max ranges.\r\n  const int value_count = 3;\r\n  AddInputFromArray<qint32>(TensorShape({value_count}),\r\n                            {-(1 << 23), 0, (1 << 23)});\r\n  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\r\n  AddInputFromArray<float>(TensorShape({1}), {256.0f});\r\n  TF_ASSERT_OK(RunOpKernel());\r\n  Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\r\n  test::FillValues<quint8>(&expected, {0, 127, 255});\r\n  test::ExpectTensorEqual<quint8>(expected, *GetOutput(0));\r\n  Tensor expected_min(allocator(), DT_FLOAT, TensorShape({}));\r\n  test::FillValues<float>(&expected_min, {-1.0f});\r\n  test::ExpectTensorEqual<float>(expected_min, *GetOutput(1));\r\n  Tensor expected_max(allocator(), DT_FLOAT, TensorShape({}));\r\n  test::FillValues<float>(&expected_max, {1.0f});\r\n  test::ExpectTensorEqual<float>(expected_max, *GetOutput(2));\r\n}\r\n```\r\nand \r\n```\r\n// Runs a manually generated array through the operator, and makes sure that the\r\n// results match the expected hand-calculated values.\r\nTEST_F(RequantizeTest, HandCraftedRequantize) {\r\n  ConfigureRequantize();\r\n  const int value_count = 3;\r\n\r\n  // Requantize to -1 to 1.\r\n  AddInputFromArray<qint32>(TensorShape({value_count}),\r\n                            {-(1 << 23), 0, (1 << 23)});\r\n  AddInputFromArray<float>(TensorShape({1}), {-256.0f});\r\n  AddInputFromArray<float>(TensorShape({1}), {256.0f});\r\n  AddInputFromArray<float>(TensorShape({1}), {-1.0f});\r\n  AddInputFromArray<float>(TensorShape({1}), {1.0f});\r\n  TF_ASSERT_OK(RunOpKernel());\r\n  Tensor expected(allocator(), DT_QUINT8, TensorShape({value_count}));\r\n  test::FillValues<quint8>(&expected, {0, 128, 255});\r\n  test::ExpectTensorEqual<quint8>(expected, *GetOutput(0));\r\n  test::ExpectTensorEqual<float>(test::AsScalar<float>(-1.0f), *GetOutput(1));\r\n  test::ExpectTensorEqual<float>(test::AsScalar<float>(1.0f), *GetOutput(2));\r\n}\r\n```\r\nhowever one expects the result to be 127 and the other 128. \r\n\r\nIf you try to pinpoint the problem `RequantizeManyInNewRange` receives on both tests `min_float` as -1 but on one the bit representation is `10111111100000000000000000000000` and the other one `10111111011111111111111111111111` since `static_cast` just truncates the result differs in both executions. This can be seen on both x86 as well as on Arm. \r\n\r\nThe patch fixes this using a default rounding strategy as well as the expected value on `quantize_down_and_shrink_range_op_test` to match.", "comments": ["I should already be on the CLA list if it's possible, please rerun the check.", "> I should already be on the CLA list if it's possible, please rerun the check.\r\n\r\n@everton1984 I have checked and still it shows you are not signed CLA.  Can you please make sure to use same GitHub username and email-id associated with it.  Thank you!.", "Hi.\r\n\r\nCan you make sure this email address is used to sign the CLA?\r\n\r\n\r\n\r\nGitHub Login | Email | State\r\n-- | -- | --\r\neverton1984 | everton.constantino@linaro.org | Need CLA\r\n", "> Hi.\r\n> \r\n> Can you make sure this email address is used to sign the CLA?\r\n> GitHub Login \tEmail \tState\r\n> everton1984 \t[everton.constantino@linaro.org](mailto:everton.constantino@linaro.org) \tNeed CLA\r\n\r\nI self-signed just now. Hope it works. Thanks", "Seems to have worked."]}, {"number": 53492, "title": "Document the verbose parameter in EarlyStopping", "body": null, "comments": ["It looks like your PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou.\r\n@fchollet, @qlzh727"]}, {"number": 53490, "title": "[MHLO] add BroadcastOp constant folding and identity folding", "body": "* add BroadcastOp constant folding and identity folding", "comments": ["@joker-eph  please review again. Thanks!"]}, {"number": 53489, "title": "Off by one discrepancy for tf.nn.conv1d between cpu and gpu", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information (CPU Tensorflow code)**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.6 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**\r\n- Python version: **Python 3.9.0**\r\n-  Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n\r\n**System information (GPU Tensorflow code)**\r\n- Running Docker image **tensorflow/tensorflow:latest-gpu-jupyter**, imageID: **8da916739a38**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.3 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **unsure**\r\n- TensorFlow version (use command below): **v2.7.0-rc1-69-gc256c071bb2 2.7.0**\r\n- Python version: **3.8.10**\r\n- Bazel version (if compiling from source): **unsure**\r\n- GCC/Compiler version (if compiling from source): **unsure**\r\n- CUDA/cuDNN version: \r\n  - Cuda: **11.2.152** build **cuda_11.2.r11.2/compiler.29618528_0**\r\n  - CUDNN: **8100**\r\n- GPU model and memory: **GeForce GTX 1060** w/ **6078MiB**\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**:\r\nCode attached takes as input a binary sequence and convolves a filter over it to compute the decimal representation of each length 5 binary substring. For example, the binary sequence` [0, 1, 1, 0, 1, 1]` gets converted to `[13, 27]`.\r\n\r\nWhen the length of the string is 33 or greater, the CPU and GPU outputs differ. CPU is correct and GPU is off-by-one in some cases. There is no noticeable pattern for when the GPU is off-by-one:\r\n\r\n**CPU:**\r\nInput binary sequence: `[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0]`\r\nConverted decimal sequence `[1, 3, 7, 14, 28, 24, 16, 0, 0, 1, 3, 7, 15, 31, 31, 31, 31, 31, 31, 31, 30, 29, 27, 22, 12, 25, 19, 6, 13, 27, 23, 14, 28]`\r\n\r\n**GPU:**\r\nInput binary sequence: `[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0]`\r\nConverted decimal sequence: `[1, 3, 7, 13, 28, 23, 16, 0, 0, 1, 3, 6, 14, 30, 30, 30, 30, 31, 30, 30, 30, 29, 26, 22, 12, 25, 19, 6, 12, 26, 22, 13, 27]`\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nBoth converted decimal sequences should agree. The CPU sequence is the correct one.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): **no**\r\n- Briefly describe your candidate solution(if contributing): **N/A**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef base_2_accumulator(length: int):\r\n    powers_of_2 = tf.bitwise.left_shift(1, tf.range(length))\r\n    return powers_of_2[::-1]\r\n\r\nrng = np.random.default_rng(0)\r\n\r\ndump_file = './cpu_outputs.txt'\r\n# dump_file = './gpu_outputs.txt'\r\n\r\nf = open(dump_file, 'w')\r\n\r\nbatch_size = 1000\r\nblock_len=33\r\nmsg = tf.constant(rng.integers(0, 2, size=(batch_size, block_len, 1)), dtype=tf.int32)\r\nprint('msg', file=f)\r\nprint(msg, file=f)\r\n\r\nwindow = 5\r\nbase_2 = base_2_accumulator(window)\r\nprint('base_2', file=f)\r\nprint(base_2, file=f)\r\nprint(base_2)\r\n\r\nbase_2_filter = tf.cast(base_2[:, None, None], dtype=tf.float32)\r\nprint('base_2_filter', file=f)\r\nprint(base_2_filter, file=f)\r\n\r\nmsg_prepended = tf.pad(msg[:, :, 0], paddings=tf.constant([[0, 0], [window-1, 0]]))\r\nprint('msg_prepended', file=f)\r\nprint(msg_prepended, file=f)\r\n\r\nconv_msg_input = tf.cast(msg_prepended[:,:,None], dtype=tf.float32)\r\nprint('conv_msg_input', file=f)\r\nprint(conv_msg_input, file=f)\r\n\r\nstate_sequence = tf.cast(tf.nn.conv1d(conv_msg_input, base_2_filter, stride=1, padding='VALID'), dtype=tf.int32)[:,:,0]\r\nprint('state_sequence', file=f)\r\nprint(state_sequence, file=f)\r\nprint('first binary sequence', file=f)\r\nprint(msg_prepended[0].numpy().tolist(), file=f)\r\nprint('first state sequence', file=f)\r\nprint(state_sequence[0].numpy().tolist(), file=f)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@abhmul ,\r\nI was able to run the code without any issues on TF v2.7. Please find the gist [1](https://colab.research.google.com/gist/tilakrayal/fb0f066ca7955cdc7a8f88b1cb2eee5f/53489-cpu.ipynb) and [2](https://colab.research.google.com/gist/tilakrayal/a83c1ac1f61891dad6545c11f9a963fa/53489-gpu.ipynb) of it here.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53489\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53489\">No</a>\n"]}, {"number": 53488, "title": "Tensorflow.fit method is using 3 times more memory than it should", "body": "Hi! \r\n\r\nI am running two near-identical scripts. The code is provided in the following links:\r\nscript1.py: https://pastebin.com/66i8kEnK\r\nscript2.py: https://pastebin.com/AWfQUguq\r\n\r\nThe code for the helper-function which is in some_file_1.py:\r\nhelper.py: https://pastebin.com/fXgWcuz5\r\n\r\nscript1.py is using a 3 times more memory than script2.py. The only main difference between the two is that, in script1, I am converting my data and params to tf.tensors as I pass into model.fit method and in script2, I convert before passing into the .fit method.\r\n\r\nMy python version is 3.9.7 and tensorflow version is 2.4.1. \r\n\r\nThe OS is: Red Hat Enterprise Linux\r\nThe Kernel is: Linux 3.10.0-1160.2.1.el7.x86_64\r\nThe architecture is: x86-64\r\n\r\nThe GPU is Nvidia either P100 or Nvidia V100. \r\n\r\n", "comments": ["Hi @Chaudry24 ! Could you please provide the above code as Colab gist as its hard to replicate to this issue because of missing .py and .npy files.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 53487, "title": "F2 Score and mAP metric for object detection model.", "body": "I didn't find a metric regarding the object detection task in keras. \r\n\r\nHow to implement the above two metrics in keras and use them? In this competition, https://www.kaggle.com/c/tensorflow-great-barrier-reef/overview/evaluation uses the F2 score for its object detection task but I don't know how to evaluate my model locally with this score? \r\n\r\n> The metric sweeps over IoU thresholds in the range of 0.3 to 0.8 with a step size of 0.05, calculating an F2 score at each threshold. For example, at a threshold of 0.5, a predicted object is considered a \"hit\" if its IoU with a ground truth object is at least 0.5.\r\n", "comments": ["@Suzan009 \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "I did. But I also want to achieve this in pure tensorflow. ", "@Suzan009 \r\nPlease follow the TFOD API code and tensorflow garden, as this repo is for bugs and feature request, we request you to move this issue to closed status and for any further queries open a new issue at discussion forum which has larger community to help.\r\nThanks!", "@Saduf2019 I am not following the messy repo of the [tensorflow-model garden](https://github.com/tensorflow/models/tree/master/research/object_detection). In fact, this so-called model garden repo should be closed. \r\n\r\nI'm simply looking to get the implementation of these two metrics in pure tensorflow. Asking or suggesting to look into TFOD API, you're making someone's life harder. ", "@Suzan009 \r\n\r\nYou may take the following command as a reference to get the mAP values.\r\n\r\npython models/research/object_detection/model_main_tf2.py \r\n--model_dir = <YOUR_MODEL_PATH>\r\n--pipeline_config_path = <YOUR_PIPELINE.CONFIG_PATH>\r\n--checkpoint_dir = <YOUR_CHECKPOINT_DIR_PATH>\r\n\r\nAlternatively, you can visualize these metrics on Tensorboard by initiating Tensorboard in the eval directory.\r\nAs informed earlier this repo is for bugs you have to open this issue in tf discussion forum."]}]