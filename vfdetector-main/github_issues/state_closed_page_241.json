[{"number": 47297, "title": "Load Train Model From Checkpoint - NotFoundError", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- TensorFlow installed from (source or binary):\r\n- https://github.com/tensorflow/models\r\n- TensorFlow version: 2\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: conda and pip\r\n\r\n**Describe the problem**\r\nI have downloaded and installed tensorflow, and I'm attempting to train a custom model, but keep getting runtime errors or notfounderrors to do with tenorflow lib files.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`WORKSPACE_PATH = 'Tensorflow/workspace'\r\nSCRIPTS_PATH = 'Tensorflow/scripts'\r\nAPIMODEL_PATH = 'Tensorflow/models'\r\nANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\r\nIMAGE_PATH = WORKSPACE_PATH+'/images'\r\nMODEL_PATH = WORKSPACE_PATH+'/models'\r\nPRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\r\nCONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\r\nCHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'\r\n\r\nlabels = [{'name':'title', 'id':1}, {'name':'xaxis', 'id':2}, {'name':'yaxis', 'id':3}, {'name':'bar', 'id':4}, {'name':'key', 'id':5}]\r\n\r\nwith open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\r\n    for label in labels:\r\n        f.write('item { \\n')\r\n        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\r\n        f.write('\\tid:{}\\n'.format(label['id']))\r\n        f.write('}\\n')\r\n\r\n!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\r\n!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}\r\n\r\n!cd Tensorflow && git clone https://github.com/tensorflow/models\r\n\r\nCUSTOM_MODEL_NAME = 'my_ssd_mobnet' \r\n!mkdir {'Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}\r\n!cp {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}\r\n\r\nimport tensorflow as tf\r\nfrom object_detection.utils import config_util\r\nfrom object_detection.protos import pipeline_pb2\r\nfrom google.protobuf import text_format\r\n\r\nCONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'\r\n\r\nconfig = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\r\nconfig\r\n{'model': ssd {\r\n   num_classes: 90\r\n   image_resizer {\r\n     fixed_shape_resizer {\r\n       height: 320\r\n       width: 320\r\n     }\r\n   }\r\n   feature_extractor {\r\n     type: \"ssd_mobilenet_v2_fpn_keras\"\r\n     depth_multiplier: 1.0\r\n     min_depth: 16\r\n     conv_hyperparams {\r\n       regularizer {\r\n         l2_regularizer {\r\n           weight: 3.9999998989515007e-05\r\n         }\r\n       }\r\n       initializer {\r\n         random_normal_initializer {\r\n           mean: 0.0\r\n           stddev: 0.009999999776482582\r\n         }\r\n       }\r\n       activation: RELU_6\r\n       batch_norm {\r\n         decay: 0.996999979019165\r\n         scale: true\r\n         epsilon: 0.0010000000474974513\r\n       }\r\n     }\r\n     use_depthwise: true\r\n     override_base_feature_extractor_hyperparams: true\r\n     fpn {\r\n       min_level: 3\r\n       max_level: 7\r\n       additional_layer_depth: 128\r\n     }\r\n   }\r\n   box_coder {\r\n     faster_rcnn_box_coder {\r\n       y_scale: 10.0\r\n       x_scale: 10.0\r\n       height_scale: 5.0\r\n       width_scale: 5.0\r\n     }\r\n   }\r\n   matcher {\r\n     argmax_matcher {\r\n       matched_threshold: 0.5\r\n       unmatched_threshold: 0.5\r\n       ignore_thresholds: false\r\n       negatives_lower_than_unmatched: true\r\n       force_match_for_each_row: true\r\n       use_matmul_gather: true\r\n     }\r\n   }\r\n   similarity_calculator {\r\n     iou_similarity {\r\n     }\r\n   }\r\n   box_predictor {\r\n     weight_shared_convolutional_box_predictor {\r\n       conv_hyperparams {\r\n         regularizer {\r\n           l2_regularizer {\r\n             weight: 3.9999998989515007e-05\r\n           }\r\n         }\r\n         initializer {\r\n           random_normal_initializer {\r\n             mean: 0.0\r\n             stddev: 0.009999999776482582\r\n           }\r\n         }\r\n         activation: RELU_6\r\n         batch_norm {\r\n           decay: 0.996999979019165\r\n           scale: true\r\n           epsilon: 0.0010000000474974513\r\n         }\r\n       }\r\n       depth: 128\r\n       num_layers_before_predictor: 4\r\n       kernel_size: 3\r\n       class_prediction_bias_init: -4.599999904632568\r\n       share_prediction_tower: true\r\n       use_depthwise: true\r\n     }\r\n   }\r\n   anchor_generator {\r\n     multiscale_anchor_generator {\r\n       min_level: 3\r\n       max_level: 7\r\n       anchor_scale: 4.0\r\n       aspect_ratios: 1.0\r\n       aspect_ratios: 2.0\r\n       aspect_ratios: 0.5\r\n       scales_per_octave: 2\r\n     }\r\n   }\r\n   post_processing {\r\n     batch_non_max_suppression {\r\n       score_threshold: 9.99999993922529e-09\r\n       iou_threshold: 0.6000000238418579\r\n       max_detections_per_class: 100\r\n       max_total_detections: 100\r\n       use_static_shapes: false\r\n     }\r\n     score_converter: SIGMOID\r\n   }\r\n   normalize_loss_by_num_matches: true\r\n   loss {\r\n     localization_loss {\r\n       weighted_smooth_l1 {\r\n       }\r\n     }\r\n     classification_loss {\r\n       weighted_sigmoid_focal {\r\n         gamma: 2.0\r\n         alpha: 0.25\r\n       }\r\n     }\r\n     classification_weight: 1.0\r\n     localization_weight: 1.0\r\n   }\r\n   encode_background_as_zeros: true\r\n   normalize_loc_loss_by_codesize: true\r\n   inplace_batchnorm_update: true\r\n   freeze_batchnorm: false\r\n }, 'train_config': batch_size: 128\r\n data_augmentation_options {\r\n   random_horizontal_flip {\r\n   }\r\n }\r\n data_augmentation_options {\r\n   random_crop_image {\r\n     min_object_covered: 0.0\r\n     min_aspect_ratio: 0.75\r\n     max_aspect_ratio: 3.0\r\n     min_area: 0.75\r\n     max_area: 1.0\r\n     overlap_thresh: 0.0\r\n   }\r\n }\r\n sync_replicas: true\r\n optimizer {\r\n   momentum_optimizer {\r\n     learning_rate {\r\n       cosine_decay_learning_rate {\r\n         learning_rate_base: 0.07999999821186066\r\n         total_steps: 50000\r\n         warmup_learning_rate: 0.026666000485420227\r\n         warmup_steps: 1000\r\n       }\r\n     }\r\n     momentum_optimizer_value: 0.8999999761581421\r\n   }\r\n   use_moving_average: false\r\n }\r\n fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\"\r\n num_steps: 50000\r\n startup_delay_steps: 0.0\r\n replicas_to_aggregate: 8\r\n max_number_of_boxes: 100\r\n unpad_groundtruth_tensors: false\r\n fine_tune_checkpoint_type: \"classification\"\r\n fine_tune_checkpoint_version: V2, 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\r\n tf_record_input_reader {\r\n   input_path: \"PATH_TO_BE_CONFIGURED\"\r\n }, 'eval_config': metrics_set: \"coco_detection_metrics\"\r\n use_moving_averages: false, 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED\"\r\n shuffle: false\r\n num_epochs: 1\r\n tf_record_input_reader {\r\n   input_path: \"PATH_TO_BE_CONFIGURED\"\r\n }\r\n ], 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\r\n shuffle: false\r\n num_epochs: 1\r\n tf_record_input_reader {\r\n   input_path: \"PATH_TO_BE_CONFIGURED\"\r\n }}\r\npipeline_config = pipeline_p\r\n\r\npipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\r\nwith tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \r\n    proto_str = f.read()                                                                                                                                                                                                                                          \r\n    text_format.Merge(proto_str, pipeline_config)  \r\npipeline_config.model.ssd.num_classes = 2\r\npipeline_config.train_config.batch_size = 4\r\npipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\r\npipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\r\npipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'\r\npipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']\r\npipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'\r\npipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']\r\n\r\nconfig_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \r\nwith tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \r\n    f.write(config_text)   \r\n\r\nprint(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=5000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))\r\n\r\nimport os\r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import visualization_utils as viz_utils\r\nfrom object_detection.builders import model_builder\r\n\r\n# Load pipeline config and build a detection model\r\nconfigs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\r\ndetection_model = model_builder.build(model_config=configs['model'], is_training=False)\r\n\r\n# Restore checkpoint\r\nckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\r\nckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\r\n\r\n@tf.function\r\ndef detect_fn(image):\r\n    image, shapes = detection_model.preprocess(image)\r\n    prediction_dict = detection_model.predict(image, shapes)\r\n    detections = detection_model.postprocess(prediction_dict, shapes)\r\n    return detections\r\n#---------------------This Is Where The error Happens ----------------------------------------`\r\n\r\n\r\n**Error Message**\r\n`---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py in NewCheckpointReader(filepattern)\r\n     94   try:\r\n---> 95     return CheckpointReader(compat.as_bytes(filepattern))\r\n     96   # TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\r\n\r\nRuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in restore(self, save_path, options)\r\n   2259     try:\r\n-> 2260       status = self.read(save_path, options=options)\r\n   2261     except errors_impl.NotFoundError:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in read(self, save_path, options)\r\n   2147     options = options or checkpoint_options.CheckpointOptions()\r\n-> 2148     return self._saver.restore(save_path=save_path, options=options)\r\n   2149 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in restore(self, save_path, options)\r\n   1291       return InitializationOnlyStatus(self._graph_view, ops.uid())\r\n-> 1292     reader = py_checkpoint_reader.NewCheckpointReader(save_path)\r\n   1293     graph_building = not context.executing_eagerly()\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py in NewCheckpointReader(filepattern)\r\n     98   except RuntimeError as e:\r\n---> 99     error_translator(e)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py in error_translator(e)\r\n     34       'matching files for') in error_message:\r\n---> 35     raise errors_impl.NotFoundError(None, None, error_message)\r\n     36   elif 'Sliced checkpoints are not supported' in error_message or (\r\n\r\nNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-17-f5bf27ce595e> in <module>\r\n      5 # Restore checkpoint\r\n      6 ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\r\n----> 7 ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\r\n      8 \r\n      9 @tf.function\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py in restore(self, save_path, options)\r\n   2263           None, None,\r\n   2264           \"Could not find checkpoint or SavedModel at {}.\"\r\n-> 2265           .format(orig_save_path))\r\n   2266     # Create the save counter now so it gets initialized with other variables\r\n   2267     # when graph building. Creating it earlier would lead to errors when using,\r\n\r\nNotFoundError: Could not find checkpoint or SavedModel at Tensorflow/workspace/models/my_ssd_mobnet/ckpt-6.`", "comments": ["@Fawcett-cpu,\r\nIssues related to TensorFlow Model Garden are tracked in tensorflow/models repo. \r\n\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/models/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "This is the new issue https://github.com/tensorflow/models/issues/9743\r\n", "@Fawcett-cpu,\r\nThank you for the update. Closing this issue as it is being tracked in tensorflow/models repo."]}, {"number": 47296, "title": "Remove unnecessary np_bool conversion in basic test.", "body": "Add a tie breaking tests along axis=1.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "> @googlebot I signed it!\r\n\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "@googlebot rescan", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "@googlebot rescan", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->", "@googlebot rescan", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47296) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 47295, "title": "Update license to 2021", "body": "Update the license year from 2019 to 2021", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47295) for more info**.\n\n<!-- need_sender_cla -->", "@PawelBorkar  Can you please sign CLA. Thanks!", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47295) for more info**.\r\n\r\n@googlebot I signed it!", "> @PawelBorkar Can you please sign CLA. Thanks!\r\n\r\nI have signed it. \ud83d\ude0a", "LICENSE text should not be changed as that has copyright implications. The form as written is the right one. Unless lawyers decide a change, the text should stay the same as it was after #29523", "> \r\n> \r\n> LICENSE text should not be changed as that has copyright implications. The form as written is the right one. Unless lawyers decide a change, the text should stay the same as it was after #29523\r\n\r\nThanks for the information. Got something new to learn about the LICENSE. Have a great day."]}, {"number": 47294, "title": "Cant compile because there is no cuda.h", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source 5534f5d3208\r\n- TensorFlow version: 5534f5d3208\r\n- Python version: 3.8.5\r\n- Installed using: conda\r\n- Bazel version (if compiling from source): 3.7.2 (installed with go get )\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: GeForce RTX 3090 24267MiB\r\n- NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2 \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCant compile because there is no cuda.h\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nCan't compile with bazel because there is no cuda.h\r\n\r\n## To Reproduce\r\n\r\nUse this command inside tensorflow folder\r\n\r\n```\r\n(xla) \u2714 ~/Documents/github/pytorch/xla/third_party/tensorflow [:5534f5d3208|\u20261] \r\n00:04 $ bazel build --define framework_shared_object=false -c opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --cxxopt=-std=c++14 --cxxopt=-Wno-c++11-narrowing --cxxopt=-DXLA_CUDA=1 --config=cuda //tensorflow/compiler/xla/xla_client:libxla_computation_client.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=211\r\nINFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository local_config_cuda instantiated at:\r\n  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/WORKSPACE:12:10: in <toplevel>\r\n  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/workspace2.bzl:13:20: in workspace\r\n  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/workspace.bzl:95:19: in tf_repositories\r\nRepository rule cuda_configure defined at:\r\n  /home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl:1430:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1400, column 38, in _cuda_autoconf_impl\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl\", line 977, column 35, in _create_local_cuda_repository\r\n\t\tcuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)\r\n\tFile \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl\", line 666, column 30, in _get_cuda_config\r\n\t\tconfig = find_cuda_config(repository_ctx, find_cuda_config_script, [\"cuda\", \"cudnn\"])\r\n\tFile \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl\", line 643, column 41, in find_cuda_config\r\n\t\texec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)\r\n\tFile \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/gpus/cuda_configure.bzl\", line 637, column 19, in _exec_find_cuda_config\r\n\t\treturn execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])\r\n\tFile \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/third_party/remote_config/common.bzl\", line 217, column 13, in execute\r\n\t\tfail(\r\nError in fail: Repository command failed\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib'\r\n        '/lib/i386-linux-gnu'\r\n        '/lib/i386-linux-gnu/i686/sse2'\r\n        '/lib/i386-linux-gnu/sse2'\r\n        '/lib/x86_64-linux-gnu'\r\n        '/lib32'\r\n        '/usr'\r\n        '/usr/lib/x86_64-linux-gnu/libfakeroot'\r\n        '/usr/local/cuda'\r\n        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'\r\n        '/usr/local/lib'\r\nERROR: Skipping '//tensorflow/compiler/xla/xla_client:libxla_computation_client.so': no such package '@local_config_cuda//cuda': Repository command failed\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib'\r\n        '/lib/i386-linux-gnu'\r\n        '/lib/i386-linux-gnu/i686/sse2'\r\n        '/lib/i386-linux-gnu/sse2'\r\n        '/lib/x86_64-linux-gnu'\r\n        '/lib32'\r\n        '/usr'\r\n        '/usr/lib/x86_64-linux-gnu/libfakeroot'\r\n        '/usr/local/cuda'\r\n        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'\r\n        '/usr/local/lib'\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Repository command failed\r\nCould not find any cuda.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib'\r\n        '/lib/i386-linux-gnu'\r\n        '/lib/i386-linux-gnu/i686/sse2'\r\n        '/lib/i386-linux-gnu/sse2'\r\n        '/lib/x86_64-linux-gnu'\r\n        '/lib32'\r\n        '/usr'\r\n        '/usr/lib/x86_64-linux-gnu/libfakeroot'\r\n        '/usr/local/cuda'\r\n        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'\r\n        '/usr/local/lib'\r\nINFO: Elapsed time: 0.215s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/compiler/xla/xla_client\r\n\r\n```\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n$ cat tf_env.txt\r\n\r\n== check python ===================================================\r\npython version: 3.8.5\r\npython branch: \r\npython build version: ('default', 'Sep  4 2020 07:30:14')\r\npython compiler version: GCC 7.3.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\nCopyright (C) 2019 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                    1.20.1\r\nprotobuf                 3.15.1\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\n    147497:\tfind library=libpthread.so.0 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell:/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls:/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell:/home/tyoc213/miniconda3/envs/xla/bin/../lib/x86_64:/home/tyoc213/miniconda3/envs/xla/bin/../lib\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell/x86_64/libpthread.so.0\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/haswell/libpthread.so.0\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/x86_64/libpthread.so.0\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/tls/libpthread.so.0\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell/x86_64/libpthread.so.0\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/haswell/libpthread.so.0\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/x86_64/libpthread.so.0\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libpthread.so.0\r\n    147497:\t search cache=/etc/ld.so.cache\r\n    147497:\t  trying file=/lib/x86_64-linux-gnu/libpthread.so.0\r\n    147497:\t\r\n    147497:\tfind library=libc.so.6 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libc.so.6\r\n    147497:\t search cache=/etc/ld.so.cache\r\n    147497:\t  trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n    147497:\t\r\n    147497:\tfind library=libdl.so.2 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libdl.so.2\r\n    147497:\t search cache=/etc/ld.so.cache\r\n    147497:\t  trying file=/lib/x86_64-linux-gnu/libdl.so.2\r\n    147497:\t\r\n    147497:\tfind library=libutil.so.1 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libutil.so.1\r\n    147497:\t search cache=/etc/ld.so.cache\r\n    147497:\t  trying file=/lib/x86_64-linux-gnu/libutil.so.1\r\n    147497:\t\r\n    147497:\tfind library=librt.so.1 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/librt.so.1\r\n    147497:\t search cache=/etc/ld.so.cache\r\n    147497:\t  trying file=/lib/x86_64-linux-gnu/librt.so.1\r\n    147497:\t\r\n    147497:\tfind library=libm.so.6 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libm.so.6\r\n    147497:\t search cache=/etc/ld.so.cache\r\n    147497:\t  trying file=/lib/x86_64-linux-gnu/libm.so.6\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /lib/x86_64-linux-gnu/libpthread.so.0\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /lib/x86_64-linux-gnu/libc.so.6\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /lib/x86_64-linux-gnu/libm.so.6\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /lib/x86_64-linux-gnu/librt.so.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /lib/x86_64-linux-gnu/libutil.so.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /lib/x86_64-linux-gnu/libdl.so.2\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tinitialize program: /home/tyoc213/miniconda3/envs/xla/bin/python\r\n    147497:\t\r\n    147497:\t\r\n    147497:\ttransferring control: /home/tyoc213/miniconda3/envs/xla/bin/python\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\tfind library=libffi.so.7 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell/x86_64/libffi.so.7\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/haswell/libffi.so.7\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/x86_64/libffi.so.7\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../tls/libffi.so.7\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell/x86_64/libffi.so.7\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../haswell/libffi.so.7\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../x86_64/libffi.so.7\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libffi.so.7\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libffi.so.7\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\tfind library=libopenblasp-r0-5bebc122.3.13.dev.so [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/x86_64:/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/haswell/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/tls/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/haswell/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/x86_64/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t\r\n    147497:\tfind library=libgfortran-2e0d59d6.so.5.0.0 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0\r\n    147497:\t\r\n    147497:\tfind library=libquadmath-2d0c479f.so.0.0.0 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0\r\n    147497:\t\r\n    147497:\tfind library=libz-eb09ad1d.so.1.2.3 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3\r\n    147497:\t\r\n    147497:\tfind library=libgcc_s.so.1 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgcc_s.so.1\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/bin/../lib\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/bin/python)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/bin/../lib/libgcc_s.so.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/bin/../lib/libgcc_s.so.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\tfind library=libz.so.1 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libz.so.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libz.so.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\tfind library=liblzma.so.5 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../liblzma.so.5\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../liblzma.so.5\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\tfind library=libcrypto.so.1.1 [0]; searching\r\n    147497:\t search path=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../..\t\t(RPATH from file /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so)\r\n    147497:\t  trying file=/home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libcrypto.so.1.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libcrypto.so.1.1\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling init: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so\r\n    147497:\t\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/home/tyoc213/Documents/github/pytorch/xla/third_party/tensorflow/tensorflow/python/eager/context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\nImportError: cannot import name 'function_pb2' from 'tensorflow.core.framework' (unknown location)\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/bin/python [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_heapq.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libffi.so.7 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_struct.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/math.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bisect.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha512.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_random.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/termios.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_socket.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/select.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_csv.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/fcntl.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_datetime.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_pickle.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/_multiarray_tests.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/linalg/_umath_linalg.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libgfortran-2e0d59d6.so.5.0.0 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/bin/../lib/libgcc_s.so.1 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libz-eb09ad1d.so.1.2.3 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/core/../../numpy.libs/libquadmath-2d0c479f.so.0.0.0 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/zlib.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_bz2.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_lzma.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../liblzma.so.5 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/grp.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/fft/_pocketfft_internal.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/mtrand.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/bit_generator.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_common.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/binascii.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libz.so.1 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_hashlib.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/../../libcrypto.so.1.1 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_blake2.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_sha3.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_bounded_integers.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_mt19937.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_philox.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_pcg64.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_sfc64.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/site-packages/numpy/random/_generator.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /lib/x86_64-linux-gnu/libm.so.6 [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /home/tyoc213/miniconda3/envs/xla/lib/python3.8/lib-dynload/_opcode.cpython-38-x86_64-linux-gnu.so [0]\r\n    147497:\t\r\n    147497:\t\r\n    147497:\tcalling fini: /lib/x86_64-linux-gnu/libpthread.so.0 [0]\r\n    147497:\t\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSun Feb 21 00:14:33 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 3090    Off  | 00000000:02:00.0  On |                  N/A |\r\n| 35%   33C    P8    24W / 350W |    453MiB / 24267MiB |     12%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      9920      G   /usr/lib/xorg/Xorg                 35MiB |\r\n|    0   N/A  N/A     10224      G   /usr/lib/xorg/Xorg                209MiB |\r\n|    0   N/A  N/A     10395      G   /usr/bin/gnome-shell               71MiB |\r\n|    0   N/A  N/A     10421      G   ...mviewer/tv_bin/TeamViewer        4MiB |\r\n|    0   N/A  N/A     13811      G   ...AAAAAAAA== --shared-files       33MiB |\r\n|    0   N/A  N/A     16812      G   ...gAAAAAAAAA --shared-files       48MiB |\r\n|    0   N/A  N/A     19304      G   ...AAAAAAAAA= --shared-files       23MiB |\r\n|    0   N/A  N/A     92089      G   /usr/lib/firefox/firefox            4MiB |\r\n|    0   N/A  N/A    119133      G   /usr/lib/firefox/firefox            4MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1.74\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 5, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\nBazelisk version: development\r\nBuild label: 3.7.2\r\nBuild time: Thu Dec 17 16:57:23 2020 (1608224243)\r\nBuild timestamp: 1608224243\r\nBuild timestamp as int: 1608224243\r\n\r\n```\r\n", "comments": ["@tyoc213 \r\nFor built from source please confirm if you have followed [link](https://www.tensorflow.org/install/source),\r\nEvery TensorFlow release is compatible with certain a CUDA and cuDNN version. You can check the compatibility from the tested [build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nAlso, check these similar issues:\r\n#46093, #45667,  [link](https://stackoverflow.com/questions/63030087/tensorflow-source-build-configuration-fails-could-not-find-any-cuda-h-matching)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47294\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47294\">No</a>\n"]}, {"number": 47293, "title": "Update License year to 2021", "body": "Update license year from 2019 to 2021", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47293) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 47292, "title": "Can the GradCAM being integrate to savedModel format?", "body": "**System information**\r\nColaboratory\r\nTensorflow 2.4\r\nTested on below version\r\nv2.4.0-49-g85c8b2a817f 2.4.1\r\nv2.3.0-54-gfcc4b966f1 2.3.1\r\n\r\n**Describe the current behavior**\r\nI'm trying to make a savedModel that can generate GradCAM heatmap, however it shows errors that the tape.gradient or tf.gradients is None can't be compute, Does anyone can help with this?\r\n\r\n\r\n**Describe the expected behavior**\r\nExpected that can save to savedModel .\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nThis is the colaboratory notebook.\r\n\r\nhttps://colab.research.google.com/drive/1uX4T_DZWaptZ4pF2akawB5FYYMgRtX7y?authuser=1#scrollTo=QBsBrZ5JEk-N\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@luvwinnie \r\n\r\nPlease, grant me the access for the colab link. Thanks!", "@ravikyram I have added the permission! ", "It seems like i need to do this by disable the eager mode and save with tensorflow V1 savedModel Format.... I think this should be fixed for any model that need to compute the gradient during inference time, especially for the model that need to output such as GradCAM/GradCAM++/ScoreCAM and etc for explainability/interpretability of the model.\r\n\r\nThe below code is the work around.\r\nhttps://colab.research.google.com/drive/1HXAhebh9liIhWjgO3UAqnj_m1CTYOf9W?authuser=1#scrollTo=Osob4efBAPuF", "Any progress on this?", "@luvwinnie ,\r\n\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status.\r\n", "@tilakrayal It is not able to integrate the GradCAM inside the model, Instead I Implement the gradient calculation and GradCAM  with only numpy.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47292\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47292\">No</a>\n"]}, {"number": 47291, "title": "micro: port op LOG_SOFTMAX from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator LOG_SOFTMAX from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro making minimal changes and not including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47291\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47291\">No</a>\n"]}, {"number": 47290, "title": "micro: port op CUMSUM from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator CUMSUM from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1 (step 1): Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2 (step 2): Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\n\r\nThe next 3 steps are combined into a single PR3 with separate commits:\r\n\r\n(step 3): Copy operator from lite to micro making minimal changes and not including in the build\r\n(step 4): Delete extra code from the micro copy of the operator\r\n(step 5): Port micro copy of operator as necessary and add a corresponding test", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47290\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47290\">No</a>\n"]}, {"number": 47289, "title": "tf.keras.layers.ZeroPadding2D crashes (segfault)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\ntf.keras.layers.ZeroPadding2D crashes (segfault) when `padding` is large and input contains 0 in shape.\r\n\r\n**Describe the expected behavior**\r\nExpect graceful exception messages instead of crash\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nlayer = tf.keras.layers.ZeroPadding2D(padding=420958214)\r\nlayer(np.ones((0, 4, 4, 4)))\r\n~~~\r\n\r\nOutput\r\n~~~python\r\nSegmentation fault (core dumped)\r\n~~~\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.1, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/5a7bf0505aeefa574e68412d2f18a32e/untitled676.ipynb). Thanks!", "Was able to run your code without any errors in Tensorflow version 2.5, please find the gist [here](https://colab.research.google.com/gist/googly789/bb8e74e4bf27cc8440665f94181eb0bc/untitled96.ipynb).", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47289\">No</a>\n"]}, {"number": 47288, "title": "    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', ...]", "body": "This is not the first time I encounter this weird error, I keep getting every now and then and I don't know what causes it / how to fix it. I always end up trying alternative approaches until it's gone. Here's a colab [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) with the full code.\r\n\r\nTraining function:\r\n\r\n```\r\n    def train_step(self):\r\n        with tf.GradientTape() as tape:\r\n            (\r\n                _,\r\n                rewards,\r\n                actions,\r\n                value_logits,\r\n                dones,\r\n                _,\r\n                entropies,\r\n                actor_logits,\r\n            ) = tf.numpy_function(\r\n                self.np_train_step, [], [tf.float32 for _ in range(8)]\r\n            )\r\n            action_probs = tf.nn.softmax(actor_logits)\r\n            values = tf.reduce_sum(action_probs * value_logits, axis=-1)\r\n            action_indices = self.get_action_indices(self.batch_indices, actions)\r\n            selected_probs = tf.gather_nd(action_probs, action_indices)\r\n            selected_logits = tf.gather_nd(value_logits, action_indices)\r\n            importance_ratio = action_probs / (action_probs + self.epsilon)\r\n            action_importance = tf.gather_nd(importance_ratio, action_indices)\r\n            returns = tf.numpy_function(\r\n                self.calculate_returns,\r\n                [rewards, values, dones, selected_logits, action_importance],\r\n                tf.float32,\r\n            )\r\n            loss = self.compute_loss(\r\n                returns,\r\n                values,\r\n                entropies,\r\n                action_importance,\r\n                value_logits,\r\n                importance_ratio,\r\n                action_probs,\r\n                selected_probs,\r\n                selected_logits,\r\n            )\r\n        grads = tape.gradient(loss, self.model.trainable_variables)\r\n        if self.grad_norm is not None:\r\n            grads, _ = tf.clip_by_global_norm(grads, self.grad_norm)\r\n        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n```\r\nResults in:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-65446c8a838c> in <module>()\r\n      7 o = MovingAverage(Adam(7e-4))\r\n      8 agn = ACER(env, m, optimizer=o)\r\n----> 9 agn.fit(19)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    975           except Exception as e:  # pylint:disable=broad-except\r\n    976             if hasattr(e, \"ag_error_metadata\"):\r\n--> 977               raise e.ag_error_metadata.to_exception(e)\r\n    978             else:\r\n    979               raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-11-cc50c030ee4d>:151 train_step  *\r\n        self.model.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/optimizers/average_wrapper.py:56 apply_gradients  *\r\n        return super().apply_gradients(grads_and_vars, name, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:598 apply_gradients  **\r\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/utils.py:79 filter_empty_gradients\r\n        ([v.name for _, v in grads_and_vars],))\r\n\r\n    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'dense/kernel:0', 'dense/bias:0'].\r\n```\r\n", "comments": ["I solved the issue by replacing numpy operation with `tf.concat()`"]}, {"number": 47287, "title": " OSError: SavedModel file does not exist at: model_resnet152V2.h5/{saved_model.pbtxt|saved_model.pb}", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 113, in parse_saved_model\r\n    constants.SAVED_MODEL_FILENAME_PB))\r\n\r\nOSError: SavedModel file does not exist at: model_resnet152V2.h5/{saved_model.pbtxt|saved_model.pb}\r\n\r\n### Source code / logs\r\n\r\nfrom __future__ import division, print_function\r\n# coding=utf-8\r\nimport sys\r\nimport os\r\nimport glob\r\nimport re\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.2\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n# Keras\r\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.preprocessing import image\r\n\r\n# Flask utils\r\nfrom flask import Flask, redirect, url_for, request, render_template\r\nfrom werkzeug.utils import secure_filename\r\n#from gevent.pywsgi import WSGIServer\r\n\r\n# Define a flask app\r\napp = Flask(__name__)\r\n\r\n# Model saved with Keras model.save()\r\nMODEL_PATH ='model_resnet152V2.h5'\r\n\r\n# Load your trained model\r\nmodel = load_model(MODEL_PATH)\r\n\r\n\r\n\r\n\r\ndef model_predict(img_path, model):\r\n    print(img_path)\r\n    img = image.load_img(img_path, target_size=(224, 224))\r\n\r\n    # Preprocessing the image\r\n    x = image.img_to_array(img)\r\n    # x = np.true_divide(x, 255)\r\n    ## Scaling\r\n    x=x/255\r\n    x = np.expand_dims(x, axis=0)\r\n   \r\n\r\n    # Be careful how your trained model deals with the input\r\n    # otherwise, it won't make correct prediction!\r\n   # x = preprocess_input(x)\r\n\r\n    preds = model.predict(x)\r\n    preds=np.argmax(preds, axis=1)\r\n    if preds==0:\r\n        preds=\"The leaf is diseased cotton leaf\"\r\n    elif preds==1:\r\n        preds=\"The leaf is diseased cotton plant\"\r\n    elif preds==2:\r\n        preds=\"The leaf is fresh cotton leaf\"\r\n    else:\r\n        preds=\"The leaf is fresh cotton plant\"\r\n        \r\n    \r\n    \r\n    return preds\r\n\r\n\r\n@app.route('/', methods=['GET'])\r\ndef index():\r\n    # Main page\r\n    return render_template('index.html')\r\n\r\n\r\n@app.route('/predict', methods=['GET', 'POST'])\r\ndef upload():\r\n    if request.method == 'POST':\r\n        # Get the file from post request\r\n        f = request.files['file']\r\n\r\n        # Save the file to ./uploads\r\n        basepath = os.path.dirname(__file__)\r\n        file_path = os.path.join(\r\n            basepath, 'uploads', secure_filename(f.filename))\r\n        f.save(file_path)\r\n\r\n        # Make prediction\r\n        preds = model_predict(file_path, model)\r\n        result=preds\r\n        return result\r\n    return None\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(port=5001,debug=True)\r\n\r\n", "comments": ["@Dwarakanadh-k \r\n\r\nI ran the code shared but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/53727784730e4ca6fdb09e7501e91fce/untitled546.ipynb).\r\nFrom the error log i can see that saved_model- face_recognition_model.h5 does not exist, make sure you import the file to the right path and then try loading the model using load_model.\r\n\r\nYou may also refer to similar issues and let us know: #35909, [link](https://stackoverflow.com/questions/62620581/oserror-savedmodel-file-does-not-exist-at-model-mymodel-h5-saved-model-pbtxt)", "@Dwarakanadh-k  I successfully ran some snippets of your code cells. In you code I think the you have not provided the model's path correctly or even the model.. I have created a demo using your code. It might help.\r\n[https://colab.research.google.com/drive/1ansO4-qULpIf_euD5kOMVwpKS4hsCs51?usp=sharing](url)", "> @Dwarakanadh-k I successfully ran some snippets of your code cells. In you code I think the you have not provided the model's path correctly or even the model.. I have created a demo using your code. It might help.\r\n> [https://colab.research.google.com/drive/1ansO4-qULpIf_euD5kOMVwpKS4hsCs51?usp=sharing](url)\r\n\r\nthe given demo  link is not opening ", "> @Dwarakanadh-k\r\n> \r\n> I ran the code shared but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/53727784730e4ca6fdb09e7501e91fce/untitled546.ipynb).\r\n> From the error log i can see that saved_model- face_recognition_model.h5 does not exist, make sure you import the file to the right path and then try loading the model using load_model.\r\n> \r\n> You may also refer to similar issues and let us know: #35909, [link](https://stackoverflow.com/questions/62620581/oserror-savedmodel-file-does-not-exist-at-model-mymodel-h5-saved-model-pbtxt)\r\n\r\nimport flask\r\nfrom flask import Flask \r\nthe code is running after changing the flask error by this \r\nI refered those previously unable to understand the solution", "Yeah I didn't given path in code ,Can u please help me how to add path\n\nOn Mon, 22 Feb, 2021, 2:53 PM Siddharth Sharma, <notifications@github.com>\nwrote:\n\n> @Dwarakanadh-k <https://github.com/Dwarakanadh-k> I successfully ran some\n> snippets of your code cells. In you code I think the you have not provided\n> the model's path correctly or even the model.. I have created a demo using\n> your code. It might help.\n>\n> https://colab.research.google.com/drive/1ansO4-qULpIf_euD5kOMVwpKS4hsCs51?usp=sharing\n> <http://url>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47287#issuecomment-783228531>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN52LQANMLCVOFRJS65KHFTTAIPCRANCNFSM4X6C3HXA>\n> .\n>\n", "Just import the load_model \r\ncopy the path name of your model and paste it in\r\n **path**=\"your model path\"(for example)\r\nload_model(\"Model_Path\")\r\nand you are good to go", "firstly thanks for your response bro\nThe path is added and load model too\n\n# Model saved with Keras model.save()\nMODEL_PATH ='model_resnet152V2.h5'\n\n# Load your trained model\nmodel = load_model(MODEL_PATH)\n\nFrom past 1 week trying to solve the error but unable find the solution\nIf possible check that attached file You may solve the error\nhttps://github.com/krishnaik06/Cotton-Disease-Prediction-Deep-Learning\n\nThanks\n\n\n\n\n\nOn Mon, Feb 22, 2021 at 3:54 PM Siddharth Sharma <notifications@github.com>\nwrote:\n\n> Just import the load_model\n> initialize the model *path*=\"your model path\"(for example)\n> load_model(\"Model_Path\")\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47287#issuecomment-783267416>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN52LQDWDR7NNPPEIAUEEW3TAIWEXANCNFSM4X6C3HXA>\n> .\n>\n", "irstly thanks for your response bro\nThe path is added and load model too\n\n# Model saved with Keras model.save()\nMODEL_PATH ='model_resnet152V2.h5'\n\n# Load your trained model\nmodel = load_model(MODEL_PATH)\n\nFrom past 1 week trying to solve the error but unable find the solution\nIf possible check that attached file You may solve the error\nhttps://github.com/krishnaik06/Cotton-Disease-Prediction-Deep-Learning\n\nThanks\n\n\n\n\n[image: image.gif]\nAttachments area\n\n\n\n\n\nOn Mon, Feb 22, 2021 at 3:54 PM Siddharth Sharma <notifications@github.com>\nwrote:\n\n> Just import the load_model\n> initialize the model *path*=\"your model path\"(for example)\n> load_model(\"Model_Path\")\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47287#issuecomment-783267416>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN52LQDWDR7NNPPEIAUEEW3TAIWEXANCNFSM4X6C3HXA>\n> .\n>\n"]}, {"number": 47286, "title": "The Example Code in tf.feature_column.numeric_column is not Executable/Complete/Self-Sufficient", "body": "Please provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column#example\r\n\r\nDescription of issue (what needs changing):\r\nThe example code cannot be Executed as it is. We have to add the namespaces by searching in the **`tensorflow.org`** site.\r\n\r\nComplete/Self-Sufficient/Stand-Alone example code will be very helpful, especially for the New Developers.", "comments": ["I have created a pull request. Suggestions inputs are welcome @worldpeaceaspirer  and @rmothukuru . Thank you.", "Thanks for the PR. This is now fixed with tf-nightly docs.\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column?version=nightly#example"]}, {"number": 47284, "title": "micro: port operator ELU kernel from lite with test", "body": "Complete implementation of TFLM operator ELU and associated TFLM test code.\r\n\r\nPR step 5 of the work to port operator ELU as tracked in Issue #46323", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47283, "title": "Tensorflow 2.4.1 build failing with MKL enabled", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nUsing the following dockerfile to compile TF:\r\n[Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/6014553/Dockerfile.txt)\r\n\r\n**Describe the problem**\r\nThe build fails when run with `--config=mkl` and succeeds when this option is omitted.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSee attached Dockerfile  (NVIDIA proprietary files are mentioned, but not included)\r\n\r\n**Any other info / logs**\r\n\r\nThe error traceback looks like this:\r\n```\r\n#INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (409 packages loaded, 31937 targets configured).\r\n#INFO: Found 1 target...\r\n#[0 / 293] [Prepa] BazelWorkspaceStatusAction stable-status.txt ... (7 actions, 0 running)\r\n#[80 / 2,141] Executing genrule @local_config_cuda//cuda:cuda-include [for host]; 30s local ... (12 actions, 10 running)\r\n#[122 / 2,157] Executing genrule @local_config_cuda//cuda:cuda-include [for host]; 57s local ... (12 actions, 11 running)\r\n#[183 / 2,208] Executing genrule @local_config_cuda//cuda:cuda-include [for host]; 88s local ... (11 actions, 10 running)\r\n#[1,955 / 2,623] Compiling external/com_google_protobuf/src/google/protobuf/text_format.cc [for host]; 2s local ... (12 actions, 11 running)\r\n#[2,540 / 3,542] Compiling external/llvm-project/llvm/lib/Support/CommandLine.cpp [for host]; 2s local ... (12 actions, 11 running)\r\n#[2,819 / 3,680] Compiling external/llvm-project/llvm/utils/TableGen/GlobalISelEmitter.cpp [for host]; 5s local ... (12 actions, 11 running)\r\n#[3,635 / 7,799] Compiling external/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFVerifier.cpp [for host]; 9s local ... (12 actions running)\r\n#[3,788 / 7,799] Compiling external/llvm-project/llvm/lib/Transforms/Instrumentation/PGOInstrumentation.cpp [for host]; 7s local ... (12 actions, 11 running)\r\n#[3,966 / 7,799] Compiling external/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp [for host]; 20s local ... (12 actions, 11 running)\r\n#[4,129 / 7,799] Compiling external/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp [for host]; 6s local ... (12 actions, 11 running)\r\n#[4,363 / 8,003] Compiling external/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp [for host]; 10s local ... (12 actions running)\r\n#[5,419 / 9,077] Compiling external/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp [for host]; 25s local ... (12 actions running)\r\n#[6,657 / 9,123] Compiling external/llvm-project/mlir/lib/Dialect/SPIRV/SPIRVOps.cpp [for host]; 27s local ... (12 actions running)\r\n#[7,199 / 9,160] Compiling external/mkl_dnn_v1/src/cpu/cpu_reorder.cpp [for host]; 55s local ... (12 actions running)\r\n#[7,481 / 9,226] Compiling external/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp [for host]; 25s local ... (12 actions running)\r\n#[7,991 / 9,748] Compiling external/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp [for host]; 5s local ... (12 actions running)\r\n#[9,334 / 11,861] Compiling external/mkl_dnn_v1/src/cpu/cpu_reorder.cpp [for host]; 71s local ... (12 actions running)\r\n#[11,414 / 16,245] Compiling external/llvm-project/llvm/lib/Passes/PassBuilder.cpp [for host]; 58s local ... (12 actions, 11 running)\r\n#ERROR: /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/BUILD.bazel:25:1: Executing genrule @llvm_openmp//:kmp_i18n_default failed (Exit 2): bash failed: error executing command\r\n#  (cd /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/execroot/org_tensorflow && \\\r\n#  exec env - \\\r\n#    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-11.1/lib64 \\\r\n#    PATH=/opt/rh/devtoolset-7/root/usr/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda-11.1/bin \\\r\n#  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; perl external/llvm_openmp/runtime/tools/message-converter.pl --os=lin --prefix=kmp_i18n --default=bazel-out/host/bin/external/llvm_openmp/include/kmp_i18n_default.inc external/llvm_openmp/runtime/src/i18n/en_US.txt')\r\n#Execution platform: @local_execution_config_platform//:platform\r\n#Can't locate Data/Dumper.pm in @INC (@INC contains: /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib/tools.pm line 80.\r\n#BEGIN failed--compilation aborted at /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib/tools.pm line 80.\r\n#Compilation failed in require at external/llvm_openmp/runtime/tools/message-converter.pl line 22.\r\n#BEGIN failed--compilation aborted at external/llvm_openmp/runtime/tools/message-converter.pl line 22.\r\n#Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n#ERROR: /native/tensorflow/python/tools/BUILD:143:1 Executing genrule @llvm_openmp//:kmp_i18n_default failed (Exit 2): bash failed: error executing command\r\n#  (cd /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/execroot/org_tensorflow && \\\r\n#  exec env - \\\r\n#    LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-11.1/lib64 \\\r\n#    PATH=/opt/rh/devtoolset-7/root/usr/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/cuda-11.1/bin \\\r\n#  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; perl external/llvm_openmp/runtime/tools/message-converter.pl --os=lin --prefix=kmp_i18n --default=bazel-out/host/bin/external/llvm_openmp/include/kmp_i18n_default.inc external/llvm_openmp/runtime/src/i18n/en_US.txt')\r\n#Execution platform: @local_execution_config_platform//:platform\r\n#INFO: Elapsed time: 2046.582s, Critical Path: 116.95s\r\n#INFO: 7156 processes: 7156 local.\r\n#FAILED: Build did NOT complete successfully\r\n#FAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n", "comments": ["The builds are still failing. Will anyone look at this issue?", "```\r\n#Can't locate Data/Dumper.pm in @INC (@INC contains: /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at /root/.cache/bazel/_bazel_root/ce6b201fa2cce1ac8c0a389f89f76729/external/llvm_openmp/runtime/tools/lib/tools.pm line 80.\r\n```\r\nYou'll need to install that Perl module on your system for it to work.", "Ah, true. I thought that was a missing dependency in MKL itself, not a dependency of the build system.\r\n\r\nNow it builds correctly  +1", "@Atharex,\r\nCan you please confirm if we can close this issue as it has been resolved? Thanks! ", "@rmothukuru The build works for me, though would be nice if this Perl dependency was also mentioned somewhere as a MKL dependency. Besides that, sure go ahead and close the issue!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47283\">No</a>\n"]}, {"number": 47282, "title": "whether tensorflow2.0 suppert bert pretraining?", "body": "Question: Where tensorflow2.0 suppert bert pretraining?\r\n    Version:  tensorflow2.4\r\n   Base on my data, I want to train new pretrain-model with bert. However, When I run run_pretraining.py file, ad error occurred.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"run_pretraining.py\", line 493, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/usr/local/python3/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/python3/lib/python3.8/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/python3/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"run_pretraining.py\", line 429, in main\r\n    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'_**\r\n\r\n\r\nSo, I don't know how to deal with this problem and whether tensorflow2.0+ support bert pretraining again with my data.\r\nFinally, hope someone can give me answer and I will appreciate you, thanks!\r\n\r\n\r\n", "comments": ["@ZYG151541155 \r\n\r\nPlease, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nCan you please share complete code snippet to reproduce the issue.\r\n\r\ncontrib module is no longer available in TensorFlow 2.0, but you can use with this;\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nThanks!", "> @ZYG151541155\r\n> \r\n> Please, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> Can you please share complete code snippet to reproduce the issue.\r\n> \r\n> contrib module is no longer available in TensorFlow 2.0, but you can use with this;\r\n> import tensorflow.compat.v1 as tf\r\n> tf.disable_v2_behavior()\r\n> \r\n> Thanks!\r\n\r\nThanks your replied! However, I want to uset bert pre-trained model to train a new pretrain model with my domain data and fine-tuning with my train data! The bugs show like that:   \r\n\r\n[@localhost bert]$ python3 run_pretraining.py \r\n2021-02-20 14:36:40.090529: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n/usr/local/python3/lib/python3.8/site-packages/absl/flags/_validators.py:353: UserWarning: Flag --input_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\r\n  warnings.warn(\r\n/usr/local/python3/lib/python3.8/site-packages/absl/flags/_validators.py:353: UserWarning: Flag --bert_config_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\r\n  warnings.warn(\r\n/usr/local/python3/lib/python3.8/site-packages/absl/flags/_validators.py:353: UserWarning: Flag --output_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\r\n  warnings.warn(\r\nINFO:tensorflow:*** Input Files ***\r\nI0220 14:36:40.888833 140508959438656 run_pretraining.py:420] *** Input Files ***\r\nINFO:tensorflow:  /mnt/data/bert_2.0/water/outdir_pretrain/tf_examples.tfrecord *\r\nI0220 14:36:40.889002 140508959438656 run_pretraining.py:422]   /mnt/data/bert_2.0/water/outdir_pretrain/tf_examples.tfrecord *\r\nTraceback (most recent call last):\r\n  File \"run_pretraining.py\", line 493, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/usr/local/python3/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/python3/lib/python3.8/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/python3/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"run_pretraining.py\", line 429, in main\r\n    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'", "@ZYG151541155 \r\n\r\nPlease refer to this [issue](https://stackoverflow.com/questions/55870127/module-tensorflow-has-no-attribute-contrib) and let us know if it helps.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47282\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47282\">No</a>\n"]}, {"number": 47281, "title": "\"classes\" not working on flow_from_dataframe", "body": "The classes parameter works on flow_from_directory but not flow_from_dataframe in TF 2.4.\r\n\r\n```py\r\ntrain_generator = train_datagen.flow_from_dataframe(dataframe=df,\r\n                                                    x_col=\"img_path\",\r\n                                                    y_col=\"class\",\r\n                                                    classes=[\"rika\", \"risa\",\"yui\", \"akane\",\"neru\"],\r\n                                                    target_size=(200, 200),\r\n                                                    batch_size=32,\r\n                                                    class_mode='categorical', shuffle=False)\r\n```\r\n\r\n![](https://i.imgur.com/TyrbHcW.png)\r\n\r\nThe `train_generator.class_indices` output are not correspond my  classes lists.\r\n\r\nPlease correct me if I am wrong. Thanks!", "comments": ["The classes correspond to the directory names in alphabetical order. Therefore you see that order.", "@ymodak Thanks for your reply. I know it will alphabetical order by default. Is there any way to specify the order?\r\n\r\n", "@andy6804tw,\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Thanks!", "@amahendrakar  Sure! \r\nI take the `cnn-who-is-she` dataset for [example](https://www.kaggle.com/scott1205/cnn-who-is-she) from Kaggel.  \r\nHere is my kernel I shared. https://www.kaggle.com/andy6804tw/47281-issue.\r\n\r\nThank you!", "@ymodak,\r\nWas able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3954ada50bc6bfd98ffbc4257688a84c/47281.ipynb). Thanks!", "This was fixed earlier in `keras-team/keras` with https://github.com/keras-team/keras/issues/13637.\r\nPerhaps TF implementation is not updated.", "@ymodak @amahendrakar Thanks for your reply. I want to know when this bug will be updated on tensorflow?", "I will update this thread when the fix is in hopefully by end of this week. Thanks!\r\n", "@ymodak was wondering if you happened to implement this fix? or a workaround? Was just scratching my head for a few hours not realizing it was ignoring the `classes` variable I was specifying.\r\n\r\nThanks!", "It is working as per the alphabetical order of classes and the mapping of classes is working as expected, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/fee73604a73e58247cf244fb43fb8ae1/47281.ipynb). Thanks! ", "Hi! @sachinprasadhs It's great hearing from you. After my test, I still found the problem.\r\nplease find the gist [here](https://colab.research.google.com/drive/1Fay5Pmzi1DgBz3gK8PEH5UNt1ExEXESf). Thanks!", "It is ordering the classes based on alphabetical order, however if you still think it is not the way it is supposed to work please post this issue on keras-team/keras repo.\r\nDevelopment of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sachinprasadhs Thank you very much! I'll open new issue on keras-team/keras repo.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47281\">No</a>\n", "@andy6804tw, I already see the issue has been moved to keras repo by us, please track the issue here https://github.com/keras-team/keras/issues/15206"]}, {"number": 47279, "title": "[INTEL MKL] Refactoring code related to compiling oneDNN with threadpool.", "body": "Refactoring code to replace the macro ENABLE_MKLDNN_THREADPOOL with ENABLE_ONEDNN_OPENMP.  Now, the oneDNN based kernels will be compiled to use Eigen threadpool by default unless ENABLE_ONEDNN_OPENMP is defined. PR also includes some minor cleanup to remove unused mkl_opensource build option.", "comments": ["> Thank you and have a great weekend! :)\r\n\r\n@penpornk thanks.  There was a conflict I resolved just after you approved. Not sure if the changes registered before the testing started.", "@penpornk thanks for approving again.", "Hi @penpornk The internal Google checks failed for this PR, can you provide the logs for the failures?", "@agramesh1 Thank you for the heads up! I'll fix this internally."]}, {"number": 47278, "title": "Why is there no model parallelism api\uff1f", "body": "\r\nWhy is there no model parallelism api\uff1f\r\n\r\nAll the strategy are data parallelism. If I want to use model parallelism, do I have to use tensorflow1. X\uff1f\r\n\r\nThat is what we need now.", "comments": ["This is marked as feature request with [37563](https://github.com/tensorflow/tensorflow/issues/37563).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47276, "title": "Simple reproduction of issue decribed in #46937", "body": "Simple reproduction of issue decribed in #46937\r\n\r\nExpected output (i.e. what I get on x86):\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile run_test_renode -j8\r\n```\r\ngives:\r\n```\r\na: 45\r\ninit_to_false: 0\r\nWas initialized to false\r\ninit_to_false: 1\r\ninit_to_true: 1\r\ninit_to_nullptr: 0\r\n```\r\n\r\nWhat I get with Renode + bluepill:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_renode -j8\r\ntensorflow/lite/micro/tools/make/downloads/renode/renode\r\n```\r\nand in the renode terminal:\r\n```\r\nClear; include @tensorflow/lite/micro/testing/bluepill_nontest.resc; sysbus LoadELF @tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3_default/bin/test_renode; start\r\n```\r\n\r\ngives:\r\n```\r\na: 45\r\ninit_to_false: 239\r\nWas not initialized to false\r\ninit_to_false: 1\r\ninit_to_true: 1\r\ninit_to_nullptr: -559038737\r\n```\r\n\r\nNeed to debug further, but creating a PR in case @PiotrZierhoffer has any ideas.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "(moved the comment to the related issue https://github.com/tensorflow/tensorflow/issues/46937#issuecomment-784994556 )", "For the record, I assume it will behave the same way on hardware, so it's not really Renode-related", "Closing this PR since we know the root-cause and it is going to be fixed with #47382"]}, {"number": 47274, "title": "tf.convert_to_tensor crashes(segfault) ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.convert_to_tensor` crashes(segfault) when integer `dtype` passed\r\n\r\n**Describe the expected behavior**\r\nExpect graceful exception messages instead of crash\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.convert_to_tensor(value=np.array((10)), dtype=20)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\nSegmentation fault (core dumped)\r\n~~~\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.1, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/a7eed77cae1bf42f7395590f6020a2f2/untitled677.ipynb). Thanks!", "@DNXie\r\n`Error  line : tf.convert_to_tensor(value=np.array((10)), dtype=20)`\r\nThe error is in the `dtype` argument of `convert_to_tensor` which expects specific codes instead of numeric integers, if you want to convert your array into integer tensor you shall pass `dtype =tf.int32` for 32bit integers or `dtype  = tf.int64` for 64 bit integers instead of `20`. Also, if you want to convert it into float you may try `dtype = tf.float32 or tf.float64` . You may look code snippet\r\n![image](https://user-images.githubusercontent.com/58474875/108689057-a2755800-751e-11eb-98ac-496e10a85460.png)\r\n", "Was able to reproduce the issue with TF 2.6.0-dev20210526.Please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/58ff320751a93200f6a4e91b351e3ba6/untitled149.ipynb?authuser=1)..Thanks !", "With TF 2.5 and above the crash is avoided by raising error with appropriate message.\r\n```python\r\nValueError: Failed to convert a NumPy array to a Tensor (Unexpected numpy data type).\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47274\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47274\">No</a>\n"]}, {"number": 47273, "title": "Rename models directory to examples", "body": "Moving files under the models/ directory to live under the examples/ directory as part of the tab merge effort\r\n\r\n", "comments": ["@anirudh161 Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@anirudh161 Can you please resolve conflicts? Thanks!", "The changes in this PR have already been made on another CL. Closing this PR. "]}, {"number": 47272, "title": "Note string optimizer name is case-insensitive", "body": "A minor documentation addition. For clarity, it can be noted that the optimizer string can be put in any casing in `model.compile`, i.e. case-insensitive. e.g.\r\n\r\n```python\r\nmodel.compile(optimizer='RMSprop')\r\nmodel.compile(optimizer='rmsprop')\r\nmodel.compile(optimizer='RMSPROP')\r\n```\r\n\r\n\u2192 Are all the same. It makes sense to tell the user they do not have to mind the casing.\r\n\r\nThis is because whatever the user inputs as an optimizer string is converted to a lowercase string in [optimizers.py#L81](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/optimizers.py#L81); and then compared to other lower-cased optimizer strings.\r\n\r\nHave a great day still \u2600\ufe0f", "comments": ["Good catch and thanks for the PR! \r\n\r\nIn fact this case thing applies not only to optimizer, but also loss and metrics. We decided that this won't affect users in a significant way, so we will close the PR. Thanks again!"]}, {"number": 47269, "title": "Minimal example that exercises the tf.function(experimental_implements) functionality?", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package built from source\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tag v2.4.1\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function(experimental_implements='custom_op')\r\ndef custom_op(x, y):\r\n  return tf.matmul(x, y)\r\n\r\n@tf.function\r\ndef func_2(x):\r\n  return tf.exp(x)\r\n\r\n@tf.function\r\ndef model(x, y):\r\n  return func_2(custom_op(x, y))\r\n\r\nx = tf.Variable([[1.0, 2.0], [3.0,4.0]])\r\ny = tf.Variable([[1.0, 2.0], [3.0,4.0]])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([custom_op.get_concrete_function(x,y)])\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nAccording to documentation [here](https://www.tensorflow.org/lite/convert/operation_fusion), decorating the _custom_op_ function with _tf.function(experimental_implements='custom_op')_ should call _PrepareCompositeFunctionsPass::ConvertTFImplements_ function inside _tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc_ so that one can write customer conversion code. \r\n\r\nSo far I have been unable to achieve that, this function never gets called (easily checked by adding a exit(-1); at the beginning of the function. The code snippet in the aforementioned documentation does not compile because it relies on unmentioned other code, so it can't be used as an example either.\r\n", "comments": ["can you trying using defun like this one? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py#L5321-L5322", "Sorry for the long delay, but I tried the following adjusted example:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.eager import def_function\r\n\r\n@def_function.function(experimental_implements='custom_op')\r\ndef custom_op(x, y):\r\n  return tf.matmul(x, y)\r\n\r\n@def_function.function(experimental_implements='custom_op2')\r\ndef func_2(x):\r\n  return tf.exp(x)\r\n\r\n@def_function.function(experimental_implements='custom_op3')\r\ndef model(x, y):\r\n  return func_2(custom_op(x, y))\r\n\r\nx = tf.Variable([[1.0, 2.0], [3.0,4.0]])\r\ny = tf.Variable([[1.0, 2.0], [3.0,4.0]])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([custom_op.get_concrete_function(x,y)])\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n  \r\nagainst the current HEAD (2.5.0) and it still does not exercise` PrepareCompositeFunctionsPass::ConvertTFImplements` in `prepare_composite_functions_tf.cc` (tested by adding a simple exit(-1) into said function).\r\nCan someone confirm a) this example *should* work in the first place and b) somebody is able to make it work on their side?\r\n\r\nThanks", "One more follow-up comment:\r\nFor git SHA acbc065f8eb2ed05c7ab5c42b5c5bd6abdd2f91f, \r\nadding a \r\n\r\n`exit(-1);`\r\n\r\nat line 243 in tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc does **NOT** break any of the bazel tests from what I can tell (I ran` //tensorflow/lite:all` and `//tensorflow/python:all`). It does not look this code path ever gets executed.", "Cross-refencing PR https://github.com/tensorflow/tensorflow/pull/48856, which I believe has a fix for the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47269\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47269\">No</a>\n"]}, {"number": 47268, "title": "BoostedTreeClassifier does not support EmbeddingColumns : EmbeddingColumns has no attribute 'dtype'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary (pip install)\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nIncluding an EmbeddingColumns feature in the data results in the following error\r\n\r\n`AttributeError: 'EmbeddingColumn' object has no attribute 'dtype'`\r\n\r\n**Describe the expected behavior**\r\nDocumentation of BoostedTreesClassifier states that the `feature_columns` argument \"should be instances of classes derived from **FeatureColumn**\" EmbeddingColumns looks to be derived from **FeatureColumns** and so should result in no errors.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\ndftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\r\n\r\ny_train = dftrain.pop('survived')\r\n\r\nCATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\r\n                       'alone']\r\nNUMERIC_COLUMNS = ['age', 'fare']\r\n\r\ndef one_hot_cat_column(feature_name, vocab):\r\n  return tf.feature_column.indicator_column(\r\n      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\r\n                                                 vocab))\r\nfeature_columns = []\r\nfor feature_name in CATEGORICAL_COLUMNS:\r\n  vocabulary = dftrain[feature_name].unique()\r\n  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\r\n\r\nfor feature_name in NUMERIC_COLUMNS:\r\n  feature_columns.append(tf.feature_column.numeric_column(feature_name, \r\n                                                          dtype=tf.float32))\r\n\r\nembark = tf.feature_column.categorical_column_with_vocabulary_list('embark_town', dftrain['embark_town'].unique())\r\nfeature_columns.append(tf.feature_column.embedding_column(embark, dimension=5))\r\n                       \r\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\r\n                                         n_batches_per_layer=1)\r\n\r\nNUM_EXAMPLES = len(y_train)\r\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n    def input_fn():\r\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\r\n        if shuffle:\r\n            dataset = dataset.shuffle(NUM_EXAMPLES)\r\n        dataset = dataset.repeat(n_epochs)\r\n        dataset = dataset.batch(NUM_EXAMPLES)\r\n        return dataset\r\n    return input_fn\r\n\r\ntrain_input_fn = make_input_fn(dftrain, y_train)\r\n\r\nest.train(train_input_fn)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-25-67c937169652> in <module>\r\n     44 train_input_fn = make_input_fn(dftrain, y_train)\r\n     45 \r\n---> 46 est.train(train_input_fn)\r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    347 \r\n    348       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 349       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    350       logging.info('Loss for final step: %s.', loss)\r\n    351       return self\r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1173       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1174     else:\r\n-> 1175       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1176 \r\n   1177   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1201           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\r\n   1202       worker_hooks.extend(input_hooks)\r\n-> 1203       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\r\n   1204                                            self.config)\r\n   1205       global_step_tensor = tf.compat.v1.train.get_global_step(g)\r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1161 \r\n   1162     logging.info('Calling model_fn.')\r\n-> 1163     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1164     logging.info('Done calling model_fn.')\r\n   1165 \r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _model_fn(features, labels, mode, config)\r\n   2079 \r\n   2080     def _model_fn(features, labels, mode, config):\r\n-> 2081       return _bt_model_fn(\r\n   2082           features,\r\n   2083           labels,\r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _bt_model_fn(features, labels, mode, head, feature_columns, tree_hparams, n_batches_per_layer, config, closed_form_grad_and_hess_fn, example_id_column_name, weight_column, train_in_memory, name)\r\n   1147   logits_dimension = head.logits_dimension\r\n   1148   sorted_feature_columns = sorted(feature_columns, key=lambda tc: tc.name)\r\n-> 1149   float_columns = _get_float_feature_columns(sorted_feature_columns)\r\n   1150 \r\n   1151   with ops.name_scope(name) as name:\r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _get_float_feature_columns(sorted_feature_columns)\r\n    111   float_columns = []\r\n    112   for feature_column in sorted_feature_columns:\r\n--> 113     if _is_numeric_column(feature_column):\r\n    114       float_columns.append(feature_column)\r\n    115   return float_columns\r\n\r\n~/.pyenv/versions/3.8.7/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py in _is_numeric_column(feature_column)\r\n     77                 (feature_column_lib.DenseColumn, fc_old._DenseColumn)):\r\n     78     # NOTE: GBDT requires that all DenseColumns expose a dtype attribute\r\n---> 79     return feature_column.dtype.is_floating\r\n     80   else:\r\n     81     raise ValueError('Encountered unexpected column {}'.format(feature_column))\r\n\r\nAttributeError: 'EmbeddingColumn' object has no attribute 'dtype'\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/be336213a9bd8d00002ab559e26ea460/47268.ipynb). Thanks!", "Was able to reproduce the issue with TF v2.5.Please find the gist of it [here](https://colab.research.google.com/gist/sushreebarsa/36bbb61a8f440aa3bbf7944ca2b829f1/untitled150.ipynb?authuser=1)..Thanks !", "These are deprecated and will be removed in TF 2.9.\r\n\r\nThere is no owner for the code, which is why this PR stalled for so long. Apologies.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47268\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47268\">No</a>\n"]}, {"number": 47267, "title": "Non-deterministic graph when custom_gradient has watched variables", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7.8.2003\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.custom_gradient()` keeps track of variables that are not part of the inputs and returns gradients for them. When there are multiple such variables, the created graph (in non-eager mode) is non-deterministic. The reason is that watched variable references are stored in a frozenset and since these references are essentially ids that have different ordering from one python invocation to another, the generated graph is different across different runs.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe generated graph in the aforementioned case should be deterministic across different runs.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nRun the python script multiple times and diff the generated graphdefs. They are going to be different.\r\n\r\n```bash\r\n$ python run.py 1  # creates tf_graph.pbtxt.1\r\n$ python run.py 2  # creates tf_graph.pbtxt.2\r\n$ python run.py 3  # creates tf_graph.pbtxt.3\r\n```\r\n\r\n```python\r\n# run.py\r\nimport sys\r\nimport tensorflow as tf\r\nimport tensorflow.compat.v1 as v1\r\n\r\n\r\ng = v1.Graph()\r\nwith g.as_default():\r\n    # Create a bunch of variables that are captured in custom_gradient\r\n    captured = [tf.Variable(float(i)) for i in range(1, 20)]\r\n\r\n    @tf.custom_gradient\r\n    def FuncMult(x):\r\n        def GradMult(*dys, variables=None):\r\n            return (\r\n                4. * sum(captured) * dys[0],\r\n                [(i + 1) * x * y for i in range(len(variables))]\r\n            )\r\n\r\n        return x * sum(captured), GradMult\r\n\r\n    x = tf.Variable(6.)\r\n    y = FuncMult(x)\r\n    grad = tf.gradients(y, [x])\r\n\r\ngraph_def = g.as_graph_def(add_shapes=True)\r\nwith open(f\"tf_graph.pbtxt.{sys.argv[1]}\", \"w\") as f:\r\n    f.write(str(graph_def))\r\n```\r\n\r\n**Fix**\r\n\r\nPlease see #47266 for a potential fix of this issue.\r\n\r\n", "comments": ["I ran the code shared on tf nightly and 2.4 and do not see any output, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4f94c2ba8c17905d74915d43fcdae8a0/untitled547.ipynb).", "The python script writes the output to a file. You need to run the script multiple times (so that python's hash seed changes and ordering of strings in a forzenset change as well) and compare the generated files.", "Fix PR #47266 was merged to master. This issue should now be resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47267\">No</a>\n"]}, {"number": 47266, "title": "Sort variables by name in custom gradient for determinism", "body": "This PR is the fix for issue https://github.com/tensorflow/tensorflow/issues/47267.\r\n\r\n`tf.custom_gradient()` keeps track of variables that are not part of the inputs and returns gradients for them. When there are multiple such variables, the created graph (in non-eager mode) is non-deterministic. The reason is that watched variable references are stored in a frozenset and since these references are essentially ids that have different ordering from one python invocation to another, the generated graph is different across different runs.\r\n\r\nThis change deterministically orders the watched variables by their name to avoid that issue.", "comments": []}, {"number": 47265, "title": "Successive prediction (loop) in keras model generate NaN values", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 on google cloud\r\n- TensorFlow installed from (source or binary): conda installation\r\n- TensorFlow version (use command below):2.4.0/2.4.1\r\n- Python version: Python 3.6.12\r\n- Bazel version (if compiling from source):0.15.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.0\r\n- GPU model and memory: K80 on google cloud\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nRunning successive prediction generates NAN values from the second iteration\r\n**Describe the expected behavior**\r\nNo NaN values\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom typing import Tuple, Optional, List\r\n\r\nimport cv2\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.models import Model\r\nfrom numpy import ndarray\r\nfrom tensorflow import Tensor\r\n\r\n\r\ndef darknet53(input_data: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\r\n    \"\"\"\r\n    :darknet53: darknet53 model\r\n\r\n    :param input_data: input data tensorflow\r\n    :type input_data: Tensor\r\n    :return:\r\n    darknet53 outputs\r\n    \"\"\"\r\n\r\n    input_data = convolutional(input_data, (3, 3, 3, 32))\r\n    input_data = convolutional(input_data, (3, 3, 32, 64),\r\n                                      downsample=True)\r\n\r\n    for i in range(1):\r\n        input_data = residual_block(input_data, 64, 32, 64)\r\n\r\n    input_data = convolutional(input_data, (3, 3, 64, 128),\r\n                                      downsample=True)\r\n\r\n    for i in range(2):\r\n        input_data = residual_block(input_data, 128, 64, 128)\r\n\r\n    input_data = convolutional(input_data, (3, 3, 128, 256),\r\n                                      downsample=True)\r\n\r\n    for i in range(8):\r\n        input_data = residual_block(input_data, 256, 128, 256)\r\n\r\n    route_1 = input_data\r\n    input_data = convolutional(input_data, (3, 3, 256, 512),\r\n                                      downsample=True)\r\n\r\n    for i in range(8):\r\n        input_data = residual_block(input_data, 512, 256, 512)\r\n\r\n    route_2 = input_data\r\n    input_data = convolutional(input_data, (3, 3, 512, 1024),\r\n                                      downsample=True)\r\n\r\n    for i in range(4):\r\n        input_data = residual_block(input_data, 1024, 512, 1024)\r\n\r\n    return route_1, route_2, input_data\r\n\r\n\r\nclass BatchNormalization(tf.keras.layers.BatchNormalization):\r\n    \"\"\"\r\n    \"Frozen state\" and \"inference mode\" are two separate concepts.\r\n    `layer.trainable = False` is to freeze the layer, so the layer will use\r\n    stored moving `var` and `mean` in the \"inference mode\", and both `gama`\r\n    and `beta` will not be updated !\r\n    \"\"\"\r\n\r\n    def call(self, x, training=False):\r\n        if not training:\r\n            training = tf.constant(False)\r\n        training = tf.logical_and(training, self.trainable)\r\n        return super().call(x, training)\r\n\r\n\r\ndef convolutional(input_layer: Tensor, filters_shape: Tuple,\r\n                  downsample: bool = False,\r\n                  activate: bool = True,\r\n                  bn: bool = True):\r\n    \"\"\"\r\n    :convolutional: custom convolution layer\r\n\r\n    :param input_layer: input layer\r\n    :type input_layer: Input\r\n\r\n    :param filters_shape: filters shape\r\n    :type filters_shape: tuple\r\n\r\n    :param downsample: downsample flag\r\n    :type downsample: bool\r\n\r\n    :param activate: leaky relu\r\n    :type activate: bbol\r\n\r\n    :param bn: batchnorm flag\r\n    :type bn: bool\r\n\r\n    :return:\r\n    convolution tensor\r\n    \"\"\"\r\n    if downsample:\r\n        input_layer = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(\r\n            input_layer)\r\n        padding = 'valid'\r\n        strides = 2\r\n    else:\r\n        strides = 1\r\n        padding = 'same'\r\n\r\n    conv = tf.keras.layers.Conv2D(\r\n        filters=filters_shape[-1],\r\n        kernel_size=filters_shape[0],\r\n        strides=strides, padding=padding,\r\n        use_bias=not bn,\r\n        kernel_regularizer=tf.keras.regularizers.l2(0.0005),\r\n        kernel_initializer=tf.random_normal_initializer(stddev=0.01),\r\n        bias_initializer=tf.constant_initializer(0.))(input_layer)\r\n\r\n    if bn: conv = BatchNormalization()(conv)\r\n    if activate == True: conv = tf.nn.leaky_relu(conv, alpha=0.1)\r\n\r\n    return conv\r\n\r\n\r\ndef residual_block(input_layer: Tensor, input_channel: Tensor,\r\n                   filter_num1, filter_num2) -> Tensor:\r\n    \"\"\"\r\n    :residual_block: residual block function\r\n    :param input_layer: input tensor\r\n    :type input_layer: Tensor\r\n\r\n    :param input_channel: input tensor\r\n    :type input_channel: Tensor\r\n\r\n    :param filter_num1: size of input channels\r\n    :type filter_num1: int\r\n\r\n    :param filter_num2: size of output channels\r\n    :type filter_num2: int\r\n    :return:\r\n    output of residual block\r\n    \"\"\"\r\n    short_cut = input_layer\r\n    conv = convolutional(input_layer,\r\n                         filters_shape=(1, 1, input_channel, filter_num1))\r\n    conv = convolutional(conv, filters_shape=(3, 3, filter_num1, filter_num2))\r\n\r\n    residual_output = short_cut + conv\r\n    return residual_output\r\n\r\n\r\ndef upsample(input_layer: Tensor):\r\n    \"\"\"\r\n    :upsample: upsample function\r\n    :param input_layer: input layer tensor\r\n    :type input_layer: Tensor\r\n    :return:\r\n    resized tensor\r\n    \"\"\"\r\n    return tf.image.resize(input_layer, (\r\n        input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')\r\n\r\n\r\ndef image_preporcess(image: ndarray, target_size: int,\r\n                     gt_boxes: Optional[ndarray] = None) -> Tensor:\r\n    \"\"\"\r\n    :image_preprocess: scale with padding the image to a target size\r\n    :param image: RGB image\r\n    :type image: ndarray\r\n    :param target_size: image target size\r\n    :type target_size: int\r\n    :param gt_boxes: ndarray\r\n    :return:\r\n    \"\"\"\r\n    ih, iw = target_size\r\n    h, w, _ = image.shape\r\n\r\n    scale = min(iw / w, ih / h)\r\n    nw, nh = int(scale * w), int(scale * h)\r\n    image_resized = cv2.resize(image, (nw, nh))\r\n\r\n    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\r\n    dw, dh = (iw - nw) // 2, (ih - nh) // 2\r\n    image_paded[dh:nh + dh, dw:nw + dw, :] = image_resized\r\n    image_paded = image_paded / 255.\r\n\r\n    if gt_boxes is None:\r\n        return image_paded\r\n\r\n    else:\r\n        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\r\n        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\r\n        return image_paded, gt_boxes\r\n\r\n\r\ndef bboxes_iou(boxes1, boxes2) -> float:\r\n    \"\"\"\r\n    :bboxes_iou: intersection over union between two bboxes.\r\n\r\n    :param boxes1: bbox\r\n    :type boxes1: ndarray\r\n    :param boxes2:bbox\r\n    :type boxes2: ndarray\r\n\r\n    :return:\r\n    intersection over union score\r\n    \"\"\"\r\n    boxes1 = np.array(boxes1)\r\n    boxes2 = np.array(boxes2)\r\n\r\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (\r\n            boxes1[..., 3] - boxes1[..., 1])\r\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (\r\n            boxes2[..., 3] - boxes2[..., 1])\r\n\r\n    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\r\n    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\r\n\r\n    inter_section = np.maximum(right_down - left_up, 0.0)\r\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\r\n    union_area = boxes1_area + boxes2_area - inter_area\r\n    ious = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\r\n\r\n    return ious\r\n\r\n\r\ndef nms(bboxes: list, iou_threshold: float,\r\n        sigma: float = 0.3, method: str = 'nms'):\r\n    \"\"\"\r\n    :nms: non-maximum suppression.\r\n\r\n    :param bboxes: bbox coordinates.\r\n                   (xmin, ymin, xmax, ymax, score, class)\r\n    :type bboxes: list\r\n\r\n    :param iou_threshold: threshold to remove duplicated bbox\r\n    :type iou_threshold: float\r\n\r\n    :param sigma: sigma value for softnms\r\n    :param method: nms or soft-nms\r\n    :return:\r\n    ndarray of best bbox\r\n\r\n    :Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\r\n          https://github.com/bharatsingh430/soft-nms\r\n    \"\"\"\r\n    classes_in_img = list(set(bboxes[:, 5]))\r\n    best_bboxes = []\r\n\r\n    for cls in classes_in_img:\r\n        cls_mask = (bboxes[:, 5] == cls)\r\n        cls_bboxes = bboxes[cls_mask]\r\n\r\n        while len(cls_bboxes) > 0:\r\n            max_ind = np.argmax(cls_bboxes[:, 4])\r\n            best_bbox = cls_bboxes[max_ind]\r\n            best_bboxes.append(best_bbox)\r\n            cls_bboxes = np.concatenate(\r\n                [cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\r\n            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\r\n            weight = np.ones((len(iou),), dtype=np.float32)\r\n\r\n            assert method in ['nms', 'soft-nms']\r\n\r\n            if method == 'nms':\r\n                iou_mask = iou > iou_threshold\r\n                weight[iou_mask] = 0.0\r\n\r\n            if method == 'soft-nms':\r\n                weight = np.exp(-(1.0 * iou ** 2 / sigma))\r\n\r\n            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\r\n            score_mask = cls_bboxes[:, 4] > 0.\r\n            cls_bboxes = cls_bboxes[score_mask]\r\n\r\n    return best_bboxes\r\n\r\n\r\ndef postprocess_boxes(pred_bbox: ndarray,\r\n                      org_img_shape: int,\r\n                      input_size: int,\r\n                      score_threshold: float) -> ndarray:\r\n    \"\"\"\r\n    :postprocess_boxes: post precessing the bboxes.\r\n    :param pred_bbox: bboxes\r\n    :type pred_bbox: ndarray\r\n\r\n    :param org_img_shape: original image shape\r\n    :type org_img_shape: int\r\n\r\n    :param input_size: image input size\r\n    :type input_size: int\r\n\r\n    :param score_threshold: threshold to discard some invalid bboxes\r\n    :type score_threshold: float\r\n    :return:\r\n    \"\"\"\r\n    valid_scale = [0, np.inf]\r\n    pred_bbox = np.array(pred_bbox)\r\n\r\n    pred_xywh = pred_bbox[:, 0:4]\r\n    pred_conf = pred_bbox[:, 4]\r\n    pred_prob = pred_bbox[:, 5:]\r\n\r\n    # # (1) (x, y, w, h) --> (xmin, ymin, xmax, ymax)\r\n    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\r\n                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5],\r\n                               axis=-1)\r\n    # # (2) (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\r\n    org_h, org_w = org_img_shape\r\n    resize_ratio = min(input_size / org_w, input_size / org_h)\r\n\r\n    dw = (input_size - resize_ratio * org_w) / 2\r\n    dh = (input_size - resize_ratio * org_h) / 2\r\n\r\n    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\r\n    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\r\n\r\n    # # (3) clip some boxes those are out of range\r\n    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\r\n                                np.minimum(pred_coor[:, 2:],\r\n                                           [org_w - 1, org_h - 1])], axis=-1)\r\n    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]),\r\n                                 (pred_coor[:, 1] > pred_coor[:, 3]))\r\n    pred_coor[invalid_mask] = 0\r\n\r\n    # # (4) discard some invalid boxes\r\n    bboxes_scale = np.sqrt(\r\n        np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\r\n    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale),\r\n                                (bboxes_scale < valid_scale[1]))\r\n\r\n    # # (5) discard some boxes with low scores\r\n    classes = np.argmax(pred_prob, axis=-1)\r\n    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\r\n    score_mask = scores > score_threshold\r\n    mask = np.logical_and(scale_mask, score_mask)\r\n    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\r\n\r\n    return np.concatenate(\r\n        [coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\r\n\r\n\r\ndef get_bbox_coords(arr: ndarray) -> ndarray:\r\n    \"\"\"\r\n    :get_bbox_coords: get bboxes coordinates.\r\n\r\n    :param arr: bbox of polygons arr\r\n    :type arr: ndarray\r\n    :return:\r\n    \"\"\"\r\n    x_min, x_max = min(arr[:, 0]), max(arr[:, 0])\r\n    y_min, y_max = min(arr[:, 1]), max(arr[:, 1])\r\n\r\n    return np.array([[x_min, y_min], [x_max, y_max]])\r\n\r\n\r\ndef get_object(model: Model, input_image: ndarray,\r\n              input_size: int = 416) -> ndarray:\r\n    \"\"\"\r\n    :crop_card: run yolov3 to crop the card region.\r\n    :param model: keras Model\r\n    :type model_path: Model\r\n    :param input_image: RGB image\r\n    :type input_image: ndarray\r\n\r\n    :param input_size:input size\r\n    :return:\r\n    card crop\r\n    \"\"\"\r\n    original_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\r\n    original_image_size = original_image.shape[:2]\r\n    image_data = image_preporcess(np.copy(original_image),\r\n                                  [input_size, input_size])\r\n    image_data = image_data[np.newaxis, ...].astype(np.float32)\r\n\r\n    pred_bbox = model.predict(image_data)\r\n    print(pred_bbox)\r\n    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\r\n    pred_bbox = tf.concat(pred_bbox, axis=0)\r\n    bboxes = postprocess_boxes(pred_bbox, original_image_size,\r\n                               input_size, 0.3)\r\n    # bboxes format bboxes: [x_min, y_min, x_max, y_max, probability, cls_id]\r\n    bboxes = nms(bboxes, 0.1, method='nms')\r\n    # take only the first crop.\r\n    if len(bboxes) != 0:\r\n        bbox_1 = bboxes[0]\r\n        x_min, y_min, x_max, y_max = int(bbox_1[0]), int(bbox_1[1]), int(\r\n            bbox_1[2]), int(bbox_1[3])\r\n        # crop the original image and get the card.\r\n        h, w = original_image.shape[:2]\r\n        crop = original_image[max(0, y_min - 10):min(h, y_max + 10),\r\n               max(0, x_min - 10):min(w, x_max + 10)]\r\n        # save crop\r\n        # cv2.imwrite(\"crop_id_card.png\", crop)\r\n        print('crop done!')\r\n    else:\r\n        crop = None\r\n    return crop\r\n\r\n\r\nNUM_CLASS = 1\r\nANCHORS = np.array(\r\n    [1.25, 1.625, 2.0, 3.75, 4.125, 2.875, 1.875, 3.8125, 3.875, 2.8125,\r\n     3.6875, 7.4375, 3.625, 2.8125, 4.875, 6.1875, 11.65625, 10.1875\r\n     ]).reshape(3, 3, 2)\r\nSTRIDES = np.array([8, 16, 32])\r\nIOU_LOSS_THRESH = 0.5\r\n\r\n\r\ndef YOLOv3(input_layer: Input) -> List:\r\n    \"\"\"\r\n    Run Yolo3 model.\r\n    :param input_layer: input layer of yolo3\r\n    :type input_layer: Tensor\r\n    :return: list of features maps\r\n    \"\"\"\r\n    route_1, route_2, conv = darknet53(input_layer)\r\n\r\n    conv = convolutional(conv, (1, 1, 1024, 512))\r\n    conv = convolutional(conv, (3, 3, 512, 1024))\r\n    conv = convolutional(conv, (1, 1, 1024, 512))\r\n    conv = convolutional(conv, (3, 3, 512, 1024))\r\n    conv = convolutional(conv, (1, 1, 1024, 512))\r\n\r\n    conv_lobj_branch = convolutional(conv, (3, 3, 512, 1024))\r\n    conv_lbbox = convolutional(conv_lobj_branch,\r\n                                      (1, 1, 1024, 3 * (NUM_CLASS + 5)),\r\n                                      activate=False, bn=False)\r\n\r\n    conv = convolutional(conv, (1, 1, 512, 256))\r\n    conv = upsample(conv)\r\n\r\n    conv = tf.concat([conv, route_2], axis=-1)\r\n\r\n    conv = convolutional(conv, (1, 1, 768, 256))\r\n    conv = convolutional(conv, (3, 3, 256, 512))\r\n    conv = convolutional(conv, (1, 1, 512, 256))\r\n    conv = convolutional(conv, (3, 3, 256, 512))\r\n    conv = convolutional(conv, (1, 1, 512, 256))\r\n\r\n    conv_mobj_branch = convolutional(conv, (3, 3, 256, 512))\r\n    conv_mbbox = convolutional(conv_mobj_branch,\r\n                                      (1, 1, 512, 3 * (NUM_CLASS + 5)),\r\n                                      activate=False, bn=False)\r\n\r\n    conv = convolutional(conv, (1, 1, 256, 128))\r\n    conv = upsample(conv)\r\n\r\n    conv = tf.concat([conv, route_1], axis=-1)\r\n\r\n    conv = convolutional(conv, (1, 1, 384, 128))\r\n    conv = convolutional(conv, (3, 3, 128, 256))\r\n    conv = convolutional(conv, (1, 1, 256, 128))\r\n    conv = convolutional(conv, (3, 3, 128, 256))\r\n    conv = convolutional(conv, (1, 1, 256, 128))\r\n\r\n    conv_sobj_branch = convolutional(conv, (3, 3, 128, 256))\r\n    conv_sbbox = convolutional(conv_sobj_branch,\r\n                                      (1, 1, 256, 3 * (NUM_CLASS + 5)),\r\n                                      activate=False, bn=False)\r\n\r\n    return [conv_sbbox, conv_mbbox, conv_lbbox]\r\n\r\n\r\ndef decode(conv_output, i: int = 0) -> Tensor:\r\n    \"\"\"\r\n    :decode: Decode the output of Yolo3 to get the bbox.\r\n\r\n    :param conv_output: output feature map\r\n    :type conv_output: Tensor\r\n\r\n    :param i: anchors index\r\n    :type i: int\r\n\r\n    :return: tensor of shape [batch_size, output_size,\r\n     output_size, anchor_per_scale, 5 + num_classes]\r\n            contains (x, y, w, h, score, probability)\r\n    \"\"\"\r\n\r\n    conv_shape = tf.shape(conv_output)\r\n    batch_size = conv_shape[0]\r\n    output_size = conv_shape[1]\r\n\r\n    conv_output = tf.reshape(conv_output, (\r\n        batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\r\n\r\n    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]\r\n    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]\r\n    conv_raw_conf = conv_output[:, :, :, :, 4:5]\r\n    conv_raw_prob = conv_output[:, :, :, :, 5:]\r\n\r\n    y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis],\r\n                [1, output_size])\r\n    x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :],\r\n                [output_size, 1])\r\n\r\n    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\r\n    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :],\r\n                      [batch_size, 1, 1, 3, 1])\r\n    xy_grid = tf.cast(xy_grid, tf.float32)\r\n\r\n    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\r\n    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\r\n    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\r\n\r\n    pred_conf = tf.sigmoid(conv_raw_conf)\r\n    pred_prob = tf.sigmoid(conv_raw_prob)\r\n\r\n    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\r\n\r\n\r\ndef bbox_iou(boxes1: ndarray, boxes2: ndarray) -> float:\r\n    \"\"\"\r\n    :bbox_iou: intersection over union between two bboxes.\r\n\r\n    :param boxes1: bbox\r\n    :type boxes1: ndarray\r\n    :param boxes2:bbox\r\n    :type boxes2: ndarray\r\n\r\n    :return:\r\n    intersection over union score\r\n    \"\"\"\r\n    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\r\n    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\r\n\r\n    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\r\n                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\r\n    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\r\n                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\r\n\r\n    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\r\n    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\r\n\r\n    inter_section = tf.maximum(right_down - left_up, 0.0)\r\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\r\n    union_area = boxes1_area + boxes2_area - inter_area\r\n\r\n    return 1.0 * inter_area / union_area\r\n\r\n\r\ndef bbox_giou(boxes1, boxes2):\r\n    \"\"\"\r\n    :bbox_giou: Generalized intersection over union between two bboxes.\r\n\r\n    :param boxes1: bbox\r\n    :type boxes1: ndarray\r\n    :param boxes2:bbox\r\n    :type boxes2: ndarray\r\n\r\n    :return:\r\n    intersection over union score\r\n    \"\"\"\r\n    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\r\n                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\r\n    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\r\n                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\r\n\r\n    boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\r\n                        tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\r\n    boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\r\n                        tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\r\n\r\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (\r\n            boxes1[..., 3] - boxes1[..., 1])\r\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (\r\n            boxes2[..., 3] - boxes2[..., 1])\r\n\r\n    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\r\n    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\r\n\r\n    inter_section = tf.maximum(right_down - left_up, 0.0)\r\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\r\n    union_area = boxes1_area + boxes2_area - inter_area + 1e-10\r\n    iou = inter_area / union_area\r\n\r\n    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\r\n    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\r\n    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\r\n    enclose_area = enclose[..., 0] * enclose[..., 1]\r\n    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\r\n\r\n    return giou\r\n\r\n\r\ndef compute_loss(pred: Tensor, conv: Tensor, label: Tensor,\r\n                 bboxes: Tensor, i: int = 0) -> Tuple[Tensor, Tensor, Tensor]:\r\n    \"\"\"\r\n    :compute_loss: compute loss of yolo3\r\n    :param pred: predict tensor\r\n    :type pred: Tensor\r\n    :param conv: target tensor\r\n    :type conv: Tensor\r\n    :param label: label tensor\r\n    :type label: Tensor\r\n    :param bboxes: bboxes tensor\r\n    :type bboxes: Tensor\r\n    :param i: index\r\n    :type i: int\r\n    :return:\r\n    tuple of losses tensors\r\n    \"\"\"\r\n    conv_shape = tf.shape(conv)\r\n    batch_size = conv_shape[0]\r\n    output_size = conv_shape[1]\r\n    input_size = STRIDES[i] * output_size\r\n    conv = tf.reshape(conv,\r\n                      (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\r\n\r\n    conv_raw_conf = conv[:, :, :, :, 4:5]\r\n    conv_raw_prob = conv[:, :, :, :, 5:]\r\n\r\n    pred_xywh = pred[:, :, :, :, 0:4]\r\n    pred_conf = pred[:, :, :, :, 4:5]\r\n\r\n    label_xywh = label[:, :, :, :, 0:4]\r\n    respond_bbox = label[:, :, :, :, 4:5]\r\n    label_prob = label[:, :, :, :, 5:]\r\n\r\n    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\r\n    input_size = tf.cast(input_size, tf.float32)\r\n\r\n    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:,\r\n                                                                :, :, :,\r\n                                                                3:4] / (\r\n                              input_size ** 2)\r\n    giou_loss = respond_bbox * bbox_loss_scale * (1 - giou)\r\n\r\n    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :],\r\n                   bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\r\n    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\r\n\r\n    respond_bgd = (1.0 - respond_bbox) * tf.cast(max_iou < IOU_LOSS_THRESH,\r\n                                                 tf.float32)\r\n\r\n    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\r\n\r\n    conf_loss = conf_focal * (\r\n            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(\r\n        labels=respond_bbox, logits=conv_raw_conf)\r\n            +\r\n            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(\r\n        labels=respond_bbox, logits=conv_raw_conf)\r\n    )\r\n\r\n    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(\r\n        labels=label_prob, logits=conv_raw_prob)\r\n\r\n    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1, 2, 3, 4]))\r\n    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3, 4]))\r\n    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1, 2, 3, 4]))\r\n\r\n    return giou_loss, conf_loss, prob_loss\r\n\r\n\r\ndef create_yolo3_model(input_size: int = 416):\r\n    input_layer = tf.keras.layers.Input([input_size, input_size, 3])\r\n    feature_maps = YOLOv3(input_layer)\r\n\r\n    bbox_tensors = []\r\n    for i, fm in enumerate(feature_maps):\r\n        bbox_tensor = decode(fm, i)\r\n        bbox_tensors.append(bbox_tensor)\r\n    model = tf.keras.Model(input_layer, bbox_tensors)\r\n    return model\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = create_yolo3_model()\r\n\r\n    images = np.random.uniform(0, 255., (10, 416, 416, 3))\r\n    images = images.astype(np.float32)\r\n    for idx, image in enumerate(images):\r\n        # here i'll print pred_bbox variable in get_object\r\n        # We remark that the first iteration is good but the rest is NaN\r\n        print(f\"Iteration : {idx}\")\r\n        get_object(model, input_image=image)\r\n```\r\n", "comments": ["@rafikg \r\n\r\nI tried to reproduce the issue in TF version 2.4 and I am seeing the error message(`ModuleNotFoundError: No module named 'core'`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/df89b367a5541cf48750046db030b917/untitled678.ipynb).\r\n\r\nCan you please share colab link or reproducible code to debug further. Thanks!\r\n", "Sorry @ravikyram \r\nI edited the code. Here is an example on colab that works fine on tensorflow 2.4.1 [code](https://colab.research.google.com/gist/rafikg/d22b21085e2de8d9ae4503055f03aabe/example.ipynb)\r\nHowever, running the same thing on google cloud VM with tensorflow 2.4.1/2.4.0 still get the same problem.", "@ravikyram @jvishnuvardhan \r\nCan you please check it because I am blocked on that and I am charging the client for that.", "@rafikg Looks like this an issue with `TF2.4.1`. However, there is no issue if you use `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/b0322e606db7cb5ed48a13ab9226ec4c/example.ipynb) is a gist for your reference. Thanks!", "Thanks for your effort @jvishnuvardhan,\r\nI don't think so that the problem is because of TF 2.4.1, my gist above is working fine on 2.4.1. I tried the the code on a fresh conda env with tf-nightly on K80 GCP VM and it still getting the same problem. I changed the GPU to Tesla 4 and I still getting the same problem with TF 2.4.1 and tf 2.5.0!! It is really so strange!", "I just ran the code on a GCP deep learning VM (1 GPU) with tf nightly and tf 2.4. I am also seeing nan for all iterations except iteration 0.\r\n\r\nI'm not seeing any nans using a CPU only environment in GCP. So seems to me this is a GPU related issue.", "Thanks @nikitamaia,\r\nYes, I tried even with Tesla 4 and K8 and it is still the same problem!! I can't really see the problem.", "Was able to run your code successfully without any issues in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/a32e0fb14757f61d75bebab50a16b4a9/47265.ipynb) and confirm if your still exists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47265\">No</a>\n", "Sir, I met the same problem when I ran a similar yolov3 project. `model.predict_on_batch()` returns Nan after the first iteration. My code is in the picture. My gpu gtx1070, tensorflow==2.7.0.  Have you solved the problem?\r\n![image](https://user-images.githubusercontent.com/62379808/158105049-b9f36339-40a2-4d9d-bfc9-cce29ff02ccf.png)\r\n\r\n", "@mastergao57 No really"]}, {"number": 47264, "title": "Keras layers do not track tf.Module (not conforming to SOLID principles)", "body": "This is a reduction of https://github.com/tensorflow/probability/issues/946 to an issue with TensorFlow by itself.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.9\r\n\r\n**Describe the current behavior**\r\ntf.keras.layers.Layer is a subclass of tf.Module, but I cannot replace a use of tf.Module with tf.keras.layers.Layer, violating the Liskov substitution principle - which is a real issue for downstream projects as demonstrated by https://github.com/tensorflow/probability/issues/946.\r\n\r\n**Describe the expected behavior**\r\nI can substitute a `tf.keras.layers.Layer` anywhere I need a `tf.Module`. Specifically, `layer.trainable_variables` and `layer.trainable_weights` discover all Variable instances that are in sub-Modules, not just in sub-Layers.\r\n\r\nIdeally, `.trainable_variables` would have a consistent return type between Module and Layer.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nmodule = tf.Module()\r\nmodule.submodule = tf.Module()\r\nmodule.submodule.var = tf.Variable(1.0)\r\nassert module.trainable_variables == (module.submodule.var,)  # as expected\r\n\r\nlayer = tf.keras.layers.Layer()\r\nassert isinstance(layer, tf.Module)  # passes\r\nlayer.sublayer = tf.keras.layers.Layer()\r\nlayer.sublayer.var = tf.Variable(1.0)\r\nassert layer.trainable_variables == [layer.sublayer.var]  # acceptable\r\n\r\nlayer = tf.keras.layers.Layer()\r\nlayer.submodule = tf.Module()\r\nlayer.submodule.var = tf.Variable(1.0)\r\nassert list(layer.trainable_variables) == [layer.submodule.var]  # FAILS\r\n```", "comments": ["We can resolve this with the following Layer subclass:\r\n```python\r\nimport itertools\r\nfrom functools import wraps\r\nfrom typing import Any, Callable, Optional, Sequence\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef extend_and_filter(\r\n    extend_method: Callable[..., Sequence], filter_method: Optional[Callable[..., Sequence]] = None,\r\n) -> Callable[[Any], Any]:\r\n    \"\"\"\r\n    This decorator calls a decorated method, and extends the result with another method\r\n    on the same class. This method is called after the decorated function, with the same\r\n    arguments as the decorated function. If specified, a second filter method can be applied\r\n    to the extended list. Filter method should also be a method from the class.\r\n\r\n    :param extend_method: Callable\r\n        Accepts the same argument as the decorated method.\r\n        The returned list from `extend_method` will be added to the\r\n        decorated method's returned list.\r\n    :param filter_method: Callable\r\n        Takes in the extended list and filters it.\r\n        Defaults to no filtering for `filter_method` equal to `None`.\r\n    \"\"\"\r\n\r\n    def decorator(f: Callable) -> Callable:\r\n        @wraps(f)\r\n        def wrapped(self, *args, **kwargs):  # type: ignore\r\n            ret = f(self, *args, **kwargs)\r\n            ret.extend(extend_method(self, *args, **kwargs))\r\n            ret = filter_method(self, ret) if filter_method is not None else ret\r\n            return ret\r\n\r\n        return wrapped\r\n\r\n    return decorator\r\n\r\n\r\nclass TrackableLayer(tf.keras.layers.Layer):\r\n    \"\"\"\r\n    A tf.Layer that implements tracking of tf.Variables on the class's\r\n    attributes that are tf.Modules.\r\n\r\n    Currently, tf.Modules track the tf.Variables of their attributes that are\r\n    also tf.Modules.  Similarly, tf.Layers track the tf.Variables of their\r\n    attributes that are also tf.Layers.  However, despite the fact that\r\n    tf.Layer inherits from tf.Module, they cannot track the tf.Variables of\r\n    their attributes that are generic tf.Modules. This seems to be an issue\r\n    that the TensorFlow authors seem to want to fix in the future.\r\n    \"\"\"\r\n\r\n    @property\r\n    def _submodules(self) -> Sequence[tf.Module]:\r\n        \"\"\"Return a list of tf.Module instances that are attributes on the class. Note\r\n        this also include list or tuples of tf.Modules\"\"\"\r\n\r\n        submodules = []\r\n\r\n        def get_nested_submodules(*objs: Any) -> None:\r\n            for o in objs:\r\n                if isinstance(o, tf.Module):\r\n                    submodules.append(o)\r\n\r\n        for key, obj in self.__dict__.items():\r\n            if isinstance(obj, tf.Module):\r\n                submodules.append(obj)\r\n            elif isinstance(obj, (list, tuple)):\r\n                tf.nest.map_structure(get_nested_submodules, obj)\r\n            elif isinstance(obj, (dict,)):\r\n                tf.nest.map_structure(get_nested_submodules, obj.values())\r\n\r\n        return list(dict.fromkeys(submodules))  # remove duplicates, maintaining order (dict 3.6)\r\n\r\n    def submodule_variables(self) -> Sequence[tf.Variable]:\r\n        \"\"\"Return flat iterable of variables from the attributes that are tf.Modules\"\"\"\r\n        return list(itertools.chain(*[module.variables for module in self._submodules]))\r\n\r\n    def submodule_trainable_variables(self) -> Sequence[tf.Variable]:\r\n        \"\"\"Return flat iterable of trainable variables from attributes that are tf.Modules\"\"\"\r\n        return list(itertools.chain(*[module.trainable_variables for module in self._submodules]))\r\n\r\n    def submodule_non_trainable_variables(self) -> Sequence[tf.Variable]:\r\n        \"\"\"Return flat iterable of non trainable variables from attributes that are tf.Modules\"\"\"\r\n        return [v for module in self._submodules for v in module.variables if not v.trainable]\r\n\r\n    def _dedup_weights(self, weights):  # type: ignore\r\n        \"\"\"Deduplicate weights while maintaining order as much as possible.\"\"\"\r\n        # copy this method from the super class\r\n        # to have it in the local class' namespace\r\n        return super()._dedup_weights(weights)\r\n\r\n    @property  # type: ignore\r\n    @extend_and_filter(submodule_trainable_variables, _dedup_weights)\r\n    def trainable_weights(self) -> Sequence[tf.Variable]:\r\n        return super().trainable_weights\r\n\r\n    @property  # type: ignore\r\n    @extend_and_filter(submodule_non_trainable_variables, _dedup_weights)\r\n    def non_trainable_weights(self) -> Sequence[tf.Variable]:\r\n        return super().non_trainable_weights\r\n\r\n    @property  # type: ignore\r\n    @extend_and_filter(submodule_trainable_variables, _dedup_weights)\r\n    def trainable_variables(self) -> Sequence[tf.Variable]:\r\n        return super().trainable_variables\r\n\r\n    @property  # type: ignore\r\n    @extend_and_filter(submodule_variables, _dedup_weights)\r\n    def variables(self) -> Sequence[tf.Variable]:\r\n        return super().variables\r\n```\r\nWith this class, the following works as expected:\r\n```python\r\nlayer = TrackableLayer()\r\nlayer.submodule = tf.Module()\r\nlayer.submodule.var = tf.Variable(1.0)\r\nassert list(layer.trainable_variables) == [layer.submodule.var]  # now passes\r\n```\r\nWhat would it take to get this behaviour into TensorFlow itself? Can we simply move the above methods onto tf.keras.layers.Layer itself? I'd be happy to contribute a PR if that helps resolve this issue faster.", "There's a related work-around in Edwards2: https://github.com/google/edward2/blob/5e18aec19af3925274808b55ff22f8a60a6cebdb/edward2/tensorflow/layers/utils.py#L38", "@st--,\r\nI was able to reproduce the issue with TF v2.3 and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/1c4415b5fae7a27a55176ff2fdae1009/47264.ipynb). However, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/d74e4348ea25e0bfa07551601ab69411/47264-tf-nightly.ipynb). \r\n\r\nPlease check the linked gist for reference. Thanks!", "@amahendrakar ah, awesome - thanks for the quick response. I did upgrade to 2.4.1 to check it was still an issue but hadn't though of also checking nightly ... do you happen to know which PR/commit fixed this in the end? Would be curious to see how it actually got fixed in TF-core.", "> do you happen to know which PR/commit fixed this in the end? Would be curious to see how it actually got fixed in TF-core.\r\n\r\n@st--,\r\nSorry for the delayed response. Since the modules are regularly being updated, we cannot pin point the exact commit/PR which fixed the issue. \r\n\r\nPlease feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47264\">No</a>\n"]}, {"number": 47263, "title": "(Numpy v1.20+ compat.): Cannot convert a symbolic Tensor (sequential_1/random_rotation_1/rotation_matrix/strided_slice:0) to a numpy array", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and no. I ahve writeen custom code, bu the code that throws exception _is_ provided by tensorflow\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tf-nightly-gpu v2.5.0-dev20201214\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.0.130/7.5.1\r\n- GPU model and memory:  nvidia Tesla T4 15109 MiB\r\n\r\n\r\n\r\n**Describe the current behavior**\r\ni have been running the same scripts for months, and i just started having this issue today when i re-run them. Last time they worked was 7 days ago (i even have the git commit uploading the then freshly executed script)\r\n\r\nthe only thing that changed in between was an updated version of numpy\r\n\r\nagain, this exact code worked without a hitch a week ago\r\n\r\n**Describe the expected behavior**\r\nthe code shoould run fine\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninference_model = tf.keras.models.load_model('./S2_models/inference_model.h5')\r\n\r\n Adding data augmentation\r\ndata_augmentation = tf.keras.Sequential([\r\ntf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\r\ntf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\r\n])\r\n\r\ninputs = tf.keras.Input(shape=(224, 398, 3))\r\nx = data_augmentation(inputs)\r\noutputs = inference_model(x, training=False)\r\ntraining_model = tf.keras.Model(inputs, outputs)\r\n\r\n**Other info / logs** \r\n\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-11-4e12e0617af3> in <module>\r\n      1 inputs = tf.keras.Input(shape=(224, 398, 3))\r\n----> 2 x = data_augmentation(inputs)\r\n      3 outputs = inference_model(x, training=False)\r\n      4 training_model = tf.keras.Model(inputs, outputs)\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    955     # >> model = tf.keras.Model(inputs, outputs)\r\n    956     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n--> 957       return self._functional_construction_call(inputs, args, kwargs,\r\n    958                                                 input_list)\r\n    959 \r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1094           layer=self, inputs=inputs, build_graph=True, training=training_value):\r\n   1095         # Check input assumptions set after layer building, e.g. input shape.\r\n-> 1096         outputs = self._keras_tensor_symbolic_call(\r\n   1097             inputs, input_masks, args, kwargs)\r\n   1098 \r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    826       return nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    827     else:\r\n--> 828       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    829 \r\n    830   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    867           # TODO(kaftan): do we maybe_build here, or have we already done it?\r\n    868           self._maybe_build(inputs)\r\n--> 869           outputs = call_fn(inputs, *args, **kwargs)\r\n    870 \r\n    871         self._handle_activity_regularization(inputs, outputs)\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)\r\n    396         kwargs['training'] = training\r\n    397 \r\n--> 398       outputs = layer(inputs, **kwargs)\r\n    399 \r\n    400       if len(nest.flatten(outputs)) != 1:\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1016         with autocast_variable.enable_auto_cast_variables(\r\n   1017             self._compute_dtype_object):\r\n-> 1018           outputs = call_fn(inputs, *args, **kwargs)\r\n   1019 \r\n   1020         if self._activity_regularizer:\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in call(self, inputs, training)\r\n    866           interpolation=self.interpolation)\r\n    867 \r\n--> 868     output = control_flow_util.smart_cond(training, random_rotated_inputs,\r\n    869                                           lambda: inputs)\r\n    870     output.set_shape(inputs.shape)\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)\r\n    112     return control_flow_ops.cond(\r\n    113         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n--> 114   return smart_module.smart_cond(\r\n    115       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n    116 \r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     52   if pred_value is not None:\r\n     53     if pred_value:\r\n---> 54       return true_fn()\r\n     55     else:\r\n     56       return false_fn()\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in random_rotated_inputs()\r\n    861       return transform(\r\n    862           inputs,\r\n--> 863           get_rotation_matrix(angles, img_hd, img_wd),\r\n    864           fill_mode=self.fill_mode,\r\n    865           fill_value=self.fill_value,\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in get_rotation_matrix(angles, image_height, image_width, name)\r\n    756             math_ops.cos(angles)[:, None],\r\n    757             y_offset[:, None],\r\n--> 758             array_ops.zeros((num_angles, 2), dtypes.float32),\r\n    759         ],\r\n    760         axis=1)\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    204     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    205     try:\r\n--> 206       return target(*args, **kwargs)\r\n    207     except (TypeError, ValueError):\r\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in wrapped(*args, **kwargs)\r\n   2905 \r\n   2906   def wrapped(*args, **kwargs):\r\n-> 2907     tensor = fun(*args, **kwargs)\r\n   2908     tensor._is_zeros_tensor = True\r\n   2909     return tensor\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in zeros(shape, dtype, name)\r\n   2954           # Create a constant if it won't be very big. Otherwise create a fill\r\n   2955           # op to prevent serialized GraphDefs from becoming too large.\r\n-> 2956           output = _constant_if_small(zero, shape, dtype, name)\r\n   2957           if output is not None:\r\n   2958             return output\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in _constant_if_small(value, shape, dtype, name)\r\n   2890 def _constant_if_small(value, shape, dtype, name):\r\n   2891   try:\r\n-> 2892     if np.prod(shape) < 1000:\r\n   2893       return constant(value, shape=shape, dtype=dtype, name=name)\r\n   2894   except TypeError:\r\n\r\n<__array_function__ internals> in prod(*args, **kwargs)\r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims, initial, where)\r\n   3028     10\r\n   3029     \"\"\"\r\n-> 3030     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n   3031                           keepdims=keepdims, initial=initial, where=where)\r\n   3032 \r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\r\n     85                 return reduction(axis=axis, out=out, **passkwargs)\r\n     86 \r\n---> 87     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n     88 \r\n     89 \r\n\r\n~/miniconda3/envs/CognitiveHatch/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __array__(self)\r\n    851 \r\n    852   def __array__(self):\r\n--> 853     raise NotImplementedError(\r\n    854         \"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\r\n    855         \" This error may indicate that you're trying to pass a Tensor to\"\r\n\r\nNotImplementedError: Cannot convert a symbolic Tensor (sequential_1/random_rotation_1/rotation_matrix/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n", "comments": ["@ghylander \r\nPlease provide simple indented standalone code to reproduce the issue. Thanks!\r\n\r\nAlso, refer to these links and let us know if it helps: [link](https://stackoverflow.com/questions/58479556/notimplementederror-cannot-convert-a-symbolic-tensor-2nd-target0-to-a-numpy), [link1](https://github.com/tensorflow/tensorflow/issues/36792)", "`\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninference_model = tf.keras.models.load_model('./S2_models/inference_model.h5')\r\n\r\ndata_augmentation = tf.keras.Sequential([\r\ntf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\r\ntf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\r\n])\r\n\r\ninputs = tf.keras.Input(shape=(224, 398, 3))\r\nx = data_augmentation(inputs)\r\noutputs = inference_model(x, training=False)\r\ntraining_model = tf.keras.Model(inputs, outputs)\r\n`\r\nyou'll need to specify a path from which to load a model", "I seem to have found the problem (sort of)\r\ntensorflow seems to be incompatible with numpy v1.20+, while 1.19 works\r\n\r\nwithout making any changes whatsoever to the code, swapping between numpy v1.19 and 1.20 creates this issue\r\n\r\nit seems there is some issue in the way tf handles symbolic tensors that makes it incompatible with numpy v120+\r\n\r\nthe issue arises when declaring any \"placeholder tensor\", e.g: when declaring the input of a layer (as seen in the code I provided)\r\n\r\nshould i close the issue or rename it to \"numpy 1.20+ compatibility\"?\r\n\r\n\r\nthe code that creates this issue is found in the tf docs (for example: https://www.tensorflow.org/tutorials/images/transfer_learning?hl=en#add_a_classification_head)", "I am able to reproduce this issue and It's a pain. I can't downgrade numpy without breaking other things that depend on numpy :( I hope this gets fixed soon!", "Maybe this issue should be renamed to \"Incompatibility with numpy 1.20\"?", "@ghylander\r\nI ran the code shared and face a different issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5bd8211acc40d06956777af326b31252/untitled.ipynb\r\ndint know what 2 do).", "yes, i said you need to specify a model to load\r\nno worries, i modified the gist so it downloads one pre trained model, and so it updates numpy to lastest version (since it is numpy what causes the problem)\r\nhttps://colab.research.google.com/gist/ghylander/7e75a1da6da7f8f3eb12578f4e21eeb2/untitled.ipynb", "I am able to replicate the issue reported on tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5e343667bca2e811964a638376d1eda9/untitled.ipynb)", "@ghylander As you noticed this is more related to numpy version (>1.20). As mentioned [here](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/tools/pip_package/setup.py#L84), there is a bound on numpy versions supported by TF2.4  ( ~=1.19.2). Currently numpy version should be  >=1.19.2 and  < 1.20. \r\n\r\nAs you mentioned in the other issue, downgrading numpy 2.x to the versions mentioned above will work without any issues. Thanks!\r\n\r\ncc @mihaimaruseac \r\n\r\n", "~~I'm having this issue with numpy 1.19.5, python 3.8.5 and tf 2.4.1, in windows 10 & anaconda environment.~~\r\n\r\nconda list numpy gave 1.19.5, when I ran conda install numpy=1.19.2 it downgraded 1.20.1.  ", "having found the issue, and it being numpy compatibility, i would like to request compatibility with numpy v1.20 both to make use of the new functions added and because it's a dependency in other packages\r\n\r\nshould i close this issue and create a new one asking for compatibility with numpy 1.20, or should i keep this one open?", "We can keep this one or create a new one and deduplicate all other numpy 1.20 issues to that one.\r\n\r\nIt will take a while for TF to become compatible with numpy 1.20 given that it needs the same version both for the C++ and for the Python code. Since numpy 1.20 is not backwards compatible with previous versions, this will require a lot of changes in TF if we want to maintain backwards compatibility (or an even larger effort to upgrade all of our users to newer numpy)", "@ghylander Can you please close this and open a new `feature_request` issue so that it is easier to follow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47263\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47263\">No</a>\n", "Moved to #47691"]}, {"number": 47262, "title": "EfficientnetB1 :ValueError: You are trying to load a weight file containing 185 layers into a model with 184 layers ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nyes- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nColab - OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nSource- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n3.6- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nafter using your code efficientnet_weight_update_util.py to  converted pretrained weights model.ckpt to efficientnetb1_notop.h5\r\n\r\nplease help, im getting the mentioned error\r\nmodel = EfficientNetB1(weights=\"efficientnetb1_notop.h5\", include_top=False)\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@gunjan5489 \r\n\r\nPlease, let us know which TF version you are using?.\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47262\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47262\">No</a>\n"]}]