[{"number": 21366, "title": "Build fails: ERROR: infinite symlink expansion detected", "body": "I'm getting this error on FreeBSD, it fails in jemalloc even though I answered \"n\" when it asked if to build it:\r\n```\r\nworkspace: /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\n/usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/bazel_ot/2cc1156aa366d993caf19cae8ee5ba0d/external/org_tensorflow\r\n/usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1\r\n[end of symlink chain]\r\nERROR: /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/tensorflow/tools/pip_package/BUILD:123:1: error loading package '@jemalloc//': Encountered error while reading extension file 'third_party/common.bzl': no such package '@org_tensorflow//third_party': Could not access /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/bazel_ot/2cc1156aa366d993caf19cae8ee5ba0d/external/org_tensorflow: Infinite symlink expansion and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package '@jemalloc//': Encountered error while reading extension file 'third_party/common.bzl': no such package '@org_tensorflow//third_party': Could not access /usr/ports/science/py-tensorflow/work-py27/tensorflow-1.10.0-rc1/bazel_ot/2cc1156aa366d993caf19cae8ee5ba0d/external/org_tensorflow: Infinite symlink expansion\r\nINFO: Elapsed time: 5.260s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (75 packages loaded)\r\n```", "comments": ["Looks similar to https://github.com/tensorflow/tensorflow/issues/5732 but there are no links mentioned there.", "The FreeBSD port has been deleted because it fails to build, in part due to this reason.\r\nThis needs to be resolved for the port to be reinstated.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I will mark this as community support, as we do not have freebsd systems to experiment on. So no official support for FreeBSD exist.\r\n\r\nIn the similar issue you saw, it looks like the problem was an existing symlink in the system.\r\nIs it possible you are seeing a similar issue?\r\n\r\n", "On second look, to your issue looks different, but I am guessing this is a bad interaction between bazel and the OS.", "Hi,\r\n\r\nCannot really help but I've seen this behavior when $HOME is set to a directory inside source directory. You might want to try overriding HOME env on bazel build call to /tmp/something and see if it helps...\r\n\r\nBazel.................................................", "FYI I saw a remedy in https://github.com/NixOS/nixpkgs/blob/57004738b168663bb4053c6737053e0039cc6259/pkgs/development/tools/build-managers/bazel/python-bin-path-test.nix which is to use `bazel --output_base=/something/not/in/home`.", "I am facing issue like this on ubuntu 18.04(LTS)\r\nwhen i am installing tensorflow from source https://www.tensorflow.org/install/source\r\non running bellow line i get error\r\n./configure\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.23.0 installed.\r\nPlease downgrade your bazel installation to version 0.21.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.", "I've encountered same problem when I set custom bazel output directory with manually setting [$TEST_TMPDIR](https://docs.bazel.build/versions/master/output_directories.html#documentation-of-the-current-bazel-output-directory-layout).\r\n\r\nI still not understand fully, but I found a work-around at least for my case: **if you want to set your own bazel output directory, set it outside of tensorflow root directory.**\r\n\r\nThis issue seems to occur when setting bazel output directory inside(i.e. subdirectory) or tensorflow root directory. Symlink `${bazel_output_dir}/external/org_tensorflow` is supposed to point tensorflow root directory, but if it is inside of tensorflow root itself, it seems to occur some kind of infinite symlink expansion.\r\n\r\nIn my case, I encountered this issue as building T/F using docker. If you are in the same case, just make docker volume and mount it to `/root/.cache/bazel` which is default bazel output directory (if root used for docker). All artifacts can be stored in docker volume in this way.", "> I've encountered same problem\r\n\r\nProblems in ```bazel``` aren't fixed for months or years.\r\n"]}, {"number": 21357, "title": "dnn_dropout in DNNLinearCombinedClassifier doesn't  work", "body": "I use DNNLinearCombinedClassifier to train a wide & deep model with dnn_dropout=0.5\u3002\r\n```\r\nm = tf.estimator.DNNLinearCombinedClassifier(\r\n    model_dir=model_dir,\r\n    linear_feature_columns=crossed_columns,\r\n    dnn_feature_columns=deep_columns,\r\n    dnn_dropout=0.5,\r\n    dnn_hidden_units=[512, 256, 256, 128])\r\n\r\nm.train(\r\n    input_fn=input_fn(train_file_name, num_epochs=None, shuffle=True, \r\n    is_eval=False,steps=train_steps, \r\n    hooks=[logger_hook])\r\n```\r\n\r\n Then I evaluate model as follow: \r\n```\r\nm = tf.estimator.DNNLinearCombinedClassifier(\r\n    model_dir=model_dir,\r\n    linear_feature_columns=crossed_columns,\r\n    dnn_feature_columns=deep_columns,\r\n    dnn_dropout=1.0,\r\n    dnn_hidden_units=[512, 256, 256, 128])\r\n\r\nresults = m.evaluate(\r\n    input_fn=input_fn(test_file_name, num_epochs=1, shuffle=False, is_eval=True),\r\n    steps=None)\r\n```\r\nI find no matter I set dnn_dropout=0.1 or dnn_dropout=1.0, the result are same. It seems that this parameter doesn't work.\r\nMy version is 1.4", "comments": ["TF will raise an ValueError exception if dnn_dropout=1.0, did you forget to call train at second time ? Anyway you can check it by using two different model_dir.", "@jony0917 Trying dnn_dropout=0.1 and 0.8, whatever any kind of droupout ratio would get the same result. ", "Can anyone help\uff1f", "I'm also waiting for the answer."]}, {"number": 21349, "title": "Improve tf.data graph representation in TensorBoard", "body": "Hello,\r\n\r\nThis is a feature request for making `tf.data` more insightful for TensorBoard. Currently, having a few `.apply` or `.map` and `.filter` calls looks like the image attached.\r\n\r\nAlthough the internals of each `tf.data` inside the box look nice, it's hard to know from TensorBoard how they work together. Moreover, they are not descriptively named, so things get quite confusing.\r\n\r\nTwo suggestions come to mind:\r\n\r\n- Having the ability to add descriptive names (contrasting the current ex: `__function_library__tf_map_func_YdsTEwCNzV0`)\r\n- A graphical representation of how the different `tf.data` calls work together.\r\n\r\n![image](https://user-images.githubusercontent.com/7721540/43617282-fe068ea8-9675-11e8-8e1a-c0f4eacff4b1.png)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@TimZaman: [Tensorboard graph_visualization ](https://www.tensorflow.org/guide/graph_viz), provides a graphical representation of how the different tf.data calls work together. Did I understand a part of feature request correctly, or is there something else you are looking for?", "> @TimZaman: Tensorboard graph_visualization , provides a graphical representation of how the different tf.data calls work together.\r\n\r\nThat does not seem to be the case: see the screenshot.", "This is what I visualized in Tensorboard for a simple input pipeline:\r\n<img width=\"1104\" alt=\"screen shot 2018-08-05 at 1 49 02 pm\" src=\"https://user-images.githubusercontent.com/16565716/43689950-a46e8ccc-98b6-11e8-89d6-a23660b49540.png\">\r\n", "Adding @mrry to the loop.", "A quick update: this improvement depends on a refactoring that @rohan100jain is working on, to make the correspondence between `tf.data.Dataset` objects and nodes in the graph 1-to-1. This will give us a natural way to use op names to add descriptive names for the functions.\r\n\r\n(Rohan, I'll assign this to you as part of the \"create-as-you-go\" work you're already doing.)\r\n\r\nAs for changes to the visualization itself, that would probably be an issue to take up with the TensorBoard team... I'm not sure how functions (and the other forthcoming changes in TF 2.0) are intended to be visualized.", "As Derek said, I'm working on making the python dataset objects correspond well with nodes in the graph. Estimated ETA would be around a month. ", "Any updates :D?", "Would be great to know if we have any updates on this . Also @TimZaman were you able to get in touch with the TensorBoard team for the improved visualizations?", "No, moved to pytorch long ago"]}, {"number": 21348, "title": "[Bug] cpu memory leak while using GPU with variable length ops.", "body": "```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport gc\r\n\r\ndef NotBuggy():\r\n    with tf.device(\"/gpu:0\"):\r\n        x = tf.placeholder(dtype=tf.float32, shape=(1000,1))\r\n        col = tf.placeholder(dtype=tf.int32, shape=(None,1))\r\n\r\n        p0 = tf.gather_nd( x,col)\r\n        p1 = tf.gather_nd( x,col+1)\r\n        diff = p0 - p1\r\n    sess = tf.Session()\r\n    co = 0\r\n    while (True):\r\n        feed_dict = {x: np.expand_dims(np.arange(1000),1), \r\n                     col: np.random.randint( 0,900, size=( np.random.randint(500,20000),1 ) ) }\r\n        _res = sess.run(diff, feed_dict=feed_dict)\r\n        co = co + 1\r\n        if (co % 1000 == 0):\r\n            gc.collect()\r\n            print(_res.shape)\r\n            print(co)\r\n\r\n\r\ndef Buggy():\r\n    with tf.device(\"/gpu:0\"):\r\n        x = tf.placeholder( dtype=tf.float32 , shape=(1000))\r\n        ind = tf.placeholder( dtype=tf.int32, shape=(None) )\r\n\r\n        p0 = tf.gather_nd(tf.expand_dims(x,1),tf.expand_dims(ind,1))\r\n        p1 = tf.gather_nd(tf.expand_dims(x,1),tf.expand_dims(ind+1,1))\r\n\r\n        diff = p0-p1\r\n\r\n    sess = tf.Session()\r\n    co = 0\r\n    while(True):\r\n        #Note that the length of ind is variable\r\n        feed_dict = {x: np.arange(1000),ind: np.random.randint(0, 900, size=(np.random.randint(500, 20000)))}\r\n        #When sized is fixed : doesn't memory leak\r\n        #feed_dict = {x: np.arange(1000),ind: np.random.randint(0, 900, size=(20000,))}\r\n        _res = sess.run(diff, feed_dict=feed_dict)\r\n        co = co+1\r\n        if( co % 1000 == 0):\r\n            gc.collect()\r\n            print(_res.shape)\r\n            print(co)\r\n\r\n#NotBuggy()\r\nBuggy()\r\n```\r\n\r\nThe code above leaks cpu memory quite rapidly ~10 Mb/s.\r\nI tried some differential analysis, the bug seems weird and related to the graph construction.\r\n\r\nHave Fun :)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Lubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: Python 3.6.5 (default, May  3 2018, 10:08:28) (also bugs on Python 3.5.2 (default, Nov 17 2016, 17:05:23) )\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**:   CUDA Driver Version / Runtime Version          9.1 / 8.0 , cuDNN N/A\r\n- **GPU model and memory**: NVidia GTx1080 ti 12Go\r\n- **Exact command to reproduce**: Run code given above (It leaks memory quite rapidly ~10 Mb /s)\r\n", "comments": ["I could reproduce the memory leak. As a guess, could this be the allocate_temp in the GatherNd op doing something unexpected?\r\n", "Just saw this. Will review tomorrow", "I don't see how `allocate_temp` on the GPU kernel would leak CPU memory, but it is possible.", "I'm able to replicate the issue.", "Any update ?", "I got the same problem. I used gperf to prof the heap using status and it showed me a cpu memory increasing caused by allocate_temp, but actually i ran the funtion on a GPU kernel.", "@ebrevdo An answer/update regarding this issue would we highly appreciated. It is rather on the annoying side of bugs in my experience.", "Any updates?"]}, {"number": 21335, "title": "Possible bug / Documentation Suggestion - Dataset API + Estimator API does not throw an exception when the input filepath to the dataset is incorrect", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: CUDA 9.0  / CUDNN 7.1.4.18\r\n- **GPU model and memory**: GTX 1080 Ti - 9710 MB\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nWhen using the Dataset API along with the Estimator API, if the input file path to the dataset, while initializing **tf.data.TFRecordDataset** or **tf.data.FixedLengthRecordDataset**, is incorrect/ file does not exist, it does not immediately throw an exception. Instead the entire network graph is constructed, weights are initialized and checkpoints (if present) are loaded.\r\n\r\nThe only indication that something is wrong is when the session.run is called inside the function **_train_with_estimator_spec** in the train loop of the estimator.** The loss remains _None_ and the global_step is not updated**. Moreover, the training runs for as many epochs as requested with nothing being updated. \r\n\r\nHowever, even if one were to use the Dataset API with an incorrect file path but still overwrite the features and labels returned by it with manually set values, the loss is still not computed and remains as _None_ and the same problem as above persists. This is very counter-intuitive and hard to debug.\r\n\r\nIt would be better if the Dataset API somehow threw an exception in the beginning itself. If this is not possible, it would be nice if this was documented somewhere.\r\n\r\n### Source code\r\n\r\n1. Clone the tensorflow models from the official repository.\r\n`git clone https://github.com/tensorflow/models.git tfmodels`\r\n`cd tfmodels`\r\n\r\n2. Run the **cifar10_download_and_extract.py** file to download and extract the cifar10 dataset.\r\n3. Run the **cifar10_main.py** file to check that the original model is running correctly.\r\n\r\n4. Delete the .bin files inside the **cifar-10-batches-bin** folder that was created in step 2. Keep the folder but only delete the contents inside it. By doing so we are passing an empty string to the dataset initializer and hence the dataset iterator is probably returning garbage values.\r\n5. Re-run step 3 and you see the error described.\r\n\r\n6. Make a copy of the file **cifar10_main.py** and edit it by adding the following lines just before the return statement in the function **parse_record**.\r\n`image = tf.constant(value=0.0, shape=[32, 32, 3], dtype=tf.float32)`\r\n`label = tf.constant(value=0, shape=[], dtype=tf.int32)`\r\nBy doing so we are still using an incorrect initializer but are now replacing the garbage values returned by the reader with known constants (zeros in this case).\r\n7. Re-run the modified file and you still see the error described.", "comments": ["Thanks, @tejaswid , that does seem less than ideal. @mrry , can you comment on the expected behavior of Dataset if a passed path is incorrect, separate from estimator? Do we currently do any checking there, or is it assumed that is the role of clients?", "I think this is working as intended: the iterator should raise an exception (I think it would be `tf.errors.InvalidArgumentError`) when the first element is retrieved. \r\n\r\nHowever, I'm surprised by this part of the bug report:\r\n\r\n> By doing so we are passing an empty string to the dataset initializer and hence the dataset iterator is probably returning garbage values.\r\n\r\nCan you try reproducing this problem with a simpler example? As mentioned above, the iterator should raise an exception and never generate garbage values when the path is unrecognized.", "When I run the following simple example,\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef _parse_data(raw_tfrecord):\r\n    feature = {'/value': tf.FixedLenFeature([], tf.int64)}\r\n    data = tf.parse_single_example(raw_tfrecord, features=feature)\r\n    value = tf.cast(data['/value'], tf.int32)\r\n    return value\r\n\r\ndata = tf.data.TFRecordDataset(\"<FILENAME>\")  # see below for explanation\r\ndata_parsed = data.map(_parse_data)\r\nbatched_dataset = data_parsed.batch(1)\r\niterator = batched_dataset.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n    print(sess.run(next_element))\r\n    sess.close()\r\n```\r\nIf `<FILENAME>` is a non-existent file (like empty string) the error says\r\n```\r\nNotFoundError (see above for traceback): ; No such file or directory\r\n[[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n```\r\nIf `<FILENAME>` is an existing directory but not a _.tfrecords_ file the error says\r\n```\r\nFailedPreconditionError (see above for traceback): /media/drzadmin/Z/repos/python/datasets; Is a directory\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n```\r\nwhich I think is the expected behaviour.\r\n\r\nWhen I run the code mentioned in my first post, no error is thrown. Instead loss is not computed at all and remains **None**. The following lines in the function **_train_with_estimator_spec** in **estimator.py** is where the loss should actually be computed, but it remains None after the sess.run call. Furthermore the training continues but the **global_step value always remains at 0**.\r\n\r\n```python\r\nloss = None\r\nwhile not mon_sess.should_stop():\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\nreturn loss\r\n```\r\n", "I can reproduce the bug with [models/official/resnet/cifar10_main.py](https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py) . And the exception of [_WrappedSession](https://github.com/tensorflow/tensorflow/blob/aed8f42bafabf11c5d92ce4109a5e0408b31f9c5/tensorflow/python/training/monitored_session.py#L987) is shown below when running:\r\n\r\n```python\r\nEnd of sequence\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,32,32,3], [?]], output_types=[DT_FLOAT, DT_INT32], _device=\"/device:CPU:0\"](IteratorFromStringHandle)]]\r\n         [[Node: FunctionBufferingResourceGetNext = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FunctionBufferingResource)]]\r\ncoord exception: End of sequence\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,32,32,3], [?]], output_types=[DT_FLOAT, DT_INT32], _device=\"/device:CPU:0\"](IteratorFromStringHandle)]]\r\n         [[Node: FunctionBufferingResourceGetNext = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FunctionBufferingResource)]]\r\n```", "@robieta - can you investigate whether this is an issue with Cifar10 in particular, or more generally?", "Nagging Assignees @karmel, @robieta: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @karmel, @robieta: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We have not had spare capacity to look into this currently-- I will mark as open for contribution if someone is interested in investigating the core cause and a solution here."]}, {"number": 21323, "title": "[TensorFlow Android Camera Demo] add libSVM AAR but \"Native TF methods not found\"", "body": "I use the demo code successfully.\r\nHowever, when I wand to add libSVM AAR to the dependencies. \r\nhttps://github.com/yctung/AndroidLibSVM\r\n\r\nAnd the debug console show:\r\n```\r\nI/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\nE/zygote64: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\nI/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\nE/MTCNN: [*]load model failedjava.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.\r\nI/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\nE/zygote64: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\nI/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\n```\r\n\r\nIn build.gradle:\r\n```\r\ndependencies {\r\n    if (nativeBuildSystem == 'cmake' || nativeBuildSystem == 'none') {\r\n        compile 'org.tensorflow:tensorflow-android:+'\r\n    }\r\n    implementation project(':androidlibsvm-release') --> if I remark this, the issue will disappear.\r\n}\r\n```\r\nCould anyone know how to fix this?\r\n\r\nTensorFlow version:\r\nv1.9.0-0-g25c197e023 1.9.0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: only add the androidlibsvm-release.aar\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: pip install, v1.9.0-0-g25c197e023 1.9.0\r\nBazel version: 0.13.0\r\nCUDA/cuDNN version: v9.0\r\nGPU model and memory: GTX1080\r\nExact command to reproduce: import AAR and add into dependencies\r\nMobile device: samsung s7", "@why702 ,\r\nCan you please make sure the native shared libraries are in a location where they can be properly loaded. If you are using a custom shared library name, you'll need to manually call **`System.loadLibrary()`** first.\r\n\r\nAlso I request you please check this relevant thread [1](https://github.com/tensorflow/tensorflow/issues/22565) and [2](https://github.com/tensorflow/tensorflow/issues/19384) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 21250, "title": "tf.Print support of tf.complex types", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a (cpu execution)\r\n- **GPU model and memory**: n/a (cpu execution)\r\n- **Exact command to reproduce**: cf test case below\r\n\r\n### Problem description\r\nWhen using the `tf.Print` operation on complex data, the printed output only shows question marks (`?`) in place of the complex values. The documentation does not indicate a limitation on the type of tensors of the data argument. It would be good to add that limitation in the documentation or even better, support tf.complex types. In the meanwhile, one can work around and use tf.real and tf.imag. The test case below shows the problem and the work around.\r\nNote that the problem was already present with Tensorflow 1.7.\r\n\r\n### Source code / logs\r\nSources:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\na = np.array([[1, 2], [3, 4]])\r\n\r\ninput1 = tf.placeholder(tf.complex64, shape=[2,2], name=\"input1\")\r\ninput2 = tf.placeholder(tf.float32, shape=[2, 2], name=\"input2\")\r\n\r\ninput1 = tf.Print(input1, [input1], \"input1: \")\r\ninput1 = tf.Print(input1, [tf.real(input1)], \"input1.real: \")\r\ninput2 = tf.Print(input2, [input2], \"input2: \")\r\n\r\noutput1 = tf.identity(input1, name=\"output1\")\r\noutput2 = tf.identity(input2, name=\"output2\")\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nout1, out2 = sess.run([\"output1:0\", \"output2:0\"], {\"input1:0\": a, \"input2:0\": a})\r\n```\r\nConsole output:\r\n```\r\ninput2: [[1 2][3]...]input1: [? ? ?...]\r\n\r\ninput1.real: [[1 2][3]...]\r\n```\r\nBy the way, the formatting of the printed messages (line returns) looks weird. This appeared when going from TF1.7 to 1.9, but this is not the point of that issue.", "comments": ["I may have found the piece of code that generates the `?`. There is indeed a [TODO](https://github.com/tensorflow/tensorflow/blob/9054c9b2ac303cbd1538166d0821f389cbc75894/tensorflow/core/framework/tensor.cc#L1011) in the code.", "I looked at the code and worked on a way to support the complex types for tf.Print. Here is new output of the exact same test case proposed above:\r\n```\r\ninput2: [[1 2][3]...]input1: [[1+0j 2+0j][3+0j]...]\r\n\r\ninput1.real: [[1 2][3]...]\r\n```\r\nI can create a pull request.", "If you've already gone ahead and written the code, feel free to open a pull request! Just make sure it also prints correctly when there are non-integer-valued complex numbers.\r\n\r\nThere are a number of additional changes we plan to make to tf.Print, some of which will make it format printed messages more sensibly.\r\n", "The floating point complex numbers seem to be working as well. See bellow the new test case and output. \r\nI am checking now if there is a unit test to be added and will start the pull request.\r\n### Source:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nnp.random.seed(seed=0)\r\na = np.random.rand(2, 2) + 1j * np.random.rand(2, 2)\r\n\r\ninput1 = tf.placeholder(tf.complex64, shape=[2,2], name=\"input1\")\r\ninput2 = tf.placeholder(tf.float32, shape=[2, 2], name=\"input2\")\r\n\r\ninput1 = tf.Print(input1, [input1], \"input1: \")\r\ninput1 = tf.Print(input1, [tf.real(input1)], \"input1.real: \")\r\ninput2 = tf.Print(input2, [input2], \"input2: \")\r\n\r\noutput1 = tf.identity(input1, name=\"output1\")\r\noutput2 = tf.identity(input2, name=\"output2\")\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nout1, out2 = sess.run([\"output1:0\", \"output2:0\"], {\"input1:0\": a, \"input2:0\": a.real})\r\n```\r\n### Output:\r\n```\r\ninput2: [[0.548813522 0.715189338][0.602763355]...]input1: [[0.548813522+0.423654795j 0.715189338+0.64589411j][0.602763355+0.437587202j]...]\r\n\r\ninput1.real: [[0.548813522 0.715189338][0.602763355]...]\r\n```\r\nGreat to hear there are plans to improve even further the tf.Print, this feature is amazingly useful.", "Great, thank you Anne!\r\nWe'll take a look at the PR once you open it.", "The problem is still present in the version 1.12. But I saw that tf.Print will be deprecated and replaced by tf.print in 2.0. I don't know if the problem will still be present there. \r\n\r\nI proposed a PR that fixed the problem by modifying the print op. However it was suggested to fix that at a lower level and mirror the change in the abseil common C++ library as TF should ultimately use it for the print op. I have not found the time to look at making those changes in abseil as it is more invasive and impactful. ", "I'm using 2.0 and tried printing a tensor with values of the type tf.complex128. A question mark appears when I print with tf.print. However, if I only use the python built-in print, I can see the values.", "Could reproduce the issue with **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/84198145c8accc41dc215950d3632315/gh_21250.ipynb).", "As @iranroman pointed earlier , I could print real and imaginary values using python print though. Attaching [gist ](https://colab.sandbox.google.com/gist/mohantym/a52544075800873ad391dd62a2023076/gh_21250.ipynb#scrollTo=7ga-RXiovsq8)for reference. "]}, {"number": 21185, "title": "tf.shape output is wrong when net input shape is changed during import", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8.0 **(still present in 2.6.0, updating the code accordingly)**\r\n- **Python version**: 3.6.6\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Bazel version**: N/A\r\n- **Mobile device**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\n`tf.shape` returns an inconsistent result when a network is imported from file and its input is changed during the import. Let me create a simple net with a `batch_size` of 128, and save it to disk\r\n\r\n    import tensorflow.compat.v1 as tf\r\n    tf.disable_v2_behavior()\r\n\r\n    batch_size = 128\r\n    x = tf.placeholder(tf.float32, shape=(batch_size, 10), name='x')\r\n    b = tf.Variable(tf.zeros((10)))\r\n    y = tf.add(x, b, name='y')\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n      tf.global_variables_initializer().run()\r\n      saver.save(sess, './foo')\r\n\r\nLater, I reload this model, and replace the input placeholder with a more flexible one, with an undefined `batch_size`. \r\n\r\n    import numpy as np\r\n    import tensorflow.compat.v1 as tf\r\n    tf.disable_v2_behavior()\r\n\r\n    x = tf.placeholder(tf.float32, shape=(None, 10))\r\n    restorer = tf.train.import_meta_graph('./foo.meta', input_map={'x:0': x})\r\n    y = tf.get_default_graph().get_tensor_by_name('y:0')\r\n    y_shape = tf.shape(y)\r\n    sess = tf.Session()\r\n    restorer.restore(sess, './foo')\r\n    [y_, y_shape_] = sess.run(['y:0', y_shape], {x: np.zeros((1, 10), np.float32)})\r\n    assert np.all(y_.shape == y_shape_), 'inconsistent sizes'\r\n\r\nThis results in an `AssertionError: inconsistent sizes`, because `y_shape_` still returns the old batch size of 128, despite the output `y` being computed as expected with a batch size of 1.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "Done", "@allenlavoie Problem still persists in TF 1.11.", "I am also affected by this -- is there an official way to save a model after training while keeping it independent of the batch size?", "Is there any progress ? I also troubled by this when I want to put images with different size into the frozen model.", "Rerunning shape inference does seem like a good thing to do with an input_map. I'm not going to get to this (if it wasn't obvious from the months where I haven't gotten to it). Happy to advise if someone wants to try a fix.", "@allenlavoie I'm interested in this issue and try to fix it. Could you give some advice? They will be very helpful.", "@feihugis \r\n\r\nLooking at the code, @skye is going to be better versed in this than I am, but here's a guess at what needs doing:\r\n\r\nRight now we have a ShapeRefiner, which calls SetShape on nodes: https://github.com/tensorflow/tensorflow/blob/038675489fbf35542f6b0e1b6192876a83e8c5b6/tensorflow/core/graph/graph_constructor.cc#L623\r\n\r\nThat decodes the shape proto and sets the node's shape. It doesn't run a full pass of shape inference using shape functions, it just loads the saved shapes. This is only valid if the input_map targets have shapes which are compatible with the existing shapes.\r\n\r\nSo one possibility is if we have an input_map we topologically sort the graph and propagate the input_map target shapes using shape functions, ignoring the saved shapes. To satisfy the requests in this thread, we'd propagate the new shapes even if they're incompatible with the old shapes.", "This is tricky. I'm wary about propagating new shapes from the input_map nodes, because this might overwrite any shapes from explicit Tensor.set_shape() calls.\r\n\r\nHere's an idea I haven't thought all the way through that I think would yield the desired behavior, but would be a lot of work: we only serialize the shapes of tensors that have been explicitly set via set_shape(). When we load the serialized graph, we use regular shape inference (which would propagate the input_map shapes) unless there's an explicit shape set, and then we call refiner_->SetShape(). I _think_ this is inline with the original intent of serializing the shapes.\r\n\r\n@tazr, if you need a workaround for this, I think manually removing the _output_shapes attrs from the GraphDef produced by saver.save would cause the new shapes to be propagated. I can give more info on how to do this you want, it's not super straightforward unfortunately.  ", "If you could provide any clue, it would be fantastic. This issue is still bugging me and has been regularly ever since I started loading models from meta graphs.", "I think the following script should remove the _output_shapes attributes from the saved model:\r\n```python\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\nmetagraph_file = sys.argv[1]\r\nmetagraph = tf.MetaGraphDef()\r\n\r\nwith open(metagraph_file) as f:\r\n  metagraph.ParseFromString(f.read())\r\n\r\nfor node in metagraph.graph_def.node:\r\n  if \"_output_shapes\" in node.attr:\r\n    del node.attr[\"_output_shapes\"]\r\n\r\nwith open(metagraph_file, \"w\") as f:\r\n  f.write(metagraph.SerializeToString())\r\n```\r\n\r\nIn your example above, you should pass `./foo.meta` as the argument (this is the serialized MetaGraphDef proto).", "Our GraphDef Editor library (a version of `contrib.graph_editor` that operates over serialized graphs) has facilities for importing a serialized graph, modifying the shape/dtype of an input placeholder, then propagating the resulting shape/dtype changes forward through the graph. See https://github.com/CODAIT/graph_def_editor/blob/master/examples/batch_size_example.py for an example that modifies batch size. The rewrite code invoked that example is [here](https://github.com/CODAIT/graph_def_editor/blob/5695fdc9381ee0f935d1ece7547e5bf111cae25e/graph_def_editor/rewrite.py#L41).", "Hi @tazr ,\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mohantym Unfortunately yes, this bug reported three years ago in TF 1.8 is still present today in TF 2.6.", "@tazr ,I was getting invalid arguement  error though in above code ,  Could you attach your collab gist here for others reference ?Attaching my [GIST](https://colab.research.google.com/gist/mohantym/7a3f8b9c1d1d25dd6b06267406613f2f/github_21185.ipynb) in TF 2.6 for reference.", "@mohantym It is because you are running both programs sequentially, rather than from two different processes, as expected. In a single collab you could perhaps simulate that by using different graphs:\r\n\r\n    from glob import glob\r\n    import sys\r\n    print('sys:', sys.version)\r\n    import tensorflow.compat.v1 as tf\r\n    print('tf:', tf.version.VERSION)\r\n    tf.disable_v2_behavior()\r\n\r\n    with tf.Graph().as_default():\r\n      batch_size = 128\r\n      x = tf.placeholder(tf.float32, shape=(batch_size, 10), name='x')\r\n      b = tf.Variable(tf.zeros((10)))\r\n      y = tf.add(x, b, name='y')\r\n\r\n      saver = tf.train.Saver()\r\n      with tf.Session() as sess:\r\n        tf.global_variables_initializer().run()\r\n        assert not glob('./foo')\r\n        saver.save(sess, './foo')\r\n     \r\n     \r\n    import numpy as np\r\n    import tensorflow.compat.v1 as tf\r\n    tf.disable_v2_behavior()\r\n\r\n    with tf.Graph().as_default():\r\n      x = tf.placeholder(tf.float32, shape=(None, 10))\r\n      restorer = tf.train.import_meta_graph('./foo.meta', input_map={'x:0': x})\r\n      y = tf.get_default_graph().get_tensor_by_name('y:0')\r\n      y_shape = tf.shape(y)\r\n      sess = tf.Session()\r\n      restorer.restore(sess, './foo')\r\n      [y_, y_shape_] = sess.run(['y:0', y_shape], {x: np.zeros((1, 10), np.float32)})\r\n      assert np.all(y_.shape == y_shape_), f'inconsistent sizes: {y_.shape}'\r\n"]}, {"number": 21130, "title": "Unexpected behavior of tf.hessians on graphs with tf.reduce_prod", "body": "### System information\r\n\r\nTensorFlow 1.9 CPU version installed via pip on Linux. Replicated the error on both Python 2.7.13 and 3.5.3 on Linux. Did not try with GPU version.\r\n\r\nDetails:\r\nHave I written custom code: no\r\nOS Platform and Distribution: Ubuntu 18.04\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.9 CPU version\r\nBazel version: n/a\r\nCUDA/cuDNN version: n/a\r\nGPU model and memory: n/a\r\nExact command to reproduce: see code below\r\nMobile device: no\r\n\r\n### Describe the problem\r\n``tf.hessians`` fails on a very simple function that uses ``tf.reduce_prod``. The error only occurs at the minimum of the function, where the Hessian is zero. Here's a minimal example:\r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    \r\n    x = tf.placeholder(tf.float32, shape=[3,])\r\n    y = tf.reduce_prod(x**2)\r\n    H = tf.hessians(y, x)[0]\r\n    \r\n    with tf.Session() as sess:\r\n      print(sess.run(H, feed_dict={x: np.ones(3)}))\r\n      print(sess.run(H, feed_dict={x: np.zeros(3)}))\r\n      \r\n    # Produces\r\n    # [[2. 4. 4.]\r\n    #  [4. 2. 4.]\r\n    #  [4. 4. 2.]]\r\n    # [[nan nan nan]\r\n    #  [nan nan nan]\r\n    #  [nan nan nan]]\r\n\r\nThe Hessian at ``x=[0, 0, 0]`` is well-defined and should evaluate to a zero-matrix. This behavior is unexpected. If ``tf.hessians`` can't handle ``tf.reduce_prod`` it should raise an exception.\r\n\r\n### Source code / logs\r\nSee above for minimal example.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Updated issue with missing information.", "Looks like the last several changes to hessians have been from github.   Maybe this is community contributions welcome.  @drpngx: any opinion?", "Yes, that sounds like a really good idea.", "Just pining this Issue to see if there has been any development on this, as it looks like this is [affecting our project as well](https://github.com/diana-hep/pyhf/issues/332)."]}, {"number": 21053, "title": "Session.run() on Operation return None. Design question", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, see this issue. https://github.com/tensorflow/tensorflow/issues/20990\r\n\r\nThe documentation of Session.run from tensorflow:\r\n```\r\nrun(\r\n    fetches,\r\n    feed_dict=None,\r\n    options=None,\r\n    run_metadata=None\r\n)\r\n```\r\n> Runs operations and evaluates tensors in fetches.\r\n> \r\n> This method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values.\r\n> \r\n> The fetches argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, dict, or OrderedDict containing graph elements at its leaves. A graph element can be one of the following types:\r\n> \r\n>     **An tf.Operation. The corresponding fetched value will be None.**\r\n>     A tf.Tensor. The corresponding fetched value will be a numpy ndarray containing the value of that tensor.\r\n>     A tf.SparseTensor. The corresponding fetched value will be a tf.SparseTensorValue containing the value of that sparse tensor.\r\n>     A get_tensor_handle op. The corresponding fetched value will be a numpy ndarray containing the handle of that tensor.\r\n>     A string which is the name of a tensor or operation in the graph.\r\n\r\nI was stuck for quite a while on this issue, due to a` sess.run() `returning `None` all the time and not raising any exception.\r\nI don't quite get the design intention under such a behaviour:\r\n`Session.run()` is expected to return values. But it return `None` for `tf.operation`\r\nNot raising any exception, nor outputing any information make you think it is okay to call `sess.run()` on your parameter and that the issue is not on this function call.\r\nWhy is it okay to allow user to call this method with parameter that will result in no effect?\r\nAlso, as `tf.Operation` is a tensorflow type, wouldn't it be possible to automatically call for its `.values()` in the `Session.run()` instead of returning a `None`?\r\n", "comments": ["> Why is it okay to allow user to call this method with parameter that will result in no effect?\r\n\r\nReturning `None` does not mean resulting in no effect. Calling an op usually has an effect.\r\n\r\n> wouldn't it be possible to automatically call for its .values() in the Session.run() instead of returning a None?\r\n\r\nIt won't be possible because not every op has output values.", "Okay, that make sense, Indeed I had forgotten about side-effect nodes as I was mostly using value nodes.\r\nBut then, wouldn't it be possible to check if the node does have a side effect?\r\nThen:\r\nif it has a side effect: run it,\r\nif it has no side effect: either if it has a value or tell user that it called run on an effect-less parameter?\r\n\r\nThe behavior:\r\n\"I'm tasked to do something\"\r\n\"I do it silently\" Is ok\r\nbut the behavior:\r\n\"I'm tasked with a useless task\"\r\n\"I do nothing but silently\" does not feel natural to me.\r\n\r\nBut I guess this behavior mainly depend on the complexity of determining \"**usefulness**\" of a node (as in value-returning or side-effect-ing)", "I'm not sure what ops silently do nothing and return nothing, do you have an example?\r\n\r\nIn general, it's sometimes hard to detect these things. There can be `while_loop`s with zero iterations, for instance, but sometimes they are there for a reason (e.g. if/then/else can be implemented with two while loops).", "There are many types of tf.Operation, some of them does return None, and it serves as grouping ops together, example being training ops returned by optimizer.minimize. Just returning nothing does not mean it's doing nothing, it actually run all the ops that are being grouped.", "> I'm not sure what ops silently do nothing and return nothing, do you have an example?\r\n\r\nDoesn't running a `predict node`  via `node` instead of `node.values` does that?\r\nMaybe it computes the value internally, but from the user standpoint: the `state of the model` didn't change and `no value was returned` (technically `None` was returned).\r\n\r\n> There can be `while_loops` with zero iterations, for instance, but sometimes they are there for a reason\r\n\r\nI guess they can do nothing, but they have a **possibility** of doing something. (or if they don't, maybe a hint at the uselessness of the node would be nice)\r\n\r\n> There are many types of` tf.Operation`, some of them does return `None`, and it serves as `grouping ops `together, example being `training ops` returned by `optimizer.minimize`. Just returning nothing does not mean it's doing nothing, it actually `run` all the `ops` that are being grouped.\r\n\r\nYou are right about it **currently** returning `None`. \r\nBut isn't it be plausible to return a `status` of the node operation? like the type of node or such?\r\nI think that it is desirable for its added knowledge of state of execution and not so much for its overhead cost."]}, {"number": 20866, "title": "Why tf.losses.softmax_cross_entropy doesn't have \"dim\" (axis) argument?", "body": "Hi!\r\nThe function \"tf.losses.softmax_cross_entropy\" calls \"nn.softmax_cross_entropy_with_logits_v2\" which has a dim argument with a default value of -1.\r\nCan we add the same \"dim\" or \"axis\" parameter to the calling function \" tf.losses.softmax_cross_entropy\", so that we can choose which axis to apply the softmax  ?\r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code N/A\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A", "We currently don't have anybody working on this API. It would be great if you could help us by implementing the change that you propose and submitting a PR. Let us know if you need further clarification. Thanks!\r\n/cc @alextp @ispirmustafa ", "I think its a simple change. I am new to this repository and I would like to work on this issue.", "Just FYI there is already such implementation on keras categorical_crossentropy:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L3461", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "I'm wondering if this issue should be closed given that `tf.losses` is deprecated in TensorFlow 2.X, replaced instead by `tf.keras.losses` as pointed out [here](https://github.com/tensorflow/tensorflow/pull/21996#issuecomment-486023491)."]}, {"number": 20773, "title": "tf.Print related wired bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 (latest)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: r1.9 (v1.9.0-0-g25c197e023)\r\n- **Python version**: Python 3.6.5 |Anaconda, Inc.| [MSC v.1900 64 bit (AMD64)] on win32\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 64_7\r\n- **GPU model and memory**: GTX 1060-6GB(Laptop)\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nWhile developing a prime number generator using `tensorflow-gpu`, I came across a wired bug. Given the source code, if you use `tf.Print` result is correct and run-time is less than without using `tf.Print`. \r\n\r\n### Source code / logs\r\n``` python\r\n# check first 'n' numbers for primity using tensorflow\r\n\r\nimport warnings\r\nwith warnings.catch_warnings():\r\n    warnings.filterwarnings(\"ignore\",category=FutureWarning)\r\n    import h5py\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\ndef prime_tf_graf(max_count,dump):\r\n\tttyp = tf.int32\r\n\tdata = tf.range(2, max_count, dtype=ttyp)\r\n\tif dump:\r\n\t\tdata = tf.Print(data, [data], 'Data Loaded!')\r\n\tX, Y = tf.meshgrid(data, data); \r\n\ttemp = Y%X\r\n\tmask = tf.cast(tf.equal(temp, Y), dtype=ttyp)\r\n\ttemp = temp - Y * mask\r\n\ttemp = tf.cast(tf.not_equal(temp, 0), dtype=ttyp)\r\n\tsumr = tf.reduce_sum(temp, axis=1)\r\n\tnums = data - 2\r\n\trato = tf.cast(sumr/nums, ttyp)\r\n\tindx =  tf.cast(tf.not_equal(rato, 0), ttyp)\r\n\tshap =  tf.reshape(tf.where(indx), [-1]) + 2\r\n\treturn shap\r\n\r\ndef prime_tf(max_count,dump):\r\n\twith tf.device('/gpu:0'):\r\n\t\tgraf = prime_tf_graf(max_count, dump)\r\n\tsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\r\n\tsess.run(tf.global_variables_initializer())\r\n\tprim = sess.run(graf)\r\n\treturn prim\r\n\r\ndef bench(task):\r\n\tnow = time.time()\r\n\tres = task()\r\n\treturn res, time.time() - now\r\n\r\np, t = bench(lambda: prime_tf(10000, True))\r\nprint('With Dump: found %d primes in %f time'%(len(p), t))\r\np, t = bench(lambda: prime_tf(10000, False))\r\nprint('Without Dump: found %d primes in %f time'%(len(p), t))\r\n```\r\nand here is output on my system:\r\n\r\n```\r\nPS E:\\Research\\TFLearn> python .\\bug.py\r\nData Loaded![2 3 4...]\r\nWith Dump: found 1229 primes in 12.599950 time\r\nWithout Dump: found 1085 primes in 25.162465 time\r\n```\r\n", "comments": ["What happens if you switch ttyp from tf.int32 to tf.int64?  I.e. don't try to use int32 on GPU.", "Still similar response with performance is worse this time:\r\n```\r\nData Loaded![2 3 4...]\r\nWith Dump: found 1229 primes in 21.802515 time\r\nWithout Dump: found 1085 primes in 109.847477 time\r\n```\r\n", "I consistently get a div-by-0 error at:\r\n\r\n\trato = tf.cast(sumr/nums, ttyp)", "That may be possible but on my system, it doesn't give any error! \r\nUsing this instead: `rato = tf.cast(tf.equal(sumr, nums), ttyp)` gives correct result but performance is still worse without `tf.Print`:\r\n```\r\nData Loaded![2 3 4...]\r\nWith Dump: found 1229 primes in 13.345328 time\r\nWithout Dump: found 1229 primes in 22.766805 time\r\n```\r\nI think it may be because tf.Print somehow forces tensor to load quickly but that's just my assumption.", "@aaroey Can you take a look at this.", "Hi @NaxAlpha ! I am getting the same value now in 2.8 version . Attaching [gist ](https://colab.sandbox.google.com/gist/mohantym/8c8c11a533d5fe08f4558576bd6631b5/untitled349.ipynb)for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 20761, "title": "Error occurs when I create subprocess after creating tf.train.Server with RDMA.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Redhat 7.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: CUDA-9.0/cuDNN-7.0.5\r\n- **GPU model and memory**: Tesla-P100\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI compile the TensorFlow-v1.8.0 with RDMA support and modify the mnist_replicas.py based on: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py\r\n\r\n```\r\n...\r\n  if not FLAGS.existing_servers:\r\n    # Not using existing servers. Create an in-process server.\r\n    server = tf.train.Server(\r\n        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')\r\n    if FLAGS.job_name == \"ps\":\r\n      server.join()\r\n...\r\n```\r\n\r\nIt works just fine.\r\nThen I try to create a subprocess after `tf.train.Server` is created which leads to an error:\r\n\r\n```\r\n...\r\n  if not FLAGS.existing_servers:\r\n    # Not using existing servers. Create an in-process server.\r\n    server = tf.train.Server(\r\n        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')\r\n\r\n    import multiprocessing; multiprocessing.Process(target=time.sleep, args=(100,)).start()\r\n\r\n    if FLAGS.job_name == \"ps\":\r\n      server.join()\r\n...\r\n```\r\n\r\n Here is the log:\r\n\r\nworker:\r\n```\r\n...\r\njob name = worker\r\ntask index = 0\r\n2018-07-13 09:22:57.115547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:b5:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.27GiB\r\n2018-07-13 09:22:57.115607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Ignoring visible gpu device (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:b5:00.0, compute capability: 6.0) with Cuda compute capability 6.0. The minimum required Cuda capability is 7.0.\r\n2018-07-13 09:22:57.115628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-13 09:22:57.115636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0\r\n2018-07-13 09:22:57.115644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N\r\n2018-07-13 09:22:57.116866: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 172.30.96.109:49998}\r\n2018-07-13 09:22:57.116883: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:49999}\r\n2018-07-13 09:22:57.118450: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 172.30.96.109:49998}\r\n2018-07-13 09:22:57.118468: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:49999}\r\n2018-07-13 09:22:57.125228: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\r\n2018-07-13 09:22:57.127868: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:49999\r\n2018-07-13 09:22:57.134897: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0\r\n2018-07-13 09:22:57.770456: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_rdma_bfc\r\n2018-07-13 09:22:57.770480: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_pool\r\nWARNING:tensorflow:From mnist_replica_ib.py:217: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nWorker 0: Initializing session...\r\n2018-07-13 09:22:58.064007: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 6cb605bf203f3160 with config: device_filters: \"/job:ps\" device_filters: \"/job:worker/task:0\" allow_soft_placement: true\r\n2018-07-13 09:22:58.090607: F tensorflow/contrib/verbs/rdma.cc:691] Check failed: iter != request_table_.end()\r\nAborted\r\n```\r\n\r\nps\r\n```\r\n...\r\njob name = ps\r\ntask index = 0\r\n2018-07-13 09:22:54.747787: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n2018-07-13 09:22:54.747853: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: bms-60c5\r\n2018-07-13 09:22:54.747865: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: bms-60c5\r\n2018-07-13 09:22:54.747923: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 390.67.0\r\n2018-07-13 09:22:54.747988: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 390.67.0\r\n2018-07-13 09:22:54.748000: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 390.67.0\r\n2018-07-13 09:22:54.750241: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:49998}\r\n2018-07-13 09:22:54.750261: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 172.30.96.109:49999}\r\n2018-07-13 09:22:54.751826: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:49998}\r\n2018-07-13 09:22:54.751843: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 172.30.96.109:49999}\r\n2018-07-13 09:22:54.759391: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\r\n2018-07-13 09:22:54.762222: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:49998\r\n2018-07-13 09:22:57.770347: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:0\r\n2018-07-13 09:22:57.770929: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_rdma_bfc\r\n2018-07-13 09:22:57.770944: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_pool\r\n```\r\n\r\nThen I  try creating subprocess before `tf.train.Server` is created, it works without any error. (worker will stuck after finish training because subprocess is still sleeping.)\r\nCode:\r\n```\r\n...\r\n  if not FLAGS.existing_servers:\r\n    # Not using existing servers. Create an in-process server.\r\n    import multiprocessing; multiprocessing.Process(target=time.sleep, args=(100,)).start()\r\n    server = tf.train.Server(\r\n        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index, protocol='grpc+verbs')\r\n    if FLAGS.job_name == \"ps\":\r\n      server.join()\r\n...\r\n```\r\n\r\nAny idea?", "comments": ["@bignamehyp anyone help?", "@eladweiss Mind to take a look?", "I've seen the same error before.  Try setting env var `RDMAV_FORK_SAFE=y`.  See https://www.rdmamojo.com/2012/05/24/ibv_fork_init/ for details.", "@jeffdaily \r\n\r\nThanks for the reply. It works.\r\nBut I set env var `RDMAV_FORK_SAFE=y` and I got this new error:\r\n\r\n```\r\n...\r\n2018-12-06 15:00:47.363509: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 169.254.128.213:49901, 1 -> 169.254.128.134:49901}\r\n2018-12-06 15:00:47.363554: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:49900, 1 -> 169.254.128.134:49900}\r\n2018-12-06 15:00:47.368111: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 169.254.128.213:49901, 1 -> 169.254.128.134:49901}\r\n2018-12-06 15:00:47.368130: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:49900, 1 -> 169.254.128.134:49900}\r\n2018-12-06 15:00:47.376606: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\r\n2018-12-06 15:00:47.382908: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:332] Started server with target: grpc://localhost:49900\r\n2018-12-06 15:00:47.393284: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:worker/replica:0/task:1\r\n2018-12-06 15:00:47.395347: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:1\r\n2018-12-06 15:00:47.396658: I tensorflow/contrib/verbs/rdma_mgr.cc:128] Connected to remote node /job:ps/replica:0/task:0\r\n2018-12-06 15:00:49.238119: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cuda_host_bfc\r\n2018-12-06 15:00:49.238150: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_pool\r\n2018-12-06 15:00:49.238161: I tensorflow/contrib/verbs/rdma_mgr.cc:311] Instrumenting CPU allocator cpu_rdma_bfc\r\nWARNING:tensorflow:AsyncRawReader will be deprecated soon. Use AsyncRawGenerator instead. AsyncRawGenerator only provide a generator of dataset instead of providing a tf.data.Dataset.\r\nWARNING:tensorflow:AsyncRawReader will be deprecated soon. Use AsyncRawGenerator instead. AsyncRawGenerator only provide a generator of dataset instead of providing a tf.data.Dataset.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-12-06 15:01:13.171827: I tensorflow/core/distributed_runtime/master_session.cc:1136] Start master session 40860f7a01c67251 with config: intra_op_parallelism_threads: 1 inter_op_parallelism_threads: 40 gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true graph_options { optimizer_options { } }\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Running will end at step: 50\r\nINFO:tensorflow:step: 0(global step: 0)\tsample/sec: 16.727\tent_loss: 0.852\ttop1: 0.297\treg_loss: 1.069\ttotal_loss: 1.920\r\nmlx5: newpod-v100-1120-2-0015: got completion with error:\r\n00000000 00000000 00000000 00000000\r\n00000000 00000000 00000000 00000000\r\n00000000 00000000 00000000 00000000\r\n00000000 92005204 090019ec 0732edd3\r\n2018-12-06 15:01:58.130742: F tensorflow/contrib/verbs/rdma.cc:451] Check failed: wc_[i].status == IBV_WC_SUCCESS Failed status \r\nlocal protection error 4 -402646544 82\r\n```\r\n\r\nAny ideas ?", "Hi @sleepfin! 1.x versions are not supported any more. You can use our [migration document ](https://www.tensorflow.org/guide/migrate)to use your 1.x code in 2.x version. Please create a new issue if it still persists in 2.7 version or later. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 20624, "title": "Makefile: build_all_ios.sh - No rule to make target 'distclean'.  Stop.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.5\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**:  -\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**:  just execute ./build_all_ios.sh\r\n\r\nHello, \r\n\r\nwhen building TensorFlow for iOS using the Makefile (./build_all_ios.sh), I am getting the following error:\r\n\r\n................\r\nrm -f gogo/cpp_no_group/datasets/google_message1/proto2/.deps/cpp_no_group_benchmark-benchmark_message1_proto2.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message1/proto3/.deps/cpp_no_group_benchmark-benchmark_message1_proto3.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message2/.deps/cpp_no_group_benchmark-benchmark_message2.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_1.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_2.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_3.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_4.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_5.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_6.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_7.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message3/.deps/cpp_no_group_benchmark-benchmark_message3_8.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4_1.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4_2.pb.Po\r\nrm -f gogo/cpp_no_group/datasets/google_message4/.deps/cpp_no_group_benchmark-benchmark_message4_3.pb.Po\r\nrm -f python/.deps/libbenchmark_messages_la-python_benchmark_messages.Plo\r\nrm -f util/.deps/gogo_data_scrubber-gogo_data_scrubber.Po\r\nrm -f util/.deps/protoc_gen_gogoproto-protoc-gen-gogoproto.Po\r\nrm -f Makefile\r\nMaking distclean in third_party/googletest\r\nmake[1]: *** No rule to make target `distclean'.  Stop.\r\nmake: *** [distclean-recursive] Error 1", "comments": ["The issue is in `tensorflow/contrib/makefile/compile_ios_protobuf.sh`", "I'm also getting same in master branch. Should I pull code from branch r1.9?", "@laxmansahni, I haven't tested branch r1.9. The error happens to me in the master branch.", "I got the following error against branch r1.9.\r\n**OS Platform and Distribution**: macOS High Sierra 10.13.4\r\n\r\n2 errors generated.\r\nmake: *** [/Users/laxmansahni/Downloads/tensorflow-r1.9/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/graph_runner.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n2 errors generated.\r\nmake: *** [/Users/laxmansahni/Downloads/tensorflow-r1.9/tensorflow/contrib/makefile/gen/obj/ios_I386/tensorflow/core/common_runtime/local_device.o] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'i386 compilation failed.'\r\ni386 compilation failed.\r\n+ exit 1", "@laxmansahni, have a look at #5297, it may help.", "Code of tensorflow/contrib/makefile/compile_ios_protobuf.sh\r\n\r\n\r\nbuild_target() {\r\n--\r\n117 | case \"$1\" in\r\n118 | i386)  make distclean\r\n\u2026 | \u00a0\r\n148 | package_pb_library \"iossim_386\"\r\n149 | ;;\r\n150 | \u00a0\r\n151 | x86_64) make distclean\r\n152 | ./configure \\\r\n153 | --host=x86_64-apple-${OSX_VERSION} \\\r\n\r\n\r\n", "I tried all steps mentioned in bug #5297. However, it didn't work out.", "Same error described by @JaviBonilla in the 1st post, I'm running the script from MacOS HighSierra 10.13.6 and I have the latest TF version from master branch.", "Same error...", "I have the same error~~ \r\nCan any one give me the solve?", "@gunan can you please take a look? Thanks", "@petewarden has created this build, I have very little experience with it.", "I have the same error~~\r\nCan any one give me the solve?", "vi tensorflow/contrib/makefile/compile_ios_protobuf.sh\r\nchange:\r\n`make distclean`\r\nto:\r\n```\r\nmake distclean-am\r\nmake clean\r\n```\r\nthen it works", "@toniz What did you mean?\r\n\r\n`make distclean ./configure`\r\n\r\nto\r\n\r\n`make distclean-am ./configure`\r\n\r\nand where to insert ?\r\n\r\n`make clean`\r\n\r\n`make distclean` appears five time starting in line 118\r\n\r\nI got this:\r\n\r\n`fatal error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/lipo: /Users/MYUSER/Documents/MYFOLDER/dev/code_patterns/object-detection-anki-overdrive-cars/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_x86_64/lib/libprotobuf.a and /Users/MYUSER/Documents/MYFOLDER/dev/code_patterns/object-detection-anki-overdrive-cars/tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a have the same architectures (x86_64) and can't be in the same fat output file\r\n`", "> 118     i386)  make distclean-am\r\n119         make clean\r\n120         ./configure \\\r\n\r\n> 152     x86_64) make distclean-am\r\n153         make clean\r\n154         ./configure \\\r\n\r\nlike this.  All instead.\r\n\r\nIf i386 Failed, BUILD_TARGET remove i386.\r\n>  31 BUILD_TARGET=\"x86_64 armv7 armv7s arm64\" \r\n\r\n\r\nThe result:\r\n>  lipo -info gen/lib/libprotobuf.a\r\nArchitectures in the fat file: gen/lib/libprotobuf.a are: armv7 armv7s x86_64 arm64", "@toniz  Thanks, cool it works for me with this instructions.\r\nNo errors :-)\r\n`checking whether the g++ linker (/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ld) supports shared libraries... yes`\r\n"]}, {"number": 20567, "title": "Feature request: verify py_func tensor's shape when evaluating", "body": "`tf.py_func(my_py_func, args, dype)` returns a tensor of known type but unknown shape.\r\nWhen my_py_func runs, py_func will check that the type is what's expected and fail otherwise.\r\n\r\nBut, if you set an incorrect shape with .set_shape there's no warning. Having a loud warning would have saved me a few hours yesterday.\r\n\r\nMinimal example;\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef my_py_func(x):\r\n  return x\r\n\r\ndef main(argv):\r\n  del argv\r\n\r\n  print(tf.GIT_VERSION, tf.VERSION)\r\n\r\n  t = tf.constant(0, dtype=tf.int64)\r\n  print('tf.constant(0):', t)\r\n  t = tf.py_func(my_py_func, [t], tf.int64)\r\n  print('tf.py_func:', t)\r\n  t.set_shape([100,100])\r\n  print('t.set_shape:', t)\r\n\r\n  with tf.Session() as sess:\r\n    t = sess.run(t)\r\n    print('sess.run(t)', t)\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\nOutput;\r\n```\r\n$ python using_py_func.py\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\ntf.constant(0): Tensor(\"Const:0\", shape=(), dtype=int64)\r\ntf.py_func: Tensor(\"PyFunc:0\", dtype=int64)\r\nt.set_shape: Tensor(\"PyFunc:0\", shape=(100, 100), dtype=int64)\r\nsess.run(t) 0\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: yes\r\nOS Platform and Distribution: all\r\nTensorFlow installed from pip\r\nTensorFlow version 1.8\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce: python using_py_func.py", "Thanks for the suggestion and the report.\r\n\r\nAs you probably guessed, `tf.py_func` can't return a known shape since the shape of the returned tensor depends on Python code evaluated during graph execution. [`set_shape`](https://www.tensorflow.org/api_docs/python/tf/Tensor#set_shape) is meant to fill in gaps in shape inference at graph construction time.\r\n\r\nPerhaps you're suggesting that an error be thrown at graph execution time (`sess.run()`)?\r\nFWIW, this isn't specific to `py_func`. For example, the following demonstrates similar behavior with `placeholder`:\r\n\r\n```python\r\nx = tf.placeholder(tf.float32)  # Unknown shape\r\ny = tf.identity(x)\r\ny.set_shape([100, 100])\r\nprint(x)\r\nprint(y)\r\nwith tf.Session() as sess:\r\n  print(sess.run(y, feed_dict={x:10}))\r\n```\r\n\r\nOr with other functions where the shape can only be known at graph construction time, like `tf.unique`\r\n\r\n```python\r\nx = tf.constant([1, 1, 2])\r\ny, idx = tf.unique(x)\r\ny.set_shape([100])\r\nprint(x)\r\nprint(y)\r\nwith tf.Session() as sess:\r\n  print(sess.run(y))\r\n```\r\n\r\nOff the top of my head I'm not sure how to spec out the feature request here.", "Yes, I'm aware why py_func cannot know the shape.\r\nIndeed the idea was to print a warning during `sess.run(...)` since it should be able to spot the inconsistency between what was said and then actually was.\r\n\r\nI didn't consider that py_func isn't at all special here. As you demonstrate, the same can be true for any tensor.\r\n\r\nIn your last example;\r\n\r\n```python\r\nx = tf.constant([1, 1, 2])\r\ny, idx = tf.unique(x)\r\ny.set_shape([100])\r\nprint(x)\r\nprint(y)\r\nwith tf.Session() as sess:\r\n  print(sess.run(y))\r\n```\r\n\r\na warning would be issued for y not having the shape it was said to have.\r\n\r\nTensors either have a know, partially unknown or unknown shape. If it's unknown, fine, there are no guarantees.\r\nBut if the shape has been provided, when evaluated it should be what's expected, no?\r\nSpec: tensors of known or partially known shapes should actually have that shape when evaluated.\r\nDoes that sound reasonable?", "An interesting suggestion. If I may, an alternative way to phrase you proposal is that `y.set_shape([100])` should have the same effect as:\r\n\r\n```python\r\nwith tf.control_dependencies([tf.assert_equal(tf.shape(y), [100])]):\r\n  y = tf.identity(y)\r\n```\r\n\r\n(a little more sophistication for handling partially known shapes as well, but the general idea being to use [assertion operations](https://www.tensorflow.org/api_guides/python/check_ops) to validate shapes at graph execution time).\r\n\r\nSo far, the shape of the symbolic `Tensor` object has only been used as a hint to help catch errors during graph construction. For most operations this is enough since shapes can be statically determined then. However, there are handful of operations where this is not possible and I can see how validation at runtime can be useful. That said, I don't think the costs (both additional overhead in graph execution and the time to implement it) outweigh the benefits enough for us to prioritize this at this moment.\r\n\r\nFor now, I'm going to mark this as \"Contributions Welcome\" with the understanding that if someone has the inclination to take this on, we'd be happy to discuss a detailed proposal. Sound reasonable?", "Sounds good.", "@asimshankar  Interested in working on this issue. Need a little direction"]}, {"number": 20519, "title": "[Feature Request] Graph Transform Tool support SavedModel as Input", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nCurrently [Graph Transform Tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md) only support GraphDef as input. People normally save model using SavedModel or ckpt file. Maybe it would be better to support SavedModel as input.\r\n", "comments": ["@petewarden are you the owner of the graph transform tool?", "This is a feature we're not likely to get to soon, so adding to contributions welcome.", "Our GraphDef Editor library (a version of `contrib.graph_editor` that operates over serialized graphs) can import and export SavedModel files. You can find example usage [here](https://github.com/CODAIT/graph_def_editor/blob/5695fdc9381ee0f935d1ece7547e5bf111cae25e/examples/batch_size_example.py#L83). The main project page is at https://github.com/CODAIT/graph_def_editor\r\n\r\nAfter importing to a `graph_def_editor.Graph` object, you can extract the GraphDef representation of the graph by calling `graph_def_editor.Graph.to_graph_def()`, then pass that GraphDef protobuf to `graph_transforms.TransformGraph()` (see example code [here](https://github.com/CODAIT/graph_def_editor/blob/5695fdc9381ee0f935d1ece7547e5bf111cae25e/examples/mobilenet_example.py#L195). \r\n\r\nYou may need to reinstate some of the auxiliary SavedModel metadata to produce a valid SavedModel file after invoking the Graph Transform Tool. `graph_def_editor.Graph` has facilities for attaching things like collections and variables to a bare GraphDef, then writing out a SavedModel file.", "I am working on a PR for this issue that will add support for what should be the common case of this feature: load a single function/meta-graph signature from a SavedModel, feed the graph for that signature through the Graph Transform Tool, wrap the resulting graph in the original signature, and write out a new SavedModel.", "Update: I have this capability mostly implemented in my branch. The new SavedModel APIs are a wee bit rough around the edges, but I'm able to make do.\r\n\r\n@petewarden I'm curious what your intentions are with regard to running the Graph Transform Tool over models that have embedded functions. The V2 SavedModel APIs produce serialized models where nearly all the functionality of the model is hidden behind `FunctionDef` protos.", "Update: I have finished my implementation. But at the very end of this process, I built a copy of TensorFlow with the \"--define=tf_api_version=2\" flag and discovered that there was no Graph Transform Tool in the build output! I'm now very confused. Is this issue that I have been working on for the past 3 weeks actually an invalid issue?", "Thanks for your work! @frreiss \r\n@petewarden Is Graph Transform Tool part of TF 2.0?"]}, {"number": 20461, "title": "[Feature Request] Inverse Wishart Distribution", "body": "Mathematically, the InvWishart(X) = Wishart(X^-1)\r\n\r\nSince there is already InverseGamma in tf.contrib.distributions, it should be straightforward to implement InverseWishart. \r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n", "@bignamehyp  Can I have a crack at this?"]}, {"number": 20407, "title": "fused_batch_norm's gradient implementation is incomplete", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:N/A\r\n- **TensorFlow installed from (source or binary)**:N/A\r\n- **TensorFlow version (use command below)**:N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\nWhen `is_training=True`, the op looks like:\r\n```python\r\ny, batch_mean, batch_var = tf.nn.fused_batch_norm(x, scale, offset)\r\n```\r\nwhere `y`, `batch_mean` , and `batch_var` all depend on `x`. However the gradients of `batch_mean` and `batch_var` on `x` are currently None.\r\n\r\nWhen `is_training=False`, the op looks like:\r\n```python\r\ny, _, _ = tf.nn.fused_batch_norm(x, scale, offset, mean, variance)\r\n```\r\nwhere `y` depend on all the inputs. However currently the gradients of `y` on `mean` and `variance` is None.\r\n\r\nBoth cases are not commonly used in practice, but mathematically there should be gradients.", "comments": ["@rmlarsen Could you take a look at this?", "Please implement gradient calculation for y depend on mean and variance for tf.nn.fused_batch_norm when is_training is set to False.\r\n\r\nI am implementing a cross-gpu batch normalization strategy for segmantic segmentation and hope to use this fused batch norm to perform batch normalization using mean and variance calculated across multiple gpu. \r\n\r\nHowever, fused_batch_norm do not return gradient for y depending on mean and variance if mean and variance are given. I do not use the tf.nn.batch_normalization because it consume much more memory than fused_bathc_norm. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Why does the status of the issue become \"awaiting response\"? It's a feature that's awaiting implementation."]}, {"number": 20342, "title": "distributed training with SyncReplicasOptimizer got stuck after a number of iterations", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nI am running distributed training using SyncReplicasOptimizer, after about 10k iterations, the workers got stuck. CPU usage drops to 0 percent. \r\n\r\nThe arguments for SyncReplicasOptimizer:\r\nreplicas_to_aggregate = 60, total_num_replicas = 64 (I have 64 workers)\r\n\r\nIt might also be worth noting that this happens after 27 workers finish their training data.\r\n\r\nConnecting to one of the stuck worker processes using gdb I get the following backtraces:\r\n\r\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\n#1  0x00007f5813609de4 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007f58136095b1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007f5813606af4 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007f5813607015 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007f58111a4b23 in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f58111a54ab in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f5811187256 in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007f581118767d in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::string const&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007f5811187ceb in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007f5811468dba in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007f58114699b6 in TF_SessionRun () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007f5811119256 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007f581111939a in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007f58110d5b3e in _wrap_TF_SessionRun_wrapper () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\nAny ideas ? Thanks!\r\n", "comments": ["/CC @josh11b, any ideas?"]}, {"number": 20280, "title": "Provide a way to build a Tensorflow wheel without a dependency on Tensorboard", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 6.3.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nWhen building a custom Tensorflow wheel from source there is currently no way to disable the resulting wheels dependency on `tensorboard`. Tensorboard is great during development but is not useful when running in a headless server environment. It brings with it a large number of dependencies, and as I believe a custom tensorflow build is likely to be used in this kind of environment and it would be great to be able to disable the `install_requires` dependency on tensorboard in this case.\r\n", "comments": ["Hi, @orf. I don't believe that TensorBoard will ever be separated from TensorFlow. The TensorFlow team has worked hard to keep the package small in size, and they want TensorFlow to come with a nice visualization tool.\r\n\r\nPlease see tensorflow#12567. It doesn't solve your question, but it may be useful in deciding how to continue.", "Hey @marshalhayes, thank you for your quick reply. Let me just elaborate a bit.\r\n\r\nI have no problem with Tensorboard being bundled with the default Tensorflow package but in the context of a server environment specifically I've elaborated on a few issues below. I only ask for the idea that when building a *custom* Tensorflow package the *option* is given to disable the dependency link somehow.\r\n\r\n1. The current stable tensorflow, via tensorboard, pulls in vulnerable versions of `html5lib` and `bleach`, in the case of `html5lib` there is a CVE from 2016 (`CVE-2016-9909`). This causes headaches with compliance - \"yes sir we are indeed running vulnerable data sanitization packages with known CVEs, but trust us it's not used. we swear\".\r\n\r\n2. These sub dependencies can cause conflicts in a server environment, especially with bleach and htm5lib, but also with werkzeug and markdown.\r\n\r\n3. Python packaging itself is a headache, and if you use something like pipenv it's not easy or even supported to exclude a subdependency in the way we want. Running `pip uninstall xyz` afterwards is a hack at best and one only exclusively used for this situation. Not great.\r\n\r\nAll of these are exacerbated in my mind as Tensorboard is not used *or* required in the situations we are using it. As we are building a custom Tensorflow package that matches our CPU features I would simply love a build flag to disable the dependency link.", "Nagging Assignee @michaelisard: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This would be an opportunity to create two packages: tensorflow-core, which only installs tensorflow (and which may in fact result in some tensorflow python endpoints not working) and tensorflow (which is a virtual package that has dependencies on tensorflow-core and tensorboard. In the future, we may split tensorflow-core into more packages.\r\n\r\nI am not fundamentally opposed to this, but \r\n\r\n* there haven't been terribly compelling reasons (beyond uninstalling after installing, there are other [ugly methods](https://stackoverflow.com/questions/33441033/pip-install-to-custom-target-directory-and-exclude-specific-dependencies) to exclude dependencies during installation)\r\n* some functionality in TensorFlow does depend on TensorBoard. All of the imports are guarded and should not lead to defects beyond the actual things that won't work, but we'd like to distribute a fully functional version. This can be fixed/minimized further.\r\n\r\nMost importantly though, I am hesitant to offer another package that people will come to depend on for a fixed set of functionality. For example, I wouldn't want people to get used to tensorflow-core coming with Estimators. We're in the process of separating tf.estimator into its own package. In the future, more such splits may happen (kernel libraries, ops libraries), and I think the exact dependency tree of all those packages needs to be an implementation detail. Maybe that's ok by calling them tensorflow_private_seekrit_kernels_do_not_depend_directly_on_this, but I think depending directly on this is exactly what you want to do.\r\n\r\nI think a tensorflow-server-only package which only contains what you need to run a tensorflow server is really what you want? As in, none of the Python API either?", "> This would be an opportunity to create two packages: tensorflow-core, which only installs tensorflow (and which may in fact result in some tensorflow python endpoints not working) and tensorflow (which is a virtual package that has dependencies on tensorflow-core and tensorboard. In the future, we may split tensorflow-core into more packages.\r\n\r\nI think this might be going too far, all I'm asking for is a `TENSORFLOW_DONT_INCLUDE_TENSORBOARD` bazel config option or something to just remove it from the build `setup.py`. Nothing too complex. Splitting them like you suggest does sound like a good idea in theory, but as you said it can come with problems. For now just allow us to remove a dependency if we so require. \r\n\r\nYou distribute Tensorflow as a lowest common denominator binary. Ok, that's fine and makes sense. So we should build it ourselves for our specific CPU features when running it at any scale. Ok, that's also fine and makes sense. But we should always always always include Tensorboard as a dependency? That's not fine, especially when it leads to conflicting dependencies and those dependencies have known CVE's against them.\r\n\r\nI'd love to switch to tensorflow-server, and I'm pushing for it, but... you know. Time.\r\n\r\nAs a side note, the new Tensorflow release (congratulations!) [triggers a defect in the `pipenv` dependency resolver](https://github.com/pypa/pipenv/issues/2596). The culprit? ... `tensorboard` (and a non-backtracking resolver). Please let me remove this from our custom builds without having to resort to horrible `sed` post build hacks.", "Yeah, that's reasonable. @yifeif do you have a sense of how easy this is to achieve using bazel? I suppose we could give setup.py a flag which we set using some bazel option, or something. \r\n\r\nOr, we could simply add a flag to setup.py, and you execute setup.py bdist_wheel yourself? That's definitely feasible and looks easy to do.\r\n\r\nI've added contributions welcome if you want to propose a specific way to do it.", "That should be possible. We are already using [flags](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/pip.sh#L195) to enable/disable things in setup.py for nightly. Is that something you are looking for @orf?", "Yep, that's exactly what I am looking for. Thanks!\r\n\r\nMaybe this is a separate ticket but customizing the name of the package via a bazel flag would be awesome as well. Currently I am relying on a particular bash script that is sourced before `setup.py` is executed to overwrite a bash variable in the script that calls it.\r\n\r\nI'm currently a bit short on time but I will try and propose a PR to do both, if that is OK?", "That sounds good, thanks!", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 20222, "title": "Use shallow clones from git repos in CMake build", "body": "Please add\r\n\r\n```cmake\r\nGIT_SHALLOW 1\r\nGIT_PROGRESS 1\r\n```\r\n\r\nto all cmake files cloning git repos in ``contrib/cmake/external`", "comments": ["https://github.com/tensorflow/tensorflow/pull/20525", "@meggamind,  these should be arguments to https://cmake.org/cmake/help/v3.6/module/ExternalProject.html . And please note that this will likely not work with tags (the probability of working may ge increased by increasing depthe), but the tradeof of using bleeding edge versions of depdndencies (I prefer bleeding edge anyway) for faster time is IMHO OK.", "@KOLANICH Got it, will make the changes", "@KOLANICH I updated the PR, Please let me know if any changes needs to be done", "1 these arguments should only be added to the files using git, for files downloading zips and tars they are useless.\r\n\r\nyou can find the files using git by usage of the variables GIT_REPOSITORY\r\n\r\n2 when using shallow clones with GIT_TAG fetches will likely fail\r\n\r\n3 So we need a cache bool variable controlling if \r\nGIT_TAG should be used or GIT_SHALLOW. GIT_PROGRESS should be added unconditionally since it just showes the progress.", "Sorry for the late answer, I have just noticed your message."]}, {"number": 20201, "title": "[FEATURE REQUEST] decode_csv - optionally skip records with empty required fields", "body": "The function `decode_csv` has a `record_defaults` parameter used to specify the default value for each field. An empty value indicates that the field is required. Currently, if a required field is empty in a record, an InvalidArgumentError is raised with a message indicating the offending record and field numbers.\r\n\r\nIt would be useful to have a new parameter in `decode_csv` that allows you to optionally skip input records with empty required fields. Some examples of why this feature would be helpful:\r\n\r\n- existing data files in which fields that you require are occasionally empty\r\n- data files that are used for multiple purposes that have different required fields\r\n\r\nObligatory issue template requested by tensorflowbutler (I assume that most are irrelevant, since this is a feature request):\r\n\r\nHave I written custom code: Not yet\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes, this feature is still one that I think would be useful."]}, {"number": 20162, "title": "Improper attribution for GPLv3 in non-existent directory", "body": "### Describe the problem\r\nYou are declaring GPLv3 attribution at https://github.com/tensorflow/tensorflow/blob/v1.8.0/third_party/eigen3/LICENSE#L1011 for a directory that you have removed. The license attributed is also improperly identified as GPLv3 but instead should be GPLv2 as per Eigen's own repo, https://github.com/eigenteam/eigen-git-mirror/blob/master/bench/btl/COPYING.\r\nThe later isn't of much concern since you already removed the directory, but the former shows that you need to update your attribution to correctly reflect what is in the package. Thanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code - no\r\nOS Platform and Distribution - N/A\r\nTensorFlow installed from - N/A\r\nTensorFlow version - 1.8.0\r\nBazel version - N/A\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A. I found this issue while I was digging through the repo.", "I think you are looking at the btl subset of eigen.\r\nFrom eigen website, I see that they also have GPLv3 license for the general package:\r\nhttps://github.com/eigenteam/eigen-git-mirror/blob/master/COPYING.GPL\r\nhttps://bitbucket.org/eigen/eigen/src/36d65672189a71a100340c1f675f763fd50d9e37/COPYING.GPL?at=default&fileviewer=file-view-default\r\n\r\nbut it looks like later versions of eigen changed to MPLv2. \r\n@rmlarsen @benoitsteiner , do the code under eigen3 we have come from head of eigen, or the files we have under eigen3 are a subset of eigen that have a different license?"]}, {"number": 20140, "title": "Find a bug in tensorflow", "body": "write the code below, and save it to  a ,ckpt as a model\r\n```\r\nimport tensorflow as tf`\r\nv1 = tf.Variable(tf.constant(1.0, shape=[1]), name = \"v1\")\r\nv2 = tf.Variable(tf.constant(2.0, shape=[1]), name = \"v2\")\r\nv3 = tf.Variable(tf.constant(3.0, shape=[1]), name = \"v3\")\r\nresult=v1+v2\r\nresult2= result + v3\r\n\r\ninit_op = tf.global_variables_initializer()\r\nsaver = tf.train.Saver()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\n    writer = tf.summary.FileWriter('./graphs/const_add', sess.graph)\r\n    saver.save(sess, \"Saved_model/model.ckpt\")`\r\n```\r\nthen in another .py\uff0c we restore the model from the model.ckpt file\r\n```\r\nimport tensorflow as tf\r\nsaver = tf.train.import_meta_graph(\"Saved_model/model.ckpt.meta\")\r\nwith tf.Session() as sess:\r\n    saver.restore(sess, \"Saved_model/model.ckpt\")\r\n    print (sess.run(tf.get_default_graph().get_tensor_by_name(\"add:0\")))\r\n    #sess.run(tf.assign(v1,[10])) #\u76f4\u63a5\u8fd9\u6837\u4f7f\u7528v1\uff0c\u4f1a\u63d0\u793av1\u6ca1\u6709\u5b9a\u4e49\r\n    \r\n    #with tf.variable_scope(\"\",reuse=tf.AUTO_REUSE):\r\n    with tf.variable_scope(\"\",reuse=False):\r\n        v1=tf.get_variable(name=\"v1\",shape=[1])\r\n        print(v1.name)\r\n    sess.run(tf.assign(v1,[10]))\r\n    \"\"\"\u2463\u8f93\u51fa\u6240\u6709\u53ef\u8bad\u7ec3\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u4e5f\u5c31\u662f\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\"\"\"\r\n    trainable_variables=tf.trainable_variables()\r\n    variable_list_name = [c.name for c in tf.trainable_variables()]\r\n    variable_list = sess.run(variable_list_name)\r\n    for k,v in zip(variable_list_name,variable_list):\r\n        print(\"variable name:\",k)\r\n        print(\"shape:\",v.shape)\r\n            #print(v) \r\n    \"\"\"\u2463\u8f93\u51fa\u6240\u6709\u53ef\u8bad\u7ec3\u7684\u53d8\u91cf\u540d\u79f0\uff0c\u4e5f\u5c31\u662f\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\"\"\"\r\n    print (sess.run(tf.get_default_graph().get_tensor_by_name(\"v1:0\")))\r\n    print (sess.run(tf.get_default_graph().get_tensor_by_name(\"add:0\")))\r\n    print (sess.run(tf.get_default_graph().get_tensor_by_name(\"add_1:0\")))\r\n    print (sess.run(tf.get_default_graph().get_tensor_by_name(\"v1_1:0\")))\r\n```\r\nthe results will be as below:\r\n![image](https://user-images.githubusercontent.com/7501074/41644510-d1957e98-74a0-11e8-9327-7e2cd5ccd438.png)\r\nwe will find that:\r\nif we restore some variables from the already existed model file \"\"Saved_model/model.ckpt.meta\")\",\r\nsuch as v1,v2,v3 in this example.\r\nit will influence the process of calling get_variable. Because of these two causes as below:\r\n1. the variables restored from the model file such as v1,v2 and v3 will not exist in the scope of get_variable, it means you can only use\r\n```\r\nwith tf.variable_scope(\"\",reuse=False):\r\n        v1=tf.get_variable(name=\"v1\",shape=[1])\r\n```\r\nand create a new variable.  you can not  reuse the restored variable v1 from the model file unless you define a v1 , before you restore from the model file. like below\r\n```\r\nv1=tf.get_variable(name=\"v1\",shape=[1])\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    saver.restore(sess, \"Saved_model/model.ckpt\")\r\n    print (sess.run(result))\r\n```\r\nthat is , you can not reuse the restored variable v1 which is from restoring the model file unless you define it befor you restore.\r\n2.  although tensorflow doesnot allow reusing the restored variable v1 which is from restoring the model file if you don't define v1 before you restore the model file.\r\nBut if you call get_varialbe after you restore the model file, it will create a variable whose name is \"v1_1\" but not as name='v1' which you specify.\r\n    in my opinion, it should be corrected because it is so confusing.  how to correct it?\r\ni think get_variable should also reuse the variables which is loaded by restoring some model file.\r\nthe last sentence is what i finally want to say. \r\nMy english is to bad, you can run the code i offer and will find what i want to convey. \r\nThanks.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution\uff1awindows10\r\nTensorFlow installed from\uff1ausing anacond3 and pip to install tensorflow1.8.0 version.\r\ncuda version 9.0 cudnn.\r\n\r\nreproducing my code is very easy. just run it. @tensorflowbutler ", "If I'm understanding correctly, you're asking that when you create a variable and save the metagraph, then later load the metagraph, calling `tf.get_variable` again does not seem to know about the restored variable, and will not reuse it.\r\n\r\n/CC @lukaszkaiser, can you comment?", "I'm not really familiar with metagraphs, are we not serializing the variable store? It can be, I'm sorry I cannot help more.", "/CC @allenlavoie, do you know if the variable store is serialized, and if it should be?", "Most of the time variables aren't shared between MetaGraph imports and newly constructed graphs, which is presumably why it hasn't come up much.\r\n\r\nTo be clear, the workaround of getting the variable's Tensor from the metagraph by name works, right?", "> To be clear, the workaround of getting the variable's Tensor from the metagraph by name works, right?\r\n\r\nYes and the variable object is in `tf.trainable_variables()`. The issue is just that the info `tf.get_variable()` to see if the variable has already been created with a previous call to `tf.get_variable` is not stored in the metagraph. I'm not sure if we would want to add such functionality.", "So maybe \"contributions welcome\"? It is indeed odd behavior, but unless there's a compelling use-case I don't know of anyone who would prioritize it.", "@reedwm what you have conveyed is just what i meant. in tensorflow, now we can use get_tensor_by_name to reuse the restored variable v1, but can not use get_variable function to reuse it.\r\n@allenlavoie i think the  MetaGraph imports and newly constructed graphs should be grouped together.\r\ni give one use case.\r\nIf someone want to reuse v1 in MetaGraph by get_variable, he will find some error. and v1 varialbe is collected to the node named with  \"v1_1\u201c in actual. I think, this is confusing.\r\nIf no improvement is made, it should also be explained in the tutorial. "]}, {"number": 20098, "title": "Feature Request: Document Best Practice For Feeding New Data to a Restored Metagraph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes, custom code\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n1.7\r\n\r\n- **Python version**: \r\n\r\n3.x\r\n\r\n\r\n### Describe the problem\r\nI would like to produce and persist a model, represented by a metagraph, then restore it and feed it from a different data source, such as from a different Dataset. Despite being a natural thing to want to do, it is not easy to find out how to do this from official documentation. In particular, there's no  'best practice' shown anywhere in the docs.\r\n\r\nToday, the only way I have found of doing this is to build the graph with a feedable iterator (as described in the comments to #11679), and then saving+restoring the iterator _handle_ so that I can feed in new iterator by handle on every train step.\r\n\r\nAs a secondary issue, I think it would make more sense to save and restore a reinitializable iterator to the metagraph. Then, in the restored session, I could pull that reinitializable iterator out of the restored metagraph and reinitialize it from a new dataset.  No way that I tried of doing this actually worked. Although I could save the iterator with `make_saveable_from_iterator_`, the necessary `make_initializer` function wasn't present on the restored object; it didn't survive the roundtrip to disk.\r\n\r\n\r\n### Source code / logs\r\n@annarailton gives a full source code for the handle-based method of iterator persistence in this comment:\r\nhttps://github.com/tensorflow/tensorflow/issues/11679#issuecomment-395722710\r\n\r\nI independently came up with functionally equivalent code after several hours of work, then found her code by searching to see if anyone else was doing it with feedables. I was searching because it felt wrongish (inefficient) and the docs gave no endorsement for this approach.\r\n\r\nSo in the end, I have two related requests:\r\n\r\n1. Document the current best practice for attaching new data to the inputs of a restored metagraph.\r\n\r\ne.g. https://www.tensorflow.org/programmers_guide/datasets#saving_iterator_state should show this best practice. I believe this is by far the most common thing to want to do with a restored metagraph, likely to be far more common than resuming an existing iterator as shown in the docs.\r\n\r\n2. Provide an efficient way to attach a new data to a restored metagraph.\r\n\r\nIt may be that the handle lookup in the feedable iterator method is efficient. In that case, this second request is a no-op. \r\n\r\nFinally, I'm happy to give you a PR for datasets#saving_iterator_state to show the handle based feeding method, if you'd like one.", "comments": ["@mrry @rohan100jain @saxenasaurabh  : Any suggestions on what the best practice to follow here should be (feel free to reassign as appropriate)?", "Saurabh: please take a look at this when you get a chance. Thanks!", "Hi,\r\n\r\nAny news on this? I'm facing the exact same problem as @masonk \r\n\r\n@masonk have you found a work around with reinitializables iterators?", "@iitzco what are you trying to do? I think @masonk and I between us have bashed our heads against this quite a bit so we might be able to help you out. (Admittedly the solution might have to be some roundabout thing with feedables and handles).", "@iitzco : I haven't found a way to do it without feeding on every step. I'm still feeding on every step right now.", "Thanks for your replies! I will go then with the feedable approach based on @annarailton snippet.", "Dose there are some new practice of this issue? I found dataset api is usefull to train and evaluate model with batch, but there are less or no documents about inference with tf.data.dataset api, If I want to feed one sample to the trained graph how to complete this?", "> Dose there are some new practice of this issue? I found dataset api is usefull to train and evaluate model with batch, but there are less or no documents about inference with tf.data.dataset api, If I want to feed one sample to the trained graph how to complete this?\r\n\r\nSovled it!!!", "> Sovled it!!!\r\n@syw2014 Can you share, please?\r\n", "Ok, you may check my repo here:\nhttps://github.com/syw2014/Practice4Tensorflow/tree/master/modules/pipeline_demo,\nhope it helps!\n\nAli Salehi <notifications@github.com> \u4e8e2019\u5e746\u670815\u65e5\u5468\u516d \u4e0a\u53482:03\u5199\u9053\uff1a\n\n> Sovled it!!!\n> @syw2014 <https://github.com/syw2014> Can you share, please?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20098?email_source=notifications&email_token=ABT425SGKUYSKE5CGAYHZ2TP2PMP7A5CNFSM4FFQCE42YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXXQ2QQ#issuecomment-502205762>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABT425XMSMRS6W642CYCYZTP2PMP7ANCNFSM4FFQCE4Q>\n> .\n>\n", "@masonk Is there any new solution to fix this problem?", "This bug is more about documenting/answer what the best practice is rather\nthan how I personally solved my issue.\n\nI don't know what the best practice is - that should come from the TF team.\nAnd I still think this is a lacunae in the docs that needs addressing.\n\nI have my code saving both kinds of serialization formats at every save\npoint, and use one of them for resuming training and the other for serving.\n\nOn Mon, Aug 5, 2019 at 12:59 AM harry_tang <notifications@github.com> wrote:\n\n> @masonk <https://github.com/masonk> Is there any new solution to fix this\n> problem?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20098?email_source=notifications&email_token=AADN3TSHA66JF2QQTEFP46TQC7MUJA5CNFSM4FFQCE42YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3Q7YFQ#issuecomment-518126614>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADN3TS3Q5IH3ABEVZABFFLQC7MUJANCNFSM4FFQCE4Q>\n> .\n>\n"]}, {"number": 20086, "title": "Potential overflow in libhdfs wrapper", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nThe following snippet works fine until 2^30; 2^31 results in EINVAL, and >2^31 it produces zero-sized files. I wonder if this is an issue in the TF wrapper or rather in libhdfs itself.\r\n\r\n```python\r\n>>> for size in [2**30, 2**31, 2**32]:\r\n...     try:\r\n...         result = write_read(\"hdfs://root/user/s.lebedev/test\", size)\r\n...     except Exception as e:\r\n...         result = e\r\n...     print(\">\", size, \"<\", result)\r\n...\r\n> 1073741824 < 1073741824\r\n> 2147483648 < hdfs://root/user/s.lebedev/test; Invalid argument\r\n> 4294967296 < 0\r\n```\r\n\r\n### Source code / logs\r\n\r\n```python\r\ndef write_read(path, size):\r\n    b = bytes(memoryview(np.ones(size, dtype=np.uint8)))\r\n    with tf.gfile.Open(path, \"wb\") as f:\r\n        f.write(b)\r\n    return len(tf.gfile.Open(path, \"rb\").read())\r\n```", "comments": []}, {"number": 20072, "title": "Partial model loaded SavedModelBundle without exception", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: None\r\n- **Python version**:  None\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nBackground\r\n\r\nI am training a tensorflow with python and trying to load it in java for serving with SavedModelBundle.\r\n\r\nSample mode directory\r\n.\r\n\u251c\u2500\u2500 saved_model.pb\r\n\u2514\u2500\u2500 variables\r\n    \u251c\u2500\u2500 variables.data-00000-of-00001\r\n    \u2514\u2500\u2500 variables.index\r\n\r\nProblem\r\n\r\nThe thing is my model is published on hdfs and during downloading, some instances of the prediction server downloaded a partial model path with incomplete variables. Somehow SavedModelBundle is still able to load the partial model path into memory. Then when I actually query the service, I will get a lot of java.lang.IllegalStateException: Attempting to use uninitialized value *** in some node.\r\n\r\nMy question is how can I prevent this from happening? I can think of\r\n\r\nI need to check if downloading is successful before loading it.\r\nHave a way to throw an exception in SavedModelBundle when loading the partial model.\r\n\r\n### Source code / logs\r\nExceptions like \r\njava.lang.IllegalStateException: Attempting to use uninitialized value ***\r\n", "comments": ["I somehow got around this issue with a hacky way like\r\nval savedModelBundle: SavedModelBundle = SavedModelBundle.load(some_local_model_path, \"serve\")\r\nval isFullyLoaded: Boolean =\r\n    savedModelBundle\r\n      .graph\r\n      .operations\r\n      .asScala\r\n      .map(_.name())\r\n      .filter(x => x.contains(some_node_defined_in_python))\r\n      .map(\r\n        op => Try(savedModelBundle.session.runner.fetch(op).runAndFetchMetadata)\r\n      )\r\n      .forall(p => p.isReturn)\r\n\r\nSo for the models I tried, if the local model is incomplete, isFullyLoaded = false. If the model on disk is correct, isFullyLoaded = true. Not sure if this is the correct way to test though."]}, {"number": 20043, "title": "[BUG] tf.train.Saver in non-local filesystem", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0 release\r\n- **Python version**: python2.7\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc4.8.5\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n\r\nIn Tensorflow source code tensorflow/python/training/saver.py, there are some function call like \"os.path.isabs()\". If saving to non-local filesystem such as \"hdfs://\" and using save_relative_paths=True, the behavior is not expected. The checkpoint file still save \"hdfs://.../.../...\" instead of relative_path\r\n\r\nsource code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L982\r\n\r\nIt doesn't have any \"IsAbs()\" Interface in tensorflow::FileSystem, I think some solution:\r\n1. Add IsAbs() Interface in tensorflow::FileSystem\r\n2. add IsAbs() Interface in tensorflow::FileStatistics, we can call tensorflow::FileSystem::Stat() to get this info.\r\n\r\nThanks", "comments": ["i met the same issue. the root cause is the wrong file io used. should change to the tf.file_io apis.", "i am working on it", "i have created a PR, it is my first time, could someone tell me what should i do next. \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/20406"]}, {"number": 19991, "title": "Incorrect name returned in Tensorflow causes \u201cTensor which does not exist\u201d error while invoking get_tensor_by_name", "body": "As per the [documentation](https://www.tensorflow.org/programmers_guide/graphs#naming_operations) TensorFlow would append \"_1\", \"_2\", and so on to the name in tf.Graph namespace, in order to make it unique. Here I define two convolutional operations. It is expected that the first one will be named as \"conv2d\" and second one \"conv2d_1\". But when I try to obtain the name of the second convolution it returns \"conv2d_2\". I causes error when I try to invoke get_tensor_by_name. Here is the code:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\n\r\nx = tf.constant(np.random.randn(1,2,2,1), dtype=tf.float32)\r\nkernel_size = (1,1)\r\nno_of_out = 20\r\nstrides = (1,1)\r\nconv_out1 = tf.layers.conv2d(x, 10, (1,1), (1,1))\r\nconv_out2 = tf.layers.conv2d(x, 10, (1,1), (1,1))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print conv_out1.name # conv2d/BiasAdd:0 .  This value is correct\r\n    print conv_out2.name # conv2d_2/BiasAdd:0 .  This value is incorrect.  It should be conv2d_1/BiasAdd:0\r\n    conv_weights1 = tf.get_default_graph().get_tensor_by_name(os.path.split(conv_out1.name)[0] + '/kernel:0')\r\n    conv_weights2 = tf.get_default_graph().get_tensor_by_name('conv2d_1/kernel:0')  \r\n    conv_weights2 = tf.get_default_graph().get_tensor_by_name(os.path.split(conv_out2.name)[0] + '/kernel:0')\r\n\r\nI get error\r\n\r\n\"KeyError: \"The name 'conv2d_2/kernel:0' refers to a Tensor which does not exist. The operation, 'conv2d_2/kernel', does not exist in the graph.\"\r\n\r\n**Issue Template**\r\n1. Have I written custom code : No. I did not customize any part of TensorFlow. Directly using it as described above\r\n2. OS Platform and Distribution : Ubuntu 14.04\r\n3. TensorFlow installed from : N/A\r\n4. TensorFlow version : 1.4.0-rc1\r\n5. Bazel version : N/A\r\n6. CUDA/cuDNN version : N/A\r\n7. GPU model and memory : N/A\r\n8. Exact command to reproduce : As described above with exact code.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Thanks for the response.  Updated the original post with entries for the fields in the template.", "This problem is occurring only in that particular version of Tensorflow.  I am not much worried about it now.  Should I close this issue?"]}, {"number": 19988, "title": "Official profile python API not work on the official mnist example", "body": "Hi! I was using the official minist summaries example which uses run_metadata to track model runtime statistics:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n\r\nI tried to add simple profiler python API as suggest by \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md\r\n\r\nSo, my code is:\r\nhttps://github.com/MrWanter/mnist-proflie/blob/c0f5b59403fbf3258e8236fa7c983750623354e6/mnist-profile.py#L175-L184\r\n\r\nbut running the model gives \r\n```\r\nAdded profiling by user\r\nParsing Inputs...\r\nIncomplete shape.\r\nIncomplete shape.\r\nOptions -step=199 not found.\r\nAvailable steps: 0 \r\nAdding run metadata for 199\r\n```\r\nbut tensorboard shows the memory consumption of nodes, which suggests runtime statistics are captured:\r\n![image](https://user-images.githubusercontent.com/18298163/41357872-b3d623fe-6f59-11e8-91c2-539f55340e8a.png)\r\n\r\nwhat is incomplete shape, the input shape has None for batch size, but since it is runtime statistics, I thought it would work, but it does not, can someone help?\r\n\r\nAs in #19962, it is hard to know how to deal with profiler problem since documentation is not enough and    codes are hard to read as it is implemented with C++ and heavily wrapped\r\n\r\n**UPDATA**:\r\nTh problem is caused by input shape of None for batch size, change it to fixed shape solve the problem, but there would inevitably be a large amount of None in tensors since we have to deal with variable sized inputs, in those cases, it should be reasonable for the profiler to capture runtime statistics, so how can we do it? where am I wrong?\r\n\r\n**UPDATE**\r\nFound a quote on the official profiler doc:\r\n> It must have known \"shape\" information for RegisterStatistics('flops') to calculate the statistics. It is suggested to pass in -run_meta_path if shape is only known during runtime. tfprof can fill in the missing shape with the runtime shape information from RunMetadata. Hence, it is suggested to use -account_displayed_op_only option so that you know the statistics are only for the operations printed out.\r\n\r\nseems profiler can fill in the missing shape with runtime shape information, but how to pass in `-\r\nrun_meta_path`,  I did not find anything relevant in the profiler options:\r\n```\r\n      self._options = {'max_depth': 100,\r\n                       'min_bytes': 0,\r\n                       'min_micros': 0,\r\n                       'min_params': 0,\r\n                       'min_float_ops': 0,\r\n                       'min_occurrence': 0,\r\n                       'order_by': 'name',\r\n                       'account_type_regexes': ['.*'],\r\n                       'start_name_regexes': ['.*'],\r\n                       'trim_name_regexes': [],\r\n                       'show_name_regexes': ['.*'],\r\n                       'hide_name_regexes': [],\r\n                       'account_displayed_op_only': False,\r\n                       'select': ['micros'],\r\n                       'step': -1,\r\n                       'output': 'stdout'}\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@panyx078 do you own the profiler?", "which function to pass -run_meta_path\r\n", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Unfortunately I don't believe anyone is supporting the profiler right now, so I'm gonna mark this as contributions welcome. @martinwicke correct me if I'm wrong.", "@zheng-xq @petermattson @prb12 \r\n\r\nAnybody knows anything about this? Otherwise I think we should delete it with 2.0.", "I don't know enough to have an opinion, but when in doubt kill. We can\nalway resurrect in a 2.x build while maintaining backwards compatibility if\nwe're wrong. The reverse is not true.\n\nOn Mon, Aug 20, 2018 at 2:45 PM, Martin Wicke <notifications@github.com>\nwrote:\n\n> @zheng-xq <https://github.com/zheng-xq> @petermattson\n> <https://github.com/petermattson> @prb12 <https://github.com/prb12>\n>\n> Anybody knows anything about this? Otherwise I think we should delete it\n> with 2.0.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19988#issuecomment-414472907>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AhFaHaGCp_v17efiTtI2b14KHywOjn_Kks5uSy4UgaJpZM4UmR4_>\n> .\n>\n"]}]