[{"number": 16415, "title": "add URLEncode for the CopyObjectRequest of S3 Rename function", "body": "I found that `tf.gfile.Rename` did not work for S3 objects with UTF-8 names. A \"NoSuchKey\" error will be reported in this case:\r\n\r\n```\r\n>>> tf.gfile.Rename('s3://xxxxxxxx/\u4e0a\u6d77', 's3://xxxxxxxx/\u5317\u4eac')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xxxxxxxx/Software/Linux/anaconda2/envs/tfdev/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 402, in rename\r\n    compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\r\n  File \"/home/xxxxxxxx/Software/Linux/anaconda2/envs/tfdev/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: NoSuchKey: The specified key does not exist.\r\n```\r\n\r\nAfter debugging, I believed the error should be related to the usage of `CopyObjectRequest`. It is described that the \"CopySource\" must be URL-encoded (as in the [AWS document](http://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_s3_1_1_model_1_1_copy_object_request.html#ab3fd89c8e77ffa12d053925efbb099ae) ). Thus, I made a patch. It worked well in my environment, and now objects with UTF-8 names can be renamed by `tf.gfile.Rename`. Also, `bazel test //tensorflow/core/platform/s3:s3_file_system_test` passed.\r\n\r\nPlease take a look. Thanks!", "comments": ["@sswv Thanks!", "Should this patch be merged to the r1.5 branch? I also expect this fix in my application. Thank you! \r\n@jhseu @rmlarsen ", "We only merge security fixes for patch releases, so this will have to wait until 1.6."]}, {"number": 16414, "title": "Separate constant file for common variable", "body": "Created a separate common `constants.py` which can be used globally.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 16413, "title": "Separate constant file for global variables", "body": "Created a separate common `constants.py` which can be used globally under `ops.`\r\n\r\n**P.S:** I created the same pull request [#16401](https://github.com/tensorflow/tensorflow/pull/16401) as after pushing my latest changes I was facing CLA issues.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@rmlarsen I am struggling with CLA issues, please help me. My CI build was failing since pushed new changes after figuring out issues.", "@rajendraarora16 As the message says, some of the commits had a different author than the email you used to sign the CLA. Can you perhaps prepare a fresh PR where this is not the case?", "Closing this PR, I am facing CLA issues."]}, {"number": 16412, "title": "Documentation on build from source is unclear", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.4.1-7-gaa03bfc', '1.4.1')\r\nbuilt and installed from source with\r\ngit checkout r1.4\r\nbazel build -c opt --copt=-march=\"haswell\" --config=cuda --verbose_failures --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1\r\nnote: incompatible path flag is required with R1.4 at this time per https://github.com/tensorflow/tensorflow/issues/15492\r\nubuntu 16.04\r\nCuda 9.1, cudnn 7.0.4\r\ngcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nuname -r\r\n4.4.0-104-generic\r\nBazel 0.9\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nIt is unclear how to build and install the entire package purely from source\r\nI will attempt to log what I have done so far\r\nclone and build TF R1.4 for cuda\r\ninstall wheel into local directory  (sudo pip install /tmp/tensorflow-pkg/tensorflow*.whl -t ~/mytf_r1.4_c9.1\r\nexport PYTHONPATH=~/mytf_r1.4_c9.1\r\nmove tensorflow directory\r\n\r\ninstall common_voice files to ~/Common_voice\r\n\r\nper native client build from source instructions: https://github.com/mozilla/DeepSpeech/blob/master/native_client/README.md\r\ngit clone tensorflow\r\ncd tensorflow\r\ngit checkout r1.4\r\nln -s ../DeepSpeech/native_client ./\r\n./configure\r\nedit native_client/BUILD\r\ncomment out the following:\r\n#    tfcompile_flags = select({\r\n#        \"//tensorflow:rpi3\": str('--target_triple=\"armv6-linux-gnueabihf\" --target_cpu=\"cortex-a53\" --target_features=\"+neon-fp-armv8\"'),\r\n#        \"//conditions:default\": str('')\r\n#    }),\r\nbazel build -c opt --copt=-O3 --incompatible_load_argument_is_label=false //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so //native_client:deepspeech //native_client:deepspeech_utils //native_client:libctc_decoder_with_kenlm.so //native_client:generate_trie\r\nat this point all the native client binaries are in\r\n~/tensorflow/bazel-bin/native_client\r\nlevinth@zt-gpu-lin:~/DeepSpeech/native_client$ ls ~/tensorflow/bazel-bin/native_client/\r\ngenerate_trie\r\ngenerate_trie-2.params\r\ngenerate_trie.runfiles\r\ngenerate_trie.runfiles_manifest\r\nlibctc_decoder_with_kenlm.so\r\nlibctc_decoder_with_kenlm.so-2.params\r\nlibctc_decoder_with_kenlm.so.runfiles\r\nlibctc_decoder_with_kenlm.so.runfiles_manifest\r\nlibdeepspeech.a\r\nlibdeepspeech.a-2.params\r\nlibdeepspeech.pic.a\r\nlibdeepspeech.pic.a-2.params\r\nlibdeepspeech.so\r\nlibdeepspeech.so-2.params\r\nlibdeepspeech_utils.a\r\nlibdeepspeech_utils.a-2.params\r\nlibdeepspeech_utils.pic.a\r\nlibdeepspeech_utils.pic.a-2.params\r\nlibdeepspeech_utils.so\r\nlibdeepspeech_utils.so-2.params\r\n_objs\r\n\r\ncd ../Deepspeech/native_client\r\nexport TFDIR ~/tensorflow\r\nmake deepspeech\r\n\r\n\r\nat this point however the native client shared objects are still in bazel-bin/native client and have not been installed. the invocation of Deepspeech.py fails as it cannot find the shared objects\r\npython DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv >deepspeech_1.log 2>&1\r\ntensorflow.python.framework.errors_impl.NotFoundError: native_client/libctc_decoder_with_kenlm.so: cannot open shared object file: No such file or directory\r\n\r\nthe native_client/Makefile has sections for bindings and install..so try\r\nsudo make install\r\nand this still generates the error as install does not put\r\n~/tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so\r\ninto /usr/local/lib \r\nthough deepspeech.so and deepspeech_utils.so are installed there.\r\n\r\nmanually copy /tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so to ~/DeepSpeech/native_client and set permissions\r\nat this point the invocation now starts running but complains about\r\n------------------------------------------------------------------------\r\nWARNING: libdeepspeech failed to load, resorting to deprecated code\r\n         Refer to README.md for instructions on installing libdeepspeech\r\n------------------------------------------------------------------------\r\neven though /usr/local/lib is in the $LD_LIBRARY_PATH\r\n\r\ninvoking\r\npython DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv --display_step 1 --validation_step 10\r\n\r\n------------------------------------------------------------------------\r\nWARNING: libdeepspeech failed to load, resorting to deprecated code\r\n         Refer to README.md for instructions on installing libdeepspeech\r\n------------------------------------------------------------------------\r\nI STARTING Optimization\r\nLoading the LM will be faster if you build a binary file.\r\nReading data/lm/lm.binary\r\n----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\r\nterminate called after throwing an instance of 'lm::FormatLoadException'\r\n  what():  native_client/kenlm/lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException.\r\nfirst non-empty line was \"version https://git-lfs.github.com/spec/v1\" not \\data\\. Byte: 43\r\n\r\nI clearly have not figured this out\r\n:-)\r\n \r\n", "comments": ["a couple minutes after the last bit of output show above the program crashed\r\nAborted (core dumped)\r\n\r\n", "Close this...opened the wrong window,...this is for Mozilla Deepspeech..sorry", "Closed. "]}, {"number": 16411, "title": "Source code / logs", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16410, "title": "..", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16409, "title": "Build libjpeg-turbo ALTIVEC SIMD", "body": "The libjpeg-turbo package has ALTIVEC SIMD and this updates the\r\nthird_party build to build the ALTIVEC SIMD on the appropriate\r\nplatform.", "comments": ["LGTM. @gunan WDYT?"]}, {"number": 16408, "title": "Branch 183220585", "body": "", "comments": []}, {"number": 16407, "title": "Creating placeholder with `np.uint32` dtype fails", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes custom snippet below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nnot applicable\r\n- **GCC/Compiler version (if compiling from source)**:\r\nnot applicable\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.5\r\n- **GPU model and memory**:\r\nTitan X\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nnp.uint32 dtype is not supported while creating placeholder\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(tf.placeholder(np.int32, [None], 'ph1'))\r\nprint(tf.placeholder(np.uint32, [None], 'ph2'))\r\n```\r\nLine 3 works, line 4 fails.\r\n\r\nConsole Output:\r\n```txt\r\nTensor(\"ph1:0\", shape=(?,), dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"...\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 126, in make_type\r\n    v = dtypes.as_dtype(v).base_dtype\r\n  File \"...\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\", line 595, in as_dtype\r\n    \"Cannot convert value %r to a TensorFlow DType.\" % type_value)\r\nTypeError: Cannot convert value <class 'numpy.uint32'> to a TensorFlow DType.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/scaffold-ext/scaffold_ext/analysis/ann/bug.py\", line 4, in <module>\r\n    print(tf.placeholder(np.uint32, [None], 'ph2'))\r\n  File \"...\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1599, in placeholder\r\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"...\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3083, in _placeholder\r\n    dtype = _execute.make_type(dtype, \"dtype\")\r\n  File \"...\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 129, in make_type\r\n    (arg_name, repr(v)))\r\nTypeError: Expected DataType for argument 'dtype' not <class 'numpy.uint32'>.\r\n```", "comments": ["This has been addressed since the 1.4.0 release:\r\nhttps://github.com/tensorflow/tensorflow/commit/3bafe0a86f67dd54197c6d60bdb5053f510de7d8#diff-85e323da48079f7831b7e9f6ec24cda2R501\r\n\r\nPlease give 1.5.0 a shot.", "Okay, thanks. I will check that"]}, {"number": 16406, "title": "Correct a small typo", "body": "Small typo leaved in the retrain example.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16405, "title": "tf.contrib.framework.sort failing with <<...has no attribute 'sort'>> in TF windows ", "body": "### [Problem] : Can't use tf.contrib.framework.sort in my tensorflow code as it's failing to find the 'sort' attribute\r\n\r\nSource code:\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32, shape=(10, 10))\r\ny = tf.contrib.framework.sort(x)\r\n\r\nwith tf.Session() as sess:\r\n    rand_array = np.random.rand(10, 10)\r\n    print(sess.run(y, feed_dict={x: rand_array})) \r\n```\r\n\r\nError log:\r\n```\r\n <module>\r\n    y = tf.contrib.framework.sort(x)\r\nAttributeError: module 'tensorflow.contrib.framework' has no attribute 'sort'\r\n```\r\n\r\n### System information\r\n- I've been trying to use the tf.contrib.framework.sort  within a custom loss function but issue being reproduced with a simple call to the tf.contrib.framework.sort\r\n- Windows 10 64 bit\r\n- TF installed with native pip3\r\n- TF version: 1.4.0\r\n- Python version: 3.6.2 \r\n- TF with CPU support only\r\n", "comments": ["According to [the documentation](https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/contrib/framework/sort), `tf.contrib.framework.sort()` was added after TF 1.4, and so you will need to upgrade to TF 1.5 or a nightly build to use it.", "Still doesn't work in TF 1.12.0!"]}, {"number": 16404, "title": "remove SRU num_units == x.shape[-1] restriction", "body": "Based on the [author's response](https://github.com/taolei87/sru/issues/12), the restriction is unnecessary. Simply add a linear transform to the input will solve the issue\r\n\r\n#13094 ", "comments": ["@stegben can you add a unit test that exercises the new behavior?", "sure!", "@rmlarsen plz help me trigger CI, thank you!", "@rmlarsen I fixed the test again. I've test it on my laptop so it should be OK this time", "@stegben Thanks!", "@ebrevdo this is good to go, right? Could you hit approve to confirm?", "@ebrevdo @rmlarsen Thanks a lot!"]}, {"number": 16403, "title": "1D Convolution in Tensorflow Serving", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: tensorflow binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 9.0, 7.0\r\n- **GPU model and memory**: GTX 1050\r\n\r\n### Describe the problem\r\nThe Problem is a little bit hard to reproduce, I guess because so many steps are involved.\r\nSo, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:\r\n\r\n`\r\n           input = Input(shape=(200, 8))\r\n            x = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(input)\r\n            x = Conv1D(filters=128, kernel_size=7, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\r\n            x = Conv1D(filters=2, kernel_size=1, activation=\"softmax\")(x)\r\n\r\n`\r\n\r\nNow, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:\r\n\r\n`\r\nsession = K.get_session()\r\n\r\n        signature = tf.saved_model.signature_def_utils.build_signature_def(\r\n            inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},\r\n            outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},\r\n            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\r\n        )\r\n\r\n        b = tf.saved_model.builder.SavedModelBuilder(filename)\r\n        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\r\n        b.add_meta_graph_and_variables(session,\r\n                                       [tf.saved_model.tag_constants.SERVING],\r\n                                       signature_def_map={\r\n                                           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},\r\n                                       legacy_init_op=legacy_init_op)\r\n        b.save()\r\n`\r\n\r\nIf I am loading the model via python, everything works as expected.\r\n\r\nNow I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:\r\n\r\n`\r\nTensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()\r\n                .setSize(data.length).build();\r\n\r\n        TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()\r\n                .setSize(data[0].length).build();\r\n\r\n        TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()\r\n                .setSize(data[0][0].length).build();\r\n\r\n        TensorShapeProto shape = TensorShapeProto.newBuilder()\r\n                .addDim(dim1).addDim(dim2).addDim(dim3).build();\r\n\r\n        TensorProto.Builder builder = TensorProto.newBuilder()\r\n                .setDtype(DataType.DT_FLOAT)\r\n                .setTensorShape(shape);\r\n\r\n        for(int i = 0; i < data.length; i++) {\r\n            for(int j = 0; j < data[0].length; j++) {\r\n                for(int k = 0; k < data[0][0].length; k++) {\r\n                    builder.addFloatVal(data[k][j][i]);\r\n                }\r\n            }\r\n        }\r\n\r\n        return builder.build();\r\n`\r\n\r\nAnd do the predicition like this:\r\n\r\n`\r\npublic class ModelClientImpl implements ModelClient {\r\n\r\n    private String host;\r\n    private Integer port;\r\n    private ManagedChannel channel;\r\n    private PredictionServiceGrpc.PredictionServiceBlockingStub stub;\r\n\r\n    public void init() {\r\n        channel = ManagedChannelBuilder\r\n                .forAddress(getHost(), getPort())\r\n                .usePlaintext(true)\r\n                .build();\r\n\r\n        stub = PredictionServiceGrpc.newBlockingStub(channel);\r\n    }\r\n\r\n    @Override\r\n    public Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {\r\n        final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));\r\n\r\n        return response.getOutputsMap();\r\n    }\r\n\r\n    protected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {\r\n        final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()\r\n                .setName(signatureName)\r\n                .setSignatureName(\"serving_default\").build();\r\n\r\n        final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()\r\n                .setModelSpec(modelSpec)\r\n                .putAllInputs(inputs);\r\n\r\n        return builder.build();\r\n    }\r\n\r\n    public String getHost() {\r\n        return host;\r\n    }\r\n\r\n    public void setHost(String host) {\r\n        this.host = host;\r\n    }\r\n\r\n    public Integer getPort() {\r\n        return port;\r\n    }\r\n\r\n    public void setPort(Integer port) {\r\n        this.port = port;\r\n    }\r\n\r\n    @Override\r\n    public void close() throws Exception {\r\n        channel.shutdown().awaitTermination(5, TimeUnit.DAYS);\r\n    }\r\n}\r\n\r\n`\r\n\r\nBut the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?\r\n", "comments": ["Closing this out, since it's not a TensorFlow bug or feature request.\r\n\r\nThat said, your code that converts from 3D data to a TensorProto looks wrong.  In particular notice this snippet:\r\n```\r\n   for(int i = 0; i < data.length; i++) {\r\n        for(int j = 0; j < data[0].length; j++) {\r\n            for(int k = 0; k < data[0][0].length; k++) {\r\n                builder.addFloatVal(data[k][j][i]);\r\n            }\r\n        }\r\n    }\r\n```\r\n\r\nI think you meant to say this:\r\n```\r\n   for(int i = 0; i < data.length; i++) {\r\n        for(int j = 0; j < data[0].length; j++) {\r\n            for(int k = 0; k < data[0][0].length; k++) {\r\n                builder.addFloatVal(data[i][j][k]);\r\n            }\r\n        }\r\n    }\r\n```\r\n\r\nAlso note that the `TensorShapeProto` documentation describes how the order of dims affects the data layout.  Namely that data is in row-major order:\r\nhttps://github.com/tensorflow/tensorflow/blob/c4c19f1294599c501dd512db59ee4229b437abc8/tensorflow/core/framework/tensor_shape.proto#L31", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nExact command to reproduce"]}, {"number": 16402, "title": "Modified Implementation of ndlstm_base_dynamic.", "body": "It now uses a `BasicLSTMCell` that has `state_is_tuple=True` to address the deprecation thrown by having `state_is_tuple=False`.", "comments": ["@drpngx have time to look at this one?  original author of ndlstm is not at google anymore."]}, {"number": 16401, "title": "Separate constant file for global variables", "body": "Created a separate common `constants.py` which can be used globally under `ops`.", "comments": ["@rajendraarora16 Thanks for the cleanup.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Closing this pull request I am facing CLA issues."]}, {"number": 16400, "title": "[doc] link to \"How to Use t-SNE Effectively\" from embeddings is broken", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:\r\n- **TensorFlow installed from (source or binary) binary**:\r\n- **TensorFlow version (use command below) 1.5.0rc0 **:\r\n- **Python version  3.5.1**: \r\n- **Bazel version (if compiling from source) NOT USED**:\r\n- **GCC/Compiler version (if compiling from source) NOT USED**:\r\n- **CUDA/cuDNN version NOT USED**:\r\n- **GPU model and memory NOT USED **:\r\n- **Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/**:\r\n\r\n### Describe the problem\r\n- Link to to \"How to Use t-SNE Effectively\" is broken.\r\n- The page link is follows (before junmping)\r\n  - https://www.tensorflow.org/programmers_guide/embedding\r\n  - 404 page is following URL \r\n     - https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/\r\n- Original page should be follows. (the URL in embedding.md should rewrite to follows)\r\n  -   https://distill.pub/2016/misread-tsne/\r\n\r\n### Source code / logs\r\n- The problem code is follows.\r\n  - https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123\r\n\r\n", "comments": ["@sakaia The linke seems to have been fixed in PR #16029. I will close this issue as it has been resolved. Thanks for your contribution!"]}, {"number": 16399, "title": "Does TensorFlow 1.5 support CUDA 9.1?", "body": "My notebook has an MX150 display adapter, someone said that it's available with CUDA 9.1", "comments": ["Not officially support, but someone manage to build TF1.5 with CUDA9.1, see #15140", "Yes, I been able to compile tf-1.5rc1 with CUDA 9.1 no problems", "Thanks for the comments @qmick and @Davidnet!\r\n\r\nPlease follow-up on #15140 for CUDA 9.1 status.\r\n\r\nI'll also mention for posterity that our official compatibility (for released versions) is described in our documentation:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_sources.md\r\nhttps://www.tensorflow.org/install/install_sources", "I compile tf r1.4 with cuda9.1 and cudnn7.0.5, and success after edit /usr/local/cuda/include/math_functions.h which follows #15140."]}, {"number": 16398, "title": "Compare_and_bitpack function for bool for big endian", "body": "Added condition for endianness check and related conversion for Big Endian.\r\nRemoved the note from file: \r\n`// NOTE(ebrevdo): This assumes memory is little-endian.`\r\nPlease let me know your feedback.", "comments": ["@namrata-ibm  Have you run the unit test on a big endian machine?"]}, {"number": 16397, "title": "S3 accessing reports \"Curl returned error code 6\" after AWS SDK upgrading to 1.3.15", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0rc1 (tag) and master (2e5ff39e)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: /\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\n\r\nTensorFlow 1.4.X was working well with S3 in my environment. After upgrading to 1.5.0rc1, I found that S3 could not be accessed. \"Curl returned error code 6\" is reported.\r\n\r\nI noticed that AWS SDK had been upgraded from 1.0.90 to 1.3.15 in r1.5 and master branches. Thus, I pulled the master (2e5ff39e) and tried to change AWS SDK 1.3.15 into 1.0.90 in `tensorflow/workspace.bzl`. After this modification, it works well!\r\n\r\nI tried with both AWS S3 (with http proxy) and Minio (localhost), and the results are the same. (AWS SDK 1.0.90 is Ok, but 1.3.15 reports error)\r\n\r\nI guess there might be some incompatible changes after AWS SDK 1.3.15. Could you please take a look? Thanks! @yongtang\r\n\r\nI noticed that AWS SDK required gcc 4.9, but I was using 4.8. Thus, this issue might be related to the old versions of gcc and glib on my server. I will try with some new systems as well.\r\n\r\n### Source code / logs\r\n\r\nLogs when using TensorFlow master (2e5ff39e):\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')\r\n2018-01-25 15:34:45.145317: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1\r\n2018-01-25 15:34:45.145354: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0\r\n2018-01-25 15:34:45.145367: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default\r\n2018-01-25 15:34:45.145383: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http\r\n2018-01-25 15:34:45.145401: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2\r\n2018-01-25 15:34:45.145414: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000\r\n2018-01-25 15:34:45.145456: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/credentials for reading.\r\n2018-01-25 15:34:45.145468: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.\r\n2018-01-25 15:34:45.145479: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/config for reading.\r\n2018-01-25 15:34:45.145487: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.\r\n2018-01-25 15:34:45.145495: I tensorflow/core/platform/s3/aws_logging.cc:54] Credentials have expired attempting to repull from EC2 Metadata Service.\r\n2018-01-25 15:34:45.145614: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2\r\n2018-01-25 15:34:45.145628: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-01-25 15:34:46.146625: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28\r\n2018-01-25 15:34:46.146655: E tensorflow/core/platform/s3/aws_logging.cc:60] Http request to Ec2MetadataService failed.\r\n2018-01-25 15:34:46.146666: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.\r\n2018-01-25 15:34:46.146715: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25\r\n2018-01-25 15:34:46.146849: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2\r\n2018-01-25 15:34:46.146863: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-01-25 15:34:46.147677: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6\r\n2018-01-25 15:34:46.147704: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\r\n2018-01-25 15:34:46.147716: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 0 ms before attempting again.\r\n2018-01-25 15:34:46.147802: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-01-25 15:34:46.148107: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6\r\n...\r\nFalse\r\n```\r\n\r\nLogs after replacing AWS SDK 1.3.15 with 1.0.90:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')\r\n2018-01-25 15:44:29.134077: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1\r\n2018-01-25 15:44:29.134108: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0\r\n2018-01-25 15:44:29.134121: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default\r\n2018-01-25 15:44:29.134133: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections -864887560 and scheme Creating HttpClient with max connections %d and scheme %s\r\n2018-01-25 15:44:29.134147: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2\r\n2018-01-25 15:44:29.134157: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000\r\n2018-01-25 15:44:29.134176: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************\r\n2018-01-25 15:44:29.134184: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-01-25 15:44:29.134223: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25\r\n2018-01-25 15:44:29.134264: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************\r\n2018-01-25 15:44:29.134273: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-01-25 15:44:29.134429: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool successfully grown by 2\r\n2018-01-25 15:44:29.134442: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2018-01-25 15:44:30.048051: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************\r\n2018-01-25 15:44:30.048084: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2018-01-25 15:44:30.048159: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\nTrue\r\n```", "comments": ["@sswv I am not able to observe the behavior as you described. I am using an EC2 machine of Ubuntu 16.04 on AWS. Wondering if the CentOS you described could have an impact?", "@yongtang I tested it on both Ubuntu 16.04 and Debian 9 containers as well, and it worked well. I think the error should be related to something on CentOS 7 or the configuration of my server. I'll try to find the reason. Thanks!", "Are you by chance using a bucketname with a \".\" in it?\r\n\r\nIf so, it's either due to TLS hostname validation (which is different across different versions of openssl, libnss, gnutls etc...), or some DNS rule on Centos that prevents it from resolving.\r\n\r\nIf that's the case, there's a constructor argument to use the traditional path style bucket addressing that would likely work around this issue.", "You can turn on verbose logging in the AWS C++ SDK to get us more information. \r\nAnd also like @JonathanHenson already mentioned, it's likely that you need to turn off virtual addressing.\r\n\r\nHere's a sample:\r\n```cpp\r\nAws::SDKOptions options;\r\noptions.loggingOptions.logLevel = Aws::Utils::Logging::LogLevel::Trace;\r\nAws::InitAPI(options);\r\n\r\nS3Client client (Aws::Client::ClientConfiguration(), \r\n                          Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never, \r\n                          false /*useVirtualAdressing*/);\r\n// ...\r\n```\r\nNote, that the errors you reported have nothing to do with compiler version.\r\n\r\nmore information about S3 virtual addressing can be found [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html)", "@sswv Please report back with your findings, and take note of the excellent suggestions by @JonathanHenson and @marcomagdy.", "After building tensorflow on CentOS (inside docker container) with `useVirtualAdressing = false`, the `.` works. Also have to copy `cp /etc/pki/tls/certs/ca-bundle.crt /etc/ssl/certs/ca-certificates.crt` to avoid curl 77 error.\r\n\r\nCreated a PR #16443 for it. Not sure if this is the same issue as was raised by @sswv, though. ", "Hi, @yongtang I tested you patch and it works well in my environment! Thanks!", "@JonathanHenson @marcomagdy I tested again following your suggestions. I confirmed that the error was related to the \"virtual addressing\" feature. However, it was not due to a \".\" in the bucket name, but some other reasons. (1) For Minio, I was using IP address as hostname, and the \"virtual addressing\" feature was not available. (2) For AWS S3, the error was due to some firewall rules of my HTTP Proxy.\r\n\r\nI agree with @yongtang 's patch. I think it is good to disable \"virtual addressing\" by default, because this feature is not available for some third-party S3-compatible storage systems.\r\n\r\nThanks!", "Cool, marking \"contributions welcome\" since @yongtang has pending #16443 that might fix this.", "Since the patch https://github.com/tensorflow/tensorflow/pull/16443 had been merged, I think the issue can be closed."]}, {"number": 16396, "title": "tf.image.resize_image_with_crop_or_pad() is taking too much time to process an image or batch of images", "body": "ISSUE: \r\nI am stuck in fine tuning imagenet data rendering speed for the reason being tf.image.resize_image_with_crop_or_pad() is taking too much time to process an image or batch of images. \r\n```Testing was done by preprocessing ONE image (no batching) and an observed latency was `~160msec`. ```\r\n\r\nNOTE: I did not observe memory pressure or any possible compute bottleneck during this time. \r\nDescription:\r\nI have a tool to load ImageNet dataset which reads the dataset from processed TFRecords and returns an iterator object to iterate over the datasets with a shuffle and repeat.\r\nI have been observing slow rendering of data due to some overtime by certain steps in image (batch of images) pre-preprocessing. This is really impacting in data rendering speed and hence the slow training of models(Reference here is AlexNet). \r\n\r\nHere is the `snip` of time taken for each operation,\r\n<img width=\"1425\" alt=\"imagenet_loader_perf_profile\" src=\"https://user-images.githubusercontent.com/35795681/35406079-d4ec3584-01bc-11e8-9ba6-50e4dabba29e.png\">\r\n\r\nHere is a `snip`  of data preprocess methods,\r\n```\r\ndef _parse_example_proto(dataset, height, width, depth, dtype, num_parallel_calls):\r\n    def _input_parser(record):\r\n        keys_to_features = {\r\n            \"image/encoded\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n            \"image/format\": tf.FixedLenFeature((), tf.string, default_value=\"jpeg\"),\r\n            \"image/class/label\": tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\r\n        }\r\n        parsed = tf.parse_single_example(record, keys_to_features)\r\n        image = tf.image.decode_jpeg(parsed[\"image/encoded\"], channels=depth)\r\n        image = tf.image.convert_image_dtype(image, dtype=dtype)\r\n        label = tf.cast(parsed[\"image/class/label\"], tf.int32)\r\n        return image, label\r\n    return dataset.map(_input_parser, num_parallel_calls=num_parallel_calls)\r\n\r\ndef _dataset_preprocess_fn(dataset, height, width, num_parallel_calls):\r\n    def _preprocess_fn(image, label):\r\n        image = tf.image.resize_image_with_crop_or_pad(image, height + 8, width + 8 )\r\n        image = tf.random_crop(image, [height, width, 3])\r\n        image = tf.image.random_flip_left_right(image)\r\n        image = (ISTD * image) + MEAN  #dataset level MEAN\r\n        return image, label\r\n    return dataset.map(_preprocess_fn, num_parallel_calls=num_parallel_calls)\r\n```\r\n### System information\r\n- **OS Platform and Distribution *:  Linux Centos 7.2\r\n- **TensorFlow installed from (source or binary)**: 1.4.0 rc1 ( commit hash badd356)\r\n- **TensorFlow version (use command below)**:  b'v1.3.0-rc1-4546-gef196f3' 1.4.0-rc1\r\n- **Python version**:  3.4.5\r\n- **Bazel version (if compiling from source)**: Build label: 0.8.1\r\n- **CUDA/CUDAnn version**: CUDA 9 and  CUDAnn 7.0\r\n- **GPU model and memory**: Volta 100, 16GiB\r\n\r\n\r\nI am using Amazon Instance and here are the details,\r\n```\r\nGPUs - Tesla V100\r\nGPU Memory (GB): 16\r\nvCPUs : 8\r\nMemory (GB):61\r\nNetwork Bandwidth: Upto 10Gbps\r\nEBS Bandwidth: 1.5 Gbps\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nExact command to reproduce", "@tensorflowbutler, thank you for quick response in this regard.  Here is `snip` of code I have tried, and this is very much reproducible. \r\n\r\nHave I written custom code: `N/A`\r\n\r\nExact command to reproduce:\r\n```\r\n    FILE_NAMES = ['train-example.tfrecords']\r\n    def input_fn(split='train', **kwargs):\r\n\r\n        # Dataset args\r\n        repeat = kwargs.get('repeat', True)\r\n        shuffle = kwargs.get('shuffle', True)\r\n\r\n        # List TFrecords\r\n        # split args will have 'train' or 'validate'\r\n        dataset = tf.data.TFRecordDataset(FILE_NAMES)\r\n        # Process dataset for training only.\r\n        if split.strip().lower() == 'train':\r\n            if repeat:\r\n                dataset = dataset.repeat()\r\n            if shuffle:\r\n                dataset = dataset.shuffle(buffer_size=len(FILE_NAMES))\r\n\r\n        # Add parallelism in input process.\r\n        dataset = _parse_example_proto(dataset, 224, 224, 3, tf.float32, num_parallel_calls=4):\r\n\r\n        # tf.image.resize_image_with_crop_or_pad()  and random crop\r\n        dataset = _dataset_preprocess_fn(dataset, 224, 224, num_parallel_calls=4)\r\n\r\n        # Process dataset for training only.\r\n        if split.strip().lower() == 'train':\r\n            if shuffle:\r\n                dataset = dataset.shuffle(buffer_size=5000)\r\n        # Combines consecutive elements of this dataset into batches\r\n        dataset = dataset.batch(bsz)\r\n        # Batch iterator\r\n        iterator = dataset.make_one_shot_iterator()\r\n        input_batch, labels_batch = iterator.get_next()\r\n        labels_batch = tf.one_hot(labels_batch, 1000)\r\n        return input_batch, labels_batch\r\ndef _parse_example_proto(dataset, height, width, depth, dtype, num_parallel_calls=1):\r\n    def _input_parser(record):\r\n        keys_to_features = {\r\n            \"image/encoded\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n            \"image/format\": tf.FixedLenFeature((), tf.string, default_value=\"jpeg\"),\r\n            \"image/class/label\": tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),\r\n        }\r\n        parsed = tf.parse_single_example(record, keys_to_features)\r\n        image = tf.image.decode_jpeg(parsed[\"image/encoded\"], channels=depth)\r\n        image = tf.image.convert_image_dtype(image, dtype=dtype)\r\n        label = tf.cast(parsed[\"image/class/label\"], tf.int32)\r\n        return image, label\r\n    return dataset.map(_input_parser, num_parallel_calls=num_parallel_calls)\r\n\r\ndef _dataset_preprocess_fn(dataset, height, width, num_parallel_calls=1):\r\n    def _preprocess_fn(image, label):\r\n        image = tf.image.resize_image_with_crop_or_pad(image, height + 8, width + 8 )\r\n        image = tf.random_crop(image, [height, width, 3])\r\n        image = tf.image.random_flip_left_right(image)\r\n        image = (ISTD * image) + MEAN  #dataset level MEAN\r\n        return image, label\r\n    return dataset.map(_preprocess_fn, num_parallel_calls=num_parallel_calls)\r\n```", "Hi @tensorflowbutler , Hope you have all required data from me. Thanks.", "Assigning @raghuraman-k. This result is quite strange. It measures the graph construction time, not the actual graph execution time. So it's quite strange that tf.image.resize_image_with_crop_or_pad is taking a lot of time during graph construction.", "Nagging Assignee @raghuraman-k: It has been 228 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@aisuni Is this still an issue with the latest version of TensorFlow?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16395, "title": "change from deprecated version to a new version", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@MarkDaoust Thanks for the review!", "Failing test is unrelated."]}, {"number": 16394, "title": "cmake gpu build improvement", "body": "cmake build pass with gpu enabled\r\n\r\npython binding option can change to off now", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@jackyko1991 make sure you sign the CLA with the same email address that was used in the commits.", "I signed it!", "I have signed the CLA and has merged my code once\r\n\r\nNot sure why this time the not request for CLA again"]}, {"number": 16393, "title": "tensorflow object detection issue", "body": "i developed the custom object detector with some 96 images (train 76 and test 20) after trained 200k steps my losses(1) goes down correctly....after that i create the interference graph all ok..........but my question is when i run my code to detect the object from video it not able to find the correct object ........ thanks in advance.....\r\n\r\nactually i think we have a enough image for the dataset...because my dataset can able to detect the object from pre built video....so....it can detect the object from webcam also but not ...i dont kw the exact problem.....i think the resolution may b differ...thats y it not able to detect the object....\ufeff\r\n\r\n\r\nuse predefined \"object detection\" code with my own data set\r\nubuntu 16.04\r\ntensorflow installed in anaconda environment\r\ntensorflow 1.4.0\r\nbazel 0.4.5\r\ncpu version", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "use predefined  \"object detection\" code with my own data set\r\nubuntu 16.04\r\ntensorflow installed in anaconda environment\r\ntensorflow 1.4.0\r\nbazel 0.4.5\r\ncpu version\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16392, "title": "tensorflow object detection issue", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16391, "title": "Added early stopping and CheckpointSaverListeners to train and evaluate", "body": "Addresses the issue at https://github.com/tensorflow/tensorflow/issues/16203", "comments": ["Just wanted to ping @ispirmustafa for a review. This is my first time submitting a PR here, so wasn't sure if I was supposed to tag someone or not.", "@ispirmustafa please take a look at this.", "Pinging @xiejw ", "@xiejw could you please take a look?", "Sorry, I cannot approve this PR. \r\n\r\nSavingListener part looks good. But ValidationMonitor is known not working well in distributed model. So, we cannot support that. \r\n\r\nThe workaround for user, who needs this feature, is to define the monitors.ValidationMonitor in user code and pass to Experiment constructor (train_monitors arg). \r\n", "@xiejw I'm a little confused - I don't believe I added ValidationMonitor anywhere. I just passed arguments to its current usage.", "Hi,\r\n\r\nExperiment class is designed to support not only local (single-machine single-process, depending on definition) but also distributed training and evaluation. So, any constructor argument should fit both scenarios. Otherwise, it will confuse users and/or increase maintenance cost. \r\n\r\nValidationMonitor was in the code already as the original code only uses it to serve the purpose to do evaluation during training. This is an implementation detail, so ValidationMonitor can be replaced by SessionRunHook or SavingListener without affecting API and users. \r\n\r\nHowever, the parameters, such as early_stopping_metric, added in this PR is only supported in local case. This is a known fact.  It cannot work with distributed case -- which is a known hard problem. This is a major reason.\r\n\r\nIf early_stopping_metric, and other early stopping parameters can be added to support both local and distributed case, that will be great! The change could be huge as it needs to adjust all the code in train_and_evaluate(), train(), continuer_evaluate() and other places, as one proper definition of early stopping is, once the condition is met, all the nodes in the cluster should stop at more or less the same time. (It may have other definition also). \r\n\r\nIf user only needs this for local case, launching a python script to perform train and evaluate in one process, a workaround was mentioned. \r\n\r\nHope this helps. \r\n\r\n", "Got it, that makes more sense. Thank you!\r\n\r\nHow would someone do early stopping in the distributed version of the code right now?"]}, {"number": 16390, "title": "Maybe bug in quantize_graph.py", "body": "In line 489 of quantize_graph.py\r\n```python\r\ndef quantize_nodes_recursively(self, current_node):\r\n    \"\"\"The entry point for quantizing nodes to eight bit and back.\"\"\"\r\n    if self.already_visited[current_node.name]:\r\n      return\r\n```\r\nThe initial value of dic *self.already_visited* is empty ,so the statement\r\n```python\r\nif self.already_visited[current_node.name]:\r\n```\r\nmay raise exception.\r\nSo I think it would be better to update to \uff1a\r\n```python\r\nif self.already_visited.has_key(current_node.name):\r\n``` \r\nSame issue also in \uff1a\r\n```python\r\ndef round_nodes_recursively(self, current_node)\r\n```\r\nPlease take a look, thanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes, I can produce the error by adding test case. And there is a TODO:\r\nhttps://github.com/tensorflow/tensorflow/blob/6743031da633d1ce284a606c49eb00e793c1d729/tensorflow/tools/quantization/quantize_graph_test.py#L173\r\nThe lack of this test case hide the `KeyError` raised by `quantize_nodes_recursively()`\r\n\r\n`round_nodes_recursively` do not have this issue since it tests whether keys are in dict first.\r\n\r\nUPDATE:\r\nFound some discussion here #8999. It seems this python script is dprecated and the new Graph Transform Tool is recommended. Perhaps we can close this issue now.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16389, "title": "cherrypick bfloat16 changes", "body": "", "comments": ["@angersson lgtm, but I don't want to interfere with the release so you should merge. \r\n\r\n@panyx0718 thank you very much!", "Thanks for this!"]}, {"number": 16388, "title": "java.lang.InternalError: Cannot find requested resource bundle for locale en_US", "body": "bazel version  0.9.0\r\njava version \"1.8.0_161\"\r\nos\uff1aUbuntu 16\r\n\u7f16\u8bd1\u4e4b\u540e\u51fa\u73b0\u8fd9\u4e2a\u9519\u8bef\uff0c\r\n Building tensorflow/examples/android/libtensorflow_demo.jar (24 source files) failed (Exit 1)\r\njava.lang.InternalError: Cannot find requested resource bundle for locale en_US\r\n\r\n\u5e94\u8be5\u5982\u4f55\u4fee\u6539\u5462\uff0c\u8c22\u8c22\u5566\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "CUDA/cuDNN \uff0cGPU model and memory\u5728configure\u4e2d\u9009\u62e9\u7684no\r\n\u5f53\u6211\u6267\u884c\u8fd9\u4e2a\u547d\u4ee4\u65f6bazel build -c opt //tensorflow/examples/android:tensorflow_demo\uff0c\u6211\u540c\u65f6\u4e5f\u5728mac os\u6267\u884c\u4e86\u8fd9\u4e2a\u547d\u4ee4\uff0c\u51fa\u73b0\u4e86\u7c7b\u4f3c\u7684\u9519\u8befjava.lang.InternalError: Cannot find requested resource bundle for locale zh_CN,,\u4f46\u662f\u5f53\u6211\u6267\u884cbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \uff0c\u80fd\u591f\u6210\u529f\u7f16\u8bd1\u51faso\u6587\u4ef6\uff0c\u6240\u4ee5\u662f\u4e0d\u662f\u548c\u591a\u56fd\u8bed\u7cfb\u76f8\u5173\uff0c\u4f46\u662f\u5728\u4ee3\u7801\u4e2d\u6ca1\u6709\u627e\u5230\u76f8\u5173\u7684\u90e8\u5206,\u8fd8\u4e0d\u662f\u4e0d\u77e5\u9053\u5e94\u8be5\u5982\u4f55\u5904\u7406\uff0c\u8c22\u8c22\u60a8\u7684\u56de\u590d", "\u6211\u7ec8\u4e8e\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0csudo apt-get upgrade bazel\u4e0d\u8981\u6267\u884c\u8fd9\u4e00\u6b65\uff0c\u5b89\u88c5\u7684java\u7248\u672c\u662f1.8.0_151\uff0c\u7cfb\u7edf\u5fc5\u987b\u53ea\u5b89\u88c5\u4e00\u4e2ajava\uff0c\u6211\u628a\u6240\u6709java\u7248\u672c\u90fd\u5378\u8f7d\u4e4b\u540e\uff0c\u7136\u540e\u91cd\u65b0\u5b89\u88c5\uff0c\u5c31\u6210\u529f\u4e86"]}, {"number": 16387, "title": "Tensorflow not using GPU", "body": "I use windows in my laptop. Initially tensorflow worked well with GPU. I don't know why suddenly it's not detecting the GPU. I am using tensorflow-gpu 1.4 version (installed with pip install tensorflow-gpu) with CUDA 8.0 and cudnn 6.0. I also tried with other versions but the problem persists. Attached is the error message shown. I appreciate any help :)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: just called a session\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: NVIDIA GeForce 1050 2GB\r\n- **Exact command to reproduce**: sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\n(https://user-images.githubusercont\r\n![tferror](https://user-images.githubusercontent.com/19821962/35368823-48f11022-0153-11e8-87c3-41e41118a3fa.png)\r\nent.com/19821962/35368747-e2d4ba8c-0152-11e8-871a-a6a5aedf6663.png)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Thanks for your response. I have edited the same :)", "Try uninstalling and reinstalling from source maybe? That way it's more configured and customized to your machine.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). More people will see it there, and we don't have the bandwidth to debug installation issues here."]}, {"number": 16386, "title": "Using keras layers within an Estimator either causes training where it shouldn't or corrupts weights", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See gist\r\n\r\n### Describe the problem\r\n\r\nHey there,\r\n\r\nSo I have a `tf.keras` model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's `tf.keras.estimator.model_to_estimator` but, I'm having separate [issues with that](https://github.com/tensorflow/tensorflow/issues/16385).\r\n\r\nIn the [easily run reproduction here](https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c) I have (as a demonstration) a `tf.keras.layers.Embedding` which is initialized with all zeros and has `trainable=False`. Followed by that is a `Dense` layer with `use_bias=False` because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the `Dense` layer should always produce a zero, even after training it. Instead, it produces garbage!\r\n\r\nIn fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:\r\n1) I've set the loss to be 0 initially (l2_norm of what should start out as 0)\r\n2) Optimize with SGD using a learning rate of 0\r\n3) One training example that should have zero loss...\r\n\r\nThe output I actually get is very much non-zero. If I inspect the `embed/embeddings:0` tensor in `tfbdg`, I see this:\r\n```\r\narray([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                \r\n       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                \r\n       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                \r\n       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)\r\n```\r\n\r\nEven though it shouldn't have budged from all zeroes! \r\n\r\nSo, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line `61` where I start a new Session -- however this appears to be necessary, since I need to ensure that the `keras` backend is using the same `Graph` as `tensorflow.get_default_graph()` due to the peculiarities of how the Estimator calls the `model_fn`.\r\n\r\nNotably those values hold between:\r\n1) Runs of the estimator\r\n2) Successive runs with the same `tf.set_random_seed` value\r\n\r\nThe latter makes me think that somehow the `Embedding` layer is receiving *a* gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption. \r\n\r\nIf you need me to provide any more information let me know -- I'm sure I've left something out.\r\n\r\n", "comments": ["The solution is to create the embedding layer like this:\r\n\r\n```\r\n    emb = keras.layers.Embedding(*(embedding_mat.shape),\r\n                                 input_length=SEQ_LEN, name='embed', trainable=False,\r\n                                 embeddings_initializer='zeros')(inp_placeholder)\r\n```\r\n\r\nI think using the `weights` argument to initialize the [Embedding ](https://keras.io/layers/embeddings/)layer has been deprecated.", "Hm, ok I'll see if I can replicate this using the proper initializer, however the zeros matrix is just for demonstration.\r\n\r\nThe reason I'm using the `weights` argument here is that in reality I have a bunch of pre-trained embeddings I'm trying to initialize the layer with. Is the correct way to do that now a custom initializer for those embeddings?\r\n\r\nEDIT: Are you sure it's deprecated? The `Layer` class seems to [use](https://github.com/keras-team/keras/blob/master/keras/engine/topology.py#L321) [it](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/engine/topology.py#L183)...", "If you want to initialize the embedding with some pre-trained weights, then you can pass the [Constant initializer ](https://keras.io/initializers/)to the embedding_initiliazer argument.\r\n\r\nLooking at the [embeddings.py](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/keras/_impl/keras/layers/embeddings.py) source code in TensorFlow, the embeddings are created using the `Layer` class' `add_weight` method which does not accept the `weights` argument. So it really does look like it is deprecated. ", "Ok, I was able to get all zeros by setting the `embedding_initializer=keras.initializers.Zeros()`!\r\n\r\nI also didn't realize that the `Constant` initializer would take an arbitrarily shaped value -- I thought it only accepted scalars. It's not super clear in the docs, but I can see that it makes sense now.\r\n\r\nThanks!\r\n\r\nI'll close this but it would be nice if there was a deprecation warning that came up when trying to use the `weights` argument.", "Also, curiously, the use of `weights` is fine if I use a `keras` model and train that rather than an `Estimator`, even if I convert it to an `Estimator` first using the `tf.keras.estimator.model_to_estimator` call."]}]