[{"number": 14020, "title": "support DepthwiseConv2dNative at file tensorflow/python/tools/optimiz\u2026", "body": "support DepthwiseConv2dNative op when use python/tools/optimize_for_inference.py", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14019, "title": "avoid confict when installing pip3", "body": "When using [ parameterized_docker_build.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/parameterized_docker_build.sh) to build a tensorflow docker image from [nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04](https://hub.docker.com/r/nvidia/cuda/). There is [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/parameterized_docker_build.sh#L259) that change \"pip\" to \"pip3\" which will cause an error while installing pip3 because it change \"python-pip\" to \"python-pip3\".\r\n\r\nerror: install pip\r\n```bash\r\nFetched 24.6 MB in 19s (1269 kB/s)\r\nReading package lists...\r\nReading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nE: Unable to locate package python-pip3\r\nThe command '/bin/sh -c apt-get update && apt-get install -y --no-install-recommends         build-essential         curl         git         golang         libcurl3-dev         libfreetype6-dev         libpng12-dev         libzmq3-dev         pkg-config         python-dev python3-dev         python-pip3       rsync         software-properties-common         unzip         zip         zlib1g-dev         openjdk-8-jdk         openjdk-8-jre-headless         wget         &&     apt-get clean &&     rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100\r\nFAIL: nvidia-docker build of boss/tensorflow:latest-devel-gpu-py3 with Dockerfile /tmp/tmp.AO2xpf3Pw2/Dockerfile failed\r\n```\r\n\r\nerror: upgrade pip\r\n``` bash\r\n ---> 2798f59d37c5\r\nRemoving intermediate container 70f5cd7aac81\r\nStep 8/26 : RUN pip3 --no-cache-dir install --upgrade         pip3 setuptools\r\n ---> Running in ecd6622f86c0\r\nCollecting pip3\r\n  Could not find a version that satisfies the requirement pip3 (from versions: )\r\nNo matching distribution found for pip3\r\nThe command '/bin/sh -c pip3 --no-cache-dir install --upgrade         pip3 setuptools' returned a non-zero code: 1\r\nFAIL: nvidia-docker build of boss/tensorflow:latest-devel-gpu-py3 with Dockerfile /tmp/tmp.SFqy9rSx1b/Dockerfile failed\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "parameterized_ci_build script is something we are planning to get rid of. And the dockerfile you are editing at the moment is just to make sure we do not break our build with CUDA 9 and cudnn 7.\r\nSoon we are planning to upgrade out main dockerfiles and get rid of this extra file. Therefore I think an edit to this file at the moment is unnecessary."]}, {"number": 14018, "title": "Tutorial request for hybrid model (word+character) ", "body": "The implementation done in the paper:\r\nhttp://aclweb.org/anthology/P/P16/P16-1100.pdf\r\n\r\nis a hybrid seq2seq model with advancements where the encoder is fed with inputs based on following two cases:\r\n\r\n1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary\r\n\r\n2.Output of another LSTM network - when the word is **out of vocabulary** and a separate character based LSTM is used to **generate an embedding on the fly**\r\n\r\nConsider the following example sentence:\r\n\"The brown fox jumped over the lazy dog\"\r\n\r\nAssume these are the words present in the vocabulary: _The, brown, jumped, over, dog_ - These words are fed to the seq2seq encoder as such\r\n\r\nout of vocabulary(OOV) words are: _fox, lazy_ - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words\r\n\r\nThese both word level and character level encoder needs to be trained end to end simultaneously. \r\n\r\nSince the implementation is a bit different from the normal seq2seq can a tutorial or example of such case be added to the examples section?", "comments": ["@wolffg FYI", "second!\r\nI'm trying to implement this model in tf, ( https://arxiv.org/abs/1606.01700 ) which receives the same word at the word and character level to a lookup table and bidirectional LSTM (respectively), the output of both is then concatenated using a \"gate\" function before sending the imbedding to another LSTM.\r\nTheir Theano source-code is here: https://github.com/ nyu-dl/gated_word_char_rlm. \r\n", "@nealwu maybe a good suggestion for the model garden?", "Thanks, we'll keep this in mind.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Hi @MarkDaoust  is this issue being worked on or can I give it a try? Thanks", "It has been 29 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 14017, "title": "image_ops_test failing due to math_ops.sin() behaving differently", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.2.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5 \r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/image:image_ops_test\r\n\r\n### Describe the problem\r\nThe above test fails with the output array mismatch in [test_rotate_even](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/image/python/kernel_tests/image_ops_test.py#L48).\r\n\r\nThe exact cause of failure seems to be at [math_ops.sin(angles)](https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/image/python/ops/image_ops.py#L115), which only fails for `angle=(np.pi / 4.0) = 0.78539819` (45 deg) and works fine for 0 and 90 deg.\r\n\r\nThe output of `math_ops.sin(0.78539819).eval()` differs on Intel and s390x as below:\r\n1. Intel(test passes) : 0.70710**6769**\r\n2.  s390x(test fails) : 0.70710**6829**\r\n\r\nI verified `np.sin(0.78539819)` gives same output `0.7071067999` on both Intel and s390x.\r\n\r\nWhy is the difference seen in math_ops.sin()? Any pointers would be helpful.\r\n\r\n### Source logs\r\n----------------------------------------------------------------------\r\nFAIL: test_rotate_even (__main__.ImageOpsTest)\r\n----------------------------------------------------------------------\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/contrib/image/python/kernel_tests/image_ops_test.py\", line 75, in test_rotate_even\r\n    [1, 7, 13, 19, 25, 31], [0, 6, 12, 18, 24, 30]]])\r\n  File \"/home/test/.cache/bazel/_bazel_test/24685d064c07f7346b48c2d13ec3ad69/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 699, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b)\r\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 807, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/usr/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 733, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 1.85185185185%)\r\n x: array([[[ 0,  1,  2,  3,  4,  5],\r\n        [ 6,  7,  8,  9, 10, 11],\r\n        [12, 13, 14, 15, 16, 17],...\r\n y: array([[[ 0,  1,  2,  3,  4,  5],\r\n        [ 6,  7,  8,  9, 10, 11],\r\n        [12, 13, 14, 15, 16, 17],...\r\n\r\nnot equal where =  (array([1, 1]), array([3, 4]), array([3, 4]))\r\nnot equal lhs =  [20 32]\r\nnot equal rhs =  [21 33]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["I'm afraid we don't have the resources to support all platforms, and we don't have support for s390x. Tagging community support.", "@mrry, any thoughts on the issue? \r\nWhere can I refer the implementation for math_ops.sin/cos functions? ", "The implementations of these ops are thin wrappers around Eigen:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/0623993f08c5357302bf64a50a52120da323ed25/tensorflow/core/kernels/cwise_ops.h#L576-L592", "After debugging more into issue which @namrata-ibm had found out, it turned out an issue with NumPy (similar to [12963](https://github.com/tensorflow/tensorflow/pull/12963)).\r\n\r\nOn Intel:\r\n```\r\n>>> np.sin(np.array([0.0, np.pi / 4.0, np.pi / 2.0], dtype=np.float32))\r\narray([ 0.        ,  0.70710677,  1.        ], dtype=float32)\r\n```\r\nOn zSystems:\r\n```\r\n>>> np.sin(np.array([0.0, np.pi / 4.0, np.pi / 2.0], dtype=np.float32))\r\narray([ 0.        ,  0.70710683,  1.        ], dtype=float32)\r\n```\r\n\r\n\r\nIf we do not specify the data type to the array, NumPy is giving same results on both the platforms.\r\nOn Intel:\r\n```\r\n>>> np.sin(np.array([0.0, np.pi / 4.0, np.pi / 2.0]))\r\narray([ 0.        ,  0.70710678,  1.        ])\r\n```\r\n\r\nOn zSystems:\r\n```\r\n>>> np.sin(np.array([0.0, np.pi / 4.0, np.pi / 2.0]))\r\narray([ 0.        ,  0.70710678,  1.        ])\r\n```\r\n\r\n@mrry Please share your thoughts on this.", "I assume it\u2019s some difference between in the rounding behavior for `np.float32` and `np.float64` on zSystems. Not my area of experience, alas\u2026", "@mrry As you said correctly this is some rounding behavior difference on zSystems. Could you please share  why Tensorflow uses int32 and float32 over python/numpy's default int64 and float64? Will it be a major change if we go for 64 bit variant? Asking just out of curiosity :)", "I don't think we can change that behavior without breaking backwards compatibility. I tracked down the original commit that added [this behavior](https://github.com/tensorflow/tensorflow/blame/4cdc52964ae24f2c2b863939b4dc9cbe9cd9d104/tensorflow/python/framework/tensor_util.py#L424), by @zffchen78. There's no documentation of the rationale, but perhaps ZF can comment.\r\n\r\n(I suspect the reason is something like \"Nobody wanted to use double precision to train their networks when we started, and choosing this default avoids the need for lots of explicit `tf.float32` casts.\" The trend towards lower-precision training seems to back this up.)", "TensorFlow was largely motivated by our previous gen DL framework and applications at that time. Then, double was frowned upon by DL community for its slowness and the extra precision was not needed by those models. int64 & float64 on GPUs were also very slow and virtually nobody cared. So, int32 & float32 were more defacto data types. ", "Closing this as per discussion in #13638. "]}, {"number": 14016, "title": "CMake-Linux support for GPU-build", "body": "There had been some fixes for Linux\r\n- Enable to specify or find related libraries (cuda/cudnn)\r\n- Enable \"find_package(CUDA)\"\r\n- Address the Linux Cmake-parser issue with TF_EXTRA_CUDA_CAPABILITIES=3.0,3.5,5.2\r\n- Improve the method to find static libraries\r\n- Enable tf_stream_executor for Linux-CMake build by linking to \"libgomp\"\r\n- Adjust pywarp_tensorflow_lib configuration\r\n- Hide build option not recognized at Linux toolchains\r\n- Add resampler cu.cc files\r\n- Don't use tf_core_kernels_cpu_only for Linux (it's for Windows only)\r\n\r\nTested with OBS (open build service) + rpmbuild for bare-metal (minimal packages are installed) x64 Linux.\r\n\r\nSigned-off-by: MyungJoo Ham <myungjoo.ham@samsung.com>", "comments": ["Can one of the admins verify this patch?", "Addressing #14014", "@tensorflow-jenkins test this please.", "I just kicked off a Windows GPU build to check that there are no regressions there either: http://ci.tensorflow.org/view/All/job/tf-pr-win-cmake-gpu/15/", "@martinwicke also ready to merge"]}, {"number": 14015, "title": "Set reuse=False instead of reuse=None, and add suggestion for tf.AUTO_REUSE", "body": "As per https://github.com/tensorflow/tensorflow/issues/13887 we'd like to change the message that contains \"reuse=None\" to \"reuse=False\" and suggest the use of tf.AUTO_REUSE.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14014, "title": "Proposal: Compiling TF 1.4.0 GPU w/ CMAKE on Linux x64", "body": "Currently, I have been playing with the cmake scripts of TensorFlow-GPU to make it CMAKE-buildable in a bare-metal x64 Linux. (isolated environment, without the internet, w/ OBS)\r\n(I'm trying CMAKE because I don't want to port Java)\r\n\r\nIt's almost done (need code clean) and I'll probably send Pull-Request next week. However, there are a few things I want to check before I write additional features besides simply make it able to build with CMAKE for TF-GPU in Linux.\r\n\r\n1. Tensorflow is statically linking to pre-installed devel packages. I think in some cases, it might be better to do dynamic linking (use .so) to other libraries (to save some memory and storage size). May I simply add an option like \"tensorflow_USE_SHARED_LIBS_CUDA\"? (e.g., cuda, nccl, ...)\r\n\r\n2. Tensorflow is downloading a lot of external packages and use them \"statically\", which is pretty awful for some people. I'd like to make it use shared libraries as well (maybe along with some version restrictions). May I simply add an option like \"tensorflow_USE_SHARD_LIBS_JSONCPP\", which disables downloading \"JSONCPP\" as well?\r\n\r\nSome additional benefit with these might be reduced memory requirement for building tensorflow; memory consumption of tensorflow-build gets dangerous at the last step with ```ld```. Probably, for now, I may simply need to be satisfied with letting ```make``` reduce ```-j#``` only for ```ld``` steps.\r\n\r\n\r\nCC: @leemgs\r\nSTATUS: failing at the last step. (without ```-Dtensorflow_BUILD_SHARED_LIB```, it works anyway)\r\n```\r\n...\r\n[ 2457s] /usr/lib64/gcc/x86_64-tizen-linux-gnu/6.2.1/../../../../x86_64-tizen-linux-gnu/bin/ld: libtf_core_gpu_kernels.a(tf_core_gpu_kernels_generated_beam_search_ops_gpu.cu.cc.o): relocation R_X86_64_32 against `.data' can not be used when making a shared object; recompile with -fPIC\r\n[ 2457s] /usr/lib64/gcc/x86_64-tizen-linux-gnu/6.2.1/../../../../x86_64-tizen-linux-gnu/bin/ld: libtf_core_gpu_kernels.a(tf_core_gpu_kernels_generated_resampler_ops_gpu.cu.cc.o): relocation R_X86_64_32 against `.data' can not be used when making a shared object; recompile with -fPIC\r\n[ 2457s] /usr/lib64/gcc/x86_64-tizen-linux-gnu/6.2.1/../../../../x86_64-tizen-linux-gnu/bin/ld: final link failed: Nonrepresentable section on output\r\n[ 2457s] collect2: error: ld returned 1 exit status\r\n[ 2457s] CMakeFiles/tensorflow.dir/build.make:2235: recipe for target 'libtensorflow.so' failed\r\n[ 2457s] make[2]: *** [libtensorflow.so] Error 1\r\n[ 2457s] CMakeFiles/Makefile2:82: recipe for target 'CMakeFiles/tensorflow.dir/all' failed\r\n[ 2457s] make[1]: *** [CMakeFiles/tensorflow.dir/all] Error 2\r\n[ 2457s] make[1]: *** Waiting for unfinished jobs....\r\n[ 2458s] [100%] Linking CXX shared library libpywrap_tensorflow_internal.so\r\n[ 2474s] [100%] Built target grpc_tensorflow_server\r\n[ 2474s] [100%] Built target transform_graph\r\n```\r\n\r\n-- STATUS UPDATE: build successful w/ ```-Dtensorflow_BUILD_SHARED_LIB=ON``` as well.", "comments": ["@Garoe, @johnsrude, @dmacvicar, and @yongtang PTAL, If you have a similar experience (#13962, #13061) that you already tried to build Tensorflow with **CMake** instead of Bazel, Could you give us a hint (or share your experience)?", "@gunan @mrry can you you comment or redirect?", "We are currently evaluating the fate of the cmake build. It may either become a \"first class citizen\" or completely go away. I should be able to comment more in about a week.", "@gunan I sincerely hope it will not go away or it will be difficult to integrate TensorFlow with all the big C++ libraries out there. ", "> We are currently evaluating the fate of the cmake build. It may either become a \"first class citizen\" or completely go away. I should be able to comment more in about a week.\r\n\r\nCould you please share how it went? ", "Still looking into a few more things. I will update one we have a decision or counter proposal.", "@gunan Please do not drop cmake support!", "> Still looking into a few more things. I will update one we have a decision or counter proposal.\r\n\r\nIs there any update?", "@ewilderj @martinwicke ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "> Still looking into a few more things. I will update one we have a decision or counter proposal.\r\n\r\n@gunan, is there any update?  ", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "> Still looking into a few more things. I will update one we have a decision or counter proposal.\r\n\r\n@gunan, @mrry is there any update? ", "If you're interested in this issue, get on the [tf-distribute Google Group](https://groups.google.com/a/tensorflow.org/forum/?utm_source=digest&utm_medium=email#!forum/tf-distribute) where they're discussing a community-based solution to the problem. \r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Yes, I think we can close this bug and redirect questions to [SIG Build](https://groups.google.com/a/tensorflow.org/forum/#!forum/build). Please see discussions there for detailed status of CMake build. TL;DR the community would rather first investigate making the Bazel build meet more use cases, so get involved there to express your use cases and help see if Bazel can work.\r\n\r\nIf Bazel doesn't work out we will keep CMake build as a repo but it will need community contribution to maintain, as now Windows builds on Bazel, the project team has no direct dependency on it."]}, {"number": 14013, "title": "#tensorflow##PYNQ# Could I install tensorflow in my PYNQ-Z1 ? ", "body": "I wanna build some Classifier with it. I'm used to build Classifiers with TensorFlow. \r\nSo I want to know whether it is technically possible.THX!!\r\n[http://www.pynq.io/](url)", "comments": ["I am not very familiar with pynq but [the only mention of pynq and TF together I could find](https://groups.google.com/forum/#!topic/pynq_project/kG8ZtRg4k_A) suggests that pynq is 32-bit. If that's the case I don't think it will work because TensorFlow is 64-bit only.", "We have never tried this ourselves. You can try to adapt our raspberry pi or mobile build for your device. Those are the builds that has the most chance to work on a 32-bit system (but even they could fail).\r\nBut I suspect there will be compilation failures. You will need to work through them yourself, as we do not have this device ourselves and we wont be able to reproduce and fix these issues.\r\n\r\nOnce you have a working build, we would happily accept your fixes into our main repository, but we won't be able to provide support for you with this build.", "From [this repository](https://github.com/hillhao/PYNQ-project) I found the following:\r\n\r\n> The commands for installing TensorFlow in PYNQ board as follows:\r\n```\r\n# For Python 3.4\r\nwget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl\r\nsudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl\r\n```", "i try that command but not work on my pynq :(", "See details here:\r\nhttps://github.com/hillhao/PYNQ-project#for-python-programming-enironment-in-pynq-board", "i try that command but not work on my pynq z2:(\r\n\r\n The directory '/home/xilinx/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nThe directory '/home/xilinx/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nRequirement 'tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl' looks like a filename, but the file does not exist\r\n\r\ntensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl is not a supported wheel on this platform.rm.\r\n\r\n", "wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v0.9.0/tensorflow-0.9.0-py3-none-any.whl \r\n\r\n/usr/bin/python3 -m pip install tensorflow-0.9.0-py3-none-any.whl ", "the latest tensorflow version in github is 0.11.0, but the tensorflow version older than 1.9.0 has the drawback: tf.session.close() can't release the memory of the gpu card\uff0cso the memory can exhaust, \r\nthe version newer than or equal to 1.9.0 solves the problem;\r\nso we need a tensorflow version newer than or equal to 1.9.0 that can run on pynq;\r\nIf there is a detailed introduction about how to cross-compile tensorflow running on pynq or Raspberry?\r\n\r\n> wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v0.9.0/tensorflow-0.9.0-py3-none-any.whl\r\n> \r\n> /usr/bin/python3 -m pip install tensorflow-0.9.0-py3-none-any.whl\r\n\r\n"]}, {"number": 14011, "title": "iOS/RPi Add the ability to choose ANDROID_TYPES_FULL", "body": "Some networks require \"full\" types instead of \"slim\" so remove\r\nthe hard coding of SLIM in iOS and RPi. It still defaults to\r\nbuilding SLIM for them if not ENV var is specified but now\r\nyou can build with\r\n\r\nANDROID_TYPES=\"-D__ANDROID_TYPES_FULL\" \\\r\n./tensorflow/contrib/makefile/build_all_ios.sh\r\n\r\nTEST: Verify the  -D__ANDROID_TYPES_SLIM__ flag is default and\r\nyou can override with an env var", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14010, "title": "Fix documentation error in tf.size()", "body": "`tf.size()` returns a symbolic `Tensor`, not an integer.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 14009, "title": "Add link to datasets doc", "body": "Link to datasets doc to make it easier to find.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.\r\n"]}, {"number": 14008, "title": "tf.python.keras can't import np_utils", "body": "Typically i'm able to replace any \"from keras.X import Y\" with \"from tensorflow.python.keras.X import Y\"\r\n\r\nThis works with most things, but it does not translates to keras.utils. I can run:\r\nfrom keras.utils import np_utils\r\n\r\nBut the following fails:\r\nfrom tensorflow.python.keras.utils import np_utils\r\n\r\nThe workaround is just to use: \r\nfrom tensorflow.python.keras._impl.keras.utils import np_utils\r\n\r\nUnsure if this is intended or not. I guess I'd consider this either a bug fix or a feature request. ", "comments": ["@fchollet ", "`np_utils` is not part of the public API. Please import symbols from `tensorflow.keras.utils` instead.", "for tf 1.3.1\r\n`from tensorflow.contrib.keras.python.keras.utils    import np_utils`\r\n\r\nfor tf 1.4.1\r\n`from tensorflow.python.keras          import utils`", "in tensorflow 2.0.0 ?", "In tensorflow 2.1.0\r\nfrom tensorflow.keras import utils", "in tensorflow 2.2.0?", "> in tensorflow 2.2.0?\r\n\r\nin tensorflow 2.3.0 you can do as following.  2.2.0 version might be same\r\n\r\n`import tensorflow.keras.utils as utils`\r\nor\r\n`from tensorflow.keras import utils`", "I have tensorflow 2.3.0 installed on my system !!\r\nAnd it throws the following error : AttributeError: module 'tensorflow.keras.utils' has no attribute 'np_utils'\r\nCan anyone help me solve this issue !!\r\nThanks in advance...........", "from tensorflow.keras.utils import to_categorical", "confused about where I can find the detailed announcement of this change. cuz I'm migrating from a very old version and \"from keras.utils import np_utils\" works but \"from tf.keras.utils import np_utils\" does not."]}, {"number": 14007, "title": "BatchNorm not working in a _FuncGraph", "body": "I am working on macOS 10.12.6 with TensorFlow 1.3.0, Python version: 3.6.2. CPU only.\r\n\r\nI found that if `tf.contrib.layers.batch_norm` is called inside a `Defun`, a `TypeError` will be thrown. To clarify, the function here is a `map_func` that will be used in `Dataset.map` invoke, instead of a normal python function.\r\n\r\nTo regenerate the scenario, try this piece of code:\r\n\r\n```\r\nDataset.range(27).batch(27) \\\r\n  .map(lambda x: tf.cast(tf.reshape(x, (3, 3, 3)), tf.float32) / 32.0) \\\r\n  .map(lambda img: tf.contrib.layers.batch_norm(img)) \\\r\n  .make_one_shot_iterator().get_next().eval()\r\n```\r\n\r\nYou will get an error like this:\r\n\r\n```\r\nTypeError: In op 'BatchNorm/AssignMovingAvg', input types ([tf.float32, tf.float32]) are not compatible with expected types ([tf.float32_ref, tf.float32])\r\n```\r\n\r\nI did some investigation. One thing that I found is that inside the `batch_norm`, we will assign a new value to the `mean` variable by calling `assign_sub`, which accept a variable with **ref** type and a value with a **basic type**. But the variable has been created some where in the `_FuncGraph` with a **basic type**, instead of a **ref** type, which make the compatibility check failed.\r\n\r\nI will keep digging into this issue, but I think this seems a bug.", "comments": ["@mrry if this isn't supported can you document/give a better error message do you think?", "Reassigning to @akshayka, since he is currently revamping the function API.", "This somehow slipped under my radar; I'll dig into this ...", "Nagging Assignee @akshayka: It has been 120 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This won't be fixed for Defun(), which is being replaced with tf.contrib.eager.defun. Once Datasets switch to using tf.contrib.eager.defun, the pasted code will work as intended."]}, {"number": 14006, "title": "Branch 173560463", "body": "", "comments": []}, {"number": 14005, "title": "Branch 173553770", "body": "", "comments": ["There are too many test failures. I'll make another pull request once the fixes available."]}, {"number": 14004, "title": "[Windows] Speech commands tutorial does not work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I'm running `python tensorflow/examples/speech_commands/train.py`\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64\r\n- **TensorFlow installed from (source or binary)**: `pip install tf-nightly`  (tf_nightly-1.5.0.dev20171026-cp35-cp35m-win_amd64.whl)\r\n- **TensorFlow version (use command below)**: b'unknown' 1.5.0-dev20171026\r\n- **Python version**: Python 3.5.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.1 x64\r\n- **GPU model and memory**: 2x Nvidia GTX 670 2GB\r\n- **Exact command to reproduce**: `python tensorflow/examples/speech_commands/train.py`\r\n\r\n### Describe the problem\r\nIt appears that the currently nightlies (or 1.4.0rc1) do not contain the gen_audio_ops module. However the documentation for r1.4 or master (https://www.tensorflow.org/versions/r1.4/tutorials/audio_recognition or https://www.tensorflow.org/versions/master/tutorials/audio_recognition) appears to indicate that the speech_commands demo should work.\r\n\r\nRelated: #13031\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 81, in <module>\r\n    import input_data\r\n  File \"E:\\Tom\\Documents\\GIT\\tensorflow\\tensorflow\\examples\\speech_commands\\input_data.py\", line 35, in <module>\r\n    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\n  File \"C:\\Users\\Tom\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\audio_ops.py\", line 31, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import *\r\nImportError: No module named 'tensorflow.python.ops.gen_audio_ops'\r\n```\r\n", "comments": ["@petewarden Are there any platform dependencies holding these ops back from being available on Windows?", "(Actually, this might be a very easy fix, so I'm sending you a CL that might just fix it.)", "Based on the logs available @ http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=35/90/consoleText (the tf-nightly-windows build for python 3.5 of November 3rd, 2017), it appears that gen_audio_ops.py is still not generated.\r\n\r\n```\r\n[...]\r\n// should be here (~line 9056)\r\n  Generating tf_python/tensorflow/python/ops/gen_array_ops.py\r\n  Generating tf_python/tensorflow/python/ops/gen_bitwise_ops.py\r\n[...]\r\n// should be here (~line 66164)\r\n  copying tensorflow\\python\\ops\\gen_array_ops.py -> build\\lib\\tensorflow\\python\\ops\r\n  copying tensorflow\\python\\ops\\gen_bitwise_ops.py -> build\\lib\\tensorflow\\python\\ops\r\n```", "Thanks for letting us know. Looks like I missed a line in the CMake build... will send a PR shortly.", "I have the same problem. The code is pull 5 hours ago.\r\n\r\n**System information**\r\n**Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** No, I'm running python tensorflow/examples/speech_commands/train.py\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Windows 10 x64\r\n**TensorFlow installed from (source or binary):** pip install tf-nightly (tf_nightly-1.5.0.dev20171026-cp35-cp35m-win_amd64.whl)\r\n**TensorFlow version (use command below):** b'unknown' 1.5.0-dev20171104\r\n**Python version:** Python 3.6.3\r\n**Bazel version (if compiling from source):** N/A\r\n**CUDA/cuDNN version:** CUDA 8.0, cuDNN 6.0\r\n**GPU model and memory:** Nvidia GTX 960M , 8GB\r\n**Exact command to reproduce:** python tensorflow/examples/speech_commands/train.py", "@NearLinHere The  Wheel filename includes `20171026`, which suggests that it's the nightly build from October 26th 2017. Can you try against with the latest nightly build?", "This is the error I get:\r\n`c:\\Users\\Brian\\Anaconda3\\Lib\\site-packages\\tensorflow\\examples\\speech_commands>python train.py\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    import input_data\r\n  File \"c:\\Users\\Brian\\Anaconda3\\Lib\\site-packages\\tensorflow\\examples\\speech_commands\\input_data.py\", line 35, in <module>\r\n    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\n  File \"C:\\Users\\Brian\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\audio_ops.py\", line 31, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import *\r\nModuleNotFoundError: No module named 'tensorflow.python.ops.gen_audio_ops'\r\n\r\nc:\\Users\\Brian\\Anaconda3\\Lib\\site-packages\\tensorflow\\examples\\speech_commands>`\r\n\r\nCan someone tell me how to update my TensorFlow install to get the missing libraries once this problem is fixed?", "@KevlarTheGreat Which version of TensorFlow have you installed? The fix is only available in the `tf-nightly` package, and will be part of TF 1.5.", "@mrry it's fixed. thank you \ud83d\udc4d ", "I have the same problem.\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, I'm running python tensorflow/examples/speech_commands/train.py\r\n    OS Platform and Distribution: Windows 10 x64\r\n    TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow\r\n    TensorFlow version (use command below): tensorflow-1.4.0-cp36-cp36m-win_amd64.whl\r\n    Python version: Python 3.6\r\n    Exact command to reproduce: python tensorflow/examples/speech_commands/train.py\r\n\r\nSpeech Commands example was downloaded yesterday from github:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands\r\n", "@przemoo9621 See my comment [above](https://github.com/tensorflow/tensorflow/issues/14004#issuecomment-342582086). You'll need to upgrade to a newer version (`tf-nightly` or wait for the 1.5 release).", "@mrry thanks for your reply. I changed my tensor flow version to a newer version tf_nightly. This solve my problem, but now I have another one.\r\n\r\nPS C:\\Python36\\lib\\site-packages> python tensorflow\\examples\\speech_commands\\train.py\r\n2017-11-11 16:59:17.282443: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu\r\n_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nTraceback (most recent call last):\r\n  File \"tensorflow\\examples\\speech_commands\\train.py\", line 429, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 133, in run\r\n    _sys.exit(main(argv))\r\n  File \"tensorflow\\examples\\speech_commands\\train.py\", line 106, in main\r\n    FLAGS.testing_percentage, model_settings)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\examples\\speech_commands\\input_data.py\", line 161, in __init__\r\n    testing_percentage)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\examples\\speech_commands\\input_data.py\", line 243, in prepare_data_index\r\n    word = re.search('.*/([^/]+)/.*.wav', wav_path).group(1).lower()\r\nAttributeError: 'NoneType' object has no attribute 'group'", "@petewarden If I'm reading that line of code correctly, it's trying to extract a path component using a regular expression with `'/'` as the path separator.", "Thanks for letting us know. It looks like that code is relying on `'/'` being the path separator. I've just sent PR #14519 that should fix this.", "@mrry Thanks for your help, everything works fine now. ", "@mrry thank you for your kindly help\r\nI met another problem when I try to let model recognize more words. Here is my command: \r\n\r\n`python tensorflow/tensorflow/examples/speech_commands/train.py --data_dir=speech_commands_data --model_architecture=low_latency_conv --wanted_words=yes,no,up,down,left,right,on,off,stop,go,happy,house,cat,dog,bird,three,tree`\r\n\r\nand after 400 steps. The following error message is shown:\r\n`Traceback (most recent call last):\r\n  File \"tensorflow/tensorflow/examples/speech_commands/train.py\", line 429, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"D:\\[TS040]\\201711 audio recognition\\py36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/tensorflow/examples/speech_commands/train.py\", line 248, in main\r\n    total_conf_matrix += conf_matrix\r\nValueError: operands could not be broadcast together with shapes (19,19) (18,18) (19,19)`\r\n", "so i changed it to \r\n```\r\n_, word = os.path.split(os.path.dirname(wav_path))\r\nword = word.lower()\r\n\r\n```\r\nin line 243 on train.py and it works!\r\n\r\nThanks in advance for the developers! Cheers", "@tenapril Thanks for confirming that it works!\r\n\r\n@NearLinHere I'm not sure what the problem is there... I don't think it's platform related... but hopefully @petewarden can shed some light.", "@NearLinHere A quick update: I think PR #14600 should fix it. Thanks for letting us know the problem!", "@mrry I follow the indirection  #14600  and it fixed. Thank you very much.", "Hi all the nightly build 1.5 fixed the training but I met with another problem when freezing the model. Here's the error...\r\n2017-12-19 08:52:25.336229: W C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-19 08:52:26.539263: W C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-19 08:52:27.734690: W C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-19 08:52:28.926234: W C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-19 08:52:30.113852: W C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\n2017-12-19 08:52:31.136618: W C:\\tf_jenkins\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1198] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for\r\nTraceback (most recent call last):"]}, {"number": 14003, "title": "..", "body": "", "comments": []}, {"number": 14002, "title": "Bazel build for CUDA failed on Ubuntu w/ lastest tip + CUDNN 6.0 + CUDA 8.0", "body": "Hi,\r\n\r\nI am trying to build latest TF with CUDNN 6.0 + CUDA 8.0 on Ubuntu 14 but it failed with the following error message. I found some similar issue (https://github.com/tensorflow/tensorflow/issues/469) reported in the past, not sure if the latest tip has fixed it?\r\n\r\nAny suggestion would be appreciated.\r\n\r\nThanks\r\n\r\n**Build Command**\r\n- build command for CUDA that failed\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package `\r\n\r\n- build for CPU works well\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**System Info**\r\n- bazel version : Build label: 0.5.4\r\n- CUDA: 8.0\r\n- CUDNN 6.0\r\n- TF origin/master latest sync as 10/26/17 (cb7cb40 Merge pull request #13972 from taehoonlee/fix_typos)\r\n\r\n**Error message:**\r\n`ERROR: $PROJECT_ROOT/tensorflow/tensorflow/stream_executor/BUILD:52:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:cuda_platform':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_blas.cc':\r\n  '/usr/local/cuda/include/cublas_api.h'\r\n  '/usr/local/cuda/include/driver_types.h'\r\n  '/usr/local/cuda/include/host_defines.h'\r\n  '/usr/local/cuda/include/cuComplex.h'\r\n  '/usr/local/cuda/include/vector_types.h'\r\n  '/usr/local/cuda/include/builtin_types.h'\r\n  '/usr/local/cuda/include/device_types.h'\r\n  '/usr/local/cuda/include/surface_types.h'\r\n  '/usr/local/cuda/include/texture_types.h'\r\n  '/usr/local/cuda/include/cuda_fp16.h'\r\n  '/usr/local/cuda/include/library_types.h'\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cudaDataType_t perftools::gputools::cuda::{anonymous}::CUDAComputationType(perftools::gputools::blas::ComputationType)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:527:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\n`", "comments": ["@yifeif can you take a look or redirect? Thanks.", "Just to make sure, could you share how you ran `configure` script?\r\nI believe this is a duplicate of #10665", "Thanks for reply @skye and @gunan . I found the root cause, it turn out there was a problem in the CUDNN header that I modified the path, which is not added to bazel header list properly.\r\n\r\nClosing the issue now.\r\n\r\nThanks", "Hi, can you provide more details on the CUDNN header problem and the solution provided ?\r\nActually compiling Tensorflow from one version to the other on not officially supported distributions is always a mess. Sharing helps a lot.\r\nThanks in advance"]}, {"number": 14001, "title": "layers-based cuDNN RNN functionality not working", "body": "I'm trying to use the newly added layers-style cuDNN RNN functionality. I'm running TF 1.4.0-rc0 on Ubuntu with Pascal GPUs, compiled from source with CUDA 8 and cuDNN 7. When trying to import the relevant library:\r\n\r\n`from tensorflow.contrib.cudnn_rnn.python.layers import cudnn_rnn`\r\n\r\nI get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 33, in <module>\r\n    resource_loader.get_path_to_datafile(\"_cudnn_rnn_ops.so\"))\r\n  File \"/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /usr/local/tensorflow-1.4.0-rc0/local/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/_cudnn_rnn_ops.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\nNote that the non-layer based cuDNN RNN functionality works fine. I.e. I can run this:\r\n\r\n`from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops`\r\n\r\nand run cuDNN-based RNNs with no problem otherwise.", "comments": ["@alquraishi Did you try to set dynamic path to solve the error `cannot open shared object file: No such file or directory`?\r\nCan you fill the ISSUE_TEMPLATE [here](https://github.com/printdhruv/tensorflow/blob/master/ISSUE_TEMPLATE.md)?", "Not sure what you mean by \"dynamic path\", but if I create a symbolic link in `.../layers/_cudnn_rnn_ops.so` to `.../ops/_cudnn_rnn_ops.so` it solves the problem. I believe this is a bug. I think the path in `tensorflow/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py` pointing to `_cudnn_rnn_ops.so` is incorrect.\r\n\r\nHere's the rest of the information you requested\r\nOS: Ubuntu 14.04\r\nTensorFlow 1.4.0-rc0 installed from source\r\nPython version: 2.6\r\nBazel version (if compiling from source): 0.7.0\r\nCUDA/cuDNN version: 8/7\r\nGPU model and memory: Titan X Pascal", "@alquraishi Yes. By \"dynamic path\" I meant to play with the symbolic link! Can you tell what you changed/modified in `tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py` \r\n\r\nfor the lines 32-33 which worked for you ,\r\n\r\n`_cudnn_rnn_ops_so = loader.load_op_library(\r\n    resource_loader.get_path_to_datafile(\"_cudnn_rnn_ops.so\"))`", "I actually didn't muck with the `.py` file. I just created a symbolic link in the system so that the `layers` directory effectively contains its own copy of `_cudnn_rnn_ops.so`. I was just saying that the bug is in `tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py`", "I see! Can you try to play with the `cudnn_rnn.py` ? It would be great if you can find solution without adding symbolic link?\r\nAlso,please `reply/paste the the exact symbolic link `which you have used!", "Hi, the symbolic link command was:\r\n\r\n`ln -s ../ops/_cudnn_rnn_ops.so _cudnn_rnn_ops.so`\r\n\r\nexecuted in `/usr/local/tensorflow-1.4.0-rc0/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers`\r\n\r\nAlso, modifying the `cudnn_rnn.py` file so that lines 32-33 read:\r\n\r\n`_cudnn_rnn_ops_so = loader.load_op_library(\r\n    resource_loader.get_path_to_datafile(\"../ops/_cudnn_rnn_ops.so\"))`\r\n\r\nfixes the problem as well.", "In `_cudnn_rnn_ops_so = loader.load_op_library( resource_loader.get_path_to_datafile(\"../ops/_cudnn_rnn_ops.so\"))` is the `../ops/_cudnn_rnn_ops.so` sufficient ?", "Yes that's all that is necessary to make it work.", "@alquraishi Thanks for sending out a reply. I am not sure will `../ops/_cudnn_rnn_ops.so` be enough for \r\nall OS, as `load_op_library and get_path_to_datafile` are responsible for handling OS specific operations?\r\nI sent a PR to for approval.", "Yes you're right it would have to be platform-specific, presumably `os.path` should work.", "Seems like the PR is in flight, so marking this as contributions welcome.", "Addressed in https://github.com/tensorflow/tensorflow/commit/a6a61884396ef1d51b01f8e13df21becb23fd0c8 "]}, {"number": 14000, "title": "Model trained on GPU does not restore properly when ran on CPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux  3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u5 (2017-09-19) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux  3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u5 (2017-09-19) x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-gpu (1.4.0rc0)\r\ntensorflow-tensorboard (0.1.5)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0-rc0\r\ntf.GIT_VERSION = v1.3.0-rc1-3112-g65b6a75\r\ntf.COMPILER_VERSION = v1.3.0-rc1-3112-g65b6a75\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /cuda/cuda-8.0/lib64:/cuda/cudnn-8.0-linux-x64-v6.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Oct 26 16:47:55 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GT 610      Off  | 0000:01:00.0     N/A |                  N/A |\r\n| N/A   42C    P0    N/A /  N/A |      0MiB /   963MiB |     N/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0                  Not Supported                                         |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n```\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled binary with pip.\r\n- **TensorFlow version (use command below)**:\r\n('v1.3.0-rc1-3112-g65b6a75', '1.4.0-rc0')\r\nThe same bug happens on Tensorflow 1.3.0 too.\r\n- **Python version**: \r\n2.7\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0\r\ncuDNN 6.0\r\n- **GPU model and memory**:\r\nI am testing on two different computers. On one PC I have this GPU:\r\nname: GeForce GTX TITAN Black major: 3 minor: 5 memoryClockRate(GHz): 0.98\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 5.94GiB freeMemory: 5.87GiB\r\nOn my PC I do not have a GPU.\r\n\r\n- **Exact command to reproduce**:\r\n\r\nThis (latest now) checkout of tensorflow models:\r\nhttps://github.com/tensorflow/models/tree/edcd29f2dbb4b3eaed387fe17cb5270f867aec42/official/mnist\r\n\r\n```sh\r\n$ python convert_to_records.py --directory ~/tmp/mnist_data\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting /home/amir/tmp/mnist_data/train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nExtracting /home/amir/tmp/mnist_data/train-labels-idx1-ubyte.gz\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting /home/amir/tmp/mnist_data/t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting /home/amir/tmp/mnist_data/t10k-labels-idx1-ubyte.gz\r\nWriting /home/amir/tmp/mnist_data/train.tfrecords\r\nWriting /home/amir/tmp/mnist_data/validation.tfrecords\r\nWriting /home/amir/tmp/mnist_data/test.tfrecords\r\n$ python mnist.py --data_dir ~/tmp/mnist_data --model_dir ~/tmp/mnist_model --steps 2000\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_clu\r\nster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff6e918d910>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000\r\n, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 1, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_di\r\nr': '/home/amir/tmp/mnist_model', '_save_summary_steps': 100}\r\nWARNING:tensorflow:From mnist.py:84: __init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.TFRecordDataset`.\r\nWARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with num_threads is deprecated and will be removed in a fut\r\nure version.\r\nInstructions for updating:\r\nReplace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\r\nWARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed i\r\nn a future version.\r\nInstructions for updating:\r\nReplace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n2017-10-26 15:52:17.067286: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to us\r\ne: SSE4.1 SSE4.2 AVX\r\n2017-10-26 15:52:18.075205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there mu\r\nst be at least one NUMA node, so returning NUMA node zero\r\n2017-10-26 15:52:18.075593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX TITAN Black major: 3 minor: 5 memoryClockRate(GHz): 0.98\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 5.94GiB freeMemory: 5.87GiB\r\n2017-10-26 15:52:18.075610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX T\r\nITAN Black, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\nINFO:tensorflow:Saving checkpoints for 1 into /home/amir/tmp/mnist_model/model.ckpt.\r\nINFO:tensorflow:train_accuracy = 0.13\r\nINFO:tensorflow:loss = 2.28967, step = 1\r\nINFO:tensorflow:global_step/sec: 86.9528\r\nINFO:tensorflow:train_accuracy = 0.485 (1.150 sec)\r\nINFO:tensorflow:loss = 0.519769, step = 101 (1.150 sec)\r\nINFO:tensorflow:global_step/sec: 91.0098\r\nINFO:tensorflow:train_accuracy = 0.61 (1.099 sec)\r\nINFO:tensorflow:loss = 0.464828, step = 201 (1.099 sec)\r\nINFO:tensorflow:global_step/sec: 95.5818\r\nINFO:tensorflow:train_accuracy = 0.6825 (1.046 sec)\r\nINFO:tensorflow:loss = 0.299279, step = 301 (1.046 sec)\r\n...\r\nEvaluation results:\r\n    {'loss': 0.027056012, 'global_step': 20000, 'accuracy': 0.99309999}\r\n$ # comment out the .train line (lines 232-235) in mnist.py\r\n$ # somehow disable GPU\r\n$ python mnist.py --data_dir ~/tmp/mnist_data --model_dir ~/tmp/mnist_model --steps 2000 --data_format channels_last\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_clu\r\nster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4a9afcfd10>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000\r\n, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': 1, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_di\r\nr': '/home/amir/tmp/mnist_model', '_save_summary_steps': 100}\r\nWARNING:tensorflow:From mnist.py:84: __init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.TFRecordDataset`.\r\nWARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with num_threads is deprecated and will be removed in a fut\r\nure version.\r\nInstructions for updating:\r\nReplace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\r\nWARNING:tensorflow:From mnist.py:92: calling map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed i\r\nn a future version.\r\nInstructions for updating:\r\nReplace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\r\nINFO:tensorflow:Starting evaluation at 2017-10-26-14:33:53\r\n2017-10-26 16:33:53.385155: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to us\r\ne: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-10-26 16:33:53.907636: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n2017-10-26 16:33:53.907712: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: italix14\r\n2017-10-26 16:33:53.907722: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: italix14\r\n2017-10-26 16:33:53.907800: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Invalid argument: expected %d.%d or %d.%d.%d f\r\norm for driver version; got \"1\"\r\n2017-10-26 16:33:53.907837: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:369] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Mo\r\ndule  375.26  Thu Dec  8 18:36:43 PST 2016\r\nGCC version:  gcc version 4.8.4 (Debian 4.8.4-1)\r\n\"\"\"\r\n2017-10-26 16:33:53.907871: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 375.26.0\r\nINFO:tensorflow:Restoring parameters from /home/amir/tmp/mnist_model/model.ckpt-20000\r\nINFO:tensorflow:Finished evaluation at 2017-10-26-14:34:20\r\nINFO:tensorflow:Saving dict for global step 20000: accuracy = 0.0962, global_step = 20000, loss = 6.32954\r\n\r\nEvaluation results:\r\n    {'loss': 6.3295369, 'global_step': 20000, 'accuracy': 0.096199997}\r\n```\r\n\r\n\r\n### Describe the problem\r\nI have trained the official mnist model on GPU (`channels_first`,  `NCHW`) but when I test it on CPU (`channels_last`,  `NHWC`) I get different results. On GPU, I get an accuracy of `0.99309999` but the same model (when tested on CPU) gives `0.096199997`. I believe when the model is restored from the checkpoint the kernel weights are not restored properly to accommodate for the new channel format.\r\n\r\nThe problem I have shown here is using the official mnist model but in reality I have a model that I have already trained using the `channels_first` format on GPU which took several days to train. But, now I cannot evaluate this model on CPU.\r\n\r\n", "comments": ["@martinwicke can you comment or redirect? Thanks!", "@nealwu This is an unfortunate side effect of automatically choosing the data format [here](https://github.com/tensorflow/models/blob/edcd29f2dbb4b3eaed387fe17cb5270f867aec42/official/mnist/mnist.py#L107). Maybe we should give people the option to override the data format via flag so they can train on GPU and predict on CPU.", "Ahh, this is a very good point.\r\n\r\n@martinwicke We actually just added a data_format flag two days ago: https://github.com/tensorflow/models/pull/2583. I'm not quite sure if that's enough to solve this problem. Let me look into it and try to determine the right solution.", "Thank you for looking into this issue.\r\nI think (this is my guess) this happens during saving and restoring the model.\r\nSomething like below would also trigger this issue:\r\n```python\r\ndata_format = 'channels_first'\r\ninput_layer = tf.placeholder(tf.float32, shape=[3, 28, 28], name=\"data\")\r\nconv1 = tf.layers.conv2d(\r\n            inputs=input_layer,\r\n            filters=32,\r\n            kernel_size=(3,3),\r\n            padding=\"same\",\r\n            activation=tf.nn.relu,\r\n            data_format=data_format)\r\n# assign something like range(3*3*3*32) to conv layer kernel variables\r\n# print out kernel variables' values as reference\r\n# save the conv layer into a checkpoint\r\n```\r\nin a new python process:\r\n```python\r\ndata_format = 'channels_last'  # change to channels last format\r\ninput_layer = tf.placeholder(tf.float32, shape=[28, 28, 3], name=\"data\")\r\nconv1 = tf.layers.conv2d(\r\n            inputs=input_layer,\r\n            filters=32,\r\n            kernel_size=(3,3),\r\n            padding=\"same\",\r\n            activation=tf.nn.relu,\r\n            data_format=data_format)\r\n# restore checkpoint\r\n# check if the loaded kernel values are the same still.\r\n```\r\n", "FYI, the `--data_format` flag we added is able to handle this appropriately on my end for both the MNIST model and the ResNet ImageNet model. I can resume training and/or evaluate on the same checkpoint even if I swap the `data_format` value. Let me know if you are having any issues @183amir; otherwise closing this.", "Hi @nealwu \r\n\r\nI tried with the following code by train and test with different orders and the same order.\r\nI don't think the restore process corrects the order for me...\r\nDid I miss something in your code?  Thanks\r\n\r\nPS I am using TF ver 1.6 with Cuda 9.\r\n```\r\npython test.py -o channels_first\r\npython test.py --test -o channels_first # get the same accuracy\r\npython test.py --test -o channels_last # get different accuracy \r\n```\r\n\r\nSample code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom argparse import ArgumentParser\r\n\r\n\r\nimport matplotlib\r\nmatplotlib.use('Agg')\r\nimport matplotlib.pylab as plt\r\n\r\nsave_path = \"log/test_save.ckpt\"\r\nsample_size = 1024\r\nbatch_size = 128\r\nniter = 10\r\n\r\nif __name__ == \"__main__\":\r\n    parser = ArgumentParser()\r\n    parser.add_argument(\"--test\", dest='train_flag', action='store_false', default=True)\r\n    parser.add_argument(\"--order\", \"-o\", default=\"channels_first\", choices=[\"channels_first\", \"channels_last\"])\r\n    args = parser.parse_args()\r\n\r\n    n_batch = int(sample_size/batch_size)\r\n\r\n    np.random.seed(123)\r\n    label = np.random.randint(0, 10, size=sample_size)\r\n    if args.order == \"channels_first\":\r\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 1, 28, 28)\r\n        plt.imshow(img[0,0,:,:])\r\n        plt.savefig(\"img_first.png\")\r\n    else:\r\n        img = np.random.randn(sample_size*28*28).reshape(sample_size, 28, 28, 1)\r\n        plt.imshow(img[0, :, :, 0])\r\n        plt.savefig(\"img_last.png\")\r\n\r\n    img_shape = list(img.shape)\r\n    img_shape[0] = None\r\n    tf_input = tf.placeholder(tf.float32, shape=img_shape)\r\n    tf_label = tf.placeholder(tf.int32, shape=[None])\r\n\r\n    v = tf.layers.conv2d(tf_input, 6, 5, data_format=args.order, activation=tf.nn.relu)\r\n    if args.order == \"channels_last\":  # force to load channels_first model - just demonstration purpose.\r\n        v = tf.transpose(v, [0,3,1,2])\r\n    v = tf.layers.flatten(v)\r\n    v = tf.layers.dense(v, 10)\r\n\r\n    tf_sparse_labels = tf.stop_gradient(tf.one_hot(tf_label, 10))\r\n    tf_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_sparse_labels, logits=v)\r\n\r\n    opt = tf.train.AdamOptimizer()\r\n    opt_op = opt.minimize(tf_loss)\r\n\r\n    tf_pred = tf.argmax(v, axis=-1, output_type=tf.int32)\r\n    tf_accu = tf.reduce_mean(tf.cast(tf.equal(tf_pred, tf_label), tf.float32))\r\n\r\n    saver = tf.train.Saver()\r\n\r\n    with tf.Session(graph=tf.get_default_graph()) as sess:\r\n        if args.train_flag:\r\n            print(\"training\")\r\n            sess.run(tf.global_variables_initializer())\r\n            for _ in range(niter):\r\n                for i in range(n_batch):\r\n                    start = i * batch_size\r\n                    end = start + batch_size\r\n                    feed_dict = {tf_input: img[start:end], tf_label:label[start:end]}\r\n                    sess.run([opt_op], feed_dict=feed_dict)\r\n\r\n                accu = sess.run(tf_accu, feed_dict={tf_input: img, tf_label: label})\r\n                print(accu)\r\n            saver.save(sess, save_path)\r\n        else:\r\n            print(\"testing \")\r\n            saver.restore(sess, save_path)\r\n            feed_dict = {tf_input: img, tf_label: label}\r\n            accu = sess.run(tf_accu, feed_dict=feed_dict)\r\n            print(accu)\r\n```", "Hi @jiayiliu, unfortunately we aren't able to help you debug your custom code here. Please [ask on StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).", "Just to close my open point - The official tf.train.saver is not account for the channel order.\r\nAnd the weight need to be reordered before tf.flatten step, assuming from 2D conv layers to fc layers.\r\nLuckily/thoughtfully, the MNIST has only 1 channel, so no need for reordering. \r\nAnd ResNet uses average pooling before the dense layer and it properly handles the spatial dimensions before the last step.  So both are working as mentioned by @nealwu.\r\n\r\nI have corrected my code above to demonstrate how to restore the trained model and reuse with CPU case. -- just a demo.", "Hi.... how was this resolved? \r\n\r\nI have just developed and trained a model on PC with GPU then brought over the work to lap-top with no GPU and found that the same code and model that works fine on the GPU machine will not work on the lap-top.\r\n\r\ntensorflow==2.3.1\r\ntensorflow-cpu==2.3.1\r\ntensorflow-cpu-estimator==1.15.1\r\ntensorflow-estimator==2.3.0\r\n\r\n```\r\ntensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Default MaxPoolingOp only supports NHWC on device type CPU\r\n         [[node sequential/max_pooling2d_1/MaxPool (defined at .\\metatrader.py:106) ]] [Op:__inference_predict_function_445]\r\n```"]}, {"number": 13999, "title": "Memory Leak While Reading from TFRecord", "body": "### Problem\r\nAs I mentioned in [my previous issue](https://github.com/tensorflow/tensorflow/issues/6599), I have memory leak in my code. Finally I can write a sample code that can reproduce the problem.\r\n\r\n### Source code\r\n```\r\nimport os\r\nimport random\r\n\r\nimport psutil\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_tf_example(inputs, outputs):\r\n    example = tf.train.Example(features=tf.train.Features(feature={\r\n        'inputs': tf.train.Feature(int64_list=tf.train.Int64List(value=inputs)),\r\n        'outputs': tf.train.Feature(int64_list=tf.train.Int64List(value=outputs))\r\n        }))\r\n    return example.SerializeToString()\r\n\r\n\r\ndef memory():\r\n    pid = os.getpid()\r\n    py = psutil.Process(pid)\r\n    memory_use = py.memory_info()[0]/2.**30\r\n    print('memory use:', memory_use)\r\n\r\n\r\ndef main():\r\n    tfrecord_file = tf.python_io.TFRecordWriter('data.tfrecord')\r\n    for i in range(10000):\r\n        random_numbers = [random.randint(0, 100) for _ in range(10)]\r\n        without_an_element = filter(lambda e: e != 100, random_numbers)\r\n        tf_example = get_tf_example(random_numbers, without_an_element)\r\n        tfrecord_file.write(tf_example)\r\n    tfrecord_file.close()\r\n\r\n    filename_queue = tf.train.string_input_producer(['data.tfrecord'])\r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n    features = tf.parse_single_example(\r\n        serialized_example,\r\n        features={\r\n            'inputs': tf.FixedLenFeature(10, dtype=tf.int64),\r\n            'outputs': tf.VarLenFeature(dtype=tf.int64)\r\n        })\r\n    images, labels = tf.train.shuffle_batch([features['inputs'], features['outputs']], batch_size=100, capacity=200,\r\n                                            min_after_dequeue=100)\r\n    sess = tf.Session()\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n    for i in range(10000):\r\n        _ = sess.run(images)\r\n        if (i+1) % 100 == 0:\r\n            memory()\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n### logs\r\n```\r\n('memory use:', 0.1137542724609375)\r\n('memory use:', 0.11678314208984375)\r\n('memory use:', 0.11930084228515625)\r\n('memory use:', 0.12792205810546875)\r\n('memory use:', 0.13238525390625)\r\n('memory use:', 0.135406494140625)\r\n('memory use:', 0.13817596435546875)\r\n('memory use:', 0.14484786987304688)\r\n('memory use:', 0.1512603759765625)\r\n('memory use:', 0.15402984619140625)\r\n('memory use:', 0.15654754638671875)\r\n('memory use:', 0.16101455688476562)\r\n('memory use:', 0.17060470581054688)\r\n('memory use:', 0.17287063598632812)\r\n('memory use:', 0.17589187622070312)\r\n('memory use:', 0.18036270141601562)\r\n('memory use:', 0.18872833251953125)\r\n('memory use:', 0.19124221801757812)\r\n('memory use:', 0.19401168823242188)\r\n('memory use:', 0.19848251342773438)\r\n('memory use:', 0.20489883422851562)\r\n('memory use:', 0.20936203002929688)\r\n('memory use:', 0.21213150024414062)\r\n('memory use:', 0.21490097045898438)\r\n('memory use:', 0.22327041625976562)\r\n('memory use:', 0.22988128662109375)\r\n('memory use:', 0.232147216796875)\r\n('memory use:', 0.23491668701171875)\r\n('memory use:', 0.23967361450195312)\r\n('memory use:', 0.2477874755859375)\r\n('memory use:', 0.2508087158203125)\r\n('memory use:', 0.2528228759765625)\r\n('memory use:', 0.2577934265136719)\r\n('memory use:', 0.2661628723144531)\r\n('memory use:', 0.2686767578125)\r\n('memory use:', 0.27144622802734375)\r\n('memory use:', 0.2759132385253906)\r\n('memory use:', 0.2823295593261719)\r\n('memory use:', 0.28704833984375)\r\n('memory use:', 0.2895660400390625)\r\n('memory use:', 0.2940330505371094)\r\n('memory use:', 0.30095672607421875)\r\n('memory use:', 0.30516815185546875)\r\n('memory use:', 0.30768585205078125)\r\n('memory use:', 0.3126564025878906)\r\n('memory use:', 0.31687164306640625)\r\n('memory use:', 0.3235435485839844)\r\n('memory use:', 0.32605743408203125)\r\n('memory use:', 0.3305244445800781)\r\n('memory use:', 0.3332939147949219)\r\n('memory use:', 0.3397102355957031)\r\n('memory use:', 0.34417724609375)\r\n('memory use:', 0.3529624938964844)\r\n('memory use:', 0.3552284240722656)\r\n('memory use:', 0.36199188232421875)\r\n('memory use:', 0.3664588928222656)\r\n('memory use:', 0.37117767333984375)\r\n('memory use:', 0.373443603515625)\r\n('memory use:', 0.3801116943359375)\r\n('memory use:', 0.38458251953125)\r\n('memory use:', 0.3892974853515625)\r\n('memory use:', 0.3923187255859375)\r\n('memory use:', 0.3982391357421875)\r\n('memory use:', 0.40125274658203125)\r\n('memory use:', 0.4074211120605469)\r\n('memory use:', 0.410186767578125)\r\n('memory use:', 0.4146575927734375)\r\n('memory use:', 0.41912078857421875)\r\n('memory use:', 0.4257926940917969)\r\n('memory use:', 0.4280548095703125)\r\n('memory use:', 0.4310760498046875)\r\n('memory use:', 0.4372406005859375)\r\n('memory use:', 0.44196319580078125)\r\n('memory use:', 0.446929931640625)\r\n('memory use:', 0.44919586181640625)\r\n('memory use:', 0.45586395263671875)\r\n('memory use:', 0.4581298828125)\r\n('memory use:', 0.46504974365234375)\r\n('memory use:', 0.467315673828125)\r\n('memory use:', 0.4739875793457031)\r\n('memory use:', 0.47650146484375)\r\n('memory use:', 0.48291778564453125)\r\n('memory use:', 0.485687255859375)\r\n('memory use:', 0.4921112060546875)\r\n('memory use:', 0.49462127685546875)\r\n('memory use:', 0.5015411376953125)\r\n('memory use:', 0.5038070678710938)\r\n('memory use:', 0.5085296630859375)\r\n('memory use:', 0.5129928588867188)\r\n('memory use:', 0.5174598693847656)\r\n('memory use:', 0.522430419921875)\r\n('memory use:', 0.5246963500976562)\r\n('memory use:', 0.5313644409179688)\r\n('memory use:', 0.5355796813964844)\r\n('memory use:', 0.5404815673828125)\r\n('memory use:', 0.5429954528808594)\r\n('memory use:', 0.5494194030761719)\r\n('memory use:', 0.5541419982910156)\r\n('memory use:', 0.5586051940917969)\r\n('memory use:', 0.5611228942871094)\r\n```\r\n\r\nI can't find the reason. Many thanks for your consideration.\r\n\r\n### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7.6\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n", "comments": ["@mrry would you take a look or assign to whomever is supporting queue-based inputs?", "Interestingly, the leak seems to be plugged if you replace the line:\r\n\r\n```python\r\n        _ = sess.run(images)\r\n```\r\n\r\n...with the following line:\r\n\r\n```python\r\n        _ = sess.run([images, labels])\r\n```\r\n\r\nI suspect it is not a coincidence that `labels` is a `tf.SparseTensor`, because `tf.train.shuffle_batch()` handles these by storing them in a buffer, replacing them with a key (to pass through the batch queue), and then removing them from the buffer when the key is emitted from the batch queue.\r\n\r\nPerhaps if you don't use the value, the remove-from-buffer code doesn't get triggered?\r\n\r\nAssigning to @ebrevdo, since he knows that part of the code best.", "Eugene: perhaps we need all the returned tensors to carry a control dependency on the `_restore_sparse` ops here?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e9d2b60ed3d94eef7a3cf139cc27ec629f510681/tensorflow/python/training/input.py#L555", "@mrry great idea!  a long term fix is to use Variants here, but I'll make the change you suggested.", "@ebrevdo  @mrry is there any way to fix this without update tf code? I use aliyun's pai running the code, and it's tf version is 1.2 that i can't change it.", "You may be able to patch my pr into your tf installation\n\nOn Tue, Oct 31, 2017, 3:03 AM \u65b9\u6842\u9534 <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> @mrry <https://github.com/mrry> is\n> there any way to fix this without update tf code? I use aliyun's pai\n> running the code, and it's tf version is 1.2 that i can't change it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13999#issuecomment-340714531>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-mJqZtSx5vg4w-jc6FocCBOdolTks5sxvBqgaJpZM4QHqTE>\n> .\n>\n", "Tried the fix in my app and it seems to solve the problem. "]}, {"number": 13998, "title": "Add customerized kernel implementation for clip_by_value", "body": "This fix tries to address the issue raised in #7225 where `tf.clip_by_value` does not have a custom kernel and reused `tf.maximum` and `tf.mimimum`. In case scalar values are passed to `tf.clip_by_value`, unnecessary memory usage might incur.\r\n\r\nThis fix adds the customerized kernel implementation for `tf.clip_by_value`.\r\n\r\nThis fix fixes #7225.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks @benoitsteiner. I was relatively new to grad code base and didn't take the grad registration into consideration before. Let me take a look and update the pull request accordingly.", "Thanks @benoitsteiner for the review. The PR has been updated with grad registration added. Test cases for gradients of clip_by_value have also been added. Please take a look.", "@benoitsteiner, can you take another look?", "Jenkins, test this please.", "@yongtang  Can you run tensorflow/core/api_def/update_api_def.sh to update our API checks ? I should be able to merge this PR once that's done.", "@benoitsteiner Thanks for the help! The PR has been updated.", "Looks like this is also failing the API checks. Also reverting this PR."]}, {"number": 13997, "title": "Fix list formatting", "body": "Markdown needs a blank line before the list to render it correctly.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig#replace", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13996, "title": "Proper way to handle csv input for cpu training?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**: \r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: \r\nPython 2.7.12 from Anaconda\r\n\r\n### Describe the problem\r\nI am training some classification model with an 32-cpu Ubuntu machine and one of the problem is to feed data fast enough to the training process. \r\n\r\nI am trying to read data from some csv file but the default tf.csv or tf.data module seems to be slow.\r\nA speed test for reading 1000000 row * 17 column csv file shows a speed like : \r\n* tf.decode_csv with queue and theads  :   ~192 seconds\r\n* tf.data :  ~164 seconds\r\n* hand write cpp reading op :  ~25 seconds\r\n* pure python code with help from pandas : ~23 seconds\r\n\r\nIt is fast enough to use pandas for one single file, but it might face the GIL problem if try to speed up with more threads. \r\n\r\nCodes can be found below. I am not sure if I use it the right way, is there any official benchmarks or guidelines for this? \r\n\r\n### Source code / logs\r\nhttps://github.com/littleDing/mini_csv_reader\r\nThe speed test is run through speed_test.py ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I think it is a feature request.\r\n\r\nThere is a simple demo on the official website about reading data, but never a best-practice under different scenes, or any official benchmarks. I could run according the demos with small datasets but once it became some million or billions it would be an nightmare.\r\n\r\nIt took me very long time to figure out how to feed csv data into training process efficiently. But as @mrry suggested in https://github.com/tensorflow/tensorflow/issues/13883, there might still be some upgrade that might be useful, so I do the test and post this issue.\r\n\r\nTo be honest,  I am still confused now and I think lots of developers would feel the same before any official guidelines is posted. "]}, {"number": 13995, "title": "How to output each class accuracy ?", "body": "Using slim I can get the evaluation output Top-1 and Top-5. But how to calculate the accuracy for each class and output them?\r\nThanks!", "comments": ["Hi @MacwinWin, your question is better suited for https://stackoverflow.com/questions/tagged/tensorflow. \r\nGithub issues are for bugs and feature requests. "]}, {"number": 13994, "title": "How to output each class accuracy through modifing", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 13993, "title": "Compiling from source, ./configure, issue finding cudnn", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **CUDA/cuDNN version**: 7\r\n- **GPU model and memory**: Nvidia 1070 8GB\r\n- **Exact command to reproduce**: On ./configure\r\n\r\nWhen following the building from source installation instructions, the first step involves running a ./configure.\r\n\r\nDuring this script, you are prompted for the versions and locations of features you want support for in the build.\r\n\r\nWhen getting down to the CUDA SKD version, you will get something like:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 9.0\r\n\r\nI'm using 9.0...\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7.0\r\n\r\nI'm using 7.0...\r\nHowever, if you type 7.0, you get the following:\r\nInvalid path to cuDNN  toolkit. Neither of the following two files can be found:\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0\r\n/usr/local/cuda-9.0/libcudnn.so.7.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0\r\n\r\nBut if you answer the following as:\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7\r\nIt works.\r\n\r\nThe actual name is libcudnn.so.7, can can't find libcudnn.so.7.0. Otherwise it won't find it, and it looks like you have installed incorrectly.\r\n\r\nCan we get a fix for this?\r\n", "comments": ["Yes, you have to use the name of the actual library that is installed. \r\n\r\nBecause the naming scheme is not very predictable and varies between platforms, we cannot really guess without risking bad side effects."]}, {"number": 13992, "title": "tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.py test is failing with array mismatch error", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n      Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source\r\n- **TensorFlow version (use command below)**:\r\n      TF 1.3.1\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n     bazel-0.5.4\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n `bazel test --config=opt //tensorflow/contrib/slim/python/slim/nets:resnet_v1_test`\r\n\r\n### Describe the problem\r\nThis test is failing due to array mismatch error \u2013 a single value in an array of 100+ elements differing i.e.  `0.69775391 VS expected 0.69799805` (minor mismatch), I feel this is not \"critical\".\r\n\r\nHence, I tried running this test by changing minor tolerance `(atol=1e-4 to atol=2e-4)`, and test is passing -\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.py#L389\r\nOriginal : `output.eval(), expected.eval(), atol=1e-4, rtol=1e-4) `\r\nUpdate to : ` output.eval(), expected.eval(), atol=2e-4, rtol=1e-4)`\r\n\r\nIs it OK to raise a PR with this changes ?. Please provide your comments on this.Thanks!\r\n\r\n### Source code / logs\r\n `bazel test --config=opt //tensorflow/contrib/slim/python/slim/nets:resnet_v1_test`\r\n\r\n```\r\n-----------------------------------------------------------------------------\r\n..F..............\r\n======================================================================\r\nFAIL: testAtrousFullyConvolutionalValues (__main__.ResnetCompleteNetworkTest)\r\nVerify dense feature extraction with atrous convolution.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/3429064d1bfc0bd7253170e3e9255ca6/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.py\", line 389, in testAtrousFullyConvolutionalValues\r\n    output.eval(), expected.eval(), atol=1e-4, rtol=1e-4)\r\n  File \"/root/.cache/bazel/_bazel_root/3429064d1bfc0bd7253170e3e9255ca6/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 687, in assertAllClose\r\n    self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)\r\n  File \"/root/.cache/bazel/_bazel_root/3429064d1bfc0bd7253170e3e9255ca6/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/slim/python/slim/nets/resnet_v1_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 657, in _assertArrayLikeAllClose\r\n    np.testing.assert_allclose(b, a, rtol=rtol, atol=atol, err_msg=msg)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\nNone\r\n(mismatch 0.173611111111%)\r\n x: array([[[[  0.000000e+00,   3.163050e+02,   1.810576e+02,   0.000000e+00,\r\n            0.000000e+00,   7.204424e+01,   2.393519e+02,   2.917337e+02,\r\n            0.000000e+00,   0.000000e+00,   4.375999e+02,   6.552710e+01,...\r\n y: array([[[[  0.000000e+00,   3.163050e+02,   1.810576e+02,   0.000000e+00,\r\n            0.000000e+00,   7.204424e+01,   2.393519e+02,   2.917337e+02,\r\n            0.000000e+00,   0.000000e+00,   4.375999e+02,   6.552710e+01,...\r\n\r\n----------------------------------------------------------------------\r\nRan 17 tests in 21.369s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([1]), array([2]), array([1]), array([7]))\r\nnot close lhs =  [ 0.69799805]\r\nnot close rhs =  [ 0.69775391]\r\nnot close dif =  [ 0.00024414]\r\nnot close tol =  [ 0.00016978]\r\ndtype = float32, shape = (2, 3, 3, 32)\r\n```\r\n", "comments": ["This seems like a reasonable PR to me, I say go for it.", "Thanks @skye , I have created a PR - https://github.com/tensorflow/tensorflow/pull/14086, please have a look.\r\n\r\nI will close this issue once PR merged.", "PR got merged, hence closing."]}, {"number": 13991, "title": "Support fold batch norm for atrous conv2d", "body": "Fix #13990\r\n\r\nAs we can fold batch norm with convolution, we should also fold batch norm with atrous convolution, which has not been implemented.\r\n", "comments": ["Can one of the admins verify this patch?", "@petewarden could you take a look or assign someone who can?", "I updated the the Graph Transform Tool to fold batchnorm with atrous convolution.\r\nAnd add unit-test for it.", "Ping @petewarden ", "@petewarden any luck with this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The test failed (`TestFoldFusedBatchNormsWithBatchToSpace`):\r\n```\r\ntensorflow/tools/graph_transforms/fold_old_batch_norms_test.cc:356\r\n      Expected: ::tensorflow::Status::OK()\r\n      Which is: OK\r\nTo be equal to: (root.ToGraphDef(&original_graph_def))\r\n      Which is: Invalid argument: Shapes must be equal rank, but are 1 and 2 for 'batch_to_space_op' (op: 'BatchToSpaceND') with input shapes: [1,1,5,2], [2], [2].\r\n```", "ping @liyinhgqw, could you look at the failing test?", "ping @liyinhgqw, could you look at the failing test?", "okay. I'll check it out.", "@petewarden @drpngx @martinwicke @protoget , I fixed some test data. Could you please check it again?"]}, {"number": 13990, "title": "Support fold batch norm for atrous conv2d", "body": "As we can fold batch norm with convolution, we should also fold batch norm with atrous convolution, which has not been implemented.\r\n\r\nissue: #13989", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I signed it!"]}]