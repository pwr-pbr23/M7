[{"number": 55670, "title": "ppc64le: eigen: using the new flag to enable MMA dynamic dispatch", "body": "Eigen removed the dynamic dispatch by default. Now, it's necessary to add\r\nEIGEN_ALTIVEC_ENABLE_MMA_DYNAMIC_DISPATCH=1 to enable this feature.\r\n\r\nhttps://gitlab.com/libeigen/eigen/-/commit/7b10795e3939518f1be24f5799e01dbaa1e0c04e\r\nhttps://gitlab.com/libeigen/eigen/-/commit/591906477bc8c8102dbefceefe10d81648865394", "comments": ["Hi @maxiwell, I just want to confirm this is actually what IBM wants.  It was removed by default because it's broken on system-installed compilers on many systems (e.g. ubuntu), since it requires a specific ld version that we cannot detect from within Eigen.  The thinking was that build systems might be able to detect this and turn on the flag if desired."]}, {"number": 55669, "title": "[TRT] TF 2.9.0-rc0 ssd_mobilenet_v2 TRT FP16 conversion failed. (-1,-1,-1,3), float32", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.9.0-rc0\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 18.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.7\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN/TensorRT version\r\n\r\n11.2 / 8.1.1.33 / 7.2.3-1\r\n\r\n### GPU model and memory\r\n\r\nNvidia Tesla T4 16GB (aws g4dn.2xlarge)\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nssd_mobilenet_v2 model is from TF2 model Zoo.\r\nIt was exported with `--input_type=float_image_tensor` parameter.\r\nExported model input is `<tf.Tensor 'input_tensor:0' shape=(None, None, None, 3) dtype=float32>`.\r\n\r\nConverting the model to TRT with FP16 PrecisionMode fails.\r\nError: `ValueError: Received a shape scalar with unknown static value.  A static value of '-1' is required to represent an unknown shape.`\r\n\r\nThe conversion worked fine in TF 2.4, 2.6, 2.7, 2.8.\r\nThe issue started to happen in TF 2.9.0-rc0\r\n```\r\n\r\n### Standlone code to reproduce the issue\r\nDownload [SSD MobileNet v2 320x320](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz) from [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)\r\n\r\nExport the model using `--input_type=float_image_tensor` parameter. \r\nIt will create a model with input `<tf.Tensor 'input_tensor:0' shape=(None, None, None, 3) dtype=float32>`:\r\n```shell\r\n# Export the Model\r\nMODEL=ssd_mobilenet_v2_320x320_coco17_tpu-8\r\npython3 object_detection/exporter_main_v2.py \\\r\n    --input_type=float_image_tensor \\\r\n    --pipeline_config_path=$MODEL/pipeline.config \\\r\n    --trained_checkpoint_dir=$MODEL/checkpoint \\\r\n    --output_directory=output/${MODEL}_float32_batchN\r\n```\r\nExported model can be downloaded from [here](https://www.dropbox.com/s/1uznvtf6u4czgot/ssd_mobilenet_v2_320x320_coco17_tpu-8_float32_batchN.tar.gz?dl=0)\r\n```\r\n# Convert the model to TRT FP16\r\n\r\ncd output/ssd_mobilenet_v2_320x320_coco17_tpu-8_float32_batchN\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nconversion_params = trt.TrtConversionParams(precision_mode=trt.TrtPrecisionMode.FP16)\r\nconverter = trt.TrtGraphConverterV2(\r\n    input_saved_model_dir=\"saved_model\",\r\n    conversion_params=conversion_params)\r\nconverter.convert()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n>>> converter = trt.TrtGraphConverterV2(\r\n...     input_saved_model_dir=\"saved_model\",\r\n...     conversion_params=conversion_params)\r\nINFO:tensorflow:Linked TensorRT version: (7, 2, 2)\r\nINFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\r\nINFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT minor/patch upgrades are backward compatible.\r\n>>> converter.convert()\r\n2022-04-19 18:59:33.559459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:33.566342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:33.568044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:33.569815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-04-19 18:59:33.570320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:33.571871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:33.573363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:34.263138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:34.264552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:34.265704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:34.266826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13596 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n2022-04-19 18:59:45.455700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:45.457173: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-04-19 18:59:45.457334: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-04-19 18:59:45.457786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:45.459086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:45.460342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:45.461644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:45.462905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:45.464134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13596 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n2022-04-19 18:59:53.296013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:53.297496: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-04-19 18:59:53.297612: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-04-19 18:59:53.297949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:53.299278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:53.300541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:53.301857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:53.303121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 18:59:53.304366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13596 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n2022-04-19 18:59:54.739820: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:186] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.\r\n2022-04-19 18:59:54.922287: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:952]\r\n\r\n################################################################################\r\nTensorRT unsupported/non-converted OP Report:\r\n\t- Placeholder -> 1267x\r\n\t- GatherV2 -> 390x\r\n\t- ConcatV2 -> 186x\r\n\t- Fill -> 184x\r\n\t- StridedSlice -> 133x\r\n\t- Sub -> 117x\r\n\t- Shape -> 117x\r\n\t- Reshape -> 113x\r\n\t- Slice -> 102x\r\n\t- Select -> 102x\r\n\t- AddV2 -> 100x\r\n\t- Less -> 94x\r\n\t- ZerosLike -> 92x\r\n\t- NonMaxSuppressionV5 -> 90x\r\n\t- Pack -> 35x\r\n\t- Merge -> 25x\r\n\t- Enter -> 25x\r\n\t- NextIteration -> 25x\r\n\t- Switch -> 25x\r\n\t- Mul -> 21x\r\n\t- Exit -> 20x\r\n\t- Identity -> 12x\r\n\t- Greater -> 12x\r\n\t- ExpandDims -> 10x\r\n\t- Cast -> 9x\r\n\t- TensorListSetItem -> 9x\r\n\t- TensorListFromTensor -> 8x\r\n\t- TensorListGetItem -> 8x\r\n\t- TensorListStack -> 8x\r\n\t- Pad -> 6x\r\n\t- DataFormatVecPermute -> 6x\r\n\t- Minimum -> 6x\r\n\t- Unpack -> 6x\r\n\t- TensorListReserve -> 4x\r\n\t- Split -> 4x\r\n\t- NoOp -> 4x\r\n\t- Maximum -> 4x\r\n\t- Range -> 3x\r\n\t- Transpose -> 3x\r\n\t- Tile -> 2x\r\n\t- TopKV2 -> 2x\r\n\t- Exp -> 2x\r\n\t- Reciprocal -> 2x\r\n\t- LoopCond -> 2x\r\n\t- LogicalAnd -> 2x\r\n\t- Sum -> 1x\r\n\t- Squeeze -> 1x\r\n\t- ResizeBilinear -> 1x\r\n\t- GreaterEqual -> 1x\r\n\t- Where -> 1x\r\n--------------------------------------------------------------------------------\r\n\t- Total nonconverted OPs: 3402\r\n\t- Total nonconverted OP Types: 50\r\nFor more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\r\n################################################################################\r\n\r\n2022-04-19 18:59:54.996948: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1280] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 3 TRT Engines with  at least minimum_segment_size=3 nodes.\r\n2022-04-19 18:59:55.004054: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:795] Number of TensorRT candidate segments: 3\r\n2022-04-19 18:59:55.057662: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 0 consisting of 536 nodes by TRTEngineOp_000_000.\r\n2022-04-19 18:59:55.058968: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 1 consisting of 3 nodes by TRTEngineOp_000_001.\r\n2022-04-19 18:59:55.059085: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:909] Replaced segment 2 consisting of 4 nodes by TRTEngineOp_000_002.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1234, in convert\r\n    [tensor.name for tensor in frozen_func.outputs])\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py\", line 659, in function_from_graph_def\r\n    nest.map_structure(import_graph.as_graph_element, outputs))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py\", line 336, in prune\r\n    base_graph=self._func_graph)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/lift_to_graph.py\", line 337, in lift_to_graph\r\n    op=op, graph=graph, op_map=op_map, base_graph=base_graph)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/lift_to_graph.py\", line 131, in _copy_non_source\r\n    name=op.name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 561, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 1963, in _create_c_op\r\n    raise ValueError(e.message)\r\nValueError: Received a shape scalar with unknown static value.  A static value of '-1' is required to represent an unknown shape.\r\n```\r\n</details>", "comments": ["The issue started to happen after [PR-53171](https://github.com/tensorflow/tensorflow/pull/53171).\r\nThe change in [trt_convert.py](https://github.com/tensorflow/tensorflow/pull/53171/files#diff-8e470ff4b5b82fda4fb3fe008b694323d63484f3cbf490d4e3ba104894f914e6):\r\n```\r\n    rewriter_config_with_trt.optimizers.extend(\r\n-        [\"constfold\", \"layout\", \"constfold\"])\r\n+        [\"pruning\", \"debug_stripper\", \"layout\", \"dependency\", \"constfold\",\r\n+         \"common_subgraph_elimination\"])\r\n```\r\n\r\nThe issue can be resolved by removing `common_subgraph_elimination` from the list of optimizers\r\n@Nyrio , @bixia1 ", "Hi @apivovarov,\r\nThanks for reporting this error. I will look into this."]}, {"number": 55668, "title": "Inconsistent documentation regarding supported input data type and ParameterServerStrategy", "body": "### Issue Type\r\n\r\nDocumentation Bug\r\n\r\n### Current Behaviour?\r\n\r\nThere is a direct conflict between the `ParameterServerStrategy` [tutorial doc][1], which states:\r\n\r\n```\r\nKeras Model.fit with tf.distribute.ParameterServerStrategy can take input data in the form of a tf.data.Dataset, tf.distribute.DistributedDataset, or a tf.keras.utils.experimental.DatasetCreator, with Dataset being the recommended option for ease of use.\r\n```\r\n\r\nAnd these pages:\r\n\r\n* [ParameterServerStrategy][2]:\r\n\r\n```\r\nWhen using Model.fit, tf.distribute.experimental.ParameterServerStrategy must be used with a tf.keras.utils.experimental.DatasetCreator, and steps_per_epoch must be specified.\r\n```\r\n\r\n* [Distributed Input][3]:\r\n\r\n```\r\nUsers of tf.distribute.experimental.ParameterServerStrategy with the Model.fit API need to use a tf.keras.utils.experimental.DatasetCreator as the input\r\n```\r\n\r\n* [Model.fit][4]:\r\n\r\n```\r\nIf using tf.distribute.experimental.ParameterServerStrategy, only DatasetCreator type is supported for X\r\n```\r\n\r\n**The former recommends using `Dataset` while the latter explicitly states only `DatasetCreator` is supported.**\r\n\r\n[1]: https://www.tensorflow.org/tutorials/distribute/parameter_server_training#input_data\r\n[2]: https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy\r\n[3]: https://www.tensorflow.org/tutorials/distribute/input\r\n[4]: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n", "comments": []}, {"number": 55666, "title": "[ROCm] Rocm unit test fixes", "body": "/cc @chsigg @cheshire @jurahul ", "comments": ["The fix in rocm_dnn.cc here fixes pooling on ROCm and fairly high priority\r\n@chsigg  FYI"]}, {"number": 55665, "title": "[TF-TRT] Change default max workspace size to INT_MAX, as recommended by TRT", "body": null, "comments": ["~~TODO: TRT 8.4 gate~~\r\ndone\r\n"]}, {"number": 55664, "title": "java.lang.IllegalArgumentException: Error occurred when initializing ObjectDetector: Mobile SSD models are expected to have exactly 4 outputs, found 8", "body": "I have quantized my custom trained TensorFlow model into tflite model using the following code but when I put my quantized tflite model into Tensorflow's Object detection android app with a label file, above mentioned error occurred in android studio. Please guide me to solve this problem. Thanks a Lot.\r\n\r\nCustom Code used for quantization of TensorFlow model:\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"/saved_model\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_quant_model = converter.convert()\r\n\r\nprint(len(tflite_quant_model))\r\nwith open(\"tflite_quant_model.tflite\", \"wb\") as f:\r\n    f.write(tflite_quant_model)\r\n```\r\n\r\n\r\nThe Output which I got after adding the quantized model into Tensorflow's Android application:\r\n\r\n```shell\r\nE/TaskJniUtils: Error getting native address of native library: task_vision_jni\r\n    java.lang.IllegalArgumentException: Error occurred when initializing ObjectDetector: Mobile SSD models are expected to have exactly 4 outputs, found 8\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$100(ObjectDetector.java:88)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector$3.createHandle(ObjectDetector.java:223)\r\n        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:219)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.<init>(TFLiteObjectDetectionAPIModel.java:88)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:82)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:105)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)\r\n        at android.os.Handler.dispatchMessage(Handler.java:107)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7356)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\r\nE/tensorflow: CameraActivity: Exception!\r\n    java.lang.IllegalStateException: Error getting native address of native library: task_vision_jni\r\n        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:95)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:219)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.<init>(TFLiteObjectDetectionAPIModel.java:88)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:82)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:105)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)\r\n        at android.os.Handler.dispatchMessage(Handler.java:107)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7356)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\r\n     Caused by: java.lang.IllegalArgumentException: Error occurred when initializing ObjectDetector: Mobile SSD models are expected to have exactly 4 outputs, found 8\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$100(ObjectDetector.java:88)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector$3.createHandle(ObjectDetector.java:223)\r\n        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:219)\u00a0\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.<init>(TFLiteObjectDetectionAPIModel.java:88)\u00a0\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:82)\u00a0\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:105)\u00a0\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\u00a0\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1221)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:107)\u00a0\r\n        at android.os.Looper.loop(Looper.java:214)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:7356)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:492)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:930)\u00a0\r\n```\r\n</details>", "comments": ["Hi @iamdanniemirz ! Could you provide the code snippets to model files and confirm your Tensorflow version too?  It is creating an Tflite model with 8 outputs where 4 output are expected. Attaching relevant [thread](https://stackoverflow.com/questions/66488745/tensorflow-lite-android-object-detection-mobile-ssd-models-are-expected-to-ha) for reference. Thanks!", "@mohantym  I have used this custom code for the quantization of the TensorFlow model. I used Tensorflow 2.8 version for quantization purposes.\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"/saved_model\")\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_quant_model = converter.convert()\r\n\r\nprint(len(tflite_quant_model))\r\nwith open(\"tflite_quant_model.tflite\", \"wb\") as f:\r\n    f.write(tflite_quant_model)\r\n```", "@mohantym  Official Repository's [code](https://github.com/tensorflow/models/blob/master/research/object_detection/exporter_main_v2.py) for exporting tflite model from pb without quantization.", "@iamdanniemirz ! Did you try the suggested setup in this [comment](https://stackoverflow.com/a/66595929/11530462)? (SSD with Fixed_shape_resizer of  (320,320) , Using Colab , Luwang's [metadata writer](https://stackoverflow.com/a/64493506/11530462)) .Thanks!", "@mohantym Yes, I have tried to quantize the model using TF 2.3 aswell. But the same thing happened again. Also tried to write metadata as well using Luwang's [metadata writer](https://stackoverflow.com/a/64493506/11530462) but this code does not work for 8 outputs.", "Hi @sachinprasadhs ! Could you please  look at this issue? Thanks!"]}, {"number": 55661, "title": "Addressing #55510 and comments from PR:  https://github.com/keras-team/keras/pull/16417", "body": "As @fchollet  & @chenmoneygithub  mentioned, replacing deprecated tensorflow/python/keras dependency  with keras packaging works. \r\nAlso there are dependencies on python/keras for other modules.\r\n@gbaned I guess this change should be from the tf side? \r\nPrevious PR: #55593", "comments": ["@abhilash1910 Thanks for the PR! \r\n\r\nKeras code in Tensorflow repo is no longer maintained, so generally we just point users to the Keras repo. "]}, {"number": 55660, "title": "Post-training quantization for efficientnet_lite give bad accuracy ", "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nPerformance\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ntf 2.8\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nubntu 18.04\n\n### Mobile device\n\nNone\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\nNone\n\n### GCC/Compiler version\n\nNone\n\n### CUDA/cuDNN version\n\nNone\n\n### GPU model and memory\n\nNone\n\n### Current Behaviour?\n\n```shell\nThank for your image classification model [maker](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification). By follow your tutorial, i create a training script with a purpose to use efficientnet-lite as our classifier.\r\n\r\nHowever, after export tflite model (efficientnet_lite2). Without quantization, out model get high accuracy (99.4% on my test dataset), very low speed (1.3748071193695068/image).\r\nSo i decide to use quantization to get better speed. \r\nBut the accuray is really bad. \r\n\r\n\r\nimport os\r\n\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nassert tf.__version__.startswith('2')\r\n\r\nfrom tflite_model_maker import model_spec\r\nfrom tflite_model_maker import image_classifier\r\nfrom tflite_model_maker.config import ExportFormat\r\nfrom tflite_model_maker.config import QuantizationConfig\r\nfrom tflite_model_maker.image_classifier import DataLoader\r\n\r\n# PATH TO FOLDER CONTAIN DATASET\r\nimage_path = \"/ws/dataset/current_training\"\r\n\r\ndata = DataLoader.from_folder(image_path)\r\n\r\ntrain_data, rest_data = data.split(0.8)\r\nvalidation_data, test_data = rest_data.split(0.5)\r\n\r\n\r\n# IMAGE_CLASSIFICATION_MODELS = [\r\n    # 'efficientnet_lite0', 'efficientnet_lite1', 'efficientnet_lite2',\r\n    # 'efficientnet_lite3', 'efficientnet_lite4', 'mobilenet_v2', 'resnet_50'\r\n# ]\r\n# model = image_classifier.create(train_data, model_spec=model_spec.get('mobilenet_v2'), validation_data=validation_data)\r\nmodel_dir = \"/ws/tensorflow/tensorflow/lite/g3doc/tutorials/models/6\"\r\nmodel = image_classifier.create(train_data, model_spec=model_spec.get('efficientnet_lite0'), validation_data=validation_data, \\\r\n    batch_size=128, shuffle=True, model_dir=model_dir)\r\n\r\nmodel.summary()\r\n\r\nloss, accuracy = model.evaluate(test_data)\r\n\r\nprint(\"############### FOR FP32 MODEL\")\r\n# model.export(export_dir=model_dir, \\\r\n#     tflite_filename='model.tflite')\r\n# # model.evaluate_tflite(os.path.join(model_dir, 'model.tflite'), test_data)\r\n\r\nmodel.export(export_dir='.')\r\nmodel.evaluate_tflite('model.tflite', test_data)\r\n\r\n\r\n\r\nprint(\"############### FOR FP16 MODEL\")\r\nconfig = QuantizationConfig.for_float16()\r\nmodel.export(export_dir=model_dir, \\\r\n    tflite_filename='model_fp16.tflite', quantization_config=config)\r\n# model.evaluate_tflite(os.path.join(model_dir, 'model_fp16.tflite'), test_data)\r\n\r\n```\n```\n\n\n### Standlone code to reproduce the issue\n\n```shell\nimport shutil\r\nfrom sklearn.utils import shuffle\r\nimport tflite_runtime.interpreter as tflite     \r\nimport cv2\r\nimport numpy as np\r\nimport os\r\nimport time\r\n\r\nclass FR_Anti_Spoofing():\r\n    def __init__(self, model_file=\"/ws/tensorflow/tensorflow/lite/g3doc/tutorials/model.tflite\"):\r\n        self.model_file = model_file\r\n    \r\n    def prepare(self):\r\n        self.interpreter = tflite.Interpreter(model_path=self.model_file)\r\n        self.interpreter.allocate_tensors()\r\n        self.input_details = self.interpreter.get_input_details()\r\n        self.output_details = self.interpreter.get_output_details()\r\n        self.width = self.input_details[0]['shape'][1]\r\n        self.height = self.input_details[0]['shape'][2]\r\n\r\n    def preprocess(self, img):\r\n        img = cv2.resize(img, (self.width, self.height))\r\n        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype('float32')\r\n        # img = (img - 128.) / 128.\r\n        \r\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n        input_data = np.expand_dims(img, axis=0)\r\n        return input_data\r\n\r\n    def run(self, img):\r\n        input_data = self.preprocess(img)\r\n        tic = time.time()\r\n\r\n        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\r\n        self.interpreter.invoke()\r\n\r\n        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\r\n        results = np.squeeze(output_data)\r\n\r\n        top_k = np.argmax(results)\r\n        print(\"results: \", results, time.time() - tic)\r\n        return top_k\r\n\r\n\r\n\r\nanti_model = FR_Anti_Spoofing()\r\nanti_model.prepare()\r\n\r\ntest_dir = \"/ws/dataset/current_training/spoof/\"\r\n# test_dir = \"/ws/dataset/pred_false/liveness\"\r\n\r\nfilter_dir = \"/ws/dataset/pred_false/spoof/\"\r\n\r\nimage_names = os.listdir(test_dir)\r\nfor image_name in image_names:\r\n    print(\"##########################################\")\r\n    tic = time.time()\r\n    path = os.path.join(test_dir, image_name)\r\n    print(\"path: \", path)\r\n    image = cv2.imread(path)\r\n    pred = anti_model.run(image)\r\n    print(pred, time.time() - tic)\r\n    # if pred == 0:\r\n    #     shutil.move(path, filter_dir)\n```\n\n\n### Relevant log output\n\n```shell\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nhub_keras_layer_v1v2 (HubKer (None, 1280)              4869168   \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 1280)              0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 2)                 2562      \r\n=================================================================\r\nTotal params: 4,871,730\r\nTrainable params: 2,562\r\nNon-trainable params: 4,869,168\r\n_________________________________________________________________\r\nNone\r\n/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\r\n  warnings.warn(\r\nEpoch 1/5\r\n2022-04-19 03:05:06.677678: I tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Loaded cuDNN version 8204\r\n455/455 [==============================] - 204s 442ms/step - loss: 0.2681 - accuracy: 0.9708 - val_loss: 0.2391 - val_accuracy: 0.9897\r\nEpoch 2/5\r\n455/455 [==============================] - 202s 441ms/step - loss: 0.2428 - accuracy: 0.9881 - val_loss: 0.2343 - val_accuracy: 0.9930\r\nEpoch 3/5\r\n455/455 [==============================] - 203s 443ms/step - loss: 0.2396 - accuracy: 0.9901 - val_loss: 0.2320 - val_accuracy: 0.9933\r\nEpoch 4/5\r\n455/455 [==============================] - 203s 445ms/step - loss: 0.2373 - accuracy: 0.9909 - val_loss: 0.2308 - val_accuracy: 0.9941\r\nEpoch 5/5\r\n455/455 [==============================] - 202s 442ms/step - loss: 0.2365 - accuracy: 0.9913 - val_loss: 0.2301 - val_accuracy: 0.9940\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nhub_keras_layer_v1v2 (HubKer (None, 1280)              4869168   \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 1280)              0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 2)                 2562      \r\n=================================================================\r\nTotal params: 4,871,730\r\nTrainable params: 2,562\r\nNon-trainable params: 4,869,168\r\n_________________________________________________________________\r\n228/228 [==============================] - 33s 54ms/step - loss: 0.2290 - accuracy: 0.9951\r\n############### FOR FP32 MODEL\r\n2022-04-19 03:22:31.386912: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2022-04-19 03:22:33.971451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:22:33.972101: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-04-19 03:22:33.972400: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n2022-04-19 03:22:33.973941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:22:33.974352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:22:33.974627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:22:33.976490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:22:33.976763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:22:33.977095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8938 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:2d:00.0, compute capability: 7.5\r\n2022-04-19 03:22:34.005010: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 1188 nodes (856), 1203 edges (869), time = 13.073ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.225ms.\r\n\r\n2022-04-19 03:22:34.653218: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\r\n2022-04-19 03:22:34.653265: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\r\n2022-04-19 03:22:34.711780: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nfully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\r\nWARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\r\n############### FOR FP16 MODEL\r\n2022-04-19 03:32:07.372624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:32:07.372880: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-04-19 03:32:07.372951: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n2022-04-19 03:32:07.373226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:32:07.373494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:32:07.373724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:32:07.373987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:32:07.374212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-19 03:32:07.374402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8938 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:2d:00.0, compute capability: 7.5\r\n2022-04-19 03:32:07.397998: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 1188 nodes (856), 1203 edges (869), time = 12.634ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.21ms.\r\n\r\n2022-04-19 03:32:08.278801: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\r\n2022-04-19 03:32:08.278847: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n```\n</details>", "comments": []}, {"number": 55659, "title": "The callbacks in the training functions in kernels_experimental.cc unnecessarily transfer ownership of the tensors to the caller", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.9\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 16.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n5.1.0\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nCurrently, the callbacks in the `kernels_experimental.cc` file transfer ownership of the tensors to the caller. Although we explain this behavior in the header, this is not intuitive because the tensors don't need to outlive the callback. A better solution would be to automatically free the tensors as soon as the callback returns, thus avoiding instances where callers could mistakenly omit to free the memory themselves.\r\n\r\n\r\n### Standlone code to reproduce the issue\r\n\r\n```c++\r\nvoid callback(TF_OpKernelContext* ctx, TF_Tensor* source, TF_Tensor* dest) {\r\n  // We are not releasing the memory for `source` and `dest, so memory leak occurs\r\n}\r\n\r\nTF_AssignVariable(ctx, 0, 1, callback, status);\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_</details>", "comments": ["@PatriceVignola  \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!"]}, {"number": 55658, "title": "Got this BUG while attempting to save the model", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): I don't know, I ran on a cloud server that I rent\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 11.1.1\r\n- GPU model and memory: GeForce RTX 3090, 23G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n> root@625d1712cc5660deb2f669c7:~# python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n> 2022-04-18 21:15:44.795209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n> v2.4.0-49-g85c8b2a817f 2.4.1\r\n\r\n\r\n**Describe the current behavior**\r\nA checkpoint model is supposed to be saved at the 100st iteration, while attempting to save it, this BUG occurred and is shown under **Other info / logs**, and then the process stopped.\r\n\r\n**Describe the expected behavior**\r\nAfter successfully saving the checkpoint model at the 100st iteration, the process should go on training normaly.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): not sure what a PR is\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://drive.google.com/drive/folders/1fKO0aQmXu1KSNBeny_HWt8SiAtvZ4D_b?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n> [libprotobuf ERROR google/protobuf/wire_format_lite.cc:577] String field 'tensorflow.TensorShapeProto.Dim.name' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes. \r\n> Traceback (most recent call last):\r\n>   File \"train.py\", line 77, in <module>\r\n>     train(model)\r\n>   File \"train.py\", line 24, in train\r\n>     model.optimize()\r\n>   File \"/user-data/HyperBox-main/script/model/box_model.py\", line 329, in optimize\r\n>     self.save_model(itr)\r\n>   File \"/user-data/HyperBox-main/script/model/box_model.py\", line 140, in save_model\r\n>     self.saver.save(self.sess, filename)\r\n>   File \"/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 1208, in save\r\n>     self.export_meta_graph(\r\n>   File \"/opt/conda/lib/python3.8/site-packages/tensorflow/python/training/saver.py\", line 1254, in export_meta_graph\r\n>     graph_def=ops.get_default_graph().as_graph_def(add_shapes=True),\r\n>   File \"/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3345, in as_graph_def\r\n>     result, _ = self._as_graph_def(from_version, add_shapes)\r\n>   File \"/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3262, in _as_graph_def\r\n>     graph.ParseFromString(compat.as_bytes(data))\r\n> google.protobuf.message.DecodeError: Error parsing message\r\n> ", "comments": ["@zjk000 Could you please refer to this [link](https://www.tensorflow.org/tutorials/keras/save_and_load) and try with the latest TF version .Please let us know the outcome?\r\nThanks!"]}, {"number": 55657, "title": "Add GPU-determinism to tf.nn.depthwise_conv2d", "body": "This PR adds GPU-determinism (with cuDNN version 7.6.3 or newer) to `tf.nn.depthwise_conv2d` backprop to `filter` (and therefore also `tf.keras.layers.DepthwiseConv2D`; and `tf.errors.UnimplementedError` is no longer thrown) when op-determinism is enabled via `tf.config.experimental.enable_op_determinism`.\r\n\r\nIncluded in the update to the release notes is the statement, \"This closes issue [47174](https://github.com/tensorflow/tensorflow/issues/47174)\", which is to be confirmed, but is almost certainly true.\r\n\r\ncc @reedwm ", "comments": ["@reedwm, `depthwise_conv_op_test` is [excluded \"from running in the official TF OSS test infrastructure\"](https://www.tensorflow.org/community/contribute/tests#be_aware_where_the_test_executes) in the [BUILD](https://github.com/tensorflow/tensorflow/blob/780be9cbab73a7ae73ec1951d192e1ebe58b53f1/tensorflow/python/kernel_tests/nn_ops/BUILD#L313-L333) file with the tag: `\"no_oss\",  # TODO(b/118842098): Re-enable this test in Kokoro.\"` Do you know if this test ever gets run in CI and also can you give me some insight into what the issue is with this test?", "> @reedwm, `depthwise_conv_op_test` is [excluded \"from running in the official TF OSS test infrastructure\"](https://www.tensorflow.org/community/contribute/tests#be_aware_where_the_test_executes) in the [BUILD](https://github.com/tensorflow/tensorflow/blob/780be9cbab73a7ae73ec1951d192e1ebe58b53f1/tensorflow/python/kernel_tests/nn_ops/BUILD#L313-L333) file with the tag: `\"no_oss\", # TODO(b/118842098): Re-enable this test in Kokoro.\"` Do you know if this test ever gets run in CI and also can you give me some insight into what the issue is with this test?\r\n\r\nThe CI will still run the test in our internal codebase, but will not run using the GitHub codebase. Unfortunately, the logs pointed to in the internal bug, b/118842098, no longer exist, so I have no idea what the issue is. We should probably just remove the `no_oss` tag. If you want to do this, do it in a separate PR to prevent this PR from being rolled back."]}, {"number": 55655, "title": "TFTRT: Respect device placement requested by user", "body": "Previously TRT engines would always default to running on GPU 0. On a multi GPU system, this does not make the best use of the available resources. This PR adds the ability to specify the GPU on which the TRT engine should run.\r\n\r\nSo now, when the user requests a specific device, TFTRT will convert and run the engine on that specified device. e.g.:\r\n\r\n```\r\nwith tf.device('gpu:2'):\r\n    from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n    converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)\r\n    converter.convert()\r\n```\r\n\r\nthe engine will be built and run on  GPU 2, if it exists on the system.\r\n\r\nRunning `converter.summary()` will report the actual GPU on which the engine was placed, e.g. for the above case:\r\n\r\n```\r\nFound the following GPUs:\r\n        - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\r\n        - PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\r\n        - PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\r\nINFO:tensorflow:Assets written to: ./saved_model/my_model/assets\r\nINFO:tensorflow:Linked TensorRT version: (8, 2, 1)\r\nINFO:tensorflow:Loaded TensorRT version: (8, 2, 3)\r\nINFO:tensorflow:Loaded TensorRT 8.2.3 and linked TensorFlow against TensorRT 8.2.1. This is supported because TensorRT minor/patch upgrades are backward compatible.\r\nINFO:tensorflow:Placing imported graph from `./saved_model/my_model` on device: /job:localhost/replica:0/task:0/device:GPU:2\r\nTRTEngineOP Name                 Device        # Nodes # Inputs      # Outputs     Input DTypes       Output Dtypes      Input Shapes       Output Shapes     \r\n================================================================================================================================================================\r\nTRTEngineOp_0_0                  device:GPU:2  5       1             1             ['float32']        ['float32']        [[-1, 784]]        [[-1, 10]]        \r\n\r\n        - Const: 2x\r\n        - MatMul: 2x\r\n        - Relu: 1x\r\n\r\n================================================================================================================================================================\r\n[*] Total number of TensorRT engines: 1\r\n[*] % of OPs Converted: 71.43% [5/7]\r\n```\r\nCC: @bixia1 @DEKHTIARJonathan  @tfeher \r\n\r\n\r\nSigned-off-by: Meenakshi Venkataraman <meenakshiv@nvidia.com>", "comments": ["Can we have a test to show how we would handle this situations: some nodes in the imported graph are already assigned with a gpu ?", "@bixia1  -- I was looking into this, and realized that TF2 saved models don't contain device information. Do you mean at test of this nature:\r\n\r\n```\r\n    model = create_model()\r\n    input_saved_model_dir='./saved_model/my_model'\r\n    model.save(input_saved_model_dir)\r\n\r\n    with tf.device('gpu:1'):\r\n        model_loaded = tf.saved_model.load(export_dir='./saved_model/my_model')\r\n        print(\"Loaded Model device:\", model_loaded.variables[0].device)\r\n\r\n    with tf.device('gpu:2'):\r\n        from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n        converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)\r\n        converter.convert()\r\n        converter.summary()\r\n```\r\n\r\nThe output for the above with the feature is:\r\n```\r\nINFO:tensorflow:Assets written to: ./saved_model/my_model/assets\r\nLoaded Model device: /job:localhost/replica:0/task:0/device:GPU:1\r\nINFO:tensorflow:Linked TensorRT version: (8, 2, 4)\r\nINFO:tensorflow:Loaded TensorRT version: (8, 2, 4)\r\nINFO:tensorflow:Placing imported graph from `./saved_model/my_model` on device: /job:localhost/replica:0/task:0/device:GPU:2\r\nTRTEngineOP Name                 Device        # Nodes # Inputs      # Outputs     Input DTypes       Output Dtypes      Input Shapes       Output Shapes     \r\n================================================================================================================================================================\r\nTRTEngineOp_000_000              device:GPU:2  5       1             1             ['float32']        ['float32']        [[-1, 784]]        [[-1, 10]]        \r\n\r\n        - Const: 2x\r\n        - MatMul: 2x\r\n        - Relu: 1x\r\n\r\n================================================================================================================================================================\r\n[*] Total number of TensorRT engines: 1\r\n[*] % of OPs Converted: 71.43% [5/7]\r\n```\r\n\r\nSupport for TF1 saved models has not been added yet, and is in WIP.", "> @bixia1 -- I was looking into this, and realized that TF2 saved models don't contain device information. Do you mean at test of this nature:\r\n> \r\n> ```\r\n>     model = create_model()\r\n>     input_saved_model_dir='./saved_model/my_model'\r\n>     model.save(input_saved_model_dir)\r\n> \r\n>     with tf.device('gpu:1'):\r\n>         model_loaded = tf.saved_model.load(export_dir='./saved_model/my_model')\r\n>         print(\"Loaded Model device:\", model_loaded.variables[0].device)\r\n> \r\n>     with tf.device('gpu:2'):\r\n>         from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n>         converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)\r\n>         converter.convert()\r\n>         converter.summary()\r\n> ```\r\n> \r\n> The output for the above with the feature is:\r\n> \r\n> ```\r\n> INFO:tensorflow:Assets written to: ./saved_model/my_model/assets\r\n> Loaded Model device: /job:localhost/replica:0/task:0/device:GPU:1\r\n> INFO:tensorflow:Linked TensorRT version: (8, 2, 4)\r\n> INFO:tensorflow:Loaded TensorRT version: (8, 2, 4)\r\n> INFO:tensorflow:Placing imported graph from `./saved_model/my_model` on device: /job:localhost/replica:0/task:0/device:GPU:2\r\n> TRTEngineOP Name                 Device        # Nodes # Inputs      # Outputs     Input DTypes       Output Dtypes      Input Shapes       Output Shapes     \r\n> ================================================================================================================================================================\r\n> TRTEngineOp_000_000              device:GPU:2  5       1             1             ['float32']        ['float32']        [[-1, 784]]        [[-1, 10]]        \r\n> \r\n>         - Const: 2x\r\n>         - MatMul: 2x\r\n>         - Relu: 1x\r\n> \r\n> ================================================================================================================================================================\r\n> [*] Total number of TensorRT engines: 1\r\n> [*] % of OPs Converted: 71.43% [5/7]\r\n> ```\r\n> \r\n> Support for TF1 saved models has not been added yet, and is in WIP.\r\n\r\nThanks for explaining this.", "@bixia1 , @DEKHTIARJonathan  -- I've added support for assigning device to TF1 models as well, please take a look.\r\n"]}, {"number": 55652, "title": "hlo-legalize-to-linalg is incorrect for mhlo::DotGeneralOp", "body": "The usage of `i` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc#L2945 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc#L2970 is incorrect.  You need another index, say `j` that only increments when `assigned_dims[i]` is false.\r\n\r\nSuggested fix\r\n```\r\n      for (int i = 0, j = 0; i < lhs_rank; ++i) {\r\n        if (!assigned_dims[i]) {\r\n          lhs_indices[i] =\r\n              rewriter.getAffineDimExpr(j + lhs_batching_dims.size());\r\n              j++;\r\n        }\r\n      }\r\n```\r\n\r\nand similarly for the corresponding loop for RHS map.  Here's a test case\r\n\r\n```\r\nfunc.func @dot_general_2(%arg0: tensor<64x16x128x32xf32>,\r\n                  %arg1: tensor<16x256x32x128xf32>) -> tensor<16x32x64x256xf32> {\r\n  %0 = \"mhlo.dot_general\"(%arg0, %arg1) {\r\n    dot_dimension_numbers = #mhlo.dot<\r\n      lhs_batching_dimensions = [1, 3],\r\n      lhs_contracting_dimensions = [2],\r\n      rhs_batching_dimensions = [0, 2],\r\n      rhs_contracting_dimensions = [3]\r\n    >,\r\n    precision_config = [#mhlo<\"precision DEFAULT\">, #mhlo<\"precision DEFAULT\">],\r\n    someattr\r\n  } : (tensor<64x16x128x32xf32>, tensor<16x256x32x128xf32>) -> tensor<16x32x64x256xf32>\r\n  func.return %0 : tensor<16x32x64x256xf32>\r\n}\r\n// The iterations are (Batch Dim, Batch Dim, LHS Other Dim, RHS Other dim, Contracting Dim)\r\n// CHECK: #[[MAP0:.*]] = affine_map<(d0, d1, d2, d3, d4) -> (d2, d0, d4, d1)>\r\n// CHECK: #[[MAP1:.*]] = affine_map<(d0, d1, d2, d3, d4) -> (d0, d3, d1, d4)>\r\n// Output is the iterators excluding contracting\r\n// CHECK: #[[MAP2:.*]] = affine_map<(d0, d1, d2, d3, d4) -> (d0, d1, d2, d3)>\r\n// CHECK: func @dot_general_2(\r\n// CHECK-SAME: %[[ARG0:.*]]: tensor<64x16x128x32xf32>, %[[ARG1:.*]]: tensor<16x256x32x128xf32>)\r\n// CHECK: %[[INIT:.*]] = linalg.init_tensor [16, 32, 64, 256]\r\n// CHECK: %[[FILL:.*]] = linalg.fill ins(%{{.*}}{{.*}}outs(%[[INIT]]\r\n// CHECK: linalg.generic\r\n// CHECK-SAME: indexing_maps = [#[[MAP0]], #[[MAP1]], #[[MAP2]]]\r\n// Only contracting dims are reductions\r\n// CHECK-SAME: iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\", \"reduction\"]\r\n// CHECK-SAME: ins(%[[ARG0]], %[[ARG1]] : tensor<64x16x128x32xf32>, tensor<16x256x32x128xf32>)\r\n// CHECK-SAME: outs(%[[FILL]] : tensor<16x32x64x256xf32>)\r\n// CHECK-SAME: {someattr}\r\n// CHECK:   ^bb0(%[[ARG2:.*]]: f32, %[[ARG3:.*]]: f32, %[[ARG4:.*]]: f32):\r\n// CHECK:     %[[MUL:.*]] = arith.mulf %[[ARG2]], %[[ARG3]] : f32\r\n// CHECK:     %[[SUM:.*]] = arith.addf %[[MUL]], %[[ARG4]] : f32\r\n// CHECK:     linalg.yield %[[SUM]] : f32\r\n// CHECK: } -> tensor<16x32x64x256xf32>\r\n```\r\n\r\nIt might be worth considering adding even more test cases, as obviously the current coverage isn't enough.", "comments": ["Hello @srcarroll ,\r\nI request you,can you please feel free to submit a PR for the requested change is to be made.Thanks!", "Yes I can do that.  However, the issue is that I need to contribute from my work account which will require some amount of bureaucracy.  This is why I decided to post the fix here, since it's a very small change.  If this can wait, I'd be happy to submit a PR after we get things sorted out."]}, {"number": 55651, "title": "Not able to encode tensor in any type of image format that other libraries can understand", "body": "I am trying to write a tensor to an image format As seen in colab in bottom, when I take the encoded bytes and write it a file using python, it doesn't work. Trying opening the image it gets corrupted. I have used gif, png, jpeg\r\nWhen I use tf.io.write_file it works. I also tried taking a png and using tf.io_decode_png and than tf.io.encoding_png and the image string bytes differ from the original png. I am using tensorflow serving and using this png result through the rest api, where I don't have access to tf.io.write_file so I was wondering what encode  does differently, than traditional encodings.\r\n\r\nhttps://www.loom.com/share/4b32b330a0e742cab2ea3a1a9af11b22\r\nhttps://colab.research.google.com/drive/13Y4ET8jAHZDq7qwUYgg8nuBkxqniUqGu#scrollTo=IcXVZepnhkW4\r\n", "comments": ["@sushreebarsa anything I can do to make the issue more clear", "@rohanmuplara \r\nWe could see a duplicate[ ticket](https://github.com/tensorflow/serving/issues/1993) in the serving repo of this issue. Could you please move this issue to closed status as it  will be addressed in the serving repo? \r\nThanks!", "@sushreebarsa I actually think this more of a tensorflow core issue.  I can remove it from there if you prefer as this is a problem I am seeing in tensorflow core and people might encounter this even if they are not using tensorflow serving; ", "@rohanmuplara Thank you for the update!\r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!"]}, {"number": 55650, "title": "FusedBatchNormV3 split/fuse to/from Cheaper primitive ops with TF-1.15/TF-2.8", "body": "Hi, \r\n\r\nI am doing Inference with Bert Large & Bert Base models with TF-1.15 and TF-2.8. \r\n\r\n**Inference with Checkpoints:**\r\nFor TF-2.8, I observed set of primitive ops are Fused to single node FusedBatchNormV3 whereas in TF-1.15 fusion of primitive ops is not happening(The graph consists of Primitive ops).\r\n\r\n**Inference with Frozen Graph:**\r\nWith frozen graph(.pb) the graph itself has FusedBatchNormV3 node. For TF-1.15 the FusedBatchNormV3 is split into set of primitive ops. whereas in TF-2.8 split of ops is not happening (The graph consists of Fused op).\r\n\r\nI am trying to find where this fusion/split is happening when it differs TF versions. After going through fusions that happens at grappler phase(remapper.cc) I found the fusion is happening before it reaches grappler phase. \r\n\r\nCan someone help me to point out where the fusion/split of FusedBatchNormV3 is being taken place?\r\n\r\nThanks!!", "comments": ["Hello @Alavandar08 ,\r\nI request you, please specify the Use Cases/colab gist or share the link for the mentioned statement.It helps to debug the issue.Thanks!\r\n", "Hi @tilakrayal \r\nAs mentioned above one of the use case is set of primitive ops are Fused into single op FusedBatchNormV3 in Bert Large with TF-2.8\r\nHere I am adding the before and after fusion of FusedBatchNormV3 with checkpoints snapshots for the reference\r\n\r\n**Before Fusion:**\r\n<img width=\"253\" alt=\"FusedBatchNormV3-primitve_ops\" src=\"https://user-images.githubusercontent.com/86424477/163922325-e0b6cfec-2331-4458-b850-0c3acba1d30b.png\">\r\n\r\n**After Fusion:**\r\n<img width=\"253\" alt=\"FusedBatchNormV3\" src=\"https://user-images.githubusercontent.com/86424477/163922390-7f8571bd-b950-43c1-bbe5-a813ee68d5b2.png\">\r\n\r\nThe file that I referred to in grappler -  [https://github.com/tensorflow/tensorflow/blob/r2.8/tensorflow/core/grappler/optimizers/remapper.cc](https://github.com/tensorflow/tensorflow/blob/r2.8/tensorflow/core/grappler/optimizers/remapper.cc)", "@Alavandar08 ,\r\nThe provided grappler file link is re-directing to github home-page.Can you please provide the correct link or code to reproduce the issue.It helps to debug the issue.Thanks!", "@tilakrayal  Here is the correct url of the grappler file [https://github.com/tensorflow/tensorflow/blob/r2.8/tensorflow/core/grappler/optimizers/remapper.cc](https://github.com/tensorflow/tensorflow/blob/r2.8/tensorflow/core/grappler/optimizers/remapper.cc)\r\nFor use cases we are trying to improve FusedBatchNorm performance so we want to know where the fusion happens.", "In Tensorflow 1.x with the graph mode as default it optimizes the graph computations and the same reason you see mainly primitive ops.\r\nWith Tensorflow 2.x with eager execution as default, in order to optimize the overall computation and to reduce the memory footprints fused operations are mainly used. \r\nApart from the code link which you shared, you can refer this documentation for [FusedBatchNormV3](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fused-batch-norm-v3#summary).\r\nFor other options to optimize graphs you can refer [this](https://www.tensorflow.org/guide/graph_optimization) document.\r\n[Here](https://github.com/tensorflow/tensorflow/blob/r2.8/tensorflow/core/ops/nn_ops.cc#L203) is an other code implementation of NN ops with `FusedNormV3`.\r\nHope this detail helps you. Thanks!"]}, {"number": 55649, "title": "Invalid number of inputs provided for running a SignatureDef, expected 4 vs provided 1", "body": "### 1. System information\r\n\r\nOS: Windows 10:\r\nTensorFlow: 2.7:\r\n\r\n### 2. Code\r\n\r\npython version : 3.7.13  / 64 bit\r\n\r\n#### Code used to create and convert transfer learning tflite model \r\n\r\n```\r\n    # * TESTED\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n    def extract_weights(self,checkpoint_path):\r\n        \"\"\"\r\n        Extracts the traininable weights of the head model as a list of tensors.\r\n\r\n        Paramaters:\r\n\r\n        Returns:\r\n            Map of extracted weights and biases.\r\n        \"\"\"\r\n        tmp_dict = {}\r\n        tensor_names = [weight.name for weight in self.head_model.weights]\r\n        tensors_to_save = [weight.read_value() for weight in self.head_model.weights]\r\n        for index, layer in enumerate(tensors_to_save):\r\n            tmp_dict[tensor_names[index]] = layer\r\n\r\n        return tmp_dict\r\n\r\n\r\n    @tf.function(input_signature=[SIGNATURE_DICT])\r\n    def initialize_weights(self, weights):\r\n        \"\"\"\r\n        Initializes weights of the head model.\r\n\r\n        Paramaters:\r\n            weights : dict of tensors used for initialization.\r\n        Returns:\r\n            NONE\r\n        \"\"\"\r\n\r\n        restored_tensors={}\r\n        tensor_names = [weight.name for weight in self.head_model.weights]\r\n        for i, tensor in enumerate(self.head_model.weights):\r\n            tensor.assign(weights[tensor_names[i]])\r\n            restored_tensors[tensor.name] = tensor\r\n        \r\n        return restored_tensors\r\n```\r\nAndroid Dependency:\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite:2.7.0'\r\n\r\nThis is the android method that uses the extract signature.\r\n```\r\n  float[][] extractWeights(){\r\n    //ce ar trebui sa pun pe input ?\r\n    //da eroare daca e null\r\n    Map<String, Object> inputs = new HashMap<>();\r\n    File outputFile=new File(\"workaround\");\r\n    inputs.put(\"checkpoint_path\",outputFile.getAbsolutePath());\r\n\r\n    Map<String,Object> outputs=new HashMap<>();\r\n      // cred ca pe outputs trebuie sa imi fac legit 4\r\n      // inserari ptr fiecare layer de weightsuri de dims ( [1280,128],[128,],[128,11],[11,]\r\n    float[][] layer1_kernel=new float[BOTTLENECK_SIZE][128];\r\n    float[][] layer1_bias  =new float[128][]; //sau ar trebui sa aibe doar o dimensiune ?\r\n    float[][] layer2_kernel=new float[128][numClasses];\r\n    float[][] layer2_bias  =new float[numClasses][];\r\n\r\n    outputs.put(\"dense_1/kernel:0\", layer1_kernel);\r\n    outputs.put(\"dense_1/bias:0\", layer1_bias);\r\n    outputs.put(\"dense_2/kernel:0\", layer2_kernel);\r\n    outputs.put(\"dense_2/bias:0\", layer2_bias);\r\n\r\n    this.interpreter.runSignature(inputs,outputs,\"extract\");\r\n\r\n    return outputs;\r\n  }\r\n```\r\n\r\n\r\nError got :\r\n\r\nInvalid number of inputs provided for running a SignatureDef, expected 4 vs provided 1\r\n\r\nFrom my point of view the output of extract should be passable as input parameter to initialize giving that the output and the input sgnature are exactly the same as data types and structure.\r\n\r\nYou can find a colab with the implementation problems here [colab](https://colab.research.google.com/drive/1XC8vgEWm32TS9w0rszlGEnn8G6BOYlcM?usp=sharing) . ", "comments": ["Hi @chunduriv  ! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/6778c34dc7a625f1016b28c523d5b60c/input_signature_issue.ipynb#scrollTo=lv_qQc5mhrrP) , [2.8](https://colab.sandbox.google.com/gist/mohantym/471da4f781368037940d7a76cc61f9d8/input_signature_issue.ipynb) and [2.9.0rc0](https://colab.sandbox.google.com/gist/mohantym/5bc9740016426dcb577b8336d0928e8e/input_signature_issue.ipynb#scrollTo=lv_qQc5mhrrP). Thanks!", "@karimnosseir Could you please take a look at this issue?"]}, {"number": 55648, "title": "Gradients not computed upon `tf.concat()`", "body": "Hi!\r\n\r\nI am trying to re-write MAVNet (https://github.com/sudakshin/imitation_learning/blob/master/2.train_model/MavNet.py) into genuine Tensorflow (instead of TFLearn) and train it. Note that MAVNet consists of numerous `tf.keras.layers.Concatenate()`, which involves `tf.concat()`.\r\nFor some reason, gradient flow seems to break... and I just found out that the flow breaks whenever it passes `tf.concat()`.\r\n\r\nPlease pardon if this issue is a duplicate of #37726 , but I feel that the previous issue has been closed too early without being thoroughly investigated and resolved.\r\n\r\nThe details of this issue is as follows:\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0-2.8.0 (issue reproduced from all versions included in this range)\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 11.4 + cuDNN 8.1.1\r\n- GPU model and memory: NVIDIA GeForce GTX Titan X\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorflow is not able to compute gradients after concatenating multiple feature maps with `tf.concat()`.\r\n\r\n**Describe the expected behavior**\r\nTensorflow should be able to compute gradients after concatenating multiple feature maps with `tf.concat()`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n(Here, we assume all necessary libraries (including Tensorflow) are imported)\r\n\r\n    class LocalResponseNormalization(tf.keras.layers.Layer):\r\n        def __init__(self, depth_radius=5, bias=1, alpha=1, beta=0.5):\r\n            super(LocalResponseNormalization, self).__init__()\r\n            self.depth_radius = depth_radius\r\n            self.bias = bias\r\n            self.alpha = alpha\r\n            self.beta = beta\r\n\r\n        def build(self,input):\r\n            pass\r\n\r\n        def call(self, input):\r\n            return tf.nn.local_response_normalization(input, alpha=self.alpha, beta=self.beta)\r\n\r\n        def get_config(self):\r\n            config = super().get_config().copy()\r\n            return config\r\n\r\n    def MavNet(input_shape, num_classes=2):\r\n        img_input = tf.keras.layers.Input(shape=input_shape)\r\n\r\n        conv1_7_7 = tf.keras.layers.Conv2D(64, 7, strides=2, activation='relu', padding='same', name = 'conv1_7_7_s2')(img_input)\r\n        pool1_3_3 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2)(conv1_7_7)\r\n        pool1_3_3 = LocalResponseNormalization(alpha=0.0001, beta=0.75)(pool1_3_3)\r\n        conv2_3_3_reduce = tf.keras.layers.Conv2D(64,1, activation='relu', padding='same', name = 'conv2_3_3_reduce')(pool1_3_3)\r\n        conv2_3_3 = tf.keras.layers.Conv2D(192,3, activation='relu', padding='same', name='conv2_3_3')(conv2_3_3_reduce)\r\n        conv2_3_3 = LocalResponseNormalization(alpha=0.0001, beta=0.75)(conv2_3_3)\r\n        pool2_3_3 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, name='pool2_3_3_s2')(conv2_3_3)\r\n\r\n        mavnet_3a_1_1 = tf.keras.layers.Conv2D(64, 1, activation='relu', padding='same', name='mavnet_3a_1_1')(pool2_3_3)\r\n        mavnet_3a_3_3_reduce = tf.keras.layers.Conv2D(96,1, activation='relu', padding='same',  name='mavnet_3a_3_3_reduce')(pool2_3_3)\r\n        mavnet_3a_3_3 = tf.keras.layers.Conv2D(128,kernel_size=3,  activation='relu', padding='same',  name = 'mavnet_3a_3_3')(mavnet_3a_3_3_reduce)\r\n        mavnet_3a_pool = tf.keras.layers.MaxPooling2D(pool_size=3, strides=1, padding='same')(pool2_3_3)\r\n        mavnet_3a_pool_1_1 = tf.keras.layers.Conv2D(32, kernel_size=1, activation='relu', padding='same', name='mavnet_3a_pool_1_1')(mavnet_3a_pool)\r\n        mavnet_3a_output = tf.keras.layers.Concatenate(axis=3)([mavnet_3a_1_1, mavnet_3a_3_3, mavnet_3a_pool_1_1])\r\n\r\n        mavnet_3b_1_1 = tf.keras.layers.Conv2D(128,kernel_size=1,activation='relu', padding='same', name= 'mavnet_3b_1_1' )(mavnet_3a_output)\r\n        mavnet_3b_3_3_reduce = tf.keras.layers.Conv2D( 128, kernel_size=1, activation='relu', padding='same', name='mavnet_3b_3_3_reduce')(mavnet_3a_output)\r\n        mavnet_3b_3_3 = tf.keras.layers.Conv2D( 192, kernel_size=3,  activation='relu', padding='same', name='mavnet_3b_3_3')(mavnet_3b_3_3_reduce)\r\n        mavnet_3b_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same', name='mavnet_3b_pool')(mavnet_3a_output)\r\n        mavnet_3b_pool_1_1 = tf.keras.layers.Conv2D( 64, kernel_size=1,activation='relu', padding='same', name='mavnet_3b_pool_1_1')(mavnet_3b_pool)\r\n        mavnet_3b_output = tf.keras.layers.Concatenate(axis=3,name='mavnet_3b_output')([mavnet_3b_1_1, mavnet_3b_3_3, mavnet_3b_pool_1_1])\r\n\r\n        pool3_3_3 = tf.keras.layers.MaxPooling2D( pool_size=3, strides=2, padding='same', name='pool3_3_3')(mavnet_3b_output)\r\n\r\n        mavnet_4a_1_1 = tf.keras.layers.Conv2D( 192, kernel_size=1, activation='relu', padding='same', name='mavnet_4a_1_1')(pool3_3_3)\r\n        mavnet_4a_3_3_reduce = tf.keras.layers.Conv2D( 96, kernel_size=1, activation='relu', padding='same', name='mavnet_4a_3_3_reduce')(pool3_3_3)\r\n        mavnet_4a_3_3 = tf.keras.layers.Conv2D( 208, kernel_size=3,  activation='relu', padding='same', name='mavnet_4a_3_3')(mavnet_4a_3_3_reduce)\r\n        mavnet_4a_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same', name='mavnet_4a_pool')(pool3_3_3)\r\n        mavnet_4a_pool_1_1 = tf.keras.layers.Conv2D( 64, kernel_size=1, activation='relu', padding='same', name='mavnet_4a_pool_1_1')(mavnet_4a_pool)\r\n        mavnet_4a_output = tf.keras.layers.Concatenate(axis=3, name='mavnet_4a_output')([mavnet_4a_1_1, mavnet_4a_3_3, mavnet_4a_pool_1_1])\r\n\r\n        mavnet_4b_1_1 = tf.keras.layers.Conv2D( 160, kernel_size=1, activation='relu', padding='same', name='mavnet_4b_1_1')(mavnet_4a_output)\r\n        mavnet_4b_3_3_reduce = tf.keras.layers.Conv2D( 112, kernel_size=1, activation='relu', padding='same', name='mavnet_4b_3_3_reduce')(mavnet_4a_output)\r\n        mavnet_4b_3_3 = tf.keras.layers.Conv2D( 224, kernel_size=3, activation='relu', padding='same', name='mavnet_4b_3_3')(mavnet_4b_3_3_reduce)\r\n        mavnet_4b_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same', name='mavnet_4b_pool')(mavnet_4a_output)\r\n        mavnet_4b_pool_1_1 = tf.keras.layers.Conv2D( 64, kernel_size=1, activation='relu', padding='same', name='mavnet_4b_pool_1_1')(mavnet_4b_pool)\r\n        mavnet_4b_output = tf.keras.layers.Concatenate(axis=3, name='mavnet_4b_output')([mavnet_4b_1_1, mavnet_4b_3_3, mavnet_4b_pool_1_1])\r\n\r\n        mavnet_4c_1_1 = tf.keras.layers.Conv2D( 128, kernel_size=1, activation='relu', padding='same', name='mavnet_4c_1_1')(mavnet_4b_output)\r\n        mavnet_4c_3_3_reduce = tf.keras.layers.Conv2D( 128, kernel_size=1, activation='relu', padding='same', name='mavnet_4c_3_3_reduce')(mavnet_4b_output)\r\n        mavnet_4c_3_3 = tf.keras.layers.Conv2D( 256,  kernel_size=3, activation='relu', padding='same', name='mavnet_4c_3_3')(mavnet_4c_3_3_reduce)\r\n        mavnet_4c_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same')(mavnet_4b_output)\r\n        mavnet_4c_pool_1_1 = tf.keras.layers.Conv2D( 64, kernel_size=1, activation='relu', padding='same', name='mavnet_4c_pool_1_1')(mavnet_4c_pool)\r\n        mavnet_4c_output = tf.keras.layers.Concatenate(axis=3,name='mavnet_4c_output')([mavnet_4c_1_1, mavnet_4c_3_3, mavnet_4c_pool_1_1])\r\n\r\n        mavnet_4d_1_1 = tf.keras.layers.Conv2D( 112, kernel_size=1, activation='relu', padding='same', name='mavnet_4d_1_1')(mavnet_4c_output)\r\n        mavnet_4d_3_3_reduce = tf.keras.layers.Conv2D( 144, kernel_size=1, activation='relu', padding='same', name='mavnet_4d_3_3_reduce')(mavnet_4c_output)\r\n        mavnet_4d_3_3 = tf.keras.layers.Conv2D( 288, kernel_size=3, activation='relu', padding='same', name='mavnet_4d_3_3')(mavnet_4d_3_3_reduce)\r\n        mavnet_4d_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same', name='mavnet_4d_pool')(mavnet_4c_output)\r\n        mavnet_4d_pool_1_1 = tf.keras.layers.Conv2D( 64, kernel_size=1, activation='relu', padding='same', name='mavnet_4d_pool_1_1')(mavnet_4d_pool)\r\n        mavnet_4d_output = tf.keras.layers.Concatenate(axis=3, name='mavnet_4d_output')([mavnet_4d_1_1, mavnet_4d_3_3, mavnet_4d_pool_1_1])\r\n\r\n        mavnet_4e_1_1 = tf.keras.layers.Conv2D( 256, kernel_size=1, activation='relu', padding='same', name='mavnet_4e_1_1')(mavnet_4d_output)\r\n        mavnet_4e_3_3_reduce = tf.keras.layers.Conv2D( 160, kernel_size=1, activation='relu', padding='same', name='mavnet_4e_3_3_reduce')(mavnet_4d_output)\r\n        mavnet_4e_3_3 = tf.keras.layers.Conv2D( 320, kernel_size=3, activation='relu', padding='same', name='mavnet_4e_3_3')(mavnet_4e_3_3_reduce)\r\n        mavnet_4e_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same', name='mavnet_4e_pool')(mavnet_4d_output)\r\n        mavnet_4e_pool_1_1 = tf.keras.layers.Conv2D( 128, kernel_size=1, activation='relu', padding='same', name='mavnet_4e_pool_1_1')(mavnet_4e_pool)\r\n        mavnet_4e_output = tf.keras.layers.Concatenate(axis=3)([mavnet_4e_1_1, mavnet_4e_3_3,mavnet_4e_pool_1_1])\r\n\r\n        pool4_3_3 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=2, padding='same', name='pool_3_3')(mavnet_4e_output)\r\n\r\n        mavnet_5a_1_1 = tf.keras.layers.Conv2D( 256, kernel_size=1, activation='relu', padding='same', name='mavnet_5a_1_1')(pool4_3_3)\r\n        mavnet_5a_3_3_reduce = tf.keras.layers.Conv2D( 160, kernel_size=1, activation='relu', padding='same', name='mavnet_5a_3_3_reduce')(pool4_3_3)\r\n        mavnet_5a_3_3 = tf.keras.layers.Conv2D( 320, kernel_size=3, activation='relu', padding='same', name='mavnet_5a_3_3')(mavnet_5a_3_3_reduce)\r\n        mavnet_5a_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same', name='mavnet_5a_pool')(pool4_3_3)\r\n        mavnet_5a_pool_1_1 = tf.keras.layers.Conv2D( 128, kernel_size=1,activation='relu', padding='same', name='mavnet_5a_pool_1_1')(mavnet_5a_pool)\r\n        mavnet_5a_output = tf.keras.layers.Concatenate(axis=3)([mavnet_5a_1_1, mavnet_5a_3_3, mavnet_5a_pool_1_1])\r\n\r\n        mavnet_5b_1_1 = tf.keras.layers.Conv2D( 384, kernel_size=1,activation='relu', padding='same', name='mavnet_5b_1_1')(mavnet_5a_output)\r\n        mavnet_5b_3_3_reduce = tf.keras.layers.Conv2D( 192, kernel_size=1, activation='relu', padding='same', name='mavnet_5b_3_3_reduce')(mavnet_5a_output)\r\n        mavnet_5b_3_3 = tf.keras.layers.Conv2D( 384,  kernel_size=3,activation='relu', padding='same', name='mavnet_5b_3_3')(mavnet_5b_3_3_reduce)\r\n        mavnet_5b_pool = tf.keras.layers.MaxPooling2D( pool_size=3, strides=1, padding='same', name='mavnet_5b_pool')(mavnet_5a_output)\r\n        mavnet_5b_pool_1_1 = tf.keras.layers.Conv2D( 128, kernel_size=1, activation='relu', padding='same', name='mavnet_5b_pool_1_1')(mavnet_5b_pool)\r\n        mavnet_5b_output = tf.keras.layers.Concatenate(axis=3)([mavnet_5b_1_1, mavnet_5b_3_3, mavnet_5b_pool_1_1])\r\n\r\n        pool5_7_7 = tf.keras.layers.AveragePooling2D(pool_size=7, strides=1, padding='same')(mavnet_5b_output)\r\n        pool5_7_7 = tf.keras.layers.Dropout(0.4)(pool5_7_7)\r\n        pool5_7_7_flatten = tf.keras.layers.Flatten()(pool5_7_7)\r\n        output = tf.keras.layers.Dense(num_classes, activation='softmax')(pool5_7_7_flatten)\r\n\r\n        model = tf.keras.Model(img_input, output)\r\n        model.compile(optimizer=tf.keras.optimizers.SGD(0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\n        return model\r\n\r\n    model = MavNet((100, 100, 1), num_classes)\r\n\r\n    # Here, you can put input data of your own\r\n    model.fit(data)\r\n\r\n\r\n\r\nIn addition, according to #37726, you can refer to a simple example in which gradient computation after `tf.concat` does not work : https://colab.research.google.com/drive/1dkCcL5jfBmo47EsvmhNumjIkCGIdeFd5\r\n", "comments": ["@joonjeon Thank you for raising this issue!\r\nPlease refer to this [link](https://www.tensorflow.org/guide/autodiff) and for any further queries  please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!"]}, {"number": 55647, "title": "ValueError: Input 0 of node model/prune_low_magnitude_conv2d/AssignVariableOp was passed float from model/prune_low_magnitude_conv2d/Mul/ReadVariableOp/resource:0 incompatible with expected resource.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution:\r\ncentos 7\r\n- TensorFlow installation (pip package or built from source):\r\npip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\npip package\uff0c tensorflow2.6,  tensorflow-model-optimization==0.7.2\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n`\r\n            self.model = create_model(args['networks'])\r\n\r\n            pruning_params = {\r\n                'pruning_schedule':\r\n                    PolynomialDecay(\r\n                        initial_sparsity=0.0,\r\n                        final_sparsity=0.50,\r\n                        begin_step=0,\r\n                        end_step=500000000,\r\n                        frequency=100),\r\n            }\r\n\r\n            self.model = prune.prune_low_magnitude(\r\n                self.model, **pruning_params)\r\n\r\n        self.lg.info('Create model successfully! Params: [{:.2f}]K'.format(self.model.count_params()/1e3))\r\n        self.lg.info('Create model successfully! Params: [{:.2f}]'.format(self.model.count_params()))\r\n\r\n        self.train_data = train_data\r\n        self.val_data = val_data\r\n        self.writer = writer\r\n\r\n        self.optimizer = tf.keras.optimizers.Adam(lr=self.opt['lr'])\r\n        lr_scheduler = LearningRateScheduler(self.scheduler)\r\n        epoch_end_call = Epoch_End_Callback(self.val_data, self.train_data, self.lg, self.writer, args['paths'], self.opt['val_step'], state=self.state)\r\n        self.callback = [lr_scheduler, epoch_end_call]\r\n\r\n        self.prune_callbacks = [\r\n            lr_scheduler,\r\n            epoch_end_call,\r\n            pruning_callbacks.UpdatePruningStep(),\r\n            pruning_callbacks.PruningSummaries(log_dir=\"./logs\"),\r\n        ]\r\n\r\n    def train(self):\r\n        if self.resume == False:\r\n            self.model.compile(optimizer=self.optimizer, loss=self.opt['loss'])\r\n        # history = self.model.fit(self.train_data, epochs=self.opt['epochs'], workers=self.opt['workers'], callbacks=self.callback, initial_epoch=self.state['current_epoch']+1)\r\n        history = self.model.fit(self.train_data, epochs=self.opt['epochs'], workers=self.opt['workers'], callbacks=self.prune_callbacks, initial_epoch=self.state['current_epoch']+1)\r\n\r\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\r\n        converter._enable_tflite_resource_variables = True\r\n        converter.optimizations = {tf.lite.Optimize.EXPERIMENTAL_SPARSITY, tf.lite.Optimize.DEFAULT}\r\n        tflite_model = converter.convert()\r\n\r\n        tflite_model_path = './logs/model.tflite'\r\n        print('model is saved to {}'.format(tflite_model_path))\r\n        with open(tflite_model_path, 'wb') as f:\r\n            f.write(tflite_model)\r\n\r\n\r\n`\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and/or has lesser accuracy.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\n`\r\n/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  category=CustomMaskWarning)\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 68, in <module>\r\n    solver.train()\r\n  File \"/home/usrname/SR/TF/mymodel_03/solvers/solver.py\", line 122, in train\r\n    tflite_model = converter.convert()\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 729, in wrapper\r\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 715, in _convert_and_export_metrics\r\n    result = convert_func(self, *args, **kwargs)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1123, in convert\r\n    self._freeze_keras_model())\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py\", line 218, in wrapper\r\n    raise error from None  # Re-throws the exception.\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/convert_phase.py\", line 208, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1080, in _freeze_keras_model\r\n    self._funcs[0], lower_control_flow=False))\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1235, in convert_variables_to_constants_v2_as_graph\r\n    converted_input_indices)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1080, in _construct_concrete_function\r\n    new_output_names)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 650, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 628, in wrap_function\r\n    collections={}),\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 1007, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 87, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 93, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py\", line 648, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 549, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 405, in import_graph_def\r\n    producer_op_list=producer_op_list)\r\n  File \"/home/usrname/anaconda3/envs/tf2.6_cudnn8.1/lib/python3.7/site-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Input 0 of node model/prune_low_magnitude_conv2d/AssignVariableOp was passed float from model/prune_low_magnitude_conv2d/Mul/ReadVariableOp/resource:0 incompatible with expected resource.\r\n`\r\n\r\n### 4. bug\r\nWhen I use tfmot to try to sparse a model and convert the model to tflite format, it reports an error\uff1a\r\n`\r\nValueError: Input 0 of node model/prune_low_magnitude_conv2d/AssignVariableOp was passed float from model/prune_low_magnitude_conv2d/Mul/ReadVariableOp/resource:0 incompatible with expected resource.\r\n`\r\n\r\n", "comments": ["Hi @douzaikongcheng ! Could you provide the above code as Colab gist? Please try in 2.8 /2.9 or nightly version too. Attaching relevant threads for reference [1](https://github.com/tensorflow/tensorflow/issues/43833), [2](https://stackoverflow.com/questions/69590274/warning-custom-mask-layers-require-a-config-and-must-override-when-saving-the). Thank you!", "@[mohantym](https://github.com/mohantym), Thank you, I will try."]}, {"number": 55646, "title": "Enable `unique` bool for `tf.random.uniform`! ", "body": "**System information**\r\n- TensorFlow version (you are using): 2.8\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, the `tf.random.uniform`, randomly sample values form given min and max range **without considering the repeating same value**. For example, currently:\r\n\r\n```python\r\ntf.random.uniform(shape=[5], maxval=5, dtype=tf.int32, seed=10)\r\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 3, 3, 1], dtype=int32)>\r\n````\r\n\r\nbut if it has a `unique` bool parameter, it would be possible to generate a unique random value. For example:\r\n\r\n```python\r\ntf.random.uniform(shape=[5], maxval=5, dtype=tf.int32, seed=10, unique=True)\r\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 2, 4, 3, 1], dtype=int32)>\r\n````\r\n\r\n**Will this change the current API? How?**\r\n\r\nFrom \r\n\r\n```python\r\ntf.random.uniform(\r\n    shape,\r\n    minval=0,\r\n    maxval=None,\r\n    dtype=tf.dtypes.int32,\r\n    seed=None,\r\n    name=None\r\n)\r\n```\r\n\r\nTo \r\n\r\n```python\r\ntf.random.uniform(\r\n    shape,\r\n    minval=0,\r\n    maxval=None,\r\n    dtype=tf.dtypes.int32,\r\n    seed=None,\r\n    name=None,\r\n    unique=False\r\n)\r\n```\r\n\r\n\r\n**Who will benefit from this feature?**\r\n\r\nA similar thing is possible with `import random; random.sample`. So, having `unique` bool in `tf.ranom.uniform` might be useful in some special cases. Similar param in [tf.random.uniform_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/random/uniform_candidate_sampler).\r\n\r\n\r\n\r\n**Any Other info.**\r\n\r\nI am not sure if there is any other convenient TensorFlow function that can be used to generate unique random values. I know we can do \r\n\r\n```python\r\ntf.random.shuffle(tf.range(5))\r\n```\r\n\r\nBut `tf.random.shuffle` has certain limitations to fall back to CPU. That's why we can't use it here. Let me know if I've missed something. ", "comments": []}, {"number": 55645, "title": "[PluggableDevice] Add TF_AddNVariant and TF_ZerosLikeVariant", "body": "Support for TF_VARIANT is not really available for pluggable devices through the API since it's very hard (or even impossible) to read the content of a C++ object through ABIs. Even if we were to use the exact same headers as TensorFlow uses for all C++ objects, we would most likely need to use the exact same compiler.\r\n\r\nSince some models depend heavily on variant tensors which contain `TensorList` objects, we'd like to propose 2 new APIs that address this issue:\r\n\r\n```cpp\r\nTF_CAPI_EXPORT extern void TF_AddNVariant(\r\n    TF_OpKernelContext* ctx,\r\n    void (*binaryAddFunc)(TF_OpKernelContext* ctx, const TF_Tensor* a, const TF_Tensor* b, TF_Tensor* out),\r\n    TF_Status* status);\r\n\r\nTF_CAPI_EXPORT extern void TF_ZerosLikeVariant(\r\n    TF_OpKernelContext* ctx,\r\n    void (*zerosLikeFunc)(TF_OpKernelContext* ctx, const TF_Tensor* input, TF_Tensor* out),\r\n    TF_Status* status);\r\n```\r\n\r\nLike the `TF_AssignVariable` and `TF_AssignUpdateVariable` functions currently available in `kernels_experimental.h`, the goal of `TF_AddNVariant` and `TF_ZerosLikeVariant` is to allow plugins to implement those 2 key operations by treating the Variant objects as a black box. The Variant objects are unwrapped within TensorFlow core, which then call the binary_add_func or zeros_like_func callbacks provided by the user, which are guaranteed to contain tensors of primitives (e.g. `TF_FLOAT`).", "comments": []}, {"number": 55644, "title": "TFLite: Cannot run model inference converted with SELECT_TF_OPS: ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Ubuntu 20.04.3 LTS\r\n- TensorFlow installation: from source\r\n- TensorFlow library: 2.9\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n**Code:** https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal\r\n**Model:** https://tfhub.dev/google/boundless/quarter/1 (basically any model which must be converted with `tf.lite.OpsSet.SELECT_TF_OPS`)\r\n\r\nModel conversion code (with SELECT_TF_OPS):\r\n```\r\n# Load the original model\r\nmodel = hub.load(model_handle)\r\nconcrete_function = model.signatures['default']\r\n\r\n# Convert the model to TensorFlow Lite\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\n# Serialize the model\r\nfilename = f'boundless_quarter_{quantization}.tflite' \r\nopen(filename, 'wb').write(tflite_model)\r\n\r\n```\r\n### 3. Any other info / logs\r\n**Error message when minimal.exe gets model.tflite saved with SELECT_TF_OPS:**\r\n```\r\nuser1@ubuntu:~/minimal$ ./minimal ../boundless_quarter_integer.tflite \r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nERROR: Node number 0 (FlexIdentity) failed to prepare.\r\nERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select\r\nERROR: Node number 0 (FlexIdentity) failed to prepare.\r\nError at /home/user1/tensorflow_src/tensorflow/lite/examples/minimal/minimal.cc:60\r\n```\r\n\r\n**Correct output, using any model saved without SELECT_TF_OPS (Expected behaviour):**\r\n```\r\nuser1@ubuntu:~/minimal$ ./minimal ../model.tflite \r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n=== Pre-invoke Interpreter State ===\r\nInterpreter has 1 subgraphs.\r\n\r\n-----------Subgraph-0 has 27 tensors and 4 nodes------------\r\n1 Inputs: [0] -> 4B (0.00MB)\r\n1 Outputs: [8] -> 4B (0.00MB)\r\n\r\nTensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset  \r\nTensor   0 serving_default_dense_... kTfLiteFloat32  kTfLiteArenaRw     4        / 0.00 [1,1] [0, 4)\r\nTensor   1 dense_2/bias              kTfLiteFloat32  kTfLiteMmapRo      4        / 0.00 [1] [572, 576)\r\nTensor   2 sequential/dense/MatMul   kTfLiteFloat32  kTfLiteMmapRo      4        / 0.00 [1,1] [480, 484)\r\nTensor   3 sequential/dense_1/MatMul kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16,1] [336, 400)\r\nTensor   4 sequential/dense_2/MatMul kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [1,16] [192, 256)\r\nTensor   5 sequential/dense_1/Mat... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [0, 64)\r\nTensor   6 sequential/dense/MatMul1  kTfLiteFloat32  kTfLiteArenaRw     4        / 0.00 [1,1] [-1, -1)\r\nTensor   7 sequential/dense/BiasA... kTfLiteFloat32  kTfLiteArenaRw     64       / 0.00 [1,16] [-1, -1)\r\nTensor   8 StatefulPartitionedCall:0 kTfLiteFloat32  kTfLiteArenaRw     4        / 0.00 [1,1] [64, 68)\r\nTensor   9 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  10 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  11 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  12 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  13 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  14 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  15 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  16 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  17 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  18 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  19 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  20 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  21 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  22 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  23 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  24 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  25 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  26 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\n\r\nkTfLiteArenaRw Info: \r\nTensor 0 has the max size 4 bytes (0.000 MB).\r\nThis memory arena is estimated as[0x55e3cc5bb104, 0x55e3cc5bb0c0), taking 68 bytes (0.000 MB).\r\nOne possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:\r\nTensor 0 -> 8.\r\n\r\nkTfLiteArenaRwPersistent Info: not holding any allocation.\r\n\r\nkTfLiteMmapRo Info: \r\nTensor 3 has the max size 64 bytes (0.000 MB).\r\nThis memory arena is estimated as[0x7fb2de3f4648, 0x7fb2de3f4408), taking 576 bytes (0.001 MB).\r\nOne possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:\r\nTensor 5 -> 4 -> 3 -> 2 -> 1.\r\n\r\nkTfLiteDynamic Info: not holding any allocation.\r\n\r\nNode   0 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 3)\r\n  3 Input Tensors:[0,2,-1] -> 0B (0.00MB)\r\n  1 Output Tensors:[6] -> 0B (0.00MB)\r\nNode   1 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 3)\r\n  3 Input Tensors:[6,3,5] -> 0B (0.00MB)\r\n  1 Output Tensors:[7] -> 0B (0.00MB)\r\nNode   2 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 3)\r\n  3 Input Tensors:[7,4,1] -> 0B (0.00MB)\r\n  1 Output Tensors:[8] -> 0B (0.00MB)\r\nNode   3 Operator Custom Name TfLiteXNNPackDelegate \r\n  6 Input Tensors:[0-5] -> 204B (0.00MB)\r\n  1 Output Tensors:[8] -> 4B (0.00MB)\r\n\r\nExecution plan as the list of 1 nodes invoked in-order: [3]\r\nAmong these nodes in the execution plan:\r\n  Node 3 is a TfLiteXNNPackDelegate node (0x55e3cc5ba4f0), which has delegated 3 nodes: [0-2]\r\n--------------Subgraph-0 dump has completed--------------\r\n\r\n\r\n\r\n=== Post-invoke Interpreter State ===\r\nInterpreter has 1 subgraphs.\r\n\r\n-----------Subgraph-0 has 27 tensors and 4 nodes------------\r\n1 Inputs: [0] -> 4B (0.00MB)\r\n1 Outputs: [8] -> 4B (0.00MB)\r\n\r\nTensor  ID Name                      Type            AllocType          Size (Bytes/MB)    Shape      MemAddr-Offset  \r\nTensor   0 serving_default_dense_... kTfLiteFloat32  kTfLiteArenaRw     4        / 0.00 [1,1] [0, 4)\r\nTensor   1 dense_2/bias              kTfLiteFloat32  kTfLiteMmapRo      4        / 0.00 [1] [572, 576)\r\nTensor   2 sequential/dense/MatMul   kTfLiteFloat32  kTfLiteMmapRo      4        / 0.00 [1,1] [480, 484)\r\nTensor   3 sequential/dense_1/MatMul kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16,1] [336, 400)\r\nTensor   4 sequential/dense_2/MatMul kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [1,16] [192, 256)\r\nTensor   5 sequential/dense_1/Mat... kTfLiteFloat32  kTfLiteMmapRo      64       / 0.00 [16] [0, 64)\r\nTensor   6 sequential/dense/MatMul1  kTfLiteFloat32  kTfLiteArenaRw     4        / 0.00 [1,1] [-1, -1)\r\nTensor   7 sequential/dense/BiasA... kTfLiteFloat32  kTfLiteArenaRw     64       / 0.00 [1,16] [-1, -1)\r\nTensor   8 StatefulPartitionedCall:0 kTfLiteFloat32  kTfLiteArenaRw     4        / 0.00 [1,1] [64, 68)\r\nTensor   9 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  10 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  11 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  12 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  13 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  14 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  15 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  16 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  17 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  18 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  19 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  20 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  21 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  22 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  23 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  24 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  25 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\nTensor  26 (nil)                     kTfLiteNoType   kTfLiteMemNone     0        / 0.00 (null) [-1, -1)\r\n\r\nkTfLiteArenaRw Info: \r\nTensor 0 has the max size 4 bytes (0.000 MB).\r\nThis memory arena is estimated as[0x55e3cc5bb104, 0x55e3cc5bb0c0), taking 68 bytes (0.000 MB).\r\nOne possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:\r\nTensor 0 -> 8.\r\n\r\nkTfLiteArenaRwPersistent Info: not holding any allocation.\r\n\r\nkTfLiteMmapRo Info: \r\nTensor 3 has the max size 64 bytes (0.000 MB).\r\nThis memory arena is estimated as[0x7fb2de3f4648, 0x7fb2de3f4408), taking 576 bytes (0.001 MB).\r\nOne possible set of tensors that have non-overlapping memory spaces with each other, and they take up the whole arena:\r\nTensor 5 -> 4 -> 3 -> 2 -> 1.\r\n\r\nkTfLiteDynamic Info: not holding any allocation.\r\n\r\nNode   0 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 3)\r\n  3 Input Tensors:[0,2,-1] -> 0B (0.00MB)\r\n  1 Output Tensors:[6] -> 0B (0.00MB)\r\nNode   1 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 3)\r\n  3 Input Tensors:[6,3,5] -> 0B (0.00MB)\r\n  1 Output Tensors:[7] -> 0B (0.00MB)\r\nNode   2 Operator Builtin Code   9 FULLY_CONNECTED (delegated by node 3)\r\n  3 Input Tensors:[7,4,1] -> 0B (0.00MB)\r\n  1 Output Tensors:[8] -> 0B (0.00MB)\r\nNode   3 Operator Custom Name TfLiteXNNPackDelegate \r\n  6 Input Tensors:[0-5] -> 204B (0.00MB)\r\n  1 Output Tensors:[8] -> 4B (0.00MB)\r\n\r\nExecution plan as the list of 1 nodes invoked in-order: [3]\r\nAmong these nodes in the execution plan:\r\n  Node 3 is a TfLiteXNNPackDelegate node (0x55e3cc5ba4f0), which has delegated 3 nodes: [0-2]\r\n--------------Subgraph-0 dump has completed--------------\r\n\r\n```\r\n### 4. Question\r\nHow should I build the minimal.cc to use tflite models saved with SELECT_TF_OPS. The platform where I'll use my code is Linux. I created libtensorflow_flex.so using this [tutorial ](https://www.tensorflow.org/lite/guide/ops_select#cc), but now I don't know what should I do with created shared library. I am a complete newbie in the usage of tflite models with C++, so you have to assume that I don't know most of the well-known things :)\r\n\r\nWhat steps should I do to get minimal.exe which runs smoothly with the aforementioned models? ", "comments": ["Hi @gadagashwini ! Could you please look at this issue?", "Recently I tried to make some changes in **tensorflow/lite/examples/minimal/BUILD** file:\r\n\r\n```\r\n# Description:\r\n#   TensorFlow Lite minimal example.\r\n\r\nload(\"//tensorflow/lite:build_def.bzl\", \"tflite_linkopts\")\r\n\r\npackage(\r\n    default_visibility = [\"//visibility:public\"],\r\n    licenses = [\"notice\"],\r\n)\r\n\r\ncc_binary(\r\n    name = \"minimal\",\r\n    srcs = [\r\n        \"minimal.cc\",\r\n    ],\r\n    linkopts = tflite_linkopts() + select({\r\n        \"//conditions:default\": [],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/lite:framework\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\", # <============================\r\n    ],\r\n)\r\n```\r\n\r\nAnd build the minimal with Bazel using this command:\r\n\r\n`user1@ubuntu:~/tensorflow_src$ bazel build //tensorflow/lite/examples/minimal:minimal`\r\n\r\nThe build process took some time, but at the end when the linking started the process fails without useful logs:\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/lite/examples/minimal:minimal (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\n[1 / 2] Linking tensorflow/lite/examples/minimal/minimal; 137s local\r\n```\r\n```\r\nINFO: Found 1 target...\r\nERROR: /home/user1/tensorflow_src/tensorflow/lite/examples/minimal/BUILD:11:10: Linking tensorflow/lite/examples/minimal/minimal failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/examples/minimal/minimal-2.params\r\nstderr (/home/user1/.cache/bazel/_bazel_user1/83d974b1842d484c79357d882b1fd6b9/execroot/org_tensorflow/bazel-out/_tmp/actions/stderr-2) exceeds maximum size of --experimental_ui_max_stdouterr_bytes=1048576 bytes; skipping\r\nTarget //tensorflow/lite/examples/minimal:minimal failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 137.917s, Critical Path: 137.31s\r\nINFO: 2 processes: 2 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nWhat should I do in this situation? \r\n\r\n"]}, {"number": 55643, "title": "StatelessRandomBinomial op's  first input (shape) does not support int64", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n```\r\nimport tensorflow as tf\r\nshape = tf.constant([1, 1, 1], dtype=tf.int64)\r\nseed = [52, 35]\r\ncounts = [10.1]\r\nprobs = [0.1]\r\n\r\ndata = tf.raw_ops.StatelessRandomBinomial(\r\n    shape = shape, seed=seed, counts = counts, probs=probs, dtype=tf.float32\r\n)\r\nprint(data)\r\n```\r\n- TensorFlow version (use command below):\r\n 2.6.3\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\ntensorflow/core/framework/tensor.cc:708] Check failed: dtype() == expected_dtype (9 vs. 3) int32 expected, got int64\r\nAborted\r\n\r\nI think this bug caused by the following code which are only work for int32\r\n\r\nfor (int64 i = 0; i < num_sample_dims; ++i) {\r\n      samples_per_batch *= shape_tensor.flat<**int32**>()(i);\r\n    }\r\n```\r\n\r\n", "comments": ["Hello @xuanhuo ,\r\nI have tried to execute the given code with another approach.Can you please find the [gist](https://colab.research.google.com/gist/tilakrayal/325755e4c75f32d2efde1c83714d7bbb/55643.ipynb) and let us know if it helps.Thanks!"]}, {"number": 55642, "title": "tensorflow installed but cannot be imported ", "body": "**System information**\r\n- windows 11\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.8.0\r\n- Python version:3.10\r\n- Installed using: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe installation of tensorflow using the pip3 install --upgrade  tensorflow command was successful but when I test the command import tensorflow as tf in jupyter notebook , I get the following result:\r\n\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nC:\\Users\\VIDYAS~1\\AppData\\Local\\Temp/ipykernel_8636/4294963926.py in <module>\r\n----> 1 import tensorflow\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nSteps :\r\npip install --user virtualenv\r\npip install --upgrade  tensorflow\r\n\r\n\r\n", "comments": ["Hello @Vidyasagar1099 ,\r\nYou might be facing this issue because of the following reasons.I request you to check the steps.\r\n1. I suspect your cpu model does not support AVX instructions sets.I request you to please check [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).\r\n2. Please make sure to download the latest [microsoft visual c++ redistributable](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads) from here.\r\n3. Please check Your CPU/Python is on 32 bits?Can you please refer the thread [1](https://github.com/tensorflow/tensorflow/issues/42367),[2](https://github.com/tensorflow/tensorflow/issues/45435).\r\n\r\nAlso can you please follow the instructions to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows) with compatible build [configurations](https://www.tensorflow.org/install/source_windows#cpu).It helps.Thanks!\r\n", " I am trying to install tensorflow in Windows 7 ultimate \r\nPython --verion return \r\nPython 3.7.0\r\n\r\nTensorflow version is 0.12.0\r\n\r\nI dont have Anaconda or conda environment installed on PC.\r\n\r\nProject requirements prevent me going to colab/kaggle enviroments\r\n\r\n\r\nI have path to all the requesite dll's in the PATH env variable\r\n\r\nPATH=C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;F:\\app\\nakul\\product\\11.2.0\\dbhome_1\\bin;\r\nC:\\ProgramData\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;\r\nC:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Java\\jdk1.6.0_18\\bin;\r\nC:\\Program Files (x86)\\nodejs\\;C:\\Program Files\\TortoiseHg\\;\r\nc:\\Program Files\\Mozilla Firefox;\r\nC:\\Program Files\\Mozilla Firefox;C:\\Program Files (x86)\\Java\\jdk1.8.0_45\\bin\r\n;H:\\Softwares\\apache-maven-3.8.1-bin\\apache-maven-3.8.1\\bin;\r\nC:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;\r\nC:\\Windows\\system;\r\nC:\\Program Files\\Mozilla Firefox;\r\nH:\\Python Installed\\Scripts\\;\r\nH:\\Python Installed\\;\r\nC:\\Users\\nakul\\AppData\\Roaming\\npm;\r\nI:\\MikiTextInstall\\miktex\\bin\\x64\\;\r\nC:\\Users\\nakul\\AppData\\Local\\Programs\\MiKTeX 2.9\\miktex\\bin\\x64\\;\r\nC:\\Program Files\\JetBrains\\IntelliJ IDEA 2019.2.4\\bin;;\r\nC:\\Program Files\\Mozilla Firefox;F:\\MS\\Microsoft VS Code\\bin;\r\nH:\\Softwares\\apache-maven-3.8.1-bin\\apache-maven-3.8.1\\bin;\r\nF:\\data sciece\\vivek;C:\\Users\\nakul\\AppData\\Local\\Pandoc\\;\r\nH:\\Python Installed\\Lib\\site-packages\\graphviz;search-ms:displayname=Search%20Results%20in%20Python%20Installed&crumb=location:H%3A%5CPython%20Installed\\graphviz-0.19.1.dist-info;C:\\Windows\\SysWOW64\\;C:\\Windows\\System32;C:\\Windows\\winsxs\\x86_microsoft-windows-u..rsalcrt-apifwd-win7_31bf3856ad364e35_6.1.7601.23303_none_4e5c9e3785a6634a;\r\nH:\\Python Installed;\r\n\r\nupon executing import tensorflow as tf i get following error:\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in swig_import_helper()\r\n     17         try:\r\n---> 18             fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n     19         except ImportError:\r\n\r\nh:\\python installed\\lib\\imp.py in find_module(name, path)\r\n    296     else:\r\n--> 297         raise ImportError(_ERR_MSG.format(name), name=name)\r\n    298 \r\n\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     53     # use `dlopen()` for dynamic loading.\r\n---> 54     from tensorflow.python import pywrap_tensorflow\r\n     55 except ImportError:\r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             import _pywrap_tensorflow\r\n     21             return _pywrap_tensorflow\r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-4234dfe0332e> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 from tensorflow.keras.utils import to_categorical\r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\nh:\\python installed\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     58 please exit the tensorflow source tree, and relaunch your python interpreter\r\n     59 from there.\"\"\" % traceback.format_exc()\r\n---> 60   raise ImportError(msg)\r\n     61 \r\n     62 # Protocol buffers\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"h:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"h:\\python installed\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"h:\\python installed\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"h:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"h:\\python installed\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\nAny leads on how to solve this problem? \r\nI have tried solutions mentioned at following urls:-\r\n\r\n1. https://github.com/tensorflow/tensorflow/issues/48267\r\n2. https://github.com/tensorflow/tensorflow/issues/8385\r\n3. https://github.com/tensorflow/tensorflow/issues/20690 (Reported the same issue there as well)\r\n\r\nAny leads on how to solve this problem? \r\n\r\n"]}, {"number": 55641, "title": "Separate SelectTrueCoords for the WHILE op.", "body": "Allows implementation of the TFLM WHILE op.", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55641/checks?check_run_id=6045241063)."]}, {"number": 55639, "title": "`tf.vectorized_maps` resolve fallbacks", "body": "Mirroring https://github.com/keras-team/keras-cv/issues/291 for the TF core component.\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\nmaster\r\n- Are you willing to contribute it (Yes/No):\r\nI don't know. If I have a clear contribution path/pin-pointer probably.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMissing converters PFOR and eventually mitigate other fallbacks\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nPerformance on fallback\r\n\r\n**Any Other info.**\r\n\r\n/cc @wangpengmit \r\n", "comments": ["On the TF side I add also https://github.com/keras-team/keras-cv/issues/258#issuecomment-1095117489 as it will be the next one."]}, {"number": 55635, "title": "\u4f7f\u7528cmake \u53d8\u5f02tensorflow c  \u5e93     \u5931\u8d25  cmake\u7248\u672c\u4e3a2.23.1    tensorflow \u4e3a 2.4.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @zdm-linux ! Could you please update the template with steps to replicate and error stack trace to help expedite the issue?"]}, {"number": 55634, "title": "add tensorrt 8.4 support", "body": "nvidia Jetson JetPack 5.0 includes TensorRT 8.4 [1], which is not\r\nsupported now. Use TensorRT 8.2 interface for now.\r\n\r\n[1] https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#rel-8-4-0-EA", "comments": ["@jhalakp-nvidia  @MattConley please review and test \r\nCC: @bixia1 "]}, {"number": 55633, "title": "[XLA:GPU] Failed to determine best cudnn convolution algorithm", "body": "I got `Failed to determine best cudnn convolution algorithm`  error when running `facebook/wav2vec2-base-960h` model using torch_xla on GPU in fp16 mode. This error only occurs when using fp16 and fp32 works fine.\r\n\r\nMinimal HLO to reproduce:\r\n```llvm\r\nHloModule Test\r\n\r\nENTRY main {\r\n  x = f16[44,768,1,49]{3,2,1,0} parameter(0)\r\n  y = f16[44,768,1,50]{3,2,1,0} parameter(1)\r\n  ROOT %convolution.10022 = f16[1,128,48,768]{3,2,1,0} convolution(f16[44,768,1,49]{3,2,1,0} %x, f16[44,768,1,50]{3,2,1,0} %y), window={size=1x50 pad=0_0x64_64}, dim_labels=fb01_io01->01bf, batch_group_count=16\r\n}\r\n```\r\n\r\nThe following is the full log file.\r\n```\r\n(base) ubuntu@xla-p3-8x:~/src/tensorflow/xla_benchmark$ ../bazel-bin/tensorflow/compiler/xla/tools/replay_computation_gpu --use_fake_data=true --num_runs=1 --print_result=false conv.hlo \r\n2022-04-15 00:10:23.507808: I tensorflow/compiler/xla/service/platform_util.cc:69] platform Host present but no XLA compiler available: could not find registered compiler for platform Host -- check target linkage (hint: try adding tensorflow/compiler/jit:xla_cpu_jit as a dependency)\r\n2022-04-15 00:10:24.852458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.935392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.944338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.967499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.983119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.002219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.025011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.050733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.052806: I tensorflow/compiler/xla/service/service.cc:174] XLA service 0x562e5c814b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2022-04-15 00:10:25.052832: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052839: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052844: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052849: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052854: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (4): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052859: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (5): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052863: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (6): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052868: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (7): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\nconv.hlo: is not HloSnapshot. Trying HloProto.\r\nconv.hlo: is not HloProto. Trying HLO text.\r\n2022-04-15 00:10:25.053972: I tensorflow/compiler/xla/tools/replay_computation.cc:470] Compiling 1 modules in parallel.\r\n2022-04-15 00:10:25.622602: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\r\n2022-04-15 00:10:26.108045: I tensorflow/compiler/xla/tools/replay_computation.cc:487] Done compiling; now running the modules.\r\n2022-04-15 00:10:26.108867: E tensorflow/compiler/xla/tools/replay_computation.cc:491] Compilation failed: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\r\n%cudnn-conv = (f16[1,128,48,768]{3,1,0,2}, u8[0]{0}) custom-call(f16[704,48,1,49]{0,3,2,1} %bitcast.2, f16[48,768,1,50]{0,3,2,1} %pad), window={size=1x50 pad=0_0x64_64}, dim_labels=fb01_io01->01bf, feature_group_count=16, custom_call_target=\"__cudnn$convForward\", backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\r\n\r\nOriginal error: UNKNOWN: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(3520): 'op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed\r\n\r\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.: hlo { hlo_module { name: \"Test\" entry_computation_name: \"main\" computations { name: \"main\" instructions { name: \"x\" opcode: \"parameter\" shape { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } frontend_attributes { } } instructions { name: \"y\" opcode: \"parameter\" shape { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } parameter_number: 1 id: 1 frontend_attributes { } } instructions { name: \"convolution.10022\" opcode: \"convolution\" shape { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } window { dimensions { size: 1 stride: 1 window_dilation: 1 base_dilation: 1 } dimensions { size: 50 stride: 1 padding_low: 64 padding_high: 64 window_dilation: 1 base_dilation: 1 } } convolution_dimension_numbers { kernel_output_feature_dimension: 1 kernel_spatial_dimensions: 2 kernel_spatial_dimensions: 3 input_batch_dimension: 1 output_batch_dimension: 2 output_feature_dimension: 3 input_spatial_dimensions: 2 input_spatial_dimensions: 3 output_spatial_dimensions: 0 output_spatial_dimensions: 1 } id: 2 operand_ids: 0 operand_ids: 1 feature_group_count: 1 precision_config { operand_precision: DEFAULT operand_precision: DEFAULT } batch_group_count: 16 frontend_attributes { } } program_shape { parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } result { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameter_names: \"x\" parameter_names: \"y\" } id: 2 root_id: 2 } host_program_shape { parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } result { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameter_names: \"p0\" parameter_names: \"p1\" } entry_computation_id: 2 input_output_alias { } dynamic_parameter_binding { } } }\r\n```\r\n\r\nTested with tensorflow commit 75861c43005523e2552bb3f85b2f0defc16ea9cf, CUDA 11.4, CUDNN 8.2.", "comments": []}, {"number": 55628, "title": "Support GPU fusion of matmal+bias+(activation)", "body": "This PR enables the remapping optimization for fusion patterns of matmal+bias and matmul+bias+relu on GPUs.\r\n\r\nThis optimization needs to be turned on by `TF_USE_CUBLASLT`.\r\n\r\ncc. @nluehr ", "comments": []}, {"number": 55627, "title": "`tf.io.write_graph` fails to stip debug nodes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS Monterey 12.2.1 (m1)`\r\n- TensorFlow installed from (source or binary):  source, following https://developer.apple.com/metal/tensorflow-plugin/\r\n- TensorFlow version (use command below): \r\n```\r\ntensorflow-deps==2.8.0\r\ntensorflow-macos==2.8.0\r\ntensorflow-metal==0.4.0  \r\n```\r\n(the provided command returns `unknown 2.8.0`)\r\n- Python version: `3.9`\r\n\r\n**Describe the current behavior**\r\nDespite having set:`tf.config.optimizer.set_experimental_options({'debug_stripper': True})`, the `.pbtxt` record of a simple `tf.function` graph, generated by `tf.io.write_graph` still contains `assert_shape` nodes.\r\n\r\n***Minimal example***:\r\n```python\r\nimport tensorflow as tf\r\ntf.config.optimizer.set_experimental_options({'debug_stripper': True})\r\n\r\n@tf.function(\r\n    input_signature=[\r\n        tf.TensorSpec(\r\n            shape=(2, 2),\r\n            dtype=tf.int32,\r\n            name=\"Input\",\r\n        ),\r\n    ]\r\n)\r\ndef fn(x):\r\n    tf.debugging.assert_shapes(\r\n        [\r\n            (x, (2, 2))\r\n        ]\r\n    )\r\n    return x\r\n\r\ntf.io.write_graph(\r\n    fn.get_concrete_function().graph,\r\n    logdir=\".\",\r\n    name=\"example.pbtxt\",\r\n    as_text=True,\r\n)\r\n```\r\n\r\n**Describe the expected behavior**\r\n`assert_shape` nodes should not be present in the final graph.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab notebook: https://colab.research.google.com/drive/1l_P345AjMiHsLUTqG5io2Fjyag-1qIAM?usp=sharing\r\n\r\n**Other info / logs**\r\nResulting `.pbtxt` file: \r\n```pbtxt\r\nnode {\r\n  name: \"Input\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"_user_specified_name\"\r\n    value {\r\n      s: \"Input\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n        dim {\r\n          size: 2\r\n        }\r\n        dim {\r\n          size: 2\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"assert_shapes/Shape\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_INT32\r\n        tensor_shape {\r\n          dim {\r\n            size: 2\r\n          }\r\n        }\r\n        tensor_content: \"\\002\\000\\000\\000\\002\\000\\000\\000\"\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"assert_shapes/assert_rank/rank\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_INT32\r\n        tensor_shape {\r\n        }\r\n        int_val: 2\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"assert_shapes/assert_rank/Shape\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_INT32\r\n        tensor_shape {\r\n          dim {\r\n            size: 2\r\n          }\r\n        }\r\n        tensor_content: \"\\002\\000\\000\\000\\002\\000\\000\\000\"\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"assert_shapes/assert_rank/assert_type/statically_determined_correct_type\"\r\n  op: \"NoOp\"\r\n}\r\nnode {\r\n  name: \"assert_shapes/assert_rank/static_checks_determined_all_ok\"\r\n  op: \"NoOp\"\r\n}\r\nnode {\r\n  name: \"group_deps\"\r\n  op: \"NoOp\"\r\n  input: \"^assert_shapes/assert_rank/static_checks_determined_all_ok\"\r\n}\r\nnode {\r\n  name: \"Identity\"\r\n  op: \"Identity\"\r\n  input: \"Input\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n}\r\nversions {\r\n  producer: 987\r\n}\r\n```\r\n", "comments": ["@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.7, v2.8 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/7e0af283303d49f7138ecc3fdea2b139/55627.ipynb)."]}]