[{"number": 396, "title": "Cifar10 example bug (batch #5 not loading)", "body": "In _tensorflow/models/image/cifar10/cifar10.py_ it says:\n\n```\nfilenames = [os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin',\n                        'data_batch_%d.bin' % i)\n           for i in xrange(1, 5)]\n```\n\nwhich will only load batches 1 to 4, while batch number 5 is ignored entirely. This would be correct imo:\n\n```\nfor i in xrange(1, 6)]\n```\n", "comments": ["Fixed in next push.\n"]}, {"number": 395, "title": "The `external` directory missing when building from source", "body": "Trying to run Tensorboard, and the complains with about several things missing in the tensorflow/external directory. This entire directory is not the at all, and it's not a part of any submodule (the only submodule included is protobuf).\n", "comments": ["If you build from source, you need to use bazel to build TensorBoard, e.g. `bazel build //tensorflow/tensorboard:tensorboard` and then `./bazel-bin/tensorflow/tensorboard --logdir=foo`. Or, for convenience, `bazel run //tensorflow/tensorboard:tensorboard -- --logdir=foo`. It's bazel that takes responsibility for finding TensorBoard's external dependencies and setting them up properly.\n\nIf you want to be able to run TensorBoard just using the python command, and you also want to build from source, you can build the pip package (tensorflow/tools/pip_package:build_pip_package) and then if you install from that pip package you'll have a totally self-contained TensorBoard.\n\nGoing to leave this open, please close it if this fixes the issue for you (also, maybe I should update the README with this info...)\n", "Hi @danmane. Thanks, this is exactly what I was missing. It wasn't apparent from the README unfortunately.\n"]}, {"number": 394, "title": "failed call to cuInit: CUDA_ERROR_UNKNOWN in python programs using Ubuntu bumblebee", "body": "I have a Quadro K1100M integrated gpu with compute capability 3.0. I had to install bumblebee to make CUDA work. I am now able to run the tutorials_example_trainer with the command `sudo optirun bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu`. I have been able to do that with `TF_UNOFFICIAL_SETTING=1 ./configure`. However, I am not able to run examples in python directly.\n\nFor example, if I run the convolutional.py in tensorflow/models/image/mnist with the command `optirun python convolutional.py`, I get the following error : \n\n```\ntensorflow/tensorflow/models/image/mnist$ optirun python convolutional.py \nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nE tensorflow/stream_executor/cuda/cuda_driver.cc:466] failed call to cuInit: CUDA_ERROR_UNKNOWN\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:98] retrieving CUDA diagnostic information for host: jp-pc\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:106] hostname: jp-pc\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:131] libcuda reported version is: 352.63\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:242] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:135] kernel reported version is: 352.63\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:211] kernel version seems to match DSO: 352.63\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: \nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\n```\n\nIt is like my gpu is not recognized in python programs because of the 3.0 compute capability.  Is there a way to avoid this problem?\n", "comments": ["Updated subject to reflect the environment you're trying to run in.  Hopefully someone in the community who knows more about bumblebee/optimus laptops might be able to help!\n", "Just to be clear, I am able to run any programs with Bazel build. However, when using simple python programs where the tensorflow library is imported, the gpu will not work and I am stuck with using only the cpus.\n", "@vrv: Assigning you since it doesn't let me assign zheng-xq.  Do you know why?\n", "@girving: as discussed offline, fixing that.\n\nYeah, CUDA_ERROR_UNKNOWN is not very helpful.  hopefully @zheng-xq might know more what's going on here\n", "@jpmerc, could you run your command line through sudo, similar to you C++ examples? I wonder whether it is the root access that is making the difference. The initialization logic should be the same between C++ and Python clients. \n", "It doesn't seem to find the cuda library when in sudo :  \n\n```\n$ sudo optirun python convolutional.py \n\nTraceback (most recent call last):\n  File \"convolutional.py\", line 30, in <module>\n    import tensorflow.python.platform\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.python.client.client_lib import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/client_lib.py\", line 54, in <module>\n    from tensorflow.python.client.session import InteractiveSession\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 31, in <module>\n    from tensorflow.python import pywrap_tensorflow as tf_session\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory\n```\n", "Interesting. Could you add the path to your Cuda 7.0 runtime to LD_LIBRARY_PATH? \n", "It is already in LD_LIBRARY_PATH.\n\nIn LD_LIBRARY_PATH I have : \n/usr/local/cuda-7.0/lib64\n/usr/local/cuda-7.0/lib\n\nThe library the program is looking for is there :\n/usr/local/cuda-7.0/lib/libcudart.so.7.0\n/usr/local/cuda-7.0/lib64/libcudart.so.7.0\n", "Did someone find an answer to this?\nWhen running word2vec_basic.py I get\n\n> I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\n> E tensorflow/stream_executor/cuda/cuda_driver.cc:466] failed call to cuInit:  CUDA_ERROR_UNKNOWN\n> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:98] retrieving CUDA diagnostic information for host: peter-linux\n> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:106] hostname: peter-linux\n> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:131] libcuda reported version is: 352.63\n> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:242] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\n> GCC version:  gcc version 5.2.1 20151010 (Ubuntu 5.2.1-22ubuntu2) \n> \"\"\"\n> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:135] kernel reported version is: 352.63\n> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:211] kernel version seems to match DSO: 352.63\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: \n> I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\n\nIt runs, but probably without GPU except for the end when it complains:\nPlease install sklearn and matplotlib to visualize embeddings.\nUnfortunately sklearn and matplotlib are installed.\nThat being said, I see matplotlib fails selftest. numpy version 1.10.2\n", "Could you run `nvidia-debugdump -l` or `nvidia-smi` and paste the output? I had a similar problem and in the end it was a lack of power for the graphics card. \n", "Found 1 NVIDIA devices\n        Device ID:              0\n        Device name:            GeForce GTX TITAN X   (*PrimaryCard)\n        GPU internal ID:        0420115018258\n\nTue Dec 15 23:56:17 2015  \n+------------------------------------------------------+  \n| NVIDIA-SMI 352.63     Driver Version: 352.63         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:04:00.0      On |                  N/A |\n| 22%   33C    P8    17W / 250W |    441MiB / 12287MiB |      1%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1270    G   /usr/bin/X                                     174MiB |\n|    0      2183    G   compiz                                         112MiB |\n|    0      2575    G   ...ves-passed-by-fd --v8-snapshot-passed-by-   127MiB |\n+-----------------------------------------------------------------------------+\n", "Well, it was worth a shot.\n\nOn Tue, Dec 15, 2015 at 3:28 PM PeterBeukelman notifications@github.com\nwrote:\n\n> Found 1 NVIDIA devices\n> Device ID: 0\n> Device name: GeForce GTX TITAN X (*PrimaryCard)\n> GPU internal ID: 0420115018258\n> \n> Tue Dec 15 23:56:17 2015\n> \n> +------------------------------------------------------+\n> \n> | NVIDIA-SMI 352.63 Driver Version: 352.63 |\n> \n> |-------------------------------+----------------------+----------------------+\n> | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\n> | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\n> \n> |===============================+======================+======================|\n> | 0 GeForce GTX TIT... Off | 0000:04:00.0 On | N/A |\n> | 22% 33C P8 17W / 250W | 441MiB / 12287MiB | 1% Default |\n> \n> +-------------------------------+----------------------+----------------------+\n> \n> +-----------------------------------------------------------------------------+\n> | Processes: GPU Memory |\n> | GPU PID Type Process name Usage |\n> \n> |=============================================================================|\n> | 0 1270 G /usr/bin/X 174MiB |\n> | 0 2183 G compiz 112MiB |\n> | 0 2575 G ...ves-passed-by-fd --v8-snapshot-passed-by- 127MiB |\n> \n> +-----------------------------------------------------------------------------+\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/394#issuecomment-164926222\n> .\n", "@jpmerc, could you try to set LD_LIBRARY_PATH inside your sudo? That should make sudo preserve the environment variables. \n\nsudo LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.0/lib64 optirun python convolutional.py \n\n@PeterBeukelman, to make sure you have the same problem, could you run the C++ tutorial? \n\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n", "I think the basic mnist worked with GPU early on. But I ran into problem with word2vec visualization. Somewhere down the search to fix that, I noticed 2 different driver versions being mentioned 346 and 352. This made me think I mistakenly updated the driver and tried to revert to 346. I purged everything but after install still ended up with 352.\nI also had the wrong cuda 7.5 installed initially. I have made a link from /usr/local/cuda-7.0 to /usr/local/cuda\nLD_LIBRARY_PATH\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib/python2.7/dist-packages/tensorflow/python/:\nFor bazel build, I had the problem \n<INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_cos.cu.cc:\n<In file included from third_party/gpus/cuda/include/cuda_runtime.h:62:0,\n<                 from <command-line>:0:\n<third_party/gpus/cuda/include/host_config.h:105:2: error: #error -- unsupported GNU version! gcc <4.10 and up are not supported!\n< #error -- unsupported GNU version! gcc 4.10 and up are not supported!\n < ^\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: output <'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cwise_op_gpu_cos.cu.o' was not created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: output <'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cwise_op_gpu_cos.cu.d' was not created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: not all outputs were created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: output <'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/constant_op_gpu.cu.o' was not created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: output <'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/constant_op_gpu.cu.d' was not created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: not all outputs were created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: output <'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cwise_op_gpu_ceil.cu.o' was not created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: output <'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cwise_op_gpu_ceil.cu.d' was not created.\n<ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:248:1: not all outputs were created.\n\nI had this before also and dug up some old version of gcc (3.4) if I use that version\n< gcc: unrecognized option `-no-canonical-prefixes'\n< gcc: .: linker input file unused because linking not done\n< gcc: bazel-out/local_linux-opt/genfiles: linker input file unused because linking not done\n< cc1plus: error: unrecognized command line option \"-iquote\"\n< cc1plus: error: unrecognized command line option \"-iquote\"\n< cc1plus: error: unrecognized command line option \"-fstack-protector\"\n< cc1plus: error: unrecognized command line option \"-Wno-free-nonheap-object\"\n< cc1plus: error: unrecognized command line option \"-Wno-builtin-macro-redefined\"\n< cc1plus: error: unrecognized command line option \"-std=c++11\"\n< cc1plus: .: No such file or directory\n< ERROR: /home/peter/tensorflow_sources/tensorflow/core/BUILD:115:1: C++ compilation of rule '//tensorflow/core:direct_session' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 59 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n< Target //tensorflow/cc:tutorials_example_trainer failed to build\n\nI recollect I made that passed before though. It ended with copying the binary.\nAlso had a problem that something was looking for c++ and I had to make a link to gcc\n", "I changed to csh from bash and tried again to build bazel with gcc3.4.6\nThis is what I see with verbose_failures\n[tensorflow] % sudo  bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/any.pb.cc [for host]:\ngcc: bazel-out/host/genfiles: No such file or directory\ngcc: unrecognized option `-no-canonical-prefixes'\ngcc: .: linker input file unused because linking not done\ncc1plus: error: unrecognized command line option \"-iquote\"\ncc1plus: error: unrecognized command line option \"-iquote\"\ncc1plus: error: unrecognized command line option \"-fstack-protector\"\ncc1plus: error: unrecognized command line option \"-Wno-free-nonheap-object\"\ncc1plus: error: unrecognized command line option \"-Wno-error=unused-function\"\ncc1plus: error: unrecognized command line option \"-Wno-builtin-macro-redefined\"\ncc1plus: error: unrecognized command line option \"-std=c++11\"\ncc1plus: .: No such file or directory\nERROR: /home/peter/tensorflow/lib/python2.7/site-packages/tensorflow/google/protobuf/BUILD:63:1: C++ compilation of rule '//google/protobuf:protobuf' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/0d235aa1ad00a592d6c87ed1f69bc69b/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -iquote . -iquote bazel-out/host/genfiles -isystem google/protobuf/src -isystem bazel-out/host/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/any.pb.o' -MD -MF bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/any.pb.d -c google/protobuf/src/google/protobuf/any.pb.cc -o bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/any.pb.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/0d235aa1ad00a592d6c87ed1f69bc69b/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -iquote . -iquote bazel-out/host/genfiles -isystem google/protobuf/src -isystem bazel-out/host/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/any.pb.o' -MD -MF bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/any.pb.d -c google/protobuf/src/google/protobuf/any.pb.cc -o bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/any.pb.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 0.579s, Critical Path: 0.16s\n", "@zheng-xq It seems to work fine now, but it does not use my configuration (configured for Cuda 3.0 and it tries to use 3.5).\n\n```\ntensorflow/tensorflow/models/image/mnist$ sudo LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.0/lib64 optirun python convolutional.py\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 8\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:903] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: Quadro K1100M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.7055\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.97GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:669] Ignoring gpu device (device: 0, name: Quadro K1100M, pci bus id: 0000:01:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\nI tensorflow/core/common_runtime/direct_session.cc:60] Direct session inter op parallelism threads: 8\nInitialized!\nEpoch 0.00\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\n...\n```\n", "That is an old Cuda version. Am I correct in thinking that my issues with Bazel are independent of issues with importing tensorflow in python resulting in\n\"failed call to cuInit: CUDA_ERROR_UNKNOWN\"\n", "@jpmerc, could you confirm that you run TF_UNOFFICIAL_SETTING=1 ./configure before starting the build?\n\n@PeterBeukelman, I think they are most likely separate issues. Note that Bazel is best supported on Ubuntu 14.04 at the moment. The default gcc with that is 4.8. \n", "Had the same problem like @jpmerc, with Nvidia GTX 960m. And the problem was something connected with his: https://devtalk.nvidia.com/default/topic/907350/installing-cuda-7-0-but-get-cuda-7-5-/\nI just reinstalled 7.0 and everything worked fine.\n", "@martinwicke, @zheng-xq: Is this obsolete now that we support 7.5?\n", "I had the same problem, this fixed it: `sudo apt-get install nvidia-modprobe`\n", "It should be fixed. I'll close this for now -- we can reopen if it's still a problem.\n", "It's worth adding that \n`sudo apt-get install nvidia-modprobe`\nfixed it for me too even though I had already installed it on a previous session directly before installing Tensorflow.\n", "sudo apt-get install nvidia-modprobe, this is magic\n", "sudo apt-get install nvidia-modprobe, this fixed it for me too\n", "Same for me. Does anyone know _why_ this fixes it?\n", "No, I still can't fix with the nvidia-modprobe method. \n\n> $   python -m tensorflow.models.image.mnist.convolutional\n> /usr/bin/python: libcudart.so.7.5: cannot open shared object file: No such file or directory\n", "@immartian that looks like a different issue. Symlinks set up in /usr/local/cuda/lib64 set up correctly? Seems I've got that when I've had duplicated files instead of a symlink chain.\n", "Installing nvidia-modprobe worked for me too.\n", "@immartian I have the same problem as you. Have you fixed this problem? \n", "sudo apt-get install nvidia-modprobe fixed it for me too\n", "This is not working for me .... GPU is available and works until i put the computer to sleep/suspend after waking up the computer i always get the message below and GPU is unavailable when i run code (only CPU is available). \r\n\r\n`E tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_UNKNOWN`\r\n\r\nI am using nvidia-docker \r\n\r\n`nvidia-docker run -it -p 8888:8888 -v /*..../Data/docker:/docker --name TensorFlow   gcr.io/tensorflow/tensorflow:latest-gpu /bin/bash`\r\n\r\nand none of the solutions above work. Nvidia-smi and nvidia-debugdump -l both show the GPU is installed and driver is up to date. ", "I had met this error too when the computer wakes up after hibernation. Run 1_Utilities/deviceQuery example which will tell whether CUDA card is available or not.", "I ran into this issue recently. I upgraded my nvidia-driver to version 375.26 and docker to version Docker version 1.13.0. \r\nWhen training a network I would get the \r\n\r\n`cuInit: CUDA_ERROR_UNKNOWN`\r\n\r\nThe problem here is that cuda fails to initiate the 'shared GPU context'.\r\nFor some reason, the 'nvidia-cuda-mps-control' service is not active after the upgrade. I need to investigate more. \r\n\r\nHowever, try running `nvidia-cuda-mps-server` in the host machine. This solved it for me. \r\n", "I had the same issue.\r\n**Simply reboot the computer fixed the problem for me :)**\r\nSuggestion: do not suspend your computer (which caused the problem in my case)", "Is it necessary to reboot after installing(executing) 'sudo apt-get install nvidia-modprobe'?", "sudo apt-get install nvidia-modprobe works for me, with a restart.", "@LeeKyungMoon Reboot only works for me, without intalling `nvidia-modprobe` as @WeitaoVan said.", "sudo is fine for me with the same issue.", "nvidia-cuda-mps-server works for me", "In Ubuntu 17.04, `nvidia-cuda-mps-server` doesn't work, it doesn't even output anything when I run the command. I've installed `sudo apt-get install nvidia-384` and CUDA using `sudo apt-get install nvidia-cuda-toolkit`, and a simple test from [this link](https://medium.com/@at15/install-nvidia-cuda-on-ubuntu-17-04-823300ab7bcc) compiles successfully, printing the resulting array. I can run `optirun nvidia-smi`, which shows that an X server is running (probably due to the virtual display).\r\n\r\n`nvidia-modprobe` in 17.04 is still linked to `nvidia-375`, so doesn't work.\r\n\r\nMultiple reboots later, I still get this issue. \r\n\r\n`Acer Predator Helios 300, GTX 1060`", "# Are you still having trouble with this issue?\r\n\r\nThis is what worked for me.  I had previously tried an installation of CUDA using a .run file.  The installation had configured the nvidia-384 driver and this was precisely what I saw when I ran nvidia-smi.   I ran /usr/bin/nvidia-uninstall and the CUDA_ERROR_UNKNOWN went away.  Further, when I run nvidia-smi I see the expected driver version (375).  \r\n\r\nIn summary, make sure that previous installations of drivers/CUDA are not the source of the error if the suggestions in this thread don't work.   This is likely the case if one installation was done with a .run file while another was done via a .deb package. ", "Everything worked fine under `nvidia-384` drivers then suddenly broke with the `CUDA_ERROR_UNKNOWN`.\r\nWhat worked for me is the opposite to solutions above - removing `nvidia-modprobe`, probably because it's tied to `nvidia-375` as poster above mentioned.", "I am facing the \"CUDA_ERROR_UNKNOWN\" issue on Windows 2012 R2 server. Anybody tried it on windows server please help? ", "@LeeKyungMoon after installing nvidia-modprobe, reboot works for me.", "In my case, nvidia-modprobe was installed and the paths were correct. What solved was running the commands here https://devtalk.nvidia.com/default/topic/760872/ubuntu-12-04-error-cudagetdevicecount-returned-30/\r\n\r\nEspecially, running following:\r\n$ sudo modinfo nvidia-<driver_version_num>-uvm (with driver_version_num as 384 in my case)\r\n$ sudo modprobe --force-modversion nvidia-331-uvm\r\n\r\nHope this helps.", "Interestingly none of these worked for me. Adding following to .bashrc worked like charm!!\r\n\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:/usr/local/cuda-8.0/targets/x86_64-linux/lib/\"", "Same here @tharuniitk \r\n\r\nNone of these work.", "@prasad3130 Thanks a lot, that worked like a charm. Although it is worth noting that the command should be the following\r\nsudo modprobe --force-modversion nvidia-\\<nvidia-version\\>-uvm", "If you work in linux/ubuntu, check your kernel, by `find /lib/modules/ | grep -i nvidia`. Make sure the models exist 'nvidia-modeset.ko, dkms/nvidia-uvm.ko, dkms/nvidia-drm.ko, dkms/nvidia.ko' and /proc/driver/nvidia/ . My tensorflow-cuda works very well. But someday, it suddenly throw this error, when I switched to the old kernel which has the above modules, it works again. This error hanppens that ubuntu switched to the newest kernel siliently, but has less necessary modules.\r\nWish it helps", "try `sudo ldconfig` after installling cuda & cudnn.", "`sudo apt-get install nvidia-modprobe` worked for me without restart on 16.04", "`nvidia-cuda-mps-server` solved the problem for me after upgrading tensorflow to 1.7.0", "```\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Unable to locate package nvidia-modprobe\r\n\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 18.04.1 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n```", "I've had a problem running **darknet** detector on ubuntu in multiuser environment. To solve the issue, i've exported CUDA_CACHE_PATH variable as: `export CUDA_CACHE_PATH=/tmp/nvidia`\r\nbefore using GPU.", "### System information\r\n\r\n- **OS Platform and Distribution** : Ubuntu 14.04\r\n\r\n- **TensorFlow version** : 1.10 gpu\r\n\r\n- **Python version**: 2.7\r\n\r\n- **CUDA/cuDNN version**: 9.0 / 7\r\n\r\n- **GPU model and memory**: Nvidia GeForce GTX TITAN X \r\n\r\n- **`nvidia-smi`**:\r\n\r\n+------------------------------------------------------+                       \r\n| NVIDIA-SMI 352.63     Driver Version: 384.130        |                       \r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |\r\n| 22%   61C    P0    75W / 250W |      0MiB / 12206MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX TIT...  Off  | 0000:03:00.0     Off |                  N/A |\r\n| 23%   63C    P0    78W / 250W |      0MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX TIT...  Off  | 0000:83:00.0     Off |                  N/A |\r\n| 22%   60C    P0    77W / 250W |      0MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX TIT...  Off  | 0000:84:00.0     Off |                  N/A |\r\n| 22%   61C    P0    72W / 250W |      0MiB / 12207MiB |      0%      Default |\r\n+-------------------------------+----------------------+-----------------\r\n\r\n- **`find /lib/modules/ | grep -i nvidia `**\r\n\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/net/ethernet/nvidia\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/net/ethernet/nvidia/forcedeth.ko\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/video/fbdev/nvidia\r\n/lib/modules/4.2.0-27-generic/kernel/drivers/video/fbdev/nvidia/nvidiafb.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_uvm.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_modeset.ko\r\n/lib/modules/4.2.0-27-generic/updates/dkms/nvidia_384_drm.ko\r\n\r\n### Describe the problem\r\n\r\nI upgrade the nvidia driver througth command: **`sudo apt-get install nvidia-384`**.  Then i found there are serveal nvidia driver installed througth command: **`sudo dpkg --list | grep nvidia-*`**\uff0cso i uninstalled these driver except nvidia-384 use commadn: **`sudo apt-get remove xxx`**.  After that, the info as follows:\r\n\r\nii  nvidia-384                                            384.130-0ubuntu0.14.04.1                            amd64        NVIDIA binary driver - version 384.130\r\nii  nvidia-opencl-icd-384                                 384.130-0ubuntu0.14.04.1                            amd64        NVIDIA OpenCL ICD\r\nii  nvidia-prime                                          0.6.2.1                                             amd64        Tools to enable NVIDIA's Prime\r\nii  nvidia-settings                                       352.39-0ubuntu1                                     amd64        Tool for configuring the NVIDIA graphics driver\r\n\r\n**Error occured when i run tensorflow code as follows:**\r\n`2018-10-03 23:13:51.656015: E tensorflow/stream_executor/cuda/cuda_driver.cc:397   ] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\n2018-10-03 23:13:51.656131: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:163] retrieving CUDA diagnostic information for host: root0-SCW4350-16\r\n2018-10-03 23:13:51.656166: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:170] hostname: root0-SCW4350-16\r\n2018-10-03 23:13:51.656299: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:194] libcuda reported version is: 384.130.0\r\n2018-10-03 23:13:51.656428: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:198] kernel reported version is: 384.130.0\r\n2018-10-03 23:13:51.656465: I tensorflow/stream_executor/cuda/cuda_diagnostics.c   c:305] kernel version seems to match DSO: 384.130.0\r\n<tensorflow.python.client.session.Session object at 0x7f30a08ae5d0>`\r\n\r\ni have tried many solutions such as install `nvidia-modprobe` , `nvidia-cuda-mps-server` , `export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64:/usr/local/cuda-9.0/targets/x86_64-linux/lib/\"` all of these don't work. \r\n\r\nMaybe  It is noteworthy that **`nvidia-smi`** shows that `NVIDIA-SMI 352.63     Driver Version: 384.130` and **`sudo dpkg --list | grep nvidia-*`** shows that **`nvidia-settings   352.39-0ubuntu1 `**. It seems that some moulde of nvidia-352 are not uninstalled. And i tried to installed nvidia driver througth **`nvidia_xxxx.run`** file\uff0cbut the error remain while running tensorflow code.\r\n\r\nHopefully you can help me with this issue.\r\n", "in my case, the NVidia graphic driver accidently switch to another version that is not suitable. So just re-install the suitable version then it works again.", "What's the equivalent solution on Windows? ", "`\r\ndef ja():\r\n \r\n`"]}, {"number": 393, "title": "TensorBoard,No scalar summary tags were found", "body": "sess = tf.Session() \ninit = tf.initialize_all_variables()\nsess.run(init)\n\nsummary_writer = tf.train.SummaryWriter('./tmp/tensorflow_log', graph_def=sess.graph_def)\n# However after launching TensorBoard I can not see any grah def info..\n\nI also tried tf.training.summary_io.SummaryWriter, but it will not generate any log..\n\"\"\nNo scalar summary tags were found.\n\nMaybe data hasn't loaded yet, or maybe you need to add some tf.scalar_summary ops to your graph, and serialize them using the tf.training.summary_io.SummaryWriter.\n\"\"\"\n\nSeems there are people facing the same issue, on stackoverflow but not solved.\nhttp://stackoverflow.com/questions/33634764/tensorboard-doesnt-find-scalar-statistics/33652076#33652076\n", "comments": ["Have you tried following this tutorial? http://www.tensorflow.org/how_tos/summaries_and_tensorboard/index.html\n\nNote you need to create `scalar_summary` ops and run them, and write them to the SummaryWriter - just creating a SummaryWriter on its own is not enough.\n\nIn the invocation you showed there should be a graph definition written - just making sure, have you clicked the \"Graph\" tab in the top right corner? \n", "@danmane Thanks danmane, seems a problem for my firefox in my virtual machine running ubuntu. On windows from chrome I can see the graph, so not an issue, closing.\n", "@danmane I've been experiencing a similar problem. Is this because only the Chrome browser is compatible with Tensorboard?\n", "@chenghuige see my question above\n", "Which browser are you on? I think the graph visualizer is broken on FF at the moment. Does everything work if you're on chrome?\n", "@danmane I'm using Firefox on Ubuntu linux. The only thing I'm seeing is the 'Events' and 'Histograms'. No 'Images' nor 'Graph'. I don't think I have an option to install Chrome browser on Ubuntu, so I have no idea if it does work on Chrome.\n", "@jaelim I had similar problems until I realized that the summary writer did not actually write the file unless I called the summary writer's flush() function. Maybe explicitly calling the flush() function will sort it out for you too?\n\nI'm not sure, but it's probably because I had the summary writer's code snippet in a code part that did not close (and the code finished before the standard 120 seconds between each automatic flush).\n", "please solve this problem in video \n", "@esraahassan can you explain what you mean by \"please solve this problem in video\"?\n", "I met this issue and fixed it just now using the absolute directory path (/home/xxx/path2) instead of the relative path ( './path2' )."]}, {"number": 392, "title": "Mac virtualenv: TensorFlow Mechanics 101 failure", "body": "When I try to run the Mechanics 101 tutorial on a Mac using virtualenv, it fails with:\n\n$ python fully_connected_feed.py\nTraceback (most recent call last):\n  File \"fully_connected_feed.py\", line 31, in <module>\n    from tensorflow.g3doc.tutorials.mnist import input_data\nImportError: No module named g3doc.tutorials.mnist\n\nTo reproduce:\n$ virtualenv --system-site-packages ~/tensorflow\n$ source ~/tensorflow/bin/activate\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n$ python, running \"hello world\" test: works fine\n$ python -m tensorflow.models.image.mnist.convolutional: works fine\n\nFirst problem:\n$ python -c 'import site; print \"\\n\".join(site.getsitepackages())'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nAttributeError: 'module' object has no attribute 'getsitepackages'\n\nExpected behavior: should print path, according to instructions at http://www.tensorflow.org/get_started/os_setup.html#run-a-tensorflow-demo-model\nThis seems to be a problem with virtualenv: See https://github.com/pypa/virtualenv/issues/228\n\nSecond problem:\nDownload fully_connected_feed.py \n$ python fully_connected_feed.py\nTraceback (most recent call last):\n  File \"fully_connected_feed.py\", line 31, in <module>\n    from tensorflow.g3doc.tutorials.mnist import input_data\nImportError: No module named g3doc.tutorials.mnist\n\nExpected behavior: should run MNIST according to instructions at http://www.tensorflow.org/tutorials/mnist/tf/index.html#tutorial-files\n\nI suspect some sort of path problem, but due to the first problem I can't figure out the right path.\n\nSystem:\nMacBook Pro (Retina, 15-inch, Early 2013)\nOS X El Capitan, version 10.11.1 (15B42)\nPython 2.7.10 (default, Aug 22 2015, 20:33:39)\n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.1)] on darwin\n", "comments": ["This should be fixed in our 0.6.0 release.\n"]}, {"number": 391, "title": "Fatal error in TileOps: invalid combination", "body": "I have an RNN I adapted from the PTB example that works fine on CPU.  But when I try to run it on a GPU, it dies with:\n\nF tensorflow/core/kernels/tile_ops.cc:131] TileOp: Invalid combination of Device, DT and NDIM: N5Eigen9GpuDeviceE, float, 0\n\nI'm guessing this is complaining about a 0-dimensional tensor?  What's going on?\n", "comments": ["https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/tile_ops.cc#L383\n\nLooks like it :(\n\n@girving: is there a fix on the way or is this something more fundamental?\n", "Tile is a nop for scalars, so we just need to copy the input to the output.  Eigen can stay broken. :)\n", "Fix in review.\n", "Fixed.  @bndnn: Thank you for reporting!\n", "Thanks for the quick attention!  When can I expect the fix to hit github?  Or I'm happy to pull from googlesource.com if that'll happen faster.\n", "We push to github/googlesource at the same time -- I'll try to upstream our commits in a few hours.\n", "Awesome!  Thanks!\n"]}, {"number": 390, "title": "parse_example can be _much_ faster than parse_single_example", "body": "FYI I am working with Example protos for model input, and I am learning that use `tf.parse_example` to parse a (shuffled) batch of serialized examples is _much_ faster than using `tf.parse_single_example` prior to batching. For my particular dataset, using parse_single_example allows me to create `feed_dict`s with batch size 128 at about 100/min; batching the serialized Example protos and then using parse_example is running at around 3000/min.\n\nYou may want to update the [documentation](http://www.tensorflow.org/how_tos/reading_data/index.html#file-formats) to suggest using `tf.parse_example` everywhere, as is suggested when using [sparse input data](http://www.tensorflow.org/how_tos/reading_data/index.html#sparse-input-data).\n", "comments": ["Want to send us a PR to fix?\n", "I think parse_example is much faster than parse_single_example. in my case , speed change from 25000 sample/second to 32000 sample/second"]}, {"number": 389, "title": "Cifar-10 eval script verbose output", "body": "I've ran cifa10_train.py successfully. Now, I'm trying to run cifar10_eval.py, but the code's following verbose repeatedly without stopping. Any idea what this means?\n\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\n2015-12-01 08:17:18.602794: precision @ 1 = 0.855\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa024001740 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa03c001630 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa02c001630 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa0500018c0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa028001630 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa0480098d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa020001630 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa030001630 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa038001730 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa0540098d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa018001630 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa0400018e0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa0440098d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa05c055a30 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa04c0098d0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa060052d90 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/random_shuffle_queue_op.cc:282] Skipping cancelled enqueue attempt\nW tensorflow/core/common_runtime/executor.cc:1027] 0x7fa064001480 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n", "comments": ["Is this error produced reliably if you run cifar10_eval.py? If so, would you describe exactly how you produced this error as I am not seeing this error when I run the script? Thanks!\n", "I presume this error is not longer an issue so I will be marking this issue as closed but please feel free to reopen if this error persists.\n", "I get this issue - cifa10_train.py runs fine, but the above text appears when evaluating it.  I'm using a GTX960 with 2GB memory.\n", "I get this error too. Anyone has a fix for it? I tried cifar10_eval.py on both CUDA enabled TensorFlow and non-CUDA enabled TensorFlow. \n", "I also encounter this error during cifar10_eval.py. (I am using the pip installed gpu version). The installation was a little dated. \n\nHopefully this is not TMI:\n\nAfter a reinstall via pip, I started getting the conv1 weights uninitialized error during training.\nhttps://github.com/tensorflow/tensorflow/issues/642\n\nThink that perhaps the pip version is out of date, I decided to try to recompile from source.\nNext, I did `pip uninstall tensorflow` to reinistall from source. \nOn a fresh clone, `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer` terminated successfully with lots of repeated \"info\" outputs regarding Eigen::AlignedBit  being deprecated.\n\nFinally, I ran into the CUDA_ERROR_DEINITIALIZED segfault during `bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu`\nhttps://github.com/tensorflow/tensorflow/issues/723\n", "I get this error too. However I notice that this appears before everything else.\n\n2016-01-12 13:38:54.616463: precision @ 1 = 0.845\n\nCan we assume that eval_once() is successfully executed and use this result?\n", "Has someone found a solution to this? I'm having the same problem...\n", "I don't see this in behavior in the latest version (head)\n", "I am observing this exact error in a version compiled from current HEAD today. \n", "I realized that if I let the `cifar10_train.py` and `cifar10_eval.py` files keep running simultaneously, despite the `queue` errors, the `precision @ 1` is reported every 5 minutes. One can reduce this time interval by decreasing the value of `'eval_interval_secs'` in the `cifar10_eval.py` file to see more frequent updates.\n", "Is there any solution yet? Because I have met this problem too.\n", "Unfortunately, none that I am aware of.\n\nAta\n\nOn Thu, Apr 14, 2016 at 6:23 PM, yuanpengX notifications@github.com wrote:\n\n> Is there any solution yet? Because I have met this problem too.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/389#issuecomment-210235719\n", "@ebrevdo: Do you have any thoughts here?  Reassigning to you since Sherry is out. \n", "@yuanpengX @atamahjoubfar is this still an issue?  Is it just the verbose logging that's troubling you?  when you reach the end of the input file, the queue dies and the training is restarted from the beginning.  that may explain all the warnings.\n", "@ebrevdo I just tested again the CIFAR-10 code on the same machine, and the issue is resolved. Thank you.\n", "@atamahjoubfar Thanks!  Closing for now; please comment or file another if it comes up again.\n", "Issue does appear on 0.10rc0 for Mac (CPU) and Python 3 using a virtualenv installation.  First time seeing this issue, otherwise appears to be working correctly.\n"]}, {"number": 388, "title": "Rust API", "body": "TensorFlow should have a Rust interface.\n\nOriginal e-mail:\nI'd like to write Rust bindings for TensorFlow, and I had a few questions.  First of all, is anyone already working on this, and if so, can I lend a hand?  If not, is this something the TensorFlow team would be interested in?  I assume that the TensorFlow team would not be willing to commit right now to supporting Rust, so I thought a separate open source project (with the option to fold into the main project later) would be the way to go.\n", "comments": ["Moving email discussion here:\n\nRust bindings would be cool!  I don't think there's anyone working on rust bindings so far, though it might be worth an email to discuss@tensorflow.org to check.  Agreed that a separate project is probably good to start out.\n\nTo start the conversation, there are a couple different levels of bindings:\n1. Bindings that do not know about specific ops, but can create graphs \"manually\" or load them from GraphDefs, evaluate graphs (so that tensorflow models can be run inside rust servers), etc.  There's hopefully nothing blocking that.\n2. Autogenerated bindings for each C++ op.  We do this for Python, and similar things should work for Rust (ideally with a lot more type safety).  I have limited intuition for rust's handling of ad-hoc polymorphism, so I can't judge how easy it'll be to fit tensorflow's notions of polymorphism into rust's.  Again, there's hopefully nothing blocking this.\n3. Idiomatic bindings.  We currently have a lot of logic in pure Python, including both per-op sugar and key features such as automatic differentiation.  We will likely eventually move this to C++, but until then it will be difficult to capture this functionality in rust bindings without duplicating a ton of effort.  Unfortunately, automatic differentiation in particular is (mostly) necessary if you want to train models.\n\nIt's probably best to shoot for (1) to start and only move to (2) once it's clear how much of (3) will be a blocker for the desired applications.\n", "FWIW, getting something like 1) is pretty straightforward with [`rust-bindgen`](https://github.com/crabtw/rust-bindgen) and [`tensor_c_api.h`](https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/public/tensor_c_api.h), I put something together the other weekend in https://github.com/ajtulloch/tensorflow-rs/blob/master/src/tf.rs if that's useful.\n", "I would be glad to work on a Rust interface for TensorFlow if nobody is currently working on a solution.\n", "I just launched a project to provide the Rust bindings at https://github.com/google/tensorflow-rust.  The build glue is there, but most of the bindings have yet to be written.\n", "Nice!  I'm happy to help with any issues you run into.\n", "Cool!\n", "It seems that the Rust bindings are building on top of [c_api.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h) using the approach outlined in the [how-to](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/language_bindings/index.md), which is great.\n\nIf you run into concerns with the documentation or the C-API, please do create a separate issue\n\nThanks!\n", "Closing this issue. The Rust bindings have been moved to [github.com/tensorflow/rust](https://github.com/tensorflow/rust)\n"]}, {"number": 387, "title": "Any intentions of R interface?", "body": "Hi!\n\nIm working with the python wrapper for tensorflow. Is there any intentions of implement similar wrappers for the R languages?\n", "comments": ["There is some attempt, like https://github.com/terrytangyuan/rflow\n\nIt shouldn't be hard to have an R wrapper like the python one, but it will be quite time-consuming.\n", "Thanks! I looked it up and it still depends quite much on Python. \n\nWhy would it be time consuming, is TensorFlow built in Python or is mainly built in C++?\n", "@MansMeg For R interface, there are two possible solutions:\n\n(1) SWIG, used in shogun (https://github.com/shogun-toolbox/shogun)\n\n(2) Rcpp, used in mxnet (https://github.com/dmlc/mxnet/tree/master/R-package), which is also a deep learning platform\n\nAs I understand, the core part is in C++ and that's why I said it shouldn't be hard. I am part of Rcpp team, so I know it is quite easy to call C++ from R.\n\nBut there is much more work between calling tensorflow from R and building a useful interface, right?\n", "Of course, there is some effort in building an interface! I just think that it is a difference if the core also would be built in Python, then it would take much more time (and maintenance) to have an R interface - porting code and keeping it updated.\n\nThanks for the links!\n", "@thirdwing I'm also quite interested in R bindings. I think that mapping directly from the C++ API to R via either SWIG or Rcpp is a necessary but not sufficient condition to create really compelling R bindings. What we really need are idiomatic bindings that take advantage of R formulas, do automatic differentiation, etc. I think it's correct that it would take time and maintenance to do this properly but the result could be really stunning!\n\nBTW, another possible path is alluded to here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor_c_api.h#L55-L59\n\nIn short, it's suggested that some languages may choose to bypass the C++/SWIG route entirely and just generate the requisite ProtoBuf input to the C API. It's not clear to me whether this would be a wasteful duplication of the existing C++ layer or something that would actually simplify creating idiomatic R bindings. \n", "Thinking about automatic differentiation, it could be quite really interesting if we can use R formulas. I will look into this.\n", "@girving Interested in your guidance on the various ways to pursue R bindings. Read your comment here providing the lay of the land for Rust (https://github.com/tensorflow/tensorflow/issues/388#issuecomment-161019498). We obviously can pursue SWIG bindings to the existing C++ classes and then pickup additional functionality from the C++ layer once more of the features from the python layer are moved there.\n\nHowever, we're most interested in creating idiomatic bindings for R and to get this exactly right are in no way deterred by the fact that it may take a lot of effort (much of which is duplicated). I can think of a few ways to approach creating these bindings:\n1. Create a wrapper for sessions / invocation using the C API then create the graph definition bindings by going directly to proto. This possibility seems to be anticipated/encouraged here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor_c_api.h#L55-L59\n2. Call the C++ layer via SWIG bindings.\n3. Call python directly from R using e.g. boost::python or pybind11. Assuming that NumPy arrays can be marshaled to from R arrays with no copying I'm assuming this would have no performance problems since the python code is just defining a graph.\n\nTo me the first option has the greatest appeal because there would be no impedance problems associated with translating idiomatic R into graphs, we'd just figure out the right syntax and express it as a graph definition rather than contorting it through the C++ layer. The comment above seems to imply that this would be an effective path, but I haven't spent enough time looking to know whether this would be a monumental amount of work that would be constantly challenged to keep up or just something to grind through once (not deterred at all by having to spend on the order of hundreds of hours in the initial effort). I also like the idea of using the C API because those entry points could be loaded dynamically, meaning that the R bindings could be built separately and made available on CRAN and could work with any installed version of TensorFlow.\n\nWe could also combine the creation of proto-based idiomatic R bindings with another binding that parrots the python API. This would allow R users to easily translate and use python or C++ API based examples but at the same time have a first class R binding that makes maximum sense to R users.\n\nYour thoughts and feedback very much appreciated!\n", "A lot of the functionality of tensorflow is written in python, and will\nmigrate only slowly down to C++.\n\nThe GraphDef proto should only change slowly (and with the best guarantees\nfor compatibility), so writing protos directly will require the least\nvelocity of change to keep things working. If your main goal is to run\nexisting graphs from R (that's unlikely to be the case, but that's the\nsituation for node.js or Java where the main focus is on running\npre-trained models), then this would be sufficient.\n\nOn the other hand, the API will be significantly less powerful if you cut\naway the python layer entirely. Mainly this is because the gradients and\nshape inference layers are written in python, but also some ops are python\nfunctions that combine kernels into more complex ops (image_ops is a good\nexample). This makes #3 look much better than it otherwise would, even\nthough the API is changing faster than the GraphDef is.\n\nOn Sun, Dec 13, 2015 at 4:27 AM JJ Allaire notifications@github.com wrote:\n\n> @girving https://github.com/girving Interested in your guidance on the\n> various ways to pursue R bindings. Read your comment here providing the lay\n> of the land for Rust (#388 (comment)\n> https://github.com/tensorflow/tensorflow/issues/388#issuecomment-161019498).\n> We obviously can pursue SWIG bindings to the existing C++ classes and then\n> pickup additional functionality from the C++ layer once more of the\n> features from the python layer are moved there.\n> \n> However, we're most interested in creating idiomatic bindings for R and to\n> get this exactly right are in no way deterred by the fact that it may take\n> a lot of effort (much of which is duplicated). I can think of a few ways to\n> approach creating these bindings:\n> \n>    1.\n> \n>    Create a wrapper for sessions / invocation using the C API then create\n>    the graph definition bindings by going directly to proto. This possibility\n>    seems to be anticipated/encouraged here:\n>    https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor_c_api.h#L55-L59\n>    2.\n> \n>    Call the C++ layer via SWIG bindings.\n>    3.\n> \n>    Call python directly from R using e.g. boost::python or pybind11.\n>    Assuming that NumPy arrays can be marshaled to from R arrays with no\n>    copying I'm assuming this would have no performance problems since the\n>    python code is just defining a graph.\n> \n> To me the first option has the greatest appeal because there would be no\n> impedance problems associated with translating idiomatic R into graphs,\n> we'd just figure out the right syntax and express it as a graph definition\n> rather than contorting it through the C++ layer. The comment above seems to\n> imply that this would be an effective path, but I haven't spent enough time\n> looking to know whether this would be a monumental amount of work that\n> would be constantly challenged to keep up or just something to grind\n> through once (not deterred at all by having to spend on the order of\n> hundreds of hours in the initial effort). I also like the idea of using the\n> C API because those entry points could be loaded dynamically, meaning that\n> the R bindings could be built separately and made available on CRAN and\n> could work with any installed version of TensorFlow.\n> \n> We could also combine the creation of proto-based idiomatic R bindings\n> with another binding that parrots the python API. This would allow R users\n> to easily translate and use python or C++ API based examples but at the\n> same time have a first class R binding that makes maximum sense to R users.\n> \n> Your thoughts and feedback very much appreciated!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/387#issuecomment-164253513\n> .\n", "@martinwicke Thanks for the follow up. So it sounds like calling directly from R to python might be the best chance of providing a robust and complete R interface. We could first create a variation of the R interface that is entirely syntax compatible with the python interface then incrementally introduce various types of R sugar to make things more natural and compact for R users.\n\nIn order to stay in sync this would ideally take advantage of some machine readable specification of the python interface and documentation. What do you recommend as the best course here?\n", "We have the op registry, but it only contains the C++ api, as well as the functions for gradients and shape inference. The closest we have for the python layer is the API docs, which is generated by scraping the source code, and therefore should always be up to date. This quickly turns icky, but you could actually get a reasonable snapshot by going through the python doc .md files and collecting headlines.\n", "Has there been any progress? I had a few nudges from R users to throw my hat in the ring...\n", "I've got an R package that has a binding to the TensorFlow C-interface here: https://github.com/jjallaire/TensorFlow\n\nThe main trick of this package is that it doesn't actually build against the TF library (which will be impossible on CRAN unless CRAN decides to install TensorFlow on their build machines) but rather dynamically discovers it's location and loads up the symbols manually.  \n\nIn principle this is the first step of being able to run TF models defined as protobufs. However, as alluded to previously in this thread I think a more fruitful direction is going to be just calling all the Python code directly from within R. I'm not sure I'll have time to explore this soon but would certainly try to help out with any other efforts to make this happen.\n", "I was able to make a Shiny/R app that interfaces with tensorflow using rPython.  https://github.com/rajshah4/tensorflow_shiny  \nAwkward, but it worked.  Hoping that a better solution gets developed.  \n", "Any objections to closing this given that there are now relatively working R bindings https://github.com/rstudio/tensorflow\n", "No, great!\n\nSkickat fr\u00e5n min iPhone\n\n> 12 okt. 2016 kl. 21:17 skrev Andrew Selle notifications@github.com:\n> \n> Any objections to closing this given that there are now relatively working R bindings https://github.com/rstudio/tensorflow\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n"]}, {"number": 386, "title": "Implement Fast Fourier Transform ops", "body": "Hey TF,\n\nTrying to integrate the new Unitary RNN's into tensorflow. Unfortunately, they require Fast Fourier Transform and Inverse Fast Fourier Transforms. \n\nFor efficiency this needs to be done on the GPU. On the author's code (theano), they do this by making a Theano Op, and inserting scikit's Fast Fourier and Inverse Fast Fourier Here:\n\nhttps://github.com/amarshah/complex_RNN/blob/master/fftconv.py\n\nHow can this be done in TF? Upon completing the Unitary RNN, I plan to share it to everyone! I asked on the google groups, but didn't get a reply. \n", "comments": ["We'll need to add a few FFT ops to get this done -- probably using cufft or fbfft for GPU.  Assigning to @zffchen78 since he has experience in ffts and adding ops :)\n", "Thank you @vrv and @zffchen78 . Basically, these new types of RNN's are very promising and I hope to integrate them into TensorFlow over the next week if possible. I'm starting to work on it here:\nhttps://github.com/LeavesBreathe/Seq2Seq_Upgrade_TensorFlow\n\nHowever, it will not be possible to do without FFT and IFFT support so I really appreciate your help!\n", "So would this imply that the FFT ops only work for GPU and not CPU? Or just very very slow on CPU?\n", "Well the problem is that the actual unitary RNN uses FFT and IFFT within its cell. Ideally you want the cell's computation done in parallel on the GPU. If its on the CPU, its going to take forever.\n\nHere's the paper: Look at their main equation http://arxiv.org/pdf/1511.06464v1.pdf (its number 6)\n", "With 'taking forever', you mean compared to the uRNN GPU version or also compared to a 'classical' RNN or a LSTM with the same number of parameters? I would at least hope the speed to convergence demonstrated in the paper would outweigh any extra computation time per iteration. :)\n\nAnyways, I would expect that all ops defined in Tensorflow would be available for both CPU and GPU (and any future device). Me and I guess others don't have day to day access to a suitable GPU. Or is that not one of the design goals?\n", "We'll try to provide an implementation for both.\n", "Perfect! Thanks! @adder, I primarily use GPUs and if you're going to use unitary RNN's for some serious language modeling, you gotta use GPU's (10x faster training time). \n\nI know they may converge more quickly (one of the benefits), but keep in mind that an epoch usually takes at least 24 hrs with normal LSTM's and GRU's for big datasets. So we simply can't afford to put them on CPUs. \n", "@LeavesBreathe  , can you elaborate a bit more on your requirements about FFT/IFFT?\nSpecifically,\n1) do you need r2c (real-to-complex), c2r(complex-to-real), or c2c(complex-complex) fft/ifft support?\n2) do you need 1D, 2D, or 3D fft/ifft? or all of them?\n3) do you need support for batching?\n4) In your original email, you pointed to theano's code fftconv.py, which uses fft/ifft to implement conv2d operation. Do you need conv2d in the end? If that's all you want, tf has conv2d operation already and they run on gpu using cudnn. As I was told, newer version of cudnn uses fft under the hood already.\n", "I guess you should ask this questions to @LeavesBreathe :)\n", "@LeavesBreathe, is this the paper you are trying to reproduce? http://arxiv.org/pdf/1511.06464v1.pdf And formula (6) is the core computation you need to do. AFAIK, you need 2D, non-batched, complex-to-complex fft/ifft. Is that correct?\n", "Thanks @adder! @zffchen78, thanks for your help in this matter. I will do my best to answer your questions. The ultimate goal is to replicate this paper: http://arxiv.org/pdf/1511.06464v1.pdf \n\nTo do so, I was planning on making a new class in RNN_cell.py. Therefore, I believe with need complex to complex along with 2d support. \n\nThe reason why I wanted a pythonic tensorflow op is so that we can assign multiple gpu's to multiple unitary RNN's when the whole integration is complete. So one uRNN would be on gpu 1, and another uRNN would be on gpu 2. \n\nI don't know how hard it is for you to implement these fft's and ifft's but I think 3d support would be nice for future users who may try unitary conv nets. I certainly don't want to ask too much of you though!\n\nIf this is of any help to you, the goal is to replicate \"Complex_RNN\" here: https://github.com/amarshah/complex_RNN/blob/master/models.py#L532\n", "I would also very much like to see fft support, not for the training of dnn models, but for the feature extraction in a speech recognition pipeline. In this case only r2c, 1d would be required, preferably with batching both in cpu and gpu.\n", "Hey @zffchen78 , I just wanted to follow up and ask if this was implemented yet? I can't make any progress on the unitary RNN's without it. Don't mean to bring on any pressure! Just wanted to ask. \n", "I implemented 2D fft in TF code base already. Hopefully, it'll be copied in OSS soon.\n", "Okay thanks for the headsup. Looking forward to it!\n", "@LeavesBreathe Why close this, it isn't there yet!\n", "@LeavesBreathe, we pushed changes today which enables fft2d and ifft2d on gpu. You can take a look of python/kernel_tests/fft_ops_test.py to see if it works for you. We are still figuring out license issues w/ fftw where cpu support for fft2d/ifft2d needs. Hopefully, gpu supports are sufficient for you now. Let me know if you see any problems. \n", "Awesome, gpu support is really all i needed so let me test it out and get back to you. I am going to be pretty busy over the holidays but come January 4 I should be back to testing this!\n", "@LeavesBreathe, I took a brief of the paper you plan to replicate in TF. I suspect you'll encounter some nuances. E.g., some operations may not have support for complex64 as time being. They are minor problems because they can be easily patched by updating a few .cc file to expand the types they support. \n", "OKay this is very good to know @zffchen78. I will be sure to look into these formats. \n", "I think @LeavesBreathe answer to @zffchen78 question was wrong.\n\nWhat uRNN needs is batched 1d fft. Both the paper and their theano implementation indicate this.\n\nPaper wise, the Equation (6) is certainly multiplied with a complex vector for each data instance.\n\nImplementation wise, check the following call stack,\n1. https://github.com/amarshah/complex_RNN/blob/master/models.py#L14\n2. https://github.com/amarshah/complex_RNN/blob/master/fftconv.py#L147\n\nTo TF core developer: why not simply implement an interface like scikits.cuda: https://github.com/lebedov/scikit-cuda/blob/master/skcuda/fft.py\n", "We welcome the community to contribute the code.\n\nIf one wants to make the fft op supports more complete in tensorflow, feel\nfree to send us changes. It shouldn't be too hard to copy the fft2d impl,\nand add fft1d, fft3d, and batched_fft1d, batched_fft2d, batched_3d, etc. as\nneeds arises, plus testing, etc. These ops' impl will pretty much look like\n  stream->CreatePlan(...);\n  stream->DoFft(...);\n\nOn Fri, Jan 15, 2016 at 9:10 AM Raingo notifications@github.com wrote:\n\n> I think @LeavesBreathe https://github.com/LeavesBreathe answer to\n> @zffchen78 https://github.com/zffchen78 question was wrong.\n> \n> What uRNN needs is batched 1d fft. Both the paper and their theano\n> implementation indicate this.\n> \n> Paper wise, the Equation (6) is certainly multiplied with a complex vector\n> for each data instance.\n> \n> Implementation wise, check the following call stack,\n> 1. https://github.com/amarshah/complex_RNN/blob/master/models.py#L14\n> 2. https://github.com/amarshah/complex_RNN/blob/master/fftconv.py#L147\n> \n> To TF core developer: why not simply implement an interface like\n> scikits.cuda:\n> https://github.com/lebedov/scikit-cuda/blob/master/skcuda/fft.py\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/386#issuecomment-172018139\n> .\n", "@raingo @phsmit I added a batched 1D FFT (and 2D/3D) for GPUs in https://github.com/tensorflow/tensorflow/commit/784778e76d3f5fd75b9ec99138770795907f1616.\n", "Huh, interesting @pputzky -- the tests I added compare directly with numpy.\nCould you please run tensorflow/python/kernel_tests/fft_ops_test.py and\nshare the result?\n\nAlso, what version of numpy and cuda (in particular cuFFT if you know) are\nyou using?\n\nOn Wed, Mar 23, 2016, 3:54 AM pputzky notifications@github.com wrote:\n\n> @rryan https://github.com/rryan Thanks so much for your implementation.\n> I tested your version of the batch_fft2, and I am afraid it does not return\n> the expected results. There might be just a simple error in the transpose.\n> \n> I have compared your batch version to the non batch version and numpy's\n> fft2 in an ipython notebook. I have no idea how to upload the notebook\n> here, so I have attached a PDF export of the notebook here\n> test_batch_fft.pdf\n> https://github.com/tensorflow/tensorflow/files/185802/test_batch_fft.pdf\n> .\n> \n> What this shows is that your batch version of fft2 somehow seems to\n> shuffle the rows and columns in the wrong way (that's my guess). If I\n> compare TFs fft2 and numpy's fft2 they both give consistent results,\n> however the batch_fft2 does not comply with the former two.\n> \n> Good thing though: tf.batch_ifft2(tf.batch_fft2(x)) seems to return the\n> identity.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/386#issuecomment-200294816\n", "@pputzky -- ah, reading your notebook I think I see an issue:\n\n``` python\nx_batch = tf.constant(mnist.test.images[1].reshape((1,28,28,1)), tf.float32)\nx_fft_batch = batch_fourier(x_batch, (1,28,28,1))\n```\n\n`tf.fft2d` transforms the inner-most 2 dimensions so that would be doing a 28x1 FFT2D. Can you try reshaping to (1, 28, 28)? \n", "@rryan Yes, that was precisely the problem. I only realised that after posting my comment, apologies for that. It works perfectly fine with  (1, 28, 28). Thanks again for the implementation!\n", "@pputzky: :+1: glad to hear\n", "Is there any update on FFT on the CPU? I need to be able to run it on a computer without a GPU, and currently it seems to be implemented on the GPU target. \n", "@drufat unfortunately that's the case -- the current ops are GPU-only. \n\nIf nobody from the TF team takes this on I would be up for doing it -- I think the most annoying part will be adding FFTW as a TF dependency.\n", "We can't pull in FFTW because of its license implies complicated legal issues.\n", "There is always [clFFT](https://github.com/clMathLibraries/clFFT/)\n", "If your usage is not performance critical yet (if you do, probably you better get a gpu), you can always fall back to numpy for prototyping your model. E.g., here is how you can use numpy fft modules to in tf:\n\n```\n# a bit excotic function (rfft)\nwith self.test_session():\n  x = tf.constant([1., 2., 3., 4.], tf.float32)\n  def rfft(x):\n    return np.fft.rfft(x).astype(np.complex64)\n  y, = tf.py_func(rfft, [x], [tf.complex64])\n  self.assertAllClose(y.eval(), np.fft.rfft([1., 2., 3., 4.]))\n```\n", "I'm not really sure what's happening here but I'm not getting the performance from the gpu implementation of batch_fft2d that I was expecting. Somehow Numpy is much faster.\n\nI'm using Tensorflow r0.8 and I've tried this with a gtx 980 on Fedora 23 and on a K40 on RedHat enterprise 7, CUDA 7.5 from NVIDIA. I'm using Anaconda 2.7.\n\nHere is the code and results:\n\n``` python\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nx = np.ones((1, 4000, 4000), dtype=np.float32)\nx = x * .1\ne = tf.constant(x, tf.complex64)\n\nwith tf.Session() as sess:\n    with sess.graph.device('/gpu:0'):\n        print(\"starting fft\")\n        st = time.time()\n        tf.batch_fft2d(e).eval()\n        en = time.time()\n        print (\"gpu duration: {0}\".format(en-st))\n\n\nnpst = time.time()\nnp.fft.fft2(x[0])\nnpen = time.time()\n\nprint (\"npy duration: {0}\".format(npen-npst))\n```\n\nThe results are:\n\n```\ngpu duration: 14.0613360405 seconds\nnumpy duration: 1.05315804482 seconds\n```\n\nAm I missing something?\n", "@alexhock -- I haven't tried your code myself but I believe you're hitting a common TensorFlow pitfall which is that you're also measuring the time TensorFlow spends constructing the graph and also the C++/Python conversion of the resulting tensor into a numpy array.\n\n``` python\ng = tf.Graph()\nx = np.ones((1, 4000, 4000), dtype=np.float32)\nx = x * .1\nwith g.as_default(), tf.device('/gpu:0'):\n  e = tf.constant(x, tf.complex64)\n  fft = tf.batch_fft2d(e)\n\n  # This barrier executes the FFT op as a dependency but does \n  # not pay the cost of converting the result of the FFT into Python objects.\n  with tf.control_dependencies([fft]):\n    barrier = tf.no_op(name='benchmark_barrier')\n\n  with tf.Session() as sess:\n    st = time.time()\n    sess.run(barrier)\n    en = time.time()\n    print (\"gpu duration: {0}\".format(en-st))\n```\n\n_EDIT_: Fix placement of tf.device.\n\nCan you give that a try?\n\nAlso, note that to do a proper benchmark you're going to want to take multiple measurements in a loop and it's quite common to include a \"warmup\" phase of a dozen or so `session.run` calls that you ignore the results of.\n", "Also note that Python garbage collection is going to be a source of noise in your benchmark (if you measure over thousands of runs) -- you can either use timeit [1] which turns off GC or you can use TensorFlow tracing [2] [3] to measure the op timing from within TensorFlow.\n\n[1] https://docs.python.org/2/library/timeit.html\n[2] https://www.tensorflow.org/versions/r0.8/api_docs/python/client.html#Session.run\n[3] https://github.com/tensorflow/tensorflow/blob/fe454464681b036ff7fed3e42c6bb541fa52dd7c/tensorflow/core/protobuf/config.proto#L146\n\nPass a RunOptions proto with trace_level == FULL_TRACE and an empty RunMetadata proto to session.run and you'll get some tracing metrics for the run.\n", "Ryan's right. Though a tiny correction (might not matter due to auto placement),  the code misplaced the with tf.device('/gpu:0') I think. It should be \n\n```\n  # Construct a computation graph and place a constant e and fft(e) on /gpu:0\nwith g.as_default(), tf.device('/gpu:0'):\n  # Here we start to add nodes into 'g'\n  e = tf.constant(x, tf.complex64)\n  fft = tf.batch_fft2d(e)\n  # Here we finish adding nodes into 'g'. \n  # Typically, a tf proram is easier to read and is more\n  # robust to keep the graph g intact after this point.\n\n  with tf.Session() as sess:\n    for _ in range(100):\n      sess.run(barrier)\n\n```\n\ntf.device('/gpu:0') has no bearing on how sess.run exectuting stuff. It affects how the computation graph is placed.\n", "@zffchen78 --- oops, good catch. I was pre-\u2615. :) Edited my previous post to avoid future confusion.\n", "Thanks @rryan for implementing the batched form of the FFT and IFFT. Sorry I have been gone for a while, but I had other responsibilities I had to take care of.\n\nI am working both on Unitary RNN's now along with Associated LSTM's -- both of which require complex number calculations. If I can get either of them working, I'll submit a push to Tensorflow's `rnn_cell.py`\n\nRight now though, it doesn't seem like backprop is supported for complex numbers and I have inquired about it  #here:https://github.com/tensorflow/tensorflow/issues/2255\n", "Thanks @rryan! It would be great to also have real-valued FFT operations like batch_rfft2d() and batch_rifft2d() (which use half as many operations as the complex version). I think this is identical to the current implementation but using cufftExecR2C() and cufftExecC2R() instead of cufftExec()\n", "Indeed! rfft is on my rainy-day TODO list. If somebody else would like to take that on before I get to it, please feel free.\n", "I'm working on an open source implementation of uRNN right now for everyone to use. I'm hoping with what is currently implemented, this is possible. Will post back in a few days with an update.\n", "Is there any update on FFT on the CPU?  Are fftw license issues still killing us?  kissfft is bsd license, for one.  \n\n...I know it seems like kind of a minor point, since gpus are so inevitable, but I get a lot of prototyping done when I'm on the plane with my dinky laptop with its adorable little intel graphics card.  Except I can't prototype anything involving tensorflow :).  Is there some cheesy slow workaround we could consider?\n", "I'm now working on a cuFFT-based rfft.\n", "@jacksonloper CPU-based FFT is still in limbo unfortunately. I'll ping an internal thread about it.\n", "@rryan you are my hero. \n", "If the FFTW licensing is an issue for the host FFT version, how about kissfft or the Eigen version thereof?\n", "Any update on the rfft? If not I can possibly pick up some work on that.\n", "Sorry for the delay -- I have a working 1D and 2D (forward and backward) implementation. I'm working on 3D but haven't gotten the gradients correct yet. I hope to get this done soon.\n", "@rryan Are you talking about the general fft2d or the rfft in 1D and 2D?  It would be really nice to have all of them in the CPU version as well. Thank you for your help :)", "@mwoodson1 RFFT/IRFFT (1D/2D/3D with gradients for 1D/2D) is now committed internally, once the GitHub sync occurs it'll be available.\r\n\r\nEverything is still implemented with cuFFT, so it's GPU-only unfortunately. We're still working on a solution for CPU FFTs.", "Is there a workaround for CPU-based ffts? Any kind, really (real2complex, complex2complex, whatever), I'd just like to be able to do prototyping when I'm without my GPU access. :) ", "@Timeroot as a workaround until we get CPU FFT kernels added you can compute a 1D Fourier transform on CPUs with a matrix multiply:\r\n```python\r\ncomplex_signal = ...\r\nft_length = 1024\r\n# Compute the DFT matrix for a given length. scipy has a built-in for this.\r\ndft_matrix = tf.constant(scipy.linalg.dft(ft_length).astype(np.complex64))\r\nc2c_result = tf.matmul(complex_signal, dft_matrix)\r\n```\r\n\r\nIf you want the r2c, just slice the `fft_length / 2 + 1` unique components.\r\n\r\n```python\r\ncomplex_signal = tf.complex(real_signal, tf.zeros_like(real_signal))\r\n... example as above ...\r\nunique = ft_length / 2 + 1\r\nr2c_result = c2c_result[..., :unique]\r\n```", "@rryan, I have working CPU/GPU kernels for complex FFTs based on Eigen but have some issues integrating your recent RFFT changes. Would you be up for a chat to iron out those bumps so we can add the CPU kernels? Ideally, I would like to move to an interface similar to `conv2d` which takes `use_cudnn_on_gpu` as an optional argument.", "@tillahoffmann happy to chat! though I'm not on the TF team per se (just a contributor). I'll loop in some TF folks who could weigh in. Want to email me and we'll go from there? My google.com email is rjryan. Also, are the changes pushed publicly anywhere?", "@rryan, no not pushed publicly at the moment because I don't have the tests passing yet. Will drop you an e-mail.", "The current plan of record is that @tillahoffmann is working on a CPU-based FFT/RFFT using Eigen's TensorFFT module. No firm ETA but hopefully soon -- thanks @tillahoffmann! ", "@tillahoffmann, do you have any updates on progress to the CPU implementation of FFT?  Many thanks for this.", "@pawarrick, complex FFT and IFFT as well as the RFFT are now in master. The IRFFT still needs a bit of work.", "yes thanks very much for your contribution. I saw the discussion about it in this thread #9029 ", "@tillahoffmann Very much thanks for your contribution! I've made up the IRFFT part, see #10127 ", "Thanks to @tillahoffmann (#9029) and @Czxck001 (#10127), I think we can call this done!", "Great work! Now can I finally hack any fft operation on my laptop. \r\nI'm a bit curious. Do you see any chances, that these OPs could eventually be ported to mobile platforms such as Android? \r\n", "@beniroquai \r\nAs long as they are available for CPU, they can be compiled for Android.\r\nI don't know, if they are in the default op list.\r\nOtherwise you can use selective registration.\r\nSee #10254, #10299 and #10351", "Hi, I just tried running FFT ops on the CPU with the 1.2.1 release, but still get errors like this:\r\n`No OpKernel was registered to support Op 'FFT2D' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='GPU'`\r\n\r\nAm I missing something?", "@uschmidt83, have you tried the [nightly build](https://github.com/tensorflow/tensorflow#installation)?", "Is it scheduled to put it also in the release version at one point?\n\nOn 21 Jul 2017 16:49, \"Till Hoffmann\" <notifications@github.com> wrote:\n\n> @uschmidt83 <https://github.com/uschmidt83>, have you tried the nightly\n> build <https://github.com/tensorflow/tensorflow#installation>?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/386#issuecomment-317020837>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEJOuFzD6pxxPx-eCuMp5Cyib7Y93tLzks5sQLpqgaJpZM4GsRs5>\n> .\n>\n", "It should be in TF 1.3 which is in the process of being released! Also note\ntf.contrib.signal in this release which has some signal processing basics\nthat are useful for e.g. computing spectrograms.\n\nOn Fri, Jul 21, 2017, 9:09 AM beniroquai <notifications@github.com> wrote:\n\n> Is it scheduled to put it also in the release version at one point?\n>\n> On 21 Jul 2017 16:49, \"Till Hoffmann\" <notifications@github.com> wrote:\n>\n> > @uschmidt83 <https://github.com/uschmidt83>, have you tried the nightly\n> > build <https://github.com/tensorflow/tensorflow#installation>?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/386#issuecomment-317020837\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AEJOuFzD6pxxPx-eCuMp5Cyib7Y93tLzks5sQLpqgaJpZM4GsRs5\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/386#issuecomment-317042473>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABnn5k1w8JJTN6yDwV-1ZtDnoDI1dv3ks5sQM0kgaJpZM4GsRs5>\n> .\n>\n", "Thanks, everyone! It does work for me with the 1.3.0-rc0 release.", "there isn't an alias for real ffts in TF 1.3.0\r\n\r\npresent functions:\r\n`tf.spectral.fft`\r\n`tf.spectral.fft2d`\r\n`tf.spectral.fft3d`\r\n`tf.spectral.ifft`\r\n`tf.spectral.ifft2d`\r\n`tf.spectral.ifft3d`\r\n`tf.spectral.irfft`\r\n`tf.spectral.irfft2d`\r\n`tf.spectral.irfft3d`\r\n`tf.spectral.rfft`\r\n`tf.spectral.rfft2d`\r\n`tf.spectral.rfft3d`\r\n\r\npresent aliases:\r\n`tf.fft`\r\n`tf.fft2d`\r\n`tf.fft3d`\r\n`tf.ifft`\r\n`tf.ifft2d`\r\n`tf.ifft3d`\r\n\r\ncan someone please create those missing aliases for the next TF release?", "Hi Federico,\n\nThis is intentional as we are trying to avoid polluting the top-level\nnamespace in favor of more specific submodules. tf.fft and other fft\nsymbols in the tf module are deprecated now :).\n\nOn Fri, Sep 15, 2017, 8:06 AM Federico Muciaccia <notifications@github.com>\nwrote:\n\n> there isn't an alias for real ffts in TF 1.3.0\n>\n> present functions:\n> tf.spectral.fft\n> tf.spectral.fft2d\n> tf.spectral.fft3d\n> tf.spectral.ifft\n> tf.spectral.ifft2d\n> tf.spectral.ifft3d\n> tf.spectral.irfft\n> tf.spectral.irfft2d\n> tf.spectral.irfft3d\n> tf.spectral.rfft\n> tf.spectral.rfft2d\n> tf.spectral.rfft3d\n>\n> present aliases:\n> tf.fft\n> tf.fft2d\n> tf.fft3d\n> tf.ifft\n> tf.ifft2d\n> tf.ifft3d\n>\n> can someone please create those missing aliases for the next TF release?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/386#issuecomment-329808585>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABnn81zqHwa1D8Hq7cPScHY1S0J14wxks5sipJtgaJpZM4GsRs5>\n> .\n>\n", "Hi  everyone,\r\n\r\nI would like to do some work on the basis of this article, but the original author uses theano.tensor.fft module to implement batch 1-D FFT, batch 2-D FFT, batch 1-D IFFT, batch 2-D IFFT. Now i want to replace the theano.tensor.fft module directly into tensorflow in tf.fft and tf.fft2d, ok?\r\n\r\n<https://github.com/ChihebTrabelsi/deep_complex_networks/blob/master/complexnn/fft.py>\r\n\r\nWe look forward to your help, thank you very much!", "@WangNuoWa -- yes, tf.fft/tf.ifft and tf.fft2d/tf.ifft2d should be drop-in replacements for those (though I have never used theano.tensor.fft), perhaps with slightly different normalization strategies though (I see an \"ortho\" norm in there). ", "Is tf.fft differentiable?", "Yes!\n\nOn Wed, Dec 13, 2017, 5:10 PM Abhejit Rajagopal <notifications@github.com>\nwrote:\n\n> Is tf.fft differentiable?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/386#issuecomment-351576233>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABnn9VRd1EzVu5grKydHNsOzqLhNmrPks5tAHWcgaJpZM4GsRs5>\n> .\n>\n", "@rryan, is there anything special we would have to do? I get new warnings now, like these: (you can copy paste the whole thing)\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.linspace(0,10,num=10,dtype=np.complex64)\r\nw = np.linspace(0,1,num=10,dtype=np.complex64)\r\ny = np.multiply(x,w)\r\n\r\nx_tf = tf.Variable(x,dtype=tf.complex64)\r\nw_tf = tf.Variable(w,dtype=tf.complex64)\r\ny_tf = tf.multiply(x_tf,w_tf)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(y_tf)\r\ng1 = tf.gradients(y_tf,x_tf)\r\n'''\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 472, in gradients\r\n    grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)\r\n  File \"/opt/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 233, in _DefaultGradYs\r\n    y.dtype)\r\nTypeError: Gradients of complex tensors must set grad_ys (y.dtype = tf.complex64)\r\n'''\r\n\r\na_tf = tf.fft(x_tf)\r\nsess.run(a_tf)\r\ng2 = tf.gradients(a_tf,x_tf)\r\n'''\r\ng2 = tf.gradients(a_tf,x_tf)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 472, in gradients\r\n    grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)\r\n  File \"/opt/intel/intelpython3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 233, in _DefaultGradYs\r\n    y.dtype)\r\nTypeError: Gradients of complex tensors must set grad_ys (y.dtype = tf.complex64)\r\n'''\r\n\r\nprint(x_tf)\r\nprint(w_tf)\r\nprint(y_tf)\r\n'''\r\n>>> print(x_tf)\r\n<tf.Variable 'Variable:0' shape=(10,) dtype=complex64_ref>\r\n>>> print(w_tf)\r\n<tf.Variable 'Variable_1:0' shape=(10,) dtype=complex64_ref>\r\n>>> print(y_tf)\r\nTensor(\"Mul:0\", shape=(10,), dtype=complex64)\r\n'''\r\n```\r\n\r\nIf its relevant: \r\n```\r\npip list | grep tensorflow\r\ntensorflow-gpu (1.4.1)\r\n```\r\n", "> @rryan, is there anything special we would have to do? I get new warnings now, like these: (you can copy paste the whole thing)\r\n\r\nI think what you're seeing is independent of whether FFT is differentiable. [Here's a colab](\r\nhttps://colab.research.google.com/notebook#fileId=1fh1D_okZ8fRLOyjHRiaY7hOS5bBHfOIy) where I copied your example. You need to do as the error message says and provide a `grad_ys` argument to `tf.gradients` if `ys` is complex, because the TF gradient infrastructure currently won't assume that `grad_ys` is all ones if the type is complex, as it does when the type is real (I'm not totally sure why this is... maybe the original author of that code didn't want to assume that `1+1j` was the proper `grad_ys` for a complex number because its magnitude is greater than 1?). \r\n\r\nYou can see this logic here:\r\nhttps://github.com/tensorflow/tensorflow/blob/70e33e0f77cb4f92ddd70a8dd601873b178124fc/tensorflow/python/ops/gradients_impl.py#L232-L239\r\n", "Hi,\r\n\r\nI was wondering whether you guys at TF were planning to implement a non-uniform FFT layer.\r\nI don't know if this question belongs here, or if I should create a new issue by itself.\r\n\r\nCheers", "@zaccharieramzi nobody on the TF team is currently considering that, as far as I know. \r\n\r\nAIUI, the most efficient algorithms for it are defined in terms of the FFT -- is it an easy extension of the FFT ops that already exist? If so, I would be happy to code review an addition to tf.signal to add it if someone submits a pull request. \r\n\r\nIf it is more involved to implement, for example requiring custom kernels, then I think it might be a harder sell to include since there has not been high demand for it, and adding custom kernels increasing the binary size and compile time of TensorFlow.  \r\n\r\nCould you please file a GitHub issue requesting the feature? Anyone else who is interested would then have a nice place to add a +1 so we can better understand how many people would benefit from it being in TensorFlow itself (as opposed to being implemented as a standalone library).", "Hi @rryan ,\r\n\r\nUnfortunately it's not a direct extension of the FFT afaik (I am still new to the topic). So I think it does require a custom kernel. I totally understand that it could be a hard sell and I will file a github issue requesting the feature (I am totally not familiar with C/C++ so I don't think I could implement it myself).\r\n\r\nIn the meantime I will use [ODL](https://github.com/odlgroup/odl) to achieve my goals, although slower because the implementation is in python.\r\n\r\n----\r\nEDIT: done in #27193 ."]}, {"number": 385, "title": "when the tensorflow support for cluster installation", "body": "when the tensorflow support for cluster installation\n", "comments": ["DUP of https://github.com/tensorflow/tensorflow/issues/23\n"]}, {"number": 384, "title": "32-bit architecture support?", "body": "There's some discussion [pointer](http://stackoverflow.com/questions/33752772/tensorflow-on-raspberry-pi) pointing out that TensorFlow only supports 64 bit architectures. Is this correct?\n\nCan someone confirm, that given the right dependencies are met, TensorFlow could also run in 32-bit machines?\n\nThanks,\n", "comments": ["I have TensorFlow running on Arm 32bit ( Jetson TK1) and it seems to work fine.\nYou can see my instructions on how to build it at:\nhttp://cudamusing.blogspot.com/2015/11/building-tensorflow-for-jetson-tk1.html\n", "@maxcuda: thanks for doing that work!\n\nSome of those changes (the ifdef ones) might be worth integrating into TensorFlow -- feel free to send us a change via Gerrit.  Not sure what to do about the nvcc failures though -- hopefully a newer version of nvcc might help.\n", "Thanks for sharing @maxcuda. Pretty cool. I'm experiencing some issues while compiling bazel though (Java heap space). \n\nhttps://github.com/bazelbuild/bazel/issues/606#issuecomment-159749768\n\nDid you see something similar?\n", "No, I haven't seen the heap problem compiling Bazel.\n", "Have succeeded. Thanks for the support. \n"]}, {"number": 383, "title": "tf.train.saver.restore failed error", "body": "Cause training a model is time consuming, So Save a Checkpoint on training, but error occurred when to restore.\nThe `saver.restore` says as follow:\n\n```\nSignature: saver.restore(sess, save_path)\nDocstring:\nRestores previously saved variables.\n\nThis method runs the ops added by the constructor for restoring variables.\nIt requires a session in which the graph was launched.  The variables to\nrestore do not have to have been initialized, as restoring is itself a way\nto initialize variables.\n\nThe `save_path` argument is typically a value previously returned from a\n`save()` call, or a call to `latest_checkpoint()`.\n\nArgs:\n  sess: A Session to use to restore the parameters.\n  save_path: Path where parameters were previously saved.\n```\n\nSo I used it as following:\n\n```\nwith tf.Graph().as_default():\n    saver = tf.train.Saver()\n    sess = tf.Session()\n    Saver.restore(sess, \"./MNIST_data/-1\")\n```\n\nBut got the following err:\n\n```\nValueError                                Traceback (most recent call last)\n<ipython-input-10-4c62153b8108> in <module>()\n     31 \n     32 with tf.Graph().as_default():\n---> 33     saver = tf.train.Saver()\n     34     sess = tf.Session()\n     35     Saver.restore(sess, \"./MNIST_data/-1\")\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in __init__(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder)\n    678         var_list = variables.all_variables()\n    679       if not var_list:\n--> 680         raise ValueError(\"No variables to save\")\n    681       saver_def = builder.build(\n    682           var_list,\n\nValueError: No variables to save\n```\n", "comments": ["You haven't defined the graph yet: \"It requires a session in which the graph was launched.\"\n\nIf you create your graph with all of the variables / ops that you want to restore, and then create the Saver, it should work.\n", "(Feel free to reopen if creating/launching the graph doesn't solve the problem for you).\n", "@vrv  I had same problem and headed here to see if it's a bug or not,\n\nI have eval.py for calculating my CNN accuracy and used exactly same method to load ckpt file. I got this error but using `tf.Graph()` doesn't help. Instead creating a `dummy` variable solve the error.\n\n> p.s just inform you that this code is not working and got stuck in the `sess.run(acc)`\n\n``` python\nwith tf.Graph().as_default():\n\n    dummy = tf.Variable(0)  # dummy variable !!!\n    init_op = tf.initialize_all_variables()\n\n    with tf.Session() as sess:\n\n      sess.run(init_op)\n\n      saver = tf.train.Saver()\n      # Start the queue runners.\n      coord = tf.train.Coordinator()\n      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n      summary_op = tf.merge_all_summaries()\n\n      summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir,\n                                              graph_def=sess.graph_def)\n\n\n      ckpt = tf.train.get_checkpoint_state(checkpoint_dir=FLAGS.checkpoint_dir)\n      print ckpt.model_checkpoint_path\n      if ckpt and ckpt.model_checkpoint_path:\n        saver.restore(sess, ckpt.model_checkpoint_path)\n        print('Restored!')\n\n      images, labels = my_input.inputs_val()\n\n      # Build a Graph that computes the logits predictions from the\n      # inference model.\n      logits = my_cifar.inference(images)\n\n      acc = my_cifar.evaluation(logits, labels)\n\n      tf.scalar_summary('Acc', acc)\n\n      try:\n        while not coord.should_stop():\n          print('Calculating Acc:')\n          acc_r = sess.run(acc)\n          print(acc_r)\n\n          # Write results to TensorBoard\n          summary_str = sess.run(summary_op)\n          summary_writer.add_summary(summary_str)\n\n      except tf.errors.OutOfRangeError:\n        print ('Done!')\n\n      finally:\n        # When done, ask the threads to stop.\n        coord.request_stop()\n\n      coord.join(threads)\n```\n", "Hi,\nI am trying to restore a saved model . But it is returning me an error. Please help me out. \n**code to save the model : save_model.py** \n\n```\nimport tensorflow as tf\nv1 = tf.Variable(1.32, name=\"v1\")\nv2 = tf.Variable(1.33, name=\"v2\")\n\ninit = tf.initialize_all_variables()\n\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n  sess.run(init)\n  save_path = saver.save(sess, \"model.ckpt\")\n```\n\n**code to restore model : restore_model.py** \n\n```\nimport tensorflow as tf\nv1 = tf.Variable(0, name=\"v1\")\nv2 = tf.Variable(0, name=\"v2\")\n\n\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n  saver.restore(sess, \"model.ckpt\")\n  print(\"Model restored.\")\n```\n\nI have saved both the files in the same directory. \n", "I am able to restore models within the same Python script,  but I am unable to restore in a different script in the method stated above. \n", "Yes, I have so far the same problem.", "+1", "Try using\r\n\r\nsaver.restore(sess, tf.train.latest_checkpoint('./') )", "I have the same problem when I try to restore a graph trained on a different machine, although it works on the same machine.\r\nMore precisely,  I get\r\n`KeyError: 'VariableV2'`\r\non the tf.train.import_meta_graph\r\n\r\n    tf.reset_default_graph()\r\n\r\n    with tf.Session() as sess:\r\n\r\n        # Restore graph\r\n        saver = tf.train.import_meta_graph(model_filename_path)\r\n\r\n        saver.restore(sess, path_to_ckpt)\r\n\r\n", "I also had the KeyError: 'VariableV2' when restoring the graph from a checkpoint that was saved in another machine. It turned out that the tensorflow versions were different. Upgrading solved the problem", "add this should be solve\r\n\r\n'from_detection_checkpoint: true'", "modify the config file and add the 'from_detection_checkpoint: true' solve the problem", "@jakalaka where the config file?", "from_detection_checkpoint: true is already in the config file", "Just update the tensorflow to a newer version."]}, {"number": 382, "title": "Failed to bazel build when executing label_image example ", "body": "When I run the root@795c87fec335:/tensorflow# bazel build tensorflow/examples/label_image/...\nIt shows the ERRORs\n\nERROR: /tensorflow/tensorflow/core/BUILD:210:1: C++ compilation of rule '//tensorflow/core:kernels' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote . -iquote ... (remaining 47 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nINFO: Elapsed time: 91.739s, Critical Path: 73.71s\n", "comments": ["Try building with --verbose_failures so we can more easily help\n", "I'm running on docker. \n\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:1: C++ compilation of rule '//tensorflow/examples/label_image:label_image' failed: gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote . -iquote bazel-out/local_linux-fastbuild/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-fastbuild/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-fastbuild/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-fastbuild/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-fastbuild/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-fastbuild/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-fastbuild/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-fastbuild/genfiles/third_party/eigen3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-fastbuild/bin/tensorflow/examples/label_image/_objs/label_image/tensorflow/examples/label_image/main.pic.o' -MD -MF bazel-out/local_linux-fastbuild/bin/tensorflow/examples/label_image/_objs/label_image/tensorflow/examples/label_image/main.pic.d -fPIC -c tensorflow/examples/label_image/main.cc -o bazel-out/local_linux-fastbuild/bin/tensorflow/examples/label_image/_objs/label_image/tensorflow/examples/label_image/main.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/tensorflow && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -iquote . -iquote bazel-out/local_linux-fastbuild/genfiles -isystem google/protobuf/src -isystem bazel-out/local_linux-fastbuild/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-fastbuild/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-fastbuild/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-fastbuild/genfiles/external/re2 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-fastbuild/genfiles/third_party/gpus/cuda -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-fastbuild/genfiles/third_party/gpus/cuda/include -isystem third_party/eigen3 -isystem bazel-out/local_linux-fastbuild/genfiles/third_party/eigen3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-fastbuild/bin/tensorflow/examples/label_image/_objs/label_image/tensorflow/examples/label_image/main.pic.o' -MD -MF bazel-out/local_linux-fastbuild/bin/tensorflow/examples/label_image/_objs/label_image/tensorflow/examples/label_image/main.pic.d -fPIC -c tensorflow/examples/label_image/main.cc -o bazel-out/local_linux-fastbuild/bin/tensorflow/examples/label_image/_objs/label_image/tensorflow/examples/label_image/main.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nINFO: Elapsed time: 41.853s, Critical Path: 18.42s\n", "Sorry for dropping this -- I can't really see why gcc is failing to execute the command.\n\n@davidzchen, any ideas what else we should ask our users to pass to bazel to help debug these kind of failures?\n", "Thank you so much for your help!\nPrevious error was mem exhausted. I swap mem but it doesn't help. Still failed with gcc\n", "Sorry for the delay.\n\nIt is not clear to me why the command is failing from looking at the error message. @hanwen, @damienmg - how can we get more info about why this command is failing?\n", "Having the full output instead of just a snippet might help.\n", "Closing out stale-bug queue -- post more information if you're still having problems and we'll re-open\n"]}, {"number": 381, "title": "Failed to bazel build when executing label_image example ", "body": "When I build bazel it shows this ERROR\n\nroot@ee32ec72b8cb:/tensorflow/tensorflow/examples/label_image# bazel build tensorflow/tensorflow/examples/label_image/...\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: no targets found beneath 'tensorflow/examples/label_image/tensorflow/tensorflow/examples/label_image'.\nINFO: Elapsed time: 0.771s\n\nI try to run bazel fetch//\nIt shows BUILD file have ERRORs\n\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:1:3: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:118: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:243: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:256: invalid character: '!'.\n.....\n", "comments": ["Try cd to the root tensorflow directory and then do `bazel build tensorflow/examples/label_image/...` ?\n", "I tried but it still the same error. I'm doing this example. Thank you so much!\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/README.md\n", "We need to see your exact command lines and the directory you're trying to run from, etc. \n", "I'm running on tensorflow directory. Already download and extracted inception5h.zip on /label_image/data\n\nroot@ee32ec72b8cb:/tensorflow# bazel build tensorflow/tensorflow/examples/label_image/...\n\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\nERROR: no targets found beneath 'tensorflow/tensorflow/examples/label_image'.\n", "try `bazel build tensorflow/examples/label_image/...`.  you have an extra 'tensorflow' in your version.\n", "Right now it shows the ERRORs as I mentioned from the beginning. BUILD error. The BUILD is default file on lable_image directory\n\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:1:3: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:118: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:243: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:256: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:381: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:394: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:8:516: invalid character: '!'.\n...\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:51:91: invalid base-10 integer constant: 10737418240.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:51:1002: invalid character: '!'.\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:51:1118: invalid character: '&'.\n...\nERROR: /tensorflow/tensorflow/examples/label_image/BUILD:120:13181: contains syntax error(s).\nERROR: no such package 'tensorflow/examples/label_image': Package 'tensorflow/examples/label_image' contains errors.\n", "It looks like your BUILD file is corrupted -- those characters are not in the BUILD file that I can see.\n\ntry `cat tensorflow/examples/label_image/BUILD` to make sure it matches https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/BUILD\n", "No it's not matching.\n\nI'm using this function to download this file. Do u think it might be the problems. And how I can download this file on my directory. Thanks\nwget https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/BUILD\n", "You need to use git to clone the project, and then the files will be available as expected.  The following link might be helpful: https://www.udacity.com/course/how-to-use-git-and-github--ud775 -- Let us know if we can be of any further help.\n"]}, {"number": 380, "title": "wishlist: provide alternative build system", "body": "The memory consumption for bootstraping bazel is very huge, according to my observation.\nAnd I really have trouble building bazel on my machine because lacking of enough memory.\n\nSo could you please provide an alternative (traditional) build system (e.g. make, cmake) or setup.py, \nfor users who insist on building software from source by themselves.\n\nThanks.\n", "comments": ["+1\n", "+1\n", "I'm not necessarily opposed to this, but if you are observing Bazel using excessive amounts of memory, then it is a problem that we should fix.\n\nI have opened bazelbuild/bazel#670 to track this issue. @CDLuminate Can you provide some details about the following:\n1. How much memory does your machine have?\n2. How much memory are you observing Bazel using to build itself?\n\nAlso, have you considered using the [Bazel binary installer](https://github.com/bazelbuild/bazel/releases)? If so, are you also observing huge memory usage while building TensorFlow?\n", "@davidzchen  Actually I'm not sure whether I've encountered a BUG when bootstraping bazel, but anyway I'll try to describe what I've seen while bootstraping bazel, in bazel issue 670.\n", "+1\n\nBazel's java dependency is not preferable.\n", "Looping back from bazelbuild/bazel#670: the memory consumption issue appears to be unrelated to Bazel. According to @CDLuminate, the issue went away after a kernel upgrade.\n", "It sounds like adding support for CMake builds would be helpful.  I'll give it a shot.\n", "I just submitted Pull Request https://github.com/tensorflow/tensorflow/pull/728 to add very basic CMake support to TensorFlow.  It builds the tutorial C++ training example (along with all dependencies).\n\nLimitations: no GPU, no python, no tests, no install targets, tested only on MacOSX, and needs a bit of code cleanup. It's a first version to get the ball rolling.  I'll continue to work on it, your comments are more than welcome.\n", "I like Cmake but we have experienced many misalignments problems in Caffe community maintaining two build systems. Every extension or contribution that involve the build system need to be double checked. And often developer don't have confidence with both build systems contributing code with partial coverage or different quality level. Android cmake support was not really maintained and never incorporated officially (see https://github.com/taka-no-me/android-cmake/issues/64)\n", "So that means the synchronization problem between multiple build systems should also be concerned.\n", "We're aware of the issues surrounding having multiple build systems. We'd definitely prefer if bazel would just work for all platforms (in particular, #16, #17, #110), and we're pretty confident it will work for most people in the near future.\n\nHowever, it's clear that for some platforms (especially embedded), that's a while off, and cmake seems like a good solution, especially if we can find a (semi-)automatic solution to generate the cmake. It would also solve the immediate packaging issues mentioned in #720.\n", "+1.\nI think CMake's the most viable choice. KDevelop and CLion support CMake projects. CMake also has a separate GUI program. It'd make development easier.\nSee [KDevelop 5 features](https://www.kdevelop.org/news/first-beta-release-kdevelop-500-available).\n", "Has anyone tried out the CMake files I contributed (Pull Request #728)?  I only tested them on MacOSX, so getting other's feedback on different platforms would be great.  Thanks!\n", "+1\nI maintain the TensorFlow installation on Blue Waters and it's easier for me to build the entire Python stack except TensorFlow (which has over 500 packages) than it is to build TensorFlow. On supercomputers like Blue Waters, the toolchain and libraries usually aren't found in default locations. The Bazel build makes a lot of assumptions about paths and linker flags. It is also quite frustrating that it also tries to play the role of a package manager and ignores that its dependencies are already installed.\n", "Since we maintain both a (partial) makefile build and a fairly complete cmake build (both in contrib) I will close this issue. Thanks for reminding me of its existence.\n", "From what I understand, the CMake build is broken for GPU right now and therefore can't be used to build a complete TensorFlow package. Is this outdated?\n", "@mrry -- what's the latest?\n", "Right now our CMake build only supports GPU on Windows, and we're not actively working on adding GPU support for other platforms (but would accept contributions).\n", "What exactly needs to be done to add GPU support for other platforms? Our users wish to use Tensorflow on the Cray XK7 nodes (which have Tesla K20s). It seems to be linking a lot of executables that don't seem to be built with the bazel build, like *_ops_gen_*.", "I am getting thousands of errors when grpc_tensorflow_server and a few example executables try to link.", "@camaclean The other binaries are most likely part of the code generation for C++ and Python op wrappers - these are also built on Bazel, but they don't show up as prominently in your bin/ directory. From what I can tell, the GPU rules that are [conditionally executed on `WIN32`](https://github.com/tensorflow/tensorflow/blob/e297257e458654c7743c59c7f37154b7f6118c16/tensorflow/contrib/cmake/CMakeLists.txt#L147) should mostly work on Linux as well, but we haven't tested them.\r\n\r\nPlease feel free to open a new issue with details of the exact errors you're seeing."]}, {"number": 379, "title": "incorrect app.flags.DEFINE_boolean behavior ", "body": "### demo python script\n\n test.py \n\n```\n#!/usr/bin/python\nimport tensorflow as tf\ntf.app.flags.DEFINE_boolean('tea',False,'Soggy Leaves')\ncfg = tf.app.flags.FLAGS\n\nif cfg.tea:\n    print( \"have some tea\" )\nelse:\n    print( \"no tea\" )\n```\n### Expected behavior\n\nWhen the above test.py is run with any of {(no arguments), --notea, or --tea=True}, the script behaves as expected...\n\n```\n[markb@schur tmp]$ ./test.py\nno tea\n[markb@schur tmp]$ ./test.py --notea\nno tea\n[markb@schur tmp]$ ./test.py --tea=True\nhave some tea\n```\n### Weird stuff\n\nStrangely `--tea=False` is _incorrectly_ interpreted as True.  Having both tea and no tea is only possible in certain Infocom games.\n\n```\n[markb@schur tmp]$ ./test.py --tea=False\nhave some tea\n```\n\nApparently, only those who _truly_ want tea will get it.  \n\n```\n[markb@schur tmp]$ ./test.py --tea\nusage: test.py [-h] [--tea TEA] [--notea]\ntest.py: error: argument --tea: expected one argument\n```\n\nC'mon. It's a boolean flag ! \n\"Say something once.  Why say it again?\"  -- _Talking Heads_\n", "comments": ["Looks like we're getting hit by: http://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n\nIt's doing bool(\"False\") --> True.\n\nI'll try to get it to support:\n\n--tea  --> tea is True\n--tea=True -> tea is True\n--tea=False -> tea is False\n--notea --> tea is False\n", "@vrv \r\nthis issue still persists as of TF 1.12.0\r\nAll the examples above work as expected\r\nbut  `./test.py --tea False` fails as it gives output `have some tea\r\n`. Is this still expected behavior?\r\n", "TensorFlow's implementation of flags was replaced by absl: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/flags.py\r\n\r\nSo any flag related issues should probably be filed at https://github.com/abseil/abseil-py/issues now, unless there's evidence that the wrapper in TensorFlow is the issue.", "Thanks, I will raise it there", "I know this has been closed for a long time and tf.app.flags are not really supported. But defining *tea* like this will give you the expected behavior:\r\n\r\n`tf.app.flags.DEFINE_boolean('tea',None,'Soggy Leaves')`\r\n\r\n"]}, {"number": 378, "title": "matmul gradients incorrect with complex64 tensors", "body": "The conjugation step was skipped.\nSee  https://tensorflow-review.googlesource.com/#/c/1154/ for unit tests and a fix\n", "comments": ["Thanks for the fix!  We'll try to get that reviewed and integrated soon.\n", "@vrv: Are you on this, or should I grab it?\n", "Sorry, was supposed to assign this to @zffchen78 \n", "ZF: can you review https://tensorflow-review.googlesource.com/#/c/1154/ ?\n", "@vrv, @zffchen78: What happened to this? \n", "It was submitted to gerrit, but it broke the build so we had to roll it back.  So I guess this is back to 'contributions welcome' :(\n", "For whoever looks at this next: I fixed `tf.test.compute_gradient_error` a while ago to handle `complex64` input, so it should be easy to test this change.\n", "@girving  Did you get a chance to test the fix? Can you please post your findings? This will help to resolve / update the status of this issue.", "@ymodak The fact that I fixed `tf.test.compute_gradient_error` means that others can easily test the fix.  I'm not going to do it myself.", "@mborgerding Can you please test the fix and post your findings? This will help to resolve / update the status of this issue.", "Sigh ... almost three years after a issue was described, unit tests were provided and a fix was given.\r\nSorry, I don't have the time right now to go back and set my computer back up to verify this.  I seem to recall that it *has* been fixed, but I am only 95% sure of this, since it has been years.\r\n", "Closing this issue since the fix has been made."]}, {"number": 377, "title": "How to install/run/use TensorFlow on windows machines?", "body": "", "comments": ["The current Windows solution is to run TensorFlow in a Docker container. This StackOverflow thread has discussion of how to do this: http://stackoverflow.com/questions/33616094/tensorflow-is-it-or-will-it-sometime-soon-be-compatible-with-a-windows-work . If you have questions about how to get this running, S\n\nOtherwise, adding more Windows support is an open issue (#17), so I'll close this for now.\n"]}, {"number": 376, "title": "undefined symbol: clock_gettime with tensorflow on ubuntu14.04", "body": "I have posted my problem on stackoverflow.\nhttp://stackoverflow.com/questions/33980109/undefined-symbol-clock-gettime-with-tensorflow-on-ubuntu14-04\n", "comments": ["See also this issue, incl. fix: https://github.com/tensorflow/tensorflow/issues/121\n", "Let us know if @markusdr's fix doesn't work -- otherwise feel free to reopen #121 \n", "Sorry to reply so late.My laptop is very slow in building and it lasts for a very long time.Fortunately,now I can import tensorflow.@markusdr's fix works.Thanks a lot.\n"]}, {"number": 375, "title": "CUDA headers not found when implementing GPU kernel for user-op", "body": "I am following the tutorial to implement a user-op. I have three files `dna_encode_op.cc`, `dna_encode_op_gpu.cu.cc`, and `dna_encode_op.h`.\n\nI have configured TensorFlow with GPU support using `./configure`. However, during compilation of `dna_encode_op_gpu.cu.cc`, Bazel doesn't find the CUDA headers:\n\n```\nINFO: From Compiling tensorflow/core/user_ops/dna_encode_op_gpu.cu.cc [for host]:\nIn file included from ./tensorflow/core/framework/tensor_types.h:4:0,\n                 from tensorflow/core/user_ops/dna_encode_op_gpu.cu.cc:8:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:55:18: fatal error: cuda.h: No such file or directory\n #include <cuda.h>\n                  ^\ncompilation terminated.\nERROR: /home/hannes/git/DNAflow_internal/tensorflow/core/BUILD:339:1: C++ compilation of rule '//tensorflow/core:user_ops_op_lib' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 50 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n```\n\nBuilding the included ops works well, so I'm wondering whether I have to modify `BUILD` - however, there is no mention of this in the tutorials.\n", "comments": ["Btw, I discovered that when I move my files from `user_ops/` to `kernels/` they compile fine. So it seems `BUILD` is using different rules for `user_ops/` and `kernels/`, with the CUDA headers not being included for user ops.\n", "Thanks for the report -- we're trying to figure out a better solution to custom ops than the user_ops directory, and in doing so should hopefully address these cuda header issues.\n", "I added [instructions](https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html#adding-a-new-op) for building ops and kernels outside of TensorFlow source tree. Please have a look to see if it fits your need.\n", "(Reopen if this new feature isn't working as expected)\n", "The instructions work fine for CPU-only ops, but seem to be insufficient for building an op with a GPU kernel. A slightly modified `BUILD` works fine though (for GPU-only kernels):\n\n``` python\ncc_binary(\n    name = \"softmax4d_op.so\",\n    srcs = [\n        \"softmax4d_op.cc\",\n        \"softmax4d_op.h\"\n    ],\n    copts = [\n        \"-x\", \"cuda\",\n        \"-DGOOGLE_CUDA=1\"\n    ],\n    linkopts = [\n        \"-Wl,-Bsymbolic\",\n        \"-lm\",\n    ],\n    linkshared = 1,\n    linkstatic = 1,\n    deps = [\n        \"//tensorflow/core:framework\",\n        \"//tensorflow/core:cuda\",\n        \"//third_party/eigen3\"\n    ],\n)\n```\n\nI think it would be great to have a bit more details in the docs, and maybe even have a bazel rule for building custom ops, similar to `tf_kernel_library()`.\n", "@hannes-brt  hi, I encountered similar issues when I want to add a custom op which is implemented on GPU. You said it works when you copies these files from `user_ops` to `kernels`. Did you just copy the `.h`,`.cc`,`.cu.cc` to `tensorflow/cores/kernels` or did you also add build target in `tensorflow/cores/kernels/BUILD`?  "]}, {"number": 374, "title": "No module named tensorflow.python.platform", "body": "Hi, I am running tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py. I get:\n\nhiro106@hiro106-virtual-machine:~$ python tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py\", line 33, in <module>\n    import tensorflow.python.platform\nImportError: No module named tensorflow.python.platform\n\nIf you have any idea, please help me.\nThanks,\n", "comments": ["Have you installed tensorflow?  Does `import tensorflow` work in a Python shell?\n", "Closing due to lack of activity.  Please reopen if you're still having issues.\n", "I get the same error trying this example on a Docker install.\nI am able to see TensorBoard so  I know TensorFlow is working.\nhttps://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html\n`python translate.py --data_dir [your_data_directory]`\n\ntechnologiclee@docker-playground:~/mytensorflow/tensorflow/tensorflow/models/rnn/translate$ python translate.py \n--data_dir [your_data_directory]\nTraceback (most recent call last):\n  File \"translate.py\", line 40, in <module>\n    import tensorflow.python.platform\nImportError: No module named tensorflow.python.platform\ntechnologiclee@docker-playground:~/mytensorflow/tensorflow/tensorflow/models/rnn/translate$ python\nPython 2.7.3 (default, Mar 13 2014, 11:03:55) \n[GCC 4.7.2] on linux2\n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > > ImportError: No module named tensorflow\n", "I think this is the same error, but did not solve the problem for me in Docker.\n\nhttps://github.com/tensorflow/tensorflow/issues/265\n\nPYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python2.7/dist-packages/\"\nexport PYTHONPATH\n\nMake sure to check your path with this: echo $PYTHONPATH\n", "Actually I had this issue because I was trying to import tensorflow from a python session inside the tensorflow repo folder, going to some other folder and starting python and importing tensorflow worked for me.", "Somehow, the same advice was given when installing tensorflow from the the source code https://www.tensorflow.org/install/install_sources :\r\n\r\nValidate your TensorFlow installation by doing the following:\r\n\r\nStart a terminal.\r\n\r\n**Change directory (cd) to any directory on your system other than the tensorflow subdirectory from which you invoked the configure command.**\r\n\r\nInvoke python:\r\n\r\n$ python", "weird and @ushnish solution works. Anyone with idea why it happens so?", "@ushnish  It does work! But why?", "GodMan:~ ailias$ python   <== on my home dir\r\nPython 2.7.10 (default, Feb  7 2017, 00:08:15) \r\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\ndyld: warning, LC_RPATH $ORIGIN/../../_solib_darwin_x86_64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so being ignored in restricted program because it is a relative path\r\n>>> tf.__version__\r\n'1.4.0'\r\n>>> exit()   <== import success\r\nGodMan:~ ailias$ cd Documents/tensorflow     <== on my tensorflow source code dir and build success\r\nGodMan:tensorflow ailias$ python\r\nPython 2.7.10 (default, Feb  7 2017, 00:08:15) \r\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n>>>   <== import error.\r\n\r\nWhy do we get the error on different directory ? Due to tensorflow root dir has same name 'tensorflow' subdirectory, so if we import tensorflow from root dir, actually we import these dir to our module not our built tensorflow framework. So I think the subdir name should not be the same name with tensorflow. You can have a try.\r\n", "Ushnish.. That's just brilliant..", "@ushnish . please give an example . i don't get it", "@Elmirrah you can start python interpreter (in your terminal) from any folder except the tensorflow repo folder itself, and then you can import tensorflow", "Acording to @ailias you would run into this or another issue when there is a \"tensorflow\" folder in your current directory:\r\n`$ mkdir tensorflow`\r\n `$ touch tensorflow/__init__.py`\r\n `$ python -c 'import tensorflow; print(tensorflow.__version__)'`\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute '__version__'", "just reinstall the tensorflow, I solved my problem", "I put a local bazel build of TensorFlow in my `PYTHONPATH` and got this issue. I'll update if I solve this bug.", "@ushnish ,it works, and why, so brillant.", "why I deploy the tensorflow object-detection models on aws p3 instance, I got the `ModuleNotFoundError: No module named 'tensorflow.python.platform'` when ran the train.py, is any steps I missing when config? \r\n<img width=\"1099\" alt=\"Screen Shot 2019-03-29 at 16 32 01\" src=\"https://user-images.githubusercontent.com/24205486/55219943-12024000-5241-11e9-9918-a7c4270c8610.png\">\r\n", "@ushnish is that also working on the aws instance? ", "Chaning directory not working for me.\r\nRunning pip3 list returns the following:\r\n\r\nPackage              Version\r\n-------------------- -------\r\nabsl-py              0.8.0\r\nastor                0.8.0\r\nastroid              2.3.1\r\ngast                 0.3.2\r\ngoogle-pasta         0.1.7\r\ngrpcio               1.16.1\r\nh5py                 2.8.0\r\nisort                4.3.21\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\nlazy-object-proxy    1.4.2\r\nMarkdown             3.1.1\r\nmccabe               0.6.1\r\nnumpy                1.16.2\r\npip                  19.2.3\r\nprotobuf             3.9.2\r\npylint               2.4.2\r\npython-apt           1.8.4\r\nsetuptools           41.2.0\r\nsix                  1.12.0\r\ntensorboard          1.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntermcolor            1.1.0\r\ntyped-ast            1.4.0\r\nWerkzeug             0.16.0\r\nwheel                0.33.6\r\nwrapt                1.11.2\r\n\r\nOS:DietPi on Orange Pi Zero ARMV7\r\n", "`Error: While importing \"web\", an ImportError was raised:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/connor/.local/lib/python3.7/site-packages/flask/cli.py\", line 240, in locate_app\r\n    __import__(module_name)\r\n  File \"/home/connor/programming/python/Automated-Checkout/web.py\", line 3, in <module>\r\n    from prediction import NeuralNetwork\r\n  File \"/home/connor/programming/python/Automated-Checkout/prediction.py\", line 2, in <module>\r\n    from keras.preprocessing.image import img_to_array\r\n  File \"/home/connor/.local/lib/python3.7/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/home/connor/.local/lib/python3.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/home/connor/.local/lib/python3.7/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/home/connor/.local/lib/python3.7/site-packages/keras/backend/__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"/home/connor/.local/lib/python3.7/site-packages/keras/backend/load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/home/connor/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nModuleNotFoundError: No module named 'tensorflow.python.platform'\r\n`\r\nI am also getting the same error on Tensorflow 2.1.0 and Python 3.7.6", ">>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/kali/.conda/envs/chatbot/lib/python3.6/site-packages/tensorflow/__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"/home/kali/.conda/envs/chatbot/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/kali/.conda/envs/chatbot/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/home/kali/.conda/envs/chatbot/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/home/kali/.conda/envs/chatbot/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/kali/.conda/envs/chatbot/lib/python3.6/site-packages/tensorflow_core/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/kali/.conda/envs/chatbot/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nModuleNotFoundError: No module named 'tensorflow.python.platform'\r\ni got the same problem\u00a0when i try to import tensorflow \r\ni guess this heppend when i installed tensorflow-gpu\r\nany help ??", "I too got several errors after installing TensorFlow-GPU. There wasn't any other way but to reinstall tensorflow and tensorflow-gpu:\r\npip3 install --user --upgrade --force-reinstall tensorflow tensorflow-gpu (--user flag not required when using a conda or virtualenv virtual environment)\r\n\r\nAlso, if running tensorflow-gpu, please check if your hardware supports CUDA. Even older NVIDIA \r\ngraphics card won't work. \r\n[https://developer.nvidia.com/cuda-gpus](url) This page gives u a full list of NVIDIA GPUs that support CUDA.\r\n\r\nAlso, have a look at Mr. Sentex's tutorial on how to properly install tensorflow-gpu, as simply installing tensorflow-gpu might not work, either via pip or via conda. Especially on Windows, he also shows the various steps to configure your GPU.\r\nCheck this out for more: [https://pythonprogramming.net/how-to-cuda-gpu-tensorflow-deep-learning-tutorial/](url)\r\n\r\nPlease inform, if the install is still unsuccessful.", "> pip3 install --user --upgrade --force-reinstall tensorflow tensorflow-gpu\r\n\r\nit still didn't work for me", "Hi,\r\nI am encountering the same error-\r\n    from tensorflow.python.platform import self_check\r\nModuleNotFoundError: No module named 'tensorflow.python.platform'\r\n\r\nI am on windows 10 system and have tf =1.5.2 installed\r\nany solution to this ?"]}, {"number": 373, "title": "Random numbers with None value in shape", "body": "There are several use cases where the shape of random numbers is to be determined at runtime. E.g. for variational auto encoder or injection of noise into an MLPs units. As of now, this seems not possible in an elegant way with tensorflow:\n\n```\nimport tensorflow as tf\nx = tf.placeholder('float32', [None, 784])\nnoised = x + tf.random_normal(x.get_shape())\n```\n\nThe above throws [this traceback](https://gist.github.com/bayerj/12e6a80c36593173a489). I have to replace `None` above with a number known before runtime. But then I have to fix my batch size beforehand (to which the None corresponds) and need different expressions for each batch size.\n", "comments": ["TF.shape(x) should work instead.\n", "Thank you!\n", "like", "Uh! that's actually really helpful :) thanks. Why is that?", "Sorry but this doesn't seem to be working with **TF2**, as placeholders don't exist anymore.\r\n\r\nUsing None to specify the shape doesn't work either, i.e,\r\n```tf.random.normal((None, 784))```\r\ngives\r\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n\r\nAny idea, apart from using tf.shape() on another tensor of the model to extract the batchsize ?", "Also having the same problem,\r\ntf.shape() returns some wierd number for placeholder's shape.\r\n```\r\n    def call(self, inputs, training=True, **kwargs):\r\n        print(inputs)\r\n        print(tf.shape(inputs), inputs.shape)\r\n```\r\ngives me:\r\n```\r\nTensor(\"Placeholder:0\", shape=(None, 128, 128, 3), dtype=float32)\r\nTensor(\"RandomHSVPreprocessor/Shape:0\", shape=(4,), dtype=int32) (None, 128, 128, 3)\r\n```"]}, {"number": 371, "title": "Sparse tensor construction given repeated indices", "body": "TensorFlow behaves differently (and surprisingly) to SciPy when converting sparse matrices to dense matrices.\n\n``` python\nimport tensorflow as tf\nimport scipy as sp\nimport scipy.sparse\n\ni = [0, 0]\nj = [0, 0]\nvalues = [1, 2]\n\nsp_answer = sp.sparse.coo_matrix((values, (i, j))).todense()\nprint(\"SciPy's answer is \" + str(sp_answer)) # prints [[3]]\n\ntf_answer = tf.sparse_to_dense([i, j], [1, 1], values, 0)\nprint(\"TF's answer is \" + str(tf.Session().run(tf_answer))) # prints [[2]]\n```\n\nSpecifically, if an index pair is repeated, then SciPy will sum the corresponding values, while TF just keeps the last value.\n\nIs this behaviour intentional or accidental? If it's intentional, I'll make a pull request to add a mention of it to the docs. If it's accidental, I'll make a pull request to fix it.\n", "comments": ["It's intentional.  The C++ SparseTensor code has a [IndicesValid](https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/util/sparse/sparse_tensor.h#L68) method for checking validity of a SparseTensor; and this check would fail on repeated indices.  However most of the ops that use STs don't run it for efficiency.\n\nPlease note our current contributions flow is not through GitHub; see [contributing](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md).\n", "So @ebrevdo, the answer is to document this difference in behavior?\n", "@andyljones: we now accept PRs, so if you want to fix the docs, please send it!\n"]}, {"number": 370, "title": "convert_to_records.py don't write all values into .tfrecords file", "body": "Hi,\nat first i suppose there is a typing mistake in line 56 (dat.shape[0], num_examples) i think it should be (images.shape[0],num_examples). the second thing was that this code did'nt wrote me the depth values into .tfrecords file. I need to pass them as _bytes_feature(str(depth)) after that the value was written into .tfrecords file.\n", "comments": ["Thanks, we'll fix the typo.\n\nThe format of tfrecords is not a human-readable string: most likely the depth value is encoded as a field in a _binary_ serialized protocol buffer.\n\nHow are you trying to consume the output of tfrecords?  \n", "I tried the fully connected reader example but it fails. I have string Labels so i have to change this code for string Labels. After that i tried to read the .tfrecords File with the read an decode function but with out success. Acctually I'am a little bit disapointed of tensorflow but i will try it again this weekend. \n", "If you don't provide us enough information to help you (or why you are disappointed) we can't really help you.  Please feel free to re-open the bug when you have more information that we can use to help you with.\n"]}, {"number": 369, "title": "reduce_mean gradient wrong", "body": "The `reduce_mean` code was incorrectly scaling the gradient from `reduce_sum` to adjust for the number of element in the **input** tensor as well as the **output** tensor.  \nThe only adjustment should be based on the number of output elements N, since mean=sum/N.\n\nFixed in patch at https://tensorflow-review.googlesource.com/#/c/1153/\n", "comments": ["Closing this.  It seems like I jumped to conclusions. Something is wrong, but it was probably in my code.\n"]}, {"number": 368, "title": "Sampled decoding instead of argmax decoding in seq2seq (feature request for tf.choice)", "body": "Currently, the seq2seq decoders only support argmax decoding. To enable sampled decoding, we would need a `tf.choice` function to replace `tf.argmax` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/seq2seq.py#L326\n\nAny plans on implementing this? Can the desired functionality be achieved using other ways?\n\nTheano's `choice` function: http://deeplearning.net/software/theano/library/tensor/raw_random.html\nHowever, it would be preferable for `choice` to have the same interface as `argmax`.\n", "comments": ["You're right, we should have an op like choice. Currently this is not easy to do, even though a similar functionality exists in candidate sampling ops (http://www.tensorflow.org/api_docs/python/nn.html#candidate-samplers). We are working on re-implementing candidate sampling in a cleaner way, and we'll take a look at adding a choice-like op when doing this.\n", "@lukaszkaiser, @gouwsmeister: Stephan said he doesn't have active plans to work on `tf.choice`, so we're happy to accept PRs if anyone else wants to add it.  \n", "If I understand correctly, `tf.multinomial` is general enough such that a `tf.choice` is not necessary to implement sampled decoding.\n\nHere is a snippet of code from my current project, which I believe corresponds to \"sampled decoding\", as requested here.\n\n``` python\ndef sample_and_embed(choices, prev, i):\n    \"\"\"Loop_function that samples a symbol based on prev logits and embeds it.\"\"\"\n    prev_symbol = tf.multinomial(prev, 1)[:,0]\n    choices.append(prev_symbol)\n    return embedding_ops.embedding_lookup(embedding_table, prev_symbol)\n\nwith vs.variable_scope(\"decoder\"):\n    choices = []\n    loop_function = lambda prev, i: sample_and_embed(choices, prev, i)\n    outputs, states = seq2seq.rnn_decoder([start_token_value] + [None] * (SEQ_LENGTH-1),\n                                initial_state,\n                                cell,\n                                loop_function = loop_function\n                                                 )\n    loop_function(outputs[-1], len(outputs)) # Select a value for the last token!\n```\n\nEdit: removed call to `tf.nn.log_softmax` now that bug in multinomial is fixed\n", "@lukaszkaiser Does `tf.multinomial` obviate `tf.choice`?\n", "I think it does, thanks! With the example above, I think we can close this issue. Great thanks nikitakit!\n"]}, {"number": 367, "title": "Gradient for tf.cholesky", "body": "It would be really nice to have a gradient operation for `tf.cholesky` similar to Theano's [CholeskyGrad](http://deeplearning.net/software/theano/library/tensor/slinalg.html#theano.tensor.slinalg.CholeskyGrad). \n\nI'll be seeing if I can implement the gradient using `tf.control_flow_ops.While` following the suggestion provided [here](http://stackoverflow.com/questions/33962959/cholesky-factor-differentiation-in-tensorflow), but if someone with C++ experience can implement the gradient directly in C++, that'd be great.\n\nThanks!\n", "comments": ["control_flow_ops.While is not part of the public API (hence there is no documentation about it) because it is still not ready to use externally yet.\n", "@yaroslavvb: Were you pondering linear algebra gradients?\n", "Just curious, what is the use-case for differentiable Cholesky?\n", "I'm interested in constructing a differentiable cost function for a Gaussian process (the log marginal likelihood) and wanted to replace the matrix inversion with solving via Cholesky. It'd also be nice to replace log determinant with: `logdetA = 2*sum(log(diag(chol(A))));`\n\nThus the interest in a differentiable Cholesky :P\n", "matrix_inverse was recently changed so it will try to use Cholesky if the matrix is symmetric, in the interest of speed. However for the log determinant, I agree that a Cholesky gradient would be nice to have. \n", "This new paper may be of interest: [Differentiation of the Cholesky decomposition](http://arxiv.org/abs/1602.07527)\n", "Myself and James Hensman have implemented the method from the reference paper by Iain murray in TensorFlow on a fork: \n\nhttps://github.com/GPflow/tensorflow/blob/master/tensorflow/core/user_ops/chol_grad.cc\n\n@vrv we're aware it isn't yet up to TensorFlow main repo standards but would you like to discuss getting it into the main repo. \n", "Also we have a fair number of unit tests here:\n\nhttps://github.com/GPflow/tensorflow/blob/master/tensorflow/python/kernel_tests/cholesky_grad_op_test.py\n", "@rmlarsen Would you like to help? \n", "The best way would be to make a pull request. That's the best way for us to see and comment on the changes. It's ok to mark the PR as [WIP]. (see examples on our PR page).\n", "Hi Alex,\n\nSorry for the delayed response. I'd be happy to review your code and help\ngetting it integrated into the TensorFlow core.\n\nRasmus\n\nOn Wed, Mar 2, 2016 at 5:45 AM, Alexander G. de G. Matthews <\nnotifications@github.com> wrote:\n\n> @rmlarsen https://github.com/rmlarsen Would you like to help?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/367#issuecomment-191243120\n> .\n", "OK thanks. I'll put in a pull request.\n", "FYI: Alex's excellent code in PR 1465 was merged today, so you should be able to use cholesky with gradients now. We still need to add support for batch_cholesky, so I'm leaving the bug open for now.\n", "@alexggmatthews said to post here when I had something to show and to tag @rmlarsen and @yaroslavvb - hi guys!\n\nI've been working on making a CUDA version of Alex's code. I now have the following ops implemented in [my fork](https://github.com/c0g/tensorflow/tree/master/tensorflow/core/user_ops):\n- get_diag (get_diag*)\n- tril (triangle*)\n- cholesky (gpu_cholesky*)\n- cholesky_grad (gpu_cholgrad*)\n\nThere is still a lot left to do, but the algorithms are all there and I believe they numerically match Alex's code. I will be adding python wrappers and tests over the coming days, as well as tidying up source files and trying to get it all to build with Bazel.\n\nMeantime, I have a few questions:\n\n### streams\n\nIs it correct to call standard CUDA kernels and operations on the op's stream by launching with `context->eigen_device<Device>().stream()`?\n\n### stream.ThenLaunch()\n\nI think there is a way to launch kernels directly on a tensorflow stream - but there's not much information on TypedKernel or Kernel that I can find. Do I need to worry about this?\n\n### cusolver:\n\nI use cusolver for the cholesky op. This isn't yet wrapped in Stream.h so I'm linking/calling it raw. This adds a dependency on linking cusolver into the framework, and maybe wrapping it as you guys have wrapped cublas.\n\n**edit:** I will also work on reflecting the style/formatting recommendations made on Alex's code.\n", "@RuiShu (and others): are you aware that `grad(logdet(A)) = transpose(inv(A))`? Unless it's actually faster to compute the grad of the cholesky decomp (and to the other operations), then IMO it might make sense to use the cholesky decomp in getting the _value_ of the logdet and to use the inverse-transpose form for the gradient?\n", "@harpone It's been a while since I've checked up on this issue. Generally, it'd be nice to not compute the inverse explicitly. Since I was primarily interested in logdets within the context of gaussian processes, there are ways of getting around it (such as something that ultimately requires something like the cholesky decomp in the backwards pass as well :P)\n\nIn any case, I was under the impression that Alex's contribution (as well as his work in GPFlow in general) has effectively solved the issue. \n", "Are there any plans to make the tf.matrix_determinant() GPU op? Or is it possible to implement it using existing GPU ops?\n", "If the matrix is not positive definite, how to implement logdet(A) and its gradient without calculating det(A) explicit? (Because det(A) itself can overflow easily.)", "@EverettYou as for the numerical value, you could now use `s, u, v = tf.svd(M)`, where s is an array of the singular values (which are eigenvalues of M since M is square). Then log det(M) = log det(P S P^{-1}) = log det(S) = log(prod_i(s[i])) = sum_i log(s[i]), i.e. just take log of s and sum. Note that since M is not necessarily pos def, you'd actually need log *abs* det so just take the abs of s first. The gradient is the same, since log(-1) is a constant (imaginary number but a constant nevertheless).\r\n\r\nI don't know about the status of the gradient... haven't thought about this in a while. You _could_ define the gradient by RegisterGradient as `grad(logdet(M)) = transpose(inv(M))`, but as @RuiShu mentioned, it would be nice to avoid taking the inverse, at least if the matrix is big..."]}, {"number": 366, "title": "Missing graph_vis_animation.gif image in http://www.tensorflow.org/how_tos/graph_viz/index.html", "body": "http://www.tensorflow.org/how_tos/graph_viz/index.html has a bad embedded image link:\n\n```\n<p>\n  <img src=\"./graph_vis_animation.gif\" \n   alt=\"Visualization of a TensorFlow graph\" \n  title=\"Visualization of a TensorFlow graph\" />\n<em>Visualization of a TensorFlow graph.</em></p>\n```\n\nNearby: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/how_tos/graph_viz (and https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/how_tos/graph_viz/ ) are missing the file. \n", "comments": ["Fix should be up shortly!\n", "... and we're fixed.\n"]}]