[{"number": 35486, "title": "Remove function FreeAndMaybeCoalesce", "body": "Remove function FreeAndMaybeCoalesce,the function is not realized or used in bfc_allocator.cc", "comments": ["@jaingaurav it has been stuck here for a while, please take time to have a look."]}, {"number": 35485, "title": "apply_gradients Error after do weight modification", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI want to do some modification of gradient, weight separately. When I do modification of gradient, it works, but when I use similar code to do modification of weight, it does not work.\r\n\r\n```python\r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n\r\ntrain_op = tf.train.GradientDescentOptimizer(0.01)\r\nw = tf.trainable_variables()[0]\r\ngrads_vars = train_op.compute_gradients(cross_entropy, var_list=[w])\r\nresidual = tf.zeros([784, 10])\r\n\r\nfor i, (g, v) in enumerate(grads_vars):\r\n    import pdb; pdb.set_trace()\r\n    # if len(g.shape) == 2:\r\n    #     g_size = 1\r\n    #     g_shape = g.get_shape().as_list()\r\n    #     for j in g_shape:\r\n    #         g_size *= j\r\n    #     top_k_size = int(g_size * 0.1)\r\n    #     g = tf.add(g, residual)\r\n    #     g_top_k = get_top_k(g, top_k_size)\r\n    #     residual = tf.subtract(g, g_top_k)\r\n    #     grads_vars[i] = (g_top_k, v)\r\n    if len(v.shape) == 2:\r\n        v_size = 1\r\n        v_shape = v.get_shape().as_list()\r\n        for j in v_shape:\r\n            v_size *= j\r\n        top_k_size = int(v_size * 0.1)\r\n        grads_vars[i] = (g, get_top_k(v, top_k_size))\r\n\r\nopt = train_op.apply_gradients(grads_vars)\r\n```\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/usu_hu_lab/Documents/tfv1/mnist_train.py\", line 52, in <module>\r\n    if len(v.shape) == 2:\r\n  File \"/usr/local/anaconda3/envs/tfv1/lib/python3.7/site-packages/tensorflow_core/python/training/optimizer.py\", line 614, in apply_gradients\r\n    update_ops.append(processor.update_op(self, grad))\r\n  File \"/usr/local/anaconda3/envs/tfv1/lib/python3.7/site-packages/tensorflow_core/python/training/optimizer.py\", line 194, in update_op\r\n    raise NotImplementedError(\"Trying to update a Tensor \", self._v)\r\nNotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'Select:0' shape=(784, 10) dtype=float32>)\r\n```\r\n**Describe the expected behavior**\r\nActually, when I use pdb to debug the v, it is a variable, it should be updated.\r\n```python\r\n(Pdb) p v\r\n<tf.Variable 'Variable:0' shape=(784, 10) dtype=float32_ref>\r\n(Pdb) p type(v)\r\n<class 'tensorflow.python.ops.variables.RefVariable'>\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@marxwolf \r\n\r\nLooks like code is incomplete. Request you to provide colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "This is the full code.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow_core.examples.tutorials.mnist import input_data\r\n\r\ndef get_top_k(t, k):\r\n\tif len(t.shape) == 1:\r\n\t\tvalues, _ = tf.nn.top_k(t, k)\r\n\telif len(t.shape) == 2:\r\n\t\tflatten_t = tf.reshape(t, [-1])\r\n\t\tvalues, _ = tf.nn.top_k(flatten_t, k)\r\n\t\r\n\tthreshold = values[-1]\r\n\r\n\tmask = tf.greater_equal(t, threshold)\r\n\tzeros = tf.zeros_like(t)\r\n\ttop_k = tf.where(mask, t, zeros)\r\n\r\n\treturn top_k\r\n\r\n\r\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\r\n\r\nx = tf.placeholder(tf.float32, [None, 784])\r\n\r\nW1 = tf.Variable(tf.zeros([784, 10]))\r\n\r\nb1 = tf.Variable(tf.zeros([10]))\r\n\r\ny = tf.nn.softmax(tf.matmul(x, W1) + b1)\r\n\r\ny_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n\r\ntrain_op = tf.train.GradientDescentOptimizer(0.01)\r\nw = tf.trainable_variables()[0]\r\ngrads_vars = train_op.compute_gradients(cross_entropy, var_list=[w])\r\n# residual = tf.zeros([784, 10])\r\n\r\nfor i, (g, v) in enumerate(grads_vars):\r\n    # import pdb; pdb.set_trace()\r\n    # if len(g.shape) == 2:\r\n    #     g_size = 1\r\n    #     g_shape = g.get_shape().as_list()\r\n    #     for j in g_shape:\r\n    #         g_size *= j\r\n    #     top_k_size = int(g_size * 0.1)\r\n    #     grads_vars[i] = (get_top_k(g, top_k_size), v)\r\n        # g = tf.add(g, residual)\r\n        # g_top_k = get_top_k(g, top_k_size)\r\n        # residual = tf.subtract(g, g_top_k)\r\n        # grads_vars[i] = (g_top_k, v)\r\n    if len(v.shape) == 2:\r\n        v_size = 1\r\n        v_shape = v.get_shape().as_list()\r\n        for j in v_shape:\r\n            v_size *= j\r\n        top_k_size = int(v_size * 0.1)\r\n        grads_vars[i] = (g, get_top_k(v, top_k_size))\r\n\r\nopt = train_op.apply_gradients(grads_vars)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    for i in range(1000):\r\n        x_batch, y_batch = mnist.train.next_batch(100)\r\n        _, gradvar, loss = sess.run([opt, grads_vars, cross_entropy], {x: x_batch, y_: y_batch})\r\n        # import pdb; pdb.set_trace()\r\n        \r\n        if i % 100 == 0:\r\n            print(\"The %s-th steps, loss = %f\" % (i, loss))\r\n\r\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n    print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\r\n```", "I have tried on colab with TF version 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/517c5e9702ebff15b9c6359920da90c8/untitled526.ipynb). Thanks!", "@gowthamkpr When will you help to fix this problem?", "Similar ti the issue #21134. As @rohan100jain mentioned We can only compute gradients wrt Variables not tensors.", "@marxwolf Are you still facing the same issue?", "Closing this issue as it has been inactive for more thank 2 weeks. Please add additions comments and we can open the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35485\">No</a>\n"]}, {"number": 35484, "title": "variational autoencoder code sample error in TF2.0", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe code given in the guide does not run if the latent dimension is set 1, it runs fine for every latent dimension >1!\r\n\r\n### Clear description\r\n\r\nwhen the model is built and trained with code:\r\n```\r\nvae = VariationalAutoEncoder(784, 64, 1)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n```\r\nit throws the error:\r\n\r\n**ValueError: The last dimension of the inputs to Dense should be defined. Found None**\r\n\r\n### Correct links\r\n\r\nn/a\r\n\r\n### Parameters defined\r\n\r\nlatent_dim = 1\r\n(vae = VariationalAutoEncoder(784, 64, 1) )\r\n\r\n### Returns defined\r\n\r\nn/a\r\n\r\n### Raises listed and defined\r\n\r\n**ValueError: The last dimension of the inputs to Dense should be defined. Found None**\r\n\r\n### Usage example\r\n\r\ncode in the guide:\r\n\r\n```\r\nclass Sampling(layers.Layer):\r\n  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\r\n\r\n  def call(self, inputs):\r\n    z_mean, z_log_var = inputs\r\n    batch = tf.shape(z_mean)[0]\r\n    dim = tf.shape(z_mean)[1]\r\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n\r\n\r\nclass Encoder(layers.Layer):\r\n  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\r\n\r\n  def __init__(self,\r\n               latent_dim=32,\r\n               intermediate_dim=64,\r\n               name='encoder',\r\n               **kwargs):\r\n    super(Encoder, self).__init__(name=name, **kwargs)\r\n    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n    self.dense_mean = layers.Dense(latent_dim)\r\n    self.dense_log_var = layers.Dense(latent_dim)\r\n    self.sampling = Sampling()\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_proj(inputs)\r\n    z_mean = self.dense_mean(x)\r\n    z_log_var = self.dense_log_var(x)\r\n    z = self.sampling((z_mean, z_log_var))\r\n    return z_mean, z_log_var, z\r\n\r\n\r\nclass Decoder(layers.Layer):\r\n  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\r\n\r\n  def __init__(self,\r\n               original_dim,\r\n               intermediate_dim=64,\r\n               name='decoder',\r\n               **kwargs):\r\n    super(Decoder, self).__init__(name=name, **kwargs)\r\n    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n    self.dense_output = layers.Dense(original_dim, activation='sigmoid')\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_proj(inputs)\r\n    return self.dense_output(x)\r\n\r\n\r\nclass VariationalAutoEncoder(tf.keras.Model):\r\n  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n\r\n  def __init__(self,\r\n               original_dim,\r\n               intermediate_dim=64,\r\n               latent_dim=32,\r\n               name='autoencoder',\r\n               **kwargs):\r\n    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\r\n    self.original_dim = original_dim\r\n    self.encoder = Encoder(latent_dim=latent_dim,\r\n                           intermediate_dim=intermediate_dim)\r\n    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\r\n\r\n  def call(self, inputs):\r\n    z_mean, z_log_var, z = self.encoder(inputs)\r\n    reconstructed = self.decoder(z)\r\n    # Add KL divergence regularization loss.\r\n    kl_loss = - 0.5 * tf.reduce_mean(\r\n        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\r\n    self.add_loss(kl_loss)\r\n    return reconstructed\r\n\r\n\r\n\r\nvae = VariationalAutoEncoder(784, 64, 1)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n```\r\n### Request visuals, if applicable\r\n\r\nn/a\r\n\r\n### Submit a pull request?\r\n\r\nn/a\r\n", "comments": ["I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/421ce6dadb234bbe06a11cbec1f6f94a/untitled518.ipynb). Thanks!", "Ok Issue solved please see [post](https://github.com/tensorflow/tensorflow/issues/35464#issuecomment-572177889) for details."]}, {"number": 35483, "title": "High percentage of CPU usage in ssdlite_mobilenet model ", "body": "Hi all,\r\nI trained a model for license plate detection with ssdlite_mobilenet_v2_coco (training images:20000,steps:200000) and this model is going to be used in a traffic control app.\r\n\r\nThe accuracy of this model on frames of video is very good but the percentage of CPU usage on my system is between 70-80% (CPU info : Intel Core i7-6500 @ 2.50GHz.) and given that traffic control app should be constantly running on client system, this percentage is quite high.\r\n\r\nSo what are the best ways to optimize CPU usage to analyze frames of video and detect license plate? (reduce to ~30-40%)\r\nDo I need to make any changes to the config file or use another pre-trained model?\r\nPlease guide me :)", "comments": ["You may try optimizing your model using quantization, or use quantized mobilenet v2 model.\r\nSee https://www.tensorflow.org/lite/performance/best_practices#optimize_your_model"]}, {"number": 35482, "title": "non_max_suppression is full of bugs!", "body": "**System information**\r\nThe bugs are not related to my system informations. They are caused by a bad coding style in the https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cu.cc and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cc. Just to be sure those bugs could be easily reproduced on google colab. \r\n\r\n**Describe the current behavior**\r\nI ll mention two major bugs.\r\nThe first one is a garbage output when the boxes coordinates are not logical. In fact given a box(y1,x1,y2,x2)  coordinates with y1 = y2 or x1 = x2 (the coordinates of a line) or even worst the coordinates of a point ( x1 = x2 = y1 = y2) the algorithme will only output the line when reaching it. \r\n**Code to reproduce the issue**\r\nHere is the code to reproduce the behaviour on google colab: \r\n\r\n```\r\nimport numpy as np\r\n%tensorflow_version 2.x\r\nimport tensorflow as tf\r\ntf.__version__\r\nboxes = np.array([[0.1,0.1,0.2,0.2], [0.3,0.3,0.3,0.4], [0.5,0.5,0.6,0.6], [0.7,0.7,0.8,0.8]], dtype= np.float32)\r\nscores = np.array([0.9,0.8,0.7,0.6], dtype = np.float32)\r\n(tf.image.non_max_suppression(boxes, scores, 8))\r\n```\r\n\r\noutput:\r\n```\r\n'2.1.0-rc1'\r\n<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>\r\n```\r\n\r\nNow I ll explain brievely how the algorithm is coded in tensorflow. Given a list of candidate boxes containing in the beginning all the user boxes and a list of chosen boxes empty, if the box is chosen it will not immediately delete it from the candidate box. In fact, this box will be again processed as a candidate box in the next iteration. But because it is already in the chosen boxes it wont be chosen again . The reason for that is that the IOU of a box with himself is 1 which is always above the threshold. Unfortunately, the IOU of a line or a point with any box is 0. This is applied even when the IOU is calculated of the line with iteself. This will result in adding the line to the chosen boxes again and again. This behaviour is mentioned in this issue but wasn't clearly explained. https://github.com/tensorflow/tensorflow/issues/29628\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour must be decided by the tensorflow programer. He can chose between putting it only once in the result : \r\n-<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 2, 3], dtype=int32)> \r\nor deleting the line box\r\n -<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 2, 3], dtype=int32)>\r\n\r\n- The second bug is really inexplainable. Why there is only the gpu specialisation of non_max_suppression_v2 ? Did the developer forgot about it? This was mentioned in several issues under the name : non max suppression work only on cpu. This is completely understandable because the default version of non_max_suppression is v3 which dosent have a gpu specialisation. \r\n\r\n**Code to reproduce the issue**\r\non colab u can just copy this code:\r\n\r\n```\r\nimport numpy as np\r\n%tensorflow_version 2.x\r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\nboxes = np.array([[0.1,0.1,0.2,0.2], [0.3,0.3,0.3,0.4], [0.5,0.5,0.6,0.6], [0.7,0.7,0.8,0.8]], dtype= np.float32)\r\nscores = np.array([0.9,0.8,0.7,0.6], dtype = np.float32)\r\nwith tf.device('/GPU:0'):\r\n  print(tf.image.non_max_suppression(boxes, scores, 8))\r\n```\r\n\r\noutput:\r\n\r\n> Executing op NonMaxSuppressionV3 in device /job:localhost/replica:0/task:0/device:CPU:0\r\n> tf.Tensor([0 1 1 1 1 1 1 1], shape=(8,), dtype=int32)\r\n\r\n**Describe the expected behavior**\r\n\r\n> Executing op NonMaxSuppressionV3 in device /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\nCorrecting this issue is quite simple and straightforward. Gpu specialisation must be made for non_max_suppression_v3 at least.", "comments": ["Issue is replicating on colab with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/2cf976ada8376c28056285c958785c97/untitled331.ipynb). Thanks!", "@MoussaMM NMS is defined on boxes(2D and higher) and not lines(1D) or points(0D). I don't think there is any issue with the code there. I believe your inputs are not valid inputs. Please point me to the reference where NMS is defined on point or lines. Otherwise this can be closed.", "@samikama thank you a lot for your reply. Yeah you have a valid point, the operation is not designed for anything but boxes. But even if it s not defined to points or lines returning a completely unexpected results for a wrong input is an issue. That is why I made a pull request to raise an error when entering wrong coordinates : https://github.com/tensorflow/tensorflow/pull/35544 . Probably it is not an important problem in most cases but imagine that this operation will take its input from a not controlled neural network. This will make the user completely lost.\r\nIn addition, the gpu specialization are commented in the source code and were not attended since. I guess the user won't understand that this operation doesn't have a gpu specialization. Until then I wish that this issue remains opened.", "@MoussaMM,\r\nIs this still an issue?\r\n\r\nOn running the code with the latest stable version of TensorFlow v2.4.1, I got the 'expected behavior'. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c3a25e8b35823a178d85f378a6de9ac3/35482.ipynb). Thanks!", "Yes I think the issue is solved. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35482\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35482\">No</a>\n"]}, {"number": 35481, "title": "Add usage example to tf.image.draw_bounding_boxes", "body": "Add an example to `tf.image.draw_bounding_boxes` showing how to draw a bounding box in the center of an image.", "comments": ["Thanks for looking over the PR! I've made the changes."]}, {"number": 35480, "title": "tridiagonal_solve: Remove stale forward compatibility checks", "body": "`forward_compatible(2019, 10, 18)` always evaluates to `True` so a bit of stale code can be removed.", "comments": []}, {"number": 35479, "title": "Parsing: Remove stale forward compatibility checks", "body": "`compat.forward_compatible(2019, 10, 26)` always evaluates to `True` so a bit of stale code can be removed.", "comments": []}, {"number": 35478, "title": "Einsum: Remove stale forward compatibility checks", "body": "`compat.forward_compatible(2019, 10, 18)` always evaluates to `True` so quite a bit of stale code can be removed.", "comments": ["@lgeiger  Can you please resolve conflicts? Thanks!", "> @lgeiger Can you please resolve conflicts? Thanks!\r\n\r\nf75c37faf347dd926cf0e999f03dbd7818a77811 pushed the same changes, so this PR is now obsolete..."]}, {"number": 35477, "title": "SequenceEnqueuer doesn't work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n`SequenceEnqueuer` object takes a `Sequence` object which then can be used with multi-processing for fast data pipeline. But as soon as you start an `enqueuer`, it throws `NotImplemented` error.\r\n\r\n**Describe the expected behavior**\r\nIt should produce batches of data using multiprocessing.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n!pip install tensorflow\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.utils import Sequence, to_categorical, SequenceEnqueuer\r\n\r\nclass DataGenerator(Sequence):\r\n    def __init__(self, batch_size=32):\r\n        self.batch_size = batch_size\r\n        self.indices = np.arange(1024)\r\n\r\n    def __len__(self):\r\n        return 1024\r\n    \r\n    def __getitem__(self, idx):\r\n        x = np.random.rand(self.batch_size, 32, 32, 3).astype(np.float32)\r\n        y = np.random.randint(10, size=(self.batch_size))\r\n        return x, y\r\n\r\nds = DataGenerator(batch_size=32)\r\nenqueuer = SequenceEnqueuer(ds, use_multiprocessing=True)\r\nenqueuer.start(workers=2)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nNotImplementedError                       Traceback (most recent call last)\r\n\r\n<ipython-input-13-fd99ac8a9d30> in <module>()\r\n      1 enqueuer = SequenceEnqueuer(ds, use_multiprocessing=True)\r\n----> 2 enqueuer.start(workers=2)\r\n\r\n1 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py in start(self, workers, max_queue_size)\r\n    727     \"\"\"\r\n    728     if self.use_multiprocessing:\r\n--> 729       self.executor_fn = self._get_executor_init(workers)\r\n    730     else:\r\n    731       # We do not need the init since it's threads.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py in _get_executor_init(self, workers)\r\n    778         Function, a Function to initialize the pool\r\n    779     \"\"\"\r\n--> 780     raise NotImplementedError\r\n    781 \r\n    782   @abstractmethod\r\n\r\nNotImplementedError: \r\n```", "comments": ["cc: @fchollet @qlzh727 ", "I have tried on colab with TF version 2.0, 2.1.0-dev20191229 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/8ab18eb35dc9b0ff7d16572f966267dd/untitled513.ipynb). Thanks!", "Sequence is an abstract class that is intended to be subclassed, as seen here: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence?version=stable#examples_2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35477\">No</a>\n", "@karmel I think you are mixing `Sequence` with `SequenceEnqueuer`. The later should take an instance of `Sequence` and should generate batches of data. ", "@AakashKumarNain Did you find a solution for this?"]}, {"number": 35476, "title": "Usage example for tf.linalg.lu()", "body": "", "comments": ["@yashk2810 can the examples in `pbtxt` files also be used as doctests? If yes, I assume they should follow the same guidelines, right?", "Yup :)", "@abhayKashyap03 please follow the guidelines in https://www.tensorflow.org/community/contribute/docs_ref", "@abhayKashyap03 Could you please check reviewer comments and keep us posted. Thanks!", "Hey, really really sorry for the delay, I've been busy with other code-in tasks and had exams the entire month, so didn't have any time left at all.\r\nWould it be ok if I resume working on this by the end of this month?\r\nI have my exams till 29th of this month and I can start working on my PRs from then.", "Sure, though in that time someone else might implement this.", "Sure, if someone else is interested, let them resume this, I'm willing to share the code if needed. I don't want to delay this, so if this can continue in any way, I'm more than happy to support.", "@mihaimaruseac I thought that if someone opened a pull request to solve something it be bad manners to copy it.", "Or you could close this PR, I'll open a new one when I get some free time and continue working on this.", "If somebody else hasn't already done it", "@abhayKashyap03 I dont see why you cant leave this open? It might reduce the chances of someone else implementing these changes before you get your freetime...", "Sure, no problem\r\nIf it's ok with the admins to leave it open for some more time, then I have no problem.\r\nI thought it might be a problem for the admins if it's open for too long, it's been open for long enough already.\r\nI'll start working on it in some time if it remains open.", "> @mihaimaruseac I thought that if someone opened a pull request to solve something it be bad manners to copy it.\r\n\r\nIf it is plagiarism yes. But if someone opens a PR but doesn't continue with it someone else can open a new one and credit the work done initially, if it is based on that.\r\n\r\nOtherwise, someone could open PRs for all the issues and refuse to do anything with them blocking all further development.", "oh", "Hey, I'm really sorry for all the delay and problems I've caused, I've been very busy with my exams and events at my school and I don't think I'll be able to continue my work on the PR.\r\nYou can close the PR so that you won't have a 'pending' PR and others can continue the work, if anybody is.\r\nI'm really sorry again for all the troubles.", "Closing as requested."]}, {"number": 35475, "title": "Usage example for tf.linalg.det()", "body": "", "comments": ["@yashk2810 can the examples in `pbtxt` files also be used as doctests? If yes, I assume they should follow the same guidelines, right?", "> files also be used as doctests\r\n\r\nYup.\r\n\r\n> assume they should follow the same guidelines, right?\r\n\r\nYes", "@abhayKashyap03 please follow the guidelines in https://www.tensorflow.org/community/contribute/docs_ref", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@abhayKashyap03 Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "Hey, I'm really sorry for all the delay and problems I've caused, I've been very busy with my exams and events at my school and I don't think I'll be able to continue my work on the PR.\r\nYou can close the PR so that you won't have a 'pending' PR and others can continue the work, if anybody is.\r\nI'm really sorry again for all the troubles.", "Closing as requested."]}, {"number": 35474, "title": "Usage example for tf.linalg.lu()", "body": "", "comments": []}, {"number": 35473, "title": "linux aarch64 nested, exception is java.lang.UnsatisfiedLinkError", "body": "Handler dispatch failed; nested exception is java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: linux, architecture: aarch64.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@TuringZhai \r\n\r\nAny update on the issue please. Thanks!"]}, {"number": 35472, "title": "DCGANs Tutorial's BatchNormalization is maybe something wrong", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/generative/dcgan\r\n\r\n## Description of issue (what needs changing):\r\n\r\nhttps://colab.research.google.com/gist/MokkeMeguru/614e16d83d16f1eb70b5f3b73c7d070b/batchnormalization_debug.ipynb\r\n\r\nIn you tutorial, BatchNormalization will be Actnormalization in Glow(https://arxiv.org/abs/1807.03039)\r\n\r\n### Clear description\r\n\r\nWe need Correct BathNormalization\r\n\r\n### Usage example\r\n\r\nWe should input the shape when   BatchNormalization is initialized", "comments": ["Sorry, my misunderstanding about batch normalization."]}, {"number": 35471, "title": "Using Conda, The only packageURL can not work", "body": "Hi, I am using macOS and Conda to create a virtual environment, but when I install TensorFlow pip package, I found the **only packageURL**(https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.0.0-py2-none-any.whl) **for macOS can not work or even download**. Would you mind telling me a solution if anyone solve this problem. Thank you som much!\r\n", "comments": []}, {"number": 35470, "title": "Update .bazelversion", "body": "This changes the .bazelverison file to be consistent with Tensorflow CI at 2.1.0. The current bazelversion is inconsistent with the bazel used in CI for 2.1.0, see [install_bazel.sh](https://github.com/tensorflow/tensorflow/blob/v2.1.0-rc2/tensorflow/tools/ci_build/install/install_bazel.sh#L18)\r\n\r\nThis allows clients that use [bazelisk](https://github.com/bazelbuild/bazelisk#bazelisk) to automatically build with the right version of bazel.\r\n\r\nHopefully this can be cherrypicked into 2.1.0", "comments": []}, {"number": 35469, "title": "LayernormSimpleRNN added", "body": "tf.keras.experimental.LayernormSimpleRNN is based on tf.keras.layers.SimpleRNN and can apply tf.keras.layers.LayerNormalization instead of bias before the activation function of the recurrent kernel. \r\n\r\nLayernormSimpleRNN is a proposal how to extend SimpleRNN (and other RNN layers) with layer normalization. ", "comments": ["if `LayernormSimpleRNN(..., use_bias=True, use_layernorm=False,...)` then it behaves like `SimpleRNN(..., use_bias=True, ...)`\r\n\r\n```\r\na_t = input_t * W_xh + hidden_{t-1} * W_hh\r\nhidden_t = activation(a_t + bias)\r\n```\r\n\r\nif `LayernormSimpleRNN(..., use_bias=True, use_layernorm=True,...)` then bias will be disabled (`use_bias=False`) and `LayerNormalization` (by @CyberZHG) applied\r\n\r\n```\r\na_t = input_t * W_xh + hidden_{t-1} * W_hh\r\nnormalized_t = gamma / std_t * (a_t - mu_t) + beta\r\nhidden_t = activation(normalized_t)\r\n```\r\n\r\nPlease refer to page 3, formula 4 in Ba et al (2016), https://arxiv.org/abs/1607.06450\r\n\r\nIn other words, `LayernormSimpleRNN` only allows `use_bias=True` or `use_layernorm` as both together doesn't make sense. All these `bias_regulizer`, `bias_initializer`, etc. are used for `beta_...` in `LayernormSimpleRNN`  (`beta` is the bias in layer normalization; I hope it doesn't sound confusing).\r\n\r\n", "Thanks for opening this PR and sorry for the late reply.\r\n\r\nThe current code has a lot of duplication with the SimpleRNN/Cell, and only a small portion is the layer norm related. Since layer norm is quite orthogonal with RNN, can we take the python mixin class approach to the new class? eg SimpleRNNLayerNorm is a mixin of (SimpleRNN, layerNorm), where layernorm will just add some logic to the call() and get_config().\r\n\r\nAlso, I would like to add Francois as reviewer, since I am not quite sure to accept this as a core keras API. Maybe this is more suitable for tensorflow/addons.", "sure, mixin makes a lot of sense. I did not want use mixin from the start as it might obfuscate things. Do you want to make the changes?\r\n", "Please make the change in this PR. In case that we move it to tensorflow/addons, you can just copy the content and move it to the addons repository.", "Inheriting or mixin does not work, e.g. `class LayernormSimpleRNNCell(SimpleRNNCell):` or `class LayernormSimpleRNNCell(SimpleRNNCell, LayerNormalization):`. When calling the parent constructor it throws `TypeError: ('Keyword argument not understood:', 'activation')`", "The error is usually caused when you pass all the param from SimpleRNNCell to LayerNormalization. In LayernormSimpleRNNCell, what you can do is selectively passing params to __init__ SimpleRNNCell and LayerNormalization respectively.\r\n\r\nYou can google python MRO init for more details.", "Thanks about the inheritance issue. The code is refactored using the mixin approch \r\n\r\n- `class LayernormSimpleRNNCell(SimpleRNNCell, LayerNormalization):` and\r\n- `class LayernormSimpleRNN(SimpleRNN):`\r\n\r\nYou mentionend, it would be better to move the code to addon. I just saw that you already the maintainer for a LSTM with layer normalization in https://github.com/tensorflow/addons/tree/master/tensorflow_addons/rnn\r\n", "Yea, since we already had one in Addon, it makes more sense to have this one in addon.\r\n\r\nAlso, thinking about about the MRO here, maybe it makes more sense to change the LayernormSimpleRNNcell to HAVE a layernorm layer, rather than BE a layernorm layer, which is more aligned with the existing code in LayernormLSTMcell.", "Hello @qlzh727 I tried `class LayernormSimpleRNNCell(SimpleRNNCell):` before\r\n\r\nhttps://gist.github.com/ulf1/d36051d0a91073f974c9e8b082865657#file-script4-py\r\n\r\nThe difference to `class LayernormSimpleRNNCell(SimpleRNNCell, LayerNormalization):` are not much. \r\n\r\nin `__init__`\r\n\r\n- `self.layernorm = LayerNormalization(...)` versus\r\n- `LayerNormalization.__init__(...)`\r\n\r\nin `build` an extra call is required\r\n\r\n```\r\nif self.use_layernorm:\r\n      LayerNormalization.build(self, (None, self.units))\r\n```\r\n\r\nin `call`\r\n\r\n- `output = self.layernorm(output)`, versus\r\n- `output = LayerNormalization.call(self, output)`\r\n\r\nin `config`\r\n\r\n- `ln_config = self.layernorm.get_config()`, versus\r\n- `ln_config = LayerNormalization.get_config(self)`\r\n\r\nWe could also try a `LayernormMixin` for the adjustments in `get_config` (see `ln_config`) then use `class LayernormSimpleRNNCell(SimpleRNNCell, LayernormMixin):` \r\n\r\n", "I moved the code to addons https://github.com/tensorflow/addons/pull/841\r\nYou can close this PR now", "Ack, closing this PR now."]}, {"number": 35468, "title": "Fix tf.while_loop() first example", "body": "Loop_vars currently is a scalar tensor", "comments": ["After your change the loop will exit immediately instead of running for 10 iterations. We want an actual loop.", "Sorry, my confusion, here's the correct code\r\n\r\ni = tf.constant(0)\r\nc = lambda i: tf.less(i, 10)\r\nb = lambda i: (tf.add(i, 1),)\r\nr = tf.while_loop(c, b, [i])"]}, {"number": 35467, "title": "Fixed first example of tf.while()", "body": "Missing a number", "comments": []}, {"number": 35466, "title": "convolutional_recurrent.py:get_initial_state type mismatch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (pip, tf-nightly-gpu)\r\n- TensorFlow version (use command below): 2.1.0-dev20191228\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla M60 7618MiB\r\n\r\nTraining a model with a ConvLSTM2D layer and float16 (K.set_floatx('float16')) results in a type mismatch. convolutional_recurrent.py:get_initial_state initializes zeros without specifying a type, defaulting to float32.\r\n\r\nFor me, specifying the dtype as that of the function inputs resolved the issue:\r\n```\r\n    initial_state = self.cell.input_conv(initial_state,\r\n                                         array_ops.zeros(tuple(shape)),\r\n                                         padding=self.cell.padding)\r\n```\r\nbecomes\r\n```\r\n    initial_state = self.cell.input_conv(initial_state,\r\n                                         array_ops.zeros(tuple(shape),dtype=inputs.dtype),\r\n                                         padding=self.cell.padding)```\r\n", "comments": ["@dancaselden \r\n\r\nRequest you to provide simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@dancaselden \r\n\r\nAny update on this issue please. Thanks!", "@ravikyram here's an example, adapted to tensorflow-keras from the keras repo. The key change is K.set_floatx('float16'):\r\nhttps://gist.github.com/dancaselden/fe4c09d5d9c322273320e898ce075937", "@dancaselden Is this still an issue for you? I ran your code with `tf-nightly` and cannot reproduce the error. Can you PTAL at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/0d7653afc035fa77778921bdeaa3c91a/35466.ipynb). Thanks!\r\n\r\nPlease verify and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan with python 3.6 and tf-nightly-gpu on June 5 2020, this issue does not occur. closing!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35466\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35466\">No</a>\n"]}, {"number": 35464, "title": "Sample VAE code throws error: \"ValueError: The last dimension of the inputs to Dense should be defined. Found None\"", "body": "I am very new to TF2 and tried to customize the example code on the tensorflow guide documentation:\r\n\r\nhttps://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example\r\n\r\nThe code given in the guide does not run if the latent dimension is set 1, it runs fine for every latent dimension >1!\r\n\r\nFor training I tried to use the code given in the example but set the latent dim to 1:\r\n```\r\nvae = VariationalAutoEncoder(784, 64, 1)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n```\r\n\r\nThe error when trying to train is:\r\n\r\n**ValueError: The last dimension of the inputs to Dense should be defined. Found None** and is thrown upon return from the Sample function where I think:\r\n```\r\nepsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n```\r\ncan not handle shape=(?,1).\r\n\r\nCan someone help I am trying to use the code as a template but I need latent dimension to be 1!\r\n\r\nThanks", "comments": ["tf.keras and just keras has the different performance instances thus making that the tf being compatible for all the version is a different deal and thus we have to have a version that satisfies just read the documentation and study you will get it done.", "@ZoltanRado ,\r\nFor the tutorial mentioned I was able to run the code `latent_dim=1 `without any error, can you please provide link gist of colab used ?Thanks!", "Here is a gist produced by [ravikyram](https://github.com/ravikyram) on colab with TF version 2.0 and was able to reproduce the issue.Please, find the [gist](https://colab.sandbox.google.com/gist/ravikyram/421ce6dadb234bbe06a11cbec1f6f94a/untitled518.ipynb) here.\r\n\r\nI am working on TF2.0.0 with Anaconda on my machine.\r\n\r\nThanks", "Here is a gist produced by [ravikyram](https://github.com/ravikyram) on colab with TF version 2.0 and was able to reproduce the issue. Please, find the [gist](https://colab.sandbox.google.com/gist/ravikyram/421ce6dadb234bbe06a11cbec1f6f94a/untitled518.ipynb) here.  Here is his post to another thread on this: #35484 \r\n\r\nI am working on TF2.0.0 with Anaconda on my machine.\r\n\r\nThanks", "Here is the code:\r\n```\r\nclass Sampling(layers.Layer):\r\n  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\r\n\r\n  def call(self, inputs):\r\n    z_mean, z_log_var = inputs\r\n    batch = tf.shape(z_mean)[0]\r\n    dim = tf.shape(z_mean)[1]\r\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n\r\n\r\nclass Encoder(layers.Layer):\r\n  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\r\n\r\n  def __init__(self,\r\n               latent_dim=32,\r\n               intermediate_dim=64,\r\n               name='encoder',\r\n               **kwargs):\r\n    super(Encoder, self).__init__(name=name, **kwargs)\r\n    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n    self.dense_mean = layers.Dense(latent_dim)\r\n    self.dense_log_var = layers.Dense(latent_dim)\r\n    self.sampling = Sampling()\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_proj(inputs)\r\n    z_mean = self.dense_mean(x)\r\n    z_log_var = self.dense_log_var(x)\r\n    z = self.sampling((z_mean, z_log_var))\r\n    return z_mean, z_log_var, z\r\n\r\n\r\nclass Decoder(layers.Layer):\r\n  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\r\n\r\n  def __init__(self,\r\n               original_dim,\r\n               intermediate_dim=64,\r\n               name='decoder',\r\n               **kwargs):\r\n    super(Decoder, self).__init__(name=name, **kwargs)\r\n    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\r\n    self.dense_output = layers.Dense(original_dim, activation='sigmoid')\r\n\r\n  def call(self, inputs):\r\n    x = self.dense_proj(inputs)\r\n    return self.dense_output(x)\r\n\r\n\r\nclass VariationalAutoEncoder(tf.keras.Model):\r\n  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n\r\n  def __init__(self,\r\n               original_dim,\r\n               intermediate_dim=64,\r\n               latent_dim=32,\r\n               name='autoencoder',\r\n               **kwargs):\r\n    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\r\n    self.original_dim = original_dim\r\n    self.encoder = Encoder(latent_dim=latent_dim,\r\n                           intermediate_dim=intermediate_dim)\r\n    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\r\n\r\n  def call(self, inputs):\r\n    z_mean, z_log_var, z = self.encoder(inputs)\r\n    reconstructed = self.decoder(z)\r\n    # Add KL divergence regularization loss.\r\n    kl_loss = - 0.5 * tf.reduce_mean(\r\n        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\r\n    self.add_loss(kl_loss)\r\n    return reconstructed\r\n\r\n\r\n\r\nvae = VariationalAutoEncoder(784, 64, 1)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n```", "I have produced it too (my first gist :-) ) [here](https://colab.research.google.com/gist/ZoltanRado/a599188734f3cebacbdc0bfc53e62c50/untitled0.ipynb)\r\n", "Hey @ZoltanRado I am not an auto-encoder specialist, but I've also managed to reproduce a similar error in Colab with GPU acceleration enabled just now. Looping in @lamberta @MarkDaoust @yashk2810, just in case. \r\n\r\nTo save time, the Colab notebook is [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/custom_layers_and_models.ipynb#scrollTo=oDVSVl4Iu8kC) and the official guide is [here](https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example).\r\n\r\n_1) Successful run over 3 epochs, as designed:_\r\n\r\nFirst, import the libraries and run the cell straight after [\"Putting it all together: an end-to-end example\"]((https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example) ) in the \"Custom Layers and Models\" [Keras Guide](https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example):\r\n```\r\nclass Sampling(layers.Layer):\r\n...\r\nclass Encoder(layers.Layer):\r\n...\r\nclass Decoder(layers.Layer):\r\n...\r\nclass VariationalAutoEncoder(tf.keras.Model):\r\n...\r\n```\r\nand change the latent dimension argument to 1 from the default value of 32 in the next cell, such that:\r\n\r\n```python\r\noriginal_dim = 784\r\nvae = VariationalAutoEncoder(original_dim, 64, 1) # replaced 32 with 1 here\r\n...\r\n...\r\n# Iterate over epochs.\r\nfor epoch in range(epochs):\r\n...\r\n```\r\nIf you run it, you hopefully shouldn't see any errors during training:\r\n```\r\nDownloading data from ...\r\n...\r\nStart of epoch 0\r\nstep 0: mean loss = tf.Tensor(0.34020466, shape=(), dtype=float32)\r\n...\r\nStart of epoch 2\r\n...\r\nstep 900: mean loss = tf.Tensor(0.072112255, shape=(), dtype=float32)\r\n```\r\n\r\nFor the reference:\r\n```python\r\nclass VariationalAutoEncoder(tf.keras.Model):\r\n...\r\n  def __init__(self,\r\n               original_dim,\r\n               intermediate_dim=64,\r\n               latent_dim=32,\r\n               name='autoencoder',\r\n               **kwargs):\r\n...\r\n```\r\n\r\n_2) Where the error message appears:_\r\n\r\nFollowing (1) above, as soon as you run the next cell after:\r\n```markdown\r\nNote that since the VAE is subclassing `Model`, it features built-in training loops. So you could also have trained it like this:\r\n```\r\nwith the latent dimension also set at 1 instead of 32:\r\n```python\r\nvae = VariationalAutoEncoder(784, 64, 1) # replaced 32 with 1 here\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n\r\nvae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\r\nvae.fit(x_train, x_train, epochs=3, batch_size=64)\r\n```\r\nyou get:\r\n```\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\r\n```\r\n\r\nIt works if you change the `latent_dim` arg value from 1 to, say, 32 (the default value).", "Yes @8bitmp3 you are correct, I am very new to TF2 but I think there is a bug in \"tf.keras.backend.random_normal(shape=(batch, dim))\"  where it can  not handle correctly shape=(None,1).", "I think the way you have defined is best understood when you explain the\ndata that you have.\n\nOn Sat, 4 Jan 2020, 02:18 Zoltarra, <notifications@github.com> wrote:\n\n> Yes @8bitmp3 <https://github.com/8bitmp3> you are correct, I am very new\n> to TF2 but I think there is a bug in\n> \"tf.keras.backend.random_normal(shape=(batch, dim))\" where it can not\n> handle correctly shape=(None,1).\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35464?email_source=notifications&email_token=ALSXLOW25WXSRBFWRTNHCI3Q36QBZA5CNFSM4KAOU422YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEICBQOA#issuecomment-570693688>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALSXLOVH3LJ63JTMGALAKFDQ36QBZANCNFSM4KAOU42Q>\n> .\n>\n", "Hi @vishwas1234567 i don't fully understand you, the data used is the keras example is the MNIST digits data and is downloaded by the sample code.  That is the data that is used to produce the error.  I have coded different simple encoders with KL divergence and tried to use the same Gaussian-normal error sampling given in the sample\r\n```\r\nclass Sampling(layers.Layer):\r\n```\r\nand got the same error without exception when setting latent dimension to 1!\r\nfor some reason the code in the class:\r\n```\r\nreturn z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n```\r\nreturns a shape=(None,None) when latent dimension is set to 1 while if latent_dim=X (where X>1) it returns shape=(None,X) as one would expect.  I tried to trouble shoot it but I am new so was not very successful but it seem to me that **epsilon** ergo **tf.keras.backend.random_normal(shape=(batch, dim))** is the culprit.", "Hi.\r\n\r\nThe easiest way to debug this is to just add a bunch of print statements, and inspect the shapes.\r\n\r\nThe problem here is the interaction of broadcasting wth the `z_mean + tf.exp(0.5 * z_log_var) * epsilon` line.\r\n\r\nNormally  the last dimension of  z_mean, and z_log_var are known but the last dimension of epsilon is not.\r\n\r\nSince you mul and add epsilon it assumes the two have the same shape.\r\n\r\nWhen they have a last dimension of 1, It thinks you might be boradcasting z_mean and z_log_var with epsilon, it can't tell.\r\n\r\nSo the fix is to tell it that you know the shape of epsilon, and are not broadcastng. Add the following before that line: `epsilon.set_shape(z_mean.shape)`.\r\n\r\nIf you think this edge-case is important, send a PR to update https://github.com/tensorflow/docs/blob/master/site/en/guide/keras/custom_layers_and_models.ipynb.", "Also not for the future that this would be better as a stack-overflow question instead of a github bug.\r\n\r\nHave a nice day.", "Thank you so much @MarkDaoust I appreciate your help.\r\nI read up on broadcasting and it seem it works in TF the same way as in numpy.\r\nAlthough I think this is important ( I am using my custom models for parameter estimation with exclusively one dimensional latent layers) as I am very new I am not going to send a PR.\r\n\r\nI did post it at stack-overflow first for a couple of weeks but got no help!\r\n\r\nThanks again for the help."]}, {"number": 35463, "title": "tf.profiler calculating incorrect flops in mobilenetssd_v2", "body": "Below Script is giving correct result for mobilenetssd_v1 Flops but failing in calculating mobilentssd_V2 flops. Mobilenetssd_v2 flops coming out to be 185 Millions only\r\n```\r\nsess = tf.Session()\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport cv2\r\ngraph = tf.get_default_graph()\r\nimage_np=cv2.imread(\"41566-large.jpg\")\r\nimage=cv2.resize(image_np,(300, 300), interpolation = cv2.INTER_CUBIC)\r\nimage = np.expand_dims(image, axis=0)\r\n#print(image)\r\nfor i in range(1):\r\n    tf.reset_default_graph()\r\n    st='/device:cpu:'+str(i)\r\n    #with tf.device(st):\r\n    with graph.as_default():\r\n        with sess.as_default():\r\n\r\n            #restoring the model\r\n            run_metadata = tf.RunMetadata()\r\n            saver = tf.train.import_meta_graph('ssd_mobilenet_v2_coco_2018_03_29/model.ckpt.meta')\r\n            saver.restore(sess,tf.train.latest_checkpoint('ssd_mobilenet_v2_coco_2018_03_29'))\r\n            ops = tf.get_default_graph().get_operations()\r\n            all_tensor_names = {output.name for op in ops for output in op.outputs}\r\n            tensor_dict = {}\r\n            for key in ['num_detections', 'detection_boxes', 'detection_scores','detection_classes', 'detection_masks']:\r\n                tensor_name = key + ':0'\r\n                if tensor_name in all_tensor_names:\r\n                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\r\n            if 'detection_masks' in tensor_dict:\r\n                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\r\n                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\r\n                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\r\n                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\r\n                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\r\n                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\r\n                    detection_masks, detection_boxes, image.shape[1], image.shape[2])\r\n                detection_masks_reframed = tf.cast(\r\n                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\r\n                tensor_dict['detection_masks'] = tf.expand_dims(\r\n                    detection_masks_reframed, 0)\r\n            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\r\n#                 # Run inference\r\n            opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()\r\n            for i in range(1):\r\n                print(\"Inference Flag\")\r\n                start=time.time()\r\n                output_dict = sess.run(tensor_dict,options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\r\n                                        feed_dict={image_tensor: image},run_metadata=run_metadata)\r\n                end=time.time()\r\n#                 print(run_metadata)\r\n                print(end-start)\r\n            opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()\r\n            flops = tf.profiler.profile(sess.graph,run_meta=run_metadata, options = tf.profiler.ProfileOptionBuilder.float_operation())\r\n            params = tf.profiler.profile(sess.graph, run_meta=run_metadata, cmd='op', options=opts)\r\n            print('FLOP before freezing', flops.total_float_ops)\r\n            print(\"parametrs.............\",params.total_parameters)\r\n            output_dict['num_detections'] = int(output_dict['num_detections'][0])\r\n            output_dict['detection_classes'] = output_dict[\r\n                'detection_classes'][0].astype(np.int64)\r\n            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\r\n            output_dict['detection_scores'] = output_dict['detection_scores'][0]\r\n            if 'detection_masks' in output_dict:\r\n                output_dict['detection_masks'] = output_dict['detection_masks'][0]\r\n```\r\nModel is taken from tensorflow API Github Repo http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz this is the link\r\n", "comments": ["@Deadsec69, Please provide the tensorflow version. Thanks! ", "I'm using tensorflow version 1.14", "@Deadsec69 This issue doesn't belong to tensorflow/tensorflow repo.  Please Post this issue is in the [tensorflow/models](https://github.com/tensorflow/models/issues) repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35463\">No</a>\n"]}, {"number": 35462, "title": "tensorflow installation in raspberry pi : HadoopFileSystem load error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry pi 4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary in pip\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7\r\n- Installed using  pip\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\n\r\n**Describe the problem**\r\n****\r\npi@raspberrypi:~/examples/lite/examples/object_detection/raspberry_pi $ python3 detect_picamera.py   --model /tmp/detect.tflite   --labels /tmp/coco_labels.txt\r\n2019-12-28 17:43:12.464130: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\nTraceback (most recent call last):\r\n  File \"detect_picamera.py\", line 34, in <module>\r\n    from tf.lite.interpreter import Interpreter\r\nModuleNotFoundError: No module named 'tf'\r\n***\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n***\r\n Tensorflow model running issue replaced the code with in original source code\r\n\r\n> from_ tflite_runtime.interpreter import Interpreter\r\n\r\n***\r\n**WIth**\r\n***\r\n> from tf.lite import Interpreter\r\n***\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["I got today the same issue when trying to run the fresh installed tensorflow on my raspberry pi 3+", "> I got today the same issue when trying to run the fresh installed tensorflow on my raspberry pi 3+\r\n\r\nYou got any solution for that ?", "I got same issue with my raspi 4B, but I don't think it's a fatal issue since I still can run tf.constant. However, it is still weird.....", "> > I got today the same issue when trying to run the fresh installed tensorflow on my raspberry pi 3+\r\n> \r\n> You got any solution for that ?\r\n\r\nNo", "> I got today the same issue when trying to run the fresh installed tensorflow on my raspberry pi 3+\r\n\r\nWhat are you trying to run ? If it's any tensorflow package  then use from \r\n`tensorflow.*`\r\n\r\n> The core tensorflow package With version 1.14x \r\n**I got answer when I run this in object detection**", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35462\">No</a>\n", "Experiencing this problem currently. Based on comments, it looks like this issue should not be closed anyway.", "yeah issue still present, same version of software as original filing  \r\n\r\n2020-02-12 14:41:15.443282: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\n\r\n", "> yeah issue still present, same version of software as original filing\r\n> \r\n> 2020-02-12 14:41:15.443282: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\n\r\nIt is but the running of lite models in PI doesn't effect by this So. I closed the issue", "You can still use TF as usual while you got this prompt out.", "sudo apt-get install -y libhdf5-dev libc-ares-dev libeigen3-dev\r\npython3 -m pip install keras_applications==1.0.8 --no-deps\r\npython3 -m pip install keras_preprocessing==1.1.0 --no-deps\r\npython3 -m pip install h5py==2.9.0\r\nsudo apt-get install -y openmpi-bin libopenmpi-dev\r\nsudo apt-get install -y libatlas-base-dev\r\npython3 -m pip install -U six wheel mock", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35462\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35462\">No</a>\n", "> sudo apt-get install -y libhdf5-dev libc-ares-dev libeigen3-dev\r\n> python3 -m pip install keras_applications==1.0.8 --no-deps\r\n> python3 -m pip install keras_preprocessing==1.1.0 --no-deps\r\n> python3 -m pip install h5py==2.9.0\r\n> sudo apt-get install -y openmpi-bin libopenmpi-dev\r\n> sudo apt-get install -y libatlas-base-dev\r\n> python3 -m pip install -U six wheel mock\r\n\r\nDoes this fix the issue?", "I still get this error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"Object_detection_picamera.py\", line 27, in <module>\r\n    import tensorflow as tf\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```", "IF You still use TensorFlow 1.15.0  with HDFS data, you could do as below:\r\n1. Download the version of hadoop your system has been used, unzip it into \"/usr/local/hadoop/\",then leave it;\r\n2. export those env variables:\r\n    on my cloudera cluster: you should do the first step, caused cloudera cluster is setup different;if your cluster setup by your handmade from hadoop original softwares, then leave it, just do step 2. \r\nlike this:\r\n`export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera`\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.2/extras/CUPTI/lib64`\r\n`export HADOOP_HDFS_HOME=/usr/local/hadoop/hadoop-2.7.7`\r\n`source ${HADOOP_HDFS_HOME}/libexec/hadoop-config.sh`\r\n`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${JAVA_HOME}/jre/lib/amd64/server`\r\n`export CLASSPATH=$(${HADOOP_HDFS_HOME}/bin/hadoop classpath --glob)`"]}, {"number": 35461, "title": "added int8 support for negate kernel", "body": "removed the todo message, and plugged in the default **reference_ops**'s negate.", "comments": ["A simple example:\n\nLet's say your zero_point is 6 and your scale is 1.5.\n\nAnd your current int8 value is 8, so your float value is float_value =\n(quantized_value - zero_point) * scale which is (8 - 6) * 1.5 = 3.\n\nThe negate value of that one should be -3.\n\nIn your implementation which is a direct negate, so 8 -> -8, so the result\nfloat value will be (-8 - 6) * 1.5 = -21, which is way off -3.\n\nDoes that make sense?\n\nOn Mon, Jan 6, 2020 at 10:44 AM Basit Ayantunde <notifications@github.com>\nwrote:\n\n> *@lamarrr* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/lite/micro/kernels/neg.cc\n> <https://github.com/tensorflow/tensorflow/pull/35461#discussion_r363141413>\n> :\n>\n> > @@ -31,15 +31,22 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n>    const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n>    TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n>    switch (input->type) {\n> -    // TODO(wangtz): handle for kTfLiteInt8\n> +    case kTfLiteInt8:\n> +      reference_ops::Negate(GetTensorShape(input), GetTensorData<int8_t>(input),\n>\n> Works in what sense?\n> My assumption was that irregardless of the input being quantized or not,\n> negation is agnostic of the zero-point.\n> Is this assumption flawed?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35461?email_source=notifications&email_token=AIURNGORG6DJ73UP72OPMZLQ4KLHFA5CNFSM4KALN4O2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCQWE3SA#discussion_r363141413>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIURNGNGSMA7L74WF5TWEBTQ4KLHFANCNFSM4KALN4OQ>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "> A simple example: Let's say your zero_point is 6 and your scale is 1.5. And your current int8 value is 8, so your float value is float_value = (quantized_value - zero_point) * scale which is (8 - 6) * 1.5 = 3. The negate value of that one should be -3. In your implementation which is a direct negate, so 8 -> -8, so the result float value will be (-8 - 6) * 1.5 = -21, which is way off -3. Does that make sense?\r\n> [\u2026](#)\r\n> On Mon, Jan 6, 2020 at 10:44 AM Basit Ayantunde ***@***.***> wrote: ***@***.**** commented on this pull request. ------------------------------ In tensorflow/lite/micro/kernels/neg.cc <[#35461 (comment)](https://github.com/tensorflow/tensorflow/pull/35461#discussion_r363141413)> : > @@ -31,15 +31,22 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) { const TfLiteTensor* input = GetInput(context, node, kInputTensor); TfLiteTensor* output = GetOutput(context, node, kOutputTensor); switch (input->type) { - // TODO(wangtz): handle for kTfLiteInt8 + case kTfLiteInt8: + reference_ops::Negate(GetTensorShape(input), GetTensorData<int8_t>(input), Works in what sense? My assumption was that irregardless of the input being quantized or not, negation is agnostic of the zero-point. Is this assumption flawed? \u2014 You are receiving this because your review was requested. Reply to this email directly, view it on GitHub <#35461?email_source=notifications&email_token=AIURNGORG6DJ73UP72OPMZLQ4KLHFA5CNFSM4KALN4O2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCQWE3SA#discussion_r363141413>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AIURNGNGSMA7L74WF5TWEBTQ4KLHFANCNFSM4KALN4OQ> .\r\n> -- Renjie Liu renjieliu@google.com +1 (650) 253-4359\r\n\r\nMakes perfect sense, thanks so much for the heads-up! \r\n\r\nSo, now we'll definitely need to perform a multiplication and subtraction on the int8 rep.\r\nHow do I handle this? Should I accumulate on int16 and saturate to int8 (max +127) ? ", "Given it's negate: where output = -input, so I think we should make sure the output_scale == input_scale, but note the input_zeropoint maybe different from output_zeropoint.\r\n\r\nSo we have:\r\ninput_float = (input_q - input_zp) * input_scale\r\nand\r\noutput_float = (output_q - output_zp) * output_scale\r\n\r\nsince output_float = -input_float, we have:\r\n\r\n(output_q - output_zp) * output_scale = -(input_q - input_zp) * input_scale\r\n\r\nSince output_scale == input_scale,  we can get:\r\n\r\noutput_q - output_zp = input_zp - input_q\r\n\r\nwhich is:\r\n\r\noutput_q = (input_zp + output_zp) - input_q\r\n\r\nNote input_zp + output_zp is a constant, so we can pre-compute it, you will just end up with\r\n\r\noutput_q = constant - input_q\r\n\r\nand yes, you will need to saturate the result to make sure it's within the range.\r\n\r\nHope this helps.\r\n", "> Given it's negate: where output = -input, so I think we should make sure the output_scale == input_scale, but note the input_zeropoint maybe different from output_zeropoint.\r\n> \r\n> So we have:\r\n> input_float = (input_q - input_zp) * input_scale\r\n> and\r\n> output_float = (output_q - output_zp) * output_scale\r\n> \r\n> since output_float = -input_float, we have:\r\n> \r\n> (output_q - output_zp) * output_scale = -(input_q - input_zp) * input_scale\r\n> \r\n> Since output_scale == input_scale, we can get:\r\n> \r\n> output_q - output_zp = input_zp - input_q\r\n> \r\n> which is:\r\n> \r\n> output_q = (input_zp + output_zp) - input_q\r\n> \r\n> Note input_zp + output_zp is a constant, so we can pre-compute it, you will just end up with\r\n> \r\n> output_q = constant - input_q\r\n> \r\n> and yes, you will need to saturate the result to make sure it's within the range.\r\n> \r\n> Hope this helps.\r\n\r\nAwesome!\r\nI already derived the equation using different scales, I suppose yours is a specialization of that.\r\nI will be pushing updates from my local branch here soon. Thanks so much.", "Hello @renjie-liu,\r\nI have made the changes, thanks for your feedback.", "I also added the op to the reference integer_ops instead of using the default op (for float).", "The prepare function is here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/neg.cc#L31-L40\n\nOn Wed, Jan 15, 2020 at 7:19 AM Basit Ayantunde <notifications@github.com>\nwrote:\n\n> *@lamarrr* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/lite/kernels/internal/reference/integer_ops/neg.h\n> <https://github.com/tensorflow/tensorflow/pull/35461#discussion_r366624355>\n> :\n>\n> > +      static_cast<int16_t>(std::numeric_limits<int8_t>::max());\n> +  constexpr int16_t kI8Min =\n> +      static_cast<int16_t>(std::numeric_limits<int8_t>::min());\n> +\n> +  // within: [-128, 127]\n> +  TFLITE_DCHECK_GE(input_zero_point, static_cast<int32_t>(kI8Min));\n> +  TFLITE_DCHECK_LE(input_zero_point, static_cast<int32_t>(kI8Max));\n> +\n> +  // within: [-128, 127]\n> +  TFLITE_DCHECK_GE(output_zero_point, static_cast<int32_t>(kI8Min));\n> +  TFLITE_DCHECK_LE(output_zero_point, static_cast<int32_t>(kI8Max));\n> +\n> +  const int flat_size = MatchingFlatSize(input_shape, output_shape);\n> +\n> +  // already within int8 range, stored in int32\n> +  const auto prior = static_cast<int16_t>(input_zero_point + output_zero_point);\n>\n> Yes, It is only computed once.\n> The pattern I used is the same in other kernels (The Non-quantized Negate\n> especially).\n> Also, there is no Prepare function for Register_NEG's TfLiteRegistration.\n> Are you sure you still want me to change this? (Create Prepare function)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35461?email_source=notifications&email_token=AIURNGL5R376VU4FTF5A673Q5ZCBZA5CNFSM4KALN4O2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCRYEXKA#discussion_r366624355>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIURNGKEOSFOULQ3K62DS6LQ5ZCBZANCNFSM4KALN4OQ>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "> The prepare function is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/neg.cc#L31-L40\r\n> [\u2026](#)\r\n> On Wed, Jan 15, 2020 at 7:19 AM Basit Ayantunde ***@***.***> wrote: ***@***.**** commented on this pull request. ------------------------------ In tensorflow/lite/kernels/internal/reference/integer_ops/neg.h <[#35461 (comment)](https://github.com/tensorflow/tensorflow/pull/35461#discussion_r366624355)> : > + static_cast<int16_t>(std::numeric_limits<int8_t>::max()); + constexpr int16_t kI8Min = + static_cast<int16_t>(std::numeric_limits<int8_t>::min()); + + // within: [-128, 127] + TFLITE_DCHECK_GE(input_zero_point, static_cast<int32_t>(kI8Min)); + TFLITE_DCHECK_LE(input_zero_point, static_cast<int32_t>(kI8Max)); + + // within: [-128, 127] + TFLITE_DCHECK_GE(output_zero_point, static_cast<int32_t>(kI8Min)); + TFLITE_DCHECK_LE(output_zero_point, static_cast<int32_t>(kI8Max)); + + const int flat_size = MatchingFlatSize(input_shape, output_shape); + + // already within int8 range, stored in int32 + const auto prior = static_cast<int16_t>(input_zero_point + output_zero_point); Yes, It is only computed once. The pattern I used is the same in other kernels (The Non-quantized Negate especially). Also, there is no Prepare function for Register_NEG's TfLiteRegistration. Are you sure you still want me to change this? (Create Prepare function) \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#35461?email_source=notifications&email_token=AIURNGL5R376VU4FTF5A673Q5ZCBZA5CNFSM4KALN4O2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCRYEXKA#discussion_r366624355>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AIURNGKEOSFOULQ3K62DS6LQ5ZCBZANCNFSM4KALN4OQ> .\r\n> -- Renjie Liu renjieliu@google.com +1 (650) 253-4359\r\n\r\nThanks. I was looking at tensorflow/lite/micro/kernels/neg.cc.\r\nI'll make the necessary changes later today. ", "Oh, if you intend to change for micro as well. Feel free to add another\nprepare function.\n\nOn Wed, Jan 15, 2020 at 10:00 AM Basit Ayantunde <notifications@github.com>\nwrote:\n\n> The prepare function is here:\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/neg.cc#L31-L40\n> \u2026 <#m_1576323117430476474_>\n> On Wed, Jan 15, 2020 at 7:19 AM Basit Ayantunde *@*.*> wrote: @.**\n> commented on this pull request. ------------------------------ In\n> tensorflow/lite/kernels/internal/reference/integer_ops/neg.h <#35461\n> (comment)\n> <https://github.com/tensorflow/tensorflow/pull/35461#discussion_r366624355>>\n> : > + static_cast<int16_t>(std::numeric_limits<int8_t>::max()); + constexpr\n> int16_t kI8Min = +\n> static_cast<int16_t>(std::numeric_limits<int8_t>::min()); + + // within:\n> [-128, 127] + TFLITE_DCHECK_GE(input_zero_point,\n> static_cast<int32_t>(kI8Min)); + TFLITE_DCHECK_LE(input_zero_point,\n> static_cast<int32_t>(kI8Max)); + + // within: [-128, 127] +\n> TFLITE_DCHECK_GE(output_zero_point, static_cast<int32_t>(kI8Min)); +\n> TFLITE_DCHECK_LE(output_zero_point, static_cast<int32_t>(kI8Max)); + +\n> const int flat_size = MatchingFlatSize(input_shape, output_shape); + + //\n> already within int8 range, stored in int32 + const auto prior =\n> static_cast<int16_t>(input_zero_point + output_zero_point); Yes, It is only\n> computed once. The pattern I used is the same in other kernels (The\n> Non-quantized Negate especially). Also, there is no Prepare function for\n> Register_NEG's TfLiteRegistration. Are you sure you still want me to change\n> this? (Create Prepare function) \u2014 You are receiving this because you were\n> mentioned. Reply to this email directly, view it on GitHub <#35461\n> <https://github.com/tensorflow/tensorflow/pull/35461>?email_source=notifications&email_token=AIURNGL5R376VU4FTF5A673Q5ZCBZA5CNFSM4KALN4O2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCRYEXKA#discussion_r366624355>,\n> or unsubscribe\n> https://github.com/notifications/unsubscribe-auth/AIURNGKEOSFOULQ3K62DS6LQ5ZCBZANCNFSM4KALN4OQ\n> .\n> -- Renjie Liu renjieliu@google.com +1 (650) 253-4359 <(650)%20253-4359>\n>\n> Thanks. I was looking at tensorflow/lite/micro/kernels/neg.cc.\n> I'll make the necessary changes later today.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35461?email_source=notifications&email_token=AIURNGI2CPACR6WKV7BQNZDQ5ZU4LA5CNFSM4KALN4O2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEI6ZCBA#issuecomment-574460164>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIURNGJFOPZTAUZ5UCTLVEDQ5ZU4LANCNFSM4KALN4OQ>\n> .\n>\n\n\n-- \nRenjie Liu\n\nrenjieliu@google.com\n+1 (650) 253-4359\n", "> The prepare function is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/neg.cc#L31-L40\r\n> [\u2026](#)\r\n> On Wed, Jan 15, 2020 at 7:19 AM Basit Ayantunde ***@***.***> wrote: ***@***.**** commented on this pull request. ------------------------------ In tensorflow/lite/kernels/internal/reference/integer_ops/neg.h <[#35461 (comment)](https://github.com/tensorflow/tensorflow/pull/35461#discussion_r366624355)> : > + static_cast<int16_t>(std::numeric_limits<int8_t>::max()); + constexpr int16_t kI8Min = + static_cast<int16_t>(std::numeric_limits<int8_t>::min()); + + // within: [-128, 127] + TFLITE_DCHECK_GE(input_zero_point, static_cast<int32_t>(kI8Min)); + TFLITE_DCHECK_LE(input_zero_point, static_cast<int32_t>(kI8Max)); + + // within: [-128, 127] + TFLITE_DCHECK_GE(output_zero_point, static_cast<int32_t>(kI8Min)); + TFLITE_DCHECK_LE(output_zero_point, static_cast<int32_t>(kI8Max)); + + const int flat_size = MatchingFlatSize(input_shape, output_shape); + + // already within int8 range, stored in int32 + const auto prior = static_cast<int16_t>(input_zero_point + output_zero_point); Yes, It is only computed once. The pattern I used is the same in other kernels (The Non-quantized Negate especially). Also, there is no Prepare function for Register_NEG's TfLiteRegistration. Are you sure you still want me to change this? (Create Prepare function) \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#35461?email_source=notifications&email_token=AIURNGL5R376VU4FTF5A673Q5ZCBZA5CNFSM4KALN4O2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCRYEXKA#discussion_r366624355>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AIURNGKEOSFOULQ3K62DS6LQ5ZCBZANCNFSM4KALN4OQ> .\r\n> -- Renjie Liu renjieliu@google.com +1 (650) 253-4359\r\n\r\n\r\n**From my investigation**:\r\n\r\n1.  The Prepare function only checks the nodes and sets the input sizes (setting the sizes of the inputs and isn't required since there is no dynamic tensor resizing; embedded targets).\r\n2.  It does not seem `Prepare` is meant to be used for calculating the `OpData` (`prior`, in this case).\r\n3.  The `OpData` needs to be calculated in the `EvalQuantizedInt8` function or in the `Op` itself. \r\n4.  I calculated it in the `Op` itself since it is just **one parameter** and having a separate `OpData` for it will be redundant.\r\n5.  I do not know of any specific name for the sum of two `zero-points`. I feel this is an implementation-specific detail.\r\n6.  It is only calculated once, just as it would when using the `OpData`, and prevents code duplication across the Tensorflow Lite and Tensorflow Lite Micro implementations.\r\n", "you should be able to find examples about how to precompute stuff in prepare, for example, see fully_connected (precompute multiplier): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/fully_connected.cc#L168-L178\r\n\r\nalso, for zero point, you can get from quantization params: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.h#L349", "the prepare should be ran only once, the eval will be called every time the tflite interpreter got invoked. I'm fine if you find putting the pre-compute logic in prepare is tricky.\r\n\r\ncan but you add the kernel to tflite(not just micro) and test as well?\r\n\r\nthanks!", "> you should be able to find examples about how to precompute stuff in prepare, for example, see fully_connected (precompute multiplier): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/fully_connected.cc#L168-L178\r\n> \r\n> also, for zero point, you can get from quantization params: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.h#L349\r\n\r\nThanks, That only applies to TfLite, not TfLite Micro since you can't do dynamic memory allocation in the embedded context.", "> the prepare should be ran only once, the eval will be called every time the tflite interpreter got invoked. I'm fine if you find putting the pre-compute logic in prepare is tricky.\r\n> \r\n> can but you add the kernel to tflite(not just micro) and test as well?\r\n> \r\n> thanks!\r\n\r\nThanks for the clarification. I've already added the kernel and tests to TfLite.", "@renjie-liu ", "adding Tiezhen to take a look at the micro kernel & test", "> Please resolve the comments. otherwise looks good.\r\n\r\nDone, thanks!", "@lamarrr Can you please resolve conflicts? Thanks!", "> @lamarrr Can you please resolve conflicts? Thanks!\r\n\r\n\r\nSorry for being late, been occupied with exams. \r\nResolved! \r\nThanks. ", "@petewarden @renjie-liu ", "@lamarrr Still, conflicts are appearing. Can you please resolve? Thanks!", "> @lamarrr Still, conflicts are appearing. Can you please resolve? Thanks!\r\n\r\nYeah, another one appeared, The context->ReportError was changed to TF_LITE_KERNEL_LOG, seems that's how other kernels are. So I conformed with it.\r\nI've resolved it now. Thanks again! ", "> @lamarrr Still, conflicts are appearing. Can you please resolve? Thanks!\r\n\r\n@gbaned @petewarden @renjie-liu", "@petewarden @gbaned ", "@lamarrr Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n", "Sorry about that. I didn't stall intentionally. I've had some business and issues over the past couple of months and had to be away from Open Source. I'm back now.", "I've resolved the conflict @gbaned \r\n", "awaiting review @renjie-liu @petewarden ", "@lamarrr thanks for your patience.\r\n\r\nGiven the long history of this PR and the recent changes to the [Micro contribution guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md), I am going to request (with apologies for all the delays) that we take a step back and follow the new guidelines.\r\n\r\nMy specific recommendation is:\r\n * Please make a github issue [motivating the new feature](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md) that you would like -- in an effort to give contributors faster feedback and PR reviews, we need to focus efforts on features that have broad impact. This will unfortunately mean saying no to some features.\r\n\r\nIf we agree that now is a good time for this feature, I would say the next steps would be:\r\n\r\n * Since there was a lot of back and forth with @renjie-liu write up the details of the int8 math in the github issue so that is is documented for future reference and can also be referred to during the PR review.\r\n\r\n * Break the work into smaller PRs that will be easier to review and merge. In this case, I would suggest having two PRs (one each for Lite and Micro).\r\n\r\nI do apologize for asking you to start over but we truly are trying to reduce our response time with these new contribution guidelines.\r\n\r\nI'm going to close this current PR and would encourage you to make a github issue to kickstart this conversation.", "> @lamarrr thanks for your patience.\r\n> \r\n> Given the long history of this PR and the recent changes to the [Micro contribution guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md), I am going to request (with apologies for all the delays) that we take a step back and follow the new guidelines.\r\n> \r\n> My specific recommendation is:\r\n> \r\n> * Please make a github issue [motivating the new feature](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md) that you would like -- in an effort to give contributors faster feedback and PR reviews, we need to focus efforts on features that have broad impact. This will unfortunately mean saying no to some features.\r\n> \r\n> If we agree that now is a good time for this feature, I would say the next steps would be:\r\n> \r\n> * Since there was a lot of back and forth with @renjie-liu write up the details of the int8 math in the github issue so that is is documented for future reference and can also be referred to during the PR review.\r\n> * Break the work into smaller PRs that will be easier to review and merge. In this case, I would suggest having two PRs (one each for Lite and Micro).\r\n> \r\n> I do apologize for asking you to start over but we truly are trying to reduce our response time with these new contribution guidelines.\r\n> \r\n> I'm going to close this current PR and would encourage you to make a github issue to kickstart this conversation.\r\n\r\nThanks! I understand.\r\nI'll take the recommended steps."]}, {"number": 35460, "title": "integrate.odeint in tensorflow 2.0", "body": "Can't find odeint or integrate...\r\ncan't import contrib from tf.compat.v1 \r\nwhere is odeint ", "comments": ["@Tonytony123474,\r\n`integrate.odeint` is moved to Tensorflow/scientific in Tf 2.0. \r\nTry, `pip install tensorflow-scientific`\r\n`tfs.integrate.odeint`\r\nThanks!\r\n", "@Tonytony123474, Closing since its resolved. Please feel free to comment or reopen if its still an issue. Thanks!"]}, {"number": 35459, "title": "EMNIST Link Broken ", "body": "The link to the [EMNIST page](https://www.tensorflow.org/datasets/catalog/emnist) on Tensorflow's docs is broken.\r\n\r\nThe existing link is -\r\nhttps://www.nist.gov/node/1298471/emnist-dataset\r\n\r\nIt should be replaced by -\r\nhttp://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip", "comments": ["Interested to fix this @AND2797 . Do all the mentions of the link are required to be changed to [http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip](http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip) as some refer to the homepage rather than the dataset itself. Can [https://www.nist.gov/itl/products-and-services/emnist-dataset](https://www.nist.gov/itl/products-and-services/emnist-dataset) be used except for the first occurrence?", "Actually, good point. I assumed that the link led to the download of the actual dataset. Now since the link is broken I am not sure if it led to the download of the actual data set or the home page of the dataset. \r\n\r\nPerhaps one could add both, or either one. ", "Additionally, I don't think this page of documentation is based on any file in this repository. Any opinions on this @AND2797 ?", "\r\n> Additionally, I don't think this page of documentation is based on any file in this repository. Any opinions on this @AND2797 ?\r\n\r\ndo you mean based on any module source code?\r\n\r\n", "Yes @AND2797. The source code behind that page of documentation at [EMNIST PAGE](https://www.tensorflow.org/datasets/catalog/emnist).", "Not sure what you mean, but I have already created a pull request with the changes, I will close this thread if it is approved.", "@AND2797, Can you add PR link here. Thanks!", "> @AND2797, Can you add PR link here. Thanks!\r\n\r\nDo you mean this -> https://github.com/AND2797/datasets/pull/1", "@AND2797, Can you submit this PR against Tensorflow? ", "> @AND2797, Can you submit this PR against Tensorflow?\r\n\r\n@gadagashwini  Yes, I've done it  now. \r\nhttps://github.com/tensorflow/datasets/pull/1322\r\nThis is the link.", "I've submitted the PR against tensorflow, can anyone take a look?", "@conchylicultor @rsepassi  Can you PTAL? Thanks!", "Fix will go out shortly. Thank you!"]}, {"number": 35458, "title": "Fix formatting of tf.xla.experimental.jit_scope examples", "body": "The examples for `tf.xla.experimental.jit_scope` contained in its docstring are not rendering correctly (as pre-formatted text) to the TensorFlow [API docs](https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope?version=stable). This current pull request intends to fix that.\r\n\r\nI also want to also bring attention to the fact that the argument list is not rendering correctly. I don't know why that's happening, nor how to fix it.", "comments": []}, {"number": 35457, "title": "Usage example for tf.linalg.det()", "body": "", "comments": ["Make it a doctest. Please follow https://www.tensorflow.org/community/contribute/docs_ref. Also test it locally.", "@yashk2810 , you can check it now.", "Sorry, accidentally closed this PR while trying to duplicate the repo.", "We could have reopened it but you did a force push on the branch so that's impossible now.", "I made a new PR : \r\n#35475 \r\nSorry for making so many pull requests and sorry for all the inconveniences", "And it would be really great if you could review another PR, #35476 "]}, {"number": 35456, "title": "tf.print can't print chinese characters", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nwin10 1909\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow\r\n- TensorFlow version (use command below):\r\n2.0.0\r\n- Python version:\r\n3.7.4\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf \r\ntf.print(\"\u4e2d\u6587\u6d4b\u8bd5\")\r\ntf.print(\"Chinese test\")\r\nprint(\"\u4e2d\u6587\u6d4b\u8bd5\")\r\nprint(\"Chinese test\")\r\n```\r\ncurrent output is\r\n\u6d93\ue15f\u6783\u5a34\u5b2d\u762f\r\nChinese test\r\n\u4e2d\u6587\u6d4b\u8bd5\r\nChinese test\r\n\r\nexpected output is\r\n\u4e2d\u6587\u6d4b\u8bd5\r\nChinese test\r\n\u4e2d\u6587\u6d4b\u8bd5\r\nChinese test\r\n\r\n\r\n", "comments": ["I have tried on colab with TF version 2.0 , TF Nightly version 2.1.0-dev20191229 and i am not seeing any issue. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/87a35d7a47720f1293a16a80c421917e/untitled511.ipynb) Thanks!", "> I have tried on colab with TF version 2.0 , TF Nightly version 2.1.0-dev20191229 and i am not seeing any issue. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/87a35d7a47720f1293a16a80c421917e/untitled511.ipynb) Thanks!\r\n\r\nHi , @ravikyram . When I run the code on linux by using ```python test.py``` or in jupyter (no matter on linux or windows) , It works well. But when I run it in powershell or cmd , it's strange that only ```tf.print``` can't output correctly . When I change windows global character encoding from ```GBK``` to ```utf8``` , both of them works well ......", "@DachuanZhao \r\n\r\nCan we close this issue since the query is been resolved? Thanks!\r\n"]}]