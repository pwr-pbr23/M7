[{"number": 19920, "title": "Try importing TRTOps to import_pb_to_tensorboard.py", "body": "This PR tries to import TRTEngine ops to import_pb_to_tensorboard.py script to ease up of construction of tensorboard visualizations of TFTRT optimized graphs.", "comments": ["Nagging Reviewer @ringw: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19919, "title": "Branch 200114810", "body": "Manual merge in:\r\n```\r\ntensorflow/contrib/autograph/__init__.py                              \r\ntensorflow/core/grappler/optimizers/remapper.cc                            \r\ntensorflow/python/kernel_tests/py_func_test.py                        \r\ntensorflow/python/ops/script_ops.py                                              \r\n```", "comments": []}, {"number": 19918, "title": "Hi, how can I fix HTTP error 403? mnist input_data", "body": "Hi!\r\nIt is my first time to use this data set!\r\nI got HTTP error 403 when I use the following code.\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\r\n\r\nSo.. I tried several things such as turning off firewall.\r\nBut, I can't solve it and need to fix it for homework.\r\nCan you help me?", "comments": ["There was a problem with that bucket, which should now be fixed."]}, {"number": 19915, "title": "tensorflow/go: add operation Input methods + tests", "body": "The go version of tensorflow only has operation output methods. This adds in the needed code to return operation inputs and jump from output to input and vice versa. This is required to actually be able to traverse the graph.", "comments": ["CCing @aselle @andrewharp since they were assigned to review my last Go PR.", "I'm working on project to do federate machine learning as a service. It does both inference and training through a Go client library. These changes are part of what's needed in order to be able to quantize models on the fly to improve inference performance. I have an attributes PR in the works as well which should make it possible to fully inspect and manipulate the models.\r\n\r\nGo client library: https://github.com/luk-ai/lukai\r\n\r\nSite: https://luk.ai", "Think I fixed all the nits.", "Failure is in `//tensorflow/python/data/kernel_tests:batch_dataset_op_test`, which is definitely unrelated. Merging."]}, {"number": 19914, "title": "TensorFlow MNIST dataset is throwing 403", "body": "[TensorFlow official MNIST model](https://github.com/tensorflow/models/blob/master/official/mnist/dataset.py) and [TFLearn Model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py) use https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz as a data store for MNIST dataset.\r\n\r\nThis URL is returning 403 errors:\r\n```\r\n<Error>\r\n  <Code>UserProjectAccountProblem</Code>\r\n  <Message>User project billing account not in good standing.</Message>\r\n  <Details>\r\n    The billing account for project 81,941,577,218 is disabled in state delinquent\r\n  </Details>\r\n</Error>\r\n```\r\n\r\nAny ETA for the fix?\r\n\r\ncc @reedwm @martinwicke ", "comments": ["Yeah, ironically, we're having trouble with billing. \r\n\r\nPR #19905 would fix it and can be used as a workaround until we figure out how to pay ourselves (hopefully not long, otherwise we'll accept the PR).", "Just noticed this error. Good luck :)", "Would be better if the Google URL is fixed, then users can use both for a foreseeable future and don't have to hard migrate.", "This should now be fixed. Sorry about this."]}, {"number": 19913, "title": "ValueError: Could not interpret optimizer identifier (tf.keras)", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0/6.0\r\n- **GPU model and memory**:\r\nNvidia\r\n- **Exact command to reproduce**:\r\nmodel.compile(optimizer=tf.keras.optimizers.Adadelta() ...)\r\n\r\n### Describe the problem\r\nPassing in keras optimizers into a tf.keras model causes a value error, unless they are passed as strings i.e. \"Adadelta\" instead of Adadelta( ). This prevents arguments from being passed to the optimizer. Please note that when the optimizer is imported from vanilla Keras i.e. `keras.optimizers.Adadelta(rho=0.9)` there is no such issue. \r\n\r\n### Source code / logs\r\n\r\n``\r\n    import tensorflow as tf\r\n    from tensorflow.python.keras.optimizers import Adadelta, Adam\r\n\r\n    model = deepshading.get_model()\r\n    model.compile(optimizer=tf.keras.optimizers.Adadelta(rho=0.9),\r\n                  loss=DSSIMObjective(k1=0.0001, k2=0.001, kernel_size=8),\r\n                  metrics=[DSSIMObjective(k1=0.0001, k2=0.001, kernel_size=8)])\r\n``\r\n\r\nReturns trace-back\r\n``\r\nidentifier=identifier.__class__.__name__))\r\nTraceback (most recent call last):\r\n  File \"C:/Users/isultan/PycharmProjects/deep-shading/run.py\", line 69, in <module>\r\n    train()\r\n  File \"C:/Users/isultan/PycharmProjects/deep-shading/run.py\", line 35, in train\r\n    metrics=[DSSIMObjective(k1=0.0001, k2=0.001, kernel_size=8)])\r\n  File \"C:\\Users\\isultan\\AppData\\Local\\Continuum\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 604, in compile\r\n    self.optimizer = optimizers.get(optimizer)\r\n  File \"C:\\Users\\isultan\\AppData\\Local\\Continuum\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers.py\", line 768, in get\r\n    str(identifier))\r\nValueError: Could not interpret optimizer identifier: <tensorflow.python.keras._impl.keras.optimizers.Adadelta object at 0x000001E563508860>\r\n``", "comments": ["Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "isaacsultan@ I am unable to reproduce this issue. Here is the code sample I am using: [gist link](https://gist.github.com/anj-s/5c3396f3c33653c3b2a4b2a1cb0778f2). I am using Tensorflow version 1.8 and python 2.7. \r\n\r\nCan you try with another example and see if you are hitting the same issue? Another thing you could try is updating tensorflow to 1.9. ", "Hi @anj-s  I just tried with another example, and can confirm that the issue does not reproduce. I'm closing the issue, thanks. ", "Having the same issue with SGD optimizer, also tensorflow version 1.8.0\r\n\r\n```\r\nOptimizer = tf.keras.optimizers.SGD(lr=1e-3, momentum=0.3, decay=0, nesterov=False)\r\nmodel.compile(loss=tf.keras.losses.binary_crossentropy,\r\n                  optimizer=Optimizer,\r\n                  metrics=['accuracy'],options = run_opts)\r\n```\r\n\r\n> ValueError: ('Could not interpret optimizer identifier:', (<tensorflow.python.keras._impl.keras.optimizers.SGD object at 0x2b98cdfe7c50>,))\r\n\r\nIt works as soon as I do not provide any parameters to SGD  (`Optimizer = tf.keras.optimizers.SGD() ` ) ", "**please help to fix this error I am running on gpu**\r\nvalueError: ('Could not interpret optimizer identifier:', <keras.optimizers.Adadelta object at 0x7ff7cf7bb710>)", "I got this error by using the optimizer class rather than an instance of it, e.g. `optimizer = Adam` instead of `optimizer = Adam()`.", "Hi\r\n\r\nI am facing this error when I used SGD for my optimizer.\r\nI am running the code on colab, version of tensorflow 2.2.0-rc3.\r\nI took the sample code from Keras documentation.\r\nI need to be able to set and get my learning_rate and other params in my optimizer. But currently only default option is working.\r\nCould you please give a resolution or a temporary workaround for this issue, as I my work is held up for this.\r\nAttaching the screenshot here.\r\n\r\n![error 2020-04-23 222914](https://user-images.githubusercontent.com/21074002/80127437-f4251d00-85b1-11ea-8b1b-2130cc1abfb4.jpg)\r\n\r\n\r\nThank you\r\n\r\n\r\n", "'''\r\nimport argparse\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nfrom sklearn.datasets import load_iris\r\nfrom matplotlib import pyplot as plt\r\n\r\ndef overview():\r\n    print(\"python version=\", sys.version)\r\n    print(\"tensorflow version=\", tf.__version__)\r\n    if len(tf.config.list_physical_devices('GPU'))==0:\r\n        print('no gpu device is detected')\r\n    else:\r\n        print(tf.config.list_physical_devices('GPU'))\r\n\r\nclass SETTING:\r\n    epochs=30\r\n    optimizer='adam'\r\n    loss='categorical_crossentropy'\r\n\r\ndef load_data():\r\n    iris=load_data()\r\n    X=iris.data\r\n    y=iris.target\r\n    y=np.eye(len(np.unique(target)))[target]\r\n    return X, y\r\n\r\ndef set_model(fc1, fc2):\r\n    model=tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(fc1, activation='relu', input_shape=(4,)))\r\n    model.add(tf.keras.layers.Dense(fc2, activation='relu'))\r\n    model.add(tf.keras.layers.Dense(3, activation='softmax'))\r\n    return model\r\n\r\ndef train_step():\r\n    mymodel=set_model(20,10)\r\n    mymodel.compile(loss=SETTING.loss, optimizer='adam') # here\r\n    history=mymodel.fit(load_data(), epochs=SETTING.epochs, metrics=['accuracy'], validation_split=0.3)\r\n    return history\r\n\r\ndef main():\r\n    overview()\r\n    history=train_step()\r\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\r\n\r\nif __name__=='__main__':\r\n    parser=argparse.ArgumentParser(description='simpoe ML using iris datasets')\r\n    parser.add_argument('--epochs', help='setup epochs')\r\n    parser.add_argument('--loss', help='setup loss function')\r\n    parser.add_argument('--callback', help='save best model')\r\n    parser.add_argument('--optimizer', help='setup optimizer')\r\n    # parser.add_argument('--summary', help='show model summary', action=mymodel.summary())\r\n    args=parser.parse_args()\r\n    SETTING.epochs=args.epochs\r\n    SETTING.loss=args.loss\r\n    SETTING.optimizer=args.optimizer\r\n    SETTING.callback=args.callback\r\n    main()\r\n'''\r\n\r\nhi bro.\r\n\r\ni am facing same problem when i using optimizer from \"SETTING\" class. i tried to change optimizer 'adam', 'SGD', but it didn't solve.\r\n\r\nit was not worked when i using 'SETTING.optimizer', but working when i using 'adam'. i don't know what is difference between 'SETTING.optimizer' and 'adam'\r\n\r\nthank you", "```\r\nfrom tensorflow.python.keras import initializers\r\nfrom tensorflow.python.keras import layers\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.activations import relu\r\nfrom tensorflow.python.keras.layers import Input, Dense, Reshape, Flatten, LSTM, Bidirectional\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras.callbacks import TensorBoard\r\nfrom tensorflow.python.keras import regularizers\r\nfrom sklearn.model_selection import train_test_split\r\n...\r\nadam_opt = Adam(learning_rate=0.0001)\r\nloss = weighted_bce\r\nmodel.compile(loss=loss, optimizer=adam_opt, experimental_run_tf_function=False)\r\n```\r\n\r\nand I got\r\n```\r\nValueError: Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x7f4e5c06fca0>\r\n```\r\ndoes anyone have solved this problem?", "> \r\n> \r\n> ```\r\n> from tensorflow.python.keras import initializers\r\n> from tensorflow.python.keras import layers\r\n> from tensorflow.keras.optimizers import Adam\r\n> from tensorflow.python.keras.models import Model\r\n> from tensorflow.python.keras.activations import relu\r\n> from tensorflow.python.keras.layers import Input, Dense, Reshape, Flatten, LSTM, Bidirectional\r\n> from tensorflow.python.keras import backend as K\r\n> from tensorflow.python.keras.callbacks import TensorBoard\r\n> from tensorflow.python.keras import regularizers\r\n> from sklearn.model_selection import train_test_split\r\n> ...\r\n> adam_opt = Adam(learning_rate=0.0001)\r\n> loss = weighted_bce\r\n> model.compile(loss=loss, optimizer=adam_opt, experimental_run_tf_function=False)\r\n> ```\r\n> \r\n> and I got\r\n> \r\n> ```\r\n> ValueError: Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x7f4e5c06fca0>\r\n> ```\r\n> \r\n> does anyone have solved this problem?\r\n\r\nSame problem", "> > ```\r\n> > from tensorflow.python.keras import initializers\r\n> > from tensorflow.python.keras import layers\r\n> > from tensorflow.keras.optimizers import Adam\r\n> > from tensorflow.python.keras.models import Model\r\n> > from tensorflow.python.keras.activations import relu\r\n> > from tensorflow.python.keras.layers import Input, Dense, Reshape, Flatten, LSTM, Bidirectional\r\n> > from tensorflow.python.keras import backend as K\r\n> > from tensorflow.python.keras.callbacks import TensorBoard\r\n> > from tensorflow.python.keras import regularizers\r\n> > from sklearn.model_selection import train_test_split\r\n> > ...\r\n> > adam_opt = Adam(learning_rate=0.0001)\r\n> > loss = weighted_bce\r\n> > model.compile(loss=loss, optimizer=adam_opt, experimental_run_tf_function=False)\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > and I got\r\n> > ```\r\n> > ValueError: Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x7f4e5c06fca0>\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > does anyone have solved this problem?\r\n> \r\n> Same problem\r\n\r\nI solved this problem by uninstalling keras(independent package), and use `keras` in tensorflow.", "> \r\n> \r\n> > > ```\r\n> > > from tensorflow.python.keras import initializers\r\n> > > from tensorflow.python.keras import layers\r\n> > > from tensorflow.keras.optimizers import Adam\r\n> > > from tensorflow.python.keras.models import Model\r\n> > > from tensorflow.python.keras.activations import relu\r\n> > > from tensorflow.python.keras.layers import Input, Dense, Reshape, Flatten, LSTM, Bidirectional\r\n> > > from tensorflow.python.keras import backend as K\r\n> > > from tensorflow.python.keras.callbacks import TensorBoard\r\n> > > from tensorflow.python.keras import regularizers\r\n> > > from sklearn.model_selection import train_test_split\r\n> > > ...\r\n> > > adam_opt = Adam(learning_rate=0.0001)\r\n> > > loss = weighted_bce\r\n> > > model.compile(loss=loss, optimizer=adam_opt, experimental_run_tf_function=False)\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > and I got\r\n> > > ```\r\n> > > ValueError: Could not interpret optimizer identifier: <keras.optimizer_v2.adam.Adam object at 0x7f4e5c06fca0>\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > does anyone have solved this problem?\r\n> > \r\n> > \r\n> > Same problem\r\n> \r\n> I solved this problem by uninstalling keras(independent package), and use `keras` in tensorflow.\r\n\r\nThanks, I solved changing the import. Now I use `from tensorflow.python.keras.optimizer_v2.adam import Adam`", "I'm getting the same error, but I'm importing from `tensorflow_addons`. It's only occurring due to the fact that I'm attempting to load the model via `bytes()`. Due to limitations in the platform I'm using ([QuantConnect](https://lean-api-docs.netlify.app/classQuantConnect_1_1Storage_1_1ObjectStore.html)), I need to store/retrieve objects using their `ObjectStore.ReadBytes` / `ObjectStore.SaveBytes` methods. I cannot just save a file.\r\n\r\nUsing some patchwork found [here](https://github.com/tensorflow/tensorflow/issues/34697#issuecomment-627193883), I've been able to get it to the point where I get the error this issue is about but no further. I modified the solution a bit to include keras' `custom_objects`.\r\n\r\n```python\r\n# Normally this would include other custom Loss/Metrics classes...\r\ncustom_keras_objects = {}\r\n\r\ndef unpack(model, training_config, weights):\r\n    restored_model = deserialize(model, custom_keras_objects)\r\n    if training_config is not None:\r\n        restored_model.compile(\r\n            **saving_utils.compile_args_from_training_config(training_config, custom_keras_objects)\r\n        )\r\n    restored_model.set_weights(weights)\r\n    return restored_model\r\n\r\n\r\n# Hotfix function\r\ndef make_keras_picklable(*args):\r\n\r\n    # Allow caller to include classes not already a part of tensorflow\r\n    # so that deserializing won't trigger a custom_objects warning\r\n    for obj in args:\r\n        custom_keras_objects[obj.__name__] = obj\r\n\r\n    def __reduce__(self):\r\n        model_metadata = saving_utils.model_metadata(self)\r\n        training_config = model_metadata.get(\"training_config\", None)\r\n        model = serialize(self)\r\n        weights = self.get_weights()\r\n        return (unpack, (model, training_config, weights))\r\n\r\n    cls = Model\r\n    cls.__reduce__ = __reduce__\r\n\r\n\r\n# This is a grossly oversimplified and untested example...\r\nimport pickle\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, InputLayer\r\nfrom tensorflow_addons.optimizers import AdamW\r\nmake_keras_picklable(AdamW, SGDW)\r\n\r\nmodel = Sequential()\r\nmodel.add(InputLayer(batch_input_shape=(1, 1, 1)))\r\nmodel.add(Dense(1)\r\nmodel.compile(optimizer=self.optimizer)\r\n\r\n# The following calls are made elsewhere in the program, but they are how I set/retrieve the model\r\nalgorithm.ObjectStore.SaveBytes(\"model\", pickle.dumps(model))\r\npickle.loads(bytes(algorithm.ObjectStore.ReadBytes(\"model\")))\r\n```\r\nResults in an error inside the call to `compile_args_from_training_config`:\r\n\r\n> ValueError: Could not interpret optimizer identifier: <tensorflow_addons.optimizers.weight_decay_optimizers.AdamW object at 0x7f73e9b71a58>", "### I am also getting this type of error!\r\n``\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Activation, Conv1D, MaxPool1D, Flatten, Dropout\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\nfrom tensorflow.keras import regularizers\r\n\r\n# Build the CNN network\r\nmodel = Sequential()\r\n\r\nmodel.add(Conv1D(filters=128, \r\n                 kernel_size=6, \r\n                 padding=\"same\", \r\n                 strides=2, \r\n                 activation=\"relu\",\r\n                 input_shape=(x_train.shape[1],x_train.shape[2])))\r\nmodel.add(MaxPool1D(pool_size=1))\r\nmodel.add(Flatten())\r\n\r\nmodel.add(Dense(128,  activation='relu')) # Hidden 1\r\nmodel.add(Dense(64,  activation='relu')) # Hidden 1\r\nmodel.add(Dense(32,  activation='relu')) # Hidden 1\r\nmodel.add(Dense(16,  activation='relu')) # Hidden 1\r\nmodel.add(Dense(1)) # Output\r\nmodel.compile(loss='mean_squared_error', optimizer= BatAlgorithm(10, 40, 1000, 0.5, 0.5, 0.0, 2.0, -10.0, 10.0, Fun))\r\n``\r\n\r\n> ---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-37-e147193aa417>](https://localhost:8080/#) in <module>()\r\n     21 model.add(Dense(16,  activation='relu')) # Hidden 1\r\n     22 model.add(Dense(1)) # Output\r\n---> 23 model.compile(loss='mean_squared_error', optimizer= BatAlgorithm(10, 40, 1000, 0.5, 0.5, 0.0, 2.0, -10.0, 10.0, Fun))\r\n\r\n1 frames\r\n[/usr/local/lib/python3.7/dist-packages/keras/optimizers.py](https://localhost:8080/#) in get(identifier)\r\n    143   else:\r\n    144     raise ValueError(\r\n--> 145         'Could not interpret optimizer identifier: {}'.format(identifier))\r\n\r\nValueError: Could not interpret optimizer identifier: <BatAlgorithm.BatAlgorithm object at 0x7f8472aa7310>\r\n\r\n> ", "@ShahidHasib586 , from the stacktrace we can see that the library/ package from which  BatAlgorithm is called is missing. If it is a custom optimizer you wrote, maybe then you need to add to the path and supply the entire path in import for it to pick up. Not sure, if it has got anything to do with tf.keras [Refer to this link for the optimizers in tensorflow.keras https://www.tensorflow.org/api_docs/python/tf]\r\nHope this helps you"]}, {"number": 19912, "title": "tf dataset iterators producing Key error when used with tf.keras.Model fit method", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9.0-rc0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: model.fit(dataset_iterator,steps_per_epoch=2,batch_size=2,epochs=2,shuffle =True,verbose=1)\r\n\r\n### Describe the problem\r\n In the tf 1.9 rc0 release notes, it was mentioned that dataset iterator can work with Keras training and eval methods. However, the fit method is throwing the following error when used with an iterator.\r\n\r\n> No data provided for \"InputLayer\". Need data for each key in: ['InputLayer']\r\n\r\n### Source code / logs\r\n\r\n```\r\nX_dataset = tf.contrib.data.make_csv_dataset(file_names[0],4,select_columns=['Load_residential_multi_0','Load_residential_multi_1'],shuffle=False)\r\nY_dataset = tf.contrib.data.make_csv_dataset(file_names[0],4,select_columns=['Load_residential_multi_2'],shuffle=False)\r\ndataset = tf.data.Dataset.zip((X_dataset,Y_dataset))\r\ndataset = dataset.batch(2)\r\ndataset_iterator = dataset.make_one_shot_iterator()\r\n\r\nmodel = Sequential() \r\n#Input Layer\r\nmodel.add(InputLayer(input_shape=(1,),name='InputLayer'))#,input_tensor =dataset\r\n#Layer1 \r\nmodel.add(Dense(units=5,activation='relu',name='FeedForward1'))  #Add a feed forward layer\r\n#Layer2 \r\nmodel.add(Dense(units=5,activation='relu',name='FeedForward2'))  #Add a feed forward layer\r\n#Output layer \r\nmodel.add(Dense(units=2,name='OutputLayer'))\r\n#Specify loss function and optimizer\r\nmodel.compile(loss='mse',optimizer='adam',metrics=['mae'])\r\n#Summarize model\r\nmodel.summary()\r\n#Train model\r\nmodel.fit(dataset_iterator,steps_per_epoch=2,batch_size=2,epochs=2,shuffle =True,verbose=1)\r\n```\r\nThe error trace is given below:\r\n\r\n> KeyError                                  Traceback (most recent call last)\r\n> ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n>     125           if data[x].__class__.__name__ == 'DataFrame' else data[x]\r\n> --> 126           for x in names\r\n>     127       ]\r\n> \r\n> ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py in <listcomp>(.0)\r\n>     125           if data[x].__class__.__name__ == 'DataFrame' else data[x]\r\n> --> 126           for x in names\r\n>     127       ]\r\n> \r\n> KeyError: 'InputLayer'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-90-cd138934ce6f> in <module>()\r\n> ----> 1 model.fit(dataset_iterator,steps_per_epoch=2,batch_size=2,epochs=2,shuffle =True,verbose=1)\r\n> \r\n> ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n>    1256         steps_name='steps_per_epoch',\r\n>    1257         steps=steps_per_epoch,\r\n> -> 1258         validation_split=validation_split)\r\n>    1259 \r\n>    1260     # Prepare validation data.\r\n> \r\n> ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\r\n>     867         feed_input_shapes,\r\n>     868         check_batch_axis=False,  # Don't enforce the batch size.\r\n> --> 869         exception_prefix='input')\r\n>     870 \r\n>     871     if y is not None:\r\n> \r\n> ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n>     128     except KeyError as e:\r\n>     129       raise ValueError('No data provided for \"' + e.args[0] + '\". Need data '\r\n> --> 130                        'for each key in: ' + str(names))\r\n>     131   elif isinstance(data, list):\r\n>     132     if isinstance(data[0], list):\r\n> \r\n> ValueError: No data provided for \"InputLayer\". Need data for each key in: ['InputLayer']", "comments": ["@sibyjackgrove You are seeing this issue because the model is not compatible with the input provided by the csv dataset iterator. Model expects input to be a tensor or a list or a dict with key as layer name of the layer that has placeholder. In your case {'InputLayer': Tensor(...)}.\r\n\r\nHowever, iterator from make_csv_dataset provided a dictionary where key is csv column header and value is row value. This is not compatible with the model. We do not have a way of knowing whether the dictionary is from csv or if user has generated the layer name and value pairs. The not very elegant workaround you can try is to match the name of input layer with csv column name. Sorry about that.\r\n\r\nWe will make sure to add this information to our documentation and in the future see if we can improve support for this. Thank you!", "@pavithrasv thanks for pointing out why it is not working. But I think the underlying problem is being ignored. If  `make_csv_dataset ` is producing a different kind of dataset that cannot be used in places where other dataset iterators are used, why is it called a dataset in the first place.", "@sibyjackgrove i ran into the same problem - kinda confusing ...\r\nleaving out the input shape definition i even get a \r\n> ValueError: Please do not pass a dictionary as model inputs.\r\n\r\nThe workaround with renaming woun't work for multiple inputs.\r\nHow can i feed any DataSet into a Sequential Keras model if it doesn't accept dicts but prefers datasets ... \r\nbtw. using 1.11 by now ...\r\n\r\n@pavithrasv i aggree that you can't differentiate between layer dict and data dict in the validation itself however at the point where the Dataset is split in x and y parts one could implement different routines and e.g. maybe also pass a flag indicating that its a Dataset ordered dict as opposed to an input layer dict - or at this point transform it to an input layer dict which contains the list of items ... - or not?"]}, {"number": 19911, "title": "Adding report_tensor_allocations_upon_oom to RunOptions in Keras Ask Question", "body": "Im trying to add the report tensor GPU allocations for keras model to debug `tensorflow.python.framework.errors_impl.ResourceExhaustedError`.\r\n\r\nIm doing exactly like [this page](https://stackoverflow.com/questions/49665757/how-to-add-report-tensor-allocations-upon-oom-to-runoptions-in-keras?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) suggested, but I got this error\r\n\r\n```\r\nValueError: ('Some keys in session_kwargs are not supported at this time: %s', dict_keys(['options']))\r\n```\r\n\r\nIs there any other way for me to get the logs for memory allocation?\r\n\r\nIm using `tensorflow-gpu==1.8.0` and `Keras==2.2.0`", "comments": ["To be more specific,\r\n\r\nTensorflow version: `v1.8.0-0-g93bc2e2072 1.8.0`\r\n\r\nand this is the output of `tf_env_collect.sh`\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux ds-hl-experiment 4.13.0-1017-gcp #21-Ubuntu SMP Thu May 17 14:45:07 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ds-hl-experiment 4.13.0-1017-gcp #21-Ubuntu SMP Thu May 17 14:45:07 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                    1.14.4     \r\nprotobuf                 3.5.2.post1\r\ntensorflow-gpu           1.8.0      \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-9.0/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon Jun 11 17:38:34 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   35C    P0    34W / 250W |     82MiB / 16280MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1979      G   /usr/lib/xorg/Xorg                            82MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n```", "@brainnoise @fchollet : Mind taking a look. Seems like the set of supported kwargs is whitelisted in https://github.com/tensorflow/tensorflow/blob/c169282cfe03e146350d2e17f79be4bf759c4146/tensorflow/python/keras/backend.py#L2803 - but adding support for `run_options` should be trivial?", "@hardianlawi Hi, I create a PR #19932 to fix the issue. Would you like to apply the patch to your tensorflow and take a test?", "Nagging Assignees @fchollet, @pavithrasv: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @fchollet, @pavithrasv: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is there any workaround to this issue or should we just wait until they get time add the rest of the session features to the Model?", "Nagging Assignees @fchollet, @pavithrasv: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this coming in any given update?", "For anyone coming here, this might be related to https://github.com/keras-team/keras/issues/11322 .", "Is this still not fixed?", "How can I add `report_tensor_allocations_upon_oom=True` in recent Keras versions? Any updates?"]}, {"number": 19910, "title": "glorot_uniform_initializer and glorot_normal_initializer not consistent with the rest of the initializers?", "body": "In /ops/init_ops.py:\r\n\r\nAll the initializers inherit from class Initializer and implements:\r\ndef __call__(self, shape, dtype=None, partition_info=None)\r\n\r\nThis gives a different signature from the two glorot initializer functions (which are defs rather than classes), and the parameter 'partition_info' doesn't exist. This causes compatibility problems when passing them in as initializers.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hm. It looks those functions predate the Initializer class; however unifying that API would be nice.\r\n@annarev Can this be done without breaking backwards compatibility guarantees?", "This should be very straightforward as just wrapping the glorot initializers in a class inherited from Initializer, and then overriding the __call__ method to provide a default partition_info=None.\r\n\r\nThis parameter is not used in current implementation anyway. Though I assume it will be in the future to have consistent behavior with other initializers.\r\n\r\nTo tensortflowbutler,\r\nThe information requested is not relevant to my question, as it's directed to the source code.", "Added a PR #20108 to try to address the issue. Please take a look.", "Nagging Assignee @robieta: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19909, "title": "Tensorflow C++ API Batch Inference", "body": "Hello,\r\n\r\nMy question is considering the inference in C++ API using batches. I'm trying to run SSD net based on Mobilenet. It works on a single image, but when I try to use multiple images inference, I can't understand how to get the multiple outputs. It seems like my program runs properly as a run time increased for a batch inference though.\r\nHere is my sample code: I'm loading a batch of 32 images with OpenCV, transform them into a input tensor and run the Session. I need to understand what should I change in order to get 32 outputs. \r\nThank you.\r\n\r\n```\r\nint main(int argc, char* argv[]) {\r\n\t//string image(argv[1]);\r\n\tint batchSize = 32;\r\n\tstring pathFilenameImg1 = \"Patch6.jpg\";\r\n\tstring pathFilenameImg2 = \"Patch1.jpg\";\r\n\tstring pathFilenameImg3 = \"Patch2.jpg\";\r\n\tstring pathFilenameImg4 = \"Patch3.jpg\";\r\n\tstring pathFilenameImg5 = \"Patch0.jpg\";\r\n\tstring pathFilenameImg6 = \"Patch1.jpg\";\r\n\tstring pathFilenameImg7 = \"Patch2.jpg\";\r\n\tstring pathFilenameImg8 = \"Patch3.jpg\";\r\n\tstring graph = \"ssd_mobilenet.pb\";\r\n\tstring labels = \"security_labels.txt\";\r\n\tint32 input_width = 512;\r\n\tint32 input_height = 512;\r\n\tint32 input_depth = 3;\r\n\tstring input_layer = \"image_tensor\";\r\n\tvector<string> output_layer = { \"detection_boxes:0\", \"detection_scores:0\", \"detection_classes:0\", \"num_detections:0\"};\r\n\r\n\tbool self_test = false;\r\n\tstring root_dir = \"\";\r\n\r\n\t// First we load and initialize the model.\r\n\tstd::unique_ptr<tensorflow::Session> session;\r\n\tstring graph_path = tensorflow::io::JoinPath(root_dir, graph);\r\n\tLOG(ERROR) << \"graph_path:\" << graph_path;\r\n\tStatus load_graph_status = LoadGraph(graph_path, &session);\r\n\tif (!load_graph_status.ok()) {\r\n\t\tLOG(ERROR) << \"LoadGraph ERROR!!!!\" << load_graph_status;\r\n\t\treturn -1;\r\n\t}\r\n\r\n\r\n\t// Read and prepare images using OpenCV:\r\n\tstd::vector<string> imgPathArray= { pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, \r\n\t\t\t\t\t\t\t\t\t\tpathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1,\r\n\t\t\t\t\t\t\t\t\t\tpathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1,\r\n\t\tpathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1, pathFilenameImg1 ,pathFilenameImg2 ,pathFilenameImg3 ,pathFilenameImg1\r\n\t};\r\n\tstd::vector<cv::Mat> imgArray;\r\n\tfor (int i = 0; i < batchSize; i++)\r\n\t\timgArray.push_back(cv::imread(imgPathArray.at(i)));\r\n\r\n\r\n\t// creating a Tensor for storing the data\r\n\tTensor input_tensor(tensorflow::DT_UINT8, tensorflow::TensorShape({ batchSize, input_height, input_width, input_depth}));\r\n\tauto input_tensor_mapped = input_tensor.tensor<uchar, 4>();\r\n\r\n\tfor (int bz = 0; bz < batchSize; ++bz) {\r\n\t\tuchar *source_data;\r\n\t\tsource_data = imgArray.at(bz).data;\r\n\t\tfor (int y = 0; y < input_height; ++y) {\r\n\t\t\tuchar* source_row = source_data + (y * input_width * input_depth);\r\n\t\t\tfor (int x = 0; x < input_width; ++x) {\r\n\t\t\t\tuchar* source_pixel = source_row + (x * input_depth);\r\n\t\t\t\tuchar* source_B = source_pixel + 0;\r\n\t\t\t\tuchar* source_G = source_pixel + 1;\r\n\t\t\t\tuchar* source_R = source_pixel + 2;\r\n\t\t\t\tinput_tensor_mapped(bz, y, x, 0) = *source_R;\r\n\t\t\t\tinput_tensor_mapped(bz, y, x, 1) = *source_G;\r\n\t\t\t\tinput_tensor_mapped(bz, y, x, 2) = *source_B;\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\r\n\t/*// Get the image from disk as a float array of numbers, resized and normalized\r\n\t// to the specifications the main graph expects.\r\n\tstd::vector<Tensor> resized_tensors;\r\n\tstring image_path = tensorflow::io::JoinPath(root_dir, image);\r\n\tStatus read_tensor_status = ReadTensorFromImageFile(image_path, input_height, input_width, input_mean, input_std, &resized_tensors);\r\n\tif (!read_tensor_status.ok()) {\r\n\t\tLOG(ERROR) << read_tensor_status;\r\n\t\treturn -1;\r\n\t}\r\n\tconst Tensor& resized_tensor = resized_tensors[0];\r\n\r\n\tLOG(ERROR) << \"image shape:\" << resized_tensor.shape().DebugString() << \",len:\" << resized_tensors.size() << \",tensor type:\" << resized_tensor.dtype();*/\r\n\r\n\r\n\t// Actually run the image through the model.\r\n\tstd::vector<Tensor> outputs;\r\n\r\n\tstd::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();\r\n\tStatus run_status = session->Run({ { input_layer, input_tensor} }, output_layer, {}, &outputs);\r\n\tstd::chrono::steady_clock::time_point end = std::chrono::steady_clock::now();\tstd::cout << \"Time difference (sec) = \" << (std::chrono::duration_cast<std::chrono::microseconds>(end - begin).count()) / 1000000.0 << std::endl;\r\n\r\n\tif (!run_status.ok()) {\r\n\t\tLOG(ERROR) << \"Running model failed: \" << run_status;\r\n\t\treturn -1;\r\n\t}\r\n\r\n\t//int image_width = resized_tensor.dims();\r\n\t//int image_height = 0;\r\n\t//int image_height = resized_tensor.shape()[1];\r\n\r\n\t//LOG(ERROR) << \"output size:\" << outputs.size() << \",image_width:\" << image_width << \",image_height:\" << image_height << endl;\r\n\r\n\tauto boxes = outputs[0].flat_outer_dims<float, 3>();\r\n\ttensorflow::TTypes<float>::Flat scores = outputs[1].flat<float>();\r\n\ttensorflow::TTypes<float>::Flat classes = outputs[2].flat<float>();\r\n\ttensorflow::TTypes<float>::Flat num_detections = outputs[3].flat<float>();\r\n\r\n\r\n\t//LOG(ERROR) << \"num_detections:\" << num_detections(0) << \",\" << outputs[0].shape().DebugString();\r\n\tint BarcodeCnt = 0;\r\n\tint GyoshCnt = 0;\r\n\tfor (size_t i = 0; i < num_detections(0) && i < 20; ++i)\r\n\t{\r\n\t\tif (scores(i) > 0.9)\r\n\t\t{\r\n\t\t\tLOG(ERROR) <<\"score:\" << scores(i) << \",class:\" << classes(i) << \",box:\" << \",\" << boxes(0, i, 0) << \",\" << boxes(0, i, 1) << \",\" << boxes(0, i, 2) << \",\" << boxes(0, i, 3);\r\n\t\t\tif(classes(i) == 1)\r\n\t\t\t\tBarcodeCnt++;\r\n\t\t\telse\r\n\t\t\t\tGyoshCnt ++;\r\n\t\t}\r\n\t}\r\n\tLOG(ERROR) << \"Total number of Security Barcodes is :\" << BarcodeCnt;\r\n\tLOG(ERROR) << \"Total number of Security Gyoshes is :\" << GyoshCnt;\r\n\r\n\t\r\n\r\n\r\n\treturn 0;\r\n}\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code     - Yes\r\nOS Platform and Distribution - Win10 x64\r\nTensorFlow installed from - Tensorflow site\r\nTensorFlow version - 1.8\r\nBazel version - N/A\r\nCUDA/cuDNN version    CUDA 9.0/ CuDNN 7\r\nGPU model and memory   Quadro P4000, 8GB\r\nExact command to reproduce - Code pasted above in my original comment.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "You are right, it's not a bug in the code, however,  there is no explanation on this issue anywhere in Tensorflow C++ API documentation. I consider it a serious issue, as I am left guessing what should be done in order to get my code working.\r\nDespite StackOverflow being a bigger community, there are no dedicated persons to deal with those specific issues. My question there remains unanswered and a previous same question from 2016 is still open.\r\n\r\nI expect you guys to have a knowledge to help developers and not leaving them frustrated.\r\n\r\nThank you,\r\nAlex.", "Thanks, @spivakoa for clarifying-- if this is specifically a request for documentation, we can address that. Can you clarify what function/process you feel lacks documentation? A link would help.", "It is in the name of the topic: How can I run inference on batch of images instead of a single one?\r\nIn my code example above I showed how I \"fill\" an input 4-D Tensor with pixel values.\r\nHere it is:\r\n\r\n``` c++\r\n// creating a Tensor for storing the data\r\nTensor input_tensor(tensorflow::DT_UINT8, tensorflow::TensorShape({ batchSize, input_height, input_width, input_depth}));\r\nauto input_tensor_mapped = input_tensor.tensor<uchar, 4>();\r\n\r\nfor (int bz = 0; bz < batchSize; ++bz) {\r\n\tuchar *source_data;\r\n\tsource_data = imgArray.at(bz).data;\r\n\tfor (int y = 0; y < input_height; ++y) {\r\n\t\tuchar* source_row = source_data + (y * input_width * input_depth);\r\n\t\tfor (int x = 0; x < input_width; ++x) {\r\n\t\t\tuchar* source_pixel = source_row + (x * input_depth);\r\n\t\t\tuchar* source_B = source_pixel + 0;\r\n\t\t\tuchar* source_G = source_pixel + 1;\r\n\t\t\tuchar* source_R = source_pixel + 2;\r\n\t\t\tinput_tensor_mapped(bz, y, x, 0) = *source_R;\r\n\t\t\tinput_tensor_mapped(bz, y, x, 1) = *source_G;\r\n\t\t\tinput_tensor_mapped(bz, y, x, 2) = *source_B;\r\n\t\t}\r\n\t}\r\n} \r\n```\r\n\r\nThen I run the Graph:\r\n\r\n```c++\r\nStatus run_status = session->Run({ { input_layer, input_tensor} }, output_layer, {}, &outputs);\r\n```\r\nHowever the outputs is vector of Tensors with size of 4, because the output layer is defined as follows:\r\n\r\n``` c++\r\noutput_layer = { \"detection_boxes:0\", \"detection_scores:0\", \"detection_classes:0\", \"num_detections:0\"};\r\n```\r\n\r\nI need to understand how should I define the output to reveal 4xbatch size values.\r\n\r\nThis what Tensorflow documentation lacks with. No explanation whatsoever of how to run multiple images using C++ API.", "@MarkDaoust - perhaps running inference would be a good notebook to include with the set of examples?", "@MarkDaoust Any thoughts? Thx.", "Hey @spivakoa \r\n\r\nIs this based on an existing example? which one (there are lots of example folders)? We could look into adding some better handling for batching there.\r\n\r\nI haven't used the c++ interface (but java is similar) so I'm not 100% sure about this, but I think the problem is here:\r\n\r\n```c++\r\n\tauto boxes = outputs[0].flat_outer_dims<float, 3>();\r\n\ttensorflow::TTypes<float>::Flat scores = outputs[1].flat<float>();\r\n\ttensorflow::TTypes<float>::Flat classes = outputs[2].flat<float>();\r\n\ttensorflow::TTypes<float>::Flat num_detections = outputs[3].flat<float>();\r\n```\r\n\r\nThe first dimension of the output tensors should be the `batch` dimension. Can you check the shapes of the tensors to confirm?\r\n\r\nBut it looks like you're flattening the tensors (to remove the batch dimension for the batch_size=1 base case). if you check the length of those resulting tensors you should see that they're `batch` times as long.\r\n\r\n If you can iterate or index along that first dimension those slices will be the single image results you're looking for.\r\n\r\n", "Hi Mark,\r\nI wrote my code based on the following code:\r\nhttps://github.com/memo/ofxMSATensorFlow/issues/34\r\n\r\n\r\nMy output Tensor is defined as follows:\r\n\r\n    output_layer = { \"detection_boxes:0\", \"detection_scores:0\", \"detection_classes:0\", \"num_detections:0\"};\r\n\r\nThose variables are taken from the net output  layer:\r\n\r\n    vector<string> output_layer = { \"detection_boxes\", \"detection_scores\", \"detection_classes\", \"num_detections\"};\r\n\r\nThus the first dimension is the detection_boxes coordinates from the SSD.\r\n\r\nI thought that using batch inference should result in an additional dimension in output variable, maybe defining it as a vector of vector of Tensors, but this apparently doesn't work. The output dimension is dependent on a number of outputs in the outer net layer, 4 in my case.\r\n\r\nIt seems like something \"good\" happens when increasing the batch size as the run time of the code also increases, I just can't figure out where to find the particular output for each batch image.\r\n\r\nThank you,\r\nAlex.  ", "Hey @MarkDaoust,\r\n\r\nDo you have any update on this issue?\r\n\r\nThank you,\r\nAlex.", "Hi, thanks for the ping.\r\n\r\n@skye, we've got a couple of questions about handling batching in the CC api.  Do you know any good examples demonstrating this? Am I pointing @spivakoa in the right direction here?\r\n\r\nI haven't gotten to look at this in more detail, but it looks like we still don't understand each other.\r\n\r\n> ```\r\n> vector<string> output_layer = { \"detection_boxes\", \"detection_scores\", \"detection_classes\", \"num_detections\"};\r\n> ```\r\n>Thus the first dimension is the detection_boxes coordinates from the SSD.\r\n\r\nThis thing returns 4 tensors.\r\nEach tensor is a multidimensional array.\r\nThe first dimension of the array is (probably) the batch size.\r\n\r\nThis line (looks like) it's flattening the `num_detections` tensor into a 1D array:\r\n\r\n    tensorflow::TTypes<float>::Flat num_detections = outputs[3].flat<float>();\r\n\r\nAnd this line is only checking the detections in the first item in the batch:\r\n\r\n    for (size_t i = 0; i < num_detections(0) && i < 20; ++i)\r\n\r\nYou need some sort of outer loop over the number of detections in each image. this is only looking at the first image, which made sense with the old batch size of 1.", "I don't know of any examples, sorry. I believe the C++ API is only supported as \"best effort\", so we don't provide canonical examples like we do for Python.", "@spivakoa Try this modified code from tensorflow examples:\r\n```cpp\r\nbool Run(const std::vector<cv::Mat> &images, std::vector<Result> &results) const {\r\n    std::string inputLayer = \"image_tensor:0\";\r\n    std::vector<std::string> outputLayer = {\"detection_boxes:0\", \"detection_scores:0\", \"detection_classes:0\",\r\n                                            \"num_detections:0\"};\r\n\r\n    int height = images.front().rows;\r\n    int width = images.front().cols;\r\n    int depth = 3;\r\n    int batch = (int) images.size();\r\n    tensorflow::Tensor input(tensorflow::DT_UINT8, tensorflow::TensorShape({batch, height, width, depth}));\r\n    {\r\n        std::uint8_t *dataPtr = input.flat<std::uint8_t>().data();\r\n        for (const auto &image : images) {\r\n            cv::Mat tmp(height, width, CV_8UC3, dataPtr);\r\n            image.convertTo(tmp, CV_8UC3);\r\n            dataPtr += height * width * depth;\r\n        }\r\n    }\r\n\r\n    std::vector<tensorflow::Tensor> outputs;\r\n    tensorflow::Status runStatus = session_->Run({{inputLayer, input}}, outputLayer, {}, &outputs);\r\n    if (!runStatus.ok())\r\n        return false;\r\n\r\n    auto boxes = outputs[0].flat_outer_dims<float, 3>();\r\n    auto scores = outputs[1].matrix<float>();\r\n    auto classes = outputs[2].matrix<float>();\r\n    auto num_detections = outputs[3].flat<float>();\r\n\r\n    for (size_t b = 0; b < batch; ++b) {\r\n        for (size_t i = 0; i < num_detections(b); ++i) {\r\n            if (scores(b, i) > kProbabilityThreshold) {\r\n                Result result;\r\n                result.Score = scores(b, i);\r\n                result.ClassId = classes(b, i);\r\n                result.Box = cv::Rect(\r\n                        cv::Point(boxes(b, i, 1) * width, boxes(b, i, 0) * height),\r\n                        cv::Point(boxes(b, i, 3) * width, boxes(b, i, 2) * height));\r\n                results.push_back(result);\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}\r\n```", "Nagging Assignee @MarkDaoust: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@spivakoa : So does @ilidar's answer solve this problem?\r\n\r\n@ilidar if this is the solution could you submit it as a PR to the file you based this on?\r\n\r\nThanks.", "@spivakoa I have the similar code about opencv and tensorflow c++ interface compile together in Ubuntu16.04, but I got many opencv error while compiling like this: /usr/include/opencv2/contrib/contrib.hpp:393:9: error: reference to 'int64' is ambiguous\r\nmy CMakeLists.txt defined like:\r\n``` cmake\r\nFILE(GLOB HEADER_FILES *.h)\r\nfind_package(OpenCV REQUIRED highgui imgproc)\r\n\r\ninclude(CheckCXXCompilerFlag)\r\n CHECK_CXX_COMPILER_FLAG(\"-std=c++11\" COMPILER_SUPPORTS_CXX11)\r\n CHECK_CXX_COMPILER_FLAG(\"-std=c++0x\" COMPILER_SUPPORTS_CXX0X)\r\n if(COMPILER_SUPPORTS_CXX11)\r\n   set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -fPIC -O3\")\r\n elseif(COMPILER_SUPPORTS_CXX0X)\r\n   set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++0x\")\r\n else()\r\n   message(STATUS \"The compiler ${CMAKE_CXX_COMPILER} has no C++11 support. Please use a different C++ compiler.\")\r\n endif()\r\n\r\nset(eigen_root_path ${PROJECT_SOURCE_DIR}/../../../software/eigen3)\r\nset(tf_root_path ${PROJECT_SOURCE_DIR}/../../../tensorflow)\r\nset(glog_root_path ${PROJECT_SOURCE_DIR}/../../../software/glog-0.3.5)\r\nset(gtest_root_path ${PROJECT_SOURCE_DIR}/../../../software/gtest-1.8.0)\r\n\r\ninclude_directories(\r\n  ${gtest_root_path}/include\r\n  ${glog_root_path}/include\r\n  ${tf_root_path}/bazel-genfiles\r\n  ${tf_root_path}/tensorflow/contrib/makefile/gen/protobuf/include\r\n  ${tf_root_path}\r\n  ${eigen_root_path}\r\n  ${OpenCV_INCLUDE_DIRS}\r\n)\r\n\r\nlink_directories(/usr/local/lib ${glog_root_path}/lib ${OpenCV_LIBRARY_DIRS})\r\n\r\nadd_library(tf_lib SHARED\r\n  util/inifile.cc\r\n  util/util.cc\r\n  ${HEADER_FILES}\r\n)\r\n\r\ntarget_link_libraries(tf_lib glog tensorflow_cc tensorflow_framework ${OpenCV_LIBS})\r\n\r\n```\r\nI think these error come from tensorflow defined symbols, that opencv redefined, how can I fix it, can you help me? ", "@mrlittlepig This error has nothing to do tensorflow. Your problem is inside the contrib library in opencv.\r\nIf you go to opencv2/contrib/contrib.hpp:393:9 I guess you will find some kind of definition like:       \r\ntypedef int64_t int64;\r\nIt is conflicting with something else in your code. Try to check this.", "@spivakoa Hi,could you tell me the batch inference is slower or not than single inference? I test it [https://stackoverflow.com/questions/57460782/batch-inference-is-as-slow-as-single-image-inference-in-tensorflow-c](url) and find the time is too long ."]}, {"number": 19908, "title": "how to used nccl.reduce_sum", "body": "is there some example to use nccl.reduce_sum , as the doc say:\r\n```\r\n// Note: This op has no kernel implementation, but is replaced by\r\n// _NcclReduceSend and _NcclReduceRecv during graph optimization stage.\r\n```\r\nso  how the nccl.reduce_sum works?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19907, "title": "CreateSession still waiting for response from worker: /job:ps/replica:0/task:0", "body": "OS Platform and Distribution : CentOS7.4\r\nTensorFlow installed from : source code tensorflow-base:1.4.0\r\nTensorFlow version: 1.4.0\r\nCUDA/cuDNN version:1.8\r\nGPU model and memory:nvidia_geforce_gtx_1080_ti\r\n NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 23%   29C    P8    16W / 250W |  10623MiB / 11172MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n| 25%   26C    P8    16W / 250W |     10MiB / 11172MiB |      0%      Default |\r\n\r\nthe pods are as below\uff1a\r\n[root@k8s-node1 ~]# kubectl --namespace=liuning get pod -owide\r\nNAME                          READY     STATUS    RESTARTS   AGE       IP              NODE\r\ntensorfenbus-ps-0             1/1       Running   0          27m       10.10.36.111    k8s-node1\r\ntensorfenbus-session-fh65h    1/1       Running   0          26m       10.10.169.152   k8s-node2\r\ntensorfenbus-tf-board-n6d28   2/2       Running   0          26m       10.10.169.151   k8s-node2\r\ntensorfenbus-worker-0         1/1       Running   0          27m       10.10.169.150   k8s-node2\r\ntensorfenbus-worker-1         1/1       Running   0          27m       10.10.36.112    k8s-node1\r\n\r\nIn tensorfenbus-worker-1 i run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \\\r\n--batch_size=32 --model=resnet50 \\\r\n--job_name=worker --ps_hosts=k8s-node1:2225 \\\r\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0 \r\n\r\nand then in tensorfenbus-ps-0 run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \\\r\n--batch_size=32 --model=resnet50 \\\r\n--job_name=ps --ps_hosts=k8s-node1:2225 \\\r\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0\r\n\r\nbut the work node log show \uff1a\r\n2018-06-11 11:34:29.568785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 10.91GiB freeMemory: 398.38MiB\r\n2018-06-11 11:34:29.568848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nE0611 11:34:29.572333602     348 ev_epoll1_linux.c:1051]     grpc epoll fd: 24\r\n2018-06-11 11:34:29.577958: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> k8s-node1:2225}\r\n2018-06-11 11:34:29.577990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2225, 1 -> k8s-node2:2225}\r\n2018-06-11 11:34:29.580147: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2225\r\nTensorFlow:  1.4\r\nModel:       resnet50\r\nMode:        training\r\nBatch size:  32 global\r\n             32 per device\r\nDevices:     ['/job:worker/task:0/gpu:0']\r\nData format: NCHW\r\nOptimizer:   sgd\r\nVariables:   parameter_server\r\nSync:        True\r\n==========\r\nGenerating model\r\nWARNING:tensorflow:From tf_cnn_benchmarks.py:772: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_or_create_global_step\r\nWARNING:tensorflow:From tf_cnn_benchmarks.py:627: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_global_step\r\n2018-06-11 11:34:44.477725: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:34:44.477831: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:34:54.477958: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:34:54.478023: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:04.478242: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:04.478340: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:14.478475: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:14.478546: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:24.478709: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:24.478773: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:34.478927: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:34.478997: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:44.479163: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:44.479227: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:35:54.479470: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:35:54.479553: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:36:04.479720: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:36:04.479781: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2018-06-11 11:36:14.479936: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2018-06-11 11:36:14.479999: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n\r\n\r\n**where's wrong**", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@cy89  i update the information,i didnot written custom code\r\n\r\nOS Platform and Distribution : CentOS7.4\r\nTensorFlow installed from : source code tensorflow-base:1.4.0\r\nTensorFlow version: 1.4.0\r\nCUDA/cuDNN version:1.8\r\nGPU model and memory:nvidia_geforce_gtx_1080_ti\r\nNVIDIA-SMI 384.90 Driver Version: 384.90 |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r\n|===============================+======================|\r\n| 0 GeForce GTX 108... Off | 00000000:02:00.0 Off | N/A |\r\n| 23% 29C P8 16W / 250W | 10623MiB / 11172MiB | 0% Default |\r\n+-------------------------------+----------------------+----------------------+\r\n| 1 GeForce GTX 108... Off | 00000000:04:00.0 Off | N/A |\r\n| 25% 26C P8 16W / 250W | 10MiB / 11172MiB | 0% Default |\r\n\r\nthe pods are as below\uff1a\r\n[root@k8s-node1 ~]# kubectl --namespace=liuning get pod -owide\r\nNAME READY STATUS RESTARTS AGE IP NODE\r\ntensorfenbus-ps-0 1/1 Running 0 27m 10.10.36.111 k8s-node1\r\ntensorfenbus-session-fh65h 1/1 Running 0 26m 10.10.169.152 k8s-node2\r\ntensorfenbus-tf-board-n6d28 2/2 Running 0 26m 10.10.169.151 k8s-node2\r\ntensorfenbus-worker-0 1/1 Running 0 27m 10.10.169.150 k8s-node2\r\ntensorfenbus-worker-1 1/1 Running 0 27m 10.10.36.112 k8s-node1\r\n\r\nIn tensorfenbus-worker-1 i run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \r\n--batch_size=32 --model=resnet50 \r\n--job_name=worker --ps_hosts=k8s-node1:2225 \r\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0\r\n\r\nand then in tensorfenbus-ps-0 run python tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=1 \r\n--batch_size=32 --model=resnet50 \r\n--job_name=ps --ps_hosts=k8s-node1:2225 \r\n--worker_hosts=k8s-node1:2225,k8s-node2:2225 --task_index=0"]}, {"number": 19906, "title": "Build OK ,but My own C++ project has error :\" std::bad_alloc \"", "body": "**System information:**\r\n\u00b7 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n\r\n\u00b7 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10-x64\r\n\r\n\u00b7 TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n\u00b7 TensorFlow version (use command below): r1.8, command :\r\ngit checkout -b v1.8 -f origin/r1.8\r\n\r\n\u00b7 Python version: Anaconda3 - python3.6\r\n\r\n\u00b7 Bazel version (if compiling from source): I used CMAKE 3.11.1\r\n\r\n\u00b7 GCC/Compiler version (if compiling from source): both Visual Studio 2015 and Visual Studio 2015' MSBuild\r\n\r\n\u00b7 CUDA/cuDNN version: CUDA9.0, cudnn-9.0-win10-7.1\r\n\r\n\u00b7 GPU model and memory: GTX-860m with 2Gb Memory\r\n\r\nExact command to reproduce:\r\n\"\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> cmake .. -A x64 -DCMAKE_BUILD_TYPE=**Release** -DSWIG_EXECUTABLE=D:/soft/TensorflowSoft/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/ProgramData/Anaconda3/python.exe -DPYTHON_LIBRARIES=D:/ProgramData/Anaconda3/libs/python36.lib -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"D:\\soft\\TensorflowSoft\\cudnn\" -G \"Visual Studio 14 2015\"\r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> set PreferredToolArchitecture=x64\r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> MSBuild /p:Configuration=**Release** ALL_BUILD.vcxproj\r\n\"\r\n\r\n**Describe the problem :**\r\n\r\nI build tensorflow-**GPU-Release** version sucessfully (even though GPU - Debug version had error). My project code is :\r\n\r\n\" \r\n```\r\nGraphDef CMFCtensorTest01Dlg::CreateGraphDef()\r\n{      \r\n     Scope root = Scope::NewRootScope();\r\n\r\n     auto X = ops::Placeholder(root.WithOpName(\"x\"), DT_FLOAT, ops::Placeholder::Shape({ -1, 2 }));\r\n\r\n     auto A = ops::Const(root, { { 3.f, 2.f },{ -1.f, 0.f } });\r\n\r\n     auto Y = ops::MatMul(root.WithOpName(\"y\"), A, X,\r\n     ops::MatMul::TransposeB(true));\r\n\r\n     GraphDef def;\r\n     TF_CHECK_OK(root.ToGraphDef(&def));\r\n     return def;\r\n}\r\n```\r\n\"\r\n\r\nError running to the second line :\"**auto X = ops::Placeholder(root.WithOpName(\"x\"), DT_FLOAT, ops::Placeholder::Shape({ -1, 2 }))**;\"\r\n\r\n\r\nThe error is : \"0x00007FF95998F218 \u5904(\u4f4d\u4e8e MFCtensorTest01.exe \u4e2d)\u5f15\u53d1\u7684\u5f02\u5e38: Microsoft C++ \u5f02\u5e38: std::bad_alloc\uff0c\u4f4d\u4e8e\u5185\u5b58\u4f4d\u7f6e 0x00000065700F9280 \u5904\u3002\"\r\n\r\nThese code worked successfully on my **CPU - Debug** version build,  **GPU - Release** version had error. I think this error is about application memory.\r\n\r\nNot long ago I tested it, if I change the **GUP - Release\u2018 s _pywrap_tensorflow_internal.pyd**  to **CPU - Debug \u2018s _pywrap_tensorflow_internal.pyd**\uff0ccode runs successfully\uff08I build Win10 - CPU - Debug version **successfully** several days ago\uff09\uff0cI think the **GUP - Release\u2018 s _pywrap_tensorflow_internal.pyd** had something wrong.", "comments": ["@tensorflowbutler  No one answer me...", "I sovled this problem\uff0c Because I Build tensorflow with Release version \uff0c My own project is Debug version\u3002 If I use Release\uff0cit  runs OK.", "Nagging Assignee @michaelisard: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "> I sovled this problem\uff0c Because I Build tensorflow with Release version \uff0c My own project is Debug version\u3002 If I use Release\uff0cit runs OK.\r\n\r\nThanks point this problem, I also meet this problem.", "Thank you for help, the problem is really caused by debug, when I changed release, it runs well.\r\nso a tips, very very carefully about  debug or release version.", "> I sovled this problem\uff0c Because I Build tensorflow with Release version \uff0c My own project is Debug version\u3002 If I use Release\uff0cit runs OK.\r\n\r\nthanks a lot!\r\nhow did you find the reason, that is really hard to recognize release debug difference cause this!"]}, {"number": 19905, "title": "Mirror link is broken and issue is difficult to debug.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.\n\nOn Mon 11 Jun, 2018, 3:16 PM googlebot, <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19905#issuecomment-396183747>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHTmT7BSdGIrJtH4rrwp58fh39N3x8Coks5t7jyBgaJpZM4UiXtE>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "cc @martinwicke @gunan \r\n\r\nDo we know why the link `https://storage.googleapis.com/cvdf-datasets/mnist/` stopped working?", "I also want to know if the google's site for mnist dataset will be back again or we should use the one from this PR.", "Since all this is deprecated, I don't mind changing the link. \r\n\r\n@wolffg I recall some more link gymnastics here, but I don't remember the outcome. Do we have our own copy?", "This was a problem with the mirror, which has now been fixed. Sorry about the disruption. We prefer to keep the mirror as to avoid DDOSing Yann Lecun's page."]}, {"number": 19903, "title": "Warm-starting moving averages (batch norm) as part of an estimator not possible", "body": "I want to use a `tf.estimators.Estimator` to fine-tune a model that contains batch normalization (e.g. ResNet). To initialize the model, I use the new `WarmStartSettings` and pass it to the estimator's `warm_start_from` argument. Unfortunately, this will only warm-start trainable variables, and the `moving_mean` and `moving_variance` created by the batch normalization layer are not part of the trainable variables collection. Thus, the moving averages will not be warm-started.\r\n\r\nBecause of this problem, TensorFlow 1.9 (rc0) introduced the possibility to pass a list of variables to the `WarmStartSettings`. Unfortunately, when using estimators, this does not help because the variables are recreated all the time and not know at the position where the warm start settings have to be defined.\r\n\r\nA possible solution might be to make it possible to pass a function to the `warm_start_from` argument of estimators that has access to the current graph and returns a `WarmStartSettings` object.\r\n\r\nMy current workaround is to specify a list of variable names rather than variables in the `WarmStartSettings`, but this is not exactly how it's supposed to be and comes with its own problems (e.g. getting the list of variable names before the model was built; I just use all the variable names that are saved in the checkpoint and exlude e.g. `global_step`, but this is problematic because it circumvents certain checks and assertions).\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: all up to 1.9rc0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "/cc @tanzhenyu @martinwicke  for https://github.com/tensorflow/tensorflow/issues/17950#issuecomment-404707663", "See also https://github.com/keras-team/keras/pull/9965#issuecomment-398183440 /cc @ppwwyyxx\r\n\r\nCheck also reviews/comments in https://openreview.net/forum?id=Hk6dkJQFx\r\n\r\n", "Assuming you are writing your own `model_fn` (not using a canned `tf.Estimator`), can you try not using `tf.estimator.WarmStartSettings` during estimator `init`, and instead directly using `tf.train.warm_start` inside your `model_fn`?  That way you have direct access to the `variables` you've created inside the `model_fn`.  This is similar to directly using `tf.init_from_checkpoint`.", "@eddie-zhou `tf.train.warm_start` excludes variables such as accumulators and moving statistics from batch norm.\r\n\r\nI wonder why ?\r\n\r\nAnd thus `tf.train.warm_start` is useless for this issues.\r\n", "You're correct that it only includes vars in TRAINABLE_VARIABLES by default, but instead of the default regex, you can specify exactly what variables to warm-start, and include the accumulators and moving stats explicitly.", "@eddie-zhou Honestly it is quite  a regular task so probably is better to have a new parameter", "@bhack Could you make explicit your suggested API change?  Note that warm-starting accumulators is definitely inadvisable in many settings (given that new vocabulary entries have newly initialized accumulators), which is why we made the choice for the default to not include them.", "I meant:\r\n> you can specify exactly what variables to warm-start, and include the accumulators and moving stats explicitly\r\n\r\nSo I think that you can have a new parameter to recover accumulators and moving stats explicitly for the batch norm case without going to cherry picking variable with regex. ", "I agree with @bhack. For many common real-world tasks related to warm-starting (fine-tuning existing networks, pretraining on a different dataset, etc.), it is important to restore not just the variables trained with backprop (i.e. the `trainable_variables`) but also the variables _trained_ using dataset statistics (e.g. accumulators, moving averages, etc.). Forgetting to restore them is such a common source of problems that are hard to notice; ideally it should be a very simple to switch between restoring `trainable_variables` and all _trainable variables_ (i.e. including accumulators, \u2026) (as you notice: I even think it's a misnomer to only refer to some of them as _trainable_ variables).\r\n", "Thanks for your thoughts, @jonasrauber!  I do agree that it can be useful in many settings, but by not being prescriptive in the API, we intentionally force users to explicitly think about exactly what variables they want to warm-start, and whether or not it is appropriate for their task.  Does that make sense?", "@eddie-zhou In principle I agree with the approach of _forcing explicit decisions_ (though I guess most ML researchers prefer more practical defaults like _restoring everything_). But in the case here, TensorFlow is not actually forcing an explicit decision, it's defaulting to a non-intuitive behavior (restoring layer-params, but not batch-norm params) that can cause silent bugs that may go unnoticed for a while (in fact, I have seen this happening to several people in our group). In practice, people will not notice that they haven't restored the batch norm params and assume they did.", "That's fair! I believe the notion of non-intuitive is subjective, though, depending on use cases across different systems that use TensorFlow.  For now, we'll lean on letting users read the documentation for `vars_to_warm_start` in [tf.estimator.WarmStartSettings](https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings) and [tf.train.warm_start](https://www.tensorflow.org/api_docs/python/tf/train/warm_start) which states `Defaults to '.*', which warm-starts all variables in the TRAINABLE_VARIABLES collection. Note that this excludes variables such as accumulators and moving statistics from batch norm.`", "That _Note_ is certainly helpful, but maybe you can make it more prominent. Also: adding an explanation _how_ to change this (how to also restore accumulators) might help (in particular in situations like the one I described when opening this issue).\r\n\r\nIn any case: Thanks a lot for listening to our feedback :) ", "I think that frequent routines need to be automated in the API then flexibility could be useful for less frequent use cases. This my interpretation of what to be intuitive mean. ", "FYI, an internal contribution has resulted in the following functionality being added:  vars_to_warm_start can now be \"a function returning a boolean indicating whether a variable should warm-started. This function will be called for each item returned by `tf.get_collection(GLOBAL_VARIABLES)`\".  This makes selecting non-trainable variables (like batch-norm) much easier. ", "Based on [current implementation](https://github.com/tensorflow/tensorflow/blob/f382bd6279141fe169e5aa8ab0867f18840ee037/tensorflow/python/training/warm_starting_util.py#L335), if a list of regex is passed as `vars_to_warm_start`, then all matched variables in GLOBAL_VARIABLES will be restored. I think the easiest way to restore everything is `vars_to_warm_start=['.*']`", "I have tried to warm up variebles  resnet_v2_101  /BatchNorm/moving_mean  and   resnet_v2_101/block3/unit_13/bottleneck_v2/conv1/BatchNorm/moving_variance.\r\n\r\nif use vars_to_warm_start=['.*'], when optimizer change, it will restore failed.\r\n\r\nany good way to do it ?", "Correction on my above statement: the internal contribution was not approved.\r\n\r\nRe: @offbye -- can you provide the failure message?  Are you sure that those are your variable names?", "Just flagging this here for others who hit the same issue (in my case, running TF 1.12).  The default behaviour affects warm starting from pre-trained Keras models that include batch norm (means and variances are not loaded after the warmstart) when calling [model_to_estimator()](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator). The function call happens [here](https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/keras.py#L502-L504).\r\n\r\nAs a temporary fix, if you need to extract features with a pretrained model, you can patch around it with @ychfan\u2019s suggestion - e.g. updating `tensorflow_estimator/python/estimator/keras.py` to use the code below, but for the general use-case this may be undesirable for other reasons:\r\n\r\n```\r\n  ws = estimator_lib.WarmStartSettings(\r\n    ckpt_to_initialize_from=warm_start_path,\r\n    vars_to_warm_start=['.*'],\r\n  )\r\n  estimator = estimator_lib.Estimator(keras_model_fn,\r\n                                      config=config,\r\n                                      warm_start_from=ws)\r\n```", "Hi, @ychfan @albanie ,\r\n\r\n I am actually a bit confused here. Does the following code warm-starts ALL variables, including TRAINABLE and GLOBAL ones ? \r\n```\r\nws = estimator_lib.WarmStartSettings(\r\n    ckpt_to_initialize_from=warm_start_path,\r\n    vars_to_warm_start=['.*'],\r\n  )\r\n  estimator = estimator_lib.Estimator(keras_model_fn,\r\n                                      config=config,\r\n                                      warm_start_from=ws)\r\n\r\n```\r\nThis doesn't seem to be the case in the code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/warm_starting_util.py#L315\r\n\r\nbut when I tried this it seems to warm-start all variables.\r\n\r\n`Update`: hmm.. OK all trainable_variables are in global_variables.\r\n ", "@jonasrauber Did you get chance to try the code suggested in previous comments. If it worked let us know  we will close this issue", "@jonasrauber  Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Since this issue was closed without an actual resolution, here's a workaround that warm starts all trainable variables and batch norm variables but none of the other global variables:\r\n\r\n```\r\nclass WarmStartHook(tf.estimator.SessionRunHook):\r\n  def __init__(self, checkpoint):\r\n    self.checkpoint = checkpoint\r\n\r\n  def begin(self):\r\n    regex = '.*batch_normalization/(moving.*|gamma|beta):0$'\r\n    tf.train.warm_start(self.checkpoint)\r\n    tf.train.warm_start(self.checkpoint, [regex])\r\n```\r\n\r\nYou can use this just like any other Estimator training hook."]}, {"number": 19902, "title": "Disable tensorflow/python/estimator:keras_test on Windows", "body": "@gunan \r\nTo fix TF postsubmit https://source.cloud.google.com/results/invocations/9b835a63-a61d-42e5-af4c-3994e6b77469/log\r\n```\r\n======================================================================\r\nERROR: test_multi_inputs_multi_outputs (__main__.TestKerasEstimator)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_ea0oxmc9\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\estimator\\keras_test.py\", line 388, in test_multi_inputs_multi_outputs\r\n    keras_model=model, config=self._config)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\", line 534, in model_to_estimator\r\n    keras_weights)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\", line 434, in _save_first_checkpoint\r\n    custom_objects)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\", line 300, in _clone_and_build_model\r\n    model = models.clone_model(keras_model, input_tensors=input_tensors)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 263, in clone_model\r\n    return _clone_functional_model(model, input_tensors=input_tensors)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 156, in _clone_functional_model\r\n    **kwargs))\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 703, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 711, in call\r\n    return self.function(inputs, **arguments)\r\n  File \"C:/Python36/lib/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 1284, in string_to_number\r\n    name=name)\r\nSystemError: unknown opcode\r\n```", "comments": []}, {"number": 19901, "title": "[TF-serving-r1.7] How to compile Tensorflow-serving r1.7 with TensorRT", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  CentOS 7\r\n- **TensorFlow installed from (source or binary)**: tensorflow-serving r1.7 source code\r\n- **TensorFlow version (use command below)**: tensorflow-serving r1.7 source code\r\n- **Python version**: Python2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc5.3\r\n- **CUDA/cuDNN version**: CUDA9.0, cuDNN7.0.5\r\n- **GPU model and memory**: TitanXP 12GB\r\n- **TensorRT Installed Version**: 4.0.4 (actually) \r\n- **Exact command to reproduce**: The way of compiling tf-serving 1.7 with TenosrRT, Please refer to my environment variables setting and compilation command as below.\r\n\r\n****************************************\r\nThis issue is originally posted in repo tensorflow-serving (https://github.com/tensorflow/serving/issues/925). After several days of effort, I still failed to find a way to solve this problem. Hope someone can give me a clue of how to build tensorflow-serving 1.7 with tensorrt, because there are really few docs about this. (@samikama )\r\n****************************************\r\nI tried to compile the Tensorflow-serving r1.7 with TensorRT 4.0.4, and the compilation is successfully done.\r\n```\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-self-assign'\r\nINFO: Elapsed time: 1452.421s, Critical Path: 479.68s\r\nINFO: Build completed successfully, 11375 total actions\r\n```\r\n\r\nBut when I start the service and load a TFTRT optimized model, I get error:\r\n```\r\n2018-06-07 17:41:40.910874: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /media/disk1/fordata/web_server/project/LdaBasedClassification_623_1.7/data/cate155_tftrt_frozen/1\r\n2018-06-07 17:41:41.030117: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-06-07 17:41:41.283451: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.74GiB\r\n2018-06-07 17:41:41.283514: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-06-07 17:41:41.601178: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-07 17:41:41.601253: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-06-07 17:41:41.601273: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-06-07 17:41:41.601561: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10970 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n2018-06-07 17:41:41.878689: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 967809 microseconds.\r\n2018-06-07 17:41:41.878771: E tensorflow_serving/util/retrier.cc:38] Loading servable: {name: inception_v3 version: 1} failed: Not found: Op type not registered 'TRTEngineOp' in binary running on bjpg-g180.yz02. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\n```\r\n\r\nLooks like the TRTEngineOp is still not supported by this execution file. \r\nThough I'm not 100% sure about my way of compiling Tensorflow-serving 1.7 with TRT, but I think the compilation indeed searched and found the libnvinfer.so, etc, and also checked the TensorRT version is correct. So I don't know why the binary executive file still can't support TRTEngineOp. \r\n\r\nHere is my environment variables:\r\n```\r\nexport TENSORRT_INSTALL_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\r\nexport TF_TENSORRT_VERSION=4.0.4\r\nexport TENSORRT_LIB_PATH=\"/home/karafuto/TensorRT-3.0.4/lib\"\r\n```\r\nThis is my compilation command:\r\n```\r\nsed -i.bak 's/@org_tensorflow\\/\\/third_party\\/gpus\\/crosstool/@local_config_cuda\\/\\/crosstool:toolchain/g' tools/bazel.rc      \r\nbazel build  --config=cuda --action_env PYTHON_BIN_PATH=\"/home/karafuto/dlpy72/dlpy/bin/python2.7\" TENSORRT_BIN_PATH=\"/home/karafuto/TensorRT-3.0.4\"  -c opt tensorflow_serving/...\r\n\r\n```\r\nI'm not sure whether my procedure is correct. Really few of docs can be found that talk about how to build the tensorflow-serving 1.7 with tensorrt. Any idea will be welcome? \r\n\r\nThanks,\r\n****************************************\r\nPS:  The tensorrt source code is downloaded from NVIDIA official website, which tar file is named \"TensorRT-3.0.4.Ubuntu-14.04.5.x86_64.cuda-9.0.cudnn7.0.tar.gz\". The weird thing is, I unpacked the tar file and find the actually version is 4.0.4 not 3.0.4. So in the tensorflow-serving-r1.7, I need to set the variable TF_TENSORRT_VERSION=4.0.4 to avoid version check failure.\r\n\r\nI encountered below 2 errors and solved them, so I think the bazel compilation shall indeed compiled the TensorRT. Post here as an evidence.\r\n****************************************\r\nThis is the error when I set wrong TENSORRT_LIB_PATH, (can't find libnvinfer.so):\r\n```\r\nERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 160\r\n\t\tauto_configure_fail(\"TensorRT library (libnvinfer) v...\")\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: TensorRT library (libnvinfer) version is not set.\r\n```\r\n*******************************************\r\nThis is when the TF_TENSORRT_VERSION is not the same with found libnvinfer:\r\n```\r\nERROR: error loading package 'tensorflow_serving/apis': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 167\r\n\t\t_trt_lib_version(repository_ctx, trt_install_path)\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/tensorrt/tensorrt_configure.bzl\", line 87, in _trt_lib_version\r\n\t\tauto_configure_fail((\"TensorRT library version detec...)))\r\n\tFile \"/home/web_server/.cache/bazel/_bazel_web_server/7039d45003118564d66f2b06f1b7ea68/external/org_tensorflow/third_party/gpus/cuda_configure.bzl\", line 210, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: TensorRT library version detected from /media/disk1/fordata/web_server/project/xiaolun/TensorRT-3.0.4/include/NvInfer.h (4.0.4) does not match TF_TENSORRT_VERSION (3.0.4). To fix this rerun configure again.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nGPU model and memory\nExact command to reproduce", "@oscarriddle: Could you advise steps of tf-serving installation on CentOS, as it seems that you have already done that. My current trials were exactly following their tutorial, but failed every single trails. Perhaps you may kindly response to the following:\r\n1. Does tensorflow and serving version matters for CentOS? How about bazel version? I have tried tf-1.8, tfs-1.8 and bazel-0.10 and above.\r\n2. When to install tensorflow (either CPU or GPU based)? Before, after the serving or simultaneously? \r\n3. Anything special for CentOS?\r\nThank you. ", "@lkluo \r\n1. I don't think tensorflow and serving needs special attention for CentOS. I'm using bazel 0.11.1, tf-serving 1.7\r\n2. For tensorflow-serving 1.7, the tensorflow is simultaneously compiled. \r\n3. refer to 1.\r\n\r\nFor the compilation procedure, I'm all relying on my own effort, as you said, there are really few docs about it. So I'm not very confident about my procedure, and waiting the clue for leaving some comments to my question. \r\nI recommend you check the environment variables setting, you need to set the CUDA, bazel to PATH, and dependencies versions, etc. For python you need to directly pass its path to bazel, please refer my compilation command in above.", "@samikama Can you provide any advice?", "Hi,\r\n\r\nI don't have any idea about tensorflow-serving. Is it part of the tensorflow package?", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "i have the same error"]}, {"number": 19900, "title": "\"Address already in use\" occurs in Distributed Estimator.train_and_evaluator when loop training", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.18.0\r\n- **Python version**:  2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nIn Distributed ResNet, it need train the model many times, but when train in the second iteration, it fail for below:\r\n```\r\n{\"created\":\"@1528701271.960402221\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external/grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc\",\"file_line\":307,\"referenced_errors\":[{\"created\":\"@1528701271.960399817\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_posix.cc\",\"file_line\":345,\"referenced_errors\":[{\"created\":\"@1528701271.960371584\",\"description\":\"OS Error\",\"errno\":97,\"file\":\"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":259,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::]:50992\"},{\"created\":\"@1528701271.960399232\",\"description\":\"Unable to configure socket\",\"fd\":74,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":203,\"referenced_errors\":[{\"created\":\"@1528701271.960393654\",\"description\":\"OS Error\",\"errno\":98,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc\",\"file_line\":176,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\n```\r\n\r\nIt means that, the worker port 50992 not release when a new iteration start. The train iteration code show as fllow:\r\n\r\n\r\n--------------------code begin----------------------------------------\r\n```\r\n\r\ntrain_spec=tf.estimator.TrainSpec(input_fn=input_fn_train,max_steps=1000)\r\n\r\neval_spec=tf.estimator.EvalSpec(input_fn=input_fn_eval,steps=1,throttle_secs=1,start_delay_secs=1)\r\n\r\nfor cycle_index in range(total_training_cycle):\r\n\r\n    print(\"start a train\")\r\n\r\n    tf.logging.info('Starting a training cycle: %d/%d',cycle_index, total_training_cycle)\r\n\r\n    tf.logging.info('Starting to evaluate.')\r\n\r\n    tf.estimator.train_and_evaluate(classifier,train_spec,eval_spec)\r\n\r\n    print(\"end a train\")\r\n\r\n```\r\n--------------------code end----------------------------------------\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19899, "title": "Documentation feature request: Explain Nesterov Accelerated Gradient implementation", "body": "I haven't filled out the provided form because this is a feature request for the documentation.\r\n\r\nThe documentation for [`tf.train.MomentumOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer) offers a `use_nesterov` parameter on which the documentation says the following:\r\n\r\n> `use_nesterov`: If True use Nesterov Momentum. See [Sutskever et al.,\r\n> 2013](http://proceedings.mlr.press/v28/sutskever13.pdf). This\r\n> implementation always computes gradients at the value of the\r\n> variable(s) passed to the optimizer. Using Nesterov Momentum makes the\r\n> variable(s) track the values called `theta_t + mu*v_t` in the paper.\r\n\r\nThe problem is that the linked paper outlines the normal NAG algorithm, which requires computation of a gradient at a different value to those provided (namely at the next step). This is not clarified in the documentation and led me to some confusion. I ended up [asking this question on StackOverflow](https://stackoverflow.com/questions/50774683/how-is-nesterovs-accelerated-gradient-descent-implemented-in-tensorflow) and cobbled together [an answer myself](https://stackoverflow.com/a/50774886/1613983), however I think the answer by `user1735003` is so complete that without too much wrangling it could greatly enhance the documentation:\r\n\r\n[Answer by `user1735003`](https://stackoverflow.com/a/50778921/1613983)\r\n\r\nSome clarification to the fact that tensorflow actually implements a modified version of the algorithm which is only correct under certain conditions would have been very helpful and would have saved me some time.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N/A\nOS Platform and Distribution: N/A\nTensorFlow installed from: N/A\nTensorFlow version: 1.5\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nOn Tue, Jun 12, 2018, 04:53 Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Thank you for your post. We noticed you have not filled out the following\n> field in the issue template. Could you update them if they are relevant in\n> your case, or leave them as N/A? Thanks.\n> Have I written custom code\n> OS Platform and Distribution\n> TensorFlow installed from\n> TensorFlow version\n> Bazel version\n> CUDA/cuDNN version\n> GPU model and memory\n> Exact command to reproduce\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19899#issuecomment-396346258>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHpQ2YEF7Kkf5SJsLTCynUt83gGgnPanks5t7ryegaJpZM4UiIX7>\n> .\n>\n", "Great, sounds good. Marking as contributions welcome as we'd welcome a pull request to update the documentation to make it more helpful."]}, {"number": 19898, "title": "Win10 c++ Debug, LNK1189\uff1aLib objects exceeded, pywrap_tensorflow_internal", "body": "**System information:**\r\n\u00b7 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n\r\n\u00b7 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10-x64\r\n\r\n\u00b7 TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n\u00b7 TensorFlow version (use command below): r1.4, r1.5, r1.6 r1.7 and r1.8, command :\r\nv1.8's command : git checkout -b v1.8 -f origin/r1.8\r\nv1.7's command : git checkout -b v1.7 -f origin/r1.7\r\nv1.6's command : git checkout -b v1.6 -f origin/r1.6\r\nand so on include v1.4 and v1.5\r\n\r\n\u00b7 Python version: Anaconda3 - python3.6\r\n\r\n\u00b7 Bazel version (if compiling from source): I used CMAKE 3.11.1\r\n\r\n\u00b7 GCC/Compiler version (if compiling from source): both Visual Studio 2015 and Visual Studio 2015' MSBuild\r\n\r\n\u00b7 CUDA/cuDNN version: CUDA9.0, cudnn-9.0-win10-7.1\r\n\r\n\u00b7 GPU model and memory: GTX-860m with 2Gb Memory\r\n\r\nExact command to reproduce:\r\n\"\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> cmake .. -A x64 -DCMAKE_BUILD_TYPE=**Debug** -DSWIG_EXECUTABLE=D:/soft/TensorflowSoft/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/ProgramData/Anaconda3/python.exe -DPYTHON_LIBRARIES=D:/ProgramData/Anaconda3/libs/python36.lib -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"D:\\soft\\TensorflowSoft\\cudnn\" -G \"Visual Studio 14 2015\"\r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> set PreferredToolArchitecture=x64\r\n\r\nD:\\ProgramData\\tensorflow\\tensorflow\\contrib\\cmake\\build> MSBuild /p:Configuration=Debug  ALL_BUILD.vcxproj\r\n\"\r\n\r\n**Describe the problem :**\r\nI build TensorFlow-CPU-r1.8-**Debug** successfully (win10-x64 + Anaconda3-python3.6 + VS2017(not VS2015) + tensorflow1.8 + cmake3.11.1 + SwigWin3.0.12).\r\n\r\nBut when I build tensorflow-GPU-**Debug** version(both tensorflow-r1.7 and r1.8), the only error has occurred: \"error LNK1189\uff1alibrary limit of 65535 objects exceeded, pywrap_tensorflow_internal\".\r\n(If I use VS2017, it shows error: \" the compiler is not supported for CUDA 9.0\" , so I switch Visual Studio version to VS2015, and CMAKE command is work successfully.)\r\n\r\nAfter that, I switched to lower versions TensorFlow-r1.6 to TensorFlow-r1.5, but Link error occurred\uff1a\r\n\r\n\u201clibprotobufd.lib(text_format.obj) : error LNK2019: unresolved external symbol __std_reverse_trivially_swappable_8 referenced in function \"void _\r\n_cdecl std::_Reverse_unchecked1<class google::protobuf::Message const * *>(class google::protobuf::Message const * * const,class google::protobuf:\r\n:Message const * * const,struct std::integral_constant<unsigned __int64,8>)\" (??$_Reverse_unchecked1@PEAPEBVMessage@protobuf@google@@@std@@yaxqeap\r\nEBVMessage@protobuf@google@@0u?$integral_constant@_K$07@0@@z) [D:\\tf\\tensorflowGPU\\tensorflow\\contrib\\cmake\\build\\proto_text.vcxproj]\r\nlibprotobufd.lib(wire_format.obj) : error LNK2001: unresolved external symbol __std_reverse_trivially_swappable_8 [D:\\tf\\tensorflowGPU\\tensorflo\r\nw\\contrib\\cmake\\build\\proto_text.vcxproj]\r\nD:\\tf\\tensorflowGPU\\tensorflow\\contrib\\cmake\\build\\Debug\\proto_text.exe : fatal error LNK1120: 1 unresolved externals [D:\\tf\\tensorflowGPU\\tenso\r\nrflow\\contrib\\cmake\\build\\proto_text.vcxproj]\u201d\r\n", "comments": ["How about RelWithDebInfo ?\r\nThat one may succeed to build.", "\u4fee\u6539 tensorflow\\contrib\\cmake\\tools\\create_def_file.py \u4e2d\u7684 EXCLUDE_RE \u53d8\u91cf\u3002\r\n\r\n> Modify the EXCLUDE_RE variable in tensorflow\\contrib\\cmake\\tools\\create_def_file.py\r\n\r\n```python\r\nEXCLUDE_RE = re.compile(r\"RTTI|deleting destructor|::internal::|::`anonymous namespace'::|<lambda_[0-9a-z]+>|\"\r\n                        r\"std::_Vector_iterator<|std::_Vector_const_iterator<|std::_Vector_alloc<|\"\r\n                        r\"std::_Deque_iterator<|std::_Deque_alloc<|\"\r\n                        r\"std::_Tree_iterator<|std::_Tree_const_iterator<|std::_Tree_unchecked_const_iterator<|std::_Tree_comp_alloc<|std::_Tree_node<|\"\r\n                        r\"std::_List_iterator<|std::_List_const_iterator<|std::_List_unchecked_const_iterator<|std::_List_alloc<|\"\r\n                        r\"std::_Iterator012<|std::_Compressed_pair<\"\r\n)\r\n```\r\n\r\n\u5728 v1.8 \u6d4b\u8bd5\u53ef\u7528\u3002\r\n\r\n> In v1.8 test available.\r\n"]}, {"number": 19897, "title": "Feature Request: Propagate Gradients through map_fn", "body": "I would like to request that `tf.map_fn` be capable of propagating gradients. @drpngx [suggested](https://github.com/tensorflow/tensorflow/issues/3972#issuecomment-396067543) opening a new issue and cc'ing @skye . My [specific use case](https://github.com/tensorflow/tensorflow/issues/19896) is applying the same convolutional layers to each step in a batched sequence of images i.e. along axis 1 for a tensor of shape `[batch_size, max_sequence_length, image_height, image_width, number_of_channels]`. However, I'm sure others would have use for this as well.\r\n\r\nSystem information\r\nOS Platform and Distribution: Ubuntu 16.04.4 LTS\r\nTensorFlow installed from binary.\r\nTensorFlow version: 'v1.8.0-2159-gea9fb80'\r\nPython version: 2.7.12\r\n\r\nEdit:\r\nHave I written custom code: No.\r\nBazel version: 0.13.1\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: NA", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Done.", "Hi @RylanSchaeffer, can you provide a small script to repro the problem? This will make it much easier to get started on this (at least to determine how much work it'll be to fix).", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19896, "title": "How to Feed Batched Sequences of Images through Tensorflow conv2d", "body": "This seems like a trivial question, but I've been unable to find the answer. Maybe I'm just bad at Googling, but I think the question is sufficiently common to justify an easily accessible answer.\r\n\r\nI have batched sequences of images of shape:\r\n\r\n`[batch_size, number_of_frames, frame_height, frame_width, number_of_channels]`\r\n\r\nand I would like to pass each frame through a few convolutional and pooling layers. However, `tf.layers.conv2d` accepts 4D inputs of shape:\r\n\r\n`[batch_size, frame_height, frame_width, number_of_channels]`\r\n\r\nMy first attempt was to use tf.map_fn over axis=1, but I discovered that [this function does not propagate gradients](https://github.com/tensorflow/tensorflow/issues/3972) - which isn't mentioned in the [documentation](https://www.tensorflow.org/api_docs/python/tf/map_fn) (but should be!).\r\n\r\nMy second attempt was to use `tf.unstack` over the first dimension and then use `tf.while_loop`. However, my batch_size and number_of_frames are dynamically determined (i.e. both are None), and `tf.unstack` raises `{ValueError} Cannot infer num from shape (?, ?, 30, 30, 3) if num is unspecified`. I tried specifying `num=tf.shape(observations)[1]`, but this raises `{TypeError} Expected int for argument 'num' not <tf.Tensor 'A2C/infer/strided_slice:0' shape=() dtype=int32>`.\r\n\r\nWhat is the proper way to feed batched sequences of images through `conv2d`?\r\n\r\n### System information\r\nOS Platform and Distribution: Ubuntu 16.04.4 LTS\r\nTensorFlow installed from binary.\r\nTensorFlow version: 'v1.8.0-2159-gea9fb80'\r\nPython version: 2.7.12\r\n", "comments": ["You could use `tf.contrib.recurrent.Recurrent` to instantiate a range for loop.", "This seems strange - I shouldn't need recurrency in my network to apply the same function to each batch of images for each step in the sequence. Is there no alternative way?", "My roommate just pointed out a trivial solution: reshape into \r\n\r\n`[batch_size * number_of_frames, frame_height, frame_width, number_of_channels]`\r\n\r\nand then reshape after.", "How about using conv3d whose kernel is like [1,\u2026\u2026 ]? "]}, {"number": 19895, "title": "Cmake build with GPU fails on Windows 10 with a MSB6006 error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 \r\n- **TensorFlow installed from (source or binary)**: Source, from Git tag v1.8.0\r\n- **TensorFlow version (use command below)**: 1.8 (Git tag v1.8.0)\r\n- **Python version**: 3.5.5 (Anaconda Python)\r\n- **Bazel version (if compiling from source)**: Cmake 3.11.3\r\n- **GCC/Compiler version (if compiling from source)**: Visual Studio 2015 Update 3 (14.0.25431.01)\r\n- **CUDA/cuDNN version**: CUDA 9.2, cuDNN 7.1\r\n- **GPU model and memory**: Nvidia GeForce 940M\r\n- **Exact command to reproduce**:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow\r\n....\r\ngit checkout v1.8.0\r\n....\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\vcvarsall.bat amd64\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n -DSWIG_EXECUTABLE=C:/Dev_Tools/TFBuild/tools/swigwin-3.0.12/swig.exe ^\r\n -DPYTHON_EXECUTABLE=C:/Users/UAMARTH/AppData/Local/Continuum/anaconda3/envs/tfbuild/python.exe ^\r\n -DPYTHON_LIBRARIES=C:/Users/UAMARTH/AppData/Local/Continuum/anaconda3/envs/tfbuild/libs/python35.lib ^\r\n -Dtensorflow_ENABLE_GPU=ON ^\r\n -Dtensorflow_CUDA_VERSION=9.2 ^\r\n -Dtensorflow_CUDNN_VERSION=7 ^\r\n -DCUDNN_HOME=\"C:\\Dev_Tools\\TFBuild\\tools\\cudnn-9.2-windows10-x64-v7.1\" ^\r\n -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n\r\n\r\nMSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n```\r\n\r\n### Describe the problem\r\nThe build fails with a \"error MSB6006: \"cmd.exe\" exited with code 1\". Full error message below.\r\nThe line 259 of referred to in the error message points to a \"tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\tools\\api\\generator\\api\\keras\\preprocessing\\text\\__init__.py\" file, which does not exist. The .......\\preprocessing\\text\\ directory is empty.\r\n\r\n### Source code / logs\r\n```\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (\r\n1) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj\" (default target) (259) ->\r\n(CustomBuild target) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" e\r\nxited with code 1. [C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj]\r\n\r\n    19591 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 04:30:49.38\r\n```\r\n", "comments": ["@gunan Can you take a look at this?", "@annarev or @case540 may be able to help more on this failure. Looks like api generation failed?", "I will look more into it but for now a random guess:\r\nDo you have tensorflow installed with anaconda on the same machine? We saw some issue before where building TensorFlow with cmake might fail if there is existing tensorflow installation. (Basically one step in api generation would \"import tensorflow\" which won't be the tensorflow we just built but instead the one installed with anaconda)", "@annarev I have several anaconda environments setup on the machine I attempted to build TensorFlow, and one of the environments does have TensorFlow installed from pip. The anaconda environment which I attempted the build however does not have TensorFlow installed. My environment details are below,\r\n```\r\nconda info --env\r\n# conda environments:\r\n#\r\nbase                  *  C:\\Users\\UAMARTH\\AppData\\Local\\Continuum\\anaconda3\r\naws-tools                C:\\Users\\UAMARTH\\AppData\\Local\\Continuum\\anaconda3\\envs\\aws-tools\r\ndeep-learning            C:\\Users\\UAMARTH\\AppData\\Local\\Continuum\\anaconda3\\envs\\deep-learning\r\ntfbuild                  C:\\Users\\UAMARTH\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfbuild\r\n```\r\nFrom the above, the 'deep-learning' environment has TensorFlow installed from pip (tensorflow-gpu 1.5.0), while the other environments does not have it. I attempted the cmake build on the 'tfbuild' environment.\r\n\r\nBut, let me remove the environment with TensorFlow installed and re-run the build to see whether it has an effect.\r\nShould I attempt the build with the current head commit from master branch, or should I go with a stable branch such as r1.9 or r1.8?\r\n", "I attempted the following:\r\nI removed all the anaconda environments which had TensorFlow installed from the machine. I cloned a fresh copy of TensorFLow, which had the following head commit at the time of cloning,\r\n\r\n```\r\ngit log\r\ncommit 359f53686c87ee76e80353c32a3d22cfb1cf0989 (HEAD -> master, origin/master, origin/HEAD)\r\nAuthor: Yong Tang <yong.tang.github@outlook.com>\r\nDate:   Thu Jun 21 22:09:56 2018 -0700\r\n```\r\n\r\nI re-ran the build using the following command (this time using the 'base' anaconda environment, which had Python 3.6),\r\n\r\n```\r\ncmake ..  -G \"Visual Studio 14 2015 Win64\" -T host=x64 ^\r\n -DCMAKE_BUILD_TYPE=Release ^\r\n -DSWIG_EXECUTABLE=C:/Dev_Tools/TFBuild/tools/swigwin-3.0.12/swig.exe ^\r\n -DPYTHON_EXECUTABLE=C:/Users/UAMARTH/AppData/Local/Continuum/anaconda3/python.exe ^\r\n -DPYTHON_LIBRARIES=C:/Users/UAMARTH/AppData/Local/Continuum/anaconda3/libs/python36.lib ^\r\n -Dtensorflow_ENABLE_GPU=ON ^\r\n -Dtensorflow_CUDA_VERSION=9.2 ^\r\n -Dtensorflow_CUDNN_VERSION=7 ^\r\n -DCUDNN_HOME=\"C:\\Dev_Tools\\TFBuild\\tools\\cudnn-9.2-windows10-x64-v7.1\" ^\r\n -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n```\r\n\r\nAnd the build failed (after 4 hours 40 minutes) with the following error,\r\n\r\n```\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (\r\n1) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj\" (default target) (2) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default target) (3\r\n) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal_static.vcxproj\" (default tar\r\nget) (4) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_c_python_api.vcxproj\" (default target) (136) ->\r\n(ClCompile target) ->\r\n  C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\c\\python_api.cc(19): fatal error C1083: Cannot open include file: 'tensorf\r\nlow/python/framework/cpp_shape_inference.pb.h': No such file or directory [C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\c\r\nontrib\\cmake\\build\\tf_c_python_api.vcxproj]\r\n\r\n    25406 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 04:40:36.11\r\n```\r\n\r\nI found the following issue: https://github.com/tensorflow/tensorflow/issues/18931 which points to the same error with \"cpp_shape_inference.pb.h\". I will attempt the fix mentioned in that issue next.", "The fix suggested in https://github.com/tensorflow/tensorflow/issues/18931 did not solve the problem (although, the build did run few minutes longer). The build now fails with the following error.\r\n\r\n```\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (\r\n1) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj\" (default target) (2) ->\r\n(CustomBuild target) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" e\r\nxited with code 1. [C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj]\r\n\r\n\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (\r\n1) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj\" (default target) (264) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" e\r\nxited with code 1. [C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj]\r\n\r\n    25792 Warning(s)\r\n    2 Error(s)\r\n\r\nTime Elapsed 04:47:42.89\r\n```", "Sorry for not replying earlier. To answer your earlier question, any version is ok to try for building.\r\nDo you see any other error in the log? The last one doesn't give much information. Can you try searching for \"error :\" or \": error\" (don't remember which one is the right one)?", "@annarev I didn't see any other errors in the output. But let me re-run the build with 'detailed' verbosity and see if it shows any errors. ", "@annarev I re-ran the build with detailed verbosity with the latest code from the master branch as of today (26/6/18 - 91780a3a338b09ef89f5f04ab8e683b9756b4bf0). The build still fails, but now it gives a different error.\r\n\r\n```\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (\r\n1) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj\" (default target) (2) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\" (default target) (3\r\n) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal_static.vcxproj\" (default tar\r\nget) (4) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_stream_executor.vcxproj\" (default target) (148) ->\r\n(ClCompile target) ->\r\n  C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow/core/platform/test.h(34): fatal error C1083: Cannot open include file: 'gt\r\nest/gtest.h': No such file or directory (compiling source file C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\stream_execut\r\nor\\lib\\statusor_test.cc) [C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_stream_executor.vcxproj]\r\n\r\n    25383 Warning(s)\r\n    1 Error(s)\r\n\r\nTime Elapsed 04:46:45.42\r\n```\r\n\r\nI did a search for \"error:\" in the build log as you suggested, which returned the following lines,\r\n\r\n```\r\nTesting ReaderTest/parseWithOneError: OK\r\nTesting ReaderTest/parseChineseWithOneError: OK\r\nTesting ReaderTest/parseWithDetailError: OK\r\n```\r\n\r\n\": error\" didn't have any matches.\r\n\r\nPlease find the complete build log here: [msbuild.log](https://www.dropbox.com/s/mu0axcrohet0zvp/msbuild.log?dl=0). Please note that the log file is about 100MB.", "For the latest issue, I just pushed a fix to master. Could you try pulling head and retrying?", "@gunan I re-attempted the build, by cloning a fresh copy (with the head commit at bfcfad55b7b3fa4a1093fa748d4241f9457b2a84). However, it still failed with the following error,\r\n\r\n```\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (\r\n1) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj\" (default target) (2) ->\r\n(CustomBuild target) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" e\r\nxited with code 1. [C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj]\r\n\r\n\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (\r\n1) ->\r\n\"C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj\" (default target) (264) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" e\r\nxited with code 1. [C:\\Dev_Tools\\TFBuild\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj]\r\n\r\n    25797 Warning(s)\r\n    2 Error(s)\r\n\r\nTime Elapsed 04:50:43.32\r\n```\r\n\r\nPlease find the full build log [here](https://www.dropbox.com/s/w3id4nwic14q5uv/msbuild-1-7-2018.log?dl=0).\r\n\r\nQuestion: does the build depend on any python modules already being installed? Can already installed python modules conflict with it? I have verified that tensorflow is not installed anywhere in my machine before building. But, I do have protobuf, and absl-py installed. Can it cause an issue?", "Hi, I have the exact same errors, here is my config if that helps : \r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.9 (commit 80fb8679ab14ba3d180e8eb22da11509a15b9219)\r\n- Python version: 3.5.4rc1\r\n- Bazel version (if compiling from source): Cmake 3.10.0rc4\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2015 Update 3 (14.0.25431.01)\r\n- CUDA/cuDNN version: CUDA 9.0, cuDNN 7.0\r\n- GPU model and memory: Nvidia GTX 1070\r\n- Exact command to reproduce:\r\ncmake .. -G \"Visual Studio 14 2015 Win64\" -DCMAKE_BUILD_TYPE=Release ^\r\n -DSWIG_EXECUTABLE=C:\\Users\\dambr\\wipsea\\logiciels\\swigwin-3.0.12\\swigwin-3.0.12\\swig.exe ^\r\n -DPYTHON_EXECUTABLE=C:\\Users\\dambr\\AppData\\Local\\Programs\\Python\\Python35\\python.exe ^\r\n -DPYTHON_LIBRARIES=C:\\Users\\dambr\\AppData\\Local\\Programs\\Python\\Python35\\libs\\python35.lib ^\r\n -Dtensorflow_ENABLE_GPU=ON ^\r\n -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\" ^\r\n -DCUDA_TOOLKIT_ROOT_DIR=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\"\r\n\r\nMSBuild /m:4 /p:Configuration=Release /p:PreferredToolArchitecture=x64 tensorflow.sln\r\n(run as admin)\r\n\r\n\r\nAnd the resulting errors : \r\n\r\n       \"C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tensorflow.sln\" (default target) (1) ->\r\n       \"C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj.metaproj\" (default target)\r\n       (257) ->\r\n       \"C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj\" (default target) (530) ->\r\n       (CustomBuild target) ->\r\n         C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd\r\n       .exe\" exited with code 1. [C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_api.vcxproj]\r\n\r\n\r\n       \"C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tensorflow.sln\" (default target) (1) ->\r\n       \"C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj.metaproj\" (default t\r\n       arget) (238) ->\r\n       \"C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj\" (default target) (5\r\n       28) ->\r\n         C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd\r\n       .exe\" exited with code 1. [C:\\Users\\dambr\\code\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vc\r\n       xproj]\r\n\r\n    102 Warning(s)\r\n    2 Error(s)\r\n\r\n", "For windows and cmake buiding, there is an issue when generating python api, in the scipt `create_python_api.py`. See my response to #20669 .", "Thank you @LoSealL , uninstalling `tensorflow-gpu` and `grpcio` (`pip uninstall`) according to your comment on #20669 solved the issue for me. I was able compile without errors and run `Release/tf_tutorials_example_trainer.exe` as advised for checking [there](https://joe-antognini.github.io/machine-learning/windows-tf-project).\r\n\r\nPS : I first tried to remove just `tensorflow-gpu`, but it failed. It lead to a new error MSB6006 raised by the _beam_search_ops project.", "Nagging Assignee @gunan: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this was fixed, but I forgot to update this issue.\r\nHowever, now this is obsolete because the recommended way to build TF on windows is now using bazel:\r\nhttps://www.tensorflow.org/install/install_sources_windows\r\n\r\nAs a bonus, building TF on windows with GPU support is 5x faster using bazel than cmake!"]}, {"number": 19894, "title": "Fix routing of quantized tensors", "body": "The original tensor was not replaced with the quantized one when it had already been quantized.\r\n\r\n### TODO\r\n\r\n- [x] Handle delayed quantization \r\n", "comments": ["Sorry for the delay.", "@suharshs could you take a look?", "Hi, sorry for the delay. Can you describe in more detail what this change tries to fix? I don't fully understand the graph that this fails on before this change", "When a kernel `Variable` is shared by two `Conv2D`s, after quantization there will be only one `Conv2D` getting the quantized kernel. \r\n\r\nFor example, suppose there is a kernel `K` shared by `Conv2D` `c1` and `Conv2D` `c2`:\r\n\r\n`K` > `c1`\r\n`K` > `c2`\r\n\r\nWhen quantizing `c1`, `Quantize` calls `_InsertQuantOp` to quantize the input kernel `K` and feeds the quantized `K` to `c1`:\r\n\r\n`K` > `quantized_K` > `c1`\r\n\r\nAfter that when quantizing `c2`,  `Quantize` calls `_InsertQuantOp` to quantize the input kernel `K` again but this time [`_InsertQuantOp` will return immediately](https://github.com/tensorflow/tensorflow/blob/2e7ae3406f0715de9e23d7657827048ed81e57cb/tensorflow/contrib/quantize/python/quantize.py#L543) since `K` has been quantized.\r\n\r\nThe quantized graph will be\r\n\r\n`K` > `quantized_K` > `c1`\r\n`K` > `c2`\r\n\r\nThis PR changes the quantized graph into\r\n\r\n`K` > `quantized_K` > `c1`\r\n`K` > `quantized_K` > `c2`\r\n", "Nagging Assignee @drpngx: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 34 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@drpngx : Can we get this change integrated? Who can  help with this?", "@raghuraman-k, @manipopopo  It appears the branch has merge conflicts that need to be resolved before this can be merged. Please resolve this.", "@suharshs, @raghuraman-k \r\nI've rebased it onto the master branch."]}, {"number": 19892, "title": "distributed estrimator hang forever.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux\r\n- **TensorFlow installed from (source or binary)**:  binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**:  2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI ran the mnist code with estimator, It's ok when running in single process, but hang when running in 3 processes for distributed mode. The code (PS) is shown below. Notice that, worker should change the task.type in TF_CONFIG to 'worker', and chief should change to 'chief'.\r\n\r\n\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\n\r\nimport pandas as pd\r\n\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nimport matplotlib.cm as cm\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef load2():\r\n    train = pd.read_csv('/tmp/train.csv')\r\n\r\n    test = pd.read_csv('/tmp/test.csv')\r\n\r\n    labels = train['label']\r\n\r\n    images = train.iloc[:, 1:]\r\n\r\n    image_size = 28\r\n\r\n    train_ds, valid_ds, train_labels, valid_labels = train_test_split(images, labels, test_size=0.33, random_state=42)\r\n\r\n    print len(train_ds)\r\n    print len(valid_ds)\r\n    print len(train_labels)\r\n    print len(valid_labels)\r\n\r\n    data = images\r\n\r\n    train_ds, train_labels = reformat(train_ds, train_labels)\r\n    valid_ds, valid_labels = reformat(valid_ds, valid_labels)\r\n    test_ds, test_labels = reformat(test, train_labels[:len(test)])\r\n\r\n    print('Training set', train_ds.shape, train_labels.shape)\r\n    print('Validation set', valid_ds.shape, valid_labels.shape)\r\n    print('Test set', test_ds.shape, test_labels.shape)\r\n\r\n\r\ndef reformat(dataset, labels):\r\n    num_channels = 1\r\n    num_labels = 10\r\n    dataset = dataset.values.reshape(\r\n        (-1, image_size, image_size, num_channels)).astype(np.float32)\r\n    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\r\n    return dataset, labels\r\n\r\n\r\ndef cnn_model_fn(features, labels, mode):\r\n    input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\r\n    conv1 = tf.layers.conv2d(\r\n        inputs=input_layer,\r\n        filters=32,\r\n        kernel_size=[5, 5],\r\n        padding=\"same\",\r\n        activation=tf.nn.relu\r\n    )\r\n\r\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)  # 14*14*32\r\n\r\n    conv2 = tf.layers.conv2d(\r\n        inputs=pool1,\r\n        filters=64,\r\n        kernel_size=[5, 5],\r\n        padding='same',\r\n        activation=tf.nn.relu\r\n    )\r\n\r\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n\r\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n    dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n    logits = tf.layers.dense(inputs=dropout, units=10)\r\n    print '####', logits\r\n\r\n    predictions = {\r\n        'classes': tf.argmax(logits, axis=1),\r\n        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n    }\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n        train_op = optimizer.minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step()\r\n        )\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\ndef main(argvs):\r\n    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n    train_data = mnist.train.images\r\n    print train_data.shape\r\n    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\n    eval_data = mnist.test.images\r\n    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\r\n    mnist_classifier = tf.estimator.Estimator(\r\n        model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")\r\n    # Set up logging for predictions\r\n    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\r\n    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\r\n    logging_hook = tf.train.LoggingTensorHook(\r\n        tensors=tensors_to_log, every_n_iter=50)\r\n    print 'labels: ', train_labels\r\n    # Train the model\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": train_data},\r\n        y=train_labels,\r\n        batch_size=100,\r\n        num_epochs=None,\r\n        shuffle=True)\r\n    mnist_classifier.train(\r\n        input_fn=train_input_fn,\r\n        steps=20000,\r\n        hooks=[logging_hook])\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": eval_data},\r\n        y=eval_labels,\r\n        num_epochs=1,\r\n        shuffle=False)\r\n\r\n\r\nif __name__ == '__main__':\r\n  import os,json\r\n  os.environ['TF_CONFIG']=json.dumps({\r\n      \"cluster\": {\r\n        \"ps\": [\r\n          \"127.0.0.1:34567\"\r\n        ],\r\n        \"chief\": [\r\n          \"127.0.0.1:34568\"\r\n        ],\r\n        \"worker\": [\r\n          \"127.0.0.1:34569\"\r\n        ]\r\n      },\r\n      \"task\": {\r\n        \"index\": 0,\r\n        \"type\": \"ps\" # optional: chief, ps, worker\r\n      }\r\n  })\r\n  tf.app.run()`\r\n\r\n\r\n\r\n\r\nWARNING:tensorflow:From ps_cnn_mnist.py:94: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/yiguang.wyg/tools/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n(55000, 784)\r\nINFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'127.0.0.1:34567'], u'chief': [u'127.0.0.1:34568'], u'worker': [u'127.0.0.1:34569']}, u'task': {u'index': 0, u'type': u'ps'}}\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff7169d49d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_master': u'grpc://127.0.0.1:34567', '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/mnist_convnet_model', '_global_id_in_cluster': 2, '_save_summary_steps': 100}\r\nlabels:  [7 3 4 ... 5 6 8]\r\nINFO:tensorflow:Calling model_fn.\r\n#### Tensor(\"dense_1/BiasAdd:0\", shape=(100, 10), dtype=float32, device=/job:ps/task:0)\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.", "comments": ["Same here.\r\n\r\n```\r\nINFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'master': ['tfjob1-master-twc1-0:2222'], 'ps': ['tfjob1-ps-twc1-0:2222'], 'worker': ['tfjob1-worker-twc1-0:2222', 'tfjob1-worker-twc1-1:2222']}, 'task': {'type': 'master', 'index': 0}, 'environment': 'cloud'}\r\nINFO:tensorflow:Using config: {'_model_dir': './out/mnist_custom_estimator', '_tf_random_seed': None, '_save_summary_steps': 20, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe07aaed8d0>, '_task_type': 'master', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://tfjob1-master-twc1-0:2222', '_evaluation_master': '', '_num_ps_replicas': 1, '_num_worker_replicas': 3, '_is_chief': True}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n```\r\n\r\nRunning the following script: https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_custom_estimator.py\r\n\r\nI'm running it in Distributed Tensorflow using Kubernetes on a shared filesystem (NFS). I can confirm that the PS and master can see the same files such as checkpoints.", "same problem", "Well, I have fixed it by using train_and_evaluate method instead of train.", "Fixd too. The point is `train` seems do not start grpc server. It may be a bug? I think it need confirmation.", "fixed with `train_and_evaluate`.", "Nagging Assignee @robieta: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "i confirm this problem, and i run the distributed trainning job on k8s, and most of workers hang at\r\n\r\n```\r\nINFO:tensorflow:Graph was finalized.\r\n```\r\n\r\ntf-1.8.0 python 2.7"]}, {"number": 19891, "title": "Fix code typo in eager programmers guide", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Ooops, sorry, I think I mistakenly approved this PR. This isn't a typo, it is intentional that we're re-using the variables in the `dense2` layer. With the changed code, the comment is incorrect (it is not re-using any variables)\r\n\r\nWill roll this back."]}, {"number": 19890, "title": "Logits all reduced to very small value when training multi-label image classification", "body": "```\r\nTensorflow version: 1.7-gpu\r\nOperating system: ubuntu 16.04\r\n```\r\n\r\nI was training a multi-label image classification model (an image could have multiple labels, and the label is like `[1,0,0,0,1,0,0,1]` with variable number of 1s).\r\n\r\nI have switched to the `sigmoid_cross_entropy_with_logits` like below:\r\n\r\n```python\r\nwith slim.arg_scope(inception_resnet_v2_arg_scope()):\r\n            logits, end_points = inception_resnet_v2(images, num_classes=dataset.num_classes, is_training=True)\r\n         \r\n        exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\r\n        variables_to_restore = slim.get_variables_to_restore(exclude=exclude)\r\n\r\n        fp_labels = tf.cast(labels, tf.float32)\r\n        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=fp_labels, logits=logits)\r\n        \r\n        total_loss = tf.losses.get_total_loss()  \r\n        \r\n        global_step = get_or_create_global_step()\r\n\r\n        # Define your exponentially decaying learning rate\r\n        lr = tf.train.exponential_decay(\r\n            learning_rate=initial_learning_rate,\r\n            global_step=global_step,\r\n            decay_steps=decay_steps,\r\n            decay_rate=learning_rate_decay_factor,\r\n            staircase=True)\r\n\r\n        # Now we can define the optimizer that takes on the learning rate\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\r\n\r\n        # Create the train_op.\r\n        train_op = slim.learning.create_train_op(total_loss, optimizer)\r\n        \r\n        def train_step(sess, train_op, global_step):\r\n   \r\n            # Check the time for each sess run\r\n            start_time = time.time()\r\n            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\r\n            time_elapsed = time.time() - start_time\r\n\r\n            # Run the logging to print some results\r\n            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, \r\n            time_elapsed)\r\n\r\n            return total_loss, global_step_count\r\n\r\n```\r\n\r\nBut when I started training the loss is decreasing very fast (from 0.5 to almost 0 in several minites), and all the logits value became almost zero for every image. \r\n\r\nI tried manual loss function implementation but also failed.\r\n\r\nAny advice would be greatly appreciated! ", "comments": ["I resolved it by replacing `tf.nn.sigmoid_cross_entropy_with_logits` by `tf.losses.sigmoid_cross_entropy`, though I don't know the reason... ", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 19889, "title": "why tf.image.crop_to_bounding_box no accept float [0,1] input", "body": "By default TF prefer [0,1] range box coordinates, however the `tf.image.crop_to_bounding_box` only accept integer box coordinates input, while `tf.image.crop_and_resize` accept [0,1] range box coordinates, why?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19888, "title": "tensorflow/tensorflow/core/framework directory  not found BUILD file .", "body": "why \uff0c tensorflow/tensorflow/core/framework directory  not found BUILD file . \uff1f ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "System information\r\nOS Platform : ubuntu12.4\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below):  current master\r\n\r\n"]}, {"number": 19887, "title": "Bug in nn.conv3d_transpose function?", "body": "**System information:**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS Sierra 10.12.5\r\nTensorFlow installed from (source or binary):Binary\r\nTensorFlow version (use command below):1.8\r\nPython version:3.6.2\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:None\r\nGPU model and memory:CPU\r\nExact command to reproduce:\r\n\r\n**Problem Description:**\r\n\r\nWhen I am using nn.conv3d_transpose function, I found there is probably a bug in the code? \r\nIn line 1461 of file nn.ops.py:\r\n\r\n<img width=\"742\" alt=\"2\" src=\"https://user-images.githubusercontent.com/19757981/41201350-32d1d18e-6cae-11e8-9ef9-505e071e0d28.png\">\r\nIt compares output_shape[4] with filter shape (which both should be the channel number). But if we use different data format such as NCDHW, the channel number should be represented by output_shape[1] right?  So I assume it should be output_shape[axis] here?\r\n\r\nI also compared the 3d function with 2d function: nn.conv2d_transpose and it uses output_shape[axis] at the same place:\r\n\r\n<img width=\"740\" alt=\"1\" src=\"https://user-images.githubusercontent.com/19757981/41201372-90735894-6cae-11e8-8a12-a8350d2f3579.png\">\r\n\r\n\r\nThanks.\r\n", "comments": ["Nagging Assignee @rmlarsen: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@kaedyn I checked the current master branch:\r\nhttps://github.com/tensorflow/tensorflow/blob/4e36393006f8462a3ef516a6ca3010542213f352/tensorflow/python/ops/nn_ops.py#L1462\r\n\r\nI think the issue has been fixed. I will close this issue but feel free to reopen if this is still a problem."]}]