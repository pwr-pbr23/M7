[{"number": 456, "title": "Batch Multinomial Function for TensorFlow", "body": "Hey TF,\n\nIs there some sort of multinomial function that can handle batches? When generating content, it is incredibly useful to introduce variance by 'rolling a dice'. I searched extensively in the docs and could not find one.\n\n[Numpy has one](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.random.multinomial.html), but it can not do batches. Tensorflow could capitalize on the fact that it uses a GPU. Thanks alot! \n", "comments": ["We don't have this at the moment, but it would be good to add.  Since the right way to do multinomial sampling is [ziggurat](https://en.wikipedia.org/wiki/Ziggurat_algorithm) which uses rejection sampling, the infrastructure to do threaded and GPU-ified random distributions needs to be tweaked to allow multiple numbers of samples.\n", "For now, depending on how multi you are, quantizing `normal_normal` may be a suffiicent workaround.\n", "@girving I apologize, but when you say `normal_normal` what exactly are you referring to? I couldn't find a function named that in tensorflow. \n\nTo clarify, I want to take a tensor [batch_size x logits_probabilties] and perform a multinomial function on each one to see which logit is selected. This would result in [batch_size x one_logit_selected].\n\nThanks again! \n", ":)  Sorry, I meant `random_normal`.  And my `random_normal` suggestion is not what you want in any case, since you're talking about a categorical distribution, not a multinormial distribution.  I'm in the middle of something, but I may (or may not) have a more useful comment in a couple hours.\n", "That would be great! Using np.multinomial has been pretty challenging due to the fact that the batch size isn't evaluated until you run the session. \n\nIf I understand things correctly, the only way to use np.multinomial is to `tensor.eval()` (to convert the tensor to a numpy array). Unless there is some other way I'm not aware of that converts tensor to numpy arrays...\n", "From an internal discussion, a workaround was suggested using Gumbel argmax trick as follows. We are still discussing whether we should include it in the python op library, or having a more efficient native implementation. \n\n---\n\nSuppose we want to sample a bunch (say, N) of K-ary categorical distributions. Suppose further that none of the probabilities are exactly zero (to simulate a zero probability for one state just use a really really really small one).\n\nSo we have an N by K matrix of of log probabilities X. So in other words we imagine our categorical distributions being expressed in softmax form as exp(X). So really we want to sample from exp(x_i) / sum_j exp(x_j).\n\nGenerate a matrix of uniforms on (0,1) of the same shape as X. Call this matrix U.\n\nnp.argmax(X - np.log(-np.log(U)), axis=1) gives us our samples. \n\nhttps://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions/\n", "@zheng-xq Thank you for the work-around. If this can be implemented in tensorflow `tf.multinomial` it would be much, much appreciated.\n\nThe main problem as I said above is that you have to run `.eval()` to input your tensor into a numpy function. Much of our code is not designed to be evaluated at that point. So having a solution that specifically handles tensorflow's `tensor` would be amazing! Thank you for the help regardless. \n", "To clarify, although the quoted workaround uses numpy function as an example. They could be implemented in TensorFlow using their TF counterpart: tf.argmax and tf.log, without invoking eval(). \n\nAnd yes, we will likely to make something similar to tf.multinomial at some point. We are still discussing how we should do it. \n", "> tf.argmax and tf.log, without invoking eval().\n\nOh duh...I apologize as I should've thought of that. Big Thanks. \n", "In case anyone is looking at this, here is the sampling function I use. Thanks @zheng-xq  again for all your help.\n\n``` python\ndef batch_sample_with_temperature(a, temperature=1.0):\n    '''this function is like sample_with_temperature except it can handle batch input a of [batch_size x logits] \n        this function takes logits input, and produces a specific number from the array. This is all done on the gpu\n        because this function uses tensorflow\n        As you increase the temperature, you will get more diversified output but with more errors (usually gramatical if you're \n            doing text)\n    args: \n        Logits -- this must be a 2d array [batch_size x logits]\n        Temperature -- how much variance you want in output\n    returns:\n        Selected number from distribution\n    '''\n\n    '''\n    Equation can be found here: https://en.wikipedia.org/wiki/Softmax_function (under reinforcement learning)\n        Karpathy did it here as well: https://github.com/karpathy/char-rnn/blob/4297a9bf69726823d944ad971555e91204f12ca8/sample.lua'''\n    '''a is [batch_size x logits]'''\n    with tf.op_scope(a+temperature, \"batch_sample_with_temperature\"):\n\n        exponent_raised = tf.exp(tf.div(a, temperature)) #start by reduction of temperature, and get rid of negative numbers with exponent\n\n        matrix_X = tf.div(exponent_raised, tf.reduce_sum(exponent_raised, reduction_indices = 1)) #this will yield probabilities!\n\n        matrix_U = tf.random_uniform([batch_size, tf.shape(a)[1]], minval = 0, maxval = 1)\n\n        final_number = tf.argmax(tf.sub(matrix_X - matrix_U), dimension = 1) #you want dimension = 1 because you are argmaxing across rows.\n\n    return final_number\n```\n", "Closing the loop on this thread: we now have a `tf.multinomial()` Op.\n", "Thanks for sharing the useful information.\r\nI'm trying to implement the same thing in chainer because it doesn't provide functions to sample a batch of multinomial distributions.\r\n\r\nHowever, in my experiments, the code provided in https://github.com/tensorflow/tensorflow/issues/456#issuecomment-165185025 is wrong, which doesn't give the correct samples. The main problem is that uniform noise is **directly substracted** from the softmax probabilities then take the argmax, differing to a *usual way* that finding index for a uniform noise from ranges defined by the multinomial CDF. By the way, the method provided by https://github.com/tensorflow/tensorflow/issues/456#issuecomment-164038802 is exactly what we want.\r\n\r\nSince testing random number generators is not easy, I use the simplest way to sample the same distribution for a million times and count their occurrence. Here's the testing snippet.\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef multinomial_issue_impl(logits):\r\n    exponent = np.exp(logits)\r\n    matrix_X = exponent / np.sum(exponent)\r\n    matrix_U = np.random.rand(*matrix_X.shape)\r\n    final_num = np.argmax(matrix_X - matrix_U)\r\n    return final_num\r\n\r\n\r\ndef multinomial_gumbel_impl(logits):\r\n    exponent = np.exp(logits)\r\n    log_prob = np.log(exponent / np.sum(exponent))\r\n    U = np.random.rand(*log_prob.shape)\r\n    num = np.argmax(log_prob - np.log(-np.log(U)))\r\n    return num\r\n\r\n\r\ndef test(func):\r\n    logits = [1., 1., 1., 1., 1., 5.]  # 0 ~ 5\r\n    K = 1000000\r\n\r\n    count = [0] * 6\r\n    for i in xrange(K):\r\n        num = func(logits)\r\n        count[num] += 1\r\n\r\n    print count\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    print \"test:\"\r\n    test(multinomial_issue_impl)\r\n    print \"gumbel:\"\r\n    test(multinomial_gumbel_impl)\r\n```\r\n\r\nwhich yields\r\n\r\n```text\r\ntest:\r\n[4525, 4415, 4449, 4395, 4451, 977765]\r\ngumbel:\r\n[16674, 16570, 16929, 16902, 16878, 916047]\r\n```\r\n\r\nGiven the logits `[1., 1., 1., 1., 1., 5.]`, the probabilities should be `[0.01677904,  0.01677904,  0.01677904,  0.01677904,  0.01677904, 0.91610478]`. Thus the result tells that gumbel trick is correct.\r\n"]}, {"number": 455, "title": "Problem with L2_Loss on GPU", "body": "The following code works correctly when running on CPU but produces an error when run on GPU. This has been experienced with both a workstation with an Nvidia GTX 980 and a Nvidia Titan X. The error message indicates theres a problem on the CUDA Backend.\n\n---\n\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\n\ninput = tf.placeholder(\"float32\", shape=[None, 2])\ntarget = tf.placeholder(\"float32\", shape=[None, 2])\n\nw=tf.Variable(tf.zeros([2, 2]))\nb=tf.Variable(tf.zeros([1]))\nl=tf.nn.relu(tf.matmul(input, w) + b)\n\ncost = tf.reduce_sum(tf.nn.l2_loss(l - target)) # this breaks\n# cost = tf.reduce_sum( tf.pow(l - target, 2)/2 )\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n\nsess.run(tf.initialize_all_variables())\ndict={input: [[1,1],[1,1]], target: [[1,1],[1,1]]}\ntrain_step.run(feed_dict=dict) # <---- this throws \"F tensorflow/core/kernels/tile_ops.cc:116] TileOp: Invalid combination of Device, DT and NDIM: N5Eigen9GpuDeviceE, float, 0\"\n", "comments": ["@kvamaraju, could you sync to the latest TensorFlow and retry? I think this problem should have been fixed by a recent CL. \n"]}, {"number": 454, "title": "Help Needed To Understand Tensor Flow Weights and Gradient Histogram", "body": "Hi,\n\nI am experimenting with conv nets for sentiment analysis.\nI used a basic convent architecture, my network looks like:\n![screen shot 2015-12-09 at 10 58 08 am](https://cloud.githubusercontent.com/assets/15708499/11695403/cb3e4644-9e63-11e5-8bc2-c3d71baf1b2f.png)\nI am trying to analyze the learning, I have a bad feeling about weights, Although I am early in the learning process, but the weights for 5x5 filter essentially suggest no updates/ update on only very few component , because histogram essentially remains the same.\n\nAny hints on what I could be missing?\n\n[TensorBoard_basic_convnet.pdf](https://github.com/tensorflow/tensorflow/files/57216/TensorBoard_basic_convnet.pdf)\n\nEveryone has said that, none the less, I would say it too, Thanks for the amazing library.\n\nRegards,\nVaibhav Singh\n", "comments": ["This is a better question for stackoverflow, since it does not seem to be about a bug in tensorflow.  As a first guess, you may need to either wait longer or increase your learning rate, but it is hard to tell from so little information.\n", "Thanks Girving, I meant for it to be a more of how to question than a bug, Looking for if I interpret the histogram correctly or missing some basics. I will check on stackoverflow as well.\n", "To clarify: it's a good question, but we're trying to keep questions going to stackoverflow since it makes it easier for future people who hit the same problem to find the solution.\n"]}, {"number": 453, "title": "Change needed in index", "body": "The url to mnist.py here is https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/examples/tutorials/mnist/mnist.py which works. \n\nHowever, at https://www.tensorflow.org/versions/master/tutorials/mnist/tf/index.html which I believe is the compiled version of this file, the url reads \nhttps://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/mnist.py which leads nowhere. I'm not sure exactly where this mistake is, but it should definitely be fixed because otherwise people can't follow along with the tensorflow mechanics tutorial.\n", "comments": ["@zfrenchee, can you send this request via gerrit?  See https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md and http://blog.mdda.net/ai/2015/11/10/contributing-to-tensorflow/\n", "I'm not quite sure how to go about fixing this, because I'm not quite sure about the mapping between the g3doc and the public tensorflow webpage. I expected the link to be wrong all the way down, but it's right here, but wrong on the website. Is the website not part of this repo, or am I missing something? \n", "Website is pushed based on the repo, but it's not automatically pushed (yet), so there's some manual delay involved.  We're probably going to push the newest website soon, and if the URL is still wrong, feel free to let us know and we'll fix the website on our end\n"]}, {"number": 452, "title": "bazel build error for PolymerElements", "body": "I am trying to build tensorflow from source, and bazel is giving some unrelated error \n\n```\neddie7@albus:~/lab/tensorflow$ git pull\nAlready up-to-date.\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n.......\nERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@iron-validatable-behavior//': https://github.com/PolymerElements/iron-validatable-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 129.792s\n\n```\n\n_FYI: I have also build bazel from source_\n", "comments": ["@danmane: Do you know what might be going wrong?\n", "I have the same issue after adding Android NDK to the `WORKSPACE`.\nIt happened to me after syncing my tensorflow directory with cd53f3c3302c9312c1840389a9988a879b8b9dd5 change number 109591880 that apparently load `pthread` shared library after building it through the top level build flow. (related to issue #419)\n", "@hamidb: that sounds like a red herring.\n\nCan you try \nbazel clean\n\nfollowed by\n\nbazel build -j 1 --spawn_strategy=standalone \n\n(These are the instructions we've been suggested for those having trouble with bazel builds)...\n", "@vrv \nI tried to rebuild tensorflow after, executing your instruction, still no luck\n\n```\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -j 1 --spawn_strategy=standalone\n.......\nINFO: Found 0 targets...\nINFO: Elapsed time: 2.096s, Critical Path: 0.01s\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel clean\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\neddie7@albus:~/lab/tensorflow$ git pull\nremote: Counting objects: 34, done.\nremote: Total 34 (delta 28), reused 28 (delta 28), pack-reused 5\nUnpacking objects: 100% (34/34), done.\nFrom https://github.com/tensorflow/tensorflow\n   f88fb6f..8242b4d  master     -> origin/master\n * [new tag]         0.6.0      -> 0.6.0\nUpdating f88fb6f..8242b4d\nFast-forward\n tensorflow/core/public/version.h                                | 2 +-\n tensorflow/models/image/imagenet/BUILD                          | 1 +\n tensorflow/python/BUILD                                         | 1 +\n tensorflow/python/kernel_tests/parsing_ops_test.py              | 4 ++--\n tensorflow/python/kernel_tests/sparse_serialization_ops_test.py | 2 +-\n tensorflow/python/ops/gradients.py                              | 2 +-\n tensorflow/python/ops/image_ops_test.py                         | 4 ++--\n tensorflow/tools/pip_package/setup.py                           | 2 +-\n 8 files changed, 10 insertions(+), 8 deletions(-)\neddie7@albus:~/lab/tensorflow$ ./configure \nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] n\nNo GPU support will be enabled for TensorFlow\nConfiguration finished\neddie7@albus:~/lab/tensorflow$ ~/lab/bazel/bazel-bin/src/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@mocha//': https://github.com/mochajs/mocha.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 129.614s\n```\n\nit seems issue with bazel rather than tensorflow, as far as I think.\nI have never used bazel before too, so can't tell anything\n", "Sorry, I meant:\n$ ~/lab/bazel/bazel-bin/src/bazel build -j 1 --spawn_strategy=standalone -c opt //tensorflow/tools/pip_package:build_pip_package\n\nThat is indeed weird that it can't open the mocha.git file...\n", "@vrv I have no problem with mocha.git. It was @jayendra13's error.\nI was able to fix my issue. I was trying to build the android sample. I noticed that to be able to build the android sample, you need to first build other dependencies such as protobuf, etc. \nI would ideally expect calling\n `bazel build //tensorflow/examples/android:tensorflow_demo` ,\nshould automatically build dependencies if already not built. Am I right?\nSo if you have a clean workspace, build bazel android_demo does not work.\n", "Assigning to andrew to take a look.\n", "I am unable to reproduce this issue using a fresh bazel install (from source) and clone of the TF repo. tensorflow/examples/android:tensorflow_demo still builds cleanly for me.\n\nNote that unless you clone the TF repo with --recursive, you will not grab the protocol module into your local tree. Could this be related to your issue?\n", "I had protobuf already cloned.\nI will try to reproduce it again and provide more details. \n", "@andrewharp \nI can reproduce it when using `bazel 0.1.1`. It compiles with `0.1.2`\nHere are the steps to reproduce:\n\n1- bazel clean --expunge\n\n2- cd google/protpbuf/ & make clean\n\n3- rm -rf ~/.cache/bazel/_bazel_user\n\n4- compile bazel\n\n5- ./configure tensorflow without gpu\n\n6- bazel build --verbose_failures  //tensorflow/examples/android:tensorflow_demo\n\nI get this:\n\n```\nERROR: /home/hamid/.cache/bazel/_bazel_hamid/f7e3d3cb1f79b9a7f53120cc4d577bcd/external/default_android_tools/src/main/protobuf/BUILD:19:2: every rule of type gensrcjar implicitly depends upon the target '//third_party:protoc',  but this target could not be found. If this is an integration test, maybe you forgot to add a mock for your new tool?.\nERROR: Loading failed; build aborted.\n```\n\nIt will be OK if I first compile protobuf and then build the demo. But it would be nicer for the demo build flow to automatically resolve it.\n", "To confirm, you are unable to reproduce the issue when building with bazel 0.1.2?\n\nI am unable to follow your reproduction steps for \"cd google/protobuf & make clean\":\n\n~/tf_git/tensorflow$ cd google/protobuf/ & make clean\n[1] 939\nmake: **\\* No rule to make target `clean'.  Stop.\n\nGiven that \"//third_party:protoc\" appears nowhere in the Tensorflow or google/protobuf repos, I'm guessing this sounds like a bad config is being pulled in from somewhere external, likely Bazel.\n\nAt its root, when you chase down what tensorflow/core:protos_cc actually is (this nessecitates following several layers of indirection between tensorflow/core/BUILD, tensorflow/tensorflow.bzl, tensorflow/core/platform/default/build_config/BUILD, and tensorflow/core/platform/default/build_config.bzl), you find it's a cc_proto_library from /google/protobuf/protobuf.bzl. \n\nAre you able to build tensorflow/core:protos_cc independently?\n", "@andrewharp \nYou don't need to make clean if you are using fresh protobuf repos. It will be automatically created. And yes. With 0.1.2 it compiles OK.\nI will explore more since I still can reproduce it\n", "@ahumesky Does this look like a Bazel issue that was fixed between 0.1.1 and 0.1.2?\n", "My best guess is that this was fixed by https://github.com/bazelbuild/bazel/commit/c74ee3784d3fd3a722f92c39805e57b4353a4509\n\nWhen you clean out everything some pregenerated binary must be getting lost. I would suggest using Bazel 0.1.2 or later if that solves your problem. There are no Java protocol buffers used by the demo, so  figuring out a way to somehow force the unecessary //third_party:protoc to exist for < 0.1.2 builds after cleaning would be something of a hack, especially since the root issue has already been fixed.\n", "Yes. For me, it only happens when I use 0.1.1\n", "@jayendra13 Are you still experiencing your original problem?\n", "'//third_party:protoc' is referring to the protoc in bazel here:\nhttps://github.com/bazelbuild/bazel/tree/master/third_party/protobuf\nhttps://github.com/bazelbuild/bazel/blob/master/third_party/BUILD#L20\n\nHow were you doing this step: \"4- compile bazel\"? If you were doing just \"compile.sh compile\", then the embedded tools might not have been created. If you do just \"compile.sh\" it will do all the steps.\n@lberki may know which commit may have fixed this issue, we've done a few different things in this area\n", "@andrewharp , Yes I am still facing the problem, this time I also tried to install bazel from ready-made shell script instead of building it. It is still giving errors!!\n\n```\neddie7@albus:~/lab/tensorflow$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nExtracting Bazel installation...\n......\nERROR: /home/eddie7/lab/tensorflow/tensorflow/tensorboard/bower/BUILD:3:1: no such package '@paper-dropdown-menu//': https://github.com/PolymerElements/paper-dropdown-menu.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 130.648s.\n```\n", "@hamidb \nI'm trying to make sure I completely understand this thread. If I understand correctly, you originally encountered this issue:\n\"cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\"\n\nWhat was the solution for you, if you found one, and how was this connected to the protobuf issue you also related here?\n\n@jayendra13 \nWhich version of Bazel are you using?\n", "@lberki, @davidzchen in case they've seen this before.\n\n@jayendra13: try searching for \"cannot open git-upload-pack\"  on the web and see if you can find some resources to help.  At this point it is out of scope of the TensorFlow team.\n", "@jayendra13 - Can you paste the output of running that bazel command with `--verbose_failures`? Are you getting any similar errors with other remote repositories? I am wondering whether this is related to bazelbuild/bazel#587.\n", "`cannot open git-upload-pack` seems to happen when the host name in the `remote=` attribute of `git_repository` cannot be resolved. Try running `git clone` with the URI mentioned in the offending `git_repository` rule. My hunch is that it won't work, and then it's a network misconfiguration (also make sure there is no sandboxing, firewalls or whatever in the way)\n\nBazel could be more explicit about it, though.\n", "I'm getting this error while installing from source. Any help? I'm now behind any proxy system. I'm able to access the link on chrome.\n\n```\nanish@anish:~/Projects/tensorflow$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \nERROR: /home/anish/Projects/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_styles//': Error cloning repository: https://github.com/polymerelements/paper-styles.git: cannot open git-upload-pack caused by https://github.com/polymerelements/paper-styles.git: cannot open git-upload-pack caused by https://github.com/polymerelements/paper-styles.git: cannot open git-upload-pack caused by https://github.com/polymerelements/paper-styles.git: cannot open git-upload-pack caused by https://github.com/polymerelements/paper-styles.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 128.161s\n```\n\nAfter doing `bazel clean` and running build again.. I get different error:\n\n```\nanish@anish:~/Projects/tensorflow$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \nERROR: /home/anish/Projects/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@iron_checked_element_behavior//': Error cloning repository: https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack caused by https://github.com/polymerelements/iron-checked-element-behavior.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 129.496s\n```\n\nPlease help.\n", "Hi\n\nI'm trying to build a docker image on DockerHub, and I get the same error. For the full log:\n\nhttps://hub.docker.com/r/nitnelave/tensorflow/builds/bbhn9ycakwkbmzyqupkzjbh/\n\nThe dependencies are installed from https://hub.docker.com/r/nitnelave/tensorflow-dependencies/builds/bujqarnnsp7hmbetck5a6ug/ , so Bazel 0.2.1\n\nIt works on my machine, but only randomly on DockerHub (and then it times out, but that's another issue).\n\nSadly, I have no control on the DockerHub settings, so I have no more information than you.\n", "+cc @kchodorow\n\n@AnishShah IIRC the problem with using the git_\\* rules behind a proxy has been fixed in the most recent versions of Bazel. Which version are you using?\n\n@nitnelave It seems that your build was able to successfully clone a bunch of the other repositories and this particular failure seems to be caused by spurious errors when cloning a repository. AFAIK, the git_repository rules currently do not support retrying failures.\n", "@davidzchen It could be useful to have a retry, or to find what is causing the problem in the cloning process... I can't reproduce it on my machine, but it's annoying to have the build system fail every other build because of this.\n", "@nitnelave Agreed, might be good to see if we jgit supports retries. Can you open a bug on bazelbuild/bazel for the Git retry issue?\n\n@AnishShah If you are using an old version of Bazel, can you try using the latest version and see if that works for you?\n", "same problem here. Using the newest tf and bazel. The error message is as following:\n\n`no such package '@paper_dialog//': Error cloning repository: https://github.com/polymerelements/paper-dialog.git: cannot open git-upload-pack caused by https://github.com/polymerelements/paper-dialog.git: cannot open git-upload-pack caused by https://github.com/polymerelements/paper-dialog.git: cannot open git-upload-pack and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Loading failed; build aborted.\n`\n", "@lightingghost Can you paste the full output from running your `bazel` command with `--verbose_failures`?\n", "@davidzchen Thanks for helping.\n\nI am using bazel 0.2.2b at that time, when I changed to 0.2.2, it works fine.\n", "@davidzchen: What's the status of this? \n", "experiencing same issue with bazel 0.3.0:\n\nERROR: /home/ray/tensorflow/tensorflow/tensorboard/bower/BUILD:5:1: no such package '@paper_radio_group//': Error cloning repository: https://github.com/polymerelements/paper-radio-group.git: cannot open git-upload-pack caused by https://github.com/polymerelements/paper-radio-group.git: cannot open git-upload-pack caused by Connection reset and referenced by '//tensorflow/tensorboard/bower:bower'.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 19.680s\n", "@davidzchen Friendly ping, do you have any update on this?\n", "Sorry for the wait. This fell off my radar, and I just returned from vacation.\n\nbazelbuild/bazel#1408 is the tracking issue for Git rules written in Skylark. @nelhage has already committed an initial implementation of the [Skylark Git rules](https://github.com/bazelbuild/bazel/blob/master/tools/build_defs/repo/git_repositories.bzl). There is still some work to do before we can fully migrate over, but in the meantime, you can give them a try by `load()`ing `@bazel_tools//tools/build_defs/repo:git_repositories.bzl`. These rules should be drop-in replacements for the Java ones and use the Git binaries installed on your system rather than jgit.\n\nNote that it seems the Git binaries [also do not support retries](http://stackoverflow.com/questions/35014012/git-retry-if-http-request-failed). If you are still having the same issue with the Skylark Git rules, then we should look into making use of [`git-retry`](https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/git-retry.html), which is included with Chromium's `depot_tools` and not the standard Git distribution.\n", "Closing due to lack of recent activity. Please open a new issue referencing this one if it is still an issue. Thanks!"]}, {"number": 451, "title": "MNIST tutorial data cant be downloaded (404)", "body": "Hey, tried to work my way through tf but the download link seems down\n\nhttps://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/mnist/input_data.py\n\n404s\n", "comments": ["The file was moved to tensorflow/examples/tutorials/mnist/input-data.py -- I think our website hasn't updated yet.\n\nYou might want to consider just git cloning the repository instead of downloading files individually.\n"]}, {"number": 450, "title": "tf.placeholder API docs should mention possibility of None", "body": "For `tf.placeholder`, it's possible to have `None` as one or more of the dimensions passed to the `shape` option. This is described in the MNIST tutorial, but is nowhere to be found in the API docs. The latter only mentions that the `shape` option itself may be `None`, but that is an entirely different thing.\n", "comments": ["Thanks, that should be fixed.\n", "This looks like it was fixed\n"]}, {"number": 449, "title": "TF code crashes python kernel", "body": "The following piece of code:\n\n```\nimport tensorflow as tf\na = tf.placeholder(tf.float32, shape=(-1, 10))\nsess = tf.Session()\nsess.run(tf.array_ops.shape(a))\n```\n\ncrashes the Python kernel. I realize it's not right (for one thing I'm not feeding it a `feed_dict`), but it should give an error and not crash the kernel.\n", "comments": ["Nice catch!  Yes, that is a bad bug.\n", "I have this fixed at the kernel level, so it won't crash anymore.  However, I could also fix it at the shape inference level so it throws a Python exception at graph construction time.  To do this, I'd need to make `tf.Dimension` insist on nonnegative input.\n\n@vrv: Do you think that's wise?  I'd be a bit worried that negative dimensions are used somewhere, but it's currently a bit inconsistent that C++ `TensorShape` requires nonnegative but Python `TensorShape` does not.\n", "C++ part in review.\n", "I'm not sure yet if its wise -- the C++ TensorShape class thinks about shapes more concretely: it doesn't support unspecified dimensions / -1 / shape inference, etc.\n\nI think when we push shape inference down into the C++ code, we'll likely have a C++ class that more closely represents what the python TensorShape class does.\n", "Cool, I'm happy to leave it the way it is now that the C++ is fixed.\n", "try using numpy befor importing tensorflow or keras\r\n"]}, {"number": 448, "title": "Image bug fix for tf.gather documentation", "body": "<img width=\"791\" alt=\"screen shot 2015-12-08 at 2 21 45 pm\" src=\"https://cloud.githubusercontent.com/assets/2761597/11665778/1fc3de48-9db7-11e5-9fb3-c086aa9aaa3c.png\">\n", "comments": ["Yeah, @colah needs to update the arrow\n", "And make sure to make a new image, not overwrite the old one!\n\nOn Tue, Dec 8, 2015 at 11:27 AM Vijay Vasudevan notifications@github.com\nwrote:\n\n> Yeah, @colah https://github.com/colah needs to update the arrow\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/448#issuecomment-162989238\n> .\n"]}, {"number": 447, "title": "Python get_attr on DT_FLOAT returns proto type, not python DType", "body": "If my graph has a Sub operation, and I get the \"T\" key for the op, it always returns an integer type incorrectly.\n\n```\nop: \"Sub\"\ninput: \"ResizeBilinear\"\ninput: \"Sub/y\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\n```\n\n```\nprint(ops[8].get_attr(\"T\"))\n1\n\nprint(type(ops[8].get_attr(\"T\")))\nint\n```\n", "comments": ["The problem is that `get_attr` on a type attr returns the enum value for that type, which happens to be 1 in this case.  It would make vastly more sense if it returned a `DType`.  Until we fix it, you can convert it to a `DType` via `tf.as_dtype(ops[8].get_attr('T'))`\n", "Retitled\n", "@girving Thanks.\nHow does one get the value itself? i.e. how do I get the current value of T in python as a float32?\n", "The T attribute for sub indicates the datatype of the tensors being operated on.  So T:DT_FLOAT just means that the two inputs are floating point tensors.\n\nWhat value specifically are you trying to get?  The output?  (Do you have a larger piece of code to share so that we have some more context?)\n", "@vrv oh, my bad then. Sorry about that. I was trying to get the current constant multiplier for a Mul op in the graph.\n\nHere is the relevant code: https://gist.github.com/anonymous/7399027e97d560c0f41c#file-dump_weights_hdf5-py-L218-L220\n\nI dumped all the weights of the inceptionv3 model into hdf5 files, but I cant get the info on the constants used for the Mul and Sub nodes. In general, there are two pain points that I found:\n- get_attr takes a name, but there is no current way (from the docs) to get a list of attributes of a node in the graph. I had to look through the source code for this.\n- I didn't find a way to get the Constant tensors (like parameters of a Conv2D) without running the graph in a session.\n", "Use `op.input(n)` for that.\n", "@girving: do we want to be smart / special about any other types of attributes in get_attr, or is special-casing for types_pb2.DataType enough?\n", "@vrv: I'm not sure, but the possibilities are shapes, tensors, and lists of types, shapes, and tensors.  I imagine shapes is already fine, and hopefully tensors as well.\n", "i tried op.input(n), but that doesn't really work for me. I'll try to find a bit more on how to get the current scalar value for the Mul op, thanks for all your help.\n", "Reopening since we'll use this bug to track cleaning up `get_attr`.\n", "As for your issues with `op.input`: is that problem that it returns a `Tensor` instead of the actual value?  If so, `tf.tensor_util.ConstantValue` will get you the underlying value (or `None`).  Unfortunately, it's technically not part of the public API, so if you use it your code will break at some point and you'll have to use `tf.constant_value` or whatever we rename it to.\n"]}, {"number": 446, "title": "tf.train.L_BFGS_Optimizer", "body": "This would be a great addition to tensorflow, and is conspicuously missing. Is there some specific reason it's missing, or is it in the works? \n", "comments": ["It is not currently in the works, and it would be a great addition to TensorFlow. Any contribution in this area would be very welcome.\n", "Putting in a full implementation of L-BFGS seems like a nontrivial task, but a first step could be introducing some sort of line search functionality, which could be an option to `GradientDescentOptimizer` as well when in full-batch (no-dropout) mode?\n\nPerhaps there could be e.g. a `FullBatchOptimizer` base class that implements line searches, with a virtual function `StepDirection` to compute line search directions (which could then be implemented separately for gradient descent, conjugate gradient, L-BFGS, etc.)?\n\nThoughts?\n", "I agree that it would be a great addition and I would be happy to contribute. I have been using something analogous to this slick implementation [lbfgs_cpp](https://github.com/js850/lbfgs_cpp) by @js850.\nFor instance the [`lbfgs_cpp::compute_lbfgs_step`](https://github.com/js850/lbfgs_cpp/blob/master/lbfgs.cpp#L165) would fit within your proposed `StepDirection` virtual function, although it would have to be rewritten to support gpus. Likewise for the backtracking linesearch which one might want to use on top of it.\n", "Mark Schmidt's [minFunc](https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html) is famous for its state of the art implementation of line searches. It's probably a very good reference implementation to benchmark against.\n\nAs a side note, the implementation of l-BFGS-b in scipy is a wrapper for the official FORTRAN implementation:\n\nhttp://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html\nhttp://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\nwhich is probably another good reference implementation to benchmark against.\n", "BTW, scipy lBFGS seems to be slower than Matlab's minFunc\nI tried running https://github.com/jatinshah/ufldl_tutorial, which uses scipy's lBFGS, and it felt an order of magnitude slower than original sparse-autoencoder Matlab version from http://deeplearning.stanford.edu/wiki/index.php/Neural_Network_Vectorization\n", "@yaroslavvb you have to be careful to use an equivalent stopping criterion which is not always easy to do.\n", "I had a question to the authors on testing an implementation of this.\n\nFor the other optimisers like Adam there is a python function that does the equivalent optimisation and then the test checks that the real implementation comes up with the same result. That seems reasonable given how simple the other optimisers are, but what about something more involved like L-BFGS-B?\n\nThe implementation could be compared to what scipy gets, but then again bringing in scipy as a dependency just for the sake of one test seemed, well, excessive. A full Python implementation just for the tests seems brittle as the Python implementation could just as easily be buggy. Perhaps a pure python implementation is the way to go?\n", "We have an external optimizer interface module coming to contrib/ that should address this. Allows controlling a TF session using e.g. scipy.optimize.minimize(). Should be available soon!\n", "@joshburkart - Sounds like exactly what I was looking for!\n", "This is done now and should be closed. :) See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/external_optimizer.py#L235) for usage instructions. \n", "Do you have any numbers on the speed? I've been working on a GPU version of l-BFGS optimizer in tensorflow, so it would be interesting to compare\n", "Hmm nope no numbers. It involves moving data not only off the GPU but also into Python for every loss or gradient evaluation, so I imagine you wouldn't want to use it for small optimization problems if efficiency is an issue. But as your loss becomes more expensive to evaluate this disadvantage becomes less important.\n", "Could persistent tensors be used to avoid the transfers from GPU to python?  You may want to take a look at python/ops/session_ops.py and python/kernel_tests/session_ops_test.py.\n", "@yuanbyu Not if we're using a Scipy optimizer to control a TF session. That's what we have working now. Keeping everything in TF the whole time would require a custom TF implementation.\n", "I guess that the Scipy optimizer makes a sequence of run calls in a TF session? Are there cases that a run call produces a tensor but the tensor is only used in a subsequent run call?  Persistent tensor would make it possible to keep such tensors \"in-place\" on the device that produced them, avoid moving them back and forth between the python client and the device.\n\nOn the other hand, it would be really nice to have a real native TF implementation, possibly using a nested while loop. \n", "So I have [a version](https://github.com/yaroslavvb/lbfgs/blob/master/lbfgs_optimize_reference.py) of l-BFGS that relies on persistent tensors to keep data in TensorFlow runtime, and [Immediate mode](https://github.com/tensorflow/tensorflow/pull/2747) for syntactic sugar. It's a line-by-line port of [lbfgs.lua](https://github.com/torch/optim/blob/master/lbfgs.lua) -- here's [a comparison](https://docs.google.com/spreadsheets/d/17RwQSIV6E0uXSATec2-Lu_GdhoqEsbabFEWHg1k5wzc/edit#gid=0) . I checked numerics on some toy problems and it tracks Lua implementation [almost exactly](https://github.com/yaroslavvb/lbfgs/blob/master/Porting%20Torch%20l-BFGS%20to%20immediate.ipynb) .\n\nPersistent tensor operations adds 200-300 usec overhead for each TensorFlow operation, so if \"the meat\" of each tensor operation is below 300usec, then a fully native TensorFlow implementation will be more efficient. \n", "If you use persistent tensors directly or have \"operator batching\" in your immediate mode, you could reduce significantly the number of run calls, and hence eliminate a lot of those 200 usec overheads. \n", "@yaroslavvb Any plan about when to add your implementation into official TensorFlow code? \n", "@weiliu620 It will not go into official TensorFlow code. There's considerable complexity in the module and nobody in the core team feels comfortable enough to assume ownership of it. However, since it's pure Python, it doesn't need to be part of the core. I'm working on getting a single-file version working so you could \"install\" the mode by copy pasting it into your file. The main core dependency is persistent tensors which should be working in upcoming release (0.10) after which I can release this mode. Do you have any specific use-cases in mind? In particular, I have the regular l-BFGS, but wondering if I need to do specific line-search additions\n", "@yaroslavvb While I'm testing the neural-style code by @jcjohnson, I found using lbfgs optimizer got cost function ~12% less than using adam optimizer, with fixed number of iterations. There are also some [discussions ](https://www.reddit.com/r/MachineLearning/comments/3u1ssk/neural_style_in_tensorflow/) on these two optimizers on style transfer. \n\nSo that is the use case that I run into this issue. Thanks for the work!\n", "@joshburkart Can you provide a minimal example of using the `loss_callback` with `fetches` ? The in-code examples are great, just not using loss_callback. Thanks!\n", "Edit: this post was wrong. See below.\n", "Whoa! Please ignore my previous completely incorrect post. Forgot that I actually did implement this in a reasonable way the first time. :P The callback does in fact get arbitrary fetches as appropriate-shaped arrays. So here's an example where the callback receives the loss itself (but it could receive any other tensor in the graph):\n\n``` python\nloss = ...\n\ni = 0\ndef callback(loss):\n  nonlocal i\n  print('Loss evaluation #', i, ', loss:', loss)\n  i += 1\n\nscipy_optimizer =  tf.contrib.opt.ScipyOptimizerInterface(loss)\nscipy_optimizer.minimize(session, fetches=[loss], loss_callback=callback)\n```\n\nI should really rename `fetches` to `callback_fetches`, since this is confusing...\n", "Is there any progress on this enhancement? I'm working on that and need a comparison.\n", "@joshburkart tested and the callback works. Thanks for the great work!\nOne question: when I add the variable that I optimize into the list of  `fetches`, I got error `Variable:0 is both fed and fetched.` \n", "Hi @joshburkart and @weiliu620, would you mid elaborating a bit further on how to use a BFGS optimizer? I'm a bit too novice to fill in the missing pieces from Josh's example. I have a basic running net that uses the following:\n\n`loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(trn_logits, tf_train_labels))\noptimizer = tf.train.AdamOptimizer(0.05).minimize(loss)\n`\nI'm a bit confused as to how to deal with the: session, fetches=[loss], loss_callback=callback part.\n\nIf you have suggestions or some pointers to where to look for answers I'd really appreciate it.\nThanks!\n", "@olegaulov There are some examples in the [docstring](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/external_optimizer.py#L246) :)\n", "Thanks @joshburkart ! Just what I was looking for.\n", "Thanks @joshburkart for your contribution. Would you mind to close this issue? @zfrenchee @martinwicke \n", "For future lurkers, here's an end-to-end example of using this optimizer on the UFDL sparse ae tutorial\r\n\r\nhttps://github.com/ncullen93/UFDL_Tensorflow/blob/master/tf_sparse_ae.py\r\n", "@ncullen93 the link is not valid anymore. does anyone have a backup copy?", "@zhaojunz Yes, the link is dead. Maybe this will help on how to use a scipy optimizer (L-BFGS for example):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/external_optimizer_test.py\r\n\r\n", "@weiliu620 @joshburkart \r\nI have tried the L-BFGS method in the neural-art code by @jcjohnson . \r\nIt does work and return more stable results than Adam.\r\nHowever, I find the optimization became a little slower.\r\n\r\nAlso, is there any way to probe at iteration progress (print stuff / generate checkpoint images using interval variables) like the original code does.\r\nSince I find in the official comment doc that \"variables subject to optimization are updated in-place at the end of optimization\".", "Hello @flowice \r\nDo you mind sharing the codes of the L-BFGS in tensorflow with me? So you have the working code? \r\nthanks a lot\r\nBest\r\nRabeeh", "@rabeehk  it is easy to use. I just follow the official doc.\r\nThe core code is here:\r\n\r\n```\r\ntrain_step = tf.contrib.opt.ScipyOptimizerInterface(\r\n                loss,\r\n                method='L-BFGS-B',\r\n                options={'maxiter': iterations})\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    train_step.minimize(sess)\r\n    \r\n```\r\n", "We have added LBFGS based optimizers in Breeze which is being used in Spark MLlib for example and the C++ port from fortran is available through multiple projects (I benchmarked this for example https://github.com/chokkan/liblbfgs) but given that neural nets are based on non-convex loss (original distbelief paper also found that sgd + adagrad works better than LBFGS), is it useful to add LBFGS in tensorflow ? Most likely users are now running convex models in tensorflow and for such models LBFGS is a good fit compared to SGD + Adagrad ? ", "@debasish83 I've implemented l-BFGS in TensorFlow [here](https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py).\r\n\r\nSome benchmarks are [here](https://medium.com/@yaroslavvb/tensorflow-meets-pytorch-with-eager-mode-714cce161e6c). It's not as performant as it could be, but @alextp and friends are working on optimizing the underlying framework.\r\n\r\nOne thing that should significantly speed up implementation above is to reduce Python/TensorFlow round-trips by replacing inner python loops (two-step recursion) with defuns, something like this (h/t @agarwal-ashish)\r\n\r\n```\r\ndef f(k, q, *args):\r\n  old_dirs = args[:k]\r\n  ro = args[k:2*k]\r\n  old_steps = args[2*k:3*k]\r\n  for i in range(k-1, -1, -1):\r\n    al = dot(old_dirs[i], q) * ro[i]\r\n    q = q - al * old_steps[i]\r\n  return q\r\n\r\n@function.defun\r\ndef g(q, *args):\r\n  return f(100, q, *args)\r\n\r\n q = -g\r\n  if k == 100:\r\n    q = g(q, *(old_dirs + ro + old_steps))\r\n  else:\r\n    q = f(k, q, *(old_dirs + ro + old_steps))\r\n\r\n```", "@yaroslavvb If the model is distributed on a set of ps nodes, will this https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py work ? I think we need to run BFGS loop on the chief node which coordinate the worker and ps nodes...distbelief paper did that...", "Nope, there's no distributed support in Eager. So if you want to large scale implementation, you'll need to use classic execution mode.\r\n\r\nThere used to be l-BFGS implemented in standard TensorFlow but it was deleted because it never got robust enough. Because tensorflow distributed framework is quite low-level so people sometimes get surprised by how much work it takes to get something robust. Following comment/commits in sync_replicas_optimizer gives an overview of some pitfalls -- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer.py", "K-FAC is a second-order optimizer, isn't it?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/kfac", "Hi all, \r\nHave any one considered implementing the Scaled Conjugate Gradient Algorithm (SCG) in Tensorflow?\r\nSCG avoids the line search alltogether. SCG is currently implemented in Mathlab as 'trainscg' method. \r\nI would like to contribute, but I am, however, currently not that familiar with Tensorflow. \r\nFor reference see: \r\n- A scaled conjugate gradient algorithm for fast supervised learning, M. F. Moller, Neural Networks 6 (4), 525-533, 1993.\r\n and more general info on: \r\n- Efficient Training of Feed-Forward Neural Networks\r\n\r\nThe Hessian times a vector (used in the SCG) can be calculated exact and/or approximated. \r\n--\r\n[A scaled conjugate gradient algorithm for fast supervised learning.pdf](https://github.com/tensorflow/tensorflow/files/2253652/A.scaled.conjugate.gradient.algorithm.for.fast.supervised.learning.pdf)\r\n\r\n[Efficient training of feed-forward neural networks - moller - phd thesis.pdf](https://github.com/tensorflow/tensorflow/files/2253657/Efficient.training.of.feed-forward.neural.networks.-.moller.-.phd.thesis.pdf)\r\n\r\n\r\n", "fyi, there is a BFGS-Implementation in [tensorflow/probability](https://github.com/tensorflow/probability):\r\nhttps://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/optimizer/bfgs.py", "I have implemented L-BFGS optimizer in my dinrhiw2 C++ machine learning library. When implemented well line search doesn't take that much time and seem to be get results equally fast than with first order optimizers. For some problems L-BFGS can seem to be able to side-step some local minimas and get better results.\r\n\r\nhttps://github.com/cslr/dinrhiw2/blob/RBM_test/src/math/LBFGS.cpp", "FYI -- i've put out a request to update and make the scipyoptimizer interface compatible with eager execution.  Additionally would like to tie in an early stopping option (i list a hack-y way to do this already).  Anyone willing to help would be much appreciated.  The scipy optimizer has L-BFGS + CG + others, so it would be worth the time!\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/23973", "@yaroslavvb Do you have any benchmarks againsts scipy's CPU version ? That would be very interesting.", "@flowice Thank you very much for your answer. But could I apply the ```bfgs``` method in each batch of data and do the parameter update?\r\n\r\nI have also checked the other package like ```neupy``` and it seems like they provide an answer. But it seems like this package relies on ```theano``` and makes it very difficult to run codes in the 64-bit computer....", "this blog posts explains how to use `tfp.optimizer.lbfgs_minimize` and provides a reproducible example. \r\nhttps://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/"]}, {"number": 445, "title": "tensorflow in armv7l", "body": "Hi,\n\nI've cross-compiled tensorflow for `armv7l` and generated a wheel successfully however when deploying it into an embedded board with the same architecture (e.g.: Raspberry Pi 2), i get the following when executing https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1%20-%20Introduction/helloworld.py:\n\n```\nerle@erle-brain-2 ~/TensorFlow-Examples/examples/1 - Introduction $ python helloworld.py \nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\npure virtual method called\nterminate called without an active exception\nI tensorflow/core/common_runtime/direct_session.cc:60] Direct session inter op parallelism threads: 4\nAborted\n```\n\nDigging a bit more:\n\n```\nerle@erle-brain-2 ~/TensorFlow-Examples/examples/1 - Introduction $ gdb -ex r --args python helloworld.py\nGNU gdb (Raspbian 7.7.1+dfsg-5) 7.7.1\nCopyright (C) 2014 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"arm-linux-gnueabihf\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from python...(no debugging symbols found)...done.\nStarting program: /usr/bin/python helloworld.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/arm-linux-gnueabihf/libthread_db.so.1\".\n\nProgram received signal SIGILL, Illegal instruction.\n0x73f1cd08 in ?? () from /usr/lib/arm-linux-gnueabihf/libcrypto.so.1.0.0\n(gdb) bt\n#0  0x73f1cd08 in ?? () from /usr/lib/arm-linux-gnueabihf/libcrypto.so.1.0.0\n#1  0x73f193f4 in OPENSSL_cpuid_setup () from /usr/lib/arm-linux-gnueabihf/libcrypto.so.1.0.0\n#2  0x76fdf058 in call_init (l=<optimized out>, argc=2, argv=0x7efff184, env=0x7efff190) at dl-init.c:78\n#3  0x76fdf134 in _dl_init (main_map=main_map@entry=0x8e9268, argc=2, argv=0x7efff184, env=0x7efff190) at dl-init.c:126\n#4  0x76fe36b4 in dl_open_worker (a=<optimized out>) at dl-open.c:577\n#5  0x76fdeef0 in _dl_catch_error (objname=0x76fdeef0 <_dl_catch_error+112>, objname@entry=0x7effcc04, errstring=0x76ff6510, errstring@entry=0x7effcc08, \n    mallocedp=0x7effcc04, mallocedp@entry=0x7effcc03, operate=0x7effcc03, args=args@entry=0x7effcc0c) at dl-error.c:187\n#6  0x76fe2da4 in _dl_open (file=0x9094e0 \"/usr/lib/python2.7/lib-dynload/_hashlib.arm-linux-gnueabihf.so\", mode=-2147483646, \n    caller_dlopen=0x10aa94 <_PyImport_GetDynLoadFunc+272>, nsid=-2, argc=2, argv=0x7efff184, env=0x7efff190) at dl-open.c:661\n#7  0x76f66ba8 in dlopen_doit (a=0x7effce58) at dlopen.c:66\n#8  0x76fdeef0 in _dl_catch_error (objname=0x76fdeef0 <_dl_catch_error+112>, errstring=0x76ff6510, mallocedp=0x4e93a4, operate=0x4e93a0, args=0x7effce58)\n    at dl-error.c:187\n#9  0x76f672a8 in _dlerror_run (operate=0x76f66b28 <dlopen_doit>, args=args@entry=0x7effce58) at dlerror.c:163\n#10 0x76f66c74 in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87\n#11 0x0010aa94 in _PyImport_GetDynLoadFunc ()\n#12 0x0010a338 in _PyImport_LoadDynamicModule ()\n#13 0x00067844 in ?? ()\nBacktrace stopped: previous frame identical to this frame (corrupt stack?)\n(gdb) Quit\n(gdb) quit\nA debugging session is active.\n```\n\n`cryt*` libraries in the machine used to cross compile tensorflow:\n\n```\nroot@debian-arm:~/TensorFlow-Examples/examples/1 - Introduction# dpkg -l|grep crypt\nii  libcryptsetup4:armhf          2:1.6.6-5                 armhf        disk encryption support - shared library\nii  libgcrypt20:armhf             1.6.3-2                   armhf        LGPL Crypto library - runtime library\nii  libhogweed2:armhf             2.7.1-5                   armhf        low level cryptographic library (public-key cryptos)\nii  libk5crypto3:armhf            1.12.1+dfsg-19            armhf        MIT Kerberos runtime libraries - Crypto Library\nii  libnettle4:armhf              2.7.1-5                   armhf        low level cryptographic library (symmetric and one-way cryptos)\nii  openssl                       1.0.1k-3+deb8u1           armhf        Secure Sockets Layer toolkit - cryptographic utility\nii  python-cryptography           0.6.1-1                   armhf        Python library exposing cryptographic recipes and primitives (Python 2)\n\n```\n\n`cryt*` libraries in the target machine (Raspberry Pi 2):\n\n```\nerle@erle-brain-2 ~/TensorFlow-Examples/examples/1 - Introduction $ dpkg -l|grep crypt\nii  cryptsetup-bin                         2:1.6.6-5                                 armhf        disk encryption support - command line tools\nii  libcryptsetup4:armhf                   2:1.6.6-5                                 armhf        disk encryption support - shared library\nii  libgcrypt20:armhf                      1.6.3-2                                   armhf        LGPL Crypto library - runtime library\nii  libhcrypto4-heimdal:armhf              1.6~rc2+dfsg-9+rpi1                       armhf        Heimdal Kerberos - crypto library\nii  libhogweed2:armhf                      2.7.1-5                                   armhf        low level cryptographic library (public-key cryptos)\nii  libk5crypto3:armhf                     1.12.1+dfsg-19                            armhf        MIT Kerberos runtime libraries - Crypto Library\nii  libmhash2:armhf                        0.9.9.9-7                                 armhf        Library for cryptographic hashing and message authentication\nii  libnettle4:armhf                       2.7.1-5                                   armhf        low level cryptographic library (symmetric and one-way crypos)\nii  libpococrypto9                         1.3.6p1-5                                 armhf        C++ Portable Components (POCO) Crypto library\nii  openssl                                1.0.1k-3+deb8u1                           armhf        Secure Sockets Layer toolkit - cryptographic utility\n```\n", "comments": ["For completeness:\n\n```\nPython 2.7.9 (default, Mar  8 2015, 00:52:26) \n[GCC 4.9.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> sess = tf.Session()\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\npure virtual method called\nI tensorflow/core/common_runtime/direct_session.cc:60] Direct session inter op parallelism threads: 4\nterminate called without an active exception\nAborted\n```\n", "Any idea what the problem is here?\n", "No expert here, but pretty sure that exception trace is an artifact of the debugger. See https://bugs.launchpad.net/raspbian/+bug/1154042 for a similar issue with a good explanation.\n\nBTW, this is TensorFlow compiled in CPU only mode for the Pi, right?\n", "Looks like this fell through the cracks.  @petewarden: Any thoughts here?\n", "In the meantime, the somewhat faster Pi v3 shipped. Perhaps that affects the possibilities (and feasibilities) around Tensorflow for Pi? \n\nhttps://en.wikipedia.org/wiki/Raspberry_Pi says \"Raspberry Pi 3 has a new BCM2837 SoC retaining compatibility with the GPU, CPU and connectors of its predecessors BCM2835 (Pi 1) and BCM2836 (Pi 2), so all those projects and tutorials for Pi 1 and Pi 2 hardware should continue to work. The 900 MHz 32-bit quad-core ARM Cortex-A7 CPU complex has been replaced by a 1.2 GHz 64-bit quad-core ARM Cortex-A53. Combining a 33% increase in clock speed with various architectural enhancements, this provides a 50\u201360% increase in performance in 32-bit mode versus Raspberry Pi 2, or roughly a factor of ten over the original Pi 1.\"\n", "From the use case perspective I'd definitely like to see TensorFlow on Pi, at least for the predict function. \n\nI have a Pi that mines text and data from web daily and I am also planning to add NN processing to my scripts. I would train the model on my regular Core i7 dev box with a GPU but would love to run the prediction daily on the Pi (perhaps retraining the model monthly). Why Pi? Because it has more than enough power to run Python scripts and do the data processing that I need, and consumes only 2W at full load (the Pi2 model, Pi3 I believe is more like 3W).\n", "@martinwicke: What's the state of cmake in contrib?  Would it be practical to get tensorflow compiling _on_ a Pi rather than cross compiling? \n", "My plan is to get Blaze running on a Pi if I can. CMake isn't very pleasant to work with for this, in my experience. I hope to get to this asap.\n", "For what it's worth, I'm attempting to natively compile TensorFlow on a Raspberry Pi 3 running Raspbian. For the most part, I'm using a modified version of [these instructions for building on a Jetson TK1](http://cudamusing.blogspot.com/2015/11/building-tensorflow-for-jetson-tk1.html), with the main changes being that I'm building Bazel 1.4 instead of 1.0, and I'm not building for Cuda.\n\n---\n\nHere's where I'm getting hung up right now: I tried using Bazel to build the `tutorials_example_trainer` binaries with the following command:\n\n```\nbazel build -c opt --local_resources 1024,0.5,1.0 --verbose_failures tensorflow/cc:tutorials_example_trainer\n```\n\nAfter and 1.5 hours, it threw the following error message:\n\n```\nERROR: /home/pi/programming/tensorflow/tensorflow/core/kernels/BUILD:639:1: C++ compilation of rule '//tensorflow/core/kernels:argmax_op' failed: gcc failed: error executing command \n  (cd /home/pi/.cache/bazel/_bazel_pi/ddee419d9c6b5440629c2870bc1d9b2e/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-db7b61411772 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-db7b61411772 -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.d -c tensorflow/core/kernels/argmax_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4: gcc failed: error executing command \n  (cd /home/pi/.cache/bazel/_bazel_pi/ddee419d9c6b5440629c2870bc1d9b2e/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-db7b61411772 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-db7b61411772 -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.d -c tensorflow/core/kernels/argmax_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 5398.942s, Critical Path: 5019.43s\n```\n\nThe first message said that the `tensorflow/core/kernels:argmax_op` build instruction was failing, so I ran Bazel on just that part to see what messages I'd see along the way. Here's the command I ran:\n\n```\nbazel build -c opt --local_resources 1024,0.5,1.0 --verbose_failures tensorflow/core/kernels:argmax_op\n```\n\nAnd here's the full output, including Bazel messages before the error:\n\n```\n..............................................\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/util/internal/field_mask_utility.cc:\ngoogle/protobuf/src/google/protobuf/util/internal/field_mask_utility.cc:47:14: warning: 'google::protobuf::util::Status google::protobuf::util::converter::{anonymous}::CreatePublicError(google::protobuf::util::error::Code, const string&)' defined but not used [-Wunused-function]\n util::Status CreatePublicError(util::error::Code code,\n              ^\nINFO: From Compiling google/protobuf/src/google/protobuf/util/internal/utility.cc:\ngoogle/protobuf/src/google/protobuf/util/internal/utility.cc:50:19: warning: 'const google::protobuf::StringPiece google::protobuf::util::converter::{anonymous}::SkipWhiteSpace(google::protobuf::StringPiece)' defined but not used [-Wunused-function]\n const StringPiece SkipWhiteSpace(StringPiece str) {\n                   ^\nINFO: From Compiling google/protobuf/src/google/protobuf/util/time_util.cc:\ngoogle/protobuf/src/google/protobuf/util/time_util.cc:371:6: warning: 'void google::protobuf::{anonymous}::ToUint128(const google::protobuf::Timestamp&, google::protobuf::uint128*, bool*)' defined but not used [-Wunused-function]\n void ToUint128(const Timestamp& value, uint128* result, bool* negative) {\n      ^\ngoogle/protobuf/src/google/protobuf/util/time_util.cc:396:6: warning: 'void google::protobuf::{anonymous}::ToTimestamp(const google::protobuf::uint128&, bool, google::protobuf::Timestamp*)' defined but not used [-Wunused-function]\n void ToTimestamp(const uint128& value, bool negative, Timestamp* timestamp) {\n      ^\nINFO: From Compiling tensorflow/core/util/tensor_slice_reader.cc:\nIn file included from ./tensorflow/core/platform/default/logging.h:23:0,\n                 from ./tensorflow/core/platform/logging.h:24,\n                 from ./tensorflow/core/lib/core/status.h:24,\n                 from ./tensorflow/core/lib/core/errors.h:19,\n                 from ./tensorflow/core/framework/tensor_shape.h:24,\n                 from ./tensorflow/core/util/tensor_slice_reader.h:26,\n                 from tensorflow/core/util/tensor_slice_reader.cc:16:\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_LTImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = unsigned int; std::string = std::basic_string<char>]':\ntensorflow/core/util/tensor_slice_reader.cc:136:3:   required from here\n./tensorflow/core/platform/default/logging.h:197:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, < )\n                                   ^\n./tensorflow/core/platform/macros.h:54:29: note: in definition of macro 'TF_PREDICT_TRUE'\n #define TF_PREDICT_TRUE(x) (x)\n                             ^\n./tensorflow/core/platform/default/logging.h:197:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, < )\n ^\nINFO: From Compiling tensorflow/core/kernels/transpose_functor_cpu.cc:\nIn file included from ./tensorflow/core/platform/default/logging.h:23:0,\n                 from ./tensorflow/core/platform/logging.h:24,\n                 from ./tensorflow/core/lib/gtl/array_slice_internal.h:32,\n                 from ./tensorflow/core/lib/gtl/array_slice.h:101,\n                 from ./tensorflow/core/framework/types.h:33,\n                 from ./tensorflow/core/framework/type_traits.h:22,\n                 from ./tensorflow/core/framework/allocator.h:25,\n                 from ./tensorflow/core/framework/tensor.h:21,\n                 from ./tensorflow/core/kernels/transpose_functor.h:19,\n                 from tensorflow/core/kernels/transpose_functor_cpu.cc:18:\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = unsigned int; std::string = std::basic_string<char>]':\ntensorflow/core/kernels/transpose_functor_cpu.cc:78:3:   required from here\n./tensorflow/core/platform/default/logging.h:194:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\n                         ^\n./tensorflow/core/platform/macros.h:54:29: note: in definition of macro 'TF_PREDICT_TRUE'\n #define TF_PREDICT_TRUE(x) (x)\n                             ^\n./tensorflow/core/platform/default/logging.h:193:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\n ^\nINFO: From Compiling tensorflow/core/graph/costmodel.cc:\nIn file included from ./tensorflow/core/platform/default/logging.h:23:0,\n                 from ./tensorflow/core/platform/logging.h:24,\n                 from ./tensorflow/core/lib/core/status.h:24,\n                 from ./tensorflow/core/framework/op_def_builder.h:25,\n                 from ./tensorflow/core/framework/op.h:23,\n                 from ./tensorflow/core/graph/graph.h:44,\n                 from ./tensorflow/core/graph/costmodel.h:22,\n                 from tensorflow/core/graph/costmodel.cc:16:\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = unsigned int; std::string = std::basic_string<char>]':\ntensorflow/core/graph/costmodel.cc:65:9:   required from here\n./tensorflow/core/platform/default/logging.h:194:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\n                         ^\n./tensorflow/core/platform/macros.h:54:29: note: in definition of macro 'TF_PREDICT_TRUE'\n #define TF_PREDICT_TRUE(x) (x)\n                             ^\n./tensorflow/core/platform/default/logging.h:193:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\n ^\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_LTImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = unsigned int; std::string = std::basic_string<char>]':\ntensorflow/core/graph/costmodel.cc:146:3:   required from here\n./tensorflow/core/platform/default/logging.h:197:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, < )\n                                   ^\n./tensorflow/core/platform/macros.h:54:29: note: in definition of macro 'TF_PREDICT_TRUE'\n #define TF_PREDICT_TRUE(x) (x)\n                             ^\n./tensorflow/core/platform/default/logging.h:197:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, < )\n ^\nINFO: From Compiling tensorflow/core/common_runtime/function.cc:\nIn file included from ./tensorflow/core/platform/default/logging.h:23:0,\n                 from ./tensorflow/core/platform/logging.h:24,\n                 from ./tensorflow/core/lib/gtl/array_slice_internal.h:32,\n                 from ./tensorflow/core/lib/gtl/array_slice.h:101,\n                 from ./tensorflow/core/framework/types.h:33,\n                 from ./tensorflow/core/framework/type_traits.h:22,\n                 from ./tensorflow/core/framework/allocator.h:25,\n                 from ./tensorflow/core/common_runtime/device.h:35,\n                 from ./tensorflow/core/common_runtime/function.h:21,\n                 from tensorflow/core/common_runtime/function.cc:16:\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = unsigned int; std::string = std::basic_string<char>]':\ntensorflow/core/common_runtime/function.cc:147:3:   required from here\n./tensorflow/core/platform/default/logging.h:194:25: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\n                         ^\n./tensorflow/core/platform/macros.h:54:29: note: in definition of macro 'TF_PREDICT_TRUE'\n #define TF_PREDICT_TRUE(x) (x)\n                             ^\n./tensorflow/core/platform/default/logging.h:193:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\n ^\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_EQImpl(const T1&, const T2&, const char*) [with T1 = unsi                         == )  // Compilation error with CHECK_EQ(NULL, x)?\n                         ^\n./tensorflow/core/platform/macros.h:54:29: note: in definition of macro 'TF_PREDICT_TRUE'\n #define TF_PREDICT_TRUE(x) (x)\n                             ^\n./tensorflow/core/platform/default/logging.h:193:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\n TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\n ^\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::string* tensorflow::internal::Check_LTImpl(const T1&, const T2&, const char*) [with T1 = int; T2 = unsigned int; std::string = std::basic_string<char>]':\ntensorflow/core/common_runtime/function.cc:1157:5:   required from here\n./tensorflow/core/platform/default/logging.h:197:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, < )\n                                   ^\n./tensorflow/core/platform/macros.h:54:29: note: in definition of macro 'TF_PREDICT_TRUE'\n #define TF_PREDICT_TRUE(x) (x)\n                             ^\n./tensorflow/core/platform/default/logging.h:197:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\n TF_DEFINE_CHECK_OP_IMPL(Check_LT, < )\n ^\nINFO: From Compiling tensorflow/core/kernels/argmax_op.cc:\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-4.9/README.Bugs> for instructions.\nERROR: /home/pi/programming/tensorflow/tensorflow/core/kernels/BUILD:639:1: C++ compilation of rule '//tensorflow/core/kernels:argmax_op' failed: gcc failed: error executing command \n  (cd /home/pi/.cache/bazel/_bazel_pi/ddee419d9c6b5440629c2870bc1d9b2e/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-db7b61411772 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-db7b61411772 -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.d -c tensorflow/core/kernels/argmax_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4: gcc failed: error executing command \n  (cd /home/pi/.cache/bazel/_bazel_pi/ddee419d9c6b5440629c2870bc1d9b2e/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-db7b61411772 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-db7b61411772 -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.d -c tensorflow/core/kernels/argmax_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\nTarget //tensorflow/core/kernels:argmax_op failed to build\nINFO: Elapsed time: 2301.254s, Critical Path: 1884.37s\n```\n\n---\n\nNot sure if this helps, but I figured I should share what I've got.\n", "Thanks for the update! That is helpful to see. I got as far as getting protobuf compiling, but I haven't made it to Bazel or TensorFlow itself yet. I'm hoping to work on that over the next few days.\n", "Should I upload my Pi's Bazel repository/binary/whatever somewhere? It may (or may not) save you some time.\n\nAdditionally, I ran the failed `gcc` command as mentioned in the Bazel error: \n\n```\ngcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-db7b61411772 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-db7b61411772 -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.d -c tensorflow/core/kernels/argmax_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/argmax_op/tensorflow/core/kernels/argmax_op.o\n```\n\nAnd got the following message:\n\n```\nIn file included from ./tensorflow/core/kernels/argmax_op.h:20:0,\n                 from tensorflow/core/kernels/argmax_op.cc:24:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:67: fatal error: eigen-eigen-db7b61411772/unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"eigen-eigen-db7b61411772/unsupported/Eigen/CXX11/Tensor\"\n```\n\nThis is my project for the next few days, so I'll try to post progress here. Let me know if messages are irrelevant- I'll remove those bits so I'm not clogging this thread.\n", "Well, I added a USB drive as swap space (RIP that drive soon), pulled the latest files from yesterday and tried it again, and after 3 hours it looks like I got `tutorials_example_trainer` compiled! Going to try compiling the pip package now.\n", "I believe I've got it working. I removed the swap drive after installing and am able to run the base MNIST tutorial scripts from tensorflow/examples/tutorials/mnist. I can also play with the TensorFlow package in a Python REPL, and a loose look at `htop` indicates that it's using system resources pretty well.\n\nI've got the process documented decently. I'm going to clean the instructions, test them on another Pi, attempt to install straight from the wheel file, and try out some other shenanigans. Hopefully have this knocked out over the weekend!\n", "Thanks Sam! I managed to get the label_image example compiling and running on my Pi 2, here are my notes:\n- I mostly followed the Jetson instructions too.\n- There was an error in GPUBFCAllocator that meant I had to comment out a couple of lines.\n- I had to link in the rt library to fix a clock_gettime() linking error, by adding \"-lrt\" to label_image's link_opts.\n- The resulting binary ran extremely slowly, taking over a minute to run the Inception network. I believe this is because the compiler is not using NEON by default (since the Pi 1 doesn't have that). I will be retrying with NEON enabled.\n", "Excellent! I was hoping to try out the process on a Pi 2 as well. Couple responses and additional notes\n- The GPUBFCAllocator compiling issues are most likely due to changes made in ab6ffc92992f124. On my first go-through (on Friday, just before I pulled in that commit), Bazel didn't complain. The next day (after that commit), I started going through the process again on another Pi to make sure I had it documented properly, but I got errors at gpu_bfc_allocator.cc (probably the same as what you saw). \n- ~~I think the clock_gettime() linking problem will be a recurring theme- running the MNIST example gives completely inaccurate time-to-train values now.~~ On second glance, things appear to be working correctly for the MNIST training. Disregard this bullet!\n- I don't have a good idea of how fast the Pi should be running these models- let me know if you see improvements with NEON!\n- I have been able to compile the distributed runtime, so I'm hoping to play with inter-device communication between Pis and a Mac today.\n", "As another data point, I compiled and ran the label_image binary, and it took much less than a minute. I did not play around with any of the compiler settings. Could be a difference between the Pi2 and the Pi3. I'm also running Raspbian 8.0.\n\nOutput:\n\n```\nW tensorflow/core/kernels/batch_norm_op.cc:36] Op is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nI tensorflow/examples/label_image/main.cc:207] military uniform (866): 0.647298\nI tensorflow/examples/label_image/main.cc:207] suit (794): 0.0477194\nI tensorflow/examples/label_image/main.cc:207] academic gown (896): 0.0232409\nI tensorflow/examples/label_image/main.cc:207] bow tie (817): 0.0157354\nI tensorflow/examples/label_image/main.cc:207] bolo tie (940): 0.0145024\n```\n", "Sorry about the RPi breakage, should be fixed in d2a06c2df73ed09d9eff60efb559fd38bc47e2d3\n", "Thanks @vrv, I rebuilt from source on a RPi3 and can confirm that it appears to work fine. \n\nI'll release an unofficial step-by-step guide on how I built the standard TensorFlow binaries from source specifically for the Raspberry Pi 3, as well as a link to a pre-compiled pip wheel. The wheel worked on a fresh Pi without any headaches, so I'm hoping people can use that if they don't absolutely have to go through the whole process.\n\nOnce a \"correct\" process of building is established, what kind of tests should be run on the build to make you guys comfortable with putting a Raspberry Pi/Raspbian-targeted wheel on PyPi? Or would that be way too much maintenance overhead for you all? I only ask because I think it'd be pretty sexy to be able to `pip install tensorflow` on a little RPi :P\n", "[Here's the process I used to get TensorFlow running](https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md). \n", "Awesome work! Thanks @samjabrahams for putting everything together. Many of us tried this out. Glad someone succeeded.\n\nClosing this now.\n", "Thanks @vmayoral for all the effort you put into this on both the TensorFlow and Bazel front- you got the momentum for this rolling in the first place. Let me know if the instructions/wheel file work for you!\n", "Great work @samjabrahams and everyone else! I look forward testing and I think this is an exciting contribution for the machine learning + physical computing world overall.\n", "I install tensorflow in Beaglebone black with Docker successfully. But when I predict model, it's failed that:\r\nIllegal instruction (core dumped)\r\n", "As an update, I believe the original \"pure virtual method called\" crash should be addressed by this solution from the Pi StackOverflow board: https://raspberrypi.stackexchange.com/questions/48225/whats-causing-these-crashes-after-cross-compiling\r\n\r\nI'm testing it for a hopeful NEON-enabled Pi wheel we're working on at https://github.com/tensorflow/tensorflow/pull/11675", "As an update, the fix mentioned in the answer above did seem to work.", "@vmayoral I have the same issue. \r\n>>>sess = tf.Session()\r\n>>>pure virtual method called\r\nterminate called without an active exception\r\nAborted\r\nHow did you fix it?", "I followed your step and i got no any error  in installing tensorflow , but when i use it , it showed me error\r\n\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow"]}, {"number": 444, "title": "Python3 bug, filter has no len", "body": "I have build the latest source (a5d8217c4ed) and I tried running the mnist tutorial. This fails with:\n\n  File \"/l/psmit/tensorflow/env_py3/lib/python3.4/site-packages/tensorflow/python/ops/gradients.py\", line 447, in gradients\n    if gate_gradients and len(filter(None, in_grads)) > 1:\nTypeError: object of type 'filter' has no len()\n\nA filter in py3 is not a list, so it has not length. This could be solved by changing \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients.py#L447\n\nto e.g. sum(1 for _ in filter())\n", "comments": ["Good catch.  We should fix this and add a unit test.\n", "Fix in review.\n"]}, {"number": 443, "title": "Feature Request: LogSoftmax layer", "body": "Torch has a LogSoftmax layer, that does what the name imply: it is the equivalent of a softmax followed by a log. However, I could not find a similar layer in tensorflow.\n\nLogSoftmax is quite convenient because I am often more interested in log-probabilities (eg for computing a log-likelihood) than in the probabilities themselves. \n\nFurther, I suspect a LogSoftmax can be implemented more efficiently than a Softmax (it should at least saves a log call, if one is interested in the log-probabilities; plus it is less sensitive to underflow/overflows). Indeed Torch documentation indicates it is faster when one need the log-probabilities.\n\nWould you consider adding such a layer at some point?\n", "comments": ["Yep, this is a good feature request.\n", "@girving can this issue be closed?\n", "Yep.  For future reference: If you include \"Fixes #<issue>\" in a commit message it'll autoclose.\n", "Does the logsoftmax be added? Still I can't use logsoftmax \n"]}, {"number": 442, "title": "cannot find numpy/arrayobject.h", "body": "INFO: From Compiling tensorflow/python/client/tf_session_helper.cc:\nIn file included from tensorflow/python/client/tf_session_helper.cc:20:0:\n./tensorflow/python/client/tf_session_helper.h:34:31: fatal error: numpy/                                                            arrayobject.h: No such file or directory\n #include \"numpy/arrayobject.h\"\n                               ^\ncompilation terminated.\nERROR: /root/download/tensorflowgit/tensorflow/tensorflow/python/BUILD:70                                                            4:1: C++ compilation of rule '//tensorflow/python:tf_session_helper' fail                                                            ed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-                                                            D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parame                                                            ter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ff                                                            unction-sections ... (remaining 51 argument(s) skipped): com.google.devto                                                            ols.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n", "comments": ["Just to make sure: did you run `configure`?\n", "What version of numpy do you have installed in the python interpreter you\nhave used with configure?\nOn Dec 8, 2015 7:26 AM, \"Geoffrey Irving\" notifications@github.com wrote:\n\n> Just to make sure: did you run configure?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/442#issuecomment-162952660\n> .\n", "Comment if you'd still like help with this and we'll re-open.\n", "I am having this issue and I did run configure"]}, {"number": 441, "title": "max_images defaults to 3 images instead of batch_size in tf.image_summary", "body": "Even though the default value for the max_images parameter in tf.image_summary is None, a maximum of 3 images are displayed by default.\n\nIt looks like this is [by design](https://github.com/tensorflow/tensorflow/blob/a5d8217c4ed90041bea2616c14a8ddcf11ec8c03/tensorflow/core/ops/ops.pbtxt#L2993), but I can't find it mentioned in the documentation.  Should a note be added to the docs, or possibly the default value changed to 3 instead of None?\n\n``` python\nimport tensorflow as tf\n\nimages = tf.Variable(tf.truncated_normal([10, 48, 48, 1]))\n\ntf.image_summary(\"max_images_bug_no_param\", images) # 3 images displayed\n\ntf.image_summary(\"max_images_bug_none_param\", images, max_images=None) # 3 images displayed\n\ntf.image_summary(\"max_images_bug_100_param\", images, max_images=100) # 10 images displayed\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    summary_op = tf.merge_all_summaries()\n    writer = tf.train.SummaryWriter('max_images_bug')\n    summary_str = sess.run(summary_op)\n    writer.add_summary(summary_str, 0)\n```\n", "comments": ["Fixed.  You're correct that the default in Python should be `3` instead of `None` so that it shows up in the docs.  Thanks!\n"]}, {"number": 440, "title": "Will TensorFlow run on an AMD FX processor?", "body": "I'm looking at buying a new PC to run TensorFlow on.  The system I'm looking at has an AMD FX processor and a GTX 960 with 2GB of memory.  Will TensorFlow run fine on that processor and be able to take advantage of the GTX 960 for CUDA processing?  The O/S I'm planning on is Ubuntu Linux.\n", "comments": ["It's an x86 processor so it should be fine on CPU, and I believe we support the GTX960 (It's a Maxwell GPU with cuda compute capability 5).\n", "I don't think AMD FX CPU is a problem. But GTX 960 with 2 GB might be an\nissue.\nThanks.\n\nJunli\n\nOn Mon, Dec 7, 2015 at 9:03 PM, roschler notifications@github.com wrote:\n\n> I'm looking at buying a new PC to run TensorFlow on. The system I'm\n> looking at has an AMD FX processor and a GTX 960 with 2GB of memory. Will\n> TensorFlow run fine on that processor and be able to take advantage of the\n> GTX 960 for CUDA processing? The O/S I'm planning on is Ubuntu Linux.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/440.\n\n## \n\n---\n\nJunli Gu--\u8c37\u4fca\u4e3d\nCoordinated Science Lab\nUniversity of Illinois at Urbana-Champaign\n\n---\n", "Yeah, it would be supported by TensorFlow, but is definitely on the low-side compared to most compute GPUs.\n", "Thanks everyone.  Would it be enough of an improvement to get a GTX 960 with 4GB, or is anything less than something like a GTX TitanX going to be underpowered?\n", "@roschler , I would get at least a 980TI, but if I had to do it all over again, I would definitely get the titanx\n", "@LeavesBreathe - Ok, that's what I'll do.  Thanks.\n", "Does GTX 960 work on pip installation or must it be installed from sources? I have a GTX 960 build, but doesn't seem to get recognized by tensorflow. Following \"Build your target with GPU support\", line:\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu, gives following error:\n\n---\n\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_UNKNOWN\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: server01\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: server01\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.63\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.63  Sat Nov  7 21:25:42 PST 2015\nGCC version:  gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.63\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:226] kernel version seems to match DSO: 352.63\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: \nF tensorflow/cc/tutorials/example_trainer.cc:118] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'\n     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=\"/gpu:0\"]()]])\nF tensorflow/cc/tutorials/example_trainer.cc:118] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'\n     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=\"/gpu:0\"]()]])\nF tensorflow/cc/tutorials/example_trainer.cc:118] Check failed: ::tensorflow::Status::OK() == (session->Run({{\"x\", x}}, {\"y:0\", \"y_normalized:0\"}, {}, &outputs)) (OK vs. Invalid argument: Cannot assign a device to node 'Const/_2': Could not satisfy explicit device specification '/gpu:0'\n     [[Node: Const/_2 = Const[dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 0>, _device=\"/gpu:0\"]()]])\nAborted (core dumped)\n\n---\n\nSolved, as it turned out to be permission issue. Run with sudo worked.\n"]}, {"number": 439, "title": "Say that we need gcc >= 4.8 (was \"Need help on compilation error on Mac\")", "body": "Having met the following compilation errors:\n\ntensorflow/core/kernels/fifo_queue.cc:247:46: error: expected body of lambda expression\n          [callback, this](Attempt* attempt) EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n                                                               ^\n\nPlease let me know if you have any ideas. Thanks!\n", "comments": ["Please include the entire error message.\n", "It seems my problem is how to configure things right to use non-system default GCC. I upgraded to gcc47. And now I got the following errors. thx.\n\n| => bazel build -c opt --copt='-fabi-version=6' --linkopt='-lrt'  //tensorflow/tools/pip_package:build_pip_package --verbose_failures\nINFO: Found 1 target...\nINFO: From Compiling tensorflow/core/lib/histogram/histogram.cc:\nIn file included from ./tensorflow/core/lib/gtl/array_slice.h:101:0,\n                 from ./tensorflow/core/lib/histogram/histogram.h:20,\n                 from tensorflow/core/lib/histogram/histogram.cc:16:\n./tensorflow/core/lib/gtl/array_slice_internal.h:231:38: error: 'tensorflow::gtl::array_slice_internal::ArraySliceImplBase<const T>::ArraySliceImplBase' names constructor\n./tensorflow/core/lib/gtl/array_slice_internal.h:251:32: error: 'tensorflow::gtl::array_slice_internal::ArraySliceImplBase<T>::ArraySliceImplBase' names constructor\nIn file included from ./tensorflow/core/lib/histogram/histogram.h:20:0,\n                 from tensorflow/core/lib/histogram/histogram.cc:16:\n./tensorflow/core/lib/gtl/array_slice.h: In instantiation of 'tensorflow::gtl::ArraySlice<T>::ArraySlice(const std::vector<_RealType>&) [with T = double]':\ntensorflow/core/lib/histogram/histogram.cc:48:11:   required from here\n./tensorflow/core/lib/gtl/array_slice.h:132:33: error: no matching function for call to 'tensorflow::gtl::array_slice_internal::ArraySliceImpl<double>::ArraySliceImpl(const double*, std::vector<double>::size_type)'\n./tensorflow/core/lib/gtl/array_slice.h:132:33: note: candidates are:\nIn file included from ./tensorflow/core/lib/gtl/array_slice.h:101:0,\n                 from ./tensorflow/core/lib/histogram/histogram.h:20,\n                 from tensorflow/core/lib/histogram/histogram.cc:16:\n./tensorflow/core/lib/gtl/array_slice_internal.h:243:12: note: template<class C> tensorflow::gtl::array_slice_internal::ArraySliceImpl::ArraySliceImpl(const C&)\n./tensorflow/core/lib/gtl/array_slice_internal.h:243:12: note:   template argument deduction/substitution failed:\nIn file included from ./tensorflow/core/lib/histogram/histogram.h:20:0,\n                 from tensorflow/core/lib/histogram/histogram.cc:16:\n./tensorflow/core/lib/gtl/array_slice.h:132:33: note:   candidate expects 1 argument, 2 provided\nIn file included from ./tensorflow/core/lib/gtl/array_slice.h:101:0,\n                 from ./tensorflow/core/lib/histogram/histogram.h:20,\n                 from tensorflow/core/lib/histogram/histogram.cc:16:\n./tensorflow/core/lib/gtl/array_slice_internal.h:229:7: note: constexpr tensorflow::gtl::array_slice_internal::ArraySliceImpl<double>::ArraySliceImpl(const tensorflow::gtl::array_slice_internal::ArraySliceImpl<double>&)\n./tensorflow/core/lib/gtl/array_slice_internal.h:229:7: note:   candidate expects 1 argument, 2 provided\n./tensorflow/core/lib/gtl/array_slice_internal.h:229:7: note: constexpr tensorflow::gtl::array_slice_internal::ArraySliceImpl<double>::ArraySliceImpl(tensorflow::gtl::array_slice_internal::ArraySliceImpl<double>&&)\n./tensorflow/core/lib/gtl/array_slice_internal.h:229:7: note:   candidate expects 1 argument, 2 provided\n", "@vrv: What's our range of supported compilers?  Should gcc 4.7 be fine?\n", "I'm not sure, 4.8+ is the safe assumption (the earliest we've tried successfully).\n", "We certainly haven't tested with it, so if it doesn't work, that may be a\nsign that it isn't presently supported.\n\nOn Tue, Dec 8, 2015 at 9:39 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> @vrv https://github.com/vrv: What's our range of supported compilers?\n> Should gcc 4.7 be fine?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/439#issuecomment-163116441\n> .\n", "From the error messages I've seen on our Bazel issue tracker, I'd say tensorflow definitely requires at least gcc 4.8 due to certain C++11 features being used.\n", "We should give the compiler range in the README.\n", "Ok, I upgraded to gcc4.8.5, and now I am having issue to compile protobuf:\n\nINFO: From Compiling google/protobuf/src/google/protobuf/map_field.cc [for host]:\nIn file included from /opt/local/include/gcc48/c++/unordered_map:47:0,\n                 from google/protobuf/src/google/protobuf/stubs/hash.h:139,\n                 from google/protobuf/src/google/protobuf/map.h:35,\n                 from google/protobuf/src/google/protobuf/map_field_lite.h:34,\n                 from google/protobuf/src/google/protobuf/map_field.h:40,\n                 from google/protobuf/src/google/protobuf/map_field.cc:31:\n/opt/local/include/gcc48/c++/bits/hashtable.h: In instantiation of 'std::_Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal, _H1, _H2, _Hash, _RehashPolicy, _Traits>::__node_type\\* std::_Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal, _H1, _H2, _Hash, _RehashPolicy, _Traits>::_M_allocate_node(_Args&& ...) [with _Args = {const std::piecewise_construct_t&, std::tuple<const google::protobuf::MapKey&>, std::tuple<>}; _Key = google::protobuf::MapKey; _Value = std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>; _Alloc = google::protobuf::Map<google::protobuf::MapKey, google::protobuf::MapValueRef>::MapAllocator<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_> >; _ExtractKey = std::__detail::_Select1st; _Equal = std::equal_togoogle::protobuf::MapKey; _H1 = google::protobuf::hashgoogle::protobuf::MapKey; _H2 = std::__detail::_Mod_range_hashing; _Hash = std::__detail::_Default_ranged_hash; _RehashPolicy = std::__detail::_Prime_rehash_policy; _Traits = std::__detail::_Hashtable_traits<true, false, true>; std::_Hashtable<_Key, _Value, _Alloc, _ExtractKey, _Equal, _H1, _H2, _Hash, _RehashPolicy, _Traits>::__node_type = std::__detail::_Hash_node<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>, true>]':\n/opt/local/include/gcc48/c++/bits/hashtable_policy.h:493:8:   required from 'std::__detail::_Map_base<_Key, _Pair, _Alloc, std::__detail::_Select1st, _Equal, _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::mapped_type& std::__detail::_Map_base<_Key, _Pair, _Alloc, std::__detail::_Select1st, _Equal, _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::operator[](const key_type&) [with _Key = google::protobuf::MapKey; _Pair = std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>; _Alloc = google::protobuf::Map<google::protobuf::MapKey, google::protobuf::MapValueRef>::MapAllocator<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_> >; _Equal = std::equal_togoogle::protobuf::MapKey; _H1 = google::protobuf::hashgoogle::protobuf::MapKey; _H2 = std::__detail::_Mod_range_hashing; _Hash = std::__detail::_Default_ranged_hash; _RehashPolicy = std::__detail::_Prime_rehash_policy; _Traits = std::__detail::_Hashtable_traits<true, false, true>; std::__detail::_Map_base<_Key, _Pair, _Alloc, std::__detail::_Select1st, _Equal, _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::mapped_type = google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_; std::__detail::_Map_base<_Key, _Pair, _Alloc, std::__detail::_Select1st, _Equal, _H1, _H2, _Hash, _RehashPolicy, _Traits, true>::key_type = google::protobuf::MapKey]'\n/opt/local/include/gcc48/c++/bits/unordered_map.h:596:20:   required from 'std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::mapped_type& std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::operator[](const key_type&) [with _Key = google::protobuf::MapKey; _Tp = google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_; _Hash = google::protobuf::hashgoogle::protobuf::MapKey; _Pred = std::equal_togoogle::protobuf::MapKey; _Alloc = google::protobuf::Map<google::protobuf::MapKey, google::protobuf::MapValueRef>::MapAllocator<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_> >; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::mapped_type = google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_; std::unordered_map<_Key, _Tp, _Hash, _Pred, _Alloc>::key_type = google::protobuf::MapKey]'\ngoogle/protobuf/src/google/protobuf/map.h:674:36:   required from 'T& google::protobuf::Map<Key, T>::operator[](const key_type&) [with Key = google::protobuf::MapKey; T = google::protobuf::MapValueRef; google::protobuf::Map<Key, T>::key_type = google::protobuf::MapKey]'\ngoogle/protobuf/src/google/protobuf/map_field.cc:185:49:   required from here\n/opt/local/include/gcc48/c++/bits/hashtable.h:727:6: error: no matching function for call to 'google::protobuf::Map<google::protobuf::MapKey, google::protobuf::MapValueRef>::MapAllocator<std::__detail::_Hash_node<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>, true> >::construct(std::_Hashtable<google::protobuf::MapKey, std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>, google::protobuf::Map<google::protobuf::MapKey, google::protobuf::MapValueRef>::MapAllocator<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_> >, std::__detail::_Select1st, std::equal_togoogle::protobuf::MapKey, google::protobuf::hashgoogle::protobuf::MapKey, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::__node_type_&, const std::piecewise_construct_t&, std::tuple<const google::protobuf::MapKey&>, std::tuple<>)'\n      _M_node_allocator().construct(__n, std::forward<_Args>(__args)...);\n      ^\n/opt/local/include/gcc48/c++/bits/hashtable.h:727:6: note: candidate is:\nIn file included from google/protobuf/src/google/protobuf/map_field_lite.h:34:0,\n                 from google/protobuf/src/google/protobuf/map_field.h:40,\n                 from google/protobuf/src/google/protobuf/map_field.cc:31:\ngoogle/protobuf/src/google/protobuf/map.h:562:10: note: void google::protobuf::Map<Key, T>::MapAllocator<U>::construct(google::protobuf::Map<Key, T>::MapAllocator<U>::pointer, google::protobuf::Map<Key, T>::MapAllocator<U>::const_reference) [with U = std::__detail::_Hash_node<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>, true>; Key = google::protobuf::MapKey; T = google::protobuf::MapValueRef; google::protobuf::Map<Key, T>::MapAllocator<U>::pointer = std::__detail::_Hash_node<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>, true>_; google::protobuf::Map<Key, T>::MapAllocator<U>::value_type = std::__detail::_Hash_node<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>, true>; google::protobuf::Map<Key, T>::MapAllocator<U>::const_reference = const std::__detail::_Hash_node<std::pair<const google::protobuf::MapKey, google::protobuf::MapPair<google::protobuf::MapKey, google::protobuf::MapValueRef>_>, true>&]\n     void construct(pointer p, const_reference t) { new (p) value_type(t); }\n          ^\ngoogle/protobuf/src/google/protobuf/map.h:562:10: note:   candidate expects 2 arguments, 4 provided\nERROR: /Users/chaotan/Workspace/deep_learning/tensorflow/tensorflow1/google/protobuf/BUILD:64:1: C++ compilation of rule '//google/protobuf:protobuf' failed: osx_gcc_wrapper.sh failed: error executing command \n  (cd /private/var/tmp/_bazel_chaotan/58945d877128dff569ec8b54b7c14ab6/tensorflow1 && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/opt/local/bin:/usr/local/git/bin:/sw/bin/:/usr/local/bin:/usr/local/:/usr/local/sbin:/usr/local/mysql/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/texbin:/usr/local/bin/ \\\n    TMPDIR=/var/folders/4r/gpk5b_zn30qf4f5z7xc33k0w002l2g/T/ \\\n  tools/cpp/osx_gcc_wrapper.sh '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++0x' -iquote . -iquote bazel-out/host/genfiles -isystem google/protobuf/src -isystem bazel-out/host/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/map_field.o' -MD -MF bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/map_field.d -c google/protobuf/src/google/protobuf/map_field.cc -o bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/map_field.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: osx_gcc_wrapper.sh failed: error executing command \n  (cd /private/var/tmp/_bazel_chaotan/58945d877128dff569ec8b54b7c14ab6/tensorflow1 && \\\n  exec env - \\\n    INTERCEPT_LOCALLY_EXECUTABLE=1 \\\n    PATH=/opt/local/bin:/usr/local/git/bin:/sw/bin/:/usr/local/bin:/usr/local/:/usr/local/sbin:/usr/local/mysql/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/texbin:/usr/local/bin/ \\\n    TMPDIR=/var/folders/4r/gpk5b_zn30qf4f5z7xc33k0w002l2g/T/ \\\n  tools/cpp/osx_gcc_wrapper.sh '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++0x' -iquote . -iquote bazel-out/host/genfiles -isystem google/protobuf/src -isystem bazel-out/host/genfiles/google/protobuf/src -isystem tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare '-Wno-error=unused-function' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/map_field.o' -MD -MF bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/map_field.d -c google/protobuf/src/google/protobuf/map_field.cc -o bazel-out/host/bin/google/protobuf/_objs/protobuf/google/protobuf/src/google/protobuf/map_field.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 3.012s, Critical Path: 2.06s\n", "Wow, that's fairly unpleasant.  Unfortunate I have no idea what's going wrong, and it seems to be fairly deep inside protobuf.  Would you mind filing a bug over at https://github.com/google/protobuf?  This seems like an issue on their end (apologies for the side stepping).\n", "Will do. I am mainly referring to g3doc/get_started/os_setup.md to set up my stuff. It will be great things can be made more clear in terms of the version dependencies of all the required components. Especially since a section is dedicated to MacOS, I suppose someone is actively working on this platform. :)\n\nThanks!\n", "(You're definitely helping us to figure out what those dependencies are, so thanks :)\n", "Incidentally, if you want to get unblocked, you might try clang.  I think we haven't found all these issues yet because most people on OS X compile with clang.  Though I suppose I'm not sure what bazel uses by default.\n", "How do you use clang to compile? thx.\n", "https://groups.google.com/forum/#!topic/bazel-discuss/m2y2ZEB7zJE seems like a helpful thread on this topic\n", "I solve this question simply and roughly.EXCLUSIVE_LOCKS_REQUIRED(mu_)  is used about GPU. Because Mac doesn't support GPU, we can delete EXCLUSIVE_LOCKS_REQUIRED(mu_). so correspond syntax of lambda and compile successfully.\n", "I encountered the same error \n\n```\nexpected body of lambda expression\n          [tuple, this](Attempt* attempt) EXCLUSIVE_LOCKS_REQUIRED(mu_)\n```\n\nin file `tensorflow/tensorflow/core/kernels/fifo_queue.cc`.\nMy build host is\n\n```\nclang++ -v\nApple LLVM version 5.1 (clang-503.0.40) (based on LLVM 3.4svn)\nTarget: x86_64-apple-darwin13.2.0\n```\n\nThough I am using cmake instead of bazel, I think it probably uses the default clang compiler.\nThus I think this issue is not related to compiler, but compiler features flags. The compiler seems to detect that is a lambda expression, but do not expect that macro, thus the error?\n", "@martinwicke Do you want to triage this further, or should we just close?\n", "We can close this. It's hard to see what we can do about it. It may actually be fixed now that we made the GPU code work on Mac.\n"]}, {"number": 438, "title": "build error - target names may not contain ' '.", "body": "bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\nERROR: /tensorflow/BUILD:27:1: //tensorflow:all_files: invalid label 'TensorFlow-Examples/examples/1 - Introduction/basic_operations.py' in element 4 of attribute 'srcs' in 'filegroup' rule: invalid target name 'TensorFlow-Examples/examples/1 - Introduction/basic_operations.py': target names may not contain ' '.\n", "comments": ["Hm, I can't find basic_operations.py in our repo. I suspect you might have some local changes, possibly intermingled from https://github.com/aymericdamien/TensorFlow-Examples\n", "thanks! rm -rf TensorFlow-Examples solved the issue.\n"]}, {"number": 437, "title": "Update MNIST Beginner tutorial", "body": "The MNIST data is actually split into 3 parts: train, test, and validation. Validation was missing from the description.\n", "comments": ["Hi Jeff, we sadly don't accept requests through github, only gerrit: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\n\nIf you want, I can integrate these changes myself, or feel free to make the request through gerrit.  Let me know!\n", "Accepted via gerrit!\n"]}, {"number": 436, "title": "Need better error message for error in checkpoint loading", "body": "Hi, I'm loading a previously-built model using the tf.train.Saver and running into an error: `tensorflow.python.pywrap_tensorflow.StatusNotOK: Internal: Unable to get element from the feed.`.\n\nI'm not quite sure what this is referring to as I can load other models just fine. This one also builds and runs as expected. It's only after, when retrieving it again, do I get this error. The ckpt file also points to the correct locations and is reading from the right place.\n\n```\nTraceback (most recent call last):\n  File \"bridge.py\", line 518, in <module>\n    tf.app.run()\n  ...\n  File \"bridge.py\", line 375, in train\n    saver.restore(sess, checkpoint)\n  File \".../site-packages/tensorflow/python/training/saver.py\", line 887, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \".../site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \".../site-packages/tensorflow/python/client/session.py\", line 446, in _do_run\n    six.reraise(e_type, e_value, e_traceback)\n  File \".../site-packages/tensorflow/python/client/session.py\", line 428, in _do_run\n    target_list)\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Internal: Unable to get element from the feed.\n```\n\nThoughts?\n", "comments": ["That's a terribly uninformative error message which should be fixed, but until then: can I see the whole `bridge.py` or at least more of the context?\n", "Sorry about that! I posted more on what the bridge creates below. This version is just a linear mapping using cosine similarity. \n\n```\nclass Bridge(object):\n    def __init__(self, state_size, bucket_sizes, bridge_learning_rate, \n                        bridge_learning_decay_factor, bridge_regularizer_factor):\n        bridge_learning_rate = tf.get_variable(\n            'bridge_learning_rate', shape=[], trainable=False,\n            initializer=tf.constant_initializer(float(bridge_learning_rate)))   \n        bridge_learning_decay_factor_op = self.bridge_learning_rate.assign(\n            self.bridge_learning_rate * bridge_learning_decay_factor)\n        bridge_global_step = tf.get_variable(\n            'bridge_global_step', shape=[], trainable=False,\n            initializer=tf.constant_initializer(0))\n\n        initializer = tf.truncated_normal_initializer\n\n        self.s1 = tf.placeholder(\"float\", [None, state_size])\n        self.s2 = tf.placeholder(\"float\", [None, state_size])\n\n        # W, b are the weight / bias matrices for the states.                                                                                                                                               \n        # shape: [state_size x state_size], [state_size]                                                                                                                                                    \n        W = tf.get_variable(\n            'weights', [state_size, state_size],\n            initializer=initializer())\n        b = tf.get_variable(\n            'bias', [state_size],\n            initializer=initializer())\n\n        self.y = tf.nn.xw_plus_b(self.s1, W, b)\n\n        cost = self._cosine_cost(self.s2, self.y)\n        reg = tf.reduce_sum(tf.square(W)) * bridge_regularizer_factor\n        self.cost = tf.add(cost, reg)\n\n        self.train_step = tf.train.AdagradOptimizer(\n            bridge_learning_rate\n            ).minimize(self.cost, global_step=bridge_global_step)\n\n        tf.initialize_all_variables().run()\n\n    @staticmethod\n    def _cosine_cost(y, y_):\n        def norm(v):\n            return tf.sqrt(tf.reduce_sum(tf.square(v), 1, keep_dims=True))\n\n        y = y / norm(y)\n        y_ = y_ / norm(y_)\n        return -tf.reduce_mean(tf.matmul(y, y_, transpose_b=True))\n```\n\nAfter creating the bridge, I then looked at `tf.all_variables()` and this was the result: \n\n```\n[u'bridge/bridge_learning_rate:0', u'bridge/bridge_global_step:0', \n u'bridge/weights:0', u'bridge/bias:0', \n u'bridge/bridge/weights/Adagrad:0', u'bridge/bridge/bias/Adagrad:0']\n```\n", "It looks like you didn't include the code for the line that triggers the error, so I don't know what the problem is.  Wild, unlikely guess: Do you have Python 2 with `unicode_literals` imported from `__future__`?  I'm a bit surprised those variable names are showing up as unicode.\n", "Ahh, this is my screwup. For note to others, this error happens when you try to restore using the object `ckpt = tf.train.get_checkpoint_state(model_dir)` and not the ckpt.model_checkpoint_path. Thanks so much for looking at this girving! My apologies for not being more careful :(\n", "I think I've made that mistake too, so it would be good to get a more sensible error message.  I'll try to do that, though it not be this year.\n", ":+1: \n", "Apologies: looks like this one fell through the cracks.  Removing assignment for purposes of upcoming bug scrub.\n", "@girving This doesn't appear to be assigned to anybody on GitHub.  Any update here?  \n", "@prb12 @aselle I was expecting this to be triaged during the bug scrub, but I guess that didn't happen.  I'll close for now, but if someone wants to fix it I can reopen and mark contributions welcome.\n", "+1 on this. The error also arose if checkpoint_path=None. The error message should warn the ckpt file may not exist. \r\nAlso it may be useful to know that a missing protobuf in the checkpoint_path directory gives  checkpoint_path=None while an error message would be more useful. HTH.", "assigned to girving mostly because he has been following this bug. Feel free to re-assign. Thanks.", "@jmchen-g Unassigning myself a second time.  I'll mark this contributions welcome.", "@girving this seems like a low-effort and beginner-friendly task. If so, I'd like to get started with this. \ud83d\ude42 ", "@martinwicke: Handing off to you. ", "@sahildua2305 Please go ahead! Send a PR whenever you're ready, and mention this issue in its description.", "@sahildua2305 Just wanted to follow up. Did you get a chance to build a PR for this issue?.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 435, "title": "Cross products?", "body": "Can we get support for cross products? This is a fundamental / elementary mathematical operation and I suspect it can be more efficiently implemented in C++, for example as in [this example](http://fastcpp.blogspot.com/2011/04/vector-cross-product-using-sse-code.html). A function that would take two tensors of 3D vectors and return the cross products of the 3D vectors would be nice. This would be done element wise, so for two tensors x and y, the returned tensor would be [x1 x y1, x2 x y2, ...].\n", "comments": ["Thanks, that's a good feature request.  Not sure if it'll fly, but in past computational geometry work I also like defining cross product for 2D by 2D to 1D, 1D by 2D to 1D, and 2D by 1D to 1D (treating 1D as vertical).\n", "I also want an operator that makes a vector v into the matrix V for which\nVw = v x w. Just for completeness, and we'll likely use it quickly if we\never compute gradients rotations.\n\nOn Mon, Dec 7, 2015 at 1:13 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> Thanks, that's a good feature request. Not sure if it'll fly, but in past\n> computational geometry work I also like defining cross product for 2D by 2D\n> to 1D, 1D by 2D to 1D, and 2D by 1D to 1D (treating 1D as vertical).\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/435#issuecomment-162664934\n> .\n", "The cross product of two vector W x V  can be implemented as a matrix multiplication using skew-symmetric matrix. \n\nhttps://en.wikipedia.org/wiki/Cross_product#Conversion_to_matrix_multiplication\n\nwhich can also be generalized to higher dimensions (not only 3D) \n", "Yes, and we should have the star operator that converts a vector to that matrix as well, but we should still have the normal cross product operator for efficiency.\n", "Any updates on this? Is the (core) functionality coming anytime soon or no?\n", "I don't think anyone is working on it internally, but patches are welcome if you want to give it a try!  If anyone does so, it's fine to ignore my 2-D desires in the first pass; they have somewhat ambiguous shape semantics anyways.\n", "Do you have a sense of the likely performance improvement if I were to implement it as a native op in C++ vs. just python TF code? I already have a python implementation that's very simple, basically just this:\n\n```\ndef cross(u, v, name=None):\n    with tf.op_scope([u, v], name, 'cross') as scope:\n        u = tf.convert_to_tensor(u, name='u')\n        v = tf.convert_to_tensor(v, name='v')\n\n        u1, u2, u3 = tf.split(1, 3, u)\n        v1, v2, v3 = tf.split(1, 3, v)\n\n        return tf.concat(1, [(u2 * v3) - (u3 * v2),\n                             (u3 * v1) - (u1 * v3),\n                             (u1 * v2) - (u2 * v1)], name=scope)\n```\n\nAnd I'm wondering if it would be worth the extra investment. A speed up of 30% wouldn't be for me, but something like >3x would. Interestingly the above implementation is about 5x faster on CPUs than GPUs (Titan X).\n", "I'd expect well over 3x faster if native, but that's > 3x for just this op, not necessarily for your whole graph.\n", "Fixed in 3c13ae058ea45d855d8029b3d19f6567b86430b5.\n", "Thanks!\n", "Great, thank you!\n", "I benchmarked the new `tf.cross` function against my own implementation written in python TensorFlow (see my earlier comment on Jan 12). The performance is actually worse on the CPU, which is rather surprising. Using this snippet of code:\n\n```\nNUM_DIMENSIONS = 3\nBATCH_SIZE = 128\nNUM_STEPS = 900\n\nnpr.seed(1)\nmats = npr.rand(NUM_STEPS, BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\nbase = npr.rand(BATCH_SIZE, NUM_DIMENSIONS).astype('float32')\n\ncrosses = [base]\nfor mat in mats:\n    with tf.device('/cpu:0'):\n        new_cross = tf.cross(mat, crosses[-1])\n    crosses.append(new_cross)\n```\n\nAnd evaluating the last element of `crosses`, i.e. `crosses[-1].eval()`, takes about 0.1 seconds on a Xeon E5-2643 v3 with 4 threads enabled. In comparison my own implementation runs in 0.06 seconds despite being written in high-level TF. This is benchmarked on multiple runs and averaged, with very small standard error.\n\nOn the GPU (Titan X) the situation is different. The new `tf.cross` runs in 0.025 seconds, while my implementation runs in 0.22 seconds.\n\nThe poor CPU performance seems surprising. Any thoughts on why and if it may be improved?\n", "Just a guess: I would believe your version can parallelize since all the ops are independent. What happens if you run on only one thread?\n", "Hmmm I'm getting some very strange results. So just to be sure, the way I'm controlling threads is by invoking a session this way:\n\n```\nnum_threads = 4\nsess = tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=num_threads,\n                                                   intra_op_parallelism_threads=num_threads,\n                                                   log_device_placement=False,\n                                                   allow_soft_placement=False))\n```\n\nStrangely, when I set `num_threads` to 1, _both_ implementations speed up, contrary to your expectation. My implementation goes from 0.06 secs to 0.03 secs, and `tf.cross` goes from 0.1 secs to 0.05 secs. Not sure why this is happening. The tests are very minimal, just basically what I wrote earlier.\n", "There's a fair amount of per-op overhead, so for small data sets, you could be seeing contention on the thread pool. You could try going the other way, of course; splitting the tf.cross input in four, one for each CPU, and seeing if it matches your expectations.\n\nThe cross op right now has the unfortunate effect of starting three CUDA kernels (since it does three operator= statement on device); it should really have been one for better GPU efficiency, but I didn't think of it at the time.\n", "But that still doesn't explain why my TF-based implementation gets 0.03 secs on a single CPU thread, while `tf.cross` gets 0.05 secs, right? The numbers are very repeatable so the difference is real, and is nearly 2x. Why would that be the case?\n", "It's hard to say off-hand; you'd need to look at a profile for what takes up the CPU time.\n", "Hi, is there any update on having a Skew-symmetric operation? I've done a naive implementation in python by it's the bottle-neck of my model atm. "]}, {"number": 434, "title": "Identity matrix initialization (was Custom Initialization of Weights)", "body": "Hey TF,\n\nIn your seq2seq library you define your weights W, by setting your initializer to none:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L675\n\nI'm trying to build Identity RNN's and Unitary RNN's in tensorflow. For identity rnn's they need to be initialized by a identity matrix. [Numpy easily makes this](http://docs.scipy.org/doc/numpy/reference/generated/numpy.identity.html)\n\nHow can this be done using `get_variable`? Is there way you can custom build your own matrix and then use `get_variable` and its initializer?\n\nAlso, when you set `initializer = None` in `get_variable` what is created? A matrix full of zeros? Thanks alot!  \n", "comments": ["This is a better question for stackoverflow, but the answer is easy, so: `get_variable` takes an `initializer` argument which you can set to `constant_initializer` with any numpy array.  You can also easily build your own initializers if you want to build the initializer with tensorflow ops; look at the source for `constant_initializer` to see how.\n", "Thanks girving, I apologize for asking this in the wrong place. I will look at the source code for constant_initializer. If I get it working, I'll post here with the solution for identity matrix. \n", "try this:\n\n``` python\ndef identity_initializer():\n    def _initializer(shape, dtype=tf.float32):\n        if len(shape) == 1:\n            return tf.constant_op.constant(0., dtype=dtype, shape=shape)\n        elif len(shape) == 2 and shape[0] == shape[1]:\n            return tf.constant_op.constant(np.identity(shape[0], dtype))\n        elif len(shape) == 4 and shape[2] == shape[3]:\n            array = np.zeros(shape, dtype=float)\n            cx, cy = shape[0]/2, shape[1]/2\n            for i in range(shape[2]):\n                array[cx, cy, i, i] = 1\n            return tf.constant_op.constant(array, dtype=dtype)\n        else:\n            raise\n    return _initializer\n```\n", "Actually, reopening and saying enhancement, since this is clearly something common enough to want in the code.\n", "@alexatknit: That looks pretty good, though before we check it in we'll probably want to split it into two functions: one op the does the work and one `identity_initializer` that uses it.\n", "@alexatknit thanks for the help! Works great. I'll be working on an intializer for complex numbers for the unitary RNN's . Again I appreciate both of your help. \n", "drive-by: probably would be easy to do using np.diag()  (or tf.diag(), its equivalent).\n", "I think get_variable can now take a tf.Tensor as an initializer, which is probably sufficient if that tensor is a tf.constant set by a numpy array.  Closing due to inactivity, though we could re-open if this was insufficient.\n", "Since `tf.orthogonal_initializer` exists, I'd expect `tf.identity_initializer` to exist as well. Could this be reopened?", "You can do identity initializer as follows\r\n```\r\na = tf.Variable(tf.eye(3))\r\nsess.run(a.initializer)\r\nprint(sess.run(a))\r\n```", "@yaroslavvb, thanks! \r\n\r\nMatter of taste, I guess, but it's cleaner to keep with the `tf.get_variable` / `tf.variable_initializer` API instead of mixing in `tf.Variable`, especially for variable scoping and sharing variables. \r\n\r\nPassing a `tf.constant` to `tf.get_variable`, or making a custom initializer just for this, isn't very clean IMO. E.g. I like:\r\n\r\n```py\r\nwith tf.variable_scope('my_scope', initializer=tf.identity_initializer()):\r\n  x = tf.get_variable(...)\r\n  y = tf.get_variable(...)\r\n  # ...and so on...\r\n```\r\n", "OK, that seems like a case that's not covered. Not sure what's the state of initializers are (if they are going to get deprecated at some point), cc  @fchollet for opinion on having identity_initializer", "@carlthome So given that `glorot_uniform_initializer` was just added to core, I think if you wrote `identity_initializer` that used `tf.ones`, that would probably get accepted", "Neat. :+1: \r\n\r\nSidebar: I see some merit of deprecating `tf.get_variable` actually. Python already has classes, so much of it is redundant. It's also a bit weird to see neural network specific stuff like Glorot initialization in TensorFlow's top package instead of in `tf.nn`.", "@yaroslavvb I think we do need an `Identity` initializer. Btw, note that we are currently refactoring initializers to be classes instead of functions that return functions. Let's add the new initializers afterwards.", "I would like to take up this issue, should I start working on a PR?", "I think the identity initializer has been added in commit (https://github.com/tensorflow/tensorflow/commit/d5f1790530db0ee430d223497a59cfc26ab5f4d8) recently. Maybe this issue could be closed?", "Good idea, fixed by https://github.com/tensorflow/tensorflow/commit/d5f1790530db0ee430d223497a59cfc26ab5f4d8"]}, {"number": 433, "title": "display bug in mnist_softmax.py?", "body": "Hi,\n\nI ran the mnist_softmax.py example and get an test accuracy of 0.91 or so.\n\nBut when I tried to looked at the values of W and b:\n`>>>  W.eval()`\n\n```\narray([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)\n```\n\n`>>> b.eval()`\n\n```\nOut[6]: \narray([-0.53140152,  0.38159987,  0.19011307, -0.29761535, -0.02665209,\n        1.94714248, -0.21013401,  0.93898344, -1.9967736 , -0.39526111], dtype=float32)\n```\n\nThe value of W is still zero, not chagned? If this is the case, then why the prediction accuracy is 0.91.\n", "comments": ["@colah: Do you have any idea what might be going on?\n", "Actually: can you show exactly when you did those print statements?  Did you import `mnist_softmax` or modify it in place?\n", "I simply add two lines after `mnist_softmax.py` and run `python mnist_softmax.py`:\n\n```\nprint(W.eval())\nprint(b.eval())\n```\n", "here is all the output:\n\n```\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\n0.9199\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n[-0.58581531  0.39544231  0.24995884 -0.31424311 -0.05476126  1.98324513\n -0.20759612  0.97400361 -2.06627345 -0.37395671]\n```\n", "In fact, the contents of `W` are **not** all zeroes, but the first three and last three rows always are. (Presumably, this is because the top left and bottom right corners of the MNIST data are always zero, and `W` is initialized to zero, so those units are never activated.)\n\nPrinting individual rows shows that the first 12 units are set to zero, but subsequent rows have values:\n\n```\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n[ -7.28181760e-07  -3.82109283e-06  -6.77165575e-04  -1.74198867e-05\n  -1.33392937e-03  -3.06003931e-05   2.73057609e-03  -3.93098890e-06\n  -9.00173545e-05  -5.72962977e-04]\n[ -2.09516406e-06  -8.40815756e-06  -1.45535904e-03  -3.82293329e-05\n  -2.93175876e-03  -6.70131849e-05   5.96566778e-03  -9.00283248e-06\n  -1.97683170e-04  -1.25611678e-03]\n[ -5.57180044e-07  -4.59386023e-08   3.04910136e-05  -9.54624539e-08\n  -1.21446392e-05  -9.88030724e-09  -1.48593099e-05  -4.39917812e-07\n  -6.41222869e-07  -1.69754321e-06]\n...\n```\n", "oh, right... didn't look carefully...\n"]}, {"number": 432, "title": "Fix array padding typo in docs", "body": "", "comments": ["For now, we accept changes only via Gerrit, see: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#contributing-code\n"]}, {"number": 431, "title": "No module named ptb", "body": "When running the ptb example/model/tutorial:\nhttps://www.tensorflow.org/versions/master/tutorials/recurrent/index.html\n`~/ai/tensorflow/tensorflow/models/rnn/ptb$ python ptb_word_lm.py`\nTraceback (most recent call last):\n  File \"./ptb_word_lm.py\", line 72, in <module>\n    from tensorflow.models.rnn.ptb import reader\nImportError: No module named ptb\n\nTrying to fix it via\n export PYTHONPATH=$PYTHONPATH:/Users/me/ai/tensorflow\nyields the other common error:\nImportError: No module named core.framework.graph_pb2\n", "comments": ["Update: \n\nFound the solution in the code, line 50 ff\n\nTo compile on CPU:\n  bazel build -c opt tensorflow/models/rnn/ptb:ptb_word_lm\n  bazel build -c opt //tensorflow/models/rnn/ptb:ptb_word_lm\nTo compile on GPU:\n  bazel build -c opt tensorflow --config=cuda \\\n    tensorflow/models/rnn/ptb:ptb_word_lm\nTo run:\n  ./bazel-bin/.../ptb_word_lm --data_path=/tmp/simple-examples/data/\n", "We're fixing this in the next release so you can just type 'python ptb_word_lm.py', but thanks for the report.\n", "Is there a way to fix this without using Bazel?\n", "The pip install for 0.6.0 should have fixed this\n", "You still need to update the website tutorials to match this fix: https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html#download-and-setup\n(0.5.0 -> 0.6.0)\nand \nhttps://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html#run-the-code\n(remove hint about bazel build)\nI didn't realize I was using version 0.5.0 until I came across this issue\n", "Updating the website now.\n\nOn Wed, Feb 10, 2016 at 12:18 PM Amr Abed notifications@github.com wrote:\n\n> You still need to update the website tutorials to match this fix:\n> https://www.tensorflow.org/versions/0.6.0/get_started/os_setup.html#download-and-setup\n> (0.5.0 -> 0.6.0)\n> and\n> \n> https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html#run-the-code\n> (remove hint about bazel build)\n> I didn't realize I was using version 0.5.0 until I came across this issue\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/431#issuecomment-182563896\n> .\n"]}, {"number": 430, "title": "scalar_summary does not support int32 datatypes", "body": "```\nimport tensorflow as tf\nx=tf.constant(2)\ntf.scalar_summary(\"x\",x)\nTypeError: DataType int32 for attr 'T' not in list of allowed values: float32, float64\n```\n", "comments": ["Thanks, this is a good feature request.\n", "In review.\n", "I fixed this ages ago, but for some reason the commit message does show up in the history.\n"]}, {"number": 429, "title": "Bazel error on six_archive when compiling from source", "body": "Hello,\n\nI am having issues building tensorflow from source. I have a local checkout and the required packages installed, following the manual from the tensorflow.org getting-started pages. However, when trying to use bazel to compile the pip-package I get the following error. Can anyone help me to solve this problem? Thanks in advance!\n\n**Command:** bazel build -c opt //tensorflow/tools/pip_package:build_pip_package -s --verbose_failures\n\n**ERROR:**  ~/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/external/six_archive/BUILD:1:1: Executing genrule @six_archive//:copy_six failed: bash failed: error executing command \n  (cd ~/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/bin:/usr/bin:/bin:/usr/X11R6/bin::/usr/lib64/java/bin:/usr/lib64/java/jre/bin:/usr/lib64/java/bin:/usr/lib64/kde4/libexec:/usr/lib64/qt/bin:.:/usr/local/tmake/bin:/home/alexander/bin:/home/alexander/development/alexander/bazel/ \\\n    TMPDIR=/home/alexander/development/alexander/.tmp \\\n  /bin/bash -c 'source tools/genrule/genrule-setup.sh; cp external/six_archive/six-1.10.0/six.py bazel-out/local_linux-opt/genfiles/external/six_archive/six.py'): bash failed: error executing command \n", "comments": ["@craigcitro: Do you know what's happening me?  My knowledge of pip is scant.\n", "I'd guess that the download failed for some reason, and we're just getting a bad error from it.\n\n@severun  -- Can you run again with `bazel build -s` to see the full command-line and output?\n", "@craigcitro -- I've run the build again using the following command: \"bazel build -s  --verbose_failures //tensorflow/tools/pip_package:build_pip_package\"\n\nAnd this is a part of the output that I get, containing the error. I hope this is enough output?\nhttp://pastebin.com/ihL6gm2S\n", "@severun well, it confirms that the failing step is indeed something about copying the file out of the downloaded six archive.\n\ni'll admit that i'm shooting in the dark a bit here; two things to try:\n- could you try running again with `-j 1` and see if that fixes things? (it'll be _much_ slower, since it forces bazel to do things serially, but we've seen lots of nondeterminism issues.)\n- similarly, can you run `blaze build -s @six_archive//:copy_six` to confirm it works by itself?\n", "Interesting, when I build without the <code>-j 1</code> I get a random error, either from <code>six_archive</code> or from <code>//google/protobuf:python_srcs_genrule</code>.\n\nI've run the build again using <code>-j 1</code> and this now consistently shows the following error:\n<code>ERROR: /local.mnt/vol1/development/alexander/tensorflow/google/protobuf/BUILD:497:1: Executing genrule //google/protobuf:python_srcs_genrule failed: bash failed: error executing command </code>\n\nWhen I try to build only this package by itself, using: \n<code>bazel build -s -j 1 --verbose_failures //google/protobuf:python_srcs_genrule</code>\nI get the following output:\n\nhttp://pastebin.com/GSZNpiRW\n", "So this one is also an error just trying to copy files; two questions:\n- any chance you're doing this on a partition that's nearly full?\n- when you cloned the github repo, did you do `--recursive` (which also clones protobuf, the one git submodule we have)?\n", "The command that I used to clone the github repo:\n<code>git clone --recurse-submodules https://github.com/tensorflow/tensorflow</code>\n\nOnly our /tmp folder is almost full, so I have a <code>export TMPDIR=....</code> to an alternative directory to use as temporary folder. Could this be the problem? I noticed a couple of 'source' commands in the bazel output, so maybe this tmpdir variable is overwritten?\n", "`--recursive` and `--recurse-submodules` are the same, so no worries there.\n\nIt looks like bazel is using `TMPDIR=/home/alexander/development/alexander/.tmp`; can you do `df -h /home/alexander/development/alexander/.tmp` and see if it's short on space?\n", "df shows there is 270G still available, so I doubt that to be the problem.\n", "OK, so all my easy hypotheses are no good.\n\nSummoning @lberki, knower of all things bazel -- Lukas, how should we diagnose what's happening here? `bazel build` is failing at a copy step, even with `-j 1`.\n", "@craigcitro Thanks for the effort :)\n", "What happens if you try to run this command:\n\n`cp external/six_archive/six-1.10.0/six.py bazel-out/local_linux-fastbuild/genfiles/external/six_archive/six.py` \n\nin `<root>/bazel-tensorflow`, or, equivalently, in `/local.mnt/vol1/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow`? My suspicion is that `cp` silently fails for some reason. If that works, try\n\n`/bin/bash -c 'source tools/genrule/genrule-setup.sh; cp external/six_archive/six-1.10.0/six.py bazel-out/local_linux-fastbuild/genfiles/external/six_archive/six.py'` in the same directory.\n", "Both commands work fine, <code>six.py</code> is successfully copied.\n", "@craigcitro : Tensorflow uses Bazel 0.1.1, right? \n@severun : Does this work if you add the `--spawn_strategy=standalone --genrule_strategy=standalone` command line options to the Bazel command line?\n", "@severun : do you also have ample disk space under `/local.mnt/vol1/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow`?\n", "@lberki for the docker containers, we use 0.1.1. @severun -- what version of bazel are you using locally?\n", "(and install instructions suggest 0.1.1.)\n", "When I run <code>bazel help</code> it outputs: [bazel release 0.1.1], which seems to be the correct version required for tensorflow.\n\nWhen running <code>bazel build -s -j 1 --verbose_failures --spawn_strategy=standalone --genrule_strategy=standalone //tensorflow/tools/pip_package:build_pip_package</code> it outputs the following error: http://pastebin.com/AKC7UADW\n", "How about just with `--genrule_strategy=standalone` then? Also, do you have ample disk space under `/local.mnt/vol1/development/alexander/.cache/bazel/_bazel_alexande/44250d582377ce08fbe503824f986778/tensorflow`?\n", "Yes I have enough disk-space, still 270gb free space left. Also running the bazel command with only <code>--genrule_strategy=standalone</code> does not help, same error.\n\nHowever, I noticed that the generated commands include <code>/usr/bin/gcc</code> to compile the code, and manually running this command failes. However, if I use <code>/usr/bin/g++</code> the code does compile, without any errors w.r.t. the symbols.\n", "Okay, I'm shooting in the dark now. Since the `cp` genrule works when run in the execroot manually, my next guess is something sandbox-related. Could you try to run the build with the `--sandbox_debug` command line option? For best results, do a build without the option, then a build with it so that the only rule that's run with debugging is the offending genrule. Otherwise you'd get quite a lot of log spam on the console.\n", "Ok I reran the build using the <code>--sanbox_debug</code> parameter, which results in the following output: http://pastebin.com/TSCrrHWF\n", "I think I found the Issue. When i run the command <code>exec env -</code> I am disconnected from the server. This might be some security feature installed on the server I guess?\nWhen I manully repeat the commands from the debug output, without the <code>exec</code> command it works fine...\n", "The `exec env -` part is not actually executed by Blaze, it's just (IIRC) so that it's easy to achieve similar behavior when you copy-paste the command into your terminal.\n\nIt's getting late here, so I'll continue tomorrow. Adding @philwo and @ulfjack in case they have a clever idea.\n", "What's \"disconnected from the server\"? You mean as in, logged out from your shell? It's expected, since `exec` replaces your shell with `env`, which then promptly exits)\n", "I connect to the server via ssh, and after running <code>exec env -</code> I am disconnected from the ssh session. I am no linux guru so I am not much of a help. Just close this 'issue' if its taking too much time. I'm sure you guys have more interesting things to work on :)\n", "Hehe :) The reason why your SSH connection closes is simple, but also rather surprising (now I wonder why Bazel even prints \"exec env\" - this is never something you'd want to paste into your shell):\n\nLet's assume you simply type \"env\" into your shell and press Enter. The shell then spawns a new process in which it executes the binary \"env\". It prints some stuff to your terminal, exits and now you're back in your shell.\n\nHowever, when you run \"exec env\", the shell actually _replaces itself_ with the newly spawned process \"env\". Now when that process exits, your SSH connection gets closed, because the process your terminal was connected to has exited.\n\nYou can try it yourself on your local machine (just tried it on my Macbook): Open a Terminal, enter \"exec env\" and your terminal closes or will print something like \"[Process completed]\" and no longer responds to input. You can simply leave the \"exec\" away and it should work fine.\n", "This error message in your log looks very strange:\n\"/lib64/libm.so.6: could not read symbols: Invalid operation\"\n\nThis suggests that Bazel is not linking in libm when building protoc, see http://stackoverflow.com/questions/9934549/very-strange-linker-behavior for some discussion about a similar error (unrelated to Bazel).\n\nThis error message: \"bash failed: error executing command\" actually says that bash could not be executed. It means that the sandbox might be missing a shared library.\n\nSomething is very different with your Linux machine compared to the ones we test Bazel on :) Which Linux distribution are you using? Is there anything \"special\" about your machine or Linux setup?\n", "<code>uname -o</code> \"outputs GNU/Linux\". I also found that <code>cat /etc/slackware-version</code> outputs \"Slackware 14.0\"\n\nI also noted above that when manually running the Bazel commands, and replacing gcc with g++ it does seem to properly work. Is it possible to change the compiler that Bazel uses?\n", "An update for those interested\nI've managed to get past the compiling problems of </code>protoc</code>. I managed to get past the linker error by adding <code>-lm</code> in 'google/protobuf/BUILD'.\n\nHowever, now I managed to get an actual compiling error, similar to the one mentioned here: #439 \nI'll monitor the other issue and see if a solution for that ticket can be applied to my problem as well.\n\nMy error log:\n......\nIn file included from /usr/lib64/gcc/x86_64-slackware-linux/4.7.1/../../../../include/c++/4.7.1/x86_64-slackware-linux/bits/os_defines.h:40:0,\n                 from /usr/lib64/gcc/x86_64-slackware-linux/4.7.1/../../../../include/c++/4.7.1/x86_64-slackware-linux/bits/c++config.h:414,\n                 from /usr/lib64/gcc/x86_64-slackware-linux/4.7.1/../../../../include/c++/4.7.1/string:40,\n                 from ./tensorflow/core/platform/port.h:19,\n                 from ./tensorflow/core/lib/core/command_line_flags.h:19,\n                 from tensorflow/core/lib/core/command_line_flags.cc:16:\n/usr/include/features.h:328:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\nIn file included from ./tensorflow/core/lib/gtl/array_slice.h:101:0,\n                 from ./tensorflow/core/lib/strings/str_util.h:21,\n                 from tensorflow/core/lib/core/command_line_flags.cc:18:\n./tensorflow/core/lib/gtl/array_slice_internal.h:231:38: error: 'tensorflow::gtl::array_slice_internal::ArraySliceImplBase<const T>::ArraySliceImplBase' names constructor\n./tensorflow/core/lib/gtl/array_slice_internal.h:251:32: error: 'tensorflow::gtl::array_slice_internal::ArraySliceImplBase<T>::ArraySliceImplBase' names constructor\n...\n", "I think your gcc is too old - Slackware 14.0 ships gcc 4.7.1, but the error messages suggests that a C++11 feature called \"Inheriting Constructors\" is being used by tensorflow, which requires at least gcc 4.8.\n\nCan you upgrade to Slackware 14.1? That has gcc 4.8.2, which should be fine.\n", "@philwo kindly too a look and at least we now know why the genrule failed: the error message `error executing command` is raised not because the command fails, but because bash cannot be executed. It's probably because some libraries needed by bash are not mounted into the sandbox. An `ldd bash` and a good hard look and the sandbox debugging output would provide a clue, but you probably simply want to disable the sandbox with the `--*_strategy=standalone` options.\n", "@philwo I've asked the system admin to upgrade the gcc version and I now have access to gcc5.2, However, I'm getting some compiler errors like <code>Error: expecting string instruction after `rep'</code> which is probably due to an old version of the binutils. So I'll give an update once this has been fixed.\n", "Ok guys, I got it working! Thanks for all the support and sticking with me on this :) :+1: \nIf you are interested in any additional information about this case, please let me know!\n"]}, {"number": 428, "title": "Support more image formats", "body": "Hello\n\nForgive me if Ive missed something obvious - new to Tensorflow / machine learning.\n\nI notice there are image creation functions which deal with JPG and PNG image creation via parsing those formats.\n\nIs it planned to support uncompressed images? Client applications may want to decompress video frames and use them for analysis.\n\nPerhaps I am missing a way to construct a tensor from an uncompressed byte array? How would one handle some video optimization for byte alignment / stride etc? \n\nIf there is a more appropriate place to ask beginner questions I am happy to take the conversation there. Thank you!\n", "comments": ["This is a perfectly reasonable feature request. I've change the title to be more general.\n\nFor uncompressed images in particular the workaround is rather easy: If you can load your image into a [width, height, channels] shaped numpy array, you can pass that to any TensorFlow function. That doesn't solve the reading from file question for ppm/bmp files or the like, but there are solutions for that in python, see for instance the pillow module: https://python-pillow.github.io/\n", "You can also do some small amount of decoding using [decode_raw](https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#decode_raw) and [string_to_number](https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#string_to_number).\n\nIt would be easy to add python wrappers around these for specific formats, assuming the image format encodes exactly as a raw byte grid.\n", "Thanks for the link and info - very glad this seems reasonable!\n\nI suppose my only additional 'concern' so to speak, is how many video codecs and engines bytesPerRow is aligned for memory access optimization and not tightly aligned on actual pixel data. If not taken into account in a tensor I imagine would _seriously_ throw off any learned models.\n\nFor example, I work with Apples Core Video which is quite fast, and their CPU pixel buffer may have padding - this would require either clients to strip the padding and do memory copies into TF, or for TF to have some sort of knowledge about this sort of format and handle it internally somehow.\n\nTotally get this is a vastly different prospect for a machine learning tool to inherit the prospect of images not being 'tightly packed' and convoluting what a tensor might be. Just throwing out a concern from a video nerd.\n\nAnyway, speaking aloud. I'm the kind of 'glue libraries together' programmer that prefers when API's are as simple as possible and I can just pass in my byte array and the right thing happens :) \n\nThanks again for listening / considering.\n", "TensorFlow uses Eigen Tensor::Map to map memory internally.  Tensors are always assumed dense, row-major, and memory-aligned; for example, [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_types.h#L80) is the definition of a Matrix.  Working with external memory directly may be possible, but would require a lot of additional consideration; copying is really much easier.  And anyway for most image stuff you'd want to work on the GPU anyway, so you'll want to copy. Anyway, @keveman or @vrv could say more about working with existing memory directly.  You'd have to create an op that's either similar to a [VariableOp](https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/variable_ops.cc) or that subclasses / is similar to [InitializableLookupTable](https://github.com/tensorflow/tensorflow/blob/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/core/kernels/initializable_lookup_table.h#L25) and returns the frames wrapped as Tensors.  Again, there'll be some memory management considerations.\n", "Thanks for the information. Seems like a copy is lowest cost of entry and having things just work\u2122 is good. Looking forward to this feature.\n", "How do i convert from ImageFileTensor back to Array[byte] ? thank you in advance!"]}, {"number": 427, "title": "Cannot find 'util/python/python_include'", "body": "```\nroot@debian-arm:~/tensorflow# ../bazel-0.1.1/output/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --local_resources 2048,.5,1.0 \nINFO: Waiting for response from Bazel server (pid 3825)...\nINFO: Found 1 target...\nINFO: From Executing genrule //util/python:python_check:\n\n\nERROR: Cannot find 'util/python/python_include'.  Did you run configure?\n\n\nERROR: /root/tensorflow/util/python/BUILD:14:1: Executing genrule //util/python:python_check failed: bash failed: error executing command /bin/bash -c 'source tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/local_linux-opt/genfiles/util/python/\"; ./util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked': com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 46.917s, Critical Path: 1.53s\n\n```\n\nChecked around and i don't seem to be missing any fundamental package. Is this a configuration issue and if so, how can I bypass it?\n\nThanks,\n", "comments": ["Did you run ./configure from the base of the source tree?\n\nOn Mon, Dec 7, 2015 at 5:34 AM, V\u00edctor Mayoral Vilches <\nnotifications@github.com> wrote:\n\n> root@debian-arm:~/tensorflow# ../bazel-0.1.1/output/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --local_resources 2048,.5,1.0\n> INFO: Waiting for response from Bazel server (pid 3825)...\n> INFO: Found 1 target...\n> INFO: From Executing genrule //util/python:python_check:\n> \n> ERROR: Cannot find 'util/python/python_include'.  Did you run configure?\n> \n> ERROR: /root/tensorflow/util/python/BUILD:14:1: Executing genrule //util/python:python_check failed: bash failed: error executing command /bin/bash -c 'source tools/genrule/genrule-setup.sh; OUTPUTDIR=\"bazel-out/local_linux-opt/genfiles/util/python/\"; ./util/python/python_config.sh --check && touch $OUTPUTDIR/python_checked': com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> INFO: Elapsed time: 46.917s, Critical Path: 1.53s\n> \n> Checked around and i don't seem to be missing any fundamental package. Is\n> this a configuration issue and if so, how can I bypass it?\n> \n> Thanks,\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/427.\n", "Thanks @ebrevdo, that was it.\n", "Ran `./configure` and got `Can't find swig.  Ensure swig is in $PATH or set $SWIG_PATH.`\nReminder to install swig `brew install swig`\n", "installed through brew but still getting the following Can't find swig.  Ensure swig is in $PATH or set $SWIG_PATH.", "And you tried brew install swig as well @swarathesh? ", "yes\r\n", "swigs installed through brew /\"swig-3.0.11 already installed\"\r\nbut still getting this can't find swig. Ensure swig is in $PATH or set $SWIG_PATH.\r\nwhile doing ./configure\r\n"]}]