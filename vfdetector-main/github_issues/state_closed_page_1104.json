[{"number": 20139, "title": "Aborted (core dumped) while running tftrt test case with tensorflow 1.9.0-rc1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.9.0-rc1\r\n- **Python version**: \r\nboth 2.7 and 3.5\r\n- **Bazel version (if compiling from source)**:\r\n0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\ncuda 9.0 / cuDNN7.0\r\n- **GPU model and memory**:\r\nTitanXp 12G\r\n- **Exact command to reproduce**:\r\nenv TEST_TMPDIR=/tmp/bazel n=1 TF_NEED_GCP=0 TF_NEED_HDFS=1 \\\r\n    TF_NEED_S3=0 TF_ENABLE_XLA=1 TF_NEED_GDR=1 TF_NEED_VERBS=1 \\\r\n    TF_NEED_JEMALLOC=1 \\\r\n    TF_NEED_OPENCL=0 TF_NEED_CUDA=1 TF_NEED_MPI=0 \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    CC_OPT_FLAGS=\"-march=native\" \\\r\n    TF_CUDA_VERSION=9.0 \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_SET_ANDROID_WORKSPACE=false \\\r\n    CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu/ \\\r\n    TF_NEED_KAFKA=0 \\\r\n    TF_NEED_TENSORRT=1 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    NCCL_INSTALL_PATH=/usr/local/nccl2/ \\\r\n    TENSORRT_INSTALL_PATH=/usr/lib/x86_64-linux-gnu/ \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1  ./configure \r\n\r\nbazel build -c opt --copt=-mavx --copt=-mavx2  \\\r\n--copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 \\\r\n--config=cuda --config=mkl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n\r\nbuild_pip_package then pip3 install \r\n\r\npython3 tensorflow/tensorflow/contrib/tensorrt/test/test_tftrt.py\r\n\r\n### Describe the problem\r\nI built tensorflow from source with above script, there is no problem with git branch r1.8, after I checked out to branch r1.9 and rebuilt, tftrt test case will crash.\r\n\r\n### Source code / logs\r\nroot@5f4922d0faef:~/share/tensorflow/tensorflow/contrib/tensorrt/test# python test_tftrt.py \r\n2018-06-20 06:58:51.032087: F tensorflow/core/framework/op.cc:55] Non-OK-status: RegisterAlreadyLocked(op_data_factory) status: Already exists: Op with name _ScopedAllocator\r\nAborted (core dumped)\r\n", "comments": ["I am seeing the same error on both TensorFlow GPU 1.9.0rc0 and 1.9.1rc1. Running on Ubuntu 16.04.\r\nJust need to import tensorrt in Python to see the error.\r\n`import tensorflow.contrib.tensorrt`", "Same here:\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes (but the crash happens before it even reaches my code)\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\nTensorFlow installed from (source or binary):\r\nsource\r\nTensorFlow version (use command below):\r\n1.9.0-rc0\r\nPython version:\r\n2.7\r\nBazel version (if compiling from source):\r\n0.14.1\r\nGCC/Compiler version (if compiling from source):\r\n5.4.0\r\nCUDA/cuDNN version:\r\ncuda 9.0 / cuDNN7.1.4\r\nGPU model and memory:\r\nGTX1070 8G\r\nExact command to reproduce:\r\nimport tensorflow.contrib.tensorrt\r\nDescribe the problem\r\nThere was no problem with git branch r1.8 on another PC, but on this PC with branch r1.9 the 'import' causes the crash.\r\n![import-tensorflow-contrib-tensorrt-as-trt-crash](https://user-images.githubusercontent.com/5282003/41799731-6c1af57e-7627-11e8-85a5-c64c2ac91748.png)\r\n\r\nSource code / logs\r\n2018-06-22 13:43:56.537067: F tensorflow/core/framework/op.cc:55] Non-OK-status: RegisterAlreadyLocked(op_data_factory) status: Already exists: Op with name _ScopedAllocator\r\nAborted (core dumped)\r\n\r\nFor whatever it's worth, when the crash first happened, there was a pop-up window with the message \"Sorry the application Python 2.7 has stopped unexpected\". I took a screenshot of the \"JournalErrors\" section. The error seemed to be related to a file under /etc/udev/rules.d, so I removed that file and rebooted. Now I am still seeing the crash and core dump, but the pop-up window no longer appears.", "The nVidia device driver version is 384.130", "Same with:\r\n\r\nUbuntu 18.04\r\ngcc 7.3.0\r\nbazel  0.15.0\r\nTensorflow r1.9 rc1\r\nCUDA 9.2\r\ncuDNN 7.1.4\r\nTensorRT 4\r\n", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It seems this issue is fixed in master branch, but haven't been merged into r1.9 branch.", "I meet the same error in tf1.9 ...", "@zheng-xq, @gunan, should we make a tf1.9.1 to fix this?\r\n", "This was an issues caused by not having enough test coverage on the integration. Tests are being added to prevent this from happening in the future. Meanwhile we decided not to do a patch release for r1.9, so would you please try to build from master before 1.10 is out?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "1.10 is released and this particular problem is fixed there.", "2019-05-05 18:30:24.660685: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2019-05-05 18:30:25.215100: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'resnet_v1_50/', converted to graph\r\n2019-05-05 18:30:25.334662: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n*** Error in `python': munmap_chunk(): invalid pointer: 0x00007ffe6e056190 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f21cc53c7e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(cfree+0x1a8)[0x7f21cc549698]\r\n", "Hi @mhsszm, could you provide a repro for your case? And do you mind filing a new issue? Thanks.", "Still seeing this problem with 1.12.0-rc2 built from source and python 2.7.\r\n```\r\n2020-09-13 12:33:59.042299: F tensorflow/core/framework/op.cc:55] Non-OK-status: RegisterAlreadyLocked(op_data_factory) status: Already exists: Op with name TRTEngineOp\r\nAborted (core dumped)\r\n```"]}, {"number": 20138, "title": "Can I insert a node in a graph with graph_editor?", "body": "For example:\r\n\r\nI have a graph like:\r\n\r\n```\r\nA -> B -> C\r\n```\r\n\r\nAnd I want to select some indices of the output of B to , like that:\r\n\r\n```\r\nA -> B -> tf.gather(B, indices) -> C\r\n```\r\n\r\nIs it possilbe?", "comments": ["I have tried graph_editor.reroute_ts, but the shape is not matched.", "@shivaniag ", "We are facing the same issue. Looking forward to solutions. @shivaniag ", "If you don't mind working on serialized graphs, you should be able to do this transformation using our project, GraphDef Editor. See https://github.com/CODAIT/graph_def_editor for more info.\r\n\r\nThe basic approach is:\r\n1. Create a temporary graph with the `GatherV2` node, plus some placeholders for its inputs, by running something like:\r\n    ```python\r\n    temp_graph = tf.Graph()\r\n    with temp_graph.as_default():\r\n        b_placeholder = tf.placeholder(name=\"b_placeholder\", \r\n                                       dtype=tf.float32, shape=[2,3,4])\r\n        ix_placeholder = tf.placeholder(name=\"ix_placeholder\", dtype=tf.int32, \r\n                                        shape=[3])\r\n        gather = tf.gather(b_placeholder, ix_placeholder)\r\n    ```\r\n2. Import your original graph and the temporary graph into GraphDef Editor, something like:\r\n    ```python\r\n    import graph_def_editor as gde\r\n    g = gde.Graph(my_original_graph)\r\n    temp_g = gde.Graph(temp_graph)\r\n    gde.copy(temp_g, g)\r\n    ```\r\n3. Reroute the links of the graph using `gde.reroute_ts()`, then remove the temporary placeholder nodes from the graph.\r\n5. Invoke shape inference by calling the `gde.Graph.infer_shapes_and_dtypes()` or by calling `infer_outputs()` on individual Nodes of the graph.\r\n6. Export your modified graph back to TensorFlow using `gde.Graph.to_graph_def()`, `gde.Graph.to_saved_model()`, or `tf.graph.to_tf_graph()`.", "Hi @psyyz10 ! This issue seems to be resolved from above [comment](https://github.com/tensorflow/tensorflow/issues/20138#issuecomment-471060320). Can we move this to closed status now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20137, "title": "Segfault in tensorflow", "body": "Hello,\r\nI  got one segfault so many times. I've just installed gdb and got a back trace below. My system is a fresh Ubuntu 16.04 (newly installed), keras 2.2, tf 1.9.0rc1, numpy 1.14.5 (compiled from source). Please help.\r\nThanks in advance.\r\n\r\n`Thread 20 \"python3.5\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fff779bf700 (LWP 2911)]\r\n0x0000000000000045 in ?? ()\r\n(gdb) bt\r\n#0  0x0000000000000045 in ?? ()\r\n#1  0x00007fffaf185466 in tensorflow::Tensor::~Tensor() ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#2  0x00007fffaf3180db in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fffaf319a2a in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fffaf377fba in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fffaf377062 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflo---Type <return> to continue, or q <return> to quit---\r\nw_framework.so\r\n#6  0x00007ffff253bc80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#7  0x00007ffff7bc16ba in start_thread (arg=0x7fff779bf700)\r\n    at pthread_create.c:333\r\n#8  0x00007ffff78f741d in clone ()\r\n    at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109`", "comments": ["Hi,\r\nSource code and data files attached. I am sure you will get the same segfault. Please help.\r\nTuan \r\n[files.tar.gz](https://github.com/tensorflow/tensorflow/files/2118141/files.tar.gz)\r\n", "My GPU has 11G RAM, and my model is (correct me if I am wrong):\r\n\r\n    parameters: 4,596,739 --> 4,596,739 * 4* 3 approx. 52M for model\r\n    activations (including the input): 20569 --> 128 * 20569 * 4 approx. 10M\r\n    I removed StandardScaler, still the same (as many times before). Reduce batch_size to 64, same result. I also check cuda (9.0) and cudnn (7.0.5). I also check GPU (cudamemtest --stress) for 7h, no errors.", "Not sure if it is the same error.\r\n\r\n`Thread 28 \"python3.5\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fff3e7fc700 (LWP 4803)]\r\n0x0000000000000035 in ?? ()\r\n(gdb) bt\r\n#0  0x0000000000000035 in ?? ()\r\n#1  0x00007fffb951a466 in tensorflow::Tensor::~Tensor() ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#2  0x00007fffb96ad0db in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fffb96aea2a in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007fffb970cfba in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fffb970c062 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#6  0x00007fff9f481c80 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#7  0x00007ffff7bc16ba in start_thread (arg=0x7fff3e7fc700) at pthread_create.c:333\r\n#8  0x00007ffff78f741d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n(gdb) `"]}, {"number": 20136, "title": "Pass dataset tuple as input to Keras model", "body": "This change allows passing nested tuples ((x_1, x_2, ...), (y_1, y_2, ...)) from a Dataset to a Keras model.", "comments": ["Nudge @fchollet for review/approval.", "This is very handy when using Keras together with `tf.data.Datasets` since tuples are used rather than lists.\r\n\r\nLooks like the CI failure is unrelated.", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Looks like this was fixed on master as part of f5a7bea2b78c8a8b4b76060978369c3436b60b55", "Is this PR still alive? If the issue has been fixed by other commit, we probably can close it here.", "Looks like this was fixed, as @lgeiger said. This can be closed.", "Ack, closing this PR now."]}, {"number": 20135, "title": "TENSOR FLOW CNN MNIST EXAMPLE: HOW BATCH SIZE WORKS IN THE MODEL", "body": "In the CNN MNIST example of tensorflow I do not understand how batch size works, when they call the model they specify the size of the bach in 100:\r\n\r\n`train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\nx={\"x\": train_data},\r\ny=train_labels,\r\nbatch_size=100,\r\nnum_epochs=None,shuffle=True)\r\nmnist_classifier.train(input_fn=train_input_fn,steps=20000,hooks=[logging_hook])`\r\n\r\nBut when the model is called\r\n\r\n`def cnn_model_fn(features, labels, mode):\r\n  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])`\r\n\r\nThey put -1 in batch size , I read in tensorflow tutorials and -1 they used it when they told the computer to infer that dimension What I do not understand is that before we put 100 and now because -1 do not understand how is entering the batch size to the model could you help me explaining? Thank you.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20134, "title": "IndexError: tuple index out of range CNN Tensor flow", "body": "I'm working on a convolutional network in tensor flow and I get the following error:\r\n\r\n IndexError: tuple index out of range\r\n\r\nwhen I feed my data to the model  for training I do not know how to fix it. Below is the main function code. I think that my error is in y (labels). I think that the problem is the format of the labels that I can get from input_pipeline function, but I don't know how to fix it.\r\n\r\n`def main(unused_argv):\r\n\r\n    images_batch,labels_batch=input_pipeline(train_path,batch_size,num_epochs)\r\n\r\n    with tf.Session() as sess:\r\n\r\n        #Inicializamos las variables\r\n        init_op=tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\r\n        sess.run(init_op)\r\n\r\n        #Corremos las filas(queue) que se crearon en el grafico computacional \r\n        tf.train.start_queue_runners(sess=sess)\r\n\r\n       detector=tf.estimator.Estimator(model_fn=cnn_model,model_dir=\"/tmp/gun_cnn_detector\")\r\n\r\n        train_fn=tf.estimator.inputs.numpy_input_fn(\r\n            x={\"x\":images_batch.eval()},\r\n            y=labels_batch.eval(),\r\n            batch_size=10,\r\n            num_epochs=None,\r\n            shuffle=True)\r\n       detector.train(\r\n            input_fn=train_fn,\r\n            steps=2)\r\n\r\n        writer = tf.summary.FileWriter('.')\r\n        writer.add_graph(tf.get_default_graph())`\r\n\r\n`def input_pipeline(filenames,batch_size,num_epochs):\r\n    filename_queue=tf.train.string_input_producer([filenames],num_epochs=num_epochs,shuffle=True)\r\n    images,labels=read_file(filename_queue)\r\n\r\n    return images,labels`\r\n\r\nI tried a lot of things. Below is the read function. Also I have another question: when I decode my image, what is the correct format? float32 or uint8?\r\n\r\n`def read_file(filename_queue):\r\n\r\n    #Funcion para leer el archivo tf.record, y retornamos el next recrod\r\n    reader=tf.TFRecordReader()\r\n    _,serialized_example=reader.read(filename_queue)\r\n\r\n    #Se decodifica el tf.record retornando un diccionario \r\n    feature={'train/image':tf.FixedLenFeature([],tf.string),\r\n             'train/label':tf.FixedLenFeature([],tf.int64)}\r\n    features=tf.parse_single_example(serialized_example,features=feature)\r\n\r\n    #Convertimos el string a numeros de los decodificados features\r\n    image=tf.decode_raw(features['train/image'],tf.float32)* (1 / 255.0)\r\n\r\n    #Convertimos a datos\r\n    label=tf.cast(features['train/label'],dtype=tf.int32)\r\n\r\n    #Reshape data\r\n    image=tf.reshape(image,[224,224,3]) \r\n\r\n    return image,label`\r\n\r\n\r\nThis is the error I get:\r\n\r\n`/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/init.py:36: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type. from ._conv import register_converters as _register_converters Traceback (most recent call last): File \"/Users/David/Desktop/David/General/Tesis/Practica/Programas/CNN/CNN.py\", line 113, in tf.app.run() File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 126, in run _sys.exit(main(argv)) File \"/Users/David/Desktop/David/General/Tesis/Practica/Programas/CNN/CNN.py\", line 104, in main steps=2) File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train loss = self._train_model(input_fn, hooks, saving_listeners) File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 843, in _train_model return self._train_model_default(input_fn, hooks, saving_listeners) File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 853, in _train_model_default input_fn, model_fn_lib.ModeKeys.TRAIN)) File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 691, in _get_features_and_labels_from_input_fn result = self._call_input_fn(input_fn, mode) File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 798, in _call_input_fn return input_fn(**kwargs) File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py\", line 175, in input_fn if len(set(v.shape[0] for v in ordered_dict_data.values())) != 1: File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py\", line 175, in if len(set(v.shape[0] for v in ordered_dict_data.values())) != 1: IndexError: tuple index out of range [Finished in 3.9s with exit code 1]`\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20133, "title": "[Java] Support addition of gradient operations in a graph", "body": "This calls the C-api `TF_AddGradients` method through a new JNI binding for adding gradient nodes to a graph. It also includes an `AddGradients` wrapper for invoking this operation smoothly while building a graph using the new Java Ops API.", "comments": ["Windows build failure seems to be a flaky. Submitting now."]}, {"number": 20132, "title": "tfevents not generated if specifying model_dir for tf.estimator.Estimator", "body": "### System information\r\n- I've been following the tutorial to create a custom estimator on this page: https://www.tensorflow.org/get_started/custom_estimators\r\n- OS: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: installed through pip\r\n- **TensorFlow version (use command below)**: ('v1.8.0-0-g93bc2e2072', '1.8.0')\r\n- **Python version**: Python 2.7\r\n\r\n### Describe the problem\r\ntfevents file is not found in the `model_dir` if I specify `model_dir` when creating the Estimator. However, graph file and checkpoints are present.\r\n\r\n```\r\nautoencoder = tf.estimator.Estimator(\r\n    model_fn = my_model_fn,\r\n    #model_dir = logs_path, # <--- tfevents not created if I specify model_dir here...\r\n    params = {\r\n        'feature_columns': feature_columns,\r\n        'input_units_num': num_input\r\n    },\r\n    #config = tf.estimator.RunConfig(model_dir = logs_path) # <--- ... nor if I specify model_dir here.\r\n)\r\n```\r\n\r\nHowever, if I don't specify `model_dir` parameter neither in the config, nor as one of the Estimator parameters, tfevents file is written into the `/tmp/<folder>` along with graph and checkpoints.\r\n\r\nMany thanks!", "comments": ["Apologies, very strange issue. If I specified `model_dir` as `/home/<user_name>/<directory>/tensorflow`, the tfevents file wasn't generated. But if I changed the `model_dir` to `/home/<user_name>/<directory>/tensorflow123` or to `/tmp/tensorflow`, the tfevents file was present. Closing this issue."]}, {"number": 20131, "title": "Computation Paths of different attention mechanism", "body": "I was checking the computation paths of the different attention mechanism and compared them with the attention wrapper implementation. I noticed that the computation paths follows the Luong implementation more closely. People brought up this issue in [9635](https://github.com/tensorflow/tensorflow/issues/9635)  but the thread was closed since the main issue was answered already.\r\n\r\nTo summarize there were two attention mechanisms. Luong uses the computation path  h(t)->a(t)->c(t)->h\u02dc(t) while Bahnadau uses the h(t-1)->a(t)->c(t)->h(t).\r\n\r\nI seems that right now this is not a priority to implement as the people in the discussion are saying that there is no significant difference.  However I think it would be better to document that the attention wrapper implementation is more similar the the Luong attention just so people are aware. Also the output_attention does not really change the computation paths so I think the description for the flag should be changed.\r\n\r\nThat being said, this is my first time to read deeply into the internal tensorflow code. If just me misunderstanding the implementation, then feel free to point out the things I might have missed. Thanks.\r\n\r\nEdit:\r\nI looked at the code once again. It seems like the I was just having trouble due to the recursive nature of the code.\r\nWhen using output_attention=True, the recursion goes to h(t)->a(t)->c(t)->h\u02dc(t).\r\nWhen using output_attention=False, the recursion goes to c(t)->h(t)->a(t+1)->c(t+1) which is the same if to h(t-1)->a(t)->c(t)->h(t).\r\n\r\nIf things work like my edit, feel free to close the issue.\r\n\r\nEdit 2:\r\nIn [issues 9635](https://github.com/tensorflow/tensorflow/issues/9635) other people also saw this the first time I saw it (that the paths are different) so a verification or second opinion will be helpful.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: master branch \r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "@lmthang to comment.", "Nagging Assignee @lmthang: It has been 41 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Close issue as the author has made a few edits and already replied in https://github.com/tensorflow/tensorflow/issues/9635. It seems that your understanding of the computation paths is right (though I missed something as this code has been written for some time)."]}, {"number": 20130, "title": "[GCS] Typo in ConfigureGcsHook.", "body": "This commit fixes a typo on ConfigureGcsHook that prevented its correct\r\noperation.", "comments": []}, {"number": 20129, "title": "Cleanup NMT notebook, fix image links", "body": "Stage: https://colab.sandbox.google.com/github/lamberta/tensorflow/blob/fix-nmt-nb/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb", "comments": ["@yashk2810 PTAL", "Looks good to me :)"]}, {"number": 20128, "title": "Created Tflite file is empty", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: bazel run --config = opt //tensorflow/contrib/lite/toco:toco -- --input_format=TENSORFLOW_GRAPHDEF --input_file=/emulator/tensorflow_xtensa/test/assets/conv_actions_frozen.pb --output_format=TFLITE --output_file=/emulator/tensorflow_xtensa/tensorflow/tensorflow/contrib/lite/examples/android/assets/conv_actions_frozen.pb --output_file=/emulator/tensorflow_xtensa/tensorflow/tensorflow/contrib/lite/examples/android/assets/conv_actions_frozen.tflite --output_array=add_2 --input_array=Conv2D \r\n\r\n### Describe the problem\r\nI am trying to create the speech model tflite for anrdoid from the .pb file. I noticed that when I give --allow_custom_ops, it creates a tflite file of 3MB. But, without that, the created tflite files is zero bytes.\r\nWhen I upload this tflite file with --allow_custom_ops, it gives a run time error asking for the custom ops to be defined. \r\n", "comments": ["toco --input_file=/usr/local/google/home/aselle/Downloads/conv_actions_frozen.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=conv_actions_frozen.tflite --input_arrays=decoded_sample_data --output_arrays=labels_softmax --allow_custom_ops\r\n\r\nAs far as custom operations.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/custom_operators.md\r\n", "That helps to get an understanding. Thank you. \r\nWhen I tried to run with the tflite file generated using --allow_custom_ops, I got the following run time error. \"Didn't find custom op for name 'DecodeWav' with version 1\". Based on the exampled given for 'sin' wave in the above link, should I define this operation and the others? \r\n\r\nIf that's the case, could you please point to a resource for the same if it exist?\r\nThanks! ", "You could try this converter I wrote... https://github.com/aselle/tensorflow/blob/7e71aa528111cd73cabc9fefdbb68422e35c16ce/tensorflow/examples/speech_commands/conv_only.py\r\n\r\nthe main problem is you are using two early of an op in the graph, because you are expected to decode your own wave into a pcm array first (unlike the tf graph).", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20127, "title": "Fix kCall comparison", "body": "The HloInstruction::Identical is broken for kCall instructions.  It is possible that this API isn't used by anything but the subcomputation unification pipeline stage, which I guess isn't in use by any backends but the GC one.\r\n\r\nThe call_test in the xla/tests was showing up a Casting error.  The kCall was using the same tests as the cross replica sum, as you can see.  However, the replica_group_ids() function was trying to cast the op to a cross replica sum subclass, and failing.\r\n\r\n", "comments": []}, {"number": 20126, "title": "Using MirroredStrategy to distribute GPU processing causes assertion error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRunning Amazon Deep Learning AMI (Amazon Linux) Version 10.0 - ami-9d0d7fe2\r\n> uname -a\r\nLinux ip-172-30-4-195 4.9.93-41.60.amzn1.x86_64 #1 SMP Fri Apr 13 21:58:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n> cat /etc/os-release\r\nNAME=\"Amazon Linux AMI\"\r\nVERSION=\"2017.09\"\r\nID=\"amzn\"\r\nID_LIKE=\"rhel fedora\"\r\nVERSION_ID=\"2017.09\"\r\nPRETTY_NAME=\"Amazon Linux AMI 2017.09\"\r\nANSI_COLOR=\"0;33\"\r\nCPE_NAME=\"cpe:/o:amazon:linux:2017.09:ga\"\r\nHOME_URL=\"http://aws.amazon.com/amazon-linux-ami/\"\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nPre-installed on Amazon instance.  \r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-3-gf91bd2f8c4 1.8.0\r\n\r\n- **Python version**: \r\nPython 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.11.1- (@non-git)\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n> gcc --version\r\ngcc (GCC) 4.8.5\r\n\r\n- **CUDA/cuDNN version**:\r\n>  nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n\r\n- **GPU model and memory**:\r\nTesla K80, 11439MiB\r\n\r\n- **Exact command to reproduce**:\r\nUnzip the test_distributed.zip.  It should unzip to a script called \"test_distributed.py\".  Run the script using Python3.  It is completely self-contained.  It will throw the assertion error described below.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI ported word2vec training code from low-level tensorflow flow API to the Estimator framework, where it ran OK, making use of the GPUs. When I modified the code to use multiple GPU's however (e.g., putting a MirroredStrategy in the configuration to the Estimator), it threw an assertion error, apparently not finding that a variable was of type MirroredVariable.  The stack trace is below.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nTraceback (most recent call last):\r\n  File \"test_distributed.py\", line 54, in <module>\r\n    classifier.train(input_fn=lambda:input_fn(train_features,train_labels,batch_size))\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\r\n    self.config)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 756, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 702, in _distributed_apply\r\n    for grad, var in grads_and_vars\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 703, in <listcomp>\r\n    for op in distribution.unwrap(distribution.update(var, update, grad))\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 838, in update\r\n    return self._update(var, fn, *args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 300, in _update\r\n    assert isinstance(var, values.MirroredVariable)\r\nAssertionError\r\n\r\n\r\n\r\n[test_distributed.zip](https://github.com/tensorflow/tensorflow/files/2116330/test_distributed.zip)\r\n", "comments": ["(I don't know if the following will help you track this down, but I'll leave this as a hint.)\r\nI decided to spend the day looking at the code to see if I could somehow fix it locally.  I'm still looking, but here's what I found so far.  I put print statements at in the MirroredVariable constructor and also before the assert that gave rise to the error.  Here are the MirroredVariables that are being constructed:\r\n\r\n MirroredVariable:{'/job:localhost/replica:0/task:0/device:GPU:0': <tf.Variable 'global_step:0' shape=() dtype=int64>, '/job:localhost/replica:0/task:0/device:GPU:1': <tf.Variable 'global_step/replica_1:0' shape=() dtype=int64>, '/job:localhost/replica:0/task:0/device:GPU:2': <tf.Variable 'global_step/replica_2:0' shape=() dtype=int64>, '/job:localhost/replica:0/task:0/device:GPU:3': <tf.Variable 'global_step/replica_3:0' shape=() dtype=int64>, '/job:localhost/replica:0/task:0/device:GPU:4': <tf.Variable 'global_step/replica_4:0' shape=() dtype=int64>, '/job:localhost/replica:0/task:0/device:GPU:5': <tf.Variable 'global_step/replica_5:0' shape=() dtype=int64>, '/job:localhost/replica:0/task:0/device:GPU:6': <tf.Variable 'global_step/replica_6:0' shape=() dtype=int64>, '/job:localhost/replica:0/task:0/device:GPU:7': <tf.Variable 'global_step/replica_7:0' shape=() dtype=int64>}\r\n\r\n MirroredVariable:{'/job:localhost/replica:0/task:0/device:GPU:0': <tf.Variable 'embeddings:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:1': <tf.Variable 'embeddings/replica_1:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:2': <tf.Variable 'embeddings/replica_2:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:3': <tf.Variable 'embeddings/replica_3:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:4': <tf.Variable 'embeddings/replica_4:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:5': <tf.Variable 'embeddings/replica_5:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:6': <tf.Variable 'embeddings/replica_6:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:7': <tf.Variable 'embeddings/replica_7:0' shape=(20, 300) dtype=float32>}\r\n\r\nOK, this makes sense to me.  The global_step variable is some TF variable that is maintained and updated by TF and the embeddings variable are the weights of my word2vec matrix, which I'm trying to learn.   Now let's take a look at the variables that the assertion is being applied to:\r\n\r\nMirroredVariable:{'/job:localhost/replica:0/task:0/device:GPU:0': <tf.Variable 'embeddings:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:1': <tf.Variable 'embeddings/replica_1:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:2': <tf.Variable 'embeddings/replica_2:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:3': <tf.Variable 'embeddings/replica_3:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:4': <tf.Variable 'embeddings/replica_4:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:5': <tf.Variable 'embeddings/replica_5:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:6': <tf.Variable 'embeddings/replica_6:0' shape=(20, 300) dtype=float32>, '/job:localhost/replica:0/task:0/device:GPU:7': <tf.Variable 'embeddings/replica_7:0' shape=(20, 300) dtype=float32>}\r\n\r\nPerDevice:{'/job:localhost/replica:0/task:0/device:GPU:0': <tf.Variable 'Variable:0' shape=(20, 300) dtype=float32_ref>, '/job:localhost/replica:0/task:0/device:GPU:1': <tf.Variable 'tower_1/Variable:0' shape=(20, 300) dtype=float32_ref>, '/job:localhost/replica:0/task:0/device:GPU:2': <tf.Variable 'tower_2/Variable:0' shape=(20, 300) dtype=float32_ref>, '/job:localhost/replica:0/task:0/device:GPU:3': <tf.Variable 'tower_3/Variable:0' shape=(20, 300) dtype=float32_ref>, '/job:localhost/replica:0/task:0/device:GPU:4': <tf.Variable 'tower_4/Variable:0' shape=(20, 300) dtype=float32_ref>, '/job:localhost/replica:0/task:0/device:GPU:5': <tf.Variable 'tower_5/Variable:0' shape=(20, 300) dtype=float32_ref>, '/job:localhost/replica:0/task:0/device:GPU:6': <tf.Variable 'tower_6/Variable:0' shape=(20, 300) dtype=float32_ref>, '/job:localhost/replica:0/task:0/device:GPU:7': <tf.Variable 'tower_7/Variable:0' shape=(20, 300) dtype=float32_ref>}\r\n\r\nThe first thing we see is a MirroredVariable for the embeddings.  This makes sense, and it passes the assertion.  The next thing we see is a PerDevice variable that has the same shape as the embeddings variable, but is unnamed.  Hmmmmm....not one of my variables.  What might this be?  Is this possibly an internal tensor of gradients computed at a training step and being passed around?  If so, is the optimizer failing to create a MirroredVariable?  In any case, this is the variable that fails the assertion.\r\n\r\n\r\n\r\n\r\n\r\n", "I **have same problem** with my code. Just trying to use your MirroredStrategy functional, and it's not working for me, and I have only some **assertion error**;\r\n\r\n**0) Have I written custom code:**\r\n[Too much lines, so my code on pastebin](https://pastebin.com/LzED1tF3)\r\n\r\n**1) OS Platform and Distribution:**\r\n-Linux UbuntuPC 4.4.0-130-generic #156-Ubuntu SMP Thu Jun 14 08:53:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n**2) TensorFlow installed from:**\r\nsudo pip3.5 install ***\r\n\r\n**3) TensorFlow version:**\r\nChecked on two versions of tf:\r\n     a) 1.10.nightly\r\n     tf_nightly_gpu-1.10.0.dev20180620-cp35-cp35m-manylinux1_x86_64.whl\r\n     b) 1.9.rc2\r\n     tensorflow_gpu-1.9.0rc2-cp35-cp35m-manylinux1_x86_64.whl\r\n\r\n**4) Bazel version:**\r\n> Extracting Bazel installation...\r\n> Build label: 0.13.1\r\n> Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n> Build time: Wed May 23 11:17:23 2018 (1527074243)\r\n> Build timestamp: 1527074243\r\n> Build timestamp as int: 1527074243\r\n\r\n**5) CUDA/cuDNN version**\r\nCUDA - 9.0.176.2\r\ncuDNN - 7.1.2\r\n\r\n**6) GPU model and memory:**\r\nNvidia GeForce GTX 1080 8Gb\r\n\r\n**7) Exact command to reproduce:**\r\n\r\n> $ python3.5 my_tf_script.py\r\n\r\n\r\nWhen I use only 1 GPU (GPU_NUM = 1 on line \u211617) it works.\r\nBut when I use your \"magic word\" - \r\n**distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=GPU_NUM)**\r\nIt failed on next step where model.train()\r\nSo my error:\r\n**++++++++++++++++ERROR:++++++++++++++++**\r\n```\r\n\r\n/usr/bin/python3.5 /home/user/SOME_PATH/bin/my_tf_script.py\r\nData #1 loaded\r\nData loading complete\r\nINFO:tensorflow:Using config: {'_master': '', '_task_type': 'worker', '_service': None, '_save_checkpoints_steps': None, '_tf_random_seed': None, '_session_config': None, '_save_summary_steps': 100, '_is_chief': True, '_global_id_in_cluster': 0, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_task_id': 0, '_num_worker_replicas': 1, '_model_dir': './checkpoints_train/', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f51e086b4a8>, '_device_fn': None, '_evaluation_master': '', '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f51e086b5f8>}\r\n2018-07-14 00:04:57.193208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-07-14 00:04:57.489755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.80GiB\r\n2018-07-14 00:04:57.491371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\r\n2018-07-14 00:04:58.205491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-14 00:04:58.205548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \r\n2018-07-14 00:04:58.205562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \r\n2018-07-14 00:04:58.205575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \r\n2018-07-14 00:04:58.206129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 7534 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2018-07-14 00:04:58.282580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:1 with 7534 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\r\nINFO:tensorflow:Configured nccl all-reduce.\r\n2018-07-14 00:04:58.438084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\r\n2018-07-14 00:04:58.438251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-14 00:04:58.438271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \r\n2018-07-14 00:04:58.438298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \r\n2018-07-14 00:04:58.438315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \r\n2018-07-14 00:04:58.438712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7534 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2018-07-14 00:04:58.438878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7534 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Calling model_fn.\r\nTRAINABLE_VARIABLES: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[:1]: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[1:]: [<tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>]\r\nINFO:tensorflow:Calling model_fn.\r\nTRAINABLE_VARIABLES: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>, <tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[:1]: [<tf.Variable 'V:0' shape=(3449, 13, 64) dtype=float32_ref>]\r\nTRAINABLE_VARIABLES[1:]: [<tf.Variable 'S:0' shape=(902113, 64) dtype=float32_ref>, <tf.Variable 'tower_1/V:0' shape=(3449, 13, 64) dtype=float32_ref>, <tf.Variable 'tower_1/S:0' shape=(902113, 64) dtype=float32_ref>]\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nWARNING:tensorflow:Efficient allreduce is not supported for IndexedSlices.\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\r\n    for grad, var in grads_and_vars\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in <listcomp>\r\n    for op in distribution.unwrap(distribution.update(var, update, grad))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\r\n    return self._update(var, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\r\n    assert isinstance(var, values.MirroredVariable)\r\nAssertionError\r\nTraceback (most recent call last):\r\n  File \"tst.py\", line 198, in <module>\r\n    model.train(input_fn=make_full_data_dataset, steps=300000)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 375, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1131, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1171, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 811, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 279, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 273, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 685, in _distributed_apply\r\n    for grad, var in grads_and_vars\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 686, in <listcomp>\r\n    for op in distribution.unwrap(distribution.update(var, update, grad))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 894, in update\r\n    return self._update(var, fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 325, in _update\r\n    assert isinstance(var, values.MirroredVariable)\r\nAssertionError\r\n```", "@bpinette and @Bocharick  - `tf.get_variable` is the recommended way to create variables, instead of `tf.Variable`,  as the former will create the appropriate MirroredVariable (as well as re-use variables etc). See https://www.tensorflow.org/guide/variables. Could you try with tf.get_variable instead, in places where you use `tf.Variable`? ", "Nagging Assignee @guptapriya: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please re-open if the above suggestion does not address the problem. ", "\r\n\r\n\r\n> @bpinette and @Bocharick - `tf.get_variable` is the recommended way to create variables, instead of `tf.Variable`, as the former will create the appropriate MirroredVariable (as well as re-use variables etc). See https://www.tensorflow.org/guide/variables. Could you try with tf.get_variable instead, in places where you use `tf.Variable`?\r\n\r\n@guptapriya there is no tf.get_variable in tf2, how to use it?", "@roadcode in TF2, you can do `tf.Variable(..)` directly - that is the recommended way. See https://www.tensorflow.org/guide/variable"]}, {"number": 20125, "title": "WishartCholesky with 1-D batch shape fails to calculate mean", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See source below\r\n\r\n### Describe the problem\r\nInitializing a `tf.contrib.distributions.WishartCholesky` distribution with parameters identical to those provided in the one of the examples, the `mean()` function fails an attempted multiplication.\r\n\r\n### Source code / logs\r\n```\r\n# Initialize two 3x3 Wisharts with Cholesky factored scale matrices.\r\ndf = [5, 4]\r\nchol_scale = tf.cholesky(tf.ones([2.,3.,3.]))  # Shape is [2, 3, 3].\r\ndist = tf.contrib.distributions.WishartCholesky(df=df, scale=chol_scale)\r\ndist.mean()\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 2 and 3 for 'WishartCholesky/WishartCholesky/mean/mul' (op: 'Mul') with input shapes: [2], [2,3,3].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/ops/distributions/distribution.py\", line 923, in mean\r\n    return self._mean()\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/wishart.py\", line 375, in _mean\r\n    return self.df * self._square_scale_operator()\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 979, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1211, in _mul_dispatch\r\n    return gen_math_ops.mul(x, y, name=name)\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4759, in mul\r\n    \"Mul\", x=x, y=y, name=name)\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1734, in __init__\r\n    control_input_ops)\r\n  File \"/Users/equint/interpretable-classification/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1570, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Dimensions must be equal, but are 2 and 3 for 'WishartCholesky/WishartCholesky/mean/mul' (op: 'Mul') with input shapes: [2], [2,3,3].\r\n```", "comments": ["I could reproduce the error as indicated.", "Nagging Assignee @srvasude: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed in TFP: https://github.com/tensorflow/probability/commit/c10cc8e1120f9e017d0117c934316d09679a52e7."]}, {"number": 20124, "title": "Stacking lstm/gru cells of different sizes using MultiRNN cell", "body": "I want to stack 2 lstm or GRU cells with size 64 and 32 respectively, however the multiple rnn cell throws this error-\r\nValueError: Dimension 2 in both shapes must be equal, but are 64 and 32. Shapes are [2,128,64] and [2,128,32].\r\n\tFrom merging shape 0 with other shapes. for 'Variable_1/initial_value' (op: 'Pack') with input shapes: [2,128,64], [2,128,32].\r\n\r\nIs there any work around for this problem?\r\nRight now I'm just specifying 64 and 32 as num_units in LSTMCell.\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo, I have followed the tensorflow tutorials and taken some inspiration from stackoverflow answers to stack the LSTM cells\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled using pip3\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A using CPU only version\r\n- **Exact command to reproduce**:\r\nN/A more info added in the next comment", "comments": ["To give more context this is how i have stacked the cells-\r\n`stacked_cells=tf.nn.rnn_cell.MultiRNNCell([get_cell(num_hidden[_],keep_prob,z_prob_cells,z_prob_states,use_dropout,use_zoneout)` for _ in range(num_hidden_layers)],state_is_tuple=True)\r\n\t\r\n\tdefault_state=stacked_cells.zero_state(128,tf.float32)\r\n\tprint(default_state)\r\n\t#init_state=tf.placeholder_with_default(default_state,shape=[num_hidden_layers,2,None,num_hidden])\r\n\tinit_state=tf.Variable(default_state,trainable=False)\r\n\tstate_per_layer=tf.unstack(init_state,axis=0)\r\n\t#print(type(state_per_layer))\r\n\trnn_tuple_state=tuple([tf.nn.rnn_cell.LSTMStateTuple(state_per_layer[idx][0],state_per_layer[idx][1]) for idx in range(num_hidden_layers)])\r\n\t\r\n\t\r\n\toutputs, new_states = tf.nn.dynamic_rnn(stacked_cells, x, dtype=tf.float32,\r\n                                sequence_length=seq,parallel_iterations=64,initial_state=rnn_tuple_state)\r\n\t#update_op=get_state_update_op(states,new_states)\r\n\tnew_states=tf.stack(new_states)`\r\n\r\nThe traceback i get is the following-\r\n\r\n`Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 2 in both shapes must be equal, but are 64 and 32. Shapes are [2,128,64] and [2,128,32].\r\n\tFrom merging shape 0 with other shapes. for 'Variable_1/initial_value' (op: 'Pack') with input shapes: [2,128,64], [2,128,32].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"model_4_f.py\", line 289, in <module>\r\n    score,state_update_op= seqRNN(x_batch,seq_batch,weights_out,biases)\r\n  File \"model_4_f.py\", line 253, in seqRNN\r\n    init_state=tf.Variable(default_state,trainable=False)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\r\n    constraint=constraint)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 355, in _init_from_args\r\n    initial_value, name=\"initial_value\", dtype=dtype)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1014, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1104, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1034, in _autopacking_conversion_function\r\n    return _autopacking_helper(v, inferred_dtype, name or \"packed\")\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 997, in _autopacking_helper\r\n    return gen_array_ops.pack(elems_as_tensors, name=scope)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 4517, in pack\r\n    \"Pack\", values=values, axis=axis, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1734, in __init__\r\n    control_input_ops)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1570, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Dimension 2 in both shapes must be equal, but are 64 and 32. Shapes are [2,128,64] and [2,128,32].\r\n\tFrom merging shape 0 with other shapes. for 'Variable_1/initial_value' (op: 'Pack') with input shapes: [2,128,64], [2,128,32].\r\n`", "Nagging Assignee @jart: It has been 167 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@archiki Looks like it is a support question. Please post it in [Stackoverflow]( https://stackoverflow.com/) where we have great community to support each other on support questions. Github is mainly for addressing bugs in installation and performance.Thanks!"]}, {"number": 20123, "title": "Fix warning in constrained_optimization test", "body": "In constrained_optimization test, keep_dims was used for reduce_sum. Since keep_dims has been deprecated it generates unnecessary warning. This fix updates keep_dims -> keepdims to disable the warning.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20122, "title": "Fix CMAKE build issue on Linux", "body": "While building tensorflow on Linux with CMAKE, as specified in README:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh\r\n```\r\n\r\nThe following build error happens:\r\n```\r\nCMakeFiles/tf_core_lib.dir/workspace/tensorflow/core/platform/cloud/curl_http_request.cc.o: In function `tensorflow::CurlHttpRequest::~CurlHttpRequest()':\r\ncurl_http_request.cc:(.text+0x11b): undefined reference to `curl_slist_free_all'\r\ncurl_http_request.cc:(.text+0x146): undefined reference to `curl_slist_free_all'\r\ncurl_http_request.cc:(.text+0x180): undefined reference to `curl_easy_cleanup'\r\n...\r\n```\r\n\r\nThere were several issues related. The file above requires curl which\r\nwas never supported on CMAKE at the first places.\r\n\r\nThis fix address the issue so that CMAKE build works on Linux.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20121, "title": "Readme of the docker directory points to dead http links", "body": "There seems to be a problem with the readme of the docker directory. The variable TF_DOCKER_BUILD_CENTRAL_PIP is initialized with HTTP links that are dead (404). This is preventing building at least the gpu image.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md#rebuilding-the-containers", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 14.04 (nividia-docker)\r\n- **TensorFlow installed from (source or binary):** source\r\n- **TensorFlow version (use command below):** N/A\r\n- **Python version:** 3\r\n- **Bazel version (if compiling from source):** N/A\r\n- **GCC/Compiler version (if compiling from source):** N/A\r\n- **CUDA/cuDNN version:** N/A\r\n- **GPU model and memory:** N/A\r\n- **Exact command to reproduce:**\r\n```bash\r\nexport TF_DOCKER_BUILD_IS_DEVEL=NO\r\nexport TF_DOCKER_BUILD_TYPE=GPU\r\nexport TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3\r\n\r\nexport NIGHTLY_VERSION=\"1.head\"\r\nexport TF_DOCKER_BUILD_CENTRAL_PIP=$(echo ${TF_DOCKER_BUILD_PYTHON_VERSION} | sed s^PYTHON2^http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=${TF_DOCKER_BUILD_PYTHON_VERSION},label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp27-cp27mu-manylinux1_x86_64.whl^ | sed s^PYTHON3^http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp35-cp35m-manylinux1_x86_64.whl^)\r\n\r\ntensorflow/tools/docker/parameterized_docker_build.sh\r\n```\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md#rebuilding-the-containers", "I've recently proposed a change to TensorFlow that obsoletes parameterized_docker_build.sh and would effectively resolve this issue. If anyone following this thread is interested in making TensorFlow's Dockerfile story better for everyone, [please take a look at the RFC](https://github.com/tensorflow/community/pull/8).", "This has been resolved by the newer images."]}, {"number": 20120, "title": "TensorRT - TRTEngineOp error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0\r\n- **GPU model and memory**: GeForce GTX 1070, 8GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI've used the code [here](https://github.com/tensorflow/models/tree/master/research/tensorrt) and I am able to run it. My problem is that I can't visualise the graph in Tensorboard anymore (I'm using the `import_pb_to_tensorboard` script) to check the architecture and I can't load it to use it afterwards.\r\n\r\n### Source code / logs\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/tensorflow/tensorflow/python/tools/import_pb_to_tensorboard.py\", line 76, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n\r\n  File \"/tensorflow/tensorflow/python/tools/import_pb_to_tensorboard.py\", line 58, in main\r\n    import_to_tensorboard(FLAGS.model_dir, FLAGS.log_dir)\r\n\r\n  File \"/tensorflow/tensorflow/python/tools/import_pb_to_tensorboard.py\", line 49, in import_to_tensorboard\r\n    importer.import_graph_def(graph_def)\r\n\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 418, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'TRTEngineOp' in binary running on 5b451011e68a. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) 'tf.contrib.resampler' should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```\r\n\r\nI've seen this error in a post on Stackoverflow ([here](https://stackoverflow.com/a/50461127/9052343)) but that didn't help me. Does anyone have an idea how to solve this issue ?\r\n\r\nThanks.", "comments": ["I think you should build tensorflow yourself with TensorRT support.", "Thanks for your answer.\r\n\r\nI did build Tensorflow from source and with TensorRT support, maybe I didn't specified the correct TensorRT location: I left the default location, I couldn't locate any TensorRT folder after installing it with the debian installation.\r\n\r\n**EDIT:** I've also tried to install TensorRT with the Tar File installation but same error.", "@rhavard Try add\r\n`import tensorflow.contrib.tensorrt as trt`\r\nin your code.", "@qinyao-he \r\nI had a similar problem which was solved by your tip. Thank you.", "@qinyao-he adding the import also fixed the issue for me.", "I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!", "Solution: add import \r\n\r\n> from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\nlink discuss: https://github.com/tensorflow/tensorflow/issues/26525", "I try to load a TFTRT converted model and execute in C++ version Tensorflow program, and I encountered this problem. How could I analyze TRTEngineOp in C++?", "@oscarriddle  Any update on the issue you were facing? "]}, {"number": 20119, "title": "toco aborts on converting quantized graph to TFLite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.5\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.1\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n```\r\ntoco \\\r\n--input_file=tmp/killfie_detector.pb \\\r\n--output_file=tmp/quantized_killfie_detector.lite \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n--input_array=input1 \\\r\n--output_array=output_node0 \\\r\n--inference_type=FLOAT \\\r\n--input_data_type=FLOAT \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--quantize_weights=true \\\r\n--mean_value=127.5 \\\r\n--std_value=127.5\r\n```\r\n\r\n### Describe the problem\r\nI am trying to convert a ResNet-50 model to TFLite after quantization. The quantized graph was obtained using `transform_graph` and `--transforms='quantize_weights'` on a .pb file. However, on running `toco` on the quantized graph using the command above, I am first asked to enter the max/min values and the following error:\r\n```\r\nArray input_1, which is an input to the Conv operator producing the output array conv1/convolution, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\n```\r\n\r\nOn adding `--default_ranges_min=0` and `--default_ranges_max=6` based on the suggestions [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md#use-dummy-quantization-to-try-out-quantized-inference-on-a-float-graph-).\r\n\r\nWhen I ran that script, I got this error message:\r\n```\r\nF tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:519] Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Dequantize) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n```\r\n\r\nIs there any way around this to get a quantized TFLite model?\r\n", "comments": ["I meet the same question. And I don't know why it lacks of some min/max information. If we set default min/max value, it will influence the accuracy.  ", "It seems your model is ResnetV2. The contrib.quantize tool doesn't support this yet, but it is something we plan to add. Particular the placement of Batch norm in ResnetV2 needs to be resolved.\r\n\r\nThis seem the same as issue https://github.com/tensorflow/tensorflow/issues/20111 ?", "Duplicate of #20141 ?"]}, {"number": 20118, "title": "Keras softmax function is not supported by tensorflow-gpu 1.4.1", "body": "I have a nvidia GeForce 940M which supports only CUDA 8.\r\nCUDA 8 needs cudnn 7 to work.\r\nCUDA 8 only supports tensorflow-gpu 1.4.1 and older.\r\ntensorflow-gpu 1.4.1 does not support the Keras softmax function. (throws an error saying recieved extra positional argument)\r\n\r\nMy only option here seems to be to buy a new GPU which supports CUDA 9, please help.", "comments": ["https://github.com/keras-team/keras/issues/10440\r\n\r\n\"As a general rule, the latest release of Keras is compatible with the latest release of TensorFlow as well as the previous release of TensorFlow (in most cases, it is actually compatible with several prior TF releases, but that is not guaranteed).\r\n\r\nTF 1.3 is now 5 releases behind (soon to be 6, since 1.9rc is already out), so you should either use a prior version of Keras or use an updated version of TensorFlow.\"\r\n\r\nMy guess is your Keras version is new and TensorFlow version is old.", "Using this patch works:\r\n\r\nhttps://github.com/keras-team/keras/pull/9700/files\r\n\r\nI think many machines are still equipped with CUDA 8.0, so the GPU driver limits prebuilt tensorflow version <= 1.4.1.\r\nIn order words, keras > 2.1.6 force to use CUDA 9.0 indirectly since it only supports TF >= 1.5.0.", "I think you are misunderstanding the TensorFlow gpu requirements:\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases\r\n\r\n```\r\nTensorFlow 1.7 may be the last time we support Cuda versions below 8.0.\r\nStarting with TensorFlow 1.8 release, 8.0 will be the minimum supported\r\nversion.\r\nTensorFlow 1.7 may be the last time we support cuDNN versions below 6.0.\r\nStarting with TensorFlow 1.8 release, 6.0 will be the minimum supported\r\nversion.\r\n```\r\n\r\nTensorFlow 1.8.0 works with CUDA 8, CUDNN 6", "@brge17 TensorflowGPU does not work with CUDA 8 CUDNN 7 or 6.\r\nI cannot use another version of Keras either as they are not compatible with my versions either. I have triple checked.\r\n", "@ghostplant That seems like it would work, really appreciate your reply, but I do not understand how I can implement that. I'm kind of new to this, could you please help?", "@brge17 Hi, I know new version should be able support CUDA 8.0. I just meant `prebuilt tensorflow version`, since tensorflow-gpu==1.4.1 is compiled with CUDA 8.0 while tensorflow-gpu==1.5.0 starts to use CUDA 9.0.\r\n\r\nConsidering custom compiling the TF source code is a little complex in terms of bazel dependencies and GPU drivers, so most ML users has little knowledge to fully handle it and they usually prefer the prebuilt version.\r\n", "@ghostplant Windows10   ':-)", "@SidJain1412 I think you need to search the location of filename `tensorflow_backend.py` which should be within a sub-directory of Python root path. Open the file with an editor, when change the line below from:\r\n\r\n```sh\r\n    return tf.nn.softmax(x, axis=axis)\r\n```\r\n\r\ninto:\r\n```sh\r\n    return tf.nn.softmax(x)    # Just remove the second argument will be okay\r\n```", "@ghostplant God, thank you so much! Got another problem now though for one of my programs.\r\n\r\nwhile_loop() got an unexpected keyword argument 'maximum_iterations'\r\n\r\nThis mentioned while_loop() is on line 2957 of the same backend.py file, and the code I have written is working on other systems with different versions of tensorflow. If you have some time could you please look at it? (I'm using LSTM to generate text and it needs to access the RNN part of the backend code, can't figure out why)\r\n\r\nThanks again!", "@SidJain1412 Are you considering just use keras 2.1.5? Since it will be a lot easier to get over these incompatibilities.\r\n\r\n```sh\r\npip install keras==2.1.5\r\n```", "@SidJain1412 Yes, keras 2.1.5 is exactly for tensorflow 1.4.1.", "If you prefer keras==2.2.0, you need to continue changing the `tensorflow_backend.py` below from:\r\n\r\n```sh\r\n            swap_memory=True,\r\n            maximum_iterations=input_length)\r\n```\r\ninto:\r\n```sh\r\n            swap_memory=True)\r\n```\r\n", "@ghostplant I didn't realise that keras>2.1.6 didn't support tensorflow<1.5 oh gosh everything's working now that I'm on keras 2.1.5 thank you so much man!"]}, {"number": 20117, "title": "tflite's  output is far from tf's", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N\r\n- **OS Platform and Distribution**: Linux 4.16.13-2-ARCH\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:'v1.8.0-3238-g52bf2fe0f6' 1.9.0-rc0\r\n- **Python version**:  3.6.5\r\n- **Bazel version (if compiling from source)**: v0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc-7.1\r\n- **CUDA/cuDNN version**: cuda-9.2,cuDNN-7.1\r\n- **GPU model and memory**: 16G\r\n- **Exact command to reproduce**:\r\n###\r\nI trained my model on mobilenet_v2-ssd with codes in models/research/object_detection/ ,export and convert the model from node `Preprocessor/sub` to `concat,concat_1` for the reason that tflite doesn't support some ops.\r\n**With the same input image and same preprocessing** I print out the values of `concat_` whose shape is [1,1917,2] (there is only one ground truth class in my dataset,so the last dimension is 1+1 == 2)\r\nthe tf's output is `[[[8.46199608 -9.81417]...`,but the tflite's is `[[[17.357746 -17.360010]...`\r\nThe only different obviously I can find is **the training network used  `FusedBatchNorm` \r\nbut in the exported tflite model the `FusedBatchNorm` was replaced by `{'fused_activation_function': 'NONE'} | ADD (opcode=0)`**\r\nDoes this differ care? Anyone help?\r\n\r\n\r\n\r\n", "comments": ["@aselle -- mind taking a look?", "Nagging Assignees @aselle, @achowdhery: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The instructions for support are coming soon. Thanks for your patience", "Please follow the blog post for end-to-end object detection training and deployment with Tensorflow Lite:\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n", "I have noticed that you just updated  an `export_tflite_ssd_graph.py` and I'm trying on it.Thanks for your work.", "I found that quantized mobilenet_v1 TFLite model is different from non-quantized TFLite model.\r\nnon-quantized uses `RELU6` fused_activation_function\r\nbut quantized uses `NONE` - which is actually not true because conv2 output tensors min/max is 0/6 - so, they actually use `RELU6`\r\nWhy quantized model metadata says `'fused_activation_function': 'NONE'`???\r\n\r\nnon-quantized model: https://pivovaa-us-west-1.s3-us-west-1.amazonaws.com/models/mobilenet_v1_0.75_224.html\r\n\r\nquantized model: https://pivovaa-us-west-1.s3-us-west-1.amazonaws.com/models/mobilenet_v1_0.75_224_quant.html\r\n", "ok, got it. instead of `RELU6` quantized model uses  `Requantize_32to8` - where I can explicitly set new min/max"]}, {"number": 20116, "title": "TensorFlow eager execution API give a wrong answer for some function", "body": "Why TensorFlow eager execution API give a wrong answer for this function?\r\n``` python\r\ndef dp1_f1(x):\r\n    return 64*x*(1-x)*(math.pow((1-2*x),2) )*math.pow((1-8*x+8*x*x), 2)\r\n```\r\nI want to get dy/dx value. I can get this value by numeric method just as below:\r\n``` python\r\ndef dp_numeric_diff(x):\r\n    delta_x = 0.0001\r\n    return (dp1_f1(x+delta_x)-dp1_f1(x))/delta_x\r\n```\r\nI use TensorFlow eager execution API to calculate this value:\r\n``` python\r\ndef dp_ad_tfe(x):\r\n    tf.enable_eager_execution()\r\n    tfe = tf.contrib.eager\r\n    grad_lx = tfe.gradients_function(dp1_f1)\r\n    x = 3.0\r\n    y = dp1_f1(x)\r\n    rst = grad_lx(x)\r\n    return y, rst[0]\r\n```\r\nI call this function with code below:\r\n``` python\r\nnumeric_diff = dp_numeric_diff(x)\r\nprint('Numeric method\uff1a{0}'.format(numeric_diff))\r\nv, d = dp_ad_tfe(x)\r\nprint('TFE\uff1a{0}'.format(d))\r\n```\r\nIt will display something like this:\r\n```\r\nNumeric method\uff1a-75290405.66440672\r\nTFE\uff1a-19208000.0\r\n```\r\nI am sure that the numeric method is right. What's wrong with my TensorFlow eager execution code? By the way the same TensorFlow eager execution code can get correct answer for simple function like x^2.", "comments": ["If I change math.pow function with this one then TensorFlow eager execution will give the right answer:\r\n``` python\r\ndef f3(x, n):\r\n    rst = 1\r\n    for i in range(n):\r\n        rst *= x\r\n    return rst\r\n```\r\nIt seems that TensorFlow eager execution API can't deal with math.pow function. I had tested tf.pow and numpy.power function. I found that TensorFlow eager execution API can't cope with this kind of functions as well.", "`tf.contrib.eager.gradients_function` can only differentiate through TensorFlow operations.\r\nSo it can't differentiate through `math.pow` but it can through `tf.pow`. Could you elaborate on which version of TensorFlow you're using? I tried the following on TensorFlow 1.8 and it seems to be correct:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\ndef dp1_f1(x):\r\n    return 64*x*(1-x)*(tf.pow((1-2*x),2) )*tf.pow((1-8*x+8*x*x), 2)\r\n\r\ndef dp_numeric_diff(x):\r\n    delta_x = 0.0001\r\n    return (dp1_f1(x+delta_x)-dp1_f1(x))/delta_x\r\n\r\ngrad = tf.contrib.eager.gradients_function(dp1_f1)\r\n\r\nprint(dp_numeric_diff(3.0).numpy()) # Prints -75300000.0\r\nprint(grad(3.0)[0].numpy())         # Prints -75279680.0\r\n```", "Thanks! @asimshankar My TensorFlow version is 1.9rc0 CPU version. I had git clone the latest source code of TensorFlow and compiled the CPU version.", "@yt7589 - do you see the same issue if you use the release binaries of 1.9.0-rc0? (Because I don't :)\r\nIf not, then perhaps something is fishy with the process you're using to build from source?", "@asimshankar I had not used binary release of 1.9.0-rc0. Because my CPU doesn't support AVX instruction set.", "Nagging Assignee @asimshankar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@yt7589 : Seems like the problem isn't reproducible on the release binaries of TensorFlow (last I tried was with the final 1.9.0 release).\r\n\r\nSo I'm at a bit of a loss without more detail on your setup. Thanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20115, "title": "AttributeError: module 'tensorflow' has no attribute 'test'", "body": "import tensorflow as tf\r\ndevice_name = tf.test.gpu_device_name()\r\nif device_name != '/device:GPU:0':\r\n  raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-32-d1680108c58e> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 device_name = tf.test.gpu_device_name()\r\n      3 if device_name != '/device:GPU:0':\r\n      4   raise SystemError('GPU device not found')\r\n      5 print('Found GPU at: {}'.format(device_name))\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'test'", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Tnanks for response\r\nTensorFlow 1.8\r\nWas running on collab\r\n\r\nThe issue solved already,\r\nprobably caused by \"failed to assign a backend GPU\"\r\n\r\nI've tried as well\r\n!pip uninstall tf-nightly\r\n!pip uninstall  tensorflow\r\nand then \r\n!pip install --upgrade tensorflow\r\n\r\n a magic happened and it works\r\n\r\n\r\n", "Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20114, "title": "hash table more types", "body": "", "comments": ["@qlzh727 Fixed. In order now.", "Ping @ysuematsu for review.", "Nudge @ebrevdo for review/approval.", "Nagging Reviewer%(reviewers): You have been added as a reviewer to this pull request. Please add your review or reassign. It has been  14ays with no activity and the `awaiting review` label has been applied.", "Registrations like this balloon the size of the tensorflow binary quite a bit, because they create many new template instantiations.  Can you restrict this PR to just the registrations that you need?", "Please resolve the merge conflict. Thanks.", "@qlzh727 ok", "Ping @ebrevdo for final approval.", "Looks like you removed some existing registrations, like all the string\nvalue ones\n\nOn Mon, Jul 30, 2018, 8:12 AM Qianli Scott Zhu <notifications@github.com>\nwrote:\n\n> Ping @ebrevdo <https://github.com/ebrevdo> for final approval.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20114#issuecomment-408898472>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim40aimTr_g0fIwki--vDyjlpG-soks5uLyJsgaJpZM4Us-DQ>\n> .\n>\n", "@ebrevdo Re-add", "@ebrevdo add int64_2_string back."]}, {"number": 20113, "title": "Failed to load the native TensorFlow runtime error in python..", "body": "As i try importing Tensorflow i get following import error. Someone please help me.\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Python34\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow' \r\n\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred: \r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow as tens\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Python34\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Python34\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow' `\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@rohanjhanepal are you still seeing this issue? Which version of the binary did you use?", "Yes @yifeif i am still having this issue.\r\nOS- Windows 10\r\nTensorflow installed from Pip install CMD\r\nTensorflow version -1.9.0\r\nGPU- Nvidia 940M with 2gb VRAM\r\nCuda version - 8.0\r\nPlease help .for the record i am using python 3.4 \r\n", "@rohanjhanepal do you have MSVCP140.DLL is installed on your system as mentioned in #8385?", "@rohanjhanepal Could you please confirm if this issue still exist ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to staleness. Please use the latest version for TensorFlow and test again. Feel free to open a new issue if it still persists. Thanks!"]}, {"number": 20112, "title": "[INTEL_MKL] Fix reorder creation failure in MklConcat op.", "body": "When inputs are all MKL layout, we wrongly use TF order's dims as Op mem's dims which will make reorder primitive creation failure.", "comments": ["Ping @rmlarsen for review."]}, {"number": 20111, "title": "ResnetV2 quantization", "body": "System information\r\n------------------------------------------\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.2\r\nTensorFlow installed from (source or binary):Anaconda python 3.6.5(conda install)\r\nTensorFlow version (use command below):1.8.0\r\nPython version: 3.6.5\r\nBazel version (if compiling from source): 0.7.0\r\nGCC/Compiler version (if compiling from source): 4.8.5\r\nCUDA/cuDNN version:cuda-9.0\r\nGPU model and memory: P40\r\nPhone: N/A\r\n\r\nDescribe the problem\r\n-----------------------------\r\nHi ~\r\n\r\nWhen I used API 'tf.contrib.quantize.experimental_create_training_graph' to add quantization nodes, it will match some layers**(not all convs layers)** to add quantization nodes. This will lead to some layers using float32, but some with float8(quantization bit is 8). When I use 'toco_convert', it just support inference_type: Currently must be {FLOAT, QUANTIZED_UINT8}. I think it need all layers to be FLOAT or QUANTIZED_UINT8. So it will raise this errors.\r\n\r\nError log\r\n------------------------\r\nF tensorflow/contrib/lite/toco/tooling_util.cc:1445] Array model/resnet_model/Relu, which is an input to the Conv operator producing the output array model/resnet_model/conv2d_1/Conv2D, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\n'\r\n\r\nSo I think **it is contradictory in the API**. pls help me, thank you.\r\n\r\nNetwork\r\n-----------------\r\nNetwork: ResNet20 \r\n\r\nTF version: r1.8(I find it dose not support --default_ranges_min and --default_ranges_max in the python api of r1.8. But in r1.9 it supports)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@suharshs   pls help me\r\n", "It seems your model is ResnetV2. The contrib.quantize tool doesn't support this yet, but it is something we plan to add. Particular the placement of Batch norm in ResnetV2 needs to be resolved.", "Duplicate of #20141 ?"]}, {"number": 20110, "title": "operators not supported by tflite: CAST, ExpandDims, FLOOR, Fill, Pow, RandomUniform, SPLIT, Stack, TensorFlowGreater, TensorFlowMaximum, TensorFlowMinimum, TensorFlowShape, TensorFlowSum, TensorFlowTile.", "body": "This is a feature request issue.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nCentOS 7\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nFrom `pip install tensorflow-gpu==1.6`\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\ntensorflow-gpu-1.6\r\n\r\n- **Python version**:\r\n\r\nPython 2.7.5\r\n \r\n- **Bazel version (if compiling from source)**: None\r\n\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n\r\n- **CUDA/cuDNN version**:  \r\n\r\nCUDA: 9.0 \r\n\r\n- **GPU model and memory**:\r\n\r\nNVIDIA GTX 1080Ti, 11178MiB\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```\r\ntoco --input_file=/tmp/mymodels/model_frozen.pb --output_file=/tmp/mymodels/converted_model.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,320,320,3 --input_array=input_images --output_array=output_num_array --inference_type=FLOAT --input_data_type=FLOAT\r\n```\r\n\r\n### Describe the problem\r\n\r\nI want to convert a trained model, which works on my GPU workstation, to a tflite model to work on Android phone. But I found that there are lots of operation not supported (see also the log below): **CAST, ExpandDims, FLOOR, Fill, Pow, RandomUniform, SPLIT, Stack, TensorFlowGreater, TensorFlowMaximum, TensorFlowMinimum, TensorFlowShape, TensorFlowSum, TensorFlowTile**. Do I need to provide customized operator for that (that is a lot of job...)? Or do newer version of toco support that? By the way, `toco` seems to not supported batch normalization either, see [this issue](https://github.com/tensorflow/tensorflow/issues/15336).\r\n\r\n### Source code / logs\r\n\r\nAfter running the command above, \r\n\r\n```\r\n2018-06-19 10:17:59.783022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: RandomUniform\r\n2018-06-19 10:17:59.920127: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Pow\r\n2018-06-19 10:17:59.964930: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 306 operators, 577 arrays (0 quantized)\r\n2018-06-19 10:17:59.971329: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 306 operators, 577 arrays (0 quantized)\r\n2018-06-19 10:18:02.355631: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 216 operators, 476 arrays (0 quantized)\r\n2018-06-19 10:18:02.360368: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 216 operators, 476 arrays (0 quantized)\r\n2018-06-19 10:18:02.364625: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 78643200 bytes, theoretical optimal value: 78643200 bytes.\r\n2018-06-19 10:18:02.365858: F tensorflow/contrib/lite/toco/tflite/export.cc:304] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: CAST, ExpandDims, FLOOR, Fill, Pow, RandomUniform, SPLIT, Stack, TensorFlowGreater, TensorFlowMaximum, TensorFlowMinimum, TensorFlowShape, TensorFlowSum, TensorFlowTile.\r\nAborted (core dumped)\r\n```", "comments": ["I met this error, and resolved by synchronizing tensorflow code.", "@wenhui7909 You mean that lastest tensorflow code has solved that ?", "Yes,\nBefore, I also met this error , about can't find slice and some other op.\nBut now, those are ok.\n\nDid u rebuild toco?\n\n2018-06-22 10:43 GMT+08:00 Yubin <notifications@github.com>:\n\n> @wenhui7909 <https://github.com/wenhui7909> You mean that lastest\n> tensorflow code has solved that ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20110#issuecomment-399303217>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AcPuIS2T97TcYdZOhn5uVnAw_GU27TADks5t_FnHgaJpZM4UswkF>\n> .\n>\n", "I will try rebuilding soon\n\nOn Fri, Jun 22, 2018 at 11:00 AM wenhui7909 <notifications@github.com>\nwrote:\n\n> Yes,\n> Before, I also met this error , about can't find slice and some other op.\n> But now, those are ok.\n>\n> Did u rebuild toco?\n>\n> 2018-06-22 10:43 GMT+08:00 Yubin <notifications@github.com>:\n>\n> > @wenhui7909 <https://github.com/wenhui7909> You mean that lastest\n> > tensorflow code has solved that ?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/20110#issuecomment-399303217\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AcPuIS2T97TcYdZOhn5uVnAw_GU27TADks5t_FnHgaJpZM4UswkF\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20110#issuecomment-399305657>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKfwFn4HiDIwGmce0pjaDUEv4pp01Wbxks5t_F24gaJpZM4UswkF>\n> .\n>\n", "@wenhui7909 I have met the same question,could you tell more about how to fix it?", "Update source code, and build toco as below:\r\nbazel build tensorflow/contrib/lite/toco:toco", "@wenhui7909 I download the tensorflow r1.7 from github again,and bazel build again,but I didn't fix it.", "Try latest version or tensorflow 1.8,\r\nand you also can check the op cc file is exist or not.\r\nMy error report that slice op is not supported, so check slice_op.cc.", "My error report that randomuniform,stack,tensorflowshape,floor\r\nTensorflow 1.8 don't support command line.I have try the 1.9 version.But the problem is still exist. \r\nMy model code don't appear to have these ops ,if I check the op not exist,it seems complex to fix it.", "Oh, I don't know why I can see the op files and you can't.\nsuch as, I can see floor.cc, and stack_ops.cc...\n\n2018-07-03 17:36 GMT+08:00 KB <notifications@github.com>:\n\n> My error report that randomuniform,stack,tensorflowshape,floor\n> Tensorflow 1.8 don't support command line.I have try the 1.9 version.But\n> the problem is still exist.\n> My model code don't appear to have these ops ,if I check the op not\n> exist,it seems complex to fix it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20110#issuecomment-402077840>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AcPuIYtBOmUuTRgSAzfPsUVd21jPcq7wks5uCzslgaJpZM4UswkF>\n> .\n>\n", "Would it be possible to provide your model ? It would help to understand if you could share what you are using those ops for.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Slice should now be supported (as well as shape and floor). Would you mind running the converter again using the latest build (1.10, or head)? randomuniform is not yet supported, and we don't yet have an ETA for that op. If you share the model we might be able to advise some intermediate workarounds.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]