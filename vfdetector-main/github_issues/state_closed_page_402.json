[{"number": 41881, "title": "tf doesn't use gpu to train ```tf.estimator``` model when I use ```tf.distribute.MirroredStrategy```", "body": "**System information**\r\nCentOS 7 \r\npip install tensorflow\r\n- TensorFlow version (use command below):\r\n2.2\r\n- Python version:\r\n3.7\r\n- CUDA/cuDNN version:\r\n10.1\r\n- GPU model and memory:\r\ngtx 1070 * 2\r\n\r\nYou can collect some of this information using our environment capture\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I use ```tf.distribute.MirroredStrategy``` , tf doesn't use gpu to train ```tf.estimator``` model . \r\n \r\n**Describe the expected behavior**\r\nWhen I use ```tf.distribute.MirroredStrategy``` , tf should use all gpu to train ```tf.estimator``` model . \r\n\r\n**code0**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef build_lr_estimator(feature_column_list:list,model_dir:str,config:dict):\r\n    \"\"\"\r\n    \"\"\"\r\n    estimator = tf.estimator.LinearClassifier(\r\n        feature_columns=feature_column_list,\r\n        optimizer=lambda: tf.keras.optimizers.Ftrl(\r\n            learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\r\n                initial_learning_rate=0.1, decay_steps=10000, decay_rate=0.96, staircase=False, name=None\r\n            ),\r\n            #learning_rate = 0.001,\r\n            l1_regularization_strength=0.001,\r\n        ),\r\n        model_dir = model_dir,\r\n        config=config,\r\n        warm_start_from=None,\r\n    )\r\n    return estimator\r\n```\r\n\r\n\r\n**code1**\r\n```\r\ntf.get_logger().setLevel('INFO')\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=None)\r\nmirrored_strategy_config = tf.estimator.RunConfig(\r\n    train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy\r\n)\r\ntrain_input_fn = make_input_fn(_type=\"train\")\r\nvalid_input_fn = make_input_fn(_type=\"valid\")\r\nfeature_column_list_path = FEATURE_COLUMN_LIST_PATH\r\nwith open(feature_column_list_path,\"rb\") as f:\r\n    feature_column_list = dill.load(f)\r\nlr_model = build_lr_estimator(\r\n    feature_column_list,\r\n    model_dir=os.path.join(_DIR,\"data\",\"model_callback\",\"train\"),\r\n    config=mirrored_strategy_config,\r\n    #config=None,\r\n)\r\nlr_model.train(train_input_fn)\r\n```\r\n**logs1**\r\n```\r\n2020-07-30 09:49:59.880322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-30 09:50:00.348416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.348838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 09:50:00.348913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.349283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 09:50:00.349457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 09:50:00.351300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-30 09:50:00.352937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-30 09:50:00.353153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-30 09:50:00.354902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-30 09:50:00.355745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-30 09:50:00.359075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-30 09:50:00.359186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.359604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.359985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.360355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.360690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-30 09:50:00.360993: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-07-30 09:50:00.366483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 4008000000 Hz\r\n2020-07-30 09:50:00.366764: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cb9dfa9590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-30 09:50:00.366787: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-30 09:50:00.439573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.441319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.441813: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cb9e033be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-30 09:50:00.441836: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2020-07-30 09:50:00.441846: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1070, Compute Capability 6.1\r\n2020-07-30 09:50:00.442648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.443005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 09:50:00.443066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.443404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 09:50:00.443436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 09:50:00.443451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-30 09:50:00.443465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-30 09:50:00.443479: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-30 09:50:00.443492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-30 09:50:00.443509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-30 09:50:00.443524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-30 09:50:00.443575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.443950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.444319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.444688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.445025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-30 09:50:00.445065: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 09:50:00.446166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-30 09:50:00.446193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1\r\n2020-07-30 09:50:00.446205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y\r\n2020-07-30 09:50:00.446213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N\r\n2020-07-30 09:50:00.446311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.446747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.447139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.447499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7553 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-07-30 09:50:00.447824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:00.452857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7554 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nINFO:tensorflow:Using config: {'_model_dir': '/home/zhaodachuan/data/lr_model_data/data/model_callback/train', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f7f69aafd90>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f7f69aafd90>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Calling model_fn.\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:540: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.add_weight` method instead.\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:144: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce: 25 all-reduces with algorithm = nccl, num_packs = 1\r\nWARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\r\nWARNING:tensorflow:AutoGraph could not transform <function _combine_distributed_scaffold.<locals>.<lambda> at 0x7f802c34c830> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n      lambda scaffold: scaffold.ready_op, args=(grouped_scaffold,))\r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py:96: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the iterator's `initializer` property instead.\r\nINFO:tensorflow:Graph was finalized.\r\n2020-07-30 09:50:53.797698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.798125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 09:50:53.798249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.798835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 09:50:53.798882: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 09:50:53.798898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-30 09:50:53.798914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-30 09:50:53.798929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-30 09:50:53.798943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-30 09:50:53.798959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-30 09:50:53.798974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-30 09:50:53.799025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.799470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.800099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.800500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.801124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-30 09:50:53.801166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-30 09:50:53.801177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1\r\n2020-07-30 09:50:53.801187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y\r\n2020-07-30 09:50:53.801194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N\r\n2020-07-30 09:50:53.801307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.801728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.802388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.802789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7553 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-07-30 09:50:53.802864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 09:50:53.803485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7554 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2020-07-30 09:50:55.361690: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nIdentity: GPU CPU XLA_CPU XLA_GPU\r\nSwitch: GPU CPU XLA_CPU XLA_GPU\r\nResourceSparseApplyFtrl: CPU\r\nCast: GPU CPU XLA_CPU XLA_GPU\r\nVarHandleOp: GPU CPU XLA_CPU XLA_GPU\r\nConst: GPU CPU XLA_CPU XLA_GPU\r\nVarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU\r\nAssignVariableOp: GPU CPU XLA_CPU XLA_GPU\r\nReadVariableOp: GPU CPU XLA_CPU XLA_GPU\r\nGatherV2: GPU CPU XLA_CPU XLA_GPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/single_visit_duration_bucketized/weights/Initializer/zeros (Const)\r\n  linear/linear_model/single_visit_duration_bucketized/weights (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/single_visit_duration_bucketized/weights/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/single_visit_duration_bucketized/weights/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/single_visit_duration_bucketized/weights/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/ReadVariableOp (ReadVariableOp)\r\n  linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup/axis (Const)\r\n  linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup (GatherV2)\r\n  zero_fraction/total_size/Size_18/ReadVariableOp (ReadVariableOp)\r\n  training/Ftrl/gradients/gradients/linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup_grad/Shape (Const)\r\n  training/Ftrl/gradients/gradients/linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup_grad/Cast (Cast)\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/Initializer/Const (Const)\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/Initializer/zeros (Const)\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/Ftrl/update_linear/linear_model/single_visit_duration_bucketized/weights/update_0/ResourceSparseApplyFtrl (ResourceSparseApplyFtrl) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_38 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_208 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_210 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_212 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_214 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables_1/VarIsInitializedOp_38 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_208 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_210 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_212 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_214 (VarIsInitializedOp)\r\n  save/Read_20/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/Read_70/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/Read_71/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/AssignVariableOp_40 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/ReadVariableOp_40 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/AssignVariableOp_140 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/ReadVariableOp_140 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/AssignVariableOp_142 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/ReadVariableOp_142 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  zero_fraction/total_zero/zero_count_18/linear/linear_model/single_visit_duration_bucketized/weights/_351 (Switch) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Func/zero_fraction/total_zero/zero_count_18/then/_346/input/_1207 (Identity) /job:localhost/replica:0/task:0/device:GPU:0\r\n  zero_fraction/total_zero/zero_count_18/else/_347/zero_fraction/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Func/zero_fraction/total_zero/zero_count_18/else/_347/input/_1212 (Identity) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.362210: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nSwitch: GPU CPU XLA_CPU XLA_GPU\r\nResourceSparseApplyFtrl: CPU\r\nCast: GPU CPU XLA_CPU XLA_GPU\r\nGatherV2: GPU CPU XLA_CPU XLA_GPU\r\nVarHandleOp: GPU CPU XLA_CPU XLA_GPU\r\nIdentity: GPU CPU XLA_CPU XLA_GPU\r\nConst: GPU CPU XLA_CPU XLA_GPU\r\nVarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU\r\nAssignVariableOp: GPU CPU XLA_CPU XLA_GPU\r\nReadVariableOp: GPU CPU XLA_CPU XLA_GPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/single_visit_duration_bucketized/weights/replica_1/Initializer/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/single_visit_duration_bucketized/weights/replica_1 (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/single_visit_duration_bucketized/weights/replica_1/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/single_visit_duration_bucketized/weights/replica_1/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/single_visit_duration_bucketized/weights/replica_1/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/replica_1/Initializer/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/replica_1 (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/replica_1/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/replica_1/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/accumulator/replica_1/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/replica_1/Initializer/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/replica_1 (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/replica_1/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/replica_1/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/single_visit_duration_bucketized/weights/linear/replica_1/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/ReadVariableOp (ReadVariableOp)\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup/axis (Const)\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup (GatherV2)\r\n  replica_1/zero_fraction/total_size/Size_18/ReadVariableOp (ReadVariableOp)\r\n  training_1/Ftrl/gradients/gradients/replica_1/linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup_grad/Shape (Const)\r\n  training_1/Ftrl/gradients/gradients/replica_1/linear/linear_model/linear/linear_model/linear/linear_model/single_visit_duration_bucketized/weighted_sum/embedding_lookup_sparse/embedding_lookup_grad/Cast (Cast)\r\n  training/Ftrl/Ftrl/update_linear/linear_model/single_visit_duration_bucketized/weights/update_1/ResourceSparseApplyFtrl (ResourceSparseApplyFtrl) /job:localhost/replica:0/task:0/device:GPU:1\r\n  report_uninitialized_variables/VarIsInitializedOp_39 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_209 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_211 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_213 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_215 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables_1/VarIsInitializedOp_39 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_209 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_211 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_213 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_215 (VarIsInitializedOp)\r\n  save/AssignVariableOp_41 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/ReadVariableOp_41 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_141 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/ReadVariableOp_141 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_143 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/ReadVariableOp_143 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_210 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_211 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/zero_fraction/total_zero/zero_count_18/linear/linear_model/single_visit_duration_bucketized/weights/replica_1/_604 (Switch) /job:localhost/replica:0/task:0/device:GPU:1\r\n  Func/replica_1/zero_fraction/total_zero/zero_count_18/then/_599/input/_1489 (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/zero_fraction/total_zero/zero_count_18/else/_600/zero_fraction/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  Func/replica_1/zero_fraction/total_zero/zero_count_18/else/_600/input/_1494 (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.362707: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nIdentity: GPU CPU XLA_CPU XLA_GPU\r\nSwitch: GPU CPU XLA_CPU XLA_GPU\r\nResourceSparseApplyFtrl: CPU\r\nCast: GPU CPU XLA_CPU XLA_GPU\r\nGatherV2: GPU CPU XLA_CPU XLA_GPU\r\nReadVariableOp: GPU CPU XLA_CPU XLA_GPU\r\nAssignVariableOp: GPU CPU XLA_CPU XLA_GPU\r\nMul: GPU CPU XLA_CPU XLA_GPU\r\nTruncatedNormal: GPU CPU XLA_CPU XLA_GPU\r\nAdd: GPU CPU XLA_CPU XLA_GPU\r\nVarHandleOp: GPU CPU XLA_CPU XLA_GPU\r\nFill: GPU CPU XLA_CPU XLA_GPU\r\nConst: GPU CPU XLA_CPU XLA_GPU\r\nVarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/uuid_embedding/embedding_weights/Initializer/truncated_normal/shape (Const)\r\n  linear/linear_model/uuid_embedding/embedding_weights/Initializer/truncated_normal/mean (Const)\r\n  linear/linear_model/uuid_embedding/embedding_weights/Initializer/truncated_normal/stddev (Const)\r\n  linear/linear_model/uuid_embedding/embedding_weights/Initializer/truncated_normal/TruncatedNormal (TruncatedNormal)\r\n  linear/linear_model/uuid_embedding/embedding_weights/Initializer/truncated_normal/mul (Mul)\r\n  linear/linear_model/uuid_embedding/embedding_weights/Initializer/truncated_normal (Add)\r\n  linear/linear_model/uuid_embedding/embedding_weights (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/uuid_embedding/embedding_weights/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/uuid_embedding/embedding_weights/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/uuid_embedding/embedding_weights/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/ReadVariableOp (ReadVariableOp)\r\n  linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup/axis (Const)\r\n  linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup (GatherV2)\r\n  zero_fraction/total_size/Size_22/ReadVariableOp (ReadVariableOp)\r\n  training/Ftrl/gradients/gradients/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup_grad/Shape (Const)\r\n  training/Ftrl/gradients/gradients/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup_grad/Cast (Cast)\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/Initializer/Const (Const)\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/Initializer/zeros/shape_as_tensor (Const)\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/Initializer/zeros/Const (Const)\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/Initializer/zeros (Fill)\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  training/Ftrl/Ftrl/update_linear/linear_model/uuid_embedding/embedding_weights/update_0/ResourceSparseApplyFtrl (ResourceSparseApplyFtrl) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_46 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_240 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_242 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_244 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_246 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables_1/VarIsInitializedOp_46 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_240 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_242 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_244 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_246 (VarIsInitializedOp)\r\n  save/Read_24/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/Read_78/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/Read_79/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/AssignVariableOp_48 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/ReadVariableOp_48 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/AssignVariableOp_156 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/ReadVariableOp_156 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/AssignVariableOp_158 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  save/ReadVariableOp_158 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  zero_fraction/total_zero/zero_count_22/linear/linear_model/uuid_embedding/embedding_weights/_387 (Switch) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Func/zero_fraction/total_zero/zero_count_22/then/_382/input/_1247 (Identity) /job:localhost/replica:0/task:0/device:GPU:0\r\n  zero_fraction/total_zero/zero_count_22/else/_383/zero_fraction/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Func/zero_fraction/total_zero/zero_count_22/else/_383/input/_1252 (Identity) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.363172: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nSwitch: GPU CPU XLA_CPU XLA_GPU\r\nResourceSparseApplyFtrl: CPU\r\nCast: GPU CPU XLA_CPU XLA_GPU\r\nGatherV2: GPU CPU XLA_CPU XLA_GPU\r\nVarHandleOp: GPU CPU XLA_CPU XLA_GPU\r\nIdentity: GPU CPU XLA_CPU XLA_GPU\r\nConst: GPU CPU XLA_CPU XLA_GPU\r\nVarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU\r\nAssignVariableOp: GPU CPU XLA_CPU XLA_GPU\r\nReadVariableOp: GPU CPU XLA_CPU XLA_GPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/uuid_embedding/embedding_weights/replica_1/Initializer/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/uuid_embedding/embedding_weights/replica_1 (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/uuid_embedding/embedding_weights/replica_1/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/uuid_embedding/embedding_weights/replica_1/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  linear/linear_model/uuid_embedding/embedding_weights/replica_1/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/replica_1/Initializer/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/replica_1 (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/replica_1/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/replica_1/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/accumulator/replica_1/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/replica_1/Initializer/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/replica_1 (VarHandleOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/replica_1/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/replica_1/Assign (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  training/Ftrl/linear/linear_model/uuid_embedding/embedding_weights/linear/replica_1/Read/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/ReadVariableOp (ReadVariableOp)\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup/axis (Const)\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup (GatherV2)\r\n  replica_1/zero_fraction/total_size/Size_22/ReadVariableOp (ReadVariableOp)\r\n  training_1/Ftrl/gradients/gradients/replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup_grad/Shape (Const)\r\n  training_1/Ftrl/gradients/gradients/replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_embedding_weights/embedding_lookup_sparse/embedding_lookup_grad/Cast (Cast)\r\n  training/Ftrl/Ftrl/update_linear/linear_model/uuid_embedding/embedding_weights/update_1/ResourceSparseApplyFtrl (ResourceSparseApplyFtrl) /job:localhost/replica:0/task:0/device:GPU:1\r\n  report_uninitialized_variables/VarIsInitializedOp_47 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_241 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_243 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_245 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables/VarIsInitializedOp_247 (VarIsInitializedOp) /job:localhost/replica:0/task:0/device:GPU:0\r\n  report_uninitialized_variables_1/VarIsInitializedOp_47 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_241 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_243 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_245 (VarIsInitializedOp)\r\n  report_uninitialized_variables_1/VarIsInitializedOp_247 (VarIsInitializedOp)\r\n  save/AssignVariableOp_49 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/ReadVariableOp_49 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_157 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/ReadVariableOp_157 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_159 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/ReadVariableOp_159 (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_218 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  save/AssignVariableOp_219 (AssignVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/zero_fraction/total_zero/zero_count_22/linear/linear_model/uuid_embedding/embedding_weights/replica_1/_640 (Switch) /job:localhost/replica:0/task:0/device:GPU:1\r\n  Func/replica_1/zero_fraction/total_zero/zero_count_22/then/_635/input/_1529 (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/zero_fraction/total_zero/zero_count_22/else/_636/zero_fraction/ReadVariableOp (ReadVariableOp) /job:localhost/replica:0/task:0/device:GPU:1\r\n  Func/replica_1/zero_fraction/total_zero/zero_count_22/else/_636/input/_1534 (Identity) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.363474: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/active_reading_period_indicator/active_reading_period_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/active_reading_period_indicator/active_reading_period_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/active_reading_period_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.363632: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/article_type_name_indicator/article_type_name_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/article_type_name_indicator/article_type_name_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/article_type_name_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.363780: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/channel_names_indicator/channel_names_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/channel_names_indicator/channel_names_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/channel_names_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.363947: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/isopen_push_indicator/isopen_push_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/isopen_push_indicator/isopen_push_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/isopen_push_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.364102: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/latest_area_indicator/latest_area_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/latest_area_indicator/latest_area_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/latest_area_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.364259: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/permanent_land_indicator/permanent_land_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/permanent_land_indicator/permanent_land_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/permanent_land_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.364404: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/phone_brand_newest_indicator/phone_brand_newest_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/phone_brand_newest_indicator/phone_brand_newest_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/phone_brand_newest_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.364625: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/source_name_indicator/source_name_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/source_name_indicator/source_name_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/source_name_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.364784: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.364952: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  linear/linear_model/linear/linear_model/linear/linear_model/visit_page_class_indicator/visit_page_class_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/visit_page_class_indicator/visit_page_class_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n  linear/linear_model/linear/linear_model/linear/linear_model/visit_page_class_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n2020-07-30 09:50:55.365386: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/active_reading_period_indicator/active_reading_period_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/active_reading_period_indicator/active_reading_period_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/active_reading_period_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.365537: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/article_type_name_indicator/article_type_name_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/article_type_name_indicator/article_type_name_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/article_type_name_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.365682: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/channel_names_indicator/channel_names_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/channel_names_indicator/channel_names_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/channel_names_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.365845: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/isopen_push_indicator/isopen_push_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/isopen_push_indicator/isopen_push_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/isopen_push_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.365990: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/latest_area_indicator/latest_area_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/latest_area_indicator/latest_area_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/latest_area_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.366147: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/permanent_land_indicator/permanent_land_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/permanent_land_indicator/permanent_land_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/permanent_land_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.366291: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/phone_brand_newest_indicator/phone_brand_newest_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/phone_brand_newest_indicator/phone_brand_newest_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/phone_brand_newest_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.366512: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/source_name_indicator/source_name_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/source_name_indicator/source_name_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/source_name_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.366668: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/uuid_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/uuid_embedding/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\n2020-07-30 09:50:55.366840: W tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n  /job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:1' supported_device_types_=[CPU] possible_devices_=[]\r\nLookupTableFindV2: CPU\r\nHashTableV2: CPU\r\nLookupTableImportV2: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/visit_page_class_indicator/visit_page_class_lookup/hash_table/hash_table (HashTableV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/visit_page_class_indicator/visit_page_class_lookup/hash_table/table_init/LookupTableImportV2 (LookupTableImportV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n  replica_1/linear/linear_model/linear/linear_model/linear/linear_model/visit_page_class_indicator/hash_table_Lookup/LookupTableFindV2 (LookupTableFindV2) /job:localhost/replica:0/task:0/device:GPU:1\r\n\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\r\nINFO:tensorflow:Saving checkpoints for 0 into /home/zhaodachuan/data/lr_model_data/data/model_callback/train/model.ckpt.\r\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\r\n2020-07-30 09:52:51.082541: W tensorflow/core/grappler/utils/graph_view.cc:832] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node {{node MultiDeviceIteratorFromStringHandle}}\r\n        .  Registered:  device='CPU'\r\n\r\n2020-07-30 09:52:51.083215: W tensorflow/core/grappler/utils/graph_view.cc:832] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node {{node MultiDeviceIteratorGetNextFromShard}}\r\n        .  Registered:  device='CPU'\r\n\r\n2020-07-30 09:52:51.089263: W tensorflow/core/grappler/utils/graph_view.cc:832] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node {{node MultiDeviceIteratorFromStringHandle}}\r\n        .  Registered:  device='CPU'\r\n\r\n2020-07-30 09:52:51.089670: W tensorflow/core/grappler/utils/graph_view.cc:832] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node {{node MultiDeviceIteratorGetNextFromShard}}\r\n        .  Registered:  device='CPU'\r\n\r\n2020-07-30 09:52:54.569784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\nINFO:tensorflow:loss = 1.3862944, step = 0\r\nINFO:tensorflow:global_step/sec: 16.243\r\nINFO:tensorflow:loss = 0.0002972064, step = 100 (6.157 sec)\r\nINFO:tensorflow:global_step/sec: 28.2076\r\n```\r\n**code2**\r\n```\r\ntf.get_logger().setLevel('INFO')\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=None)\r\nmirrored_strategy_config = tf.estimator.RunConfig(\r\n    train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy\r\n)\r\ntrain_input_fn = make_input_fn(_type=\"train\")\r\nvalid_input_fn = make_input_fn(_type=\"valid\")\r\nfeature_column_list_path = FEATURE_COLUMN_LIST_PATH\r\nwith open(feature_column_list_path,\"rb\") as f:\r\n    feature_column_list = dill.load(f)\r\nlr_model = build_lr_estimator(\r\n    feature_column_list,\r\n    model_dir=os.path.join(_DIR,\"data\",\"model_callback\",\"train\"),\r\n    #config=mirrored_strategy_config,\r\n    config=None,\r\n)\r\nlr_model.train(train_input_fn)\r\n```\r\n**logs2**\r\n```\r\n2020-07-30 10:03:36.371458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-30 10:03:36.850104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.850514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 10:03:36.850589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.851184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 10:03:36.851363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 10:03:36.852982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-30 10:03:36.854618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-30 10:03:36.854820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-30 10:03:36.856409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-30 10:03:36.857170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-30 10:03:36.860369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-30 10:03:36.860521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.861002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.861626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.862052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.862639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-30 10:03:36.862990: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-07-30 10:03:36.868372: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 4008000000 Hz\r\n2020-07-30 10:03:36.868708: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558ebdee35d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-30 10:03:36.868726: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-30 10:03:36.942375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.944161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.944639: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558ebdf6dc30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-30 10:03:36.944661: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2020-07-30 10:03:36.944670: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1070, Compute Capability 6.1\r\n2020-07-30 10:03:36.945471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.945825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 10:03:36.945886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.946224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 10:03:36.946255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 10:03:36.946270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-30 10:03:36.946283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-30 10:03:36.946296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-30 10:03:36.946313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-30 10:03:36.946327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-30 10:03:36.946341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-30 10:03:36.946390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.946760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.947164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.947604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.947981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-30 10:03:36.948020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 10:03:36.951940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-30 10:03:36.951972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1\r\n2020-07-30 10:03:36.951983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y\r\n2020-07-30 10:03:36.951991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N\r\n2020-07-30 10:03:36.952104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.952533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.953239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.953644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7553 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-07-30 10:03:36.954039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:03:36.954639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7554 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:Not using Distribute Coordinator.\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_model_dir': '/home/zhaodachuan/data/lr_model_data/data/model_callback/train', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nINFO:tensorflow:Calling model_fn.\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:540: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.add_weight` method instead.\r\nWARNING:tensorflow:From /home/zhaodachuan/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:144: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2020-07-30 10:04:01.288559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.288991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 10:04:01.289118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.289702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.7715GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-30 10:04:01.289749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 10:04:01.289769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-30 10:04:01.289786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-30 10:04:01.289803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-30 10:04:01.289832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-30 10:04:01.289852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-30 10:04:01.289868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-30 10:04:01.289925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.290325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.290944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.291342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.291952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-07-30 10:04:01.291995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-30 10:04:01.292006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1\r\n2020-07-30 10:04:01.292014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y\r\n2020-07-30 10:04:01.292022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N\r\n2020-07-30 10:04:01.292118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.292544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.293155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.293526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7553 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-07-30 10:04:01.293599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 10:04:01.294158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7554 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\r\nINFO:tensorflow:Saving checkpoints for 0 into /home/zhaodachuan/data/lr_model_data/data/model_callback/train/model.ckpt.\r\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\r\n2020-07-30 10:05:04.494990: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\nINFO:tensorflow:loss = 0.6931472, step = 0\r\nINFO:tensorflow:global_step/sec: 55.9369\r\nINFO:tensorflow:loss = 3.0821118e-06, step = 100 (1.788 sec)\r\nINFO:tensorflow:global_step/sec: 215.479\r\nINFO:tensorflow:loss = 2.562792e-05, step = 200 (0.464 sec)\r\n```", "comments": ["@DachuanZhao,\r\nCould you please provide the complete code to reproduce the issue reported here? On running the code snippet you have given, I am facing an error stating `NameError: name 'make_input_fn' is not defined`.\r\n\r\n\r\n>INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\r\n\r\nAlso, looking at the log you have given, it seems that both the GPUs are being used. To confirm this please use the [profiling tool](https://www.tensorflow.org/guide/profiler#overview_page) and check the GPU usage. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@DachuanZhao how did you solve the issue? same problem here. ", "> @DachuanZhao how did you solve the issue? same problem here.\r\n\r\nI don't use ```tf.estimator``` anymore . I use ```tf.keras``` instead .", "@DachuanZhao  - How does moving to Keras solved this? \r\nThe LookupTableFindV2 et al runs on the GPU now? what's the performance gain of that?\r\n", "> @DachuanZhao - How does moving to Keras solved this?\r\n> The LookupTableFindV2 et al runs on the GPU now? what's the performance gain of that?\r\n\r\nI rewrite the code with ```tf.keras``` . The performance is same ."]}, {"number": 41880, "title": "Extended test cases and added eager mode tests for compression ops", "body": "This PR extends the kernel testing for `tensorflow/python/data/experimental/ops/compression_ops.py` by\r\n\r\n- [x] extending the current test inputs.\r\n- [x] adding eager mode based tests.", "comments": ["@aaudiber , restructured the test combinations. ", "@kvignesh1420 can you please check below lint errors \r\n\r\n`FAIL: Found 2 non-allowlisted pylint errors:\r\ntensorflow/python/data/experimental/kernel_tests/compression_ops_test.py:85: [C0301(line-too-long), ] Line too long (83/80)\r\n\r\ntensorflow/python/data/experimental/kernel_tests/compression_ops_test.py:98: [C0301(line-too-long), ] Line too long (83/80)`", "@gbaned , seems like an unrelated windows bazel issue. How can we move forward with this? Thanks. ", "@kvignesh1420  Sorry for the delay. Here are the internal errors, can you please verify ?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  /tensorflow/python/framework/test_combinations.py\", line 315, in decorated\r\n    execute_test_method()\r\n  File \"//tensorflow/python/framework/test_combinations.py\", line 298, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"//tensorflow/python/data/experimental/kernel_tests/compression_ops_test.py\", line 93, in testCompression\r\n    compressed = compression_ops.compress(element)\r\n  File \"//tensorflow/python/data/experimental/ops/compression_ops.py\", line 35, in compress\r\n    tensor_list = structure.to_tensor_list(element_spec, element)\r\n  File \"//tensorflow/python/data/util/structure.py\", line 377, in to_tensor_list\r\n    element_spec, element)\r\n  File \"//tensorflow/python/data/util/structure.py\", line 326, in _to_tensor_list_helper\r\n    reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\r\n  File \"//tensorflow/python/data/util/structure.py\", line 323, in reduce_fn\r\n    return encode_fn(state, spec, component)\r\n  File \"//tensorflow/python/data/util/structure.py\", line 376, in <lambda>\r\n    lambda state, spec, component: state + spec._to_tensor_list(component),\r\n  File \"//tensorflow/python/ops/ragged/ragged_tensor.py\", line 2342, in _to_tensor_list\r\n    return [value._to_variant(batched_input=False)]\r\n  File \"//tensorflow/python/ops/ragged/ragged_tensor.py\", line 1929, in _to_variant\r\n    self.nested_row_splits, self.flat_values, batched_input, name)\r\n  File \"//tensorflow/python/ops/gen_ragged_conversion_ops.py\", line 406, in ragged_tensor_to_variant\r\n    name=name, ctx=_ctx)\r\n  File \"//tensorflow/python/ops/gen_ragged_conversion_ops.py\", line 448, in ragged_tensor_to_variant_eager_fallback\r\n    attrs=_attrs, ctx=ctx, name=name)\r\n  File \"//tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n    raise e\r\n  File \"//tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: RaggedConstant/Const:0\r\n```", "> @kvignesh1420 Sorry for the delay. Here are the internal errors, can you please verify ?\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   /tensorflow/python/framework/test_combinations.py\", line 315, in decorated\r\n>     execute_test_method()\r\n>   File \"//tensorflow/python/framework/test_combinations.py\", line 298, in execute_test_method\r\n>     test_method(**kwargs_to_pass)\r\n>   File \"//tensorflow/python/data/experimental/kernel_tests/compression_ops_test.py\", line 93, in testCompression\r\n>     compressed = compression_ops.compress(element)\r\n>   File \"//tensorflow/python/data/experimental/ops/compression_ops.py\", line 35, in compress\r\n>     tensor_list = structure.to_tensor_list(element_spec, element)\r\n>   File \"//tensorflow/python/data/util/structure.py\", line 377, in to_tensor_list\r\n>     element_spec, element)\r\n>   File \"//tensorflow/python/data/util/structure.py\", line 326, in _to_tensor_list_helper\r\n>     reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\r\n>   File \"//tensorflow/python/data/util/structure.py\", line 323, in reduce_fn\r\n>     return encode_fn(state, spec, component)\r\n>   File \"//tensorflow/python/data/util/structure.py\", line 376, in <lambda>\r\n>     lambda state, spec, component: state + spec._to_tensor_list(component),\r\n>   File \"//tensorflow/python/ops/ragged/ragged_tensor.py\", line 2342, in _to_tensor_list\r\n>     return [value._to_variant(batched_input=False)]\r\n>   File \"//tensorflow/python/ops/ragged/ragged_tensor.py\", line 1929, in _to_variant\r\n>     self.nested_row_splits, self.flat_values, batched_input, name)\r\n>   File \"//tensorflow/python/ops/gen_ragged_conversion_ops.py\", line 406, in ragged_tensor_to_variant\r\n>     name=name, ctx=_ctx)\r\n>   File \"//tensorflow/python/ops/gen_ragged_conversion_ops.py\", line 448, in ragged_tensor_to_variant_eager_fallback\r\n>     attrs=_attrs, ctx=ctx, name=name)\r\n>   File \"//tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n>     raise e\r\n>   File \"//tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n>     inputs, attrs, num_outputs)\r\n> TypeError: An op outside of the function building code is being passed\r\n> a \"Graph\" tensor. It is possible to have Graph tensors\r\n> leak out of the function building context by including a\r\n> tf.init_scope in your function building code.\r\n> For example, the following function will fail:\r\n>   @tf.function\r\n>   def has_init_scope():\r\n>     my_constant = tf.constant(1.)\r\n>     with tf.init_scope():\r\n>       added = my_constant * 2\r\n> The graph tensor has name: RaggedConstant/Const:0\r\n> ```\r\n\r\nWill check. Thanks.", "@gbaned I tried removing the `combinations` and `test_base` based test setup for `RaggedTensors`. Can you check the internal build with this style and let me know?\r\n\r\ncc: @aaudiber @jsimsa @rthadur ", "@aaudiber  I reverted the test cases to as they were before. (using `combinations`). Please check.", "@aaudiber  I added the `v2_eager_only_combinations` function which handles our case. Can be helpful for future tests as well. Please check.", "@aaudiber, used the name: `v2_eager_only_objects` to keep it consistent with the test base combination. Lint issue has been fixed.", "@gbaned "]}, {"number": 41879, "title": "Model was stuck when build model with inputs and outputs based on transformer.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MACOS 10.15.5 Catalina\r\n- TensorFlow version: 2.2 and 2.3\r\n- Python version: 3.6\r\n\r\n**Describe the problem**\r\nI want to build a transformer (according to this [notebook](https://www.tensorflow.org/tutorials/text/transformer)) model in non-eager execution, and everything runs smoothly only when building model with the tf.keras.layers.Input and its corresponding outputs, the whole program is stuck. By the way, when executing the for loop in block 10, it consumes extremely much time.\r\n\r\nThere are no error messages about this phenomenon, totally have no idea about what leads to this.\r\n\r\nI have replicated this issue [here](https://colab.research.google.com/drive/1sGkTwMUgOUSlfbYmyuEOaQK7inECuCp9?usp=sharing) in google colab.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI initialized an Input `tf.keras.layers.Input` and generate outputs with transformer model, then use the input and outputs to build a model but the whole program is stuck.\r\n", "comments": ["@Askfk \r\nI ran the code shared and do not face any errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b521db122ab3946669c05f4c3fd8472d/untitled310.ipynb)", "@Saduf2019 \r\nSorry I made some changes for testing which part leads to the stuck in the notebook, so based on these changes it can run without problems. In the [original notebook](https://colab.research.google.com/drive/1sGkTwMUgOUSlfbYmyuEOaQK7inECuCp9?usp=sharing) the problems are still there that the program will be stuck in the for loop.", "@Askfk \r\nI re-ran the above colab and do not face any errors.", "Thank you for your efforts, but the model is still stuck in the for loop (it costs too much time in every iteration which is abnormal).", "Hi @Askfk, are you still facing this problem? Can you try commenting out `tf.compat.v1.disable_eager_execution()`? In TF2, you should make use of @tf.function to run your code in graph mode, instead of disabling eager execution. [Linking to the tf.function guide here for reference.](https://www.tensorflow.org/guide/function)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41878, "title": "tf.keras.losses.binary_crossentropy gives worse result than calling 'binary_crossentropy'", "body": "**QUESTION 1:** Why does calling binary_crossentropy by its name with a string 'binary_crossentropy' gives different (better) results than calling, the same(?), tf.keras.losses.binary_crossentropy function inside a custom loss function?\r\n\r\n**QUESTION 2:** What am I doing wrong?\r\n\r\n**PROBLEM:** I'm trying to solve a mystery of different results I get when I use:\r\n```python\r\ndef bc_custom(y_true, y_pred): return tf.keras.losses.binary_crossentropy(y_true, y_pred)\r\nmodel1.compile(optimizer=tf.compat.v2.optimizers.Adam(learning_rate=LEARNING_RATE, loss = bc_custom, metrics=['binary_accuracy'])\r\n```\r\nand \r\n```python\r\nmodel2.compile(optimizer=tf.compat.v2.optimizers.Adam(learning_rate=LEARNING_RATE, loss = 'binary_crossentropy', metrics=['binary_accuracy'])\r\n```\r\nBasically: \r\n\r\n - the difference is in calling binary_crossentropy inside a custom function (model1) and the same function by its name with a string (model2),\r\n - following https://stackoverflow.com/a/57331394/5536388 advice, I set metrics to binary_accuracy,\r\n - both models use the same dataset which may only differ after being shuffled in the training process,\r\n - the models solve a multilabel classification problem. The final layer uses the sigmoid activation function.\r\n\r\n**RESULTS:**\r\nI made several tests and each time model1:\r\n\r\n - is being taught much slower:\r\n\r\n epoch, accuracy, loss, accuracy_val, loss_val\r\n    \r\n    0,0.01571397,0.08330947177696847,0.08565965,0.06395021689980937\r\n    1,0.14730678,0.04977951637815227,0.24956022,0.039584608338986936 \r\n    2,0.2366954,0.98736,0.2366954,0.03144164173531004,0.27801147,0.99116695,0.27801147,0.022898457557974424\r\n\r\nvs model2\r\n\r\n    0,0.98126763,0.08199372373072011,0.9798808,0.061917345579108135\r\n    1,0.98271894,0.0477434622772295,0.98628557,0.03482157470263099\r\n    2,0.9884866,0.9884866,0.24891642,0.028994456771504733,0.9914624,0.9914624,0.27778202,0.022188534344907987\r\n\r\n - model1 gives worse results, at least in the beginning (haven't waited till the final results):\r\n[![model1 results after epoch 0][1]][1]\r\nresults of model1 after epoch 0\r\n[![model2 results after epoch 0][2]][2]\r\nresults of model2 after epoch 0\r\n\r\n - what's even more interesting, when a smaller dataset, 'binary_crossentropy' still gives some prediction results while the custom function returns 0 probability across all labels :|. \r\n\r\nThe expected results are one-point-wide straight-to-top lines in several positions (10, 30, 50, 110, 140, 210, 230) as the model2 is clearly going to and, in the end, reaches the correct answer in 98% of cases. \r\n\r\n\r\n  [1]: https://i.stack.imgur.com/llgpH.png\r\n  [2]: https://i.stack.imgur.com/eFg5l.png", "comments": ["@chrismaliszewski \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also include your TensorFlow version.\r\nRequest you to share colab link or simple stand alone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram, which issue template should I use in your opinion? I wasn't sure since I don't think it's a bug nor any other from the list of templates. That's why I created \"Other issues\". \r\n\r\nIt's impossible for me to share the data I have since it's too big (39GB). My input is 256x50x3 x ~120k images. My model is VGG16 without top layers + flatten + 3x dense/dropout (8192, 0.5) + dense (256, sigmoid). \r\n\r\nI use TF ~~2.1.0~~ **2.2.0** on Windows in Anaconda. \r\n\r\nPS. I'll try creating a simple example with some other data. ", "So here is a piece of code that briefly presents what I do: https://colab.research.google.com/drive/1aTG3ApEUmr9UyhSUoALY-_wEgTTOHlaz?usp=sharing\r\n\r\nSo:\r\n\r\n1. loading data, preparing data (I preprocess my arrays for VGG16), divided it into train-test datasets 90:10 + ~10 extra for checking predictions,\r\n2. Training.\r\n3. (not shown in the code above) Checking predictions with each epoch's end. \r\n\r\nUnfortunately, the problem does not appear in the above code but it does in my case.\r\n\r\n**QUESTION:** Should there be any difference between calling 'binary_crossentropy' or tf.keras.losses.binary_crossentropy? What I mean is, does 'binary_crossentropy' calls the function without anything else or is there anything else done on the way?", "@chrismaliszewski Is this still an issue for you? I ran your code with recent `tf-nightly` and I don't see much difference between two models. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/500f5581afe9a1468a30383d43480d5f/-binary_accuracy-vs-tf-keras-losses-binary_crossentropy.ipynb).\r\n\r\nPlease note that I reduced training data and didn't consider `validation_data` to minimize computational time. Thanks!\r\n\r\nPlease verify and close the issue if this was resolved for you. Thanks!", "Hi. No, the issue still exists. I've decided to move on and I'm transferring my code to Pytorch since it is another problem on the list with TF I have and I'm hoping the problems don't exist there. Thank you for your concern. I'm closing the issue.", "Do you use sample_weights?", "No, training from scratch in both cases, using a custom loss function which returns the results from tf.keras.losses.binary_crossentropy and 'binary_crossentropy'. "]}, {"number": 41877, "title": "TFLiteConverter: ConvertError: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types", "body": "**System information**\r\n- OS Platform and Distribution: Manjaro running Ubuntu 18.04 under Docker (using the `tensorflow/tensorflow:2.2.0-gpu-jupyter` image)\r\n- TensorFlow installed from (source or binary): Using official docker image, actually not sure if it's source or binary.\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport sys\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef usage():\r\n    proggie = os.path.basename(sys.argv[0])\r\n    print(f\"\"\"\\\r\nUsage: {proggie} <input_saved_model_dir> <output>\r\n\r\nExample: {proggie} output_inference_graph_v1.pb/saved_model converted_model.tflite\"\"\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) != 3 or '-h' in sys.argv or '--help' in sys.argv:\r\n        usage()\r\n        sys.exit(1)\r\n\r\n    saved_model_dir, outfile = sys.argv[1:]\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n    tflite_model = converter.convert()\r\n    with tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n    print(f\"{outfile} written\")\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\ndocker exec -ti tf2 \\\r\n        bash -c \\\r\n        \"cd /tf/routespotter/training/routesv3/ && python /tf/routespotter/scripts/convert_to_tflite.py trained-inference-graphs/output/saved_model trained-inference-graphs/output/converted_model.tflite\"\r\n2020-07-29 21:30:35.933784: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-29 21:30:37.541131: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-29 21:30:37.554309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.835GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-29 21:30:37.554340: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-29 21:30:37.555786: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-07-29 21:30:37.557191: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-29 21:30:37.557467: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-29 21:30:37.558961: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-29 21:30:37.559781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-29 21:30:37.562688: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-29 21:30:37.563390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-07-29 21:30:37.563659: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-29 21:30:37.569752: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3301490000 Hz\r\n2020-07-29 21:30:37.571232: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xaf61e40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-29 21:30:37.571276: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-29 21:30:37.718561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xafcdab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-29 21:30:37.718596: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2020-07-29 21:30:37.719275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.835GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-29 21:30:37.719308: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-29 21:30:37.719337: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-07-29 21:30:37.719357: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-29 21:30:37.719375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-29 21:30:37.719394: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-29 21:30:37.719406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-29 21:30:37.719422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-29 21:30:37.720408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-07-29 21:30:37.720450: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-29 21:30:38.075379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-29 21:30:38.075416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-07-29 21:30:38.075422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-07-29 21:30:38.076193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7091 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-07-29 21:30:47.046860: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-07-29 21:30:47.046987: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-29 21:30:47.047691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.835GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-07-29 21:30:47.047721: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-29 21:30:47.047745: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-07-29 21:30:47.047755: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-29 21:30:47.047765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-29 21:30:47.047775: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-29 21:30:47.047788: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-29 21:30:47.047798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-29 21:30:47.048149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-07-29 21:30:47.048174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-29 21:30:47.048181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-07-29 21:30:47.048188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-07-29 21:30:47.048560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7091 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n2020-07-29 21:30:47.229748: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-07-29 21:30:47.229790: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 4275 nodes (3787), 4874 edges (4379), time = 107.259ms.\r\n2020-07-29 21:30:47.229795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 3.916ms.\r\n2020-07-29 21:30:51.658251: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-07-29 21:30:51.658289: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\nloc(\"Func/StatefulPartitionedCall/input/_0\"): error: requires all operands and results to have compatible element types\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py\", line 199, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tf/routespotter/scripts/convert_to_tflite.py\", line 22, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 900, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 633, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py\", line 574, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nCustom trained model based on [ssd_resnet50_v1_fpn_640x640_coco17_tpu-8](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz) by following the [tensorflow-object-detection-api-tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io). I know my custom trained model works because I've run it through the [object_detection_tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb) using test images from my dataset and I get successful detections. Not sure if I'm just using an unsupported operand for tflite? Having a hard time deciphering the error message...\r\n\r\n```\r\nhttps://drive.google.com/file/d/1O_c12RAALroTomEvDaH8gwukYzNZDegq/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\n- Conversion process fails\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title. :x:  \r\n\r\n**Any other info / logs**\r\n\r\nSee traceback above.", "comments": ["@mgalgs,\r\nLooks like the issue has been resolved in the latest TF-nightly. I was able to convert the model successfully after adding the below lines to the code. \r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir,signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\nPlease check [this gist](https://colab.research.google.com/gist/amahendrakar/36de658face01b4d232f25352bca1655/41877-tf-nightly.ipynb) for reference. Thanks!", "@amahendrakar awesome, that worked! Getting an error when I actually try to load the model on the mobile side but I think that might be a separate issue... I'll open another issue after some more debugging. Thanks again.", "Same issue on `ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8` with tf v2.3\r\n\r\n![image](https://user-images.githubusercontent.com/16943353/90090164-ef999580-dd55-11ea-9f89-2fd416c5bcde.png)\r\n\r\n", "After installing tf-nightly, another issue occurs:\r\n\r\n![image](https://user-images.githubusercontent.com/16943353/90093579-88ccaa00-dd5e-11ea-8f9e-a95b689b0cc0.png)\r\n\r\nIt seems like I can't import tf-nightly successfully.\r\n\r\nDevice info: Nvidia V100 on Azure.\r\n\r\nDriver info: \r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-PCIE...  On   | 00000001:00:00.0 Off |                    0 |\r\n| N/A   27C    P0    24W / 250W |      0MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n", "@DjangoPeng,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "The issue still exists in TF 2.3.0. Is anyone still facing the same problem? ", "@mgalgs @smohan10 For converting SSD TF2 models to TFLite, could you see [this comment](https://github.com/tensorflow/models/issues/9033#issuecomment-686628182)? We are tracking TFLite support for TF2 ODAPI models there."]}, {"number": 41876, "title": "TensorFlowLiteC and TensorFlowLiteSelectTfOps expose duplicate symbols on iOS", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 11 Pro Max\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.3.0\r\n\r\n**Describe the problem**\r\n\r\nWhen attempting to use TensorFlow Lite on iOS [with `TensorFlowLiteSelectTfOps` as per the documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/TensorFlowLiteSelectTfOps.md), the linker raises an error that it found `17 duplicate symbols for architecture arm64`:\r\n\r\n```\r\nduplicate symbol '_TfLiteDelegateCreate' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteFloatArrayCreate' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteFloatArrayFree' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteFloatArrayGetSizeInBytes' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayCopy' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayCreate' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayEqual' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayEqualsArray' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayFree' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteIntArrayGetSizeInBytes' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteQuantizationFree' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteSparsityFree' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorDataFree' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorFree' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorRealloc' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTensorReset' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nduplicate symbol '_TfLiteTypeGetName' in:\r\n    [...snip...]/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(common_1fe599348340cdd31b92895a3a36b237.o)\r\n    [...snip...]/Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC\r\nld: 17 duplicate symbols for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nRan `pod install` with the following lines in my `Podfile`, as per documentation:\r\n```\r\npod 'TensorFlowLiteObjC'\r\npod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly.20200729'\r\nuse_frameworks!\r\n```\r\n\r\nAttempted to build the app in Xcode.\r\n\r\n`grep Tensor Podfile.lock` returns:\r\n```\r\n  - TensorFlowLiteC (2.3.0):\r\n    - TensorFlowLiteC/Core (= 2.3.0)\r\n  - TensorFlowLiteC/Core (2.3.0)\r\n  - TensorFlowLiteObjC (2.3.0):\r\n    - TensorFlowLiteC (= 2.3.0)\r\n  - TensorFlowLiteSelectTfOps (0.0.1-nightly.20200729)\r\n  - TensorFlowLiteObjC\r\n  - TensorFlowLiteSelectTfOps (~> 0.0.1-nightly.20200729)\r\n    - TensorFlowLiteC\r\n    - TensorFlowLiteObjC\r\n    - TensorFlowLiteSelectTfOps\r\n  TensorFlowLiteC: 51f50caf5777f740a70e2c1a5dbdc149e7aeb50b\r\n  TensorFlowLiteObjC: 5b358503636cffcbbafd8b0ac9badb577fd72800\r\n  TensorFlowLiteSelectTfOps: eb44f3855f87b50470c70a379777c98aaf13a943\r\n```\r\n\r\nFriendly ping to @yyoon - I see you've been active in this code recently and may have some insight. \ud83d\ude42", "comments": ["Thanks for flagging! Let me try reproducing the issue and see what we can do about it.\r\nBetween 2.2.0 and 2.3.0, we had modified the way we expose symbols in iOS prebuilt frameworks, and that might have caused this issue.", "I have the same issue.\r\nUntil it gets fixed, is there a workaround?", "I have the same issue. \r\n\r\n` - TensorFlowLiteC (2.3.0):\r\n    - TensorFlowLiteC/Core (= 2.3.0)\r\n  - TensorFlowLiteC/Core (2.3.0)\r\n  - TensorFlowLiteSelectTfOps (0.0.1-nightly.20200819)\r\n  - TensorFlowLiteSwift (2.3.0):\r\n    - TensorFlowLiteSwift/Core (= 2.3.0)\r\n  - TensorFlowLiteSwift/Core (2.3.0):\r\n    - TensorFlowLiteC (= 2.3.0)\r\n  - TensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\n  - TensorFlowLiteSwift (= 2.3.0)\r\n    - TensorFlowLiteC\r\n    - TensorFlowLiteSelectTfOps\r\n    - TensorFlowLiteSwift\r\n  TensorFlowLiteC: 51f50caf5777f740a70e2c1a5dbdc149e7aeb50b\r\n  TensorFlowLiteSelectTfOps: 003d038ccf014b39b65b7d02627e1b2856c0a340\r\n  TensorFlowLiteSwift: fb152cc1eec36b25b03a23c07f5d58113170af58`\r\n\r\n`Undefined symbols for architecture arm64:\r\n  \"_uprv_getICUData_conversion\", referenced from:\r\n      openCommonData(char const*, int, UErrorCode*) in TensorFlowLiteSelectTfOps(udata_2985644a25bfa5ed30c4cd35b64b13bb.o)\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)`\r\n\r\n\r\nIs there a workaround", "@kkadakia : if you compile the frameworks from tip, it links without any issue. It's a bit time consuming, but in the mean time, it works.", "> @kkadakia : if you compile the frameworks from tip, it links without any issue. It's a bit time consuming, but in the mean time, it works.\n\n@tinoucas can you explain?", "> > @kkadakia : if you compile the frameworks from tip, it links without any issue. It's a bit time consuming, but in the mean time, it works.\r\n> \r\n> @tinoucas can you explain?\r\n\r\nYou need bazel version 3.1 installed, and run:\r\n\r\n`/path/to/bazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64,x86_64 \"//tensorflow/lite/experimental/ios:TensorFlowLiteC_framework\"`\r\nand\r\n`/path/to/bazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64,x86_64 \"//tensorflow/lite/experimental/ios:TensorFlowLiteSelectTfOps_framework\"`\r\n\r\nTo quickly find the frameworks built you can run `find -L . -name \\*.framework`\r\n", "> > > @kkadakia : if you compile the frameworks from tip, it links without any issue. It's a bit time consuming, but in the mean time, it works.\r\n> > \r\n> > \r\n> > @tinoucas can you explain?\r\n> \r\n> You need bazel version 3.1 installed, and run:\r\n> \r\n> `/path/to/bazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64,x86_64 \"//tensorflow/lite/experimental/ios:TensorFlowLiteC_framework\"`\r\n> and\r\n> `/path/to/bazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64,x86_64 \"//tensorflow/lite/experimental/ios:TensorFlowLiteSelectTfOps_framework\"`\r\n> \r\n> To quickly find the frameworks built you can run `find -L . -name \\*.framework`\r\n\r\n@tinoucas how do I integrate within an existing Swift project, without installing Bazel and starting a new project?", "> I have the same issue.\r\n> \r\n> ` - TensorFlowLiteC (2.3.0):\r\n> - TensorFlowLiteC/Core (= 2.3.0)\r\n> \r\n> * TensorFlowLiteC/Core (2.3.0)\r\n> * TensorFlowLiteSelectTfOps (0.0.1-nightly.20200819)\r\n> * TensorFlowLiteSwift (2.3.0):\r\n>   \r\n>   * TensorFlowLiteSwift/Core (= 2.3.0)\r\n> * TensorFlowLiteSwift/Core (2.3.0):\r\n>   \r\n>   * TensorFlowLiteC (= 2.3.0)\r\n> * TensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\n> * TensorFlowLiteSwift (= 2.3.0)\r\n>   \r\n>   * TensorFlowLiteC\r\n>   * TensorFlowLiteSelectTfOps\r\n>   * TensorFlowLiteSwift\r\n>     TensorFlowLiteC: 51f50caf5777f740a70e2c1a5dbdc149e7aeb50b\r\n>     TensorFlowLiteSelectTfOps: 003d038ccf014b39b65b7d02627e1b2856c0a340\r\n>     TensorFlowLiteSwift: fb152cc1eec36b25b03a23c07f5d58113170af58`\r\n> \r\n> `Undefined symbols for architecture arm64: \"_uprv_getICUData_conversion\", referenced from: openCommonData(char const*, int, UErrorCode*) in TensorFlowLiteSelectTfOps(udata_2985644a25bfa5ed30c4cd35b64b13bb.o) ld: symbol(s) not found for architecture arm64 clang: error: linker command failed with exit code 1 (use -v to see invocation)`\r\n> \r\n> Is there a workaround\r\n\r\nI don't do swift, but the link error you have is different. You might want to open another issue for it.", "Hi \r\n\r\nI am also getting same error.\r\nUndefined symbols for architecture arm64:\r\n  \"_uprv_getICUData_conversion\", referenced from:\r\n      openCommonData(char const*, int, UErrorCode*) in TensorFlowLiteSelectTfOps(udata_2985644a25bfa5ed30c4cd35b64b13bb.o)\r\nld: symbol(s) not found for architecture arm64\r\n\r\nAny solution for this.", "> > > > @kkadakia : if you compile the frameworks from tip, it links without any issue. It's a bit time consuming, but in the mean time, it works.\r\n> > > \r\n> > > \r\n> > > @tinoucas can you explain?\r\n> > \r\n> > \r\n> > You need bazel version 3.1 installed, and run:\r\n> > `/path/to/bazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64,x86_64 \"//tensorflow/lite/experimental/ios:TensorFlowLiteC_framework\"`\r\n> > and\r\n> > `/path/to/bazel build -c opt --config=ios --ios_multi_cpus=armv7,arm64,x86_64 \"//tensorflow/lite/experimental/ios:TensorFlowLiteSelectTfOps_framework\"`\r\n> > To quickly find the frameworks built you can run `find -L . -name \\*.framework`\r\n> \r\n> @tinoucas how do I integrate within an existing Swift project, without installing Bazel and starting a new project?\r\n\r\nI have implemented Bazel as well but same error is still coming.", "Hi TF Team,\r\n\r\nWe have integrated the below framework from the pod and also try to compile the project from Bazel as well but we are facing the same error during compile time.\r\n\r\nWe have also added force load from the Other linker sections in Xcode. \r\n\r\nFramework Used -\r\nTensorFlowLiteSwift\r\nTensorFlowLiteSelectTfOps\r\n\r\nXcode Configuration -\r\n\r\nXcode 11.3\r\nMin Deployment Target - 13.0\r\n\r\nError - \r\n\r\nUndefined symbols for architecture x86_64:\r\n\r\n  \"_uprv_getICUData_conversion\", referenced from:\r\n\r\n      openCommonData(char const*, int, UErrorCode*) in TensorFlowLiteSelectTfOps(udata_6946572031b696892300e9a29f96be3f.o)\r\n\r\nld: symbol(s) not found for architecture x86_64\r\n\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n\r\nKindly let me know the resolution for this It will be a great help for us.\r\n", "Hi all,\r\nThe duplicated error does not appear in the current nightly anymore so please use that.\r\nHowever, there is an current error: undefined symbol _uprv_getICUData_conversion which is described in b/41948\r\n", "My current configs that this error does not happen:\r\nPODS:\r\n  - TensorFlowLiteC (0.0.1-nightly.20201005):\r\n    - TensorFlowLiteC/Core (= 0.0.1-nightly.20201005)\r\n  - TensorFlowLiteC/Core (0.0.1-nightly.20201005)\r\n  - TensorFlowLiteSelectTfOps (0.0.1-nightly.20201005)\r\n  - TensorFlowLiteSwift (0.0.1-nightly.20201005):\r\n    - TensorFlowLiteSwift/Core (= 0.0.1-nightly.20201005)\r\n  - TensorFlowLiteSwift/Core (0.0.1-nightly.20201005):\r\n    - TensorFlowLiteC (= 0.0.1-nightly.20201005)\r\n\r\nDEPENDENCIES:\r\n  - TensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\n  - TensorFlowLiteSwift (~> 0.0.1-nightly)", "Please feel free to reopen this bug if you still got this error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41876\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41876\">No</a>\n", "@thaink \r\nI specified the versions as you have posted here. \r\n\r\nPODS:\r\n  - TensorFlowLiteC (0.0.1-nightly.20200818):\r\n    - TensorFlowLiteC/Core (= 0.0.1-nightly.20200818)\r\n  - TensorFlowLiteC/Core (0.0.1-nightly.20200818)\r\n  - TensorFlowLiteSelectTfOps (0.0.1-nightly.20200819)\r\n  - TensorFlowLiteSwift (0.0.1-nightly.20200818):\r\n    - TensorFlowLiteSwift/Core (= 0.0.1-nightly.20200818)\r\n  - TensorFlowLiteSwift/Core (0.0.1-nightly.20200818):\r\n    - TensorFlowLiteC (= 0.0.1-nightly.20200818)\r\n\r\nDEPENDENCIES:\r\n  - TensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\n  - TensorFlowLiteSwift (~> 0.0.1-nightly)\r\n\r\nBut I'm still getting this error.\r\nNote: Not using Bazel or something else.", "> @thaink\r\n> I specified the versions as you have posted here.\r\n> \r\n> PODS:\r\n> \r\n> * TensorFlowLiteC (0.0.1-nightly.20200818):\r\n>   \r\n>   * TensorFlowLiteC/Core (= 0.0.1-nightly.20200818)\r\n> * TensorFlowLiteC/Core (0.0.1-nightly.20200818)\r\n> * TensorFlowLiteSelectTfOps (0.0.1-nightly.20200819)\r\n> * TensorFlowLiteSwift (0.0.1-nightly.20200818):\r\n>   \r\n>   * TensorFlowLiteSwift/Core (= 0.0.1-nightly.20200818)\r\n> * TensorFlowLiteSwift/Core (0.0.1-nightly.20200818):\r\n>   \r\n>   * TensorFlowLiteC (= 0.0.1-nightly.20200818)\r\n> \r\n> DEPENDENCIES:\r\n> \r\n> * TensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\n> * TensorFlowLiteSwift (~> 0.0.1-nightly)\r\n> \r\n> But I'm still getting this error.\r\n> Note: Not using Bazel or something else.\r\n\r\nThe nightly you are using is almost 2-month old. Could you run \"pod update\" and try the latest nightly.", "> > @thaink\r\n> > I specified the versions as you have posted here.\r\n> > PODS:\r\n> > \r\n> > * TensorFlowLiteC (0.0.1-nightly.20200818):\r\n> >   \r\n> >   * TensorFlowLiteC/Core (= 0.0.1-nightly.20200818)\r\n> > * TensorFlowLiteC/Core (0.0.1-nightly.20200818)\r\n> > * TensorFlowLiteSelectTfOps (0.0.1-nightly.20200819)\r\n> > * TensorFlowLiteSwift (0.0.1-nightly.20200818):\r\n> >   \r\n> >   * TensorFlowLiteSwift/Core (= 0.0.1-nightly.20200818)\r\n> > * TensorFlowLiteSwift/Core (0.0.1-nightly.20200818):\r\n> >   \r\n> >   * TensorFlowLiteC (= 0.0.1-nightly.20200818)\r\n> > \r\n> > DEPENDENCIES:\r\n> > \r\n> > * TensorFlowLiteSelectTfOps (~> 0.0.1-nightly)\r\n> > * TensorFlowLiteSwift (~> 0.0.1-nightly)\r\n> > \r\n> > But I'm still getting this error.\r\n> > Note: Not using Bazel or something else.\r\n> \r\n> The nightly you are using is almost 2-month old. Could you run \"pod update\" and try the latest nightly.\r\n\r\nI had this done already. I even reinstalled Cocoapods. But it still downloads the same thing. \r\nIs there any other way around to download the latest version?", "@thaink I just tried pod update again and ran into errors, twice.\r\nScreenshot is attached.\r\n![Screen Shot 2020-10-07 at 18 43 43](https://user-images.githubusercontent.com/60460266/95335574-32e33100-08cd-11eb-967d-3210eb6bb9c6.png)\r\n", "@Koushik-Ogma I realized I still sometime faces this error too.\r\nWill investigate more.", "Any workarounds?  Still hitting `Undefined symbol: _uprv_getICUData_conversion` with nightly builds.\r\n\r\n```\r\nTensorFlowLiteC (0.0.1-nightly.20201010)\r\nTensorFlowLiteSelectTfOps (0.0.1-nightly.20201010)\r\nTensorFlowLiteSwift (0.0.1-nightly.20201010)\r\n```\r\n\r\n", "Same issue here.", "> Same issue here.\r\n\r\nContinuing thread over at #41948 \r\n", "The issue is still there:\r\nDependencies:\r\n```\r\nTensorFlowLiteC 0.0.1-nightly.20201019\r\nTensorFlowLiteSwift 0.0.1-nightly.20201019\r\nTensorFlowLiteSelectTfOps 0.0.1-nightly.20201021\r\n```\r\n\r\nError:\r\n```\r\nUndefined symbols for architecture arm64:\r\n  \"_uprv_getICUData_conversion\", referenced from:\r\n      openCommonData(char const*, int, UErrorCode*) in TensorFlowLiteSelectTfOps(udata_2985644a25bfa5ed30c4cd35b64b13bb.o)\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```", "I am working on a fix. If you don't need recent feature then v2.2.0 will work.", "A fix is under review internally. Please wait a bit, I'll notify when it is merged to the master.", "Thanks very much @thaink . I can confirm that it is working with 2.2.0 but as I am experimenting with Metal and CoreML on iOS delegates I need it at least version 2.3.0 or nightly working. I am looking forward to fix.", "Hi all,\r\nCommit https://github.com/tensorflow/tensorflow/commit/4b6c15218b690c5c75b28c423991096e4f0ea51b contains the fix for this error.\r\nYou can try to build the master branch or waiting for it to be included in the nightly build tomorrow.", "Hi, I'm experiencing this with the current nightly builds\r\n\r\nCurrent podfile\r\n```\r\n'TensorFlowLiteObjC', '~> 0.0.1-nightly'\r\n'TensorFlowLiteObjC/Metal', '~> 0.0.1-nightly'\r\n'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'\r\n```\r\nResolves to lockfile\r\n```\r\n  - TensorFlowLiteC (0.0.1-nightly.20210915):\r\n    - TensorFlowLiteC/Core (= 0.0.1-nightly.20210915)\r\n  - TensorFlowLiteC/Core (0.0.1-nightly.20210915)\r\n  - TensorFlowLiteC/Metal (0.0.1-nightly.20210915):\r\n    - TensorFlowLiteC/Core\r\n  - TensorFlowLiteObjC (0.0.1-nightly.20210915):\r\n    - TensorFlowLiteObjC/Core (= 0.0.1-nightly.20210915)\r\n  - TensorFlowLiteObjC/Core (0.0.1-nightly.20210915):\r\n    - TensorFlowLiteC (= 0.0.1-nightly.20210915)\r\n  - TensorFlowLiteObjC/Metal (0.0.1-nightly.20210915):\r\n    - TensorFlowLiteC/Metal (= 0.0.1-nightly.20210915)\r\n    - TensorFlowLiteObjC/Core (= 0.0.1-nightly.20210915)\r\n  - TensorFlowLiteSelectTfOps (0.0.1-nightly.20210915)\r\n```\r\n\r\nThe duplicate symbols are `_TfLiteXNNPackDelegateCreate `, `_TfLiteXNNPackDelegateDelete `, `_TfLiteXNNPackDelegateGetThreadPool `, and `_TfLiteXNNPackDelegateOptionsDefault`. \r\n\r\nThey're duplicated between `/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps(xnnpack_delegate.o)` and `Pods/TensorFlowLiteC/Frameworks/TensorFlowLiteC.framework/TensorFlowLiteC`.\r\n\r\n(If pod versions are specified as 2.6.0 then they work fine)", "@swittk Can you instead file a new issue about this? This thread is closed long ago, and the cause of the issue seems different.", "Sure thing!"]}, {"number": 41875, "title": "[Wsign-compare] warning resolutions, by directory, 11", "body": "Resolutions affecting directories:\r\n`tensorflow/c/`\r\n`tensorflow/cc/`", "comments": ["@mihaimaruseac "]}, {"number": 41874, "title": "Keras ModelCheckpoint callback not raising exception when h5 save fails", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version:3.6\r\n\r\n**Describe the current behavior**\r\nThe callback conceals the exception thrown by `save_model`. The checkpoints are not saved but no error is reported. \r\n\r\n**Describe the expected behavior**\r\nThe callback should re-throw the error after catching it. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1Nq5ARA3BHTxUyguiFFBBH74tXC54MABB?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@yixingfu The colab gist you shared cannot be opened due to permissions. Can you please save it as GitHub gist and share again? Thanks!", "Here is the gist\r\nhttps://gist.github.com/yixingfu/79e0693627c8e0aeaccaead6d4068cbf", "As per new tf-nightly version 2.4.0-dev20200803 the error is not raised while loading weights.\r\nSee [gist](https://colab.research.google.com/gist/ymodak/a6e26acf6a1bfb3d2d042cac55a8b655/callback-exception.ipynb).", "No it still raises error when loading weight. And this error comes from the checkpoint failing to save, which should actually raise an error. \r\n\r\nI think I know where the error come from. I will open a PR to fix it soon.", "You are right this works for `tf` format but not `h5`.\r\nI forgot to reset the colab runtime while testing for different formats so didn't get any error.\r\nFeel free to raise a PR and tag this issue thread. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41874\">No</a>\n"]}, {"number": 41872, "title": "[Intel MKL] BF16 enabling for MklLayoutPass test - v5", "body": "[Intel MKL] BF16 enabling for MklLayoutPass test - v5\r\nSigned-off-by: mazharul <mazharul.islam@intel.com>", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41872) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41872) for more info**.\n\n<!-- ok -->", "@noim210 Can you please check @penpornk's comments and keep us posted ? Thanks!\r\n", "@noim210  Any update on this PR? Please. Thanks!", "Yes, I am almost done with the requested review. I am going to push soon."]}, {"number": 41871, "title": "Unable to install tensorflow even after upgrading pip", "body": "\r\n![error](https://user-images.githubusercontent.com/42766576/88843049-199f7380-d1fe-11ea-9477-2bac3760a634.png)\r\n<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10.0.18363\r\n- TensorFlow installed from (source or binary): Tried to install using `pip install tensorFlow`\r\n- TensorFlow version: \r\n- Python version: Tried on both 3.6.0 and 3.8.5\r\n- Installed using pip\r\n\r\n\r\n**Describe the problem**\r\nI tried to install tensorflow but encountered an error as there is no matching distribution. By going through the previous issues I upgrade my pip version to 20.2. Tried it on both python 3.6 and python 3.8.5 ", "comments": ["Backup your libraries(lib) of current python . \r\nReinstall python of 64bit [Executable File](https://www.python.org/ftp/python/3.8.5/python-3.8.5-amd64.exe) for windows.\r\nRemove the previous path(of python 32bit) from Environment Variable .\r\nRestore libraries or reassign pip\r\nRetry the install \ud83d\ude0a\r\n", "@KapilBansal320 \r\nPlease verify if you are using python 32 bit [use 64 bit] and upgrade pip, let us know.\r\n\r\nPlease provide all details as per issue template for us to analyse, please share error log in text format for ease of use in case the issue is not resolved.\r\n", "This is either 32 bits Python or Python installed from the marketplace instead of from the official Python sources.", "Yes, I was using 32 bit python. Thanks for help.\r\nBut why this doesn't work in 32 bit python", "Because C++ datatypes are different on 32bits vs on 64bits. Since we only test on 64bits, providing a package for 32bits is impossible for us. Community can attempt builds for 32bits but it's not as simple as just building, you will need to also ensure no security issues arise from the type width change.\r\n\r\nSince this is solved, closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41871\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41871\">No</a>\n"]}, {"number": 41870, "title": "Expose LoadLibrary and GetSymbolFromLibrary", "body": "@mihaimaruseac \r\nThis PR exposes 2 functions from `core` to `c_api`", "comments": []}, {"number": 41869, "title": "Returning tf.data.UNKNOWN_CARDINALITY when the cardinality can be easily computed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): *yes*\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *dockerhub tensorflow/tensorflow:latest*\r\n- TensorFlow installed from (source or binary): *dockerhub image*\r\n- TensorFlow version (use command below): *v2.3.0-rc2-23-gb36436b087 2.3.0*\r\n\r\n**Describe the current behavior**\r\n\r\n*tf.data.Dataset.cardinality returns tf.data.UNKNOWN_CARDINALITY when the cardinality can be easily computed*\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n# prints 2.3.0\r\n\r\nnx = 2\r\n\r\nt = tf.data.Dataset.from_tensor_slices(tf.constant([[0],[1]]))\r\nt2 = t.flat_map(lambda ti: tf.data.Dataset.from_tensors(ti).repeat(nx))\r\n\r\n#########################\r\nfor ti in t2:\r\n    print(ti)\r\n\r\n# correctly prints:\r\n#tf.Tensor([0], shape=(1,), dtype=int32)\r\n#tf.Tensor([0], shape=(1,), dtype=int32)\r\n#tf.Tensor([1], shape=(1,), dtype=int32)\r\n#tf.Tensor([1], shape=(1,), dtype=int32)\r\n\r\n#############################3\r\n\r\nprint((t2.cardinality() == tf.data.UNKNOWN_CARDINALITY).numpy())\r\n# prints True\r\n\r\n\r\n```\r\n", "comments": ["I have tried in colab with TF version 2.3,nightly version(`2.4.0-dev20200729`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5da063bb8958058992570807f07fe612/untitled199.ipynb).Thanks!", "How do you propose the cardinality could be \"easily\" computed in this case?\r\n\r\nExecuting the input pipeline does not qualify as an acceptable solution because it can be in general expensive (and possibly not terminate). ", "I see that executing the pipeline could be expensive in general.  I was not saying that this was a valid alternative for the general case.\r\n\r\nI'm not a tensorflow developer, and therefore, I don't know the implementation details to suggest an alternative.  However, it could be better documented when cardinality return tf.data.UNKNOWN_CARDINALITY.  I guess the answer will be that this is the case whenever you use a 'flat_map'. If that is the case, you may look into flat_map and check if you could propagate the cardinality computation propagation through it.", "I agree that the behavior of the method should be documented better. @aaudiber could you please update the cardinality documentation to address this issue? In particular, to call out that:\r\n\r\n1) A known cardinality is returned only when it can be inferred statically (i.e. the computation does not execute the input pipeline). Consequently, input pipelines that make use of file-based source datasets or transformations which construct datasets from their input using a user-defined function are expected to return unknown cardinality.\r\n\r\n2) The users can provide cardinality hints through the `assert_cardinality` transformation.\r\n\r\n@andrescodas as for your suggestion about inspecting `flat_map` (or `interleave` for that matter). It is generally not possible to statically determine what he cardinality would be. The cardinality of `flat_map` is a function of the values generated by the `flat_map` input dataset, which are generally not known statically (i.e. without executing the input dataset) and so we do not attempt to do it.", "Thanks for bringing this up @andrescodas! I've updated the cardinality docs in https://github.com/tensorflow/tensorflow/commit/2fe61b03cd2703de2658ece095edb7a0ada3f681", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41869\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41869\">No</a>\n"]}, {"number": 41868, "title": "S3 filesystem test part 2", "body": "@mihaimaruseac \r\nThis PR ports more test from `core/platform/s3:s3_file_system_test` to modular filesystem s3 c api", "comments": []}, {"number": 41867, "title": "Remove scipy dependency.", "body": "See #40884, #35709, #40789.\r\n\r\nKeeping here in case of a patch release.", "comments": ["This is great to see merged. Thanks very much for this!"]}, {"number": 41866, "title": "Remove scipy dependency.", "body": "See #40884, #35709, #40789.\r\n\r\nKeeping here in case of a patch release.", "comments": []}, {"number": 41865, "title": "Remove scipy dependency.", "body": "See #40884, #35709, #40789.\r\n\r\nKeeping here in case of a patch release.", "comments": []}, {"number": 41864, "title": "Building of the PIP package (as described in docs) is broken", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: neither\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.2.89/7.6.5\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nThe documentation [says](https://www.tensorflow.org/install/source) that in order to build the PIP package from source you have to:\r\n\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\r\n`\r\n\r\nThis doesn't work, since there is no more `bazel-bin` in the TF source tree, and running the same command with `bazel-tensorlflow` instead of `bazel-bin` doesn't work either.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg`\r\n\r\n", "comments": ["`bazel-bin` should be created as a symlink after successful run of `bazel build //tensorflow/tools/pip_package:build_pip_package`.", "@mihaimaruseac Got it, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41864\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41864\">No</a>\n"]}, {"number": 41863, "title": "Cuda Error when training RNN", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Binary (python wheel using pip)\r\n- TensorFlow version: 2.2 AND 2.3\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: GTX 1070 8GB with the current driver 451.67\r\n\r\n\r\n\r\n**Describe the problem**\r\nI wanted to test my tensorflow-gpu installation. It works for CNNs but when I try to run the text classification tutorial where Bidirectional layers with LSTM are used(https://www.tensorflow.org/tutorials/text/text_classification_rnn) the training process crashes during early epochs with this error (I left out part of the repeating part of the stacktrace):\r\n```\r\nE tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1986): \"cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())\"\r\n\r\nW tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1449, 32, 64] \r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI installed cuda 10.1 and cuDNN 7.6.5 and edited my PATH variable accordingly + restarted, because sometimes windows is weird about changing environment variables.\r\nAfterwards I ran `pip install tensorflow-gpu` inside a conda environment.\r\n\r\nI then executed a python file containing all the code of the tutorial.\r\n\r\n**Any other info / logs**\r\nThe error occurs within the first epoch. Not directly after the start, but in early steps during the epoch, which makes me wonder where the problem originates.\r\nWhen I use a model without the `tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),` layer it works just fine.\r\n\r\nWhen I run `tf.config.list_physical_devices('GPU')` it seems to load all dll's correctly.\r\nI was also considering that this error is due to insufficient GPU memory. Even though the model is 'relatively' small, tensorflow allocated between 6 and 7GB memory. It does that also for small CNN models.", "comments": ["@Zapnuk \r\nPlease provide simple stand alone code to replicate the issue faced or is possible share a colab gist with the error reported.", "Its the afaik with any network that uses a Bidirectional Layer.\r\n\r\nI run a 100% copy of this code locally: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb", "@Zapnuk \r\nI ran the mentioned code and do not face any errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e2c913f815db9384c924eea00b661f47/untitled326.ipynb)", "This is a colab. Did you try it on a (local) Windows machine with the Cuda 10.1 and cudnn 7.6.5?\r\n\r\nI've tried it again. I followed the exact guidelines at https://www.tensorflow.org/install/gpu . The only component in my system that is of a different version is the GPU driver where CUDA 10.1 wants to install version 417 but I'd like to continue to use the newer version 451.67.", "Thanks for reporting the issue. We have similar reports for this, and the root cause in the cudnn kernel itself. Please see other issue that has been resolved to find out the solutions (mostly update the cudnn to a newer version). \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/39989\r\nhttps://github.com/tensorflow/tensorflow/issues/36508 \r\nhttps://github.com/tensorflow/tensorflow/issues/41444\r\n\r\nhttps://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800", "I am going to close this issue to avoid any duplicated tracking. Feel free to reopen if you feel you issue is different from others.\r\n\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41863\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41863\">No</a>\n", "Ty for the response. This matches exactly my issue. It seems like the only solution is to wait until tensorflow supports cudnn 8.0 or downgrade the nvidia driver to <431.86", "Amazing how this still is not fixed."]}, {"number": 41862, "title": "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details", "body": "\r\n![Capture](https://user-images.githubusercontent.com/62254038/88812450-a242f300-d1af-11ea-8d2f-e461a0e2c00e.JPG)\r\n\r\n\r\nHi All,\r\n\r\ni'm trying to train train my own models with the tensorflow 2.2, by using predefined models used this link but i'm facing the above error. Can anyone please let me know anyone was able to run the pretrained models on tensorflow 2.2??\r\n\r\nthanks in advance.", "comments": ["@Deepthi-Jain \r\n\r\nPlease, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, let us know which pretrained model you are using and share related code .Thanks!\r\n", "Having similar issue with efficientdet_d1_coco17_tpu-32 pre-trained model.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I have the same problem using ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8 and  the file:  models/research/object_detection/model_main_tf2.py ", "@ravikyram I'm having the same issue with TF 2.3.0 Object Detection and Efficientnet D1", "> @ravikyram I'm having the same issue with TF 2.3.0 Object Detection and Efficientnet D1\r\nthis will fix it\r\n  fine_tune_checkpoint_type: \"fine_tune\" instead of fine_tune_checkpoint_type: \"detection\"", "I had the same problem. Changing  fine_tune_checkpoint_type: \"classification\"  to  fine_tune_checkpoint_type: \"detection\" in the config file, solved my problem. After solving the above problem, training job work but my model did not converge so I decreased the leraning_rate and momentum. After decreasing the learning_rate and momentum my model converged.\r\n NOTE: If you got a resource exhausted error (OOM error) decrease the batch_size in the config file.\r\nThe final config file is like below:\r\n```\r\n# SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal\r\n# loss (a.k.a Retinanet).\r\n# See Lin et al, https://arxiv.org/abs/1708.02002\r\n# Trained on COCO, initialized from Imagenet classification checkpoint\r\n# Train on TPU-8\r\n#\r\n# Achieves 34.3 mAP on COCO17 Val\r\n\r\nmodel {\r\n  ssd {\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: true\r\n    num_classes: 3\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    encode_background_as_zeros: true\r\n    anchor_generator {\r\n      multiscale_anchor_generator {\r\n        min_level: 3\r\n        max_level: 7\r\n        anchor_scale: 4.0\r\n        aspect_ratios: [1.0, 2.0, 0.5]\r\n        scales_per_octave: 2\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 640\r\n        width: 640\r\n      }\r\n    }\r\n    box_predictor {\r\n      weight_shared_convolutional_box_predictor {\r\n        depth: 256\r\n        class_prediction_bias_init: -4.6\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0004\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              stddev: 0.01\r\n              mean: 0.0\r\n            }\r\n          }\r\n          batch_norm {\r\n            scale: true,\r\n            decay: 0.997,\r\n            epsilon: 0.001,\r\n          }\r\n        }\r\n        num_layers_before_predictor: 4\r\n        kernel_size: 3\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_resnet50_v1_fpn_keras'\r\n      fpn {\r\n        min_level: 3\r\n        max_level: 7\r\n      }\r\n      min_depth: 16\r\n      depth_multiplier: 1.0\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.0004\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            stddev: 0.03\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          scale: true,\r\n          decay: 0.997,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          alpha: 0.25\r\n          gamma: 2.0\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    normalize_loc_loss_by_codesize: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  fine_tune_checkpoint_version: V2\r\n  fine_tune_checkpoint: \"./test_data/ssd_resnet50/checkpoint/ckpt-0\"\r\n  fine_tune_checkpoint_type: \"detection\"\r\n  batch_size: 10\r\n  sync_replicas: true\r\n  startup_delay_steps: 0\r\n  replicas_to_aggregate: 5\r\n  use_bfloat16: true\r\n  num_steps: 100000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    random_crop_image {\r\n      min_object_covered: 0.0\r\n      min_aspect_ratio: 0.4\r\n      max_aspect_ratio: 3.0\r\n      min_area: 0.5\r\n      max_area: 1.0\r\n      overlap_thresh: 0.0\r\n    }\r\n  }\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: .0005\r\n          total_steps: 100000\r\n          warmup_learning_rate: .0001\r\n          warmup_steps: 1000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.5\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n}\r\n\r\ntrain_input_reader: {\r\n  label_map_path: \"./test_images/label_map.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"./test_images/train.record-00000-of-00001\"\r\n  }\r\n}\r\n\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n}\r\n\r\neval_input_reader: {\r\n  label_map_path: \"./test_images/label_map.pbtxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path:  \"./test_images/val.record-00000-of-00001\"\r\n  }\r\n}\r\n\r\n", "> I had the same problem. Changing fine_tune_checkpoint_type: \"classification\" to fine_tune_checkpoint_type: \"detection\" in the config file, solved my problem. After solving the above problem, training job work but my model did not converge so I decreased the leraning_rate and momentum. After decreasing the learning_rate and momentum my model converged.\r\n> NOTE: If you got a resource exhausted error (OOM error) decrease the batch_size in the config file.\r\n> The final config file is like below:\r\n> \r\n> ```\r\n> # SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal\r\n> # loss (a.k.a Retinanet).\r\n> # See Lin et al, https://arxiv.org/abs/1708.02002\r\n> # Trained on COCO, initialized from Imagenet classification checkpoint\r\n> # Train on TPU-8\r\n> #\r\n> # Achieves 34.3 mAP on COCO17 Val\r\n> \r\n> model {\r\n>   ssd {\r\n>     inplace_batchnorm_update: true\r\n>     freeze_batchnorm: true\r\n>     num_classes: 3\r\n>     box_coder {\r\n>       faster_rcnn_box_coder {\r\n>         y_scale: 10.0\r\n>         x_scale: 10.0\r\n>         height_scale: 5.0\r\n>         width_scale: 5.0\r\n>       }\r\n>     }\r\n>     matcher {\r\n>       argmax_matcher {\r\n>         matched_threshold: 0.5\r\n>         unmatched_threshold: 0.5\r\n>         ignore_thresholds: false\r\n>         negatives_lower_than_unmatched: true\r\n>         force_match_for_each_row: true\r\n>         use_matmul_gather: true\r\n>       }\r\n>     }\r\n>     similarity_calculator {\r\n>       iou_similarity {\r\n>       }\r\n>     }\r\n>     encode_background_as_zeros: true\r\n>     anchor_generator {\r\n>       multiscale_anchor_generator {\r\n>         min_level: 3\r\n>         max_level: 7\r\n>         anchor_scale: 4.0\r\n>         aspect_ratios: [1.0, 2.0, 0.5]\r\n>         scales_per_octave: 2\r\n>       }\r\n>     }\r\n>     image_resizer {\r\n>       fixed_shape_resizer {\r\n>         height: 640\r\n>         width: 640\r\n>       }\r\n>     }\r\n>     box_predictor {\r\n>       weight_shared_convolutional_box_predictor {\r\n>         depth: 256\r\n>         class_prediction_bias_init: -4.6\r\n>         conv_hyperparams {\r\n>           activation: RELU_6,\r\n>           regularizer {\r\n>             l2_regularizer {\r\n>               weight: 0.0004\r\n>             }\r\n>           }\r\n>           initializer {\r\n>             random_normal_initializer {\r\n>               stddev: 0.01\r\n>               mean: 0.0\r\n>             }\r\n>           }\r\n>           batch_norm {\r\n>             scale: true,\r\n>             decay: 0.997,\r\n>             epsilon: 0.001,\r\n>           }\r\n>         }\r\n>         num_layers_before_predictor: 4\r\n>         kernel_size: 3\r\n>       }\r\n>     }\r\n>     feature_extractor {\r\n>       type: 'ssd_resnet50_v1_fpn_keras'\r\n>       fpn {\r\n>         min_level: 3\r\n>         max_level: 7\r\n>       }\r\n>       min_depth: 16\r\n>       depth_multiplier: 1.0\r\n>       conv_hyperparams {\r\n>         activation: RELU_6,\r\n>         regularizer {\r\n>           l2_regularizer {\r\n>             weight: 0.0004\r\n>           }\r\n>         }\r\n>         initializer {\r\n>           truncated_normal_initializer {\r\n>             stddev: 0.03\r\n>             mean: 0.0\r\n>           }\r\n>         }\r\n>         batch_norm {\r\n>           scale: true,\r\n>           decay: 0.997,\r\n>           epsilon: 0.001,\r\n>         }\r\n>       }\r\n>       override_base_feature_extractor_hyperparams: true\r\n>     }\r\n>     loss {\r\n>       classification_loss {\r\n>         weighted_sigmoid_focal {\r\n>           alpha: 0.25\r\n>           gamma: 2.0\r\n>         }\r\n>       }\r\n>       localization_loss {\r\n>         weighted_smooth_l1 {\r\n>         }\r\n>       }\r\n>       classification_weight: 1.0\r\n>       localization_weight: 1.0\r\n>     }\r\n>     normalize_loss_by_num_matches: true\r\n>     normalize_loc_loss_by_codesize: true\r\n>     post_processing {\r\n>       batch_non_max_suppression {\r\n>         score_threshold: 1e-8\r\n>         iou_threshold: 0.6\r\n>         max_detections_per_class: 100\r\n>         max_total_detections: 100\r\n>       }\r\n>       score_converter: SIGMOID\r\n>     }\r\n>   }\r\n> }\r\n> \r\n> train_config: {\r\n>   fine_tune_checkpoint_version: V2\r\n>   fine_tune_checkpoint: \"./test_data/ssd_resnet50/checkpoint/ckpt-0\"\r\n>   fine_tune_checkpoint_type: \"detection\"\r\n>   batch_size: 10\r\n>   sync_replicas: true\r\n>   startup_delay_steps: 0\r\n>   replicas_to_aggregate: 5\r\n>   use_bfloat16: true\r\n>   num_steps: 100000\r\n>   data_augmentation_options {\r\n>     random_horizontal_flip {\r\n>     }\r\n>   }\r\n>   data_augmentation_options {\r\n>     random_crop_image {\r\n>       min_object_covered: 0.0\r\n>       min_aspect_ratio: 0.4\r\n>       max_aspect_ratio: 3.0\r\n>       min_area: 0.5\r\n>       max_area: 1.0\r\n>       overlap_thresh: 0.0\r\n>     }\r\n>   }\r\n>   optimizer {\r\n>     momentum_optimizer: {\r\n>       learning_rate: {\r\n>         cosine_decay_learning_rate {\r\n>           learning_rate_base: .0005\r\n>           total_steps: 100000\r\n>           warmup_learning_rate: .0001\r\n>           warmup_steps: 1000\r\n>         }\r\n>       }\r\n>       momentum_optimizer_value: 0.5\r\n>     }\r\n>     use_moving_average: false\r\n>   }\r\n>   max_number_of_boxes: 100\r\n>   unpad_groundtruth_tensors: false\r\n> }\r\n> \r\n> train_input_reader: {\r\n>   label_map_path: \"./test_images/label_map.pbtxt\"\r\n>   tf_record_input_reader {\r\n>     input_path: \"./test_images/train.record-00000-of-00001\"\r\n>   }\r\n> }\r\n> \r\n> eval_config: {\r\n>   metrics_set: \"coco_detection_metrics\"\r\n>   use_moving_averages: false\r\n> }\r\n> \r\n> eval_input_reader: {\r\n>   label_map_path: \"./test_images/label_map.pbtxt\"\r\n>   shuffle: false\r\n>   num_epochs: 1\r\n>   tf_record_input_reader {\r\n>     input_path:  \"./test_images/val.record-00000-of-00001\"\r\n>   }\r\n> }\r\n> ```\r\n\r\nStill having the same issue on faster r-cnn resnet 101", "Still having the same issue on mobilnet as well"]}, {"number": 41861, "title": "Function tracing fails after upgrading to 2.3.0", "body": "**System information**\r\n- Windows 7\r\n- TensorFlow 2.3.0 from PyPI\r\n- Python version: 3.7.7\r\n- CPU only\r\n\r\nAfter upgrading to 2.3.0 function tracing fails\r\n```\r\n\r\nWARNING:tensorflow:AutoGraph could not transform <function _convert_function_call.<locals>.f at 0x0000000015156E58> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nConverted call: <function pfor.<locals>.f at 0x00000000138BBD38>\r\n    args: ()\r\n    kwargs: {}\r\n\r\n<function pfor.<locals>.f at 0x00000000138BBD38> is not cached for subkey ConversionOptions[{}]\r\nSource code of <function pfor.<locals>.f at 0x00000000138BBD38>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\ndef f():\r\n  return _pfor_impl(loop_fn,\r\n  iters,\r\n  fallback_to_while_loop=fallback_to_while_loop,\r\n  parallel_iterations=parallel_iterations)\r\n\r\n\r\nError transforming entity <function pfor.<locals>.f at 0x00000000138BBD38>\r\nWARNING: AutoGraph could not transform <function pfor.<locals>.f at 0x00000000138BBD38> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 584, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 119, in convert\r\n    entity, program_ctx.options, program_ctx, custom_vars)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 412, in transform_function\r\n    extra_locals)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 373, in _transformed_factory\r\n    nodes, ctx = self._transform_function(fn, user_context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 339, in _transform_function\r\n    node = self.transform_ast(node, context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 70, in transform_ast\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 705, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 575, in visit_FunctionDef\r\n    node = self._visit_arg_annotations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 551, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 556, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 584, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 119, in convert\r\n    entity, program_ctx.options, program_ctx, custom_vars)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 412, in transform_function\r\n    extra_locals)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 373, in _transformed_factory\r\n    nodes, ctx = self._transform_function(fn, user_context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 339, in _transform_function\r\n    node = self.transform_ast(node, context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 70, in transform_ast\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 705, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 575, in visit_FunctionDef\r\n    node = self._visit_arg_annotations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 551, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 556, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 584, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 119, in convert\r\n    entity, program_ctx.options, program_ctx, custom_vars)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 412, in transform_function\r\n    extra_locals)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 373, in _transformed_factory\r\n    nodes, ctx = self._transform_function(fn, user_context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 339, in _transform_function\r\n    node = self.transform_ast(node, context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 70, in transform_ast\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 705, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 575, in visit_FunctionDef\r\n    node = self._visit_arg_annotations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 551, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 556, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\n\r\n```", "comments": ["@davidia Can you please provide Standalone code to reproduce the issue?", "@geetachavan1  this recreates it:\r\n\r\n```import tensorflow as tf\r\n\r\n@tf.function()\r\ndef f(x):\r\n    return x + 1\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.autograph.set_verbosity(10, True)\r\n    x = tf.constant(1)\r\n    print(f(x))\r\n\r\n```\r\nand the error is:\r\n\r\n```Converted call: <function f at 0x0000000002B89828>\r\n    args: (<tf.Tensor 'x:0' shape=() dtype=int32>,)\r\n    kwargs: {}\r\n\r\n<function f at 0x0000000002B89828> is not cached for subkey ConversionOptions[{}]\r\nSource code of <function f at 0x0000000002B89828>:\r\n\r\n@tf.function()\r\ndef f(x):\r\n    return x + 1\r\n\r\n\r\nError transforming entity <function f at 0x0000000002B89828>\r\nWARNING: AutoGraph could not transform <function f at 0x0000000002B89828> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 584, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 119, in convert\r\n    entity, program_ctx.options, program_ctx, custom_vars)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 412, in transform_function\r\n    extra_locals)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 373, in _transformed_factory\r\n    nodes, ctx = self._transform_function(fn, user_context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transpiler.py\", line 339, in _transform_function\r\n    node = self.transform_ast(node, context)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\conversion.py\", line 70, in transform_ast\r\n    node = activity.resolve(node, ctx, None)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 705, in resolve\r\n    return ActivityAnalyzer(context, parent_scope).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\transformer.py\", line 445, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\ast.py\", line 271, in visit\r\n    return visitor(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 575, in visit_FunctionDef\r\n    node = self._visit_arg_annotations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 551, in _visit_arg_annotations\r\n    node = self._visit_arg_declarations(node)\r\n  File \"C:\\ProgramData\\Miniconda3.7\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\activity.py\", line 556, in _visit_arg_declarations\r\n    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)\r\nAttributeError: 'arguments' object has no attribute 'posonlyargs'\r\nWARNING:tensorflow:AutoGraph could not transform <function f at 0x0000000002B89828> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\ntf.Tensor(2, shape=(), dtype=int32)\r\n\r\nProcess finished with exit code 0\r\n```", "This is windows only, it works as expected with the same versions on linux.", "@davidia,\r\n>tf.Tensor(2, shape=(), dtype=int32)\r\n>\r\n>Process finished with exit code 0\r\n\r\nAs we can see from the logs, the code snippet does print the output tensor.\r\n\r\nCould you please set `tf.autograph.set_verbosity` to 0 as shown below and let us know what the output is? \r\n\r\n\r\n```\r\n@tf.function()\r\ndef f(x):\r\n    return x + 1\r\nif __name__ == '__main__':\r\n    tf.autograph.set_verbosity(0, True)\r\n    x = tf.constant(1)\r\n    print(f(x))\r\n```\r\n\r\nThanks!", "It swallows the error if verbosity is 0 but of course then there will be performance issues.\r\n\r\nI tried in a clean python 3.7.7 virtual environment and I wasn't able to reproduce it. I think there is a problem upgrading from 2.2.0 with pip. Going to close it for now, thanks for your help."]}, {"number": 41860, "title": "Make target for generic cortex M4F device", "body": "This PR adds a make target for a generic cortex M4F device. It has the benefit of\r\n- creating a microlite library for use in other projects (i.e. projects that are not part of the TFLu examples but want to use TFLu as a library),\r\n- registering of a callback for DebugLog such that target library is really independent of any chip and board specifics.\r\n\r\nThe make target is executed by the command\r\n**For M4F, using cmsis-nn:**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=cortex_m_gcc_generic CORTEX_M_CORE=M4F microlite\r\n**For M4, not using cmsis-nn:**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=cortex_m_gcc_generic CORTEX_M_CORE=M4 microlite\r\n\r\nFor further info on how to use the TFLu debugging log see \r\ntensorflow/lite/micro/cortex-m-gcc-generic/README.md\r\n", "comments": ["Hi Nat Jeffries, Can you please review this PR ? Thanks!", "Bro, you've achieved the MCU static library I wanted. I will go to try.", "Apologies for having this sit in the PR queue for a long time.\r\n\r\ncc36203e7bb8e650011d2cdea5174d42cfc99138 added something similar and it would be great if the current PR could make use of that target. I recognize that your original PR came before the commit that I am referring to, so apologies again for that.\r\n\r\nWhat I am thinking is:\r\n * add the DebugLog callback to [cortex_m4_generic/debug_log.cc](https://github.com/tensorflow/tensorflow/blob/cc36203e7bb8e650011d2cdea5174d42cfc99138/tensorflow/lite/micro/cortex_m4_generic/debug_log.cc)\r\n * Pull out any `PLATFORM_FLAGS` differences into options that can be configure via the make command.\r\n\r\nLet me know if you have any questions.\r\n\r\nTagging @nkreeger in case he has additional input.", "Hi advaitjain, thx for looking into this. In fact, the make target introduced by @nkreeger is similar. Precisely, his Makefile is done for a Cortex-M4 device (without FPU hardware) while mine is done for a Cotex-M4F (with FPU hardware). With an FPU hardware, the user might want to use the optimized CMSIS DSP/NN library. To allow this, the Makefile must have the following entries:\r\n\r\n1. Download CMSIS lib:\r\n  $(eval $(call add_third_party_download,$(CMSIS_URL),$(CMSIS_MD5),cmsis,patch_cmsis))\r\n2. Enable FPU in the PLATFORM_FLAGS:\r\n    -mfloat-abi=hard \\\r\n3. Include path to CMSIS lib:\r\n  INCLUDES += \\\r\n    -isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/ \\\r\n    -isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/DSP/Include/ \\\r\n4. Use the faster depthwise conv implementation:\r\n  ALL_TAGS += portable_optimized\r\n  \r\nNot a must but personally I would do for convenience:\r\n\r\n5. add the embedded gcc to the third_party_downloads:\r\n  $(eval $(call add_third_party_download,$(GCC_EMBEDDED_URL),$(GCC_EMBEDDED_MD5),gcc_embedded,))\r\n  GCC_ARM := $(MAKEFILE_DIR)/downloads/gcc_embedded/\r\n  INCLUDES += \\\r\n      -I$(GCC_ARM)/arm-none-eabi/ \\\r\n      \r\n@advaitjain can you make a suggestion how the above list can be turned into Makefile options? An alternative would be adding a separate Makefile for the M4F device.\r\n\r\nIn addition, I suggest the following changes:\r\n\r\n5. Definition of BUILD_TYPE: I dont know if this is absolutely necessary but other targets have this definition in their Makefiles.\r\n  BUILD_TYPE := micro\r\n6. Using fprintf() for the debug log makes the assumption that the target has a file system. This is not always the case. Many targets have printf where the output is written into the serial console. And using printf requires at least the defintion of pins where the console is attached to, which makes the library dependent on chip/board specifics. Using a callback for DebugLog() avoids this problem.\r\n\r\n", "Let's rename the target that @nkreeger has to `cortex_m_makefile.inc` and have a command line option called `CORETEX_M_CORE`.\r\n\r\nThe invocation would be `make TARGET=cortex_m CORTEX_M_CORE=[M4|M4F]` and there would be if blocks in the makefile to handle the different variations.\r\n\r\nLet me know if you have more questions. I have made additional suggestions inline.\r\n\r\n> Hi advaitjain, thx for looking into this. In fact, the make target introduced by @nkreeger is similar. Precisely, his Makefile is done for a Cortex-M4 device (without FPU hardware) while mine is done for a Cotex-M4F (with FPU hardware). With an FPU hardware, the user might want to use the optimized CMSIS DSP/NN library. To allow this, the Makefile must have the following entries:\r\n\r\n> \r\n> 1. Download CMSIS lib:\r\n>    $(eval $(call add_third_party_download,$(CMSIS_URL),$(CMSIS_MD5),cmsis,patch_cmsis))\r\n\r\nWe don't need this in the M4F makefile. If we specify `TAGS=cmsis-nn` this download will happen.\r\n\r\n> 2. Enable FPU in the PLATFORM_FLAGS:\r\n>    -mfloat-abi=hard \\\r\nAnswered in the first part of my reply.\r\n\r\n> 3. Include path to CMSIS lib:\r\n>    INCLUDES += \r\n>    -isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/ \r\n>    -isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/DSP/Include/ \\\r\n\r\nAgain, not needed. `TAGS=cmsis-nn` will take care of this.\r\n\r\n> 4. Use the faster depthwise conv implementation:\r\n>    ALL_TAGS += portable_optimized\r\n> \r\nWe no longer have portable_optimized. The CMSIS-NN or reference implementations are the only two options.\r\n\r\n> Not a must but personally I would do for convenience:\r\n> \r\n> 1. add the embedded gcc to the third_party_downloads:\r\n>    $(eval $(call add_third_party_download,$(GCC_EMBEDDED_URL),$(GCC_EMBEDDED_MD5),gcc_embedded,))\r\n\r\nsounds good to me\r\n\r\n>    GCC_ARM := $(MAKEFILE_DIR)/downloads/gcc_embedded/\r\n>    INCLUDES += \r\n>    -I$(GCC_ARM)/arm-none-eabi/ \\\r\n> \r\nNot sure I understand the need for this.\r\n\r\n> @advaitjain can you make a suggestion how the above list can be turned into Makefile options? An alternative would be adding a separate Makefile for the M4F device.\r\n> \r\n> In addition, I suggest the following changes:\r\n> \r\n> 1. Definition of BUILD_TYPE: I dont know if this is absolutely necessary but other targets have this definition in their Makefiles.\r\n>    BUILD_TYPE := micro\r\n\r\nskip this. Its defined in the top-level Makefile. We will be documenting this better in the future.\r\n\r\n> 2. Using fprintf() for the debug log makes the assumption that the target has a file system. This is not always the case. Many targets have printf where the output is written into the serial console. And using printf requires at least the defintion of pins where the console is attached to, which makes the library dependent on chip/board specifics. Using a callback for DebugLog() avoids this problem.\r\n\r\nAgreed. Your solution with the callbacks is good. Let's go with that.\r\n\r\n", "Hi advaitjain, thank you for your suggestion. Yes, I agree to the command line option. And thank you for walking through all my points and sorting out what is really necessary and what is not. I used one of the example Makefiles as basis and could not judge on all the settings I found there.\r\n\r\nI would like to call the Makefile cortex-m-gcc-makefile.inc as there exist other compilers, too. Later, this Makefile can be updated to support other Cortex-M cores (e.g. M55) as well.\r\n\r\nI will prepare a modified version. I just need some time to make sure that the CMSIS optimization takes effect.", "> I would like to call the Makefile cortex-m-gcc-makefile.inc as there exist other compilers, too. Later, this Makefile can be updated to support other Cortex-M cores (e.g. M55) as well.\r\n> \r\nWorks for me, though please use underscores instead of dashes (`cortex_m_gcc_makefile.inc`). We don't enforce that but I'd like to move to underscores only (and rename `cmis-nn` to `cmsis_nn` etc.).\r\n\r\n> I will prepare a modified version. I just need some time to make sure that the CMSIS optimization takes effect.\r\n\r\nTake your time, no rush :-)\r\n", "I updated my PR according to our discussion. I was able to implement all steps as agreed except for adding CMSIS to the include path:\r\n  INCLUDES += \\\r\n    -isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/Core/Include/ \\\r\n    -isystem$(MAKEFILE_DIR)/downloads/cmsis/CMSIS/DSP/Include/\r\n\r\nWithout such lines the compiler was not able to find the files.\r\n\r\nA side note: if TF_LITE_STATIC_MEMORY is defined the microlite lib does not allocate the memory correctly. This is due to issue  #41900. So, I subtracted this define from the compile flags for the moment.\r\n\r\n"]}, {"number": 41859, "title": "Fix incorrect prepare_loss_weights error message, clarify Python list in docs", "body": "- This fixes the incorrect loss_weights error message, which previously asked for a list OF dictS instead of a list OR dict.\r\n- This also clarifies the two places where \"list\" needs a Python list rather than any list-like object. Most functions that ask for a list accept any list-like object.", "comments": ["@MingweiSamuel Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "At I glance I can't tell where everything moved to. I don't have the repo properly cloned to take a look at whats going on.\r\nClosing for now."]}, {"number": 41858, "title": "ImportError : cannot import name 'cloud\u201d from  ' tensorflow.contrib' (/usr/1ocal/lib/ python3.7/dist-packages/tensorflow/ contrib/_ init Py)", "body": "I am using TensorFlow 1.14.0 and python3.7.\r\nWhen  i run code, this erro will display.\r\n![image](https://user-images.githubusercontent.com/31792429/88808693-c7eaef00-d1e5-11ea-9df3-afd3706d4806.png)\r\n\r\n\r\n\r\n", "comments": ["@Dolphinzzx \r\nCan you please try to :\r\nfind the path corresponding to ImportError, open the __init__.py file of tensorflow, and comment out the line corresponding to the error. (and let us know)\r\n\r\nAlso please refer to below resolved issues with same error:\r\n#23976 [link](https://github.com/yaroslavvb/tensorflow-community-wheels/issues/117) [link](https://github.com/tensorflow/tensorflow/issues/23758#issuecomment-442615804) [link1](https://stackoverflow.com/questions/55172923/tensorflow-v1-12-importerror-cannot-import-name-cloud) ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "got the same error, you have to manually comment-out the import line cloud from tensorflow/contrib/__init__.py"]}, {"number": 41857, "title": "protoc fails to execute due to missing environment (LD_LIBRARY_PATH)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n\r\n**Describe the problem**\r\n\r\nBuild fails during execution of `protoc` due to unset `LD_LIBRARY_PATH\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nSetup custom GCC + binutils and export PATH/LD_LIBRARY_PATH\r\n\r\n**Any other info / logs**\r\n\r\nThis is the same problem as fixed in https://github.com/bazelbuild/bazel/pull/11860 which copied a file from grpc, fixed in https://github.com/grpc/grpc/pull/23664", "comments": ["@Flamefire Isn't this issue caused by bazel itself, not tensorflow?", "Depends. It is caused by the (IMO horrible) decision of Bazel to by default erase the user environment including essential variables like PATH. This has caused a lot of headaches for package maintainers already because issues like this one pop up again and again.\r\n\r\nHowever this particular issue is caused by grpc, I reported it in https://github.com/grpc/grpc/issues/23682 and fixed in https://github.com/grpc/grpc/pull/23664 pending merge.\r\n\r\nAs TensorFlow directly uses grpc in a specific revision it is now also an issue in tensorflow and can be fixed by updating grpc once the fix is accepted or by backporting the patch. See my PR https://github.com/tensorflow/tensorflow/pull/41889", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41857\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41857\">No</a>\n"]}, {"number": 41856, "title": "TensorFlow 2.3.0 installation fails as protoc could not be found", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n\r\n**Describe the current behavior**\r\n\r\nBuilding TensorFlow fails with:\r\n\r\n```\r\nERROR: /tmp/easybuild-tmp/eb-Lae15c/tmpdO084_-bazel-build/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed\r\n: error executing command \r\n  (cd /tmp/easybuild-tmp/eb-Lae15c/tmpdO084_-bazel-build/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/software/generic/CUDA/10.1.243 \\\r\n    CUDNN_INSTALL_PATH=/sw/installed/cuDNN/7.6.4.38-gcccuda-2019b \\\r\n    GCC_HOST_COMPILER_PATH=/sw/installed/GCCcore/8.3.0/bin/gcc \\\r\n    LD_LIBRARY_PATH=<...> \\\r\n    NCCL_INSTALL_PATH=/sw/installed/NCCL/2.4.8-gcccuda-2019b \\\r\n    PATH=<...> \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHONNOUSERSITE=1 \\\r\n    PYTHONPATH=<...> \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUBLAS_VERSION=10.2.1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.1,7.0 \\\r\n    TF_CUDA_PATHS=/sw/installed/CUDA/10.1.243 \\\r\n    TF_CUDA_VERSION=10.1 \\\r\n    TF_CUDNN_VERSION=7.6.4 \\\r\n    TF_NCCL_VERSION=2.4.8 \\\r\n    TF_NEED_CUDA=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/external/com_google_protobuf/protoc-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\ngcc: error: bazel-out/k8-opt/bin/external/com_google_protobuf/protoc: No such file or directory\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTF compiles\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nexport TF_MKL_DOWNLOAD=1 &&   bazel --output_base=/tmp/easybuild-tmp/eb-Lae15c/tmpdO084_-bazel-build --install_base=/tmp/easybuild-tmp/eb-Lae15c/tmpdO084_-bazel-build/inst_base --output_user_root=/tmp/easybuild-tmp/eb-Lae15c/tmp6IGZ5P-user_root build --compilation_mode=opt --config=opt --subcommands --verbose_failures --config=noaws --jobs=24 --copt=\"-fPIC\" --action_env=PYTHONPATH --action_env=PYTHONNOUSERSITE=1 --distinct_host_configuration=false  --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```", "comments": ["Contents of the `protoc-2.params` are:\r\n\r\n```\r\n-Wl,-no-as-needed\r\n-B\r\n-o\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/protoc\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protoc/main.o\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/libprotoc_lib.a\r\n-Wl,-whole-archive\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/libprotobuf.lo\r\n-Wl,-no-whole-archive\r\n-Wl,-whole-archive\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo\r\n-Wl,-no-whole-archive\r\nbazel-out/k8-opt/bin/external/zlib/libzlib.a\r\n-lpthread\r\n-lm\r\n-lpthread\r\n-lm\r\n-lpthread\r\n-lm\r\n-lpthread\r\n-lm\r\n-lstdc++\r\n-no-canonical-prefixes\r\n-fno-canonical-system-headers\r\n-pie\r\n-Wl,-z,relro,-z,now\r\n-Wl,--gc-sections\r\n-Wl,--build-id=md5\r\n-Wl,--hash-style=gnu\r\n```\r\n\r\nSo it seems that this is caused by our patch to work around issue https://github.com/tensorflow/tensorflow/issues/39263 where adding `-B /usr/bin` leads to it using a wrong linker (system instead of custom):\r\n\r\n```\r\n--- tensorflow-1.14.0/third_party/gpus/cuda_configure.bzl.orig\t2019-08-15 10:16:55.337496077 +0200\r\n+++ tensorflow-1.14.0/third_party/gpus/cuda_configure.bzl\t2019-08-15 10:21:35.527890849 +0200\r\n@@ -1121,7 +1121,7 @@\r\n     if should_download_clang:\r\n         cuda_defines[\"%{linker_bin_path}\"] = \"\"\r\n     else:\r\n-        cuda_defines[\"%{linker_bin_path}\"] = host_compiler_prefix\r\n+        cuda_defines[\"%{linker_bin_path}\"] = ''\r\n \r\n     cuda_defines[\"%{extra_no_canonical_prefixes_flags}\"] = \"\"\r\n     cuda_defines[\"%{unfiltered_compile_flags}\"] = \"\"\r\n\r\n```\r\n\r\nThere seemingly was a change in TF 2.3 (from 2.2 and earlier) where an empty `cuda_defines[\"%{linker_bin_path}\"]` results in adding a lone `-B` while previously it resulted in NOT adding `-B`. This looks the same as https://github.com/tensorflow/tensorflow/issues/38171.\r\n\r\nThis might be even a problem for unmodified TF when `should_download_clang` is True", "Workaround: Export `GCC_HOST_COMPILER_PREFIX` with the path to the directory containing `ld`", "@Flamefire The following lines caused the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/319257c2637ff285b27df8a21e066c37fbca5b09/third_party/gpus/crosstool/cc_toolchain_config.bzl.tpl#L501-L505\r\nI removed them and it works well.", "Yes, in particular it is the part `+ [ \"-B\" + ctx.attr.linker_bin_path, ]` which unconditionally adds the linker-bin-path although it could be empty", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41856\">No</a>\n"]}, {"number": 41855, "title": "tf.keras.Sequential() fails", "body": "**System information**\r\n- Running the most basic instruction fails, for example from the documentation page [https://www.tensorflow.org/api_docs/python/tf/keras/Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)\r\n- OS Platform and Distribution: Arch Linux kernel 5.7.10-arch1-1 (linux@archlinux) (gcc version 10.1.0 (GCC), GNU ld (GNU Binutils) 2.34.0)\r\n- TensorFlow installed from (source or binary): binary package (official Arch package)\r\n- TensorFlow version (use command below): tensorflow-cuda 2.3.0\r\n- Python version: Python 3.8.4\r\n- CUDA/cuDNN version: cuda 11.0\r\n- GPU model and memory: GeForce GTX 950M,  Driver Version: 450.57 - 2004MiB\r\n\r\n**Describe the current behavior**\r\nStart python then run:\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nm = tf.keras.Sequential()\r\n```\r\nThe last line fails with the following error messages:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 116, in __init__\r\n    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 308, in __init__\r\n    self._init_batch_counters()\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 317, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 244, in _variable_v2_call\r\n    return previous_getter(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\", line 2633, in default_variable_creator_v2\r\n    return resource_variable_ops.ResourceVariable(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1507, in __init__\r\n    self._init_from_args(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1661, in _init_from_args\r\n    handle = eager_safe_variable_handle(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 242, in eager_safe_variable_handle\r\n    return _variable_handle_from_shape_and_dtype(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 174, in _variable_handle_from_shape_and_dtype\r\n    gen_logging_ops._assert(  # pylint: disable=protected-access\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 49, in _assert\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nAn empty sequential model is created, no error.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nm = tf.keras.Sequential()\r\n```\r\n\r\n\r\n", "comments": ["@alexn11 \r\nCan you please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/38518#issuecomment-619538236) with same error and verify if more than one python process accessing tf at same time is running on your system, please verify the link and update. [also please try to disable gpu and try]\r\n\r\nSimilar issues for reference:\r\n[link](https://stackoverflow.com/questions/61200676/tensorflow2-1-invalidargumenterror-assertion-failed-0-opassert-name-eage) [link1](https://gitmemory.com/issue/tensorflow/tensorflow/38518/633284168)\r\n\r\n\r\n", "I've tried to see if any other program was using the GPU at the same time and also if any other instance of python was running. In both case it was negative:\r\n- Before:\r\n```\r\n$ pgrep python\r\n(nothing)\r\n$ nvidia-smi\r\nWed Jul 29 12:50:50 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 950M    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   47C    P8    N/A /  N/A |      0MiB /  2004MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n- During: i launch python on another terminal and type in the few command as described above then get the error.\r\n```\r\n$ pgrep python\r\n8625\r\n$ nvidia-smi\r\nWed Jul 29 12:52:16 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 950M    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   48C    P8    N/A /  N/A |   1781MiB /  2004MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      8625      C   python                           1778MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n(the amount of memory used seems to be quite high?)\r\n- After: I quit python, everything is back to nothing again:\r\n```\r\n$ pgrep python\r\n$ nvidia-smi\r\nWed Jul 29 12:53:15 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 950M    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   47C    P8    N/A /  N/A |      0MiB /  2004MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n(See below for the deactivated GPU)\r\n\r\n\r\n", "Seems to be working when deactivating the GPU:\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nimport tensorflow as tf\r\nm = tf.keras.Sequential()\r\n```\r\nAt that point I get the following output:\r\n```\r\n2020-07-29 12:57:51.389675: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-29 12:57:51.410142: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-07-29 12:57:51.410208: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: myhostname\r\n2020-07-29 12:57:51.410220: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: myhostname\r\n2020-07-29 12:57:51.410407: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.57.0\r\n2020-07-29 12:57:51.410449: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.57.0\r\n2020-07-29 12:57:51.410459: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.57.0\r\n2020-07-29 12:57:51.410912: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-29 12:57:51.419007: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300800000 Hz\r\n2020-07-29 12:57:51.419293: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d524787410 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-29 12:57:51.419314: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-29 12:57:51.420774: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n```\r\nThe model has been created:\r\n```\r\n>>> m\r\n<tensorflow.python.keras.engine.sequential.Sequential object at 0x7ff7a49cfc40>\r\n```", "@alexn11 \r\nAs its working by the deactivation, please confirm if you want to move this issue to closed status.", "Does this means that the problem is with Cuda and/or the drivers?\r\n\r\nTo be honest I don't know what I am supposed to do.\r\n", "@alexn11 , I'd recommend installing tensorflow-gpu through anaconda. It's the easiest way to get things up. BTW official installaiton guide says CUDA 10.1 (not sure if it helps).", "I am having a similar problem. The problem is that I guess you got cudnn updated to 8.0, with tf 2.3 and some other stuff.\r\n\r\nBasically, roll all the stuff to the previous version for it to work", "Hello I get the same problem when i try to use it.\r\nBut the source of the problem that the tf is asks the cudnn64_7.dll.\r\nbut in the cudnn 10.1 i just find cudnn64_8.dll.\r\nIn this reason i get older version from cudnn and get the cudnn64_7.dll. Tf is run and identified my gpu but the problem with sequential() is born, after i removed the 64_7.dll the problem solved, but i can't run on gpu.", "@alexn11 \r\nPlease update on CUDA installation, is this still an issue?", "I have cuda 11, cudnn 8 and tf 2.3, and I get the error.", "@thephet \r\nPlease confirm if you have visited the mentioned links and if you have tried after deactivating the GPU.", "@Saduf2019 If I deactivate the GPU it works fine but obviously very slow.\r\n\r\nThe error I get is the following:\r\n\r\n```\r\n2020-07-27 12:58:16.554013: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-07-27 12:58:16.871243: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-07-27 12:58:17.050553: F tensorflow/stream_executor/cuda/cuda_dnn.cc:1186] Check failed: cudnnSetRNNMatrixMathType(rnn_desc.get(), math_type) == CUDNN_STATUS_SUCCESS (3 vs. 0)\r\nAborted (core dumped) \r\n```", "@thephet \r\nThis error has been resolved in [this comment](https://github.com/tensorflow/tensorflow/issues/40877#issuecomment-653120196), please refer to this and confirm.", "@Saduf2019 Thanks for your answer. Reading that comment I am not sure what I am supposed to do to this fix the error.\r\n\r\nThe code I am trying to run is this one:\r\n\r\nhttps://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\r\n\r\nCheck at the end before the comments when he puts it all together.", "@alexn11 Can you please try with CUDA 10.1 and cuDNN 7.6 and let me know if you are still facing the same issue. Thanks!", "I have same issue too. I have 2 laptop. One has rtx2060 mobile and other gtx860M.\r\nI installed linux a while of time ago. before that it is worked smoothly but after i install windows 10 back its doesn't working anymore\r\nI m facing with this error on gtx860m. rtx one just working fine with these versions. but gtx one not working whatever i do\r\nvisual studio 2019 community with c++\r\nCuda: 10.1 update2\r\nCudnn: 7.6.5\r\nWindows10\r\n( two laptop has same installation versions) \r\nI tried all possiblities.\r\nI tried python 3.7, python 3.8, anaconda versions, i tried driver version wich comes with cuda, i tried updating driver version to 451. I tried tensorflow 2.2.0 and 2.3.0.\r\nand always i installed windows 10 from zero when trying different combination. \r\n\r\n```\r\nimport tensorflow as tf\r\ntrain_loss = tf.keras.metrics.Mean(name=\u201ctrain_loss\u201d)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nm = tf.keras.Sequential()\r\n```\r\n\r\n**or trying to train a model it s not working and giving this error**\r\n\r\n``` File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1507, in __init__\r\n\tself._init_from_args(\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1661, in _init_from_args\r\n\thandle = eager_safe_variable_handle(\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 242, in eager_safe_variable_handle\r\n\treturn _variable_handle_from_shape_and_dtype(\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 174, in _variable_handle_from_shape_and_dtype\r\n\tgen_logging_ops._assert(  # pylint: disable=protected-access\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 49, in _assert\r\n\t_ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Users\\mehmet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6843, in raise_from_not_ok_status\r\n\tsix.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\nInvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\n\r\n**or stoping like this.**\r\n\r\n```2020-08-05 17:36:46.242249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:46.242249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:48.741475: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-05 17:36:46.242249: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:48.741475: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-05 17:36:49.608631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\r\ncoreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-08-05 17:36:49.608692: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:49.613753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 17:36:49.618772: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 17:36:49.620362: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 17:36:49.626329: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 17:36:49.629463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 17:36:49.641028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 17:36:49.641171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 17:36:49.641781: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-05 17:36:49.659086: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18a99c5aff0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 17:36:49.659142: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-05 17:36:49.659427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\r\ncoreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-08-05 17:36:49.659465: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-05 17:36:49.659487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-05 17:36:49.659506: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-05 17:36:49.659523: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-05 17:36:49.659539: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-05 17:36:49.659555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-05 17:36:49.659576: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-05 17:36:49.659646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-05 17:36:49.751107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-05 17:36:49.751144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]  \t0\r\n2020-08-05 17:36:49.751154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-08-05 17:36:49.751380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3121 MB memory) -> physical GPU (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-08-05 17:36:49.755399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18a9c483ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-05 17:36:49.755435: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\r\n```\r\n", "Same as @mcagricaliskan , fresh windows install, \r\n\r\nIf I disable GPU acceleration everything works fine.\r\n\r\nCuda: 10.1 update2\r\nCudnn: 7.6.5\r\nWindows: 10 N Enterprise\r\npython 3.7 or python 3.8 (tried both)\r\ntensorflow 2.2.0 or 2.3.0, same error\r\n\r\nOutput nvidia-smi\r\n```\r\nThu Aug 06 20:13:29 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 451.67       Driver Version: 451.67       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 750 Ti WDDM  | 00000000:01:00.0  On |                  N/A |\r\n| 40%   26C    P8     1W /  38W |    375MiB /  2048MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A       888    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n|    0   N/A  N/A      1136    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A      6612    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\r\n|    0   N/A  N/A      7184    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\r\n|    0   N/A  N/A     12040    C+G   ...nputApp\\TextInputHost.exe    N/A      |\r\n|    0   N/A  N/A     12192    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nThose apps default on windows so I cannot remove them from the GPU processing,\r\n\r\nI saw that some of the apps could be using tensor in an encapsulated way and could be interfering in the execution, I've found in\r\n[This comment](https://github.com/tensorflow/tensorflow/issues/38518#issuecomment-633284168) from @pshved \r\n\r\nSo I cannot know the next steps to try to use CUDA with TF.\r\n\r\nFull Output trace.\r\n```\r\n>>> import tensorflow as tf\r\n2020-08-06 20:01:54.062996: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n>>> m = tf.keras.Sequential()\r\n2020-08-06 20:02:04.140553: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-06 20:02:04.188734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\ncoreClock: 1.0845GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 80.47GiB/s\r\n2020-08-06 20:02:04.190198: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-06 20:02:04.278519: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-06 20:02:04.340656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-06 20:02:04.368035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-06 20:02:04.441860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-06 20:02:04.482944: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-06 20:02:05.691214: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-06 20:02:05.691838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-06 20:02:05.721854: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e8dbcffce0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-06 20:02:05.722329: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-06 20:02:05.723997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 750 Ti computeCapability: 5.0\r\ncoreClock: 1.0845GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 80.47GiB/s\r\n2020-08-06 20:02:05.724720: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-08-06 20:02:05.725135: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-08-06 20:02:05.725493: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-06 20:02:05.725854: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-06 20:02:05.726529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-06 20:02:05.727025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-08-06 20:02:05.727351: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-08-06 20:02:05.727779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-06 20:02:06.240587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-06 20:02:06.241027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-08-06 20:02:06.241452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-08-06 20:02:06.242547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1455 MB memory) -> physical GPU (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-08-06 20:02:06.248210: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e8e2876090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-06 20:02:06.248571: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 750 Ti, Compute Capability 5.0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 116, in __init__\r\n    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 308, in __init__\r\n    self._init_batch_counters()\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 317, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 244, in _variable_v2_call\r\n    return previous_getter(\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2633, in default_variable_creator_v2\r\n    return resource_variable_ops.ResourceVariable(\r\n  File \"C:\\Users\\IuriAndreazza\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n```\r\n\r\n", "I have the same problem. \r\nJust two lines of code\r\n\r\n`from tensorflow.keras.models import Sequential`\r\n`model = Sequential()`\r\n\r\n> Traceback (most recent call last):\r\n  File \"C:/TF3/T2.py\", line 3, in <module>\r\n    model = Sequential()\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 464, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 116, in __init__\r\n    name=name, autocast=False)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 464, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 308, in __init__\r\n    self._init_batch_counters()\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 464, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 316, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 256, in _variable_v2_call\r\n    shape=shape)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2650, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1544, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1692, in _init_from_args\r\n    graph_mode=self._in_graph_mode)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 245, in eager_safe_variable_handle\r\n    shape, dtype, shared_name, name, graph_mode, initial_value)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 177, in _variable_handle_from_shape_and_dtype\r\n    math_ops.logical_not(exists), [exists], name=\"EagerVariableNameReuse\")\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 49, in _assert\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\TF3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6921, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n\r\n\r\nWindows 10\r\nPyCharm 2020.2\r\nTensorflow 2.3. \r\nCUDA and CUDNN compatible versions have been installed correctly.\r\n\r\nI am planning on attending the TensorFlow Certification and one of the prerequisites is to check if PyCharm works correctly with the Image Classification example on the Tensorflow page.\r\n\r\nI need the GPU and checking if it works without the GPU is irrelevant. \r\n", "[This](https://github.com/tensorflow/tensorflow/blob/c86ba08697d2c05e2a913774e70d11afe517528e/tensorflow/python/ops/resource_variable_ops.py#L167) is the failing assertion which seems to be GPU independent.  Do you know if you're doing something weird in your model?\r\n\r\n```\r\nfrom tensorflow.keras.models import Sequential\r\nmodel = Sequential()\r\n```\r\n\r\nseems to work fine for me though, on `tf-nightly`.", "> [This](https://github.com/tensorflow/tensorflow/blob/c86ba08697d2c05e2a913774e70d11afe517528e/tensorflow/python/ops/resource_variable_ops.py#L167) is the failing assertion which seems to be GPU independent. Do you know if you're doing something weird in your model?\r\n> \r\n> ```\r\n> from tensorflow.keras.models import Sequential\r\n> model = Sequential()\r\n> ```\r\n> \r\n> seems to work fine for me though, on `tf-nightly`.\r\n\r\nThe code file has nothing more than these two lines. No other imports. No further modeling. I have installed all packages in this PyCharm project by updating the interpreter. TensorFlow 2.3.0 is installed and tf-nightly 2.4.0 dev20200811.\r\n\r\nWhy would this work in Jupyter Notebooks or Colab but not in PyCharm?", "> Why would this work in Jupyter Notebooks or Colab but not in PyCharm?\r\n\r\nI'm not familiar with PyCharm so unfortunately I cannot help with this.", "I have the same problem. Python 3.7, tf 2.3 , CUDA 10.1 and cuDnn 7.6.  If  I deactivate GPU, using CPU will be so slow and it doesn't actually solve the problem. I've tried to downgrade to tf 2.2.0 and it can solve this problem.", "@gowthamkpr\r\n Right now I'm running with the following (downgraded) versions\r\n```\r\ncuda 10.2.89-5\r\ncudnn 7.6.5.32-4\r\ntensorflow-cuda 2.2.0-1\r\n```\r\nWaiting for the next update that would guarantee that my code runs.", "Same problem with\r\npython 3.7, **tf 2.3** , CUDA 10.1 and cuDnn 7.6, windows 10\r\npython 3.7, **tf-nigthly (1-SEP-2020)** , CUDA 11.0 and cuDnn 8.03, windows 10\r\n\r\nalso replicated by just `tf.Variable((2,3))`\r\n\r\nthe problem does not happen in tensorflow 2.2", "Same problem! CUDA 11.0 and tensorflow 2.3.0\r\nWhen running the code without GPU everything is fine, but slow. WIth GPU (google cloud tensorflow GPU) the programme crashes", "@gowthamkpr thank you that worked for me. CUDA 11.0 was the problem for me. Once downgrading to CUDA 10.1 everything worked as expected.", "@samuelvisscher what cuDNN version are you using in order to make it work please ? \r\nI tried 8.0.3 and 7.6.5 and it is not working...", "@q-55555 I'm using cuDNN 7.6.5.\r\n\r\nIf it helps, I setup my environment using Google Cloud AI Platform Notebooks, using the CUDA Toolkit 10.1 instance (with GPU drivers pre-installed). Just had to install tensorflow-gpu 2.3.1.", "@samuelvisscher Thanks for your reply.\r\nUnfortunately, it is not working for me on Windows 10, tensorflow-gpu 2.3.1, CUDA Toolkit 10.1 and cuDNN 7.6.5.\r\nI don't have the error with tensorflow 2.2.0 and tf-nightly.", "So I guess no other ideas to help us making it works with tensorflow 2.3.1 please ?", "> So I guess no other ideas to help us making it works with tensorflow 2.3.1 please ?\r\n\r\nMy understanding is that older GPUs are not supported anymore by the default version of either TensorFlow or Cuda. Solution would be to compile the corresponding library on your system. I tried something like that but failed to complete the compilation.", "Thank you for your reply @alexn11 \r\nI don't think the problem is related to GPUs compatibility with Tensorflow or Cuda because I don't get the error with the most recent version of tensorflow (tf-nightly).", "It seems to be a Windows related bug. I tested it on a Linux GPU colab with the same CUDA and cuDNN version and it's working fine...", "No I have the issue with Arch Linux.", "@omalleyt12, I think your commit https://github.com/tensorflow/tensorflow/commit/69565ec4003902794bc94e10ba5fe9469a0b3ae4 is creating this issue. When calling self._init_batch_counters() function in tensorflow/python/keras/engine/training.py, we get the following error: \r\n\r\n\"tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\". \r\n\r\nThe issue appears with GPU activated (but not with all GPU..., for example in colab, it is working fine).\r\n\r\nMaybe you have an idea or a workaround to propose.\r\nCould you help us please ?", "this was resolved for me after upgrading to CUDA/cuDNN: 11.1 (using `tensorflow@2.4-rc0`)\r\n\r\n(though there were a few other less critical issues \r\nhttps://github.com/tensorflow/tensorflow/issues/44192\r\nhttps://github.com/tensorflow/tensorflow/issues/44381\r\n)", "I managed to make it works with tensorflow 2.3.1 in recompiling tensorflow for CUDA 11.1 and cudnn 8.0.4.", "I have got windows 10 with GeForce GTX 850M nvida GPU and configured cuda 11.1 and cudnn 8.04 and I have tensorflow 2.3.0 but I am getting \"Could not load dynamic library 'cudart64_101.dll'\".  can any one please assist on this?", "> I managed to make it works with tensorflow 2.3.1 in recompiling tensorflow for CUDA 11.1 and cudnn 8.0.4.\r\n\r\n@q-55555 how did you managed to get it working? did you recompile the tensorflow on windows machine? any step by step guide would be appreciated.\r\n\r\nThanks", "@amitport  I tried renaming the file cudart64_111.dll to cudart64_101.dll but no luck.", "@m4masood no I deleted the previous messages because mine was with cusolver64 and not with cudart64_101.dll.\r\n\r\nif it helps, make sure you are using a tf version that looks for cuda 11.1 (nightly or 2.4 rc)\r\n2.3 will still look for cuda 10.1 on windows, so if you upgraded cuda and not tf you still have an incompatibility", "@m4masood Yes I recompiled tensorflow on Windows (See https://www.tensorflow.org/install/source_windows).\r\nYou need to specify the use of CUDA 11.1 and cudnn 8 in the configure.py step.", "@alexn11 It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41855\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41855\">No</a>\n"]}, {"number": 41854, "title": "Raspberry pi running tensorflow lite at 0.69 -0.73 fps.", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry pi 4 8gb ram\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): using TensorFlow 1.14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIt is giving incredibly low fps. 0.69 for tflite. This is the same when I am running with normal TensorFlow as well.\r\n**Describe the expected behavior**\r\nat 2 fps or above\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://github.com/Dasinator21/Replicate-Error\r\nGithub link above to my tflite model as well as the code I am running the model with.\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 41853, "title": "Using tf.keras.Model.test_on_batch inside tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nGiven a tf.keras.Model and a tf.data.Dataset, I'd like to be able to iterate a batched dataset and perform mode.test_on_batch and collect the results.\r\nNote: eager execution is disabled due to other reported bugs \r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.compt.v1.disable_eager_execution()\r\n\r\nx = tf.keras.Input(shape=(3,))\r\ny = tf.keras.layers.Dense(2)(x)\r\nmodel = tf.keras.Model([x], [y])\r\ncompile_kwargs = {'optimizer': 'sgd', 'loss': tf.keras.losses.mean_squared_error}\r\nmodel.compile(**compile_kwargs)\r\n\r\ndata = [np.full(shape=(3,), fill_value=i) for i in range(10)]\r\ndata = np.asarray(data)\r\nlabels = [np.full(shape=(2,), fill_value=i)*3 for i in range(10)]\r\nlabels = np.asarray(labels)\r\n\r\nds = tf.data.Dataset.from_tensor_slices((data, labels)).batch(2)\r\n\r\n# Option 1 - doesn't work when eager execution is disabled\r\n# Throws - RuntimeError: __iter__() is only supported inside of tf.function or when eager execution is enabled.\r\n# ins = ds.map(lambda x, y: x)\r\n# for batch in ins:\r\n   # r = model.test_on_batch(batch)\r\n   # print(r)\r\n\r\n# Option 2\r\n@tf.function\r\ndef test_on_batch_example(model, ins):\r\n  for batch in ins:\r\n    r = model.test_on_batch(batch)\r\n    print(r)\r\n  \r\nins = ds.map(lambda x, y: x)\r\ntest_on_batch_example(model, ins)\r\n```\r\n\r\nOption 2 throws:\r\n    <ipython-input-7-e1744a238c31>:5 test_on_batch_example  *\r\n        r = model.test_on_batch(batch)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:1163 test_on_batch  **\r\n        self._make_test_function()\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2054 _make_test_function\r\n        **self._function_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3943 function\r\n        model = models.Model(inputs=inputs, outputs=outputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:242 __new__\r\n        return functional.Functional(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:464 _method_wrapper\r\n        result = method(self, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:111 __init__\r\n        self._init_graph_network(inputs, outputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:464 _method_wrapper\r\n        result = method(self, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:138 _init_graph_network\r\n        base_layer_utils.create_keras_history(self._nested_outputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:189 create_keras_history\r\n        _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:249 _create_keras_history_helper\r\n        constants[i] = backend.eval_in_eager_or_function(op_input)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3880 eval_in_eager_or_function\r\n        raise ValueError('Unknown graph. Aborting.')\r\n\r\n    ValueError: Unknown graph. Aborting.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1exTEugyIZCoc4j0rGyexj1CTf58weuaO?usp=sharing\r\n\r\n", "comments": ["@louisgriffin,\r\nAs per the error log from the Colab notebook you have linked, I've moved `Model.test_on_batch` outside of `tf.function` and was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8bdf32af1485e7e7c63a3431eac2939f/41853.ipynb). Thanks!", "@amahendrakar \r\nyou missed ```tf.compat.v1.disable_eager_execution()``` that was in the original gist. \r\n\r\nthat is the reason why `model.test_on_batch`  was originally inside `tf.function` to allow for iterations over dataset.\r\nI disabled eager execution because it was causing other (reported) bugs", "Hi @louisgriffin, using `test_on_batch` inside tf.function will result in a runtime error as noted [here in the docs.](https://www.tensorflow.org/api_docs/python/tf/keras/Model#test_on_batch) This is because test_on_batch is a high-level end point that manages its own tf.function. You might want to try `Model.test_step`, which is just the logic of the testing step and is safe to use with tf.function.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41852, "title": "ERROR HINDERING MY ABILITY TO USE TENSOR FLOW", "body": "\r\n[TF Error.docx](https://github.com/tensorflow/tensorflow/files/4993986/TF.Error.docx)\r\nThank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Having successfully downloaded tensor flow on Anaconda, I continue to encounter an error when I try to import tensorflow on Jupyter. ImportError: DLL load failed: The specified module could not be found. Please advise about how I can go about this challenage.", "@fanabis \r\nPlease refer to [this link ](https://github.com/tensorflow/tensorflow/issues/36683#issuecomment-585097726 )and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41852\">No</a>\n"]}, {"number": 41851, "title": "Keras Callbacks logs / numpy_logs not in sync", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.8\r\n\r\n\r\n**Describe the current behavior**\r\nSome callbacks (e.g. ProgbarLogger, ModelCheckpoint, ...) have the flag `self._supports_tf_logs = True`. If other callbacks (especially custom Callback) don't have this property, then those callbacks do not have acces to the same logs. \r\nIn the code example below, `ModelCheckpoint` can not use the `'val_log_loss'` as a monitor value from the `CustomMetric` callback.\r\nThis results from the commit https://github.com/tensorflow/tensorflow/commit/50480faea75f56def464b84f251b4aee388dfce9 where a new `numpy_logs` property has been introduced, without making sure to sync it with the pre-existing `logs` property.\r\n\r\n**Describe the expected behavior**\r\nThe two propertys `numpy_logs` and `logs` should contain the same information OR it should be made clear in the docs (https://www.tensorflow.org/guide/keras/custom_callback#keras_callbacks_overview) what `_supports_tf_logs` does and that there could be compatibility issues.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n...\r\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint\r\n...\r\n\r\nclass CustomMetric(Callback):\r\n    def __init__(self, x_valid, y_valid):\r\n        super().__init__()\r\n        self.x_valid = x_valid\r\n        self.y_valid = y_valid\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        y_pred = self.model.predict(self.x_valid, batch_size=BATCHSIZE)\r\n\r\n        logs['val_log_loss'] = metrics.log_loss(self.y_valid, y_pred)\r\n\r\n...\r\n\r\nmodel.fit(\r\n        x_train,\r\n        y_train,\r\n        validation_data=(x_valid, y_valid),\r\n        shuffle=True,\r\n        batch_size=BATCHSIZE,\r\n        epochs=EPOCHS,\r\n        verbose=1,\r\n        callbacks=[CustomMetric(x_valid, y_valid), ModelCheckpoint('test.h5', 'val_log_loss', verbose=1, save_best_only=True, mode='min')]\r\n    )\r\n\r\n...\r\n```\r\n\r\n**Other info / logs** \r\nSee commit https://github.com/tensorflow/tensorflow/commit/50480faea75f56def464b84f251b4aee388dfce9", "comments": ["@albert-92 Can you please provide a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan Sure. Here's a standalone code to reproduce the issue:\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.optimizers import RMSprop\r\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, History\r\nfrom tensorflow.keras import utils\r\nfrom sklearn import metrics\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 2\r\n\r\n# Custom callback, where the logs are actually the numpy_logs object \r\n# if the flag self._supports_tf_logs is not set to True\r\nclass CustomMetric(Callback):\r\n    def __init__(self, x_valid, y_valid):\r\n        super().__init__()\r\n        self.x_valid = x_valid\r\n        self.y_valid = y_valid\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        y_pred = self.model.predict(self.x_valid, batch_size=batch_size)\r\n\r\n        logs['val_log_loss'] = metrics.log_loss(self.y_valid, y_pred)\r\n\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255.\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255.\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = utils.to_categorical(y_train, num_classes)\r\ny_test = utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(64, activation='relu', input_shape=(784,)))\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=RMSprop(),\r\n              metrics=['accuracy'])\r\n\r\n# The following part works partly as intended.\r\n# history.history contains the key 'val_log_loss' even though it is not printed by the ProgbarLogger\r\n# (since ProgbarLogger uses logs and CustomMetric numpy_logs)\r\nhistory = model.fit(x_train, y_train,\r\n                    batch_size=batch_size,\r\n                    epochs=epochs,\r\n                    verbose=1,\r\n                    validation_data=(x_test, y_test),\r\n                    callbacks=[\r\n                        CustomMetric(x_test, y_test)\r\n                    ])\r\n\r\nprint(history.history)\r\n\r\n# This following part does not work as intented.\r\n# ModelCheckpoint outputs the warning\r\n# \"WARNING:tensorflow:Can save best model only with val_log_loss available, skipping.\"\r\n# because 'val_log_loss' is in the numpy_logs object and ModelCheckpoint uses the logs object\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test),\r\n          callbacks=[\r\n              CustomMetric(x_test, y_test),\r\n              ModelCheckpoint('test.h5', monitor='val_log_loss', verbose=1, save_best_only=True, mode='min')\r\n          ])\r\n\r\n```", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200729`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/87ab302844f49a73e3c53b032a8565b8/untitled200.ipynb).Thanks!", "Facing the same issue when moved from 2.2 to 2.3", "@reedwm @omalleyt12 this issue is affecting our Keras callbacks in Horovod as well.  As reported in https://github.com/horovod/horovod/issues/2440, when using `MetricAverageCallback` to average metrics across workers, the history is correctly reporting averages, but the logs are not.  When setting `callback._supports_tf_logs = True` we get the exact opposite behavior: logs are correctly averaged but history is not.  \r\n\r\nCan someone from your team help in providing a fix / workaround for this?\r\n\r\nHere's a standalone script using Horovod that repros the issue:\r\n\r\n```\r\n        import tensorflow as tf\r\n        from tensorflow import keras\r\n        import horovod.tensorflow.keras as hvd\r\n\r\n        hvd.init()\r\n\r\n        opt = tf.keras.optimizers.Adam(0.01)\r\n        opt = hvd.DistributedOptimizer(opt)\r\n\r\n        def test_metric(y_true, y_pred):\r\n            return hvd.rank()\r\n\r\n        model = keras.models.Sequential()\r\n        model.add(keras.layers.Dense(2, input_shape=(3,)))\r\n        model.compile(loss=keras.losses.mean_squared_error,\r\n                      optimizer=opt,\r\n                      metrics=[test_metric],\r\n                      experimental_run_tf_function=False)\r\n\r\n        x = np.random.random((1, 3))\r\n        y = np.random.random((1, 3, 2))\r\n\r\n        callbacks = [\r\n            hvd.callbacks.BroadcastGlobalVariablesCallback(0),\r\n            hvd.callbacks.MetricAverageCallback(),\r\n        ]\r\n\r\n        train_history = model.fit(\r\n            x,\r\n            y,\r\n            steps_per_epoch=10,\r\n            callbacks=callbacks,\r\n            epochs=1\r\n        )\r\n\r\n        expected = sum(range(hvd.size())) / hvd.size()\r\n        results = train_history.history.get('test_metric')\r\n        assert results[0] == expected\r\n```", "/CC @fchollet", "I made a PR to fix this: https://github.com/tensorflow/tensorflow/pull/47922", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41851\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41851\">No</a>\n", "Thanks for the PR @lgeiger \r\nThis doesn't seem to fix the progress bar values on the example provided by tgaddair on tf2.5. Do you have any recommendation for that case?\r\nThank you"]}]