[{"number": 37711, "title": "Add interface function to get the needed arena size", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- Tensorflow version (commit SHA if source): 2.1\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm Mbed OS\r\n\r\n**Describe the problem**\r\nThe greedy memory planner places activation buffers in a smart way to re-use memory between layers. It would be nice to have a function that the application can call to find out what the actual memory usage turned out to be, to be able to dimension the arena buffer properly. One suggestion in to have add a method to the interpreter class, 'get_total_arena_size()', that can be called after all tensors has been allocated. Let me know if there has been some thoughts around this.\r\n\r\nThis issue is similar to #35070.\r\n\r\nThanks!\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["I believe this issue is solved now with the arena_used_bytes()."]}, {"number": 37710, "title": "util - minor spelling tweak", "body": "", "comments": ["Thank you for the PR but I don't think this is worth the hours of CI it will take to integrate it, especially since it is on a TODO. There are ongoing PRs which fix typos across an entire directory, and I think they already cover this too."]}, {"number": 37709, "title": "Tensorflow documentation on Image Captioning has non existing methods.", "body": "## URL(s) with the issue:\r\nI was running Image captioning example given in tensorflow website as it is in Colab. I found out that the documentation made a mention about \"**image_features_extract_model**\" which was not defined anywhere earlier. Neither could I find any method related to that on Google.\r\n\r\nThe lines that caused the error is given below:\r\n\r\n```\r\n# Get unique images\r\nencode_train = sorted(set(img_name_vector))\r\n\r\n# Feel free to change batch_size according to your system configuration\r\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\r\nimage_dataset = image_dataset.map(\r\n  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\r\n\r\nfor img, path in image_dataset:\r\n  batch_features = image_features_extract_model(img)\r\n  batch_features = tf.reshape(batch_features,\r\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\r\n\r\n  for bf, p in zip(batch_features, path):\r\n    path_of_feature = p.numpy().decode(\"utf-8\")\r\n    np.save(path_of_feature, bf.numpy())\r\n\r\n```\r\nThe original documetation can be found at the following url:\r\n[https://www.tensorflow.org/tutorials/text/image_captioning](url)\r\n\r\nI was confused whether i should report this as a bug or documentation error. I went with the latter.\r\n\r\n\r\nIs the link to the source code correct?\r\nYes\r\nThe original documetation can be found at the following url:\r\n[https://www.tensorflow.org/tutorials/text/image_captioning](url)\r\n\r\n", "comments": ["Hi, I think `image_features_extract_model` is defined in the notebook. See these line in the \"Initialize InceptionV3 and load the pretrained Imagenet weights\" section.\r\n\r\n```python\r\nimage_model = tf.keras.applications.InceptionV3(include_top=False,\r\n                                                weights='imagenet')\r\nnew_input = image_model.input\r\nhidden_layer = image_model.layers[-1].output\r\n\r\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\r\n```\r\n\r\nI'm running the notebook in Colab. If I can reproduce your error, I will report back here.", "@Mahe-git2hub, As @jaketae  mentioned `image_features_extract_model` is already defined in notebook. I have successfully compiled above mentioned tutorial and there is no error. For your reference link of gist is [here](https://colab.research.google.com/gist/khimraj/59ad82815d6a17139685a608ef5dc20d/image_captioning.ipynb).\r\nHope it helps.", "Hi, I am new here so bear with me and let me know if I should make a new thread! I get a bit similar error from the same part of the same tutorial code:\r\n\r\n```\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-5-c54eeda8599b> in <module>()\r\n----> 1 encode_train = sorted(set(img_name_vector))\r\n      2 \r\n      3 # Feel free to change batch_size according to your system configuration\r\n      4 image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\r\n      5 image_dataset = image_dataset.map(\r\n\r\nNameError: name 'img_name_vector' is not defined\r\n```\r\n\r\nHave you others not had this or have you fixed it? img_name_vector seems to be defined in the tutorial so I really wonder what the problem is."]}, {"number": 37708, "title": "applications.resnet.ResNet50 means ResNet50 or ResNet34", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50V2\r\n\r\n## Description of issue (what needs changing):\r\n\r\nTF provides two type of ResNet models. The first is ResNet50 which is implemented by the following code:\r\n\r\nhttps://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py#L423-L441\r\n\r\nwhere, `stack1` is **basic** version of residual function:\r\nhttps://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py#L64-L127\r\n\r\nAnd the second is ResNet50V2 which is implemented by the following code:\r\n\r\nhttps://github.com/keras-team/keras-applications/blob/b34c10628a0ab436542e9160f98de72b49084bbe/keras_applications/resnet_common.py#L483-L501\r\n\r\nwhere, `stack2` is **bottleneck** version of residual function:\r\nhttps://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py#L175-192\r\n\r\nThe original [paper](https://arxiv.org/pdf/1512.03385.pdf) lists different type of ResNet in Table 1.\r\n\r\nBy the original  definition, the `ResNet50` should be 34-layer ResNet in the Table 1.\r\n\r\nFrom the implementation by `pytorch`: \r\n\r\nhttps://github.com/pytorch/vision/blob/cc43e0a98368055d7a661651a2b9dbf28a19e533/torchvision/models/resnet.py#L244-L266\r\n\r\nThey claim the first one is ResNet34.\r\n\r\nSo I suggest that the `ResNet50` should change its name.\r\n\r\n\r\n\r\n", "comments": ["I'm not entirely sure if this is the case. \r\n\r\nhttps://github.com/keras-team/keras-applications/blob/b34c10628a0ab436542e9160f98de72b49084bbe/keras_applications/resnet_common.py#L101\r\n\r\nHere, you see that the convolution layers follows the structure as presented in Table 1 of the original paper. Notice how the number of filter goes from `filters`, `filters`, to `4 * filters`, whereas the stride goes from `1`, `kernel_size`, and `1`. I believe this is the correct implementation.\r\n\r\nIf I'm missing something, please let me know!", "Sorry, My fault. Your right.\r\n\r\n"]}, {"number": 37707, "title": "Fix `'for' loop initial declarations are only allowed in C99 mode`", "body": "Fix `'for' loop initial declarations are only allowed in C99 mode`", "comments": ["What toolchain are you compiling with? All compilers that are supported by TF should not complain on this", "I'm using this docker image: `amazonlinux:1`\r\n\r\n```\r\nsh-4.2# gcc -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-amazon-linux/4.8.5/lto-wrapper\r\nTarget: x86_64-amazon-linux\r\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-linker-hash-style=gnu --enable-languages=c,c++,fortran,ada,lto --enable-plugin --enable-initfini-array --disable-libgcj --with-isl=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-amazon-linux/isl-install --with-cloog=/builddir/build/BUILD/gcc-4.8.5-20150702/obj-x86_64-amazon-linux/cloog-install --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-amazon-linux\r\nThread model: posix\r\ngcc version 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC)\r\n```", "That compiler toolchain is not one of those we support so we cannot help you if you get in errors there.\r\n\r\nCan you try using the TensorFlow supplied docker instead?", "But this error is the only one I get from compile. I think it will be good to fix this.", "Sure, as this change has a noop semantic, we can take it.\r\n\r\nWas just saying in general, especially since there might be other places where this code pattern happens.", "Ah, by the way. Let me tell you why I had to use this env to compile. Because I'm trying to using tflite in AWS Lambda. And the env `amazonlinux:1` only give me GLIBC_2.17, but tflite require GLIBC_2.27"]}, {"number": 37706, "title": "problem with batch processing for tflite in android java", "body": "java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (800 != 400)\r\n    Node number 0 (RESHAPE) failed to prepare.\r\n  \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:162)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)\r\n        at id.unify.gait_sample_app.gait.GaitInferenceManager.getRawScore(GaitInferenceManager.java:55)\r\n        at id.unify.gait_sample_app.gait.Gait.score(Gait.java:160)\r\n        at id.unify.gait_sample_app.ui.main.MainFragment$2.onChanged(MainFragment.java:127)\r\n        at id.unify.gait_sample_app.ui.main.MainFragment$2.onChanged(MainFragment.java:115)\r\n        at androidx.lifecycle.LiveData.considerNotify(LiveData.java:113)\r\n        at androidx.lifecycle.LiveData.dispatchingValue(LiveData.java:131)\r\n        at androidx.lifecycle.LiveData.setValue(LiveData.java:289)\r\n        at androidx.lifecycle.MutableLiveData.setValue(MutableLiveData.java:33)\r\n        at androidx.lifecycle.LiveData$1.run(LiveData.java:91)\r\n        at android.os.Handler.handleCallback(Handler.java:751)\r\n        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6776)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1520)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1410)\r\n\r\n\r\nIt works with one single input with the dimension [1,1,4,100]\r\nBut crashing when trying to process multiple inputs. For example [2,1,4,100]\r\n// dimension [batchsize, 1, 4, 100]\r\nfloat[][][][] modelInput = convertWalkCycleTo4DFloatArray(walkCycles);\r\nmodelOutput = new float[batch.size()][1001];\r\nmodel.resizeInput(0, [batchsize,1,4, 100]);\r\nmodel.run(modelInput, modelOutput);\r\n\r\nI check method getInputTensor, and clearly the resizing has been successful.\r\nThis same tflite model can process inputs in batch in iOS and python\r\n", "comments": ["@TanDatDo,\r\nPlease provide the Tensorflow version and Provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "I use  'org.tensorflow:tensorflow-lite:2.1.0'\r\n\r\nSteps I have been through:\r\n``` \r\nInterpreter.Options options = new Interpreter.Options();\r\n options.setNumThreads(3);\r\n options.setUseNNAPI(true);\r\n Interpreter model = new Interpreter(mappedByteBuffer, options);\r\n//NOTE assign the modelInput to float array with dimension of [batchSize, 1, 4, 100]\r\nfloat[][][][] modelInput = convertWalkCycleTo4DFloatArray(walkCycles);\r\nmodelOutput = new float[batchSize][1001];\r\nint[] dimensions= new int[4];\r\ndimensions[0]=batchSize\r\ndimensions[1]=1;\r\ndimensions[2]=4;\r\ndimensions[3]=100;\r\nmodel.resizeInput(0, dimensions);\r\nmodel.run(modelInput, modelOutput);\r\n```\r\nWorking when batchsize=1, but crashing when batchSize> 1", "Until recently, there were no real guarantees that a model could be successfully resized at runtime. However, we've added the ability to convert models with proper batch dimension placeholders, such that you can now successfully resize at runtime.\r\n\r\nCan you try converting your model with the latest TF nightly build, leaving the value of the batch dimension as None?", "Oh sure! Let me try that, btw, should we use tflite nightly version for production, in general?\r\nhttps://www.tensorflow.org/lite/guide/android.\r\nOh wait, in java, it requires to use float[] instead of Float[]\r\nand float require non-null value.", "We try to maintain a pretty high bar for our nightly builds in terms of stability, but still encourage use of our versioned release builds in production. It's good for prototyping and development, though. \r\n\r\n> Oh wait, in java, it requires to use float[] instead of Float[]\r\n> and float require non-null value.\r\n\r\nRight, when I mentioned a None dimension, I was referring to model *conversion*. You still need to give explicit dimensions during model *execution*.", "thanks will try that when I can! \r\n", "Hi! it does not accept none for version 2.0.2 onward\r\nAnd nightly doesn't accept our usage of selu activation.", "> And nightly doesn't accept our usage of selu activation.\r\n\r\nCan you speak more to this? What is the exact failure?", "Yup, the converter does not allow selu\r\nwhen running converter.convert", "The `selu` activation has never been supported, have you tried using a different activation in your graph?", "In particular, `selu` requires certain initialization parameters that aren't supported by TFLite for inference (see https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal). If you'd like to run this model natively in TFLite your best bet is to find an alternative.", "> Until recently, there were no real guarantees that a model could be successfully resized at runtime. However, we've added the ability to convert models with proper batch dimension placeholders, such that you can now successfully resize at runtime.\r\n> \r\n> Can you try converting your model with the latest TF nightly build, leaving the value of the batch dimension as None?\r\n\r\nHi @jdduke , this seems to work on android `cpu`, but gives me an error for **gpu** and **nnapi** delegate. the error is same as [Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors](https://github.com/tensorflow/tensorflow/issues/38036), can we resolve this for **gpu** and **nnapi**", "Hi @sirius0503 , any chance you could share your model (or a minimal .tflite model repro) that we can use for testing?", "@jdduke Sure, here is the [tflite model](https://drive.google.com/file/d/1l37Ta6kcQMkEAHKlWL9dwPx3-U411xhv/view?usp=sharing), plz use this model for testing. \r\n\r\nIt's mostly linear algebra ops, ", "We're still working on this, hoping to have a fix for the next TF release (2.3). Will post here when a fix makes it into the nightlies.", "@TanDatDo,\r\nSorry for the delayed response. From the documentation of [Post Training Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) and [RNN TF Lite Converter Guide](https://www.tensorflow.org/lite/convert/rnn), **`Batch Processing`** seems to be implemented for **`TF Lite`**. Can you please check? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@jdduke i am also trying to get batch processing to work in Android java, but faced the same issue as reported earlier in this thread. i tried to follow your instructions regarding conversion of the model and keeping batch_size as None, but i get the following error\r\n\r\nValueError: invalid literal for int() with base 10: 'None'\r\n\r\n\r\nPls note that my model graph was generated using 1.x , so for conversion i still have to use  1.x", "@karimnosseir and @miaout17 should be able to assist further.", "thanks @jdduke , @karimnosseir and @miaout17  let me know if you need any more information on this", "@amitmate if you didn't convert the model with None shape for the dynamic shape, then resizing can be invalid. You have to convert it with dynamic shape.\r\n\r\nto verify that it converted as you expect. You can use input_details\r\n```\r\ninterpreter  = tf.lite.Interpreter(model_path=\"model.tflite\")\r\ninterpreter.get_input_details()\r\n```\r\n\r\nThis should print the details for all inputs, one of fields is shape_signature, if it doesn't have '-1' in the dimension you are trying to reshape then it is an invalid reshape.\r\n\r\nIf you're using python calling\r\n```\r\ninterpreter.resize_input_tensor with strict=True \r\n```\r\nwill validate that the resizing is valid for you (by default it is not).\r\nSame argument is also available in Java resizeInput method.\r\n\r\n\r\nFor the conversion, you need to keep the 'None' in shape in your original model (not part of conversion). Can you please how are you trying to convert your model, or when are you getting this error.\r\n\r\nThanks", "@karimnosseir  thanks for your response. This particular model for which we are trying, we had generated (trained) the frozen graph using the tf1 scripts and  used the default ssd mnet graph and checkpoints as recommended in tensorflow repo (http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz). To convert, we use the \"tflite_convert --input_shape=.. \" command , seems like the original model in tensorflow repo may be the problem, any idea on how to resolve this?", "@karimnosseir , do you need any more information on this?", "Can you verify that the original TF graph you're converting has dynamic shape, if not you might want to look in how to modify the graph - no idea how easy/hard this is.", "thanks a bunch @karimnosseir  , our graph does not have dynamic shape. also it was a tf1 graph (from tf1 modelzoo). It seems like only option to move forward (to enable batch inferencing) is to choose a new dynamic graph from tf2 modelzoo and retrain it..is this understanding  correct?\r\n\r\nif so, the there is no information to identify which of the tf2 models (objdet models from modelzoo) support dynamic batch size..https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\r\n\r\nthe tf2 version of the ssd model in the modelzoo has dynamic shape for img width and height , but not for batch size..\r\n\r\nso, the question really morphs to , can you pls point us to some ssd models in the modelzoo that support dynamic batch size?"]}, {"number": 37705, "title": "TFDS tests fail with tf-nightly", "body": "**System information** \r\n- TensorFlow version: - `tf-nightly` \r\n- Python version: - 3.6\r\n\r\n**Describe the current behavior**\r\nTensorflow-dataset tests fail with tf-nightly while it works fine with TF 1.15 and TF 2.1\r\n\r\n**Describe the expected behavior**\r\nTests should pass.\r\n\r\n**Standalone code to reproduce the issue** \r\n[Colab Link](https://colab.research.google.com/drive/1kdqmjjp-Omg3GfKO4FthubegCnJUrNqf#scrollTo=xB6MPLOCrtQ7)\r\n\r\nMore Info: https://github.com/tensorflow/datasets/issues/1670\r\n", "comments": ["i have replicated mentioned issue it [persist](https://colab.sandbox.google.com/gist/Saduf2019/9ab988049e5bda1ed0f2046abeeca426/37705.ipynb) in nightly", "@vijayphoenix This issue is already been discussed [here](https://github.com/tensorflow/datasets/issues/1670). This issue is primarily due to tfds api itself itself and has already been posted there. Closing this issue as it belongs to [tensrflow/datasets repo](https://github.com/tensorflow/datasets/issues).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37705\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37705\">No</a>\n", "Our tests are back to green. Thank you: https://source.cloud.google.com/results/invocations/1f44cdab-ab19-4b5d-8cc7-1793fb51864a/targets/tensorflow_datasets%2Fgh_testing%2Fcontinuous/log"]}, {"number": 37704, "title": "[r2.2:Cherrypick] Remove the trailing 'm' , since the tag for py38 is not cp38m but cp38.", "body": "PiperOrigin-RevId: 301682546\nChange-Id: I86e3632702ee77f82e3a59ba489a298930588ab9", "comments": []}, {"number": 37703, "title": "Compiling TF 2.2.0-rc0 with bazel 2.0.0 fails (MacOS)", "body": "**System information** \r\n\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): MacOS 10.15\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0-rc0\r\n- Python version: python3 3.7.7\r\n- Bazel 2.0.0\r\n- GCC/Compiler version (if compiling from source): Xcode 11.3.1\r\n- CUDA/cuDNN version: - GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n1. GitClone TF, checkout version 2.2.0-rc0\r\n2. ./configure with standard/default options\r\n3. run compilation as indicated by manual\r\nResult: Compilation stops unsuccessful moments after it starts (log below)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=130\r\nINFO: Reading rc options for 'build' from /Users/feranick/Desktop/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/feranick/Desktop/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /Users/feranick/Desktop/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/opt/local/bin/python3 --action_env PYTHON_LIB_PATH=/opt/local/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/opt/local/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /Users/feranick/Desktop/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/feranick/Desktop/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:opt in file /Users/feranick/Desktop/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:v2 in file /Users/feranick/Desktop/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:macos in file /Users/feranick/Desktop/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nINFO: Call stack for the definition of repository 'io_bazel_rules_closure' which is a http_archive (rule definition at /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - /Users/feranick/Desktop/tensorflow/WORKSPACE:5:1\r\nERROR: An error occurred during the fetch of repository 'io_bazel_rules_closure':\r\n   Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 72\r\n\t\t_get_auth(ctx, all_urls)\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 52, in _get_auth\r\n\t\tread_netrc(ctx, netrcfile)\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/utils.bzl\", line 219, in read_netrc\r\n\t\tctx.read(filename)\r\njava.io.FileNotFoundException: /Users/feranick/.netrc (No such file or directory)\r\nERROR: no such package '@io_bazel_rules_closure//closure': Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 72\r\n\t\t_get_auth(ctx, all_urls)\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 52, in _get_auth\r\n\t\tread_netrc(ctx, netrcfile)\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/utils.bzl\", line 219, in read_netrc\r\n\t\tctx.read(filename)\r\njava.io.FileNotFoundException: /Users/feranick/.netrc (No such file or directory)\r\nERROR: no such package '@io_bazel_rules_closure//closure': Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 72\r\n\t\t_get_auth(ctx, all_urls)\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/http.bzl\", line 52, in _get_auth\r\n\t\tread_netrc(ctx, netrcfile)\r\n\tFile \"/private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/bazel_tools/tools/build_defs/repo/utils.bzl\", line 219, in read_netrc\r\n\t\tctx.read(filename)\r\njava.io.FileNotFoundException: /Users/feranick/.netrc (No such file or directory)\r\nINFO: Elapsed time: 2.296s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)", "comments": ["@feranick \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "`wget https://github.com/bazelbuild/bazel/releases/download/2.0.0/bazel-2.0.0-installer-darwin-x86_64.sh`\r\n`chmod +x bazel-2.0.0-installer-darwin-x86_64.sh`\r\n`sudo ./bazel-2.0.0-installer-darwin-x86_64.sh`\r\n`git clone https://github.com/tensorflow/tensorflow`\r\n`cd tensorflow`\r\n`git checkout v2.2.0-rc1`\r\n`./configure`\r\n(default is selected for all options)\r\n`bazel build --config=opt --config=v2 --verbose_failures //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n", "Compilation still not possible with TF 2.2.0-rc2. Is there any plan to fix this for 2.2?", "Compilation is also not possible as the latest master in git.", "What if you just create an empty /Users/feranick/.netrc file?", "Thanks for the suggestion. Adding an empty .netrc seems to be working as the build is compiling fine so far. Will report once it is successfully completed.", "Compilation is successful, all seems to work. This can be closed. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37703\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37703\">No</a>\n"]}, {"number": 37702, "title": "Building TF 2.2.0-rc0 with Bazel 1.2.1 fails ", "body": "Summary: According to the release notes, TF 2.2.0-rc0 can be compiled with bazel 1.2.1 (the MIN version). However, when attempting this, it fails. Compilation is successful with Bazel 2.0.0. Compiling with 2.0.1 fails as well (Max seems to be set at 2.0.0)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, MacOS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 2.2rc0\r\n- **Python version**: 3.6-3.7\r\n- **Bazel version (if compiling from source)**: 1.2.1\r\n- **Exact command to reproduce**:\r\n1. Install bazel 1.2.1\r\n2. GitClone TF and checkout 2.2.0-rc0\r\n3. compile as indicated by the manual. compilation will fail immediately with bazel 1.2.1 (according to the release notes for TF 2.2.0-rc0, that is the minimum version required) but compiles successfully with bazel 2.0.0\r\n", "comments": ["This is the log when running `./configure` with bazel 1.2.1:\r\n\r\n\r\nExtracting Bazel installation...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'version' from /home/nicola/Software/tensorflow-dir/cpu/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nERROR: Unrecognized option: --experimental_repo_remote_exec\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1551, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1368, in main\r\n    _TF_MAX_BAZEL_VERSION)\r\n  File \"./configure.py\", line 483, in check_bazel_version\r\n    ['bazel', '--batch', '--bazelrc=/dev/null', 'version'])\r\n  File \"./configure.py\", line 159, in run_shell\r\n    output = subprocess.check_output(cmd, stderr=stderr)\r\n  File \"/usr/lib/python2.7/subprocess.py\", line 223, in check_output\r\n    raise CalledProcessError(retcode, cmd, output=output)\r\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 2\r\n", "This is the log when running ./configure with bazel 2.0.1:\r\n\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1551, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1368, in main\r\n    _TF_MAX_BAZEL_VERSION)\r\n  File \"./configure.py\", line 483, in check_bazel_version\r\n    ['bazel', '--batch', '--bazelrc=/dev/null', 'version'])\r\n  File \"./configure.py\", line 159, in run_shell\r\n    output = subprocess.check_output(cmd, stderr=stderr)\r\n  File \"/usr/lib/python2.7/subprocess.py\", line 223, in check_output\r\n    raise CalledProcessError(retcode, cmd, output=output)\r\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1", "[Release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc0) mention `Increasing the minimum bazel version to build TF to 1.2.1 to use Bazel's cc_experimental_shared_library.`\r\n\r\nHowever, [configure.py](https://github.com/tensorflow/tensorflow/blob/cf46f78c72bbcb86aff64d06e6fbbb54f2f31d8a/configure.py#L52) states both minimum and maximum bazel version as 2.0.0. Attached screenshot for reference.\r\n\r\n![Screenshot 2020-03-19 at 2 25 40 PM](https://user-images.githubusercontent.com/57165142/77049172-9c612800-69ed-11ea-8598-692db0806a8b.png)\r\n", "BTW, shouldn't the Max version of bazel be 2.0.1? It seems like a bug-fix-release of v.2.0.0....", "We are building with 2.0.0 and this error slipped our minds.\r\n\r\n@bmzhao can you look and confirm what are the minimum and maximum bazel versions?", "Yes, the current minimum and maximum bazel versions are **now Bazel 2.0.0**; unfortunately I forgot to add the that as a \"Release Note\" in commit [daec6e0af0dfe39aada697275a76570da7f04608](https://cs.opensource.google/tensorflow/tensorflow/+/daec6e0af0dfe39aada697275a76570da7f04608)\r\n\r\nThe rationale for the upgrade is related to changes to the implementation of cc_shared_library in bazel that only occurred in bazel 2.0 (and weren't backported to bazel 1.2.1).\r\n\r\nBazel 2.0.0 is also required now for RBE builds, as one of the experimental remote exec options is now set starting from commit [09fe958feebec0405ccac225c94fc130304fc2f4](https://cs.opensource.google/tensorflow/tensorflow/+/09fe958feebec0405ccac225c94fc130304fc2f4).", "Version 2.2.0-rc1 still has the wrong minimum version listed in the Release notes.", "That is because we did the release before the fix. We will have fixed release notes for the next rc/final release", "TF 2.2.0-rc2 has been released and release notes now correctly mention that Bazel needs to be 2.0.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37702\">No</a>\n"]}, {"number": 37701, "title": "Backport #31378 to 1.15", "body": "Since a `pipenv check` when using TF 1.15.2 now yields:\r\n```\r\n37524: tensorflow <2.0 resolved (1.15.2 installed)!\r\nTensorflow 2.0 fixes a potential security vulnerability where decoding variant tensors from proto could result in heap out of bounds memory access.\r\n```\r\nCan a 1.15.3 be made with #31378 incorporated?", "comments": ["The patch is already included in `r1.15` and later \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bab74a15d9ad6bb9066b3e31d601d6a45b1cb221/tensorflow/core/framework/tensor.cc#L518-L522", "All of these tags have the patch from the original commit: v2.2.0-rc1 v2.2.0-rc0 v2.1.0 v2.1.0-rc2 v2.1.0-rc1 v2.1.0-rc0 v1.15.2 v1.15.0 v1.15.0-rc3 v1.15.0-rc2 v1.15.0-rc1 v1.15.0-rc0 \r\n\r\nThe #31378 PR was an attempt at fixing 1.14 with a patch release. Unfortunately, 1.14 was released in a bad state and we cannot patch it. Anyway, this is not an issue since 1.14 is outside the support window anyway", "Closing this issue since it's answered. Feel free to reopen if necessary. Thanks!"]}, {"number": 37700, "title": "Pre trained Resnet model with batch size 1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Atomic Pi\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.5\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 2GB RAM, 16GB eMMC\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nII want to use pretrained ResNet model for image classification. The model I used  requires batch size of 64. Using a batch size of 64 has made Atomic Pi go out of memory.\r\n\r\nThe model used is from this git repoistory:\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n Error Log:\r\nhttps://github.com/tensorflow/models/tree/master/official/r1/resnet\r\nKilled\r\nmakefile:2: recipe for target 'default' failed\r\nmake: *** [default] Error 137\r\n", "comments": ["@raina0990 \r\nIs there any specific reason to be using tensorflow 1.15, could you please upgrade tensorflow and use with later versions, please let us know if that helps resolve your issue.\r\n\r\n[please try the following commands and let us know if it works.\r\npython3 -m pip install --upgrade pip or pip install --upgrade pip]", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37700\">No</a>\n"]}, {"number": 37699, "title": "NFC - minor spelling tweaks under lite and lite/examples directories", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite` and `tensorflow/lite/examples` directories.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac"]}, {"number": 37698, "title": "Added custom Huber loss", "body": "The Huber loss function describes the penalty incurred by an estimation procedure and the loss function is piecewise. This function is quadratic for small values of a, and linear for large values, with equal values and slopes of the different sections at the two points where the error is compared with the threshold.", "comments": ["There is a Huber loss in keras losses already. If this is a different loss function may be it should be in the add ons repo?", "Okay @pavithrasv, I will add any other custom losses to the addons repository."]}, {"number": 37697, "title": "NFC - minor spelling tweaks under lite/tools directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/tools` directory.\r\nfollow-on of #35286", "comments": []}, {"number": 37696, "title": "NFC - minor spelling tweaks under lite/toco directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/toco` directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac"]}, {"number": 37695, "title": "2.1.0 build error due to pybind11", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.7.1908\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?:  \r\n- Bazel version (if compiling from source): 0.27.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: none (CPU only build)\r\n- GPU model and memory:\r\n\r\nBuilding tensorflow 2.1.0 results in compile errors due to pybind11.  This is a build for CPU-only use without any dependency on CUDA libraries.  Example error (out of ~630):\r\n```\r\nbazel-out/host/bin/external/local_config_python/python_include/pybind11/detail/common.h:302:12: error: multiple definition of 'enum class pybind11::return_value_policy'\r\n enum class return_value_policy : uint8_t {\r\n            ^~~~~~~~~~~~~~~~~~~\r\nIn file included from external/pybind11/include/pybind11/pytypes.h:12:0,\r\n                 from external/pybind11/include/pybind11/cast.h:13,\r\n                 from external/pybind11/include/pybind11/attr.h:13,\r\n                 from external/pybind11/include/pybind11/pybind11.h:49,\r\n                 from tensorflow/python/framework/op_def_registry.cc:16:\r\nexternal/pybind11/include/pybind11/detail/common.h:302:12: note: previous definition here\r\n enum class return_value_policy : uint8_t {\r\n            ^~~~~~~~~~~~~~~~~~~\r\n```\r\nBuild steps:\r\n\r\n```\r\npython3  -m venv py37-env\r\nsource py37-env/bin/activate\r\npip install -I --no-cache-dir \"numpy==1.17.2\"\r\npip install -I --no-cache-dir \"pybind11==2.3.0\"\r\npip install -I --no-cache-dir six wheel setuptools mock 'future>=0.17.1'\r\npip install -I --no-cache-dir keras_applications --no-deps\r\npip install -I --no-cache-dir keras_preprocessing --no-deps\r\n\r\n# Run to try and keep bazel from using the wrong python\r\nexport PYTHON_BIN_PATH=`which python3`\r\nexport PYTHON_LIB_PATH=`python -c \"import sys ; print(sys.path[-1])\"`\r\nexport PYTHONPATH=$PYTHON_LIB_PATH\r\nexport TF_NEED_OPENCL_SYCL=0 \r\nexport TF_NEED_COMPUTECPP=0 \r\nexport TF_NEED_OPENCL=0 \r\nexport TF_CUDA_CLANG=0\r\nexport TF_DOWNLOAD_CLANG=0\r\nexport CC_OPT_FLAGS=\"-march=sandybridge -mtune=intel  -Wno-sign-compare\"\r\nexport TF_NEED_CUDA=0\r\nexport TF_NEED_TENSORRT=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_ENABLE_XLA=0\r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\r\n\r\n./configure\r\n\r\ntime bazel build  --verbose_failures  \\\r\n  --config=opt \\\r\n  --config=mkl \\\r\n  --config=numa \\\r\n  --config=v2 \\\r\n  --linkopt=\"-Wl,-rpath=/share/pkg.7/gcc/7.4.0/install/lib64\" \\\r\n  --linkopt=\"-L/share/pkg.7/gcc/7.4.0/install/lib64\"   \\\r\n  --host_linkopt=\"-Wl,-rpath=/share/pkg.7/gcc/7.4.0/install/lib64\" \\\r\n  --host_linkopt=\"-L/share/pkg.7/gcc/7.4.0/install/lib64\"  \\\r\n  //tensorflow/tools/pip_package:build_pip_package   \r\n```\r\n\r\n\r\n[bazel37.zip](https://github.com/tensorflow/tensorflow/files/4350233/bazel37.zip)\r\n", "comments": ["Since you are building from source, can you try building from master?", "> \r\n> \r\n> Since you are building from source, can you try building from master?\r\n\r\nI did a git checkout from master, switched to Bazel 2.0.0, and used the same build process.  It built without any problems.\r\n\r\n`Target //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 1545.899s, Critical Path: 214.52s\r\nINFO: 17016 processes: 17016 local.\r\nINFO: Build completed successfully, 18417 total actions\r\nINFO: Build completed successfully, 18417 total actions\r\n`", "Interesting. Thank you for the check.\r\n\r\nDo you want to build on 2.1 or master/2.2 is ok?", "The build from master also succeeded for Python 3.6.9.  I'd prefer to have an identical CPU-only v2.1.0 to match the tensorflow 2.1.0 install via pip for GPU usage.  However I can live with this for now so this is low priority for me.  On our cluster all the \"real\" tensorflow work is done on GPUs but we also use the CPU-only build for development, testing, and some classroom usage.    ", "If you want a CPU-only tensorflow you can also try using the `tensorflow-cpu` pip package\r\n\r\nRegarding the pybind error, we had some segfaults due to ODR violations at the time, so this could be related. We fixed them in master (and 2.2)", "Use tensorflow version 2.0 only puthon3.7x", "@jameszow2 please don't provide bad advice. TF 2.0 is not a long term release and will soon no longer receive any updates/support. Plus, the issue here is in C++, not in Python.", "@mihaimaruseac - oops, just trying to clarify my statements.  The tensorflow-cpu pip pakage for 2.1.0 works great, but the issue for us is that it uses AVX instructions and our cluster has a decent amount of Nehalem architecture CPUs that don't support AVX.  The CPU-only source code build is to support up to SSE4.2 so that tensorflow 2.1.0 code will run on the Nehalem CPUs.   Jobs that run on newer CPUs will load the code from the tensorflow-cpu pip package, and if there's a GPU installed it'll load the regular tensorflow pip package.  ", "That makes sense now. Thanks for explaining. I can dedicate some time to investigate the build failure in this case, but won't be able to do this until mid of April unfortunately :(", "Unfortunately we cannot patch old branches except for security reasons. Can you try with newer versions of TF?", "@bu-bgregor \r\nAs we have the latest stable version 2.6.0, Can you try building TF 2.6.0 or any later 2.x version and let us know if the issue still persists?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37695\">No</a>\n"]}, {"number": 37694, "title": "NFC - minor spelling tweaks under lite/kernels directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/kernels` directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac"]}, {"number": 37693, "title": "CUDA_ERROR_LAUNCH_TIMEOUT when using NCCL with MultiWorkerMirroredStrategy on multi-node systems", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Based on https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras but using bigger dataset and Resnet\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): RHEL 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: CUDA/10.1.243, cuDNN/7.6.4.38, NCCL/2.4.8\r\n- GPU model and memory:Tesla K80\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the standard training loop results in a CUDA_ERROR_LAUNCH_TIMEOUT when using NCCL.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom slurm_utils import set_tf_config\r\nfrom slurm_cluster_resolver import SlurmClusterResolver\r\n\r\ntfds.disable_progress_bar()\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 32\r\n\r\n\r\ndef make_datasets_unbatched():\r\n    def preprocess(image, label):\r\n        image = tf.image.convert_image_dtype(image, tf.float32)\r\n        image = tf.image.resize_with_pad(image, 299, 299)\r\n        image = image - [123.68, 116.78, 103.94]\r\n        return image, label\r\n\r\n    datasets, info = tfds.load(name='stanford_dogs',\r\n                               with_info=True,\r\n                               as_supervised=True)\r\n\r\n    return {\r\n        'train':\r\n        datasets['train'].map(preprocess).cache().shuffle(BUFFER_SIZE),\r\n        'test': datasets['test'].map(preprocess)\r\n    }, info\r\n\r\n\r\ndef build_and_compile_cnn_model(num_classes):\r\n    model = tf.keras.applications.InceptionV3(weights=None,\r\n                                              classes=num_classes)\r\n    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\n\r\nresolver = SlurmClusterResolver()\r\nset_tf_config(resolver)\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n    tf.distribute.experimental.CollectiveCommunication.NCCL,\r\n    cluster_resolver=resolver)\r\n\r\n# Here the batch size scales up by number of workers since\r\n# `tf.data.Dataset.batch` expects the global batch size.\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\r\nwith strategy.scope():\r\n    # Creation of dataset, and model building/compiling need to be within\r\n    # `strategy.scope()`.\r\n    options = tf.data.Options()\r\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n    datasets, info = make_datasets_unbatched()\r\n    for split in datasets:\r\n        datasets[split] = datasets[split].batch(\r\n            GLOBAL_BATCH_SIZE).with_options(options)\r\n    model = build_and_compile_cnn_model(info.features['label'].num_classes)\r\n\r\ntrain_steps = info.splits['train'].num_examples // GLOBAL_BATCH_SIZE\r\ntest_steps = info.splits['test'].num_examples // GLOBAL_BATCH_SIZE\r\nprint(\"Have %s examples -> %s batches of %s\" %\r\n      (info.splits['train'].num_examples, train_steps, GLOBAL_BATCH_SIZE))\r\n\r\nmodel.fit(x=datasets['train'].repeat(),\r\n          epochs=1,\r\n          steps_per_epoch=train_steps,\r\n          verbose=2)\r\n```\r\n\r\nNote: `SlurmClusterResolver` is the one that got recently included to TF and `set_tf_config` is:\r\n\r\n```\r\ndef set_tf_config(cluster_resolver, environment=None):\r\n    \"\"\"Set the TF_CONFIG env variable from the given cluster resolver\"\"\"\r\n    cfg = {\r\n        'cluster': cluster_resolver.cluster_spec().as_dict(),\r\n        'task': {\r\n            'type': cluster_resolver.get_task_info()[0],\r\n            'index': cluster_resolver.get_task_info()[1],\r\n        },\r\n        'rpc_layer': cluster_resolver.rpc_layer,\r\n    }\r\n    if environment:\r\n        cfg['environment'] = environment\r\n    os.environ['TF_CONFIG'] = json.dumps(cfg)\r\n    print(\"Set TF_CONFIG=\" + os.environ['TF_CONFIG'])\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis happens everytime when using more than 1 node.\r\n\r\n```\r\nSet TF_CONFIG={\"cluster\": {\"worker\": [\"taurusi2107:8888\", \"taurusi2108:8888\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}, \"rpc_layer\": \"grpc\"}\r\nHave 12000 examples -> 46 batches of 256\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nTrain for 46 steps\r\nHave 12000 examples -> 46 batches of 256\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nTrain for 46 steps\r\n2020-03-18 16:51:46.733598: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:46.733598: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:46.733599: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:46.733602: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:46.733657: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n2020-03-18 16:51:46.733656: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n2020-03-18 16:51:46.733662: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n2020-03-18 16:51:46.733671: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n[taurusi2108:15936] *** Process received signal ***\r\n2020-03-18 16:51:47.705311: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:47.705311: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:47.705349: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n2020-03-18 16:51:47.705311: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:47.705359: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n[taurusi2107:31475] *** Process received signal ***\r\n2020-03-18 16:51:47.705372: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n2020-03-18 16:51:47.705326: E tensorflow/stream_executor/cuda/cuda_driver.cc:818] failed to free device memory at 0x1d8c005800; result: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n[taurusi2107:31475] Signal: Aborted (6)\r\n[taurusi2107:31475] Signal code:  (-6)\r\n2020-03-18 16:51:47.705358: E tensorflow/stream_executor/stream.cc:332] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n2020-03-18 16:51:47.705412: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-03-18 16:51:47.705421: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\nsrun: error: taurusi2107: task 0: Aborted (core dumped)\r\nsrun: error: taurusi2108: task 1: Aborted (core dumped)\r\n\r\n```\r\n", "comments": ["CUDA launch timeout usually means the kernel is taking too long to run on GPU which is also used for an active display. GPU driver could kick out the kernel to update the display. Perhaps try updating your GPU driver and cuda version? Reducing the batch size or using a dedicated GPU card may also help.", "I tried this on 2 different kinds of nodes. In both cases I started the script via `srun` (SLURM).\r\n\r\nGeForce GTX 1080 Ti:\r\n- Runs fine with AUTO for communication\r\n- Hangs indefinitely when switching to NCCL\r\n- nvidia-smi shows no running processes\r\n- Driver Version: 418.87.01    CUDA Version: 10.1\r\n\r\nTesla K80:\r\n- Runs fine with AUTO\r\n- Shows LAUNCH_TIMEOUT when switching to NCCL\r\n- nvidia-smi shows `/usr/bin/X` and `/usr/bin/gnome-shell` with a single-digit MB usage\r\n- Driver  Version: 418.87.01    CUDA Version: 10.1\r\n\r\nBoth are only dedicated GPUs, no Display attached.\r\n\r\nReducing the batch size down to 8/worker leads to `Caught signal 11 (Segmentation fault: address not mapped to object at address 0x80)` in `tensorflow::NcclReducer::Run()`.    \r\nReducing to only 16/worker didn't change anything.", "Hi, can you clarify whether you can run nccl tests (no TF dependency) across nodes?  Can you also run with `NCCL_DEBUG` environment variable set to `INFO` and paste the logs?  Thanks.", "> can you clarify whether you can run nccl tests (no TF dependency) across nodes\r\n\r\nI used the code from https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/examples.html \"Example 2: One Device per Process or Thread\" and sucessfully run it. Compiled with `nvcc example2.c -lnccl -lmpi`. I also used the \"Example 3\" with 3 devices/process (changed the nDev line). Both show Success.\r\n\r\nI run TF with `NCCL_DEBUG=INFO`. For the GeForce GTX 1080 Ti (hang) the output is:\r\n```\r\ntaurusa3:27633:27801 [0] NCCL INFO Bootstrap : Using [0]ib0:10.1.148.3<0> [1]ib1:10.2.148.3<0>\r\ntaurusa3:27633:27801 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\ntaurusa3:27633:27801 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_0:1/IB ; OOB ib0:10.1.148.3<0>\r\nNCCL version 2.4.8+cuda10.1\r\ntaurusa5:18071:18468 [0] NCCL INFO Bootstrap : Using [0]ib0:10.1.148.5<0> [1]ib1:10.2.148.5<0>\r\ntaurusa5:18071:18468 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\ntaurusa5:18071:18468 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_0:1/IB ; OOB ib0:10.1.148.5<0>\r\ntaurusa3:27633:28209 [0] NCCL INFO Setting affinity for GPU 0 to 3f\r\ntaurusa5:18071:19032 [0] NCCL INFO Setting affinity for GPU 0 to 3f\r\ntaurusa3:27633:28210 [1] NCCL INFO Setting affinity for GPU 1 to 0fc0\r\ntaurusa5:18071:19033 [1] NCCL INFO Setting affinity for GPU 1 to 0fc0\r\ntaurusa3:27633:28212 [2] NCCL INFO Setting affinity for GPU 2 to 0fc0\r\ntaurusa5:18071:19034 [2] NCCL INFO Setting affinity for GPU 2 to 0fc0\r\ntaurusa5:18071:19032 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  PHB PHB\r\ntaurusa5:18071:19034 [2] NCCL INFO CUDA Dev 2[2], IB NIC distance :  SYS SYS\r\ntaurusa5:18071:19033 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  SYS SYS\r\ntaurusa3:27633:28209 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  PHB PHB\r\ntaurusa3:27633:28212 [2] NCCL INFO CUDA Dev 2[2], IB NIC distance :  SYS SYS\r\ntaurusa3:27633:28210 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  SYS SYS\r\ntaurusa3:27633:28209 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5\r\ntaurusa3:27633:28209 [0] NCCL INFO Channel 01 :    0   1   2   3   4   5\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 00 : 5 -> 0 [receive] via NET/IB/0\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 00 : 2 -> 3 [receive] via NET/IB/0\r\ntaurusa5:18071:19034 [2] NCCL INFO Ring 00 : 5 -> 0 [send] via NET/IB/0\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via direct shared memory\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 00 : 3[0] -> 4[1] via direct shared memory\r\ntaurusa3:27633:28212 [2] NCCL INFO Ring 00 : 2 -> 3 [send] via NET/IB/0\r\ntaurusa5:18071:19033 [1] NCCL INFO Ring 00 : 4[1] -> 5[2] via P2P/direct pointer\r\ntaurusa3:27633:28210 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via P2P/direct pointer\r\ntaurusa5:18071:19034 [2] NCCL INFO Ring 00 : 5[2] -> 4[1] via P2P/direct pointer\r\ntaurusa5:18071:19033 [1] NCCL INFO Ring 00 : 4[1] -> 3[0] via direct shared memory\r\ntaurusa3:27633:28212 [2] NCCL INFO Ring 00 : 2[2] -> 1[1] via P2P/direct pointer\r\ntaurusa3:27633:28210 [1] NCCL INFO Ring 00 : 1[1] -> 0[0] via direct shared memory\r\ntaurusa5:18071:19034 [2] NCCL INFO Ring 01 : 5 -> 0 [send] via NET/IB/1\r\ntaurusa3:27633:28212 [2] NCCL INFO Ring 01 : 2 -> 3 [send] via NET/IB/1\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 00 : 3 -> 0 [receive] via NET/IB/0\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 00 : 3 -> 0 [send] via NET/IB/0\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 00 : 0 -> 3 [receive] via NET/IB/0\r\ntaurusa3:27633:28210 [1] NCCL INFO Ring 01 : 1[1] -> 2[2] via P2P/direct pointer\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 00 : 0 -> 3 [send] via NET/IB/0\r\ntaurusa5:18071:19033 [1] NCCL INFO Ring 01 : 4[1] -> 5[2] via P2P/direct pointer\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 01 : 5 -> 0 [receive] via NET/IB/1\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 01 : 0[0] -> 1[1] via direct shared memory\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 01 : 2 -> 3 [receive] via NET/IB/1\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 01 : 3[0] -> 4[1] via direct shared memory\r\ntaurusa5:18071:19034 [2] NCCL INFO Ring 01 : 5[2] -> 4[1] via P2P/direct pointer\r\ntaurusa5:18071:19034 [2] NCCL INFO Trees [0] 4->5->-1/-1/-1 [1] 4->5->-1/-1/-1\r\ntaurusa3:27633:28212 [2] NCCL INFO Ring 01 : 2[2] -> 1[1] via P2P/direct pointer\r\ntaurusa5:18071:19033 [1] NCCL INFO Ring 01 : 4[1] -> 3[0] via direct shared memory\r\ntaurusa3:27633:28212 [2] NCCL INFO Trees [0] 1->2->-1/-1/-1 [1] 1->2->-1/-1/-1\r\ntaurusa3:27633:28210 [1] NCCL INFO Ring 01 : 1[1] -> 0[0] via direct shared memory\r\ntaurusa5:18071:19033 [1] NCCL INFO Trees [0] 3->4->5/-1/-1 [1] 3->4->5/-1/-1\r\ntaurusa3:27633:28210 [1] NCCL INFO Trees [0] 0->1->2/-1/-1 [1] 0->1->2/-1/-1\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 01 : 0 -> 3 [receive] via NET/IB/1\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 01 : 0 -> 3 [send] via NET/IB/1\r\ntaurusa3:27633:28209 [0] NCCL INFO Ring 01 : 3 -> 0 [receive] via NET/IB/1\r\ntaurusa5:18071:19032 [0] NCCL INFO Ring 01 : 3 -> 0 [send] via NET/IB/1\r\ntaurusa5:18071:19032 [0] NCCL INFO Trees [0] 0->3->4/-1/-1 [1] -1->3->4/0/-1\r\ntaurusa5:18071:19032 [0] NCCL INFO comm 0x2b81240174b0 rank 3 nranks 6 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\ntaurusa5:18071:19034 [2] NCCL INFO comm 0x2b811c001430 rank 5 nranks 6 cudaDev 2 nvmlDev 2 - Init COMPLETE\r\ntaurusa5:18071:19033 [1] NCCL INFO comm 0x2b8120001190 rank 4 nranks 6 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\ntaurusa3:27633:28209 [0] NCCL INFO Trees [0] -1->0->1/3/-1 [1] 3->0->1/-1/-1\r\ntaurusa3:27633:28209 [0] NCCL INFO Using 256 threads, Min Comp Cap 6, Trees enabled up to size 69999\r\ntaurusa3:27633:28210 [1] NCCL INFO comm 0x2ab7d024cf60 rank 1 nranks 6 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\ntaurusa3:27633:28212 [2] NCCL INFO comm 0x2ab7cc050e50 rank 2 nranks 6 cudaDev 2 nvmlDev 2 - Init COMPLETE\r\ntaurusa3:27633:28209 [0] NCCL INFO comm 0x2ab7d405e230 rank 0 nranks 6 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\ntaurusa3:27633:28021 [0] NCCL INFO Launch mode Group/CGMD\r\n```\r\n\r\nAnd for the K80 (crash):\r\n\r\n```\r\ntaurusi2107:12777:12926 [0] NCCL INFO Bootstrap : Using [0]ib0:10.1.145.121<0> [1]ib0:0:10.2.145.121<0>\r\ntaurusi2107:12777:12926 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\ntaurusi2107:12777:12926 [0] NCCL INFO NET/IB : Using [0]mlx4_0:1/IB ; OOB ib0:10.1.145.121<0>\r\nNCCL version 2.4.8+cuda10.1\r\ntaurusi2108:29975:30611 [0] NCCL INFO Bootstrap : Using [0]ib0:10.1.145.122<0> [1]ib0:0:10.2.145.122<0>\r\ntaurusi2108:29975:30611 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\ntaurusi2108:29975:30611 [0] NCCL INFO NET/IB : Using [0]mlx4_0:1/IB ; OOB ib0:10.1.145.122<0>\r\ntaurusi2107:12777:13231 [0] NCCL INFO Setting affinity for GPU 0 to 0fff\r\ntaurusi2108:29975:30730 [0] NCCL INFO Setting affinity for GPU 0 to 0fff\r\ntaurusi2108:29975:30731 [1] NCCL INFO Setting affinity for GPU 1 to 0fff\r\ntaurusi2107:12777:13245 [1] NCCL INFO Setting affinity for GPU 1 to 0fff\r\ntaurusi2108:29975:30733 [3] NCCL INFO CUDA Dev 3[3], IB NIC distance :  PHB\r\ntaurusi2108:29975:30730 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  SYS\r\ntaurusi2108:29975:30732 [2] NCCL INFO CUDA Dev 2[2], IB NIC distance :  PHB\r\ntaurusi2108:29975:30731 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  SYS\r\ntaurusi2107:12777:13231 [0] NCCL INFO CUDA Dev 0[0], IB NIC distance :  SYS\r\ntaurusi2107:12777:13246 [2] NCCL INFO CUDA Dev 2[2], IB NIC distance :  PHB\r\ntaurusi2107:12777:13245 [1] NCCL INFO CUDA Dev 1[1], IB NIC distance :  SYS\r\ntaurusi2107:12777:13247 [3] NCCL INFO CUDA Dev 3[3], IB NIC distance :  PHB\r\ntaurusi2107:12777:13231 [0] NCCL INFO Channel 00 :    0   1   6   7   4   5   2   3\r\ntaurusi2107:12777:13231 [0] NCCL INFO Channel 01 :    0   1   6   7   4   5   2   3\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 00 : 5 -> 2 [receive] via NET/IB/0\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/direct pointer\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 00 : 1 -> 6 [receive] via NET/IB/0\r\ntaurusi2108:29975:30733 [3] NCCL INFO Ring 00 : 7[3] -> 4[0] via direct shared memory\r\ntaurusi2107:12777:13231 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/direct pointer\r\ntaurusi2107:12777:13245 [1] NCCL INFO Ring 00 : 1 -> 6 [send] via NET/IB/0\r\ntaurusi2107:12777:13247 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via direct shared memory\r\ntaurusi2108:29975:30731 [1] NCCL INFO Ring 00 : 5 -> 2 [send] via NET/IB/0\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 00 : 6[2] -> 7[3] via P2P/direct pointer\r\ntaurusi2108:29975:30730 [0] NCCL INFO Ring 00 : 4[0] -> 5[1] via P2P/direct pointer\r\ntaurusi2107:12777:13245 [1] NCCL INFO Ring 00 : 1[1] -> 0[0] via P2P/direct pointer\r\ntaurusi2107:12777:13231 [0] NCCL INFO Ring 00 : 0[0] -> 3[3] via direct shared memory\r\ntaurusi2108:29975:30730 [0] NCCL INFO Ring 00 : 4[0] -> 7[3] via direct shared memory\r\ntaurusi2108:29975:30731 [1] NCCL INFO Ring 00 : 5[1] -> 4[0] via P2P/direct pointer\r\ntaurusi2108:29975:30733 [3] NCCL INFO Ring 00 : 7[3] -> 6[2] via P2P/direct pointer\r\ntaurusi2107:12777:13247 [3] NCCL INFO Ring 00 : 3[3] -> 2[2] via P2P/direct pointer\r\ntaurusi2107:12777:13245 [1] NCCL INFO Ring 01 : 1 -> 6 [send] via NET/IB/0\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 00 : 6 -> 2 [send] via NET/IB/0\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 00 : 6 -> 2 [receive] via NET/IB/0\r\ntaurusi2108:29975:30731 [1] NCCL INFO Ring 01 : 5 -> 2 [send] via NET/IB/0\r\ntaurusi2107:12777:13247 [3] NCCL INFO Ring 01 : 3[3] -> 0[0] via direct shared memory\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 00 : 2 -> 6 [receive] via NET/IB/0\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 00 : 2 -> 6 [send] via NET/IB/0\r\ntaurusi2107:12777:13231 [0] NCCL INFO Ring 01 : 0[0] -> 1[1] via P2P/direct pointer\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 01 : 5 -> 2 [receive] via NET/IB/0\r\ntaurusi2107:12777:13231 [0] NCCL INFO Ring 01 : 0[0] -> 3[3] via direct shared memory\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 01 : 2[2] -> 3[3] via P2P/direct pointer\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 01 : 2 -> 6 [send] via NET/IB/0\r\ntaurusi2107:12777:13247 [3] NCCL INFO Ring 01 : 3[3] -> 2[2] via P2P/direct pointer\r\ntaurusi2107:12777:13247 [3] NCCL INFO Trees [0] 2->3->0/-1/-1 [1] 2->3->0/-1/-1\r\ntaurusi2108:29975:30730 [0] NCCL INFO Ring 01 : 4[0] -> 5[1] via P2P/direct pointer\r\ntaurusi2108:29975:30733 [3] NCCL INFO Ring 01 : 7[3] -> 4[0] via direct shared memory\r\ntaurusi2108:29975:30731 [1] NCCL INFO Ring 01 : 5[1] -> 4[0] via P2P/direct pointer\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 01 : 1 -> 6 [receive] via NET/IB/0\r\ntaurusi2107:12777:13245 [1] NCCL INFO Ring 01 : 1[1] -> 0[0] via P2P/direct pointer\r\ntaurusi2107:12777:13245 [1] NCCL INFO Trees [0] 0->1->-1/-1/-1 [1] 0->1->-1/-1/-1\r\ntaurusi2107:12777:13231 [0] NCCL INFO Trees [0] 3->0->1/-1/-1 [1] 3->0->1/-1/-1\r\ntaurusi2107:12777:13231 [0] NCCL INFO Using 128 threads, Min Comp Cap 3, Trees enabled up to size 149999\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 01 : 6[2] -> 7[3] via P2P/direct pointer\r\ntaurusi2108:29975:30731 [1] NCCL INFO Trees [0] 4->5->-1/-1/-1 [1] 4->5->-1/-1/-1\r\ntaurusi2107:12777:13231 [0] NCCL INFO comm 0x2b6284007b10 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\ntaurusi2107:12777:13247 [3] NCCL INFO comm 0x2b6240006de0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE\r\ntaurusi2108:29975:30730 [0] NCCL INFO Ring 01 : 4[0] -> 7[3] via direct shared memory\r\ntaurusi2107:12777:13245 [1] NCCL INFO comm 0x2b6248006de0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\ntaurusi2108:29975:30730 [0] NCCL INFO Trees [0] 7->4->5/-1/-1 [1] 7->4->5/-1/-1\r\ntaurusi2108:29975:30733 [3] NCCL INFO Ring 01 : 7[3] -> 6[2] via P2P/direct pointer\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 01 : 2 -> 6 [receive] via NET/IB/0\r\ntaurusi2108:29975:30733 [3] NCCL INFO Trees [0] 6->7->4/-1/-1 [1] 6->7->4/-1/-1\r\ntaurusi2108:29975:30730 [0] NCCL INFO comm 0x2b6f50007ed0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\ntaurusi2108:29975:30731 [1] NCCL INFO comm 0x2b6f4c0071d0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\ntaurusi2107:12777:13246 [2] NCCL INFO Ring 01 : 6 -> 2 [receive] via NET/IB/0\r\ntaurusi2108:29975:30733 [3] NCCL INFO comm 0x2b6f44007190 rank 7 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE\r\ntaurusi2108:29975:30732 [2] NCCL INFO Ring 01 : 6 -> 2 [send] via NET/IB/0\r\ntaurusi2108:29975:30732 [2] NCCL INFO Trees [0] 2->6->7/-1/-1 [1] -1->6->7/2/-1\r\ntaurusi2107:12777:13246 [2] NCCL INFO Trees [0] -1->2->3/6/-1 [1] 6->2->3/-1/-1\r\ntaurusi2108:29975:30732 [2] NCCL INFO comm 0x2b6f480071d0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE\r\ntaurusi2107:12777:13246 [2] NCCL INFO comm 0x2b6244006de0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE\r\ntaurusi2107:12777:13199 [0] NCCL INFO Launch mode Group\r\n```\r\n\r\nAnything else I can do to help?", "Thanks, looking at the logs above it seems like you are running on 2 hosts each with 1 GPU, right?\r\n\r\nSince you are able to run the NCCL tests, this could be a bug in TF.  Can I bother you to run with some more logging enabled?  `TF_CPP_VMODULE=collective_ops=2,nccl_manager=2` should help.", "No, the 1080s are 3/host and the K80 are 4/host (well 2/host but each has 2 GPUs), see `cudaDev 0 - cudaDev 3` in the output above.\r\n\r\nDone on the 1080s currently can't get hold of the K80s but can report once done: \r\n[tfnccl.log](https://github.com/tensorflow/tensorflow/files/4403082/tfnccl.log)\r\n", "And here is the log for the K80s with the crash/timeout: \r\n[tfncclK80.log](https://github.com/tensorflow/tensorflow/files/4407360/tfncclK80.log) Looks like the above so I guess the 1080s simply don't throw a timeout error.\r\n", "@Flamefire It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!", "Seems to be fixed in at least 2.4.1\r\nHowever using NCCL for multi-node is (still?) broken in 2.5.0: https://github.com/tensorflow/tensorflow/issues/50926\r\n\r\nAnyway closing this", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37693\">No</a>\n"]}, {"number": 37691, "title": "Enable 3D tensor support for softmax quantized in TFLu", "body": "", "comments": ["@giorgio-arenarm Could you please resolve the conflicts? Thanks!", "Done", "Merged with commit a4c1bdc9"]}, {"number": 37690, "title": "Can I use TensorFlow Multi-worker training with two machines, which one has GPUs, the other only has CPU?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n- TensorFlow version : 2.1.0\r\n\r\nI have two machines, machine1 has GPUs and machine2 only has CPUs. I want to know if they can use Multi-worker training in Tensorflow, that is, during the distributed training, machine1 uses GPUs and machine2 uses CPUs.\r\n\r\nI followed this tutorial to experiment:\r\nhttps://tensorflow.google.cn/tutorials/distribute/multi_worker_with_keras?hl=en\r\n\r\nBut there are some error:\r\n> tensorflow.python.framework.errors_impl.InternalError: Collective Op CollectiveBcastSend: Broadcast(1) is assigned to device /job:worker/replica:0/task:0/device:GPU:0 with type GPU and group_key 1 but that group has type CPU [Op:CollectiveBcastSend]\r\n\r\nwhen I set machine1(has GPUs) not to use GPUs:\r\n```\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n```\r\nmy code run successfully.\r\n\r\nI want to know: Can I use machine1's GPUs and machine2's CPUs to do distributed learning?\r\n\r\n", "comments": ["As you can see from the error, such heterogeneous distributed training is currently not supported in TensorFlow. I would recommend you to use the machine with GPU using MirroredStrategy (single-machine training), and in fact, the training speed is most likely to surpass the distributed setting as you described because:\r\n1. The GPU machine needs to wait for the non-GPU machine for every training step to aggregate the gradients, so training speed is bounded to the slower machine.\r\n2. The network cost of gradient aggregation is usually non-negligible in distributed training. BTW TensorFlow do offer some optimizations like overlapping gradient aggregation with back propagation, if you are interested, check out [this video](https://www.youtube.com/watch?v=6ovfZW8pepo&list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&index=11&t=0s) from the recent TensorFlow dev submit. "]}, {"number": 37689, "title": "spurious error:  failed call to cuInit: UNKNOWN ERROR", "body": "ubuntu 18.04, CUDA 10.1, TF 2.1, python 3.6.5\r\n\r\non a machine *without* a GPU, the following code emits \"error:  failed call to cuInit: UNKNOWN ERROR\".  of course that should fail!  you shouldn't have even tried.  printing that requires extra logic when parsing log files to check whether jobs succeeded.\r\n\r\n```\r\nPython 3.6.5 (default, Apr  1 2018, 05:46:30) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-03-18 08:16:50.221103: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/.singularity.d/libs\r\n2020-03-18 08:16:50.221260: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/.singularity.d/libs\r\n2020-03-18 08:16:50.221275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n>>> msg = tf.constant('Hello, TensorFlow!')\r\n2020-03-18 08:16:55.455153: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/.singularity.d/libs\r\n2020-03-18 08:16:55.455184: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-03-18 08:16:55.455223: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (h10u02.int.janelia.org): /proc/driver/nvidia/version does not exist\r\n2020-03-18 08:16:55.455545: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-18 08:16:55.468119: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099935000 Hz\r\n2020-03-18 08:16:55.469505: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49b71e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-18 08:16:55.469532: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n```", "comments": ["Having a similar issue when I try running Jupyter notebook on GPU using jupyter/tensorflow-notebook Docker image from Dockerhub. It seems like CUDA fails to load so Jupyter notebook ends up running without GPU acceleration.\r\n\r\n```\r\n[I 18:36:11.117 NotebookApp] Kernel started: ef831313-da54-458c-9ce2-e854e67bec73\r\n2020-03-18 18:36:14.799904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2020-03-18 18:36:14.801346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2020-03-18 18:36:16.140605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-03-18 18:36:16.148127: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2020-03-18 18:36:16.148180: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: jupyter-gpu-9f69d4848-lfx4b\r\n2020-03-18 18:36:16.148193: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: aimp-jupyter-gpu-9f69d4848-lfx4b\r\n2020-03-18 18:36:16.148331: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.40.4\r\n2020-03-18 18:36:16.148378: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\r\n2020-03-18 18:36:16.148390: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.40.4\r\n2020-03-18 18:36:16.148896: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-18 18:36:16.155651: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596985000 Hz\r\n2020-03-18 18:36:16.156455: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556699f800e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-18 18:36:16.156486: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version \r\n```\r\n\r\nHere is the Dockerfile I use:\r\n\r\n```\r\nFROM jupyter/tensorflow-notebook\r\n\r\nUSER root\r\n\r\nRUN apt-get update && apt-get install -y gnupg2 &&\\\r\n\twget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb &&\\\r\n\tapt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub &&\\\r\n\tdpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb &&\\\r\n\trm -f cuda-repo-ubuntu1804_10.1.243-1_amd64.deb &&\\\r\n\tapt-get update &&\\\r\n\twget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb &&\\\r\n\tapt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb &&\\\r\n\trm -f nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb &&\\\r\n\tapt-get update &&\\\r\n\tapt-get install -y --no-install-recommends \\\r\n\t\tcuda-10-1 \\\r\n\t\tlibcudnn7=7.6.4.38-1+cuda10.1  \\\r\n\t\tlibcudnn7-dev=7.6.4.38-1+cuda10.1 &&\\\r\n\tapt-get install -y --no-install-recommends \\\r\n\t\tlibnvinfer6=6.0.1-1+cuda10.1 \\\r\n\t\tlibnvinfer-dev=6.0.1-1+cuda10.1 \\\r\n\t\tlibnvinfer-plugin6=6.0.1-1+cuda10.1 &&\\\r\n\tjupyter serverextension enable --py jupyterlab --sys-prefix &&\\\r\n \tpip install jupyter-tensorboard==0.2.0 certifi==2019.11.28 &&\\\r\n \tjupyter tensorboard enable\r\n```", "@bjarthur \r\n\r\nCould you check this [issue ](https://github.com/tensorflow/tensorflow/issues/19266#issuecomment-399686258 )which is similar to your issue. If those solutions doesn't work, could you uninstall and reinstall CUDA and cuDNN? Please let us know how it progresses. Also, try to uninstall and reinstall tensorflow-gpu and try following the instructions from [TensorFlow website](source). Thanks!", "@stevancvetkovic this is *not* a similar issue to yours!  please don't hijack this issue.  i am trying to run tensorflow on a machine *without a GPU*.  and so no, @ravikyram , that issue you refer me to is also not relevant as that concerns using tensorflow *with a GPU*.\r\n\r\nnot all machine learning problems, even neural network ones, are big enough to need a GPU.  mine is just as fast with CPU alone.  tensorflow 2 now just has one installation procedure irrespective of whether you use a GPU or not (`pip install tensorflow`).  this differs from TF 1 where there were two (`pip install tensorflow-gpu` or `pip install tensorflow`).\r\n\r\nmy complaint here is that TF2 emits an error message when running on a machine *without a GPU* that says it tried to initialize the CUDA libraries.  before doing that it should check to see if there is a GPU in the first place.  throwing errors like that when in fact there is not one and the code continues to run fine makes parsing log files to see that everything is okay more complicated.   seems like an easy fix to me to change TF to first check if a GPU exists before trying to initialize CUDA.  thanks!", "@ravikyram I have the same problem and it is very confusing for the users. Is anyone already working on a fix for this? I can work on it if you could point me in the correct direction as to where to look in the codebase.", "Can we suppress that error when running on CPU. This is pure noise. The app is working as expected without any problem. I can't see any reason why it would deserve an error. A warning maybe, but not an error.\r\n\r\n```\r\n2020-03-18 08:16:55.455184: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n```", "Our applications are very stable, and we setup notifications to our phones whenever there is an error, but this stupid tensorflow error blows up our phones on every deployment. Extremely annoying. Please be a good python citizen and only log errors.. when there is an error! ", "> Can we suppress that error when running on CPU. This is pure noise. The app is working as expected without any problem. I can't see any reason why it would deserve an error. A warning maybe, but not an error.\r\n> \r\n> ```\r\n> 2020-03-18 08:16:55.455184: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n> ```\r\n\r\nI have the same problem with `pytest` with `-s` flag on CPU. This is going to create problem cause it is parsed as an error. /cc @seanpmorgan ", "I've a stub PR at https://github.com/tensorflow/tensorflow/pull/39956", "@bjarthur As a workaround solution TF2 still publishes a `tensorflow-cpu` which should omit this attempt of dynamic kernel loading and is a significantly smaller package:\r\nhttps://pypi.org/project/tensorflow-cpu/#history", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37689\">No</a>\n"]}, {"number": 37688, "title": "Tensorflow lite inference crashes in Android development", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution \uff1aLinux\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version: 1.12\r\n- Target platform: Arm64 with Android kernel 3.18\r\n\r\n**Describe the problem**\r\nThis problems happens when using teilte in Android APP platform. My input is 4D tensors with 3 RGB images. \r\n\r\nI convert the bitmap arrays for the byte buffer arrays.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n```\r\n    private void convertBitmapToByteBuffer(ArrayList<Bitmap> bitmaps) {\r\n        //System.out.println(bitmaps.size());\r\n        for (int i = 0; i < 3; i++) {\r\n            ByteBuffer imgData = ByteBuffer.allocateDirect(1\r\n                    * getImageSizeX()\r\n                    * getImageSizeY()\r\n                    * 3\r\n                    * getNumBytesPerChannel())\r\n                    .order(ByteOrder.nativeOrder());\r\n            bitmaps.get(i).getPixels(intValues, 0, bitmaps.get(i).getWidth(), 0, 0, bitmaps.get(i).getWidth(), bitmaps.get(i).getHeight());\r\n            // Convert the image to floating point.\r\n            //System.out.println(intValues.length);\r\n            int pixel = 0;\r\n            for (int j = 0; j < getImageSizeX(); j++) {\r\n                for (int k = 0; k < getImageSizeY(); k++) {\r\n                    final int val = intValues[pixel];\r\n                    pixel++;\r\n                    addPixelValueByData(val, imgData);\r\n\r\n                }\r\n            }\r\n            imgDatas.add(imgData);\r\n\r\n        }\r\n    }\r\n```\r\n\r\nThen run the inference based on TensorFlow lite interpreter:\r\n```\r\n   public float[] runMultipleScaleInference(ArrayList<Bitmap> bitmaps) {\r\n        convertBitmapToByteBuffer(bitmaps);\r\n        //System.out.println(imgDatas.size());\r\n        //Object[] inputs = new Object[]{imgDatas, pitchArray};\r\n\r\n        //Object[] inputs = new Object[]{imgDatas};\r\n        \r\n        int size = imgDatas.size(); //It is 3 frames currently, imgDatas is the byte-buffer array to store the bitmap\r\n        Object[][] inputsArray = new Object[size][];\r\n        for (int i = 0; i < imgDatas.size(); i ++) {\r\n            Object[] inputs = new Object[]{imgDatas.get(i)};\r\n            inputsArray[i] = inputs;\r\n        }\r\n        Map<Integer, Object> outputs = new HashMap();\r\n        outputs.put(0, probArray);\r\n\r\n        tflite.runForMultipleInputsOutputs(inputsArray, outputs);\r\n        return getNormalizedProbability();\r\n\r\n\r\n    }\r\n```\r\n\r\n\r\nit crashes in \"tflite.runForMultipleInputsOutputs(inputsArray, outputs)\" with the stack trace log:\r\n\r\n> E/AndroidRuntime: FATAL EXCEPTION: pool-1-multi_scale-fingering-1\r\n>     Process: cn.findpiano.piano.ai.demo, PID: 4566\r\n>     java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(java.lang.Object[], java.util.Map)' on a null object reference\r\n>         at cn.findpiano.piano.ai.recognizer.MultiScaleFingeringRecognizer.runMultipleScaleInference(MultiScaleFingeringRecognizer.java:198)\r\n>         at cn.findpiano.recognizer.core.VoteRecognitionJob$Task.run(VoteRecognitionJob.java:76)\r\n>         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1162)\r\n>         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:636)\r\n>         at java.lang.Thread.run(Thread.java:764)\r\n\r\nI file this bug issue to know what is the correct pose to inference the 4D-tensors with bitmap arrays in tensorflow-lite in Android?\r\n\r\nThanks & Regards!", "comments": ["As per the error message, \r\n\r\n> java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(java.lang.Object[], java.util.Map)' on a null object reference\r\n\r\nIt seems the bug is in your code, `tflite == null` in\r\n```\r\n   tflite.runForMultipleInputsOutputs(inputsArray, outputs);\r\n```\r\n", "Hello, Thanks.\r\n\r\nI have fixed the problem.\r\n\r\nMaybe this issue can be closed.\r\n\r\nThanks for reply and help.", "hello, could you provide a solution for this issue? I got this issues too", "@vandics please file a separate bug, but in general this is indicative of an issue with client code having a null value for the Interpreter.", "Sorry for not open a new issue because my problem is the same as this one, maybe you could give me an advice. Below is my code. I just want to run tflite object detection with a custom dataset on an android device\r\n\r\n\r\n\r\nprivate ArrayList<Classifier.Recognition> getDetectionsForTiny(ByteBuffer byteBuffer, Bitmap bitmap) {\r\n        ArrayList<Classifier.Recognition> detections = new ArrayList<Classifier.Recognition>();\r\n        Map<Integer, Object> outputMap = new HashMap<>();\r\n        outputMap.put(0, new float[1][OUTPUT_WIDTH_TINY[0]][4]);\r\n        outputMap.put(1, new float[1][OUTPUT_WIDTH_TINY[1]][labels.size()]);\r\n        Object[] inputArray = {byteBuffer};\r\n        tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n        int gridWidth = OUTPUT_WIDTH_TINY[0];\r\n        float[][][] bboxes = (float [][][]) outputMap.get(0);\r\n        float[][][] out_score = (float[][][]) outputMap.get(1);\r\n\r\n        for (int i = 0; i < gridWidth;i++){\r\n            float maxClass = 0;\r\n            int detectedClass = -1;\r\n            final float[] classes = new float[labels.size()];\r\n            for (int c = 0;c< labels.size();c++){\r\n                classes [c] = out_score[0][i][c];\r\n            }\r\n            for (int c = 0;c<labels.size();++c){\r\n                if (classes[c] > maxClass){\r\n                    detectedClass = c;\r\n                    maxClass = classes[c];\r\n                }\r\n            }\r\n            final float score = maxClass;\r\n            if (score > getObjThresh()){\r\n                final float xPos = bboxes[0][i][0];\r\n                final float yPos = bboxes[0][i][1];\r\n                final float w = bboxes[0][i][2];\r\n                final float h = bboxes[0][i][3];\r\n                final RectF rectF = new RectF(\r\n                        Math.max(0, xPos - w / 2),\r\n                        Math.max(0, yPos - h / 2),\r\n                        Math.min(bitmap.getWidth() - 1, xPos + w / 2),\r\n                        Math.min(bitmap.getHeight() - 1, yPos + h / 2));\r\n                detections.add(new Classifier.Recognition(\"\" + i, labels.get(detectedClass),score,rectF,detectedClass ));\r\n            }\r\n        }\r\n        return detections;\r\n    }\r\nand below is my error\r\n\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.padi_eko, PID: 28056\r\n    java.lang.RuntimeException: Failure delivering result ResultInfo{who=null, request=2, result=-1, data=Intent { dat=content://media/external/images/media/578977 flg=0x1 }} to activity {com.example.padi_eko/com.example.padi_eko.camera}: java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(java.lang.Object[], java.util.Map)' on a null object reference\r\n        at android.app.ActivityThread.deliverResults(ActivityThread.java:4626)\r\n        at android.app.ActivityThread.handleSendResult(ActivityThread.java:4670)\r\n        at android.app.ActivityThread.-wrap19(Unknown Source:0)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1829)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:198)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7055)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:523)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:836)\r\n     Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(java.lang.Object[], java.util.Map)' on a null object reference\r\n        at com.example.padi_eko.camera.getDetectionsForTiny(camera.java:492)\r\n        at com.example.padi_eko.camera.onActivityResult(camera.java:288)\r\n        at android.app.Activity.dispatchActivityResult(Activity.java:7634)\r\n        at android.app.ActivityThread.deliverResults(ActivityThread.java:4622)\r\n\r\n\r\n", "Your `tfLite` Interpreter object is null. Either creation failed, or it wasn't created in the first place.", "I have created it with private Interpreter tfLite, also I have print the object interpreter and none of them are null. If I run object detection example from tensorflow, it is working correctly but when I run it on my app it will send an error like above."]}, {"number": 37687, "title": "[MLIR][XLA] Fix ops erase bug in DeadTempBufferRemoval", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n@sherhut @River707 \r\n\r\nI think it will break the DAG struct that recursiveErase called directly when function.walk\r\nThe operation which needs erasing should be put in a vector when function.walk firstly and erased after function.walk", "comments": ["below is a real example\r\n\r\n> *** IR Dump Before xla::mlir_gpu::{anonymous}::DeadTempBufferRemoval ***\r\n> func @fusion.96(%arg0: memref<33708x1024xf32>, %arg1: memref<33708x1024xf32>) {\r\n>   %c0 = constant 0 : index\r\n>   %c1 = constant 1 : index\r\n>   %cst = constant 3.125000e-02 : f32\r\n>   %c33708 = constant 33708 : index\r\n>   %c1024 = constant 1024 : index\r\n>   %0 = alloc() {temp = true} : memref<33708x1024xf32>\r\n>   %1 = alloc() {temp = true} : memref<f32>\r\n>   store %cst, %1[] : memref<f32>\r\n>   loop.for %arg2 = %c0 to %c33708 step %c1 {\r\n>     loop.for %arg3 = %c0 to %c1024 step %c1 {\r\n>       %2 = subview %arg0[%arg2, %arg3] [] [] : memref<33708x1024xf32> to memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>       %3 = subview %0[%arg2, %arg3] [] [] : memref<33708x1024xf32> to memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>       %4 = subview %arg1[%arg2, %arg3] [] [] : memref<33708x1024xf32> to memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>       %5 = load %3[%c0, %c0] : memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>       %cst_0 = constant 3.125000e-02 : f32\r\n>       %6 = load %1[] : memref<f32>\r\n>       store %cst_0, %3[%c0, %c0] : memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>       %7 = load %2[%c0, %c0] : memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>       %8 = load %4[%c0, %c0] : memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>       %9 = mulf %7, %cst_0 : f32\r\n>       store %9, %4[%c0, %c0] : memref<1x1xf32, affine_map<(d0, d1)[s0] -> (d0 * 1024 + s0 + d1)>>\r\n>     }\r\n>   }\r\n>   dealloc %1 : memref<f32>\r\n>   dealloc %0 : memref<33708x1024xf32>\r\n>   return\r\n> }\r\n\r\nFatal Python error: Segmentation fault", "Another question, how to add UT for the passes?", "Thanks for the example. It has a store directly after the alloc, which makes this fail. Thanks for catching this.\r\n\r\n> Another question, how to add UT for the passes?\r\n\r\nCurrently these cannot be tested, as they are not visible at all. They all have TODOs to turn them into real implementations as opposed to the prototype state they are currently in. A first step would be to move them into a separate file, adding a `PassRegistration` so that they become visible and then link them into a `mlir-opt` like tool. Unfortunately, none of this currently exists. We do not even have a directory structure for it. Let me see whether we can quickly add this.\r\n\r\nIn the meantime, this should still land."]}, {"number": 37686, "title": "The Problem with Distributed inference on multi-GPUs with tf.distribute.MirroredStrategy() strategy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): binary (install from pip) \r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n - Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: Cuda Toolkit 10.1 / cuDNN 7.6.4\r\n- GPU model and memory: 2 Titan V\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have tried to custom distributed inference on multi-GPUs (i.e 2 GPUs) follow the tutorial on page https://www.tensorflow.org/tutorials/distribute/save_and_load by using my model and `tf.distribute.MirroredStrategy()` strategy. In particular when I ran my bellow code\r\n```\r\n# Here is my code\r\n\r\nDEFAULT_FUNCTION_KEY = \"serving_default\"\r\ndata = tf.data.Dataset.from_tensor_slices(np.random.rand(32*100, 416, 416, 3).astype('float32')).batch(32)\r\n\r\n# load model and do inference on 1 GPU\r\nloaded = tf.saved_model.load('./model')\r\ninference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]\r\n\r\ntotal_time = 0.0\r\nfor batch in data:\r\n    begin = time.time()\r\n    inference_func(batch)\r\n    total_time += time.time() - begin\r\nprint('Total time on 1 GPU: ', total_time)\r\n\r\n# load model and do inference in a distributed manner\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\nwith mirrored_strategy.scope():\r\n    model = tf.saved_model.load('./model')\r\n    inference_func = model.signatures[DEFAULT_FUNCTION_KEY]\r\n\r\n    dist_predict_dataset = mirrored_strategy.experimental_distribute_dataset(data)\r\n\r\n    total_time = 0.0\r\n    for batch in dist_predict_dataset:\r\n        begin = time.time()\r\n        mirrored_strategy.run(inference_func, args=(batch,))\r\n        total_time += time.time() - begin\r\n    print('Total time on 2 GPUs: ', total_time)\r\n```\r\n*_Note: The above code is customed from your bellow sample code in the tutorial_\r\n```\r\n# Here is your sample code in the tutorial\r\n\r\nDEFAULT_FUNCTION_KEY = \"serving_default\"\r\nloaded = tf.saved_model.load(saved_model_path)\r\ninference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]\r\n\r\npredict_dataset = eval_dataset.map(lambda image, label: image)\r\nfor batch in predict_dataset.take(1):\r\n  print(inference_func(batch))\r\n\r\nanother_strategy = tf.distribute.MirroredStrategy()\r\nwith another_strategy.scope():\r\n  loaded = tf.saved_model.load(saved_model_path)\r\n  inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]\r\n\r\n  dist_predict_dataset = another_strategy.experimental_distribute_dataset(\r\n      predict_dataset)\r\n\r\n  # Calling the function in a distributed manner\r\n  for batch in dist_predict_dataset:\r\n    another_strategy.run(inference_func,args=(batch,))\r\n```\r\nand compared the total inference time of the model between 1 GPU and 2 GPUs with the same inputs. Unfortunately, I realized that the performance on 2 GPUs is **not** faster than on 1 GPU (about total time as well as the time of an iterator). It seems that the inference process executed on 2 GPUs **sequentially** (**not** **parallel as I expected**).\r\n\r\n**Describe the expected behavior**\r\nPlease let me know if there are any problems here and show me how to modify the code in order to run **distributed inference on multi-GPUs parallel** with the best performance. Thanks so much for your help\r\n\r\n**Standalone code to reproduce the issue** \r\nFiles to reproduce the test case in [example.zip](https://github.com/tensorflow/tensorflow/files/4348380/example.zip)\r\n\r\n**Other info / logs** \r\n\r\n", "comments": ["@ymodak  I've read the original code at https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/distribute/mirrored_strategy.py#L92 and see `run_concurrently = False`.  It means threads are going to run sequentially here.", "It is true that the inference_func is running on each GPU sequentially in eager mode, and you should be able to see a warning like [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/mirrored_strategy.py#L755).\r\n\r\nTo get things running in parallel, you can put the `strategy.run()` method inside a tf.function, so that graph building is sequential, but execution is parallel. See this [tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) for an example.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37686\">No</a>\n", "@ckkuang I tried following your tutorial and came to the following rough recipe: \r\n```python\r\n# https://github.com/tensorflow/tensorflow/issues/37686\r\n# https://www.tensorflow.org/tutorials/distribute/custom_training\r\ndef compute_and_write_ious_multi_gpu(path: str, filename_csv: str, include_sampled: bool):\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    util.log('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n    (ds, s, n) = dataset(path, shuffle=False, repeat=False, mask_as_input=True)\r\n    dist_ds = strategy.experimental_distribute_dataset(ds)\r\n\r\n    def predict_step(inputs):\r\n        images, labels = inputs\r\n        return model(images, training=False)\r\n\r\n    @tf.function\r\n    def distributed_predict_step(dataset_inputs):\r\n        per_replica_losses = strategy.run(predict_step, args=(dataset_inputs,))\r\n        return per_replica_losses  # unwrap!?\r\n\r\n    # https://stackoverflow.com/questions/57549448/how-to-convert-perreplica-to-tensor\r\n    def unwrap(per_replica):  # -> list of numpy arrays\r\n        if strategy.num_replicas_in_sync > 1:\r\n            out = per_replica.values\r\n        else:\r\n            out = (per_replica,)\r\n        return list(map(lambda x: x.numpy(), out))\r\n\r\n    with strategy.scope():\r\n        model = wrap_model()\r\n\r\n    util.log(f'Starting distributed prediction for {filename_csv}')\r\n    ious = [unwrap(distributed_predict_step(x)) for x in dist_ds]\r\n    t = ious\r\n    ious = [item for sublist in t for item in\r\n            sublist]  # https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\r\n    util.log(f'Distributed prediction done for {filename_csv}')\r\n    ious = np.concatenate(ious).ravel().tolist()\r\n    ious = round_ious(ious)\r\n    ious = list(zip(ious, ds.all_image_paths))\r\n    ious.sort()\r\n    write_ious(ious, filename_csv, include_sampled)\r\n```\r\nThis does distribute the load across the GPUs, but unfortunately makes very poor use of them - in my particular case the corresponding single-GPU code runs in ~12 hours, and this runs in 7.7 hours, so not even a 2x speedup despite have 8x the number of GPUs.\r\n\r\n`nvidia-smi` confirms that multi-GPU load bobs between 0 and 100%, whereas single-GPU load is almost pegged at 100%. \r\n\r\nI think it's mostly a data feeding issue due to the code structure. **Any suggestions for how to speed it up?**\r\n\r\nThe underlying dataset is created in the same way for both scenarios, and also for training where it's able to peg 8 GPUs at 100%, so it shouldn't be the dataset creation itself. \r\n\r\n(Ubuntu 16.04, tf 2.3.1, tf.keras 2.4.0, python 3.7.6, AWS p4d.24xlarge running on AWS Deep Learning AMI (Ubuntu 16.04) v38.0)"]}, {"number": 37685, "title": "Can subjoin one keras.layers operation?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): v2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n    I meet problem when i trying to spilt one layer, i need something like r**everse operation of  Concatenate**, that can **spilt  shape  [b,w,h,C]   to  [b,w,c,A]and[b,w,h,B]**, which C==A+B\r\n\r\n**Will this change the current api? How?**\r\n  It won't have any inpact influence on current api.\r\n\r\n**Who will benefit with this feature?**\r\n    Since that Tensorflow2.0 become easy to hand on, it will be more popular in acedemia if more easy and comfortable api are provided. \r\n\r\n**Any Other info.**\r\n", "comments": ["@LuuHu\r\ncould you please share a simple stand alone code for us to replicate the issue faced, along with the error logs.", "Sorry ,i mean it's not a error but suggest to add a **keras.layers function** that **spilt tensor** \r\n```\r\nimport tensorflow as tf\r\ninpt = tf.keras.Input(input_shape=(16,28,28,3))\r\nx = Conv2D(128,3)(inpt)  #(16,28,28,128)\r\nA,B = THE_SPILT_FUNCTION(axis=20)(x)  # A=(16,28,28,20) B=(16,28,28,108)\r\nmodel = tf.keras.Model(inputs=inpt, outputs=[A,B])\r\n```\r\nand it like reverse operation like tf.keras.layers.Concatenate\r\n![image](https://user-images.githubusercontent.com/41143797/76949587-b6dec700-6943-11ea-9119-f385a7b63845.png)\r\nsuch as \r\nnp.spilt operte one array \r\n```\r\nA = np.ones((100,32,32,50))\r\nprint(A.shape)\r\nA,B = np.split(A,(20,),axis=-1)\r\nprint(A.shape)\r\nprint(B.shape)\r\n```\r\n\r\nThanks", "Why not use tf.split with keras lambda layer?\r\ni.e.,:\r\n```python\r\noutputs = tf.keras.layers.Lambda(lambda x : tf.split(x, [size_A, size_B], axis=-1)\r\n```", "Wow, Thanks a lot.", "> Wow, Thanks a lot.\r\n\r\nNp!"]}, {"number": 37684, "title": "fix a bug in AddSymbolicGradients", "body": "Fixes #37593 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37684) for more info**.\n\n<!-- need_sender_cla -->", "@buaasun thank you for the contribution. Please sign CLA.", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37684) for more info**.\n\n<!-- ok -->", "@alextp updated."]}, {"number": 37683, "title": "SparseCategoricalCrossentropy not working with custom Model", "body": "I tried to do a custom model in Colab\r\n\r\n```\r\n%tensorflow_version 2.x\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nx_train = x_train[...,None]\r\nx_test = x_test[...,None]\r\n\r\nclass MyModel(keras.Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.conv1 = keras.layers.Conv2D(32, 3, activation='relu')\r\n    self.flatten = keras.layers.Flatten()\r\n    self.d1 = keras.layers.Dense(128, activation='relu')\r\n    self.d2 = keras.layers.Dense(10, activation='softmax')\r\n\r\n  def call(self, x):\r\n    x = self.conv1(x)\r\n    x = self.flatten(x)\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n\r\nmodel = MyModel()\r\n\r\nmodel(keras.layers.Input(( 28, 28, 1)))\r\n\r\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate= 1e-3), loss=keras.losses.SparseCategoricalCrossentropy(from_logits= True), metrics = [keras.metrics.Accuracy()])\r\n\r\nmodel.fit(x = x_train, y = y_train, batch_size= 32, epochs= 10)\r\n```\r\n\r\nTried training the regular MNIST data and when I do model.fit\r\nI get the following\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-2f67866b9e8a> in <module>()\r\n----> 1 model.fit(x = x_train, y = y_train, batch_size= 32, epochs= 10)\r\n\r\n29 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)\r\n   1108     \"\"\"\r\n   1109     if not self.is_compatible_with(other):\r\n-> 1110       raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n   1111 \r\n   1112   def most_specific_compatible_shape(self, other):\r\n\r\nValueError: Shapes (32, 10) and (32, 1) are incompatible\r\n```\r\n\r\nI thought that I might need to define an argmax somewhere in my model if is not supported, however the other keras.losses.sparse_categorical_crossentropy() does support it.\r\n\r\n```\r\nloss = keras.losses.sparse_categorical_crossentropy(y_true=y_train[:10], y_pred=model(x_train[:10]), from_logits= True, axis = -1)\r\nloss\r\n```\r\nGives me this output\r\n```\r\n<tf.Tensor: shape=(10,), dtype=float32, numpy=\r\narray([2.3041558, 2.304716 , 2.2958994, 2.3044436, 2.2858677, 2.308184 ,\r\n       2.2969985, 2.3054423, 2.3005517, 2.3033297], dtype=float32)>\r\n```\r\n\r\nAny reason why? I was just testing the integration of custom models and the functional API.", "comments": ["Actually, I fixed it, all I needed was\r\n metrics = [keras.metrics.SparseCategoricalAccuracy()]", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37683\">No</a>\n"]}, {"number": 37682, "title": "[MLIR/XLA] add a pass to outline kLoop/kInput fusion pattern in xla_hlo dialect", "body": "This pass implements the basic kLoop/kInput fusion in fully dynamic shape scenario. Similar to XLA, this pass also supports fusion pattern having multiple outputs if all the shape of outputs are consistent. To this end, we also add a simple shape constrain analysis. This could be built on the shape dialect once it is ready.\r\n\r\n![image](https://user-images.githubusercontent.com/15364516/76931205-1ed0e580-6923-11ea-8732-fe9d93c3bcaf.png)\r\n\r\n", "comments": ["It seems like you create a new function call for each of these \"fusion\".\r\nHave you considered a lighter-weight region-based approach?\r\n\r\nI'm thinking about the equivalent of lhlo.fusion operation? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/xla/ir/lhlo_ops.td#L388", "> It seems like you create a new function call for each of these \"fusion\".\r\n> Have you considered a lighter-weight region-based approach?\r\n> \r\n> I'm thinking about the equivalent of lhlo.fusion operation? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/xla/ir/lhlo_ops.td#L388\r\n\r\nThanks for your reply!\r\n\r\nI agree that region style is more lightweight for fusion itself. Here I use a function style because this style  is easier to play with other existing passes (e.g. hlo-legalize-to-lhlo) .  I will change to region style after the general structure review of this pass is finished.\r\n", "> I looked at the code and left some stylistic comments.\r\n> \r\n> What is the plan for this code moving forward? What do you aim to build and where does it fit in?\r\n\r\nWe plan to add E2E dynamic shape fusion&codegen support based on MLIR.\r\nThis PR mainly covers the fusion pass. As to the codegen part, we are already working on it and later another PR may be submit.\r\n\r\nThanks.", "> We plan to add E2E dynamic shape fusion&codegen support based on MLIR.\r\n> This PR mainly covers the fusion pass. As to the codegen part, we are already working on it and later another PR may be submit.\r\n\r\nThis is fantastic!\r\nCan you send RFCs and conduct discussions about the plan ahead of time? We should likely coordinate a bit more to make sure your work align with our. \r\nWe will also try to surface a better roadmap on our side to give visibility to everything that we're doing.\r\nAt the moment IREE is also doing some dynamic codegen and they are very interested in better alignment as well.", "@sherhut Thanks for your suggestion. I have done some refine work.", "@joker-eph  Here are some documents:\r\n\r\n[RFC](https://groups.google.com/a/tensorflow.org/forum/#!topic/mlir/_X48poNcbDI)  and [PPT]( https://drive.google.com/open?id=1ZDzXluB2uVc35r1fBNK5jW6rY8s82pc_)", "@joker-eph @sherhut Could you help to have a look?", "The corresponding codegen PR for kLoop/kInput fusion is as following:\r\nhttps://github.com/tensorflow/tensorflow/pull/38186/files\r\nhttps://reviews.llvm.org/D77447\r\n\r\nJust refer them here to ensure the reviewers can have a full-picture of what we are working on. ", "@wyzero Can you please check @joker-eph's comments and resolve conflicts?. Thanks!", "Sorry for the delay. I will fix it soon.", "For interface-based fusibility check, it will be implemented in next commit soon. ", "@joker-eph Could you help to take a look?", "@joker-eph Hi Mehdi, could you take another round of review for this PR? It has been pending for more than one month:) If possible, we would like to close  it ASAP.", ">Hi Mehdi, could you take another round of review for this PR? It has been pending for more than one month:) If possible, we would like to close it ASAP.\r\n\r\n@alibaba-common it has been *in review* with a few round that happened during this time, it wasn't \"pending\" as waiting for being reviewed. Most of the time I believe has been spent waiting for update on the author side to address comments. I invite you to take another look at the delays between the pushes/updates to the PR and my reviews.\r\nIt also does not help that some comments/questions are left unanswered. ", "FYI the sanity build log is complaining about formatting:\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/compiler/mlir/xla/BUILD:\r\n153d152\r\n<         \"ir/infer_fusibility_op_interface.h.inc\",\r\n154a154\r\n>         \"ir/infer_fusibility_op_interface.h.inc\",\r\n369d368\r\n<     hdrs = [\"transforms/cycle_detector.h\"],\r\n370a370\r\n>     hdrs = [\"transforms/cycle_detector.h\"],\r\n381d380\r\n<         \":hlo\",\r\n382a382\r\n>         \":hlo\",\r\n384c384\r\n<         \"//tensorflow/compiler/jit/graphcycles:graphcycles\",\r\n---\r\n>         \"//tensorflow/compiler/jit/graphcycles\",\r\n593d592\r\n<         \":xla_canonicalize_inc_gen\",\r\n594a594\r\n>         \":xla_canonicalize_inc_gen\",\r\n893d892\r\n<         \":xla_hlo_fusion\",\r\n900a900\r\n>         \":xla_hlo_fusion\",\r\nexit status 1\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```", "I suspect you should also have access to the windows build log: https://source.cloud.google.com/results/invocations/dadc78f3-a639-40d5-86bd-301ad8866b1f/log\r\n\r\n```\r\n\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(4285) : error C4716: 'mlir::xla_hlo::XorOp::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(3936) : error C4716: 'mlir::xla_hlo::SubOp::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(1752) : error C4716: 'mlir::xla_hlo::Expm1Op::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(3878) : error C4716: 'mlir::xla_hlo::SqrtOp::inferEffectiveWorkloadSize': must return a value\r\nT:\\tmp\\bigvaudl\\execroot\\org_tensorflow\\bazel-out\\x64_windows-opt\\bin\\tensorflow\\compiler\\mlir\\xla\\ir\\hlo_ops.h.inc(3744) : error C4716: 'mlir::xla_hlo::SinOp::inferEffectiveWorkloadSize': must return a value\r\n...\r\n```", "> FYI the sanity build log is complaining about formatting:\r\n> \r\n> ```\r\n> FAIL: buildifier found errors and/or warnings in above BUILD files.\r\n> buildifier suggested the following changes:\r\n> tensorflow/compiler/mlir/xla/BUILD:\r\n> 153d152\r\n> <         \"ir/infer_fusibility_op_interface.h.inc\",\r\n> 154a154\r\n> >         \"ir/infer_fusibility_op_interface.h.inc\",\r\n> 369d368\r\n> <     hdrs = [\"transforms/cycle_detector.h\"],\r\n> 370a370\r\n> >     hdrs = [\"transforms/cycle_detector.h\"],\r\n> 381d380\r\n> <         \":hlo\",\r\n> 382a382\r\n> >         \":hlo\",\r\n> 384c384\r\n> <         \"//tensorflow/compiler/jit/graphcycles:graphcycles\",\r\n> ---\r\n> >         \"//tensorflow/compiler/jit/graphcycles\",\r\n> 593d592\r\n> <         \":xla_canonicalize_inc_gen\",\r\n> 594a594\r\n> >         \":xla_canonicalize_inc_gen\",\r\n> 893d892\r\n> <         \":xla_hlo_fusion\",\r\n> 900a900\r\n> >         \":xla_hlo_fusion\",\r\n> exit status 1\r\n> Please fix manually or run buildifier <file> to auto-fix.\r\n> ```\r\n\r\nThanks for your help. It seems that there is  no difference for most of lines (except the line for `\"//tensorflow/compiler/jit/graphcycles:graphcycles\"`).  It that true?", "> Thanks for your help. It seems that there is no difference for most of lines (except the line for \"//tensorflow/compiler/jit/graphcycles:graphcycles\"). It that true?\r\n\r\nThe other lines are pointing at issues in ordering: these should be alphabetically sorted I think.", "(FYI: there is an issue with our import script I had to fix as well, but I need an internal review and I won't get it today I think)", "@joker-eph  I have fixed the above problems and rebase to master as well.", "It fails with:\r\n\r\n```\r\ntensorflow/compiler/mlir/xla/ir/infer_fusibility_op_interface.h.inc:89:61: error: use of undeclared identifier 'lhs'\r\n        assert(lhs < op.getOperation()->getNumOperands() && lhs >= 0 &&\r\n                                                            ^\r\n```", "Our internal linter also forced me to add a header guard in cycle_detector.h, add newlines at end of source files, and replace the `using namespace` directive with more targeted `using`.", "Also fixed other issues:\r\n- single-argument constructors must be marked explicit to avoid unintentional implicit conversions\r\n- private field 'planner_' is not used\r\n- function 'mlir::GraphCycles::IsReachable' has a definition with different parameter names\r\n- function 'mlir::GraphCycles::InsertEdge' has a definition with different parameter names\r\n- function 'mlir::GraphCycles::RemoveEdge' has a definition with different parameter names\r\n- function 'mlir::GraphCycles::HasEdge' has a definition with different parameter names\r\n\r\n", "> * single-argument constructors must be marked explicit to avoid unintentional implicit conversions\r\n\r\nThanks! I will fix it soon.", "@joker-eph Done! Thanks!", " @jpienaar  Thanks for your suggestions.  I have done some refine.", "@joker-eph Thanks for your suggestions. I will add some tests for `cycle_detector.cc`.  I have one question though. Do we need to use llvm testing framework or just use tensorflow testing framework? And since these codes are in TF repo. now, it seems the latter one is better?", "Please follow MLIR best practices.", "Okay. Do you mean use FileCheck (like the tests in xla_hlo_fusion.mlir) ?", "MLIR/LLVM is using gtest for testing datastructures, and FileCheck for IR test\r\n(Sorry i am terse but I am on my phone)", "@joker-eph  Done!", "@joker-eph  Done!", "> FYI: there is an issue with our import script I had to fix as well, \r\n\r\nFYI: my fix got reverted because it broke on windows and so this is blocking the integration of this PR, sorry for the delay I need to give another try to fix the script.", "I havent looked into details of this PR yet, but wanted to leave a high-level comment\r\n1) Having the OpInterfaces is useful. Please consider starting a discussion on MLIR discourse to get more info on how this could be moved into MLIR.\r\n2) Within IREE, the approach being taken is to convert from XLA-HLO to Linalg on tensors and do fusion there. Its still WIP and we can always convert an xla_hlo.fusion op to a linalg.generic op, but the advantage with fusion on linalg is much lesser surface area of ops. So makes it easier to reason about which ops can be fused and which cant be. Building up that infrastructure in core will benefit more users in the long run. But it is perfectly valid to do fusion at xla_hlo level as done here, and to some extent the use of OpInterface mitigates considering all pair-wise combinations. So not a comment on value of this approach (which is also very valuable).", ":+1: to what @MaheshRavishankar said - the op interface for making it possible to write a nice generic fusion pass would be a great addition to core. I'd have to do some more research but we may not be able to use this pass directly as we have different semantics around what fusions can be in certain cases, but reusing the op pass and the definitions on the HLO ops would dramatically simplify our version and decouple the pass from the HLO ops themselves.", "@joker-eph  @MaheshRavishankar @benvanik \r\n\r\nThanks for your nice comments.\r\n\r\nIs there any other work you suggest us to do to complete this PR merge? We want to ensure important code change from our side can be synced-up with community in a timely manner to avoid big divergence.  \r\n\r\nIn the long run, we do consider implementing the fusion pass in lower-level dialect such as Linalg mentioned before. Currently we choose HLO dialect since lots of our previous experiences on XLA can be reused in an efficient way. With the evolution of MLIR I do expect more and more exciting work can be implemented on top of the MLIR infra.\r\n\r\nThanks", "@mihaimaruseac this integration was fixed by the change you reverted from b/154887010 ; can you ping here when it is fixed?\r\n", "Assinging to me and will look for both. thanks for the bisect", "@mihaimaruseac @joker-eph  Hi,  Is there any update about the integration ^v^? Thanks!", "@mihaimaruseac @joker-eph Hi! Any update? Thanks!", "> @mihaimaruseac @joker-eph Hi! Any update? Thanks!\r\n\r\n@mihaimaruseac @joker-eph \r\n\r\nHi folks, any feedbacks and comments are highly appreciated from your side:) ", "Hi. Apologies for the delay.\r\n\r\nThe sync is broken due to a conflict between tooling not supporting a pattern that would allow some shortcuts during integration (the path @joker-eph wanted to take) and the requirement to do multiple manual changes internally since usage pattern does not work (the part that slipped between the cracks and we didn't have an assignee between me and @joker-eph)", "@mihaimaruseac @joker-eph  Hi, thanks for your feedback. Is there any plan to fix the problem and continue the integration?", "We are blocked on @joker-eph from what I recall", "@joker-eph  Hi, could you help to take a look? Thanks!", "We hit some internal both technical and organizational issue on this, to the point where I'm not sure we'll be efficient at accepting contributions here.\r\n\r\nInstead I am considering migrating/rewriting this PR to https://github.com/llvm/llvm-project right now. Let me know if you're interested in this, otherwise I'll tackle this probably later this week.", "@wyzero Can you please check @joker-eph's comments and keep us posted. Thanks!", "We're in the process of fixing it, last week I reproduced the issue (that exists only on Windows) and narrowed it down to the fact that our Kokoro CI is using an older version of the tool on Windows compared to Mac/Linux.", "@joker-eph Thanks for your update.  It would be very nice if this PR could be merged into TF repo. first since our internal repo. also use this PR and this could reduce divergence. Initially this pass was designed for the HLO dialect since many of our previous work are based on XLA.  Thanks to your nice comment, we extended the pass to use OpInterfaced to make it more general.  In the future,  we would also like to collaborate with community to make this pass even more general and finally merge it to upstream LLVM repo.\r\n", "Sorry for the delay, as mentioned before this ran into a corner case of our automation. The infrastructure that performs the import/export between our internal repo and GitHub is now updated (it used an outdated prebuilt binary of some tool on Windows), if you rebase on master I should be able to integrate it.\r\n", "@jpienaar Thanks for your comments. I have changed accordingly.", "@joker-eph Thanks for your update. I have rebased to  the latest master.", "@joker-eph  I checked the log of failed uts. As shown as below, it seems that this error is not related to what I changed? Any suggestions? Thanks!\r\n\r\n\r\n\r\n==================== Test output for //tensorflow/python/kernel_tests:metrics_test (shard 12 of 20):\r\nTraceback (most recent call last):\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/metrics_test.py\", line 36, in <module>\r\n    from tensorflow.python.ops import metrics\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/ops/metrics.py\", line 23, in <module>\r\n    from tensorflow.python.ops.metrics_impl import *\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/ops/metrics_impl.py\", line 30, in <module>\r\n    from tensorflow.python.ops import nn\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/ops/nn.py\", line 28, in <module>\r\n    from tensorflow.python.ops import ctc_ops as _ctc_ops\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/ops/ctc_ops.py\", line 39, in <module>\r\n    from tensorflow.python.ops import linalg_ops\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/ops/linalg_ops.py\", line 29, in <module>\r\n    from tensorflow.python.ops import map_fn\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/ops/map_fn.py\", line 25, in <module>\r\n    from tensorflow.python.autograph.core import ag_ctx as autograph_ctx\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/autograph/__init__.py\", line 39, in <module>\r\n    from tensorflow.python.autograph.impl.api import AutoGraphError\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/autograph/impl/api.py\", line 32, in <module>\r\n    from tensorflow.python.autograph.impl import conversion\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/autograph/impl/conversion.py\", line 35, in <module>\r\n    from tensorflow.python.autograph.converters import lists\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/metrics_test.runfiles/org_tensorflow/tensorflow/python/autograph/converters/lists.py\", line 1\r\n\r\n    ^\r\nSyntaxError: I/O error while reading\r\n================================================================================", "Closed in https://github.com/tensorflow/tensorflow/commit/9fc38527dabf8d08ca2a608dda0c052524eec058"]}, {"number": 37680, "title": "Guided Backprop Gradcam with TF 2.0 for Transfer Learning", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  \r\nYes\r\n\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\nCentos 7\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\nTensorflow 2.0 from source\r\n\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\nPython 3.6\r\n\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\nCuda 10.0, Titan V 12gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am trying to do gradient visualization using gradcam with guided backpropagation. I can do it on VGG16, but when I add layers to VGG16, I get an error that the layer is not found. \r\n\r\n**Describe the expected behavior**\r\nI want to visualize activation layers using gradcam with guided guided backprop. \r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nHere's the Colab code with both the working example using just VGG16 and the broken example using VGG16 with added layers. \r\n\r\nhttps://colab.research.google.com/drive/1098Hp2icvleIQelkaLmPoIqAKuahs7JH\r\n\r\nI'm attaching an image of a cat which is needed to run the colab. \r\n\r\n![cat 3](https://user-images.githubusercontent.com/36546688/76920921-ef21cf00-6889-11ea-8465-9b061a8f44c7.jpg)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nHere's the traceback:\r\n```\r\n\r\nTensor(\"block5_conv3_6/Identity:0\", shape=(None, 14, 14, 512), dtype=float32)\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-7-511b9293dd65> in <module>()\r\n     60 # Get the score for target class\r\n     61 with tf.GradientTape() as tape:\r\n---> 62     conv_outputs, predictions = grad_model(np.array([img]))\r\n     63     loss = predictions[:, 1]\r\n     64 print('Prediction shape:', predictions.get_shape())\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    889           with base_layer_utils.autocast_context_manager(\r\n    890               self._compute_dtype):\r\n--> 891             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    892           self._handle_activity_regularization(inputs, outputs)\r\n    893           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    706     return self._run_internal_graph(\r\n    707         inputs, training=training, mask=mask,\r\n--> 708         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n    709 \r\n    710   def compute_output_shape(self, input_shape):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)\r\n    868     output_shapes = []\r\n    869     for x in self.outputs:\r\n--> 870       assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)\r\n    871       tensor = tensor_dict[str(id(x))]\r\n    872       output_shapes.append(x.shape)\r\n\r\nAssertionError: Could not compute output Tensor(\"block5_conv3_6/Identity:0\", shape=(None, 14, 14, 512), dtype=float32)\r\n\r\n```", "comments": ["Can you please share the colab notebook via github? Since permission is getting denied.\r\nAlso, one main reason can be because you are passing a Numpy array to the model during graph mode execution. One thing you can do is either pass a Tensor of image, or execute it in Eager mode.(May be removing the `@tf.function` decorator from the training step?)", "@jdlamstein,Please provide the access to view the colab notebook. and also check the @captain-pool's workaround and let us know how it progresses. Thanks", "Thanks @gadagashwini  @captain-pool , I added the sharable link for colab. I tried @captain-pool 's suggestion to change the input from numpy array to tensor, but I got the same error. ", "I was able to replicate the issue issue.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/d05a2511db244ed3e39103ed7c5e049d/untitled468.ipynb). Thanks!", "Hey, I think I found a workaround. If you append layers directly to the base model rather than insert the base model as a layer into your new model, it's easier to grab the layers. \r\n\r\nWhen setting up the model, I had to set the input_tensor when initializing the Model.\r\n\r\n```\r\ninp = layers.Input(shape=(imsize[0], imsize[1], imsize[2]))\r\nbase_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,\r\n                                          input_shape=(imsize[0], imsize[1], imsize[2]))\r\n```\r\n\r\nI set up the vgg16 base model and grabbed the output of the pooling layer from block5. \r\n\r\n```\r\nbase_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_tensor=inp,\r\n                                          input_shape=(imsize[0], imsize[1], imsize[2]))\r\nblock5_pool = base_model.get_layer('block5_pool')\r\nx = global_average_layer(block5_pool.output)\r\nx = fc1(x)\r\n# x = fc2(x)\r\nx = prediction(x)\r\n\r\nmodel = tf.keras.models.Model(inputs = inp, outputs = x)\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\r\n                  loss='binary_crossentropy',\r\n                  metrics=['accuracy'])\r\n```\r\n\r\nThen I created a new model to output both the prediction and layer I want to visualize. \r\n\r\n```\r\n# Create a graph that outputs target convolution and output\r\ngrad_model = tf.keras.models.Model(inputs = [model.input], \r\n                                   outputs=[model.output, model.get_layer(LAYER_NAME).output])\r\n\r\nprint(model.get_layer(LAYER_NAME).output)\r\n# Get the score for target class\r\n\r\n# Get the score for target class\r\nwith tf.GradientTape() as tape:\r\n    predictions, conv_outputs = grad_model(img)\r\n    loss = predictions[:, 1]\r\nprint('Prediction shape:', predictions.get_shape())\r\n# Extract filters and gradients\r\noutput = conv_outputs[0]\r\ngrads = tape.gradient(loss, conv_outputs)[0]\r\n```\r\n\r\nI edited the colab and it worked. I added a fully connected layer, which is untrained so visualization will look at little random, but I did it a few times and it still seems to preserve the gist of it. \r\nhttps://colab.research.google.com/drive/1098Hp2icvleIQelkaLmPoIqAKuahs7JH\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37680\">No</a>\n", "how about in the case of 1D a trained CNN I would like to see which part of inputs are most related to output through the model that I have not a picture any help is appreciated "]}]