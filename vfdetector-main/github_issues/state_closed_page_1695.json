[{"number": 2050, "title": "TensorBoard: Refresh Backend when the frontend refreshes", "body": "Tensorboard seems to scan the log infrequently. Right now, I have to kill and re-start the server to make it re-scan the log. Could we have tensorboard re-scan the log when the tensorboard webpage is refreshed?\n", "comments": ["It scans every minute (although it may take longer to finish loading the data). In master you can configure this via [a flag on the TensorBoard process](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/tensorboard.py#L60) although that didn't make it into 0.8.\n\nIt would be nice to wire up the reload button to also trigger an immediate reload on the backend. But I don't have any near-term plans to implement that.\n\nIn practice, if TensorBoard seems to be scanning the log infrequently, it is more likely that this is because you are writing to multiple summary files simultaneously (ie. you have more than one SummaryWriter active at a time). TensorBoard expects that only one file is written to at a time, so if you have an early file A and a later file B, once B is ever written to, it assumes file A is finalized and will stop reading events from file A. So if most of your actvitiy is in file A it will look like TensorBoard has just stopped updating. When you kill and restart the server, it reads everything from file A before going on to file B, so the result is it loads data that was hidden to it before.\n\nWe're currently working on code to have TensorBoard detect this case and warn you in the UI, so it should become less confusing. \n", "My scenario is a bit different. I am trying to check if my model implementation is correct by over-fitting a small training set while varying the learning rate. Because the training set is small, it completes in about a second or so. But the tensorboard visualization is stale, so I end up re-starting the server. \n\nI'd be happy implement triggering the log reload on refresh if you're OK with that feature. Could you please point me to where I should poke around?\n", "I see; that makes sense then. Your help with adding this feature would be extremely welcome :) here's an overview of how we could do it.\n\nOverall, TensorBoard consists of a few major pieces. On the backend, there are the [`EventMultiplexer`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/python/summary/event_multiplexer.py) and [`EventAccumulator`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/python/summary/event_accumulator.py), a pair of classes which provide an API for accessing summary data from TensorFlow. \n\nThe [`Server`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/tensorboard/backend/server.py) instantiates these classes and creates a `MultiplexerReloadingThread` which sets a timer to automatically reload the `Multiplexer`. If the `Multiplexer` is already caught-up to historical data, it will generally load very quickly, but if you just turned TensorBoard on and it has 5 gigs of historical data to chew through, it may take a while.\n\n[`handler.py`](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/tensorboard/backend/handler.py#L460) contains all the handlers for the server's routes. The API is documented in the file [http_api.md](https://github.com/tensorflow/tensorflow/blob/0ebbb99084091bba963f7b452d9f2fa93c70f6e2/tensorflow/tensorboard/http_api.md). Please take a moment to read that (the next paragraph will assume some familiarity).\n\nThe way reloading works is as follows: the user clicks the reload button, which calls the `reload` method that is implemented by `TF.Backend.Behavior`. The behavior first reloads the run-tag mapping, and then triggers every existing data display element (e.g. a chart or image loader) to reload.\n\nThus, if we want to have the reload trigger a backend reload, we should have the first method that is called on the backend (the `runs` route) delay while the backend reloads. I think a good way to implement this would be to have an optional query parameter passed to the route (?reload), and if the query is present, then rather than respond immediately, the server reloads the backend and then responds.\n\nHowever, as I mentioned above, the reload may take a long time if the server just turned on and there is a lot of historical data to explore. So we'll need to have a timeout, say 5s - if the backend reload takes more than 5 seconds, then we resolve early even though we aren't done loading.\n\nIt would be really nice to further return some info on whether the backend is done loading or not so we can put a little \"data still loading!\" type indicator, but this change is complicated enough without doing that :)\n\nDoes that seem reasonable?\n", "@danmane: Any chance this is obviated by improvements since then? \n", "This is still a valid request. I changed the issue name to better reflect where the conversation went, though.\n", "(**Edit**: My interpretation of the issue is incorrect; see danmane's comment below)\nWait, we're not supposed to write to multiple SummaryWriters in a single session? Why does the official TensorBoard \"getting started\" tutorial showcase this behavior if it's not supported?\n\nI'm trying to (unsuccessfully) base my example off of https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html , which creates a `train_writer` and a `test_writer` and writes to them both. Is having two writers necessary?\n\nScanning multiple writers at once is also helpful to monitor several running experiments. E.g. consider watching all the loss curves for a parameter sweep/grid search for 5 different experiments at the same time.\n", "Sorry, I misspoke. You can't have multiple summary writers writing to the\nsame run, i.e. In the exact same subdirectories. Having multiple summary\nwriters pointing at different directories, as is the case in the tutorial,\nis fine.\nEl El s\u00e1b, jun 25, 2016 a la(s) 10:17 AM, gcr notifications@github.com\nescribi\u00f3:\n\n> Wait, we're not supposed to write to multiple SummaryWriters in a single\n> session? Why does the official TensorBoard \"getting started\" tutorial\n> showcase this behavior if it's not supported?\n> \n> I'm trying to (unsuccessfully) base my example off of\n> https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html\n> , which creates a train_writer and a test_writer. Aren't they both\n> necessary?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2050#issuecomment-228559462,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABVc1xP0tV7DaIKMugpm6Ou2alrqTBbRks5qPWKlgaJpZM4IMmOo\n> .\n", "Ah, I see! Thanks for the clarification.\n", "Is this in the master branch now? I see a refresh icon in the top right corner of the page next to the settings gear, so I was wondering if that was the implementation that @danmane  is referring to. However, clicking that icon doesn't seem to do anything for me. \n\nDuring development I edit my models often and run quick training sessions that take a minute or less to see the effects of my edits. I delete all the events data from the model directory, but TensorBoard continues to show the old events. I have to repeatedly stop and restart the tensorboard process to see the new data. I was happy when I noticed the refresh icon, but it doesn't seem to be working for me.\n", "Dear all,\r\nI run a program on the server (which is much faster because it has GPU supported). I download the summaries down to my local machine and try to visualize the result with Tensorboard. However, I encounter these 2 problems:\r\n1. It shows training's performance but not dev's performance (though I downloaded dev's summaries in the same sub-directory as well)\r\n2. The Tensorboard just show result of 500 first iteration steps while my experiment has >22k steps. I had to press the button \"Toggle all run\" many times to refresh the new result. Are there any better ways ?\r\nI appreciate very much for any possible help.", "@lethienhoa \r\n1. Try putting dev's summaries in a different sub-directory.\r\n2. Try launching TensorBoard with --reload_interval=5 or some similar smaller value. Then pushing the refresh button (in top right corner) should get you new data more regularly.", "Thank you very much for your answer. It worked for the first point ! However, on the second point, though I've tried --reload_interval=1 (minimum int value for this var) but it doesn't speed up too much. I still have to push the refresh button thousands of time...\r\nDo you have any other better ways ?", "@lethienhoa , try putting `setInterval(function() {document.getElementById('reload-button').click()}, 7000);` into chrome console `Ctrl+Shift+J` :)", "And is there a plan to implement something like this above?", "Hi,\r\nI've figured that we don't need to do any refresh page by hand at all. When we open the terminal by \"tensorboard --logdir yoursummarydirectory\", open up your browser and wait (~5 or 10 min depending on the size of your model) and it will automatically load all.\r\nI don't know why there is a delay like that but you definitely don't need to do boring stuffs anymore, just wait !", "@lethienhoa I think you issues is more like \"loading slow\". But for me I hope to monitoring the summary metrics on the fly while the calculation is running generating logs. Currently the page won't refresh itself but MInner's solution above seems very handy at the moment. ", "This issue has been migrated to https://github.com/tensorflow/tensorboard/issues/80.", "> @lethienhoa , try putting `setInterval(function() {document.getElementById('reload-button').click()}, 7000);` into chrome console `Ctrl+Shift+J` :)\r\n\r\nIf you are getting here from google, and the above gives you an error, it seems that you might have to change that code into this (I don't understand javascript but I did a bit of googling):\r\n\r\n```\r\nsetInterval(\r\n  function() {\r\n    document.getElementsByTagName(\"tf-tensorboard\")[0].shadowRoot.getElementById(\"reload-button\").click()\r\n  }, 7000\r\n);\r\n```"]}, {"number": 2049, "title": "FCN Crop Layer", "body": "Hi all,\n\nI am currently trying to implement FCN for semantic segmentation in TensorFlow as it was previously done in Caffe here https://github.com/shelhamer/fcn.berkeleyvision.org.\n\nUnfortunately I'm struggling with following 3 things:\n\n1) How to map `Deconvolution` layer from Caffe to TensorFlow? But I guess that I have found the solution in `tf.nn.conv2d_transpose`?\n2) How to map `Crop` layer from Caffe to TensorFlow? **Unfortunately I can't see any alternative in TensorFlow. Is there equivalent for this?**\n3) Does Caffe `SoftmaxWithLoss` correspond to TensorFlow softmax_cross_entropy_with_logits?\n\nThank you in advance for any advices, hints and help.\n", "comments": ["I am not famiilar with Caffe but:\n1. yes, conv2d_transpose is what a lot of people call 'Deconvolution': https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#conv2d_transpose\n2. Also not sure what Crop does (can't find documentation for it), but check here: https://www.tensorflow.org/versions/r0.8/api_docs/python/image.html#cropping\n3. https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#softmax_cross_entropy_with_logits is our documentation for the math behind the op -- I cannot find similar mathematical description of SoftmaxWithLoss here: http://caffe.berkeleyvision.org/tutorial/loss.html, so I can only assume that they are the same.\n\nLastly, these are questions better asked on StackOverflow, only people looking at bugs / issues are really looking here.\n", "(closing -- further discussion should be on StackOverflow)\n", "@vrv Thank you very much for your reply.\nI have tried asking on [StackOverflow](http://stackoverflow.com/questions/36746860/fcn-in-tensorflow-missing-crop-layer) and [Google groups](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/WC3tyFXILvE) before, but with no answer. This was the reason for creating the issue also here. \n"]}, {"number": 2048, "title": "Input to reshape is a tensor with 4889 values, but the requested shape has 4800", "body": "Operating System:Ubuntu 14.04\n1.I fixed my images to 200*200 .jpg,and stored into TFRecord file\n2.I read TFR file and try to train in cifar10 model,it raised error:\nInput to reshape is a tensor with 4889 values, but the requested shape has 4800\non :`depth_major = tf.reshape(image,[result.depth, result.height, result.width])`\nIt seems cause of diffrent of images' buffer size?\nShould I  fix the sizes before put into TFR file?\n", "comments": ["@neverPick, the size of the \"image\" needs to be the same as what you are reshaping it to.\n", "(in other words, \"reshape\" is not \"resize\".  There are operations for resizing available in TensorFlow though.)\n", "How did you solve the issue? Iam facing the same issue and not able to find what and where to make a correction.", "In my case, `an image size` and my `model's input shape` were different. Please check your image size again and again. Cuz, I've spent pretty long time on fixing this problem"]}, {"number": 2047, "title": "Not found: Could not find session factory for DIRECT_SESSION", "body": "Operating System:  Android\n\nbecause my android system is below Android 5.0, so I rebuild the tensorflow librarys for android with NDK android-11.  the building is success.   \n\nthen,  with these librarys, I write a android demo.  But when I run, it output the message ...\n\n```\n  **Not found: Could not find session factory for DIRECT_SESSION**\n```\n\nI don't know, what's wrong with my works.\n", "comments": ["There are problems with Bazel and NDK 11 currently; it's suggested that you use NDK r10e until this is resolved. See #1468 for details and links to relevant r10e downloads.\n\nIf the problem persists after trying r10e please let me know the bazel version and command you're using to build.\n", "@andrewharp \n\nthanks for your replay.\n\n\"Not found: Could not find session factory for DIRECT_SESSION\"\n\nmy ndk is always the NDK r10e, and i rebuild it,  the build is ok, but the problem still persists.\n\n> libandroid_tensorflow_lib.a\n> libprotobuf.a\n> libprotobuf_lite.a\n> libprotos_all_cc.a\n> libre2.a\n\nI extract these static librarys, and write my jni code, just like 'android:tensorflow_demo'.\n\n```\ntensorflow::SessionOptions options;\ntensorflow::ConfigProto& config = options.config;\nLOG(INFO) << \"Got config, \" << config.device_count_size() << \" devices\";\ntensorflow::Session* tempSession = tensorflow::NewSession(options);\n```\n\nhere, config.device_count_size() is 0,  and tempSession is nullptr, internal it print\n\n```\n    \"Not found: Could not find session factory for DIRECT_SESSION\"\n```\n\nthe device count is importance, but it's zero.\n\n---\n\nandroid-ndk-r10e-linux-x86_64.bin\nbazel-0.2.1-installer-linux-x86_64.sh\n\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow -b r0.7\n\nandroid_sdk_repository(\n    name = \"androidsdk\",\n    api_level = 23,\n    build_tools_version = \"23.0.1\",\n    path = \"/home/gzzhang/android-sdk\",\n)\n\nandroid_ndk_repository(\n    name=\"androidndk\",\n    path=\"/opt/android-ndk\",\n    api_level=11,                                        <<==  this is diff, i want to down the api_level.\n)\n\nbazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures\n\nTarget //tensorflow/examples/android:tensorflow_demo up-to-date:\n  bazel-bin/tensorflow/examples/android/tensorflow_demo_deploy.jar\n  bazel-bin/tensorflow/examples/android/tensorflow_demo_unsigned.apk\n  bazel-bin/tensorflow/examples/android/tensorflow_demo.apk\nINFO: Elapsed time: 2363.175s, Critical Path: 2126.57s\n", "I just tried with api_level=11, but was unable to reproduce the issue.\n\nIf you update tensorflow/examples/android:libtensorflow_demo.so to have the following linkopts:\n\n```\n    linkopts = [\n        \"-landroid\",\n        \"-ljnigraphics\",\n        \"-llog\",\n        \"-lm\",\n        \"-z defs\",\n    ],\n```\n\nAnd then run:\n\n```\nbazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures\nunzip bazel-bin/tensorflow/examples/android/tensorflow_demo.apk -d apk\nnm --radix=d --print-size -C --size-sort apk/lib/armeabi-v7a/libtensorflow_demo.so  | grep DirectSession\n```\n\nDo you see anything?\n\nAs a potential workaround, you can try adding alwayslink=1 to tensorflow/core:android_tensorflow_lib and see if that does anything.\n\nHow are you installing it on your device?\n", "@andrewharp \n\n\"Not found: Could not find session factory for DIRECT_SESSION\"\n\nIt seems that the DirectSessionFactory is not been regist.\n\nSo I browse the codes, and in the end of direct_session.cc, i find:\n\n//////////\nclass DirectSessionFactory : public SessionFactory {\n public:\n  DirectSessionFactory() {}\n\n  Session\\* NewSession(const SessionOptions& options) override {\n    std::vector<Device*> devices;\n    DeviceFactory::AddDevices(options, \"/job:localhost/replica:0/task:0\",\n                              &devices);\n    return new DirectSession(options, new DeviceMgr(devices));\n  }\n};\n\nclass DirectSessionRegistrar {\n public:\n  DirectSessionRegistrar() {\n    SessionFactory::Register(\"DIRECT_SESSION\", new DirectSessionFactory());\n  }\n};\nstatic DirectSessionRegistrar registrar;\n//////////\n\ntoday, i rebuild it with ndk api_level=9 and api_level=21\n\nlibandroid_tensorflow_lib.a\ndirect_session.o\n\nnm libandroid_tensorflow_lib.a | grep DirectSessionFactory    OK\nnm libandroid_tensorflow_lib.a | grep DirectSessionRegistrar  Found Nothing\n\ndirect_session.o is same.  api_level=9 and api_level=21 is same.\n\nso, you know, DIRECT_SESSION is not registed.\n\nand then, \"Not found: Could not find session factory for DIRECT_SESSION\"\n\nthe code i clone is branch r0.7, and the build is successfully.\n\n# \n\nIt really occur.  very very strange.\n", "@andrewharp\n\nIt my fault, now it's ok.\n\nI develop under Windows, and I use Android.mk to build.\n\nI must use LOCAL_WHOLE_STATIC_LIBRARIES instead of LOCAL_STATIC_LIBRARIES to link these libraries. And it also depend on android-ndk-r10e, just like bazel build.\n\nLOCAL_WHOLE_STATIC_LIBRARIES := \\\n                                android_tensorflow_lib \\\n                                protobuf re2 \\\n                                protobuf_lite \\\n                                protos_all_cc\n\nNow I can develop APP under Windows with Eclipse and Android.mk.\n\nthanks.\n"]}, {"number": 2046, "title": "Segment fault after upgrade the protobuf for >64MB limit", "body": "### Environment info\n\nOperating System:  CentOS Linux release 7.2.1511 (Core)\n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r--. 1 root root   322936 4\u6708   5 01:42 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx. 1 root root       16 4\u6708   5 01:42 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx. 1 root root       19 4\u6708   5 01:42 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x. 1 root root   383336 4\u6708   5 01:42 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r--. 1 root root   720192 4\u6708   5 01:42 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x. 1 root root 61453024 4\u6708  19 05:36 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x. 1 root root 61453024 4\u6708  19 05:36 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x. 1 root root 61453024 4\u6708  19 05:36 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rwxr-xr-x. 1 root root 11172416 4\u6708  12 01:12 /usr/local/cuda/lib64/libcudnn.so.6.5\n-rwxr-xr-x. 1 root root 11172416 4\u6708  12 01:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48\n-rwxr-xr-x. 1 root root 48217000 4\u6708  19 05:36 /usr/local/cuda/lib64/libcudnn.so.7.0\n-rwxr-xr-x. 1 root root 48217000 4\u6708  19 05:36 /usr/local/cuda/lib64/libcudnn.so.7.0.64\n-rw-r--r--. 1 root root 62025862 4\u6708  19 05:36 /usr/local/cuda/lib64/libcudnn_static.a\n\n```\n### Steps to reproduce\n1. Install the tensorflow 0.8.0rc0\n   pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl\n2. Upgrade the protobuf\n   pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl\n3. \"import tensorflow\" will have segment fault\n   No segment fault if not upgrade the protobuf.\n\nThe core info is:\n`(gdb) bt\n#0  0x00007f9606e2a2f1 in std::__detail::_Map_base<google::protobuf::Descriptor const*, std::pair<google::protobuf::Descriptor const* const, google::protobuf::DynamicMessage::TypeInfo const*>, std::allocator<std::pair<google::protobuf::Descriptor const* const, google::protobuf::DynamicMessage::TypeInfo const*> >, std::__detail::_Select1st, std::equal_to<google::protobuf::Descriptor const*>, google::protobuf::hash<google::protobuf::Descriptor const*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](google::protobuf::Descriptor const* const&) () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00007f9606e2a3d3 in google::protobuf::DynamicMessageFactory::GetPrototypeNoLock(google::protobuf::Descriptor const*) ()\n\n   from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007f9606e2b02a in google::protobuf::DynamicMessageFactory::GetPrototype(google::protobuf::Descriptor const*) () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007f95ee4f5129 in google::protobuf::python::cmessage::New (cls=<optimized out>, unused_args=<optimized out>, unused_kwargs=<optimized out>) at google/protobuf/pyext/message.cc:1255\n#4  0x00007f9618131d23 in type_call () from /lib64/libpython2.7.so.1.0\n#5  0x00007f96180dc0b3 in PyObject_Call () from /lib64/libpython2.7.so.1.0\n#6  0x00007f961817025c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\n#7  0x00007f96181740bd in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\n#8  0x00007f96181741c2 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\n#9  0x00007f9618183fac in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0\n#10 0x00007f9618184228 in load_source_module () from /lib64/libpython2.7.so.1.0`\n\nI also reported the problem here: [tensorflow_serving issue](https://github.com/tensorflow/serving/issues/24)\n", "comments": ["There is a note on the installation documentation page which says: \n\nNOTE: If you are upgrading from a previous installation of TensorFlow < 0.7.1, you should uninstall the previous TensorFlow and protobuf using pip uninstall first to make sure you get a clean installation of the updated protobuf dependency.\n(https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#pip-installation)\n\nWhen I followed these instructions, I end up with protobuf-3.0.0b2 installed...\n\n(tensorflow.virtualenv)pbar@pbar:/tmp/addone$ pip list | grep proto\nprotobuf (3.0.0b2)\n\nIs there some reason you needed to do this manually?\n\nCould you please try this again with the final release version r0.8, (removing the release candidate first) and let me know if it fixes your problem?\n\nThanks,\nPaul\n", "@prb12 I need tensorflow with >64MB protobuf support. The support is discribed here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues \nI tried to uninstall the tensorflow and protobuf, and then install tensorflow and upgrade protobuf:\n\n```\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl\n```\n\nAfter this, run `python -c \"import tensorflow\"` would have segment fault.\n", "Hi,\nI did the same mistake once. I think that If you installed tensorflow using pip only (first part of installation guide), you need to install the fast protobuf 3.0.0b2 manually. But, if you installed from sources, you do not need this step. If you did it by fault, you will end up with this stupid segmentation fault error when importing TF.\n", "@amrmousa I tried to reinstall protobuf 3.0.0b2 with modified _kDefaultTotalBytesLimit_ in `coded_stream.h` manually, while it does not have segment fault, but it is still has 64MB limit error.\nMy solution is change the `coded_stream.h` from tensorflow source code, and use bazel to build the pip_package, then use pip to install tensorflow from the generated .whl file.\n", "@doubler I am interested to know if you have this problem while installing tensorflow from source or just installing it using pip only? In my installation, I can see that protobuf 3.0.0b2 is installed but I did not test this 64MB limit issue.\n", "@amrmousa I solve this problem by installing tensorflow from source. And it does not have the 64MB limit. While I don't know whether it is the fast protobuf version?\n", "@doubler I think this is the fast one, because the documentation page states that:\n\n\"TensorFlow pip package depends on protobuf pip package version 3.0.0b2. Protobuf's pip package downloaded from PyPI (when running pip install protobuf) is a Python only library, that has Python implementations of proto serialization/deserialization which can be 10x-50x slower than the C++ implementation. Protobuf also supports a binary extension for the Python package that contains fast C++ based proto parsing. This extension is not available in the standard Python only PIP package. We have created a custom binary pip package for protobuf that contains the binary extension.\"\n\nAnd later it says:\n\n\"Note that the binary pip package already has support for protobuf larger than 64MB, that should fix errors such as these\"\n\nSo, I understand that the fast c++ one is the only one that does not have the 64MB limit.\n", "@amrmousa I think the document say \"Note that the binary pip package\" the pip package means the `protobuf-3.0.0b2.post2-cp27-none-linux_x86_64.whl` not the tensorflow package.\n", "I too am getting this error. Building from source, I could not get a cpp version to work. If I had the environment variable `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp`, then running `from google.protobuf.pyext import _message` would yield an error. This is even if I used `bazel build --define=use_fast_cpp_protos=true` as suggested in  #310 (Maybe that deserves its own issue?). \n\nWhat eventually worked was:\n-build tensorflow from source\n-uninstall protobuf\n-go to `tensorflow/google/protobuf`\n-install the fast protobuf version as explained is the readme\n\nHope this helps!\n", "@VDalibard Seems still not work:-(\nWhen I install the tensorflow(build from source), the pip will try to download the protobuf.\n\n```\npip install --no-cache-dir --upgrade /export/liujia/tensorflow_pkg/tensorflow-0.8.0-py2-none-any.whl\nProcessing /export/liujia/tensorflow_pkg/tensorflow-0.8.0-py2-none-any.whl\nRequirement already up-to-date: six>=1.10.0 in /usr/lib/python2.7/site-packages (from tensorflow==0.8.0)\nCollecting protobuf==3.0.0b2 (from tensorflow==0.8.0)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 36.5MB/s\nRequirement already up-to-date: wheel in /usr/lib/python2.7/site-packages (from tensorflow==0.8.0)\nRequirement already up-to-date: numpy>=1.8.2 in /usr/lib64/python2.7/site-packages (from tensorflow==0.8.0)\nRequirement already up-to-date: setuptools in /usr/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.8.0)\nInstalling collected packages: protobuf, tensorflow\n  Found existing installation: tensorflow 0.8.0\n    Uninstalling tensorflow-0.8.0:\n      Successfully uninstalled tensorflow-0.8.0\nSuccessfully installed protobuf-3.0.0b2 tensorflow-0.8.0\n```\n\nI build tensorflow with the following commands:\n\n```\nbazel build -c opt --config=cuda --define=use_fast_cpp_protos=true //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg/\npip install --no-cache-dir --upgrade /tmp/tensorflow_pkg/tensorflow-0.8.0-py2-none-any.whl\n```\n", "You're on the right track, what you need to do then is:\n\npip uninstall protobuf\ncd google/protobuf\n./autogen.sh\n./configure\nmake\nmake check\nsudo make install\n\nThen update your `LD_LIBRARY_PATH` as it says in `google/protobuf/src/README.md`\nThen \n\ncd python\npython setup.py build --cpp_implementation\npython setup.py test --cpp_implementation\nsudo python setup.py install --cpp_implementation\n\nAnd you should have tensorfflow working with the fast version of protobuf. If you're missing packages, look at the `README`s. A couple notes:\n-If you want >64MB protobuf, before you do any of this, and before you build tensorflow, you have to update the protobuf file `coded_stream.h` as described in #582.\n-When you exit python after having loaded tensorflow, it segfaults, but I think everything before works fine.\n", "I should say, after doing some runs I haven't observed better performance by using `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp` so it is probably the case that just by installing from source you get the fast protobuf version and that there is a bug that prevents loading tensorflow with `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp`. (namely, loading module `_message` in `google.protobuf.pyext`)\n", "I got the same problem!\n\nI installed tensorflow in a clear conda environment following: https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#anaconda-environment-installation\n\nThen: \n\n```\n$ pip list | grep protobuf\n protobuf (3.0.0b2)\n```\n\nNow, `python -c \"import tensorflow\"` works fine.  \n\nThen I intsalled the fast C++ based  `protobuf-3.0.0b2.post2` following: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#protobuf-library-related-issues\n\nThen: \n\n```\n$ pip list | grep protobuf\nprotobuf (3.0.0b2.post2)\n```\n\nHowever, `python -c \"import tensorflow\"` would have Segmentation fault (core dumped).\n\nSo, how should I install fast c++ protobuf when installing tensorflow using pip only?\n", "@VDalibard Shake hands. I meet those same problems as all you said. And didn't get better performance by using faster protobuf version neither. \n\n@cegeme you can install the fast protobuf manually. \nDownload [protobuf](https://github.com/google/protobuf/releases/download/v3.0.0-beta-2/protobuf-python-3.0.0-beta-2.tar.gz). \nunzip and install like the protobuf-3.0.0-beta-2/README.md and protobuf-3.0.0-beta-2/python/README.md instruction.\nAnd you can refer to @VDalibard said.\n", "Seeing similar issues:\n- If I install from the wheels, I get a segfault on import\n- If I build from source, I get a segfault on teardown (dtors for globals), but it seems to run fine until then\n- Using the pure python protobuf lib (either by not installing the cpp impl or setting the env var) eliminates the issue\n\nBoth segfaults seem to involve hash maps in protobuf, though that may be a red herring---could be memory corruption caused by something else.\n\nPoking around in gdb but if anyone has ideas on how to debug let me know.\n", "@VDalibard your symptoms sound similar to mine:\n\n> -When you exit python after having loaded tensorflow, it segfaults, but I think everything before works fine.\n\nI got my build to work now, by building a `-std=c++11` version of protobuf.  See #2646.  I think this may also apply to everyone else in this issue, but I don't know for sure.\n", "@martinwicke: Is there any way we can shield people against these torments, or at least generate nicer errors? \n", "@aselle is there a way to find a mismatch at runtime? Maybe by comparing the size of an unordered_map returned from a function in each version of the library? If we can identify the problem we could at least die with informative last words.\n", "@lichan, do you think stricter symbol visibility solves this problem as well (similar to #2646)?\n", "It's hard for me to say for sure, but the tighter symbol visibility in 0.9 should eliminate an entire class of issues related to these mismatches.  I _think_ it should solve it, but we'd need the OP to confirm.\n", "Closing this issue as it seems to be resolved. Please reopen or create new issue if I'm wrong.\n", "@doubler If I changed 256<<20 in coded_stream.h in protobuf source and reinstall protobuf but not reinstalled tensorflow, can I fix the 64MB limited problem? I only reinstalled protobuf seems not work..\n"]}, {"number": 2045, "title": "TypeError('{!r} is not a Python function'.format(func))", "body": "When I import tensorflow, It raise an error like this(the newest version of tensorflow via pip installtaion)\n\n`\n/usr/bin/python2.7 /home/dell/wxm/Code/ImageCaption-tensorflow/test.py\nTraceback (most recent call last):\n  File \"/home/dell/wxm/Code/ImageCaption-tensorflow/test.py\", line 3, in <module>\n    import tensorflow as tf\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 62, in <module>\n    import tensorflow.contrib as contrib\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/**init**.py\", line 26, in <module>\n    from tensorflow.contrib import learn\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/**init**.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/**init**.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/**init**.py\", line 22, in <module>\n    from tensorflow.contrib.learn.python.learn.io import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/**init**.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn.io.dask_io import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py\", line 23, in <module>\n    import dask.dataframe as dd\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/**init**.py\", line 1, in <module>\n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1234, in <module>\n    class Index(Series):\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1266, in Index\n    @derived_from(pd.Index)\n  File \"/usr/local/lib/python2.7/dist-packages/dask/utils.py\", line 526, in wrapper\n    original_args = getargspec(original_method).args\n  File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 190, in getargspec\n    return _getargspec(func)\n  File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 56, in _getargspec\n    return inspect.getargspec(func)\n  File \"/usr/lib/python2.7/inspect.py\", line 816, in getargspec\n    raise TypeError('{!r} is not a Python function'.format(func))\nTypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function\n\nProcess finished with exit code 1\n`\n", "comments": ["I have similar problem:\n\n```\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 62, in <module>\n    import tensorflow.contrib as contrib\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 26, in <module>\n    from tensorflow.contrib import learn\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/__init__.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/__init__.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 22, in <module>\n    from tensorflow.contrib.learn.python.learn.io import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/__init__.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn.io.dask_io import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py\", line 23, in <module>\n    import dask.dataframe as dd\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/__init__.py\", line 1, in <module>\n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1233, in <module>\n    class Index(Series):\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1250, in Index\n    @derived_from(pd.Index)\n  File \"/usr/local/lib/python2.7/dist-packages/dask/utils.py\", line 526, in wrapper\n    original_args = getargspec(original_method).args\n  File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 181, in getargspec\n    return _getargspec(func)\n  File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 55, in _getargspec\n    return inspect.getargspec(func)\n  File \"/usr/lib/python2.7/inspect.py\", line 816, in getargspec\n    raise TypeError('{!r} is not a Python function'.format(func))\nTypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function\n```\n", "I am also trying to install tensorflow via pip and am getting this same error:\nTo be clear, this is a problem for me with both the GPU and CPU versions.\n\n```\n$ python\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 62, in <module>\n    import tensorflow.contrib as contrib\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py\", line 26, in <module>\n    from tensorflow.contrib import learn\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/__init__.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/__init__.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 22, in <module>\n    from tensorflow.contrib.learn.python.learn.io import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/__init__.py\", line 20, in <module>\n    from tensorflow.contrib.learn.python.learn.io.dask_io import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py\", line 23, in <module>\n    import dask.dataframe as dd\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/__init__.py\", line 1, in <module>\n    from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1185, in <module>\n    class Index(Series):\n  File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1202, in Index\n    @derived_from(pd.Index)\n  File \"/usr/local/lib/python2.7/dist-packages/dask/utils.py\", line 526, in wrapper\n    original_args = getargspec(original_method).args\n  File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 181, in getargspec\n    return _getargspec(func)\n  File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 55, in _getargspec\n    return inspect.getargspec(func)\n  File \"/usr/lib/python2.7/inspect.py\", line 816, in getargspec\n    raise TypeError('{!r} is not a Python function'.format(func))\nTypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function\n\n```\n", "I've tried installing previous r.08 builds, all returned the same errors.\nHowever, r.07 builds work fine. Anyone have any fixes for this yet? (because I need some r.08 functionality.\n", "what is the r.07's whl path?\n", "I installed #45 from build history, it is 0.7.1 version. This version is fine, I also test #60, #62 and #63, these are also 0.7.1, but all of them raise similar error. You can download it from here-->http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/45/\n", "I have the same problem. \n- I have installed tensorflow by pip and anaconda and also installed the caffe. \n  I got a problem by anaconda so I reinstalled the tensorflow 8.0 by pip \n\nAfter that, I have the same problem \n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 62, in <module>\n> > >     import tensorflow.contrib as contrib\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/**init**.py\", line 26, in <module>\n> > >     from tensorflow.contrib import learn\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/**init**.py\", line 20, in <module>\n> > >     from tensorflow.contrib.learn.python.learn import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/**init**.py\", line 20, in <module>\n> > >     from tensorflow.contrib.learn.python.learn import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/**init**.py\", line 22, in <module>\n> > >     from tensorflow.contrib.learn.python.learn.io import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/**init**.py\", line 20, in <module>\n> > >     from tensorflow.contrib.learn.python.learn.io.dask_io import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/io/dask_io.py\", line 23, in <module>\n> > >     import dask.dataframe as dd\n> > >   File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/**init**.py\", line 1, in <module>\n> > >     from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n> > >   File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1234, in <module>\n> > >     class Index(Series):\n> > >   File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1266, in Index\n> > >     @derived_from(pd.Index)\n> > >   File \"/usr/local/lib/python2.7/dist-packages/dask/utils.py\", line 526, in wrapper\n> > >     original_args = getargspec(original_method).args\n> > >   File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 190, in getargspec\n> > >     return _getargspec(func)\n> > >   File \"/usr/local/lib/python2.7/dist-packages/dask/compatibility.py\", line 56, in _getargspec\n> > >     return inspect.getargspec(func)\n> > >   File \"/usr/lib/python2.7/inspect.py\", line 816, in getargspec\n> > >     raise TypeError('{!r} is not a Python function'.format(func))\n> > > TypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function\n", "ping @ilblackdragon @terrytangyuan \n", "I wasn't able to reproduce the issue under either Linux or Mac. \nTry installing from a clean virtualenv. Also try removing your dask install. \n", "After the PIP install, when you import tensorflow, this error just appears !\n\n`TypeError: <method 'max' of 'numpy.ndarray' objects> is not a Python function`\n", "Seems like you have dask installed but this `import dask.dataframe as dd` failed. Could you try uninstalling your dask and try it again? I'll contact dask people too.\n", "Taking a look now.  It sounds like regardless tensorflow should be robust to misbehavior in dask.  Perhaps you should keep imports to external and optional dependencies under try-except blocks.\n", "What are the relevant versions of Pandas on the failing machines?\n", "And what are relevant versions of numpy\n", "I can reproduce with fairly old versions of numpy/pandas.\n", "I think it should be able to avoid this easily with a new virtualenv. \n", "Dask requirements list `pandas >= 0.16.0`.  \n\nWhen using `pip` or a `requirements.txt` file is it possible that you're just installing `dask` rather than `dask[dataframe]` or `dask[complete]`?  Depending on `dask[dataframe]` should properly enforce installation of newer NumPy/Pandas libraries.\n", "Per @jreback, if you are using dask you should have a relatively recent version of pandas, say >= 0.17.0 (even better is 0.18.0); certain older versions will work, but may cause unexpected things. \n\n@meanmee @liuyifly06 @byungjo @ruilog @jessebett \nUninstall dask if you don't intend to use it. If you need to use it, use newer versions of pandas and dask. \n", "@vrv This can be closed now.\n", "I have neither dask nor pandas installed on my ubuntu machine. I still see this error for tensorflow 0.8 as soon as I import tensorflow. Although tensorflow 0.7.1 works fine for me.\n", "Looking at one of the above stack traces\n\n> File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1234, in class Index(Series):\n> File \"/usr/local/lib/python2.7/dist-packages/dask/dataframe/core.py\", line 1266, in Index @derived_from(pd.Index)\n> File \"/usr/local/lib/python2.7/dist-packages/dask/utils.py\", line 526, in wrapper original_args = getargspec(original_method).args\n\nWe see a reference to `pandas.Index` and then we see the stack trace continue on in a way that wasn't \"Error, pandas not found\" so clearly, at least on the machine for which this stack trace was produced, Pandas was installed.  \n\nIf you have a different error then you should probably post more information about how it was produced and the software environment necessary to reproduce it.\n", "Probably a different error traceback though. Could you post it here?\n", "@terrytangyuan how are you directing people to install dask?  If you are using a requirements.txt file then the correct entry is probably `dask[dataframe]` rather than just dask.  \n", "Haven't mentioned anything about installing dask yet. Just put them in try catch. It's not required but optional.  @mrocklin \n", "Ah, are they `pip installing dask` on their own then?  (I'm getting these error reports myself now) \n", "Yep on their own. We can probably add a note there.\nOn Apr 28, 2016 3:03 PM, \"Matthew Rocklin\" notifications@github.com wrote:\n\n> Ah, are they pip installing dask on their own then? (I'm getting these\n> error reports myself now)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2045#issuecomment-215545247\n", "It works for me. Thanks!\n"]}, {"number": 2044, "title": "inception", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": []}, {"number": 2043, "title": "Remove duplicate words", "body": "This PR simply removes duplicate words.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Merged. Thanks.\n"]}, {"number": 2042, "title": "R0.7", "body": "please i want download tensorflow data\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(assuming this is an accident).\n"]}, {"number": 2041, "title": "power activation function", "body": "Hi, I am a newbie with Tensorflow. I just found there is no power activation function existing in Tensorflow? How can I implement a new activation function in TF?\n", "comments": ["https://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html#pow ?  Comment if you think this isn't sufficient\n", "Ignore me. I have found the right solution. Tensor flow is fantastic. Thank you!\n\n> On Apr 22, 2016, at 2:19 PM, Vijay Vasudevan notifications@github.com wrote:\n> \n> https://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html#pow https://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html#pow ? Comment if you think this isn't sufficient\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub https://github.com/tensorflow/tensorflow/issues/2041#issuecomment-213539266\n"]}, {"number": 2040, "title": "building pip package issue", "body": "```\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n2016\u5e74 04\u6708 21\u65e5 \u661f\u671f\u56db 07:34:12 CST : === Using tmpdir: /tmp/tmp.cMsLMGw3bz\ncp: cannot stat \u2018bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/tensorflow\u2019: No such file or directory\ncp: cannot stat \u2018bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/external\u2019: No such file or directory\n```\n", "comments": ["Did you following the steps listed here? https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#create-the-pip-package-and-install\n", "I ran into this problem today using the master branch of bazel, but after rebuilding bazel from the 0.2.0 tag everything went smoothly in the tensorflow build.\n", "We are unable to reproduce this -- can you send us instructions about how to reproduce this from a fresh build / source tree?  We'll reopen if this is still a problem.\n", "It maybe caused by directory: the master branch of bazel make a `__main__` folder under the `bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles`, thus many files cannot be found. \n\n**temporary solution:** you can recursively copy all the file generated out of `__main__` folder, then run `bazel-bin/...`\n", "Hi! I have this issue to.\n\nI am using Ubuntu 16.04, with Python 3.5.1, TensorFlow master (currently at  4a4f2461533847dde239851ecebe5056088a828c) and Bazel master (`Build label: 0.2.2b-2016-05-09 (@7222943)`).\n\nHere are my steps to reproduce:\n1. Install gcc-4.9 and g++-4.9\n2. export CXX=/usr/bin/g++-4.9 and export CC=/usr/bin/gcc-4.9\n3. Change to bazel directory, run ./compile\n4. Copy  bazel/output/bazel to ~/.bazel/bazel, symlink ~/bin/bazel -> ~/.bazel/bazel\n5. Add `cxx_flag: \"-D_FORCE_INLINES\"` to tensorflow/third_party/gpus/crosstool/CROSSTOOL at line 52\n6. Run `bazel build -c opt --config=cuda --genrule_strategy=standalone --local_resources 4096,4.0,1.0 -j 4 //tensorflow/cc:tutorials_example_trainer`\n7. Run `bazel build -c opt --config=cuda --genrule_strategy=standalone --local_resources 4096,4.0,1.0 -j 4 //tensorflow/tools/pip_package:build_pip_package`\n8. Observe that the `bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/` directory contains `tensorflow` and `external` (and some other stuff) that should probably be in `bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/`\n\nRunning\n\n```\ncp -r bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/* bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/\n```\n\nworks around this issue. (Thanks to @ZhuFengdaaa for this!)\n\n@vrv Please reopen the issue and don't hesitate to ask for more information :)\n", "Actually I use `cp -r * ../`, the same.\n", "Same issue, compiling in CentOS 6.7 with the latest master branch of both tensorflow and bazel, python 2.7.6, and using the devtoolset-2 install of gcc/g++ 4.8.2.\n\nOtherwise my steps are identical to @akors, who has them two comments above mine (though I am not using the gpu build).  Same fix solves the issue.\n\nEdit: I also had to use the fix here:\n\nhttps://github.com/tensorflow/tensorflow/issues/121\n\nand add \"-lrt\" in order to get python to use the correct timing library since CentOS 6.\\* has an older glibc.\n\n@vrv I am also happy to provide more info.  My guess it's also due to bazel.\n", "@damienmg @kchodorow will you help us move towards the new runfiles location before you decide to release the next version of bazel officially?\n", "Happy to work with you on this.  TensorFlow probably needs to:\n- Define a workspace name (`workspace(name = \"org_tensorflow\")` in the WORKSPACE file).\n- Fix any scripts you have that reference runfiles to look for them under `x.runfiles/org_tensorflow` (or whatever you choose for a name), instead of `x.runfiles`. \n\nI'll take a look and see if I can reproduce/fix it myself.\n", "Oh, yeah:\n\n```\n$ grep -r '\\.runfiles' *\ntensorflow/tools/pip_package/build_pip_package.sh:    bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/{tensorflow,external} \\\ntensorflow/tools/pip_package/build_pip_package.sh:    bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/google \\\ntensorflow/tools/pip_package/build_pip_package.sh:    bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/third_party/eigen3 \\\ntensorflow/stream_executor/dso_loader.cc:  rpaths->push_back(\"driver/driver_sh.runfiles/third_party/gpus/cuda/lib\");\ntensorflow/stream_executor/dso_loader.cc:  rpaths->push_back(\"driver/driver_sh.runfiles/third_party/gpus/cuda/lib64\");\ntensorflow/g3doc/get_started/os_setup.md:ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/* .\n```\n\nAll of these just need to be changed to .runfiles/_some_name_.\n", "I'm also having this issue. I'm on the current master branch of tensorflow (7d9ab3e), bazel 0.2.3, and gcc 5.3.1 on Ubuntu 16.04. My build steps were very similar to @akors. The fix mentioned above works. I'd be happy to provide more specific information if that's helpful.\n", "Just did a fresh install on new ssd: Ubuntu 15.10, bazel 0.2.3.1463494873, tensorflow [60f6812](https://github.com/tensorflow/tensorflow/commit/60f68124494ef2aaff371e13e5f8b267b4043e6e), CUDA 7.5, cudnn v5, gcc 4.9. Same thing happens, same workaround works.\n", "/cc @kchodorow\n\nWhat is the exact workaround you are applying?\n\nThe build_pip_package shell script seems incorrect because it tries to copy external\n", "@damienmg \nI ran \n\n```\ncp -r bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/* bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/\n```\n\nto get it working.\n", "Oh yes makes sense. The corresponding PR is not yet merged. I guess @vrv will merge it soon, I think this bug should be reopen until then.\n", "Had a related (I presume) issue with current tensorflow@d42facc3 and bazel@c728a631, just adding this here for reference:\n\n```\n~/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nSat Jun 4 07:22:15 UTC 2016 : === Using tmpdir: /tmp/tmp.wBMcP4UriY\nrsync: change_dir \"/home/ubuntu/tensorflow//bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/external\" failed: No such file or directory (2)\nrsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]\n```\n\nFixed by symlinking the path referenced in the error to `~/tensorflow/bazel-bin/external`.\n", "ping @kchodorow: @kadrach issue happens with latest Bazel. Maybe we should extends the coverage of what we test of Tensorflow on ci.bazel.io\n", "I'm using bazel 0.2.3 as well, Ubuntu 16.04 + GCC 5.3.1, Cuda 7.5 + cudnn 5.0.5 .\nI strictly followed https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html#installation-for-linux, but run into the same error \n\n> $ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n> Wed Jun 8 17:48:44 PDT 2016 : === Using tmpdir: /tmp/tmp.67PiK9GeAe\n> rsync: change_dir \"/home/jiapei/Downloads/machinelearning/deeplearning/tensorflow//bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/external\" failed: No such file or directory (2)\n> rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1183) [sender=3.1.1]\n\nI noticed that I do **NOT** have the subfolder **external**, but only a folder **tensorflow**. In addition, I do **NOT** have a folder ****main**** .\n\n> .../pip_package/build_pip_package.runfiles$ ls \n> d3                             iron_collapse               iron_range_behavior        paper_dropdown_menu  paper_styles\n> dagre                          iron_dropdown               iron_resizable_behavior    paper_header_panel   paper_tabs\n> eigen_archive                  iron_fit_behavior           iron_selector              paper_icon_button    paper_toggle_button\n> es6_promise                    iron_flex_layout            iron_validatable_behavior  paper_input          paper_toolbar\n> font_roboto                    iron_form_element_behavior  lodash                     paper_item           plottable\n> graphlib                       iron_icon                   MANIFEST                   paper_material       polymer\n> **init**.py                    iron_icons                  neon_animation             paper_menu           promise_polyfill\n> iron_a11y_announcer            iron_iconset_svg            org_tensorflow             paper_menu_button    protobuf\n> iron_a11y_keys_behavior        iron_input                  paper_behaviors            paper_progress       six_archive\n> iron_ajax                      iron_list                   paper_button               paper_radio_button   web_animations_js\n> iron_autogrow_textarea         iron_menu_behavior          paper_checkbox             paper_radio_group    webcomponentsjs\n> iron_behaviors                 iron_meta                   paper_dialog               paper_ripple\n> iron_checked_element_behavior  iron_overlay_behavior       paper_dialog_behavior      paper_slider\n\nCan anybody help please?\n\nCheers\nPei\n", "I'm using Bazel 0.3 and facing this problem\n", "I am using Bazel 0.4.0, python 2.7 and Mac 10.11.6. I have the same issue with running `bazel build -c opt  --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package`\n\n> ERROR: no such package 'tensorflow/tools/pip_package': BUILD file not found on package path.\n", "i had this problem when trying to run the command: \r\n     ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\noutside of the tensorflow installation dir when back there it did work without any problem"]}, {"number": 2039, "title": "latest-gpu Docker image has broken cudnn", "body": "Running import tensorflow:\n\n> > > import tensorflow\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n> > > I tensorflow/stream_executor/cuda/cuda_dnn.cc:1562] Unable to load cuDNN DSO\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n\nNote that this seems to work with the latest-devel-gpu image. As far as I can tell the problem is caused by a missing symlink from /usr/lib/x86_64-linux-gnu/libcudnn.so to /usr/lib/x86_64-linux-gnu/libcudnn.so.4\n", "comments": ["See https://github.com/tensorflow/tensorflow/issues/808#issuecomment-187931361\nAnd: https://github.com/tensorflow/tensorflow/issues/970#issuecomment-202999836\n", "@jendap: Do you know if this is still an issue? \n", "@girving In case it's helpful, this is still an issue as of this week when using gcr.io/tensorflow/tensorflow:latest-gpu. As klopyrev mentioned, ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.4 /usr/lib/x86_64-linux-gnu/libcudnn.so , fixes the issue.\n", "@chan1 Thanks.  Switching assignment to @caisq who knows more about the docker setup.\n", "@klopyrev does the workaround help you, or do you still have an issue?\n", "Well, it's still an issue. I'm not really using Tensorflow at the moment.\n", "@alextp it's actually a bit more complicated, the image `gcr.io/tensorflow/tensorflow:latest-gpu` doesn't work because it's still version `0.8` (very old!). You should use the images on the DockerHub to get `0.9`, `0.10rc` or even nightly builds. I reported this issue here: https://github.com/tensorflow/tensorflow/issues/3669\n\nBut recently, the `Dockerfile.gpu` was changed to inherit from the `devel` CUDA image instead of the `runtime` image, so the symlink workaround is not needed because the symlink already exists.\nSee https://github.com/tensorflow/tensorflow/pull/2527\nBut this is not ideal since it's pulling extra dependencies in the `gpu` Docker image.\n", "This seems to have been fixed in more recent docker images, sorry we didn't respond on this issue thread earlier.\n\nFor example:\n\n``` sh\nnvidia-docker run -it gcr.io/tensorflow/tensorflow:latest-gpu python -c \"import tensorflow; print tensorflow.__version__\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0rc0\n```\n\nSo I'm going to close this issue for now. Please feel free to reopen or file a new one if there is any contention on this being fixed :)\n"]}, {"number": 2038, "title": "Fix", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2037, "title": "NaN problem on tutorials_example_trainer with tensorflow 0.8", "body": "I worked for several months on versions of tensorflow installed from sources. Today, I tried to upgrade it to 0.8. The installation seemed to go fine.  \"bazel-bin/tensorflow/cc/tutorials_example_trainer\" gave the expected result. However, the output of \"bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\" is unexpected: the first lines seem to be fine but then NaN values appear, the proportion of NaN values increases until there is nothing but NaN values (see the first half of the output here http://pastebin.com/EhuD2Z5S , the remaining lines are only \"nan\" lines which I omitted because the total output exceeded pastebin limit).\n\nI expected this tutorials_example_trainer to either work seamlessly or not at all (if I provided wrong paths for CUDA and CuDNN), but this in-between puzzles me.\n\nI tried to debug it with my own programs, but I can not import tensorflow :\n\n```\nimport tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"tensorflow/python/__init__.py\", line 45, in <module>\n    from tensorflow.python import pywrap_tensorflow\nImportError: cannot import name pywrap_tensorflow\n```\n\nI am not sure whether the two problems are related.\n\nIn the case I made a mistake during configuration, I typed:\n\n```\nmatthieu@gpu2:~/external/tensorflow$ ./configure \nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.0\nPlease specify the location where CUDA 7.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 4\nPlease specify the location where cuDNN 4 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\"\n```\n\nI hope this issue belongs here, else I will try stackoverflow.\n\nThanks in advance,\n### Environment info\n\nI work on ubuntu 14.04\n\nWith CUDA 7.0 and cuDNN 4\n\n```\nls /usr/local/cuda-7.0/lib/libcud*        \n/usr/local/cuda-7.0/lib/libcudadevrt.a  /usr/local/cuda-7.0/lib/libcudart.so.7.0     /usr/local/cuda-7.0/lib/libcudart_static.a\n/usr/local/cuda-7.0/lib/libcudart.so    /usr/local/cuda-7.0/lib/libcudart.so.7.0.28\n```\n\nCommit hash: e1c017624fd6bd8562c1da27e0014b32f389b524\n", "comments": ["I don't believe that this is the expected behavior, but I have been unable to reproduce this on my installation (admittedly using a single K40 GPU).   Since the example should only be using one of the GPUs I can't think why this might be happening.\n\nCould you please provide me some more information about how you installed TensorFlow?\ne.g. you mention 'from sources' and upgrading.  Is this a clean tree, built from source, or one of the binary distros?\n", "PS.  Your error when trying to import tensorflow looks like the error message I once got when I tried to run an interactive python session in the root of a tensorflow source tree.  Is this possibly what's happening?\n", "Thanks for the answers:\n\n## Concerning my installation\n\nI typed the following\n\n```\n$ git pull origin\n$ ./configure  # with the parameters I detailed above\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n```\n\nAt this point, I tried $ bazel-bin/tensorflow/cc/tutorials_example_trainer, and I encountered the NaN problem. So I went back to the \"Download and Setup\" page to determine if I had missed a step\n\n```\n$ git submodule update --recursive\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer # changed nothing\n```\n\nI downloaded and installed the last version of bazel\n\n```\n$ wget https://github.com/bazelbuild/bazel/releases/download/0.2.1/bazel-0.2.1-installer-linux-x86_64.sh\n$ chmod +x bazel-0.2.1-installer-linux-x86_64.sh\n$ ./bazel-0.2.1-installer-linux-x86_64.sh  --user\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer # changed nothing\n```\n\nI upgraded all my packages -> changed nothing.\nThen I tried to google my problem, but nobody seems to have the same issue.\n\n## Concerning the interactive python session\n\nThis was indeed the issue, but when I run the python session from another directory, I have another import issue.\n\n```\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/matthieu/external/tensorflow/_python_build/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/matthieu/external/tensorflow/_python_build/tensorflow/python/__init__.py\", line 59, in <module>\n    from tensorflow.core.protobuf.config_pb2 import *\nImportError: No module named config_pb2\n```\n\nIndeed, The config_pb2 file is not in the directory. I will investigate this further.\n", "@ernest-tg How did you solve this problem? \nI face the same issue as describe here. Please give your solution if possible. Thank you.\n", "I did not solve the problem. I ended up reinstalling the 0.7.0 version. This week-end, I plan to find by dichotomy, the first commit which gives me this behaviour. Hopefully, it will help the tensorflow team to understand/solve this problem.\n", "Found the issue when I searched the same problem. I git pulled the latest source and got this same issue: tutorials_example_trainer output some NaNs. Here are the last few lines of the output. \n\n```\n000004/000005 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n000009/000005 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000004/000003 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000001/000007 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n000006/000009 lambda =      nan x = [     nan      nan] y = [     nan      nan]\n```\n\nI'm on Ubuntu 14.04 LTS 64 bit, gcc 4.8.4, CUDA 7.5. \n", "@ernest-tg: Does this problem persist, or did you manage to get any further information?\n", "So far, I have been unable to reproduce the NaN behavior building source from the current head, or using the binary distros.\n", "I can reproduce this on the current head (592675b2b8d1cabbf923638942ea6f200abe353a)\n\nEnvironment: Ubuntu 14.04.4, CUDA 7.0, cuDNN 4.0\n", "I have the exact same software versions here, but don't get the NaNs.  Perhaps this is a hardware related issue, e.g. placement of ops on multiple GPUs?  \n@zheng-xq Are there any known multi-gpu issues with TF0.9/Cuda7.0/cuDNN4  ?\n\n@weiliu620, @ethereon   Are you also running a multi-gpu config?  \n\n@ernest-tg  Could you please try running this with the environment variable CUDA_VISIBLE_DEVICES=0 \n\nDoes the same problem happen with newer CUDA SDKs and cuDNN?\n\nMight also be worth checking where ops are running as follows:\n\n```\ndiff --git a/tensorflow/cc/tutorials/example_trainer.cc b/tensorflow/cc/tutorials/example_trainer.cc\nindex a465d98..02b42ab 100644\n--- a/tensorflow/cc/tutorials/example_trainer.cc\n+++ b/tensorflow/cc/tutorials/example_trainer.cc\n@@ -93,6 +93,7 @@ string DebugString(const Tensor& x, const Tensor& y) {\n void ConcurrentSteps(const Options* opts, int session_index) {\n   // Creates a session.\n   SessionOptions options;\n+  options.config.set_log_device_placement(true);\n   std::unique_ptr<Session> session(NewSession(options));\n   GraphDef def = CreateGraphDef();\n   if (options.target.empty()) {\n```\n", "@prb12 I'm running a single GPU config (Titan X). This appears to be a bit of a heisenbug: it occurred on every single run last week, but everything seems fine today. The build hasn't changed - everything's untouched.\n", "> > Are there any known multi-gpu issues with TF0.9/Cuda7.0/cuDNN4 ?\n\nNot to my knowledge yet. \n", "Closing this bug, as we cannot reproduce it.\n", "I can reproduce it always on GPU, on CPU all works fine. If you need the code or ssh to check it - notify me.\n", "Ubuntu 16.04, nVidia driver 367.35, GTX 1080, CUDA 7.5\n", "@tuba GTX 1080 doesn't work correctly with CUDA 7.5\nYou need CUDA 8.0, but there's no stable release of CUDA 8.0 yet\nHowever, I tried the release candidate and it worked for me\n", "Same issue, Ubuntu 16.04, nVidia driver 367.35, GTX 1070, CUDA 8\n", "Make sure you have cuDNN 5.0 for the GTX10x0 NVIDIA cards.\n", "Yup I made sure, still didn't work. \n", "did you tried the latest version of tensorflow ? 0.10 ? does the problem still persist? \n", "I'm building it from source, so I should be on the latest version right?\n", "Can also reproduce, Ubuntu 16.04, nVidia driver 367.35, GTX 1080, CUDA 8.0, cuDNN 5.1, compiled tensorflow from source. \n\nOutput comes out as expected from CPU, but when GPU is enabled, nans are made on some of the output lines. Also, lines that have nans change each time you run the test program. Also sometimes when you run the test program it crashes by saying  Floating point exception. \n\ncuda-gdb gave if I got the floating point exception: \n`0x0000555556057e80 in tensorflow::functor::CastFunctor<Eigen::GpuDevice, float, int>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<int const, 1, 1, long>, 16>) ()\n`\n\nEven stranger is that when the program ran successfully using cuda-gdb, the output would be completely normal. Except whenever I got the above floating point exception. This is so bizarre... \n", "I have the same problem as @fbolanos. I have not been able to resolve the issue, but I thought I'd mention that someone uploaded their pip wheel in another thread (https://github.com/tensorflow/tensorflow/issues/4030). I downloaded and installed it and it seem to be working fine on my system (Ubuntu 16.04, GTX 1080, CUDA 8.0 RC + Patch, cuDNN 5.1).\n", "@fbolanos, @cr145, I put on a GTX 1080 on my local machine, but couldn't reproduce the problem. \n\nMy exact steps:\n- git clone http://github.com/tensorflow/tensorflow --recursive\n  *\\* f974e8d0c2420c6f7e2a2791febb4781a266823f for my experiment. \n- cd tensorflow\n- printf \"\\n\\n\\ny\\n\\n8.0\\n/usr/local/cuda-8.0\\n5.0.5\\n/usr/local/cudnn-5.0\\n\\n\" | ./configure\n- bazel build -c opt --config=cuda tensorflow/cc/tutorials_example_trainer\n- LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cudnn-5.0/lib64 \\\n  bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n\nOutput: \n\n> > ...\n> > 000001/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n> > 000001/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n\nSetup: \n- GTX 1080, Ubuntu 14.04, Cuda 8.0, Cudnn 5.0.5, bazel 0.3.0, Cuda driver 367.35\n\nPlease let me know if you still have problem if you match my steps as close as possible. If so, something very fishy is going on. \n", "i met the same problem when i set up it in a titianx. but after a few days\uff0cthe example run well automatically. however\uff0cthe machine is about 5x slower when running lstm. I don't know whether these are related.\n\n\u53d1\u81ea\u6211\u7684 iPhone\n\n> \u5728 2016\u5e749\u67081\u65e5\uff0c\u4e0a\u53486:55\uff0czheng-xq notifications@github.com \u5199\u9053\uff1a\n> \n> @fbolanos, @cr145, I put on a GTX 1080 on my local machine, but couldn't reproduce the problem.\n> \n> My exact steps:\n> \n> git clone http://github.com/tensorflow/tensorflow --recursive *\\* f974e8d for my experiment.\n> cd tensorflow\n> printf \"\\n\\n\\ny\\n\\n8.0\\n/usr/local/cuda-8.0\\n5.0.5\\n/usr/local/cudnn-5.0\\n\\n\" | ./configure\n> bazel build -c opt --config=cuda tensorflow/cc/tutorials_example_trainer\n> LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cudnn-5.0/lib64 \\ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n> Output:\n> \n> ...\n> 000001/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n> 000001/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n> \n> Setup:\n> \n> GTX 1080, Ubuntu 14.04, Cuda 8.0, Cudnn 5.0.5, bazel 0.3.0, Cuda driver 367.35\n> Please let me know if you still have problem if you match my steps as close as possible. If so, something very fishy is going on.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Ok! So it appears that after rebooting the machine the output is correct. However, the floating point exception I showed above occurs randomly when running the test program several times. However, no more Nans! :)\n", "I think we need to get rid of the multi-threading of this example -- it's actually not safe to do in its current implementation.\n", "@fbolanos, Vijay was correct. The example uses DirectSession that didn't share the underlying GPUDevice. They are not good for parallel execution. Other types of sessions do not have this problem. There are other effort trying to unite the session implementations that will address this problem at the same time. \n\nPlease try \"--num_concurrent_sessions=1\" and \"--num_concurrent_steps=1\" for your experiments. If you don't see any exceptions with those, then everything is good. \n", "Was this solved? I'm having the same problem. Code here: https://github.com/openai/InfoGAN ran fine on CPU. On GPU, it ran successfully for a small number of epochs before loss returned NaN. All (fresh) runs now return NaN as loss. Ubuntu 14, TensforFlow 0.9.0rc0, CUDA 7.0, cuDNN 4.0, 3x geforce titan X.", "It's fixed in newer versions of TF\n\nOn Mon, Jun 5, 2017 at 6:47 PM cianeastwood <notifications@github.com>\nwrote:\n\n> Was this solved? I'm having the same problem. Code here:\n> https://github.com/openai/InfoGAN ran fine on CPU. On GPU, it ran\n> successfully for a small number of epochs before loss returned NaN. All\n> (fresh) runs now return NaN as loss. Ubuntu 14, TensforFlow 0.9.0rc0, CUDA\n> 7.0, cuDNN 4.0, 3x geforce titan X.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2037#issuecomment-306331476>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AUOS_rYazprcHf9sjZarL5UsvgfW70u6ks5sBIWQgaJpZM4IL3Aa>\n> .\n>\n"]}, {"number": 2036, "title": "Update comment", "body": "modify the comment to link the right arxiv paper on Adadelta\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2035, "title": "how to build the project for intel x86 emulator instead of armeabi v7?", "body": "Hi, I am beginner in android,\n\n  I have successfully built the android demo with bazel. Now I am trying to work with it in Android studio.\n\n I found this repository [https://github.com/miyosuda/TensorFlowAndroidDemo](url).\n It currently works well with **armeabiv7** but crashes with **intel x86**  emulator. \n\nBut I want to build it for **intel x86 and x86_64** and I want to **build it in Android studio** using gradle what should I do? Please help\n\nI am using \n                  NDK 10re \n                  Android Studio 2.0\n                  UBUNTU 14.04\n", "comments": ["I've never used Gradle before, but the project you linked to seems to be configured to only produce armeabi-v7a builds. I assume you'd edit the occurrences here: https://github.com/miyosuda/TensorFlowAndroidDemo/search?utf8=%E2%9C%93&q=armeabi-v7a\nand add x86 and x86_64 to each.\n", "I modified the **Makefile** and the **Application.mk** as you said but I am getting this error with the make command\n\n> kaushalya@kaushalya-HP-350-G1:~/TensorFlowAndroidDemo-master/jni-build$ make\n> ndk-build\n> Android NDK: WARNING:jni/Android.mk:tensorflow_demo: non-system libraries in linker flags: jni/libs/armeabi-v7a/libandroid_tensorflow_lib.a jni/libs/armeabi-v7a/libre2.a jni/libs/armeabi-v7a/libprotos_all_cc.a jni/libs/armeabi-v7a/libprotobuf.a jni/libs/armeabi-v7a/libprotobuf_lite.a /home/kaushalya/android-ndk-r10e/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a /home/kaushalya/android-ndk-r10e/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a  \n> Android NDK:     This is likely to result in incorrect builds. Try using LOCAL_STATIC_LIBRARIES  \n> Android NDK:     or LOCAL_SHARED_LIBRARIES instead to list the library dependencies of the  \n> Android NDK:     current module  \n> Android NDK: WARNING:jni/Android.mk:tensorflow_demo: non-system libraries in linker flags: jni/libs/x86/libandroid_tensorflow_lib.a jni/libs/x86/libre2.a jni/libs/x86/libprotos_all_cc.a jni/libs/x86/libprotobuf.a jni/libs/x86/libprotobuf_lite.a /home/kaushalya/android-ndk-r10e/sources/cxx-stl/gnu-libstdc++/4.9/libs/x86/libgnustl_static.a /home/kaushalya/android-ndk-r10e/sources/cxx-stl/gnu-libstdc++/4.9/libs/x86/libsupc++.a  \n> Android NDK:     This is likely to result in incorrect builds. Try using LOCAL_STATIC_LIBRARIES  \n> Android NDK:     or LOCAL_SHARED_LIBRARIES instead to list the library dependencies of the  \n> Android NDK:     current module  \n> Android NDK: WARNING:jni/Android.mk:tensorflow_demo: non-system libraries in linker flags: jni/libs/x86_64/libandroid_tensorflow_lib.a jni/libs/x86_64/libre2.a jni/libs/x86_64/libprotos_all_cc.a jni/libs/x86_64/libprotobuf.a jni/libs/x86_64/libprotobuf_lite.a /home/kaushalya/android-ndk-r10e/sources/cxx-stl/gnu-libstdc++/4.9/libs/x86_64/libgnustl_static.a /home/kaushalya/android-ndk-r10e/sources/cxx-stl/gnu-libstdc++/4.9/libs/x86_64/libsupc++.a  \n> Android NDK:     This is likely to result in incorrect builds. Try using LOCAL_STATIC_LIBRARIES  \n> Android NDK:     or LOCAL_SHARED_LIBRARIES instead to list the library dependencies of the  \n> Android NDK:     current module  \n> make[1]: Entering directory `/home/kaushalya/TensorFlowAndroidDemo-master/jni-build'\n> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= tensorflow_jni.cc\n> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= imageutils_jni.cc\n> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= jni_utils.cc\n> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= rgb2yuv.cc\n> [armeabi-v7a] Compile++ arm  : tensorflow_demo <= yuv2rgb.cc\n> [armeabi-v7a] SharedLibrary  : libtensorflow_demo.so\n> [armeabi-v7a] Install        : libtensorflow_demo.so => libs/armeabi-v7a/libtensorflow_demo.so\n> [x86] Compile++      : tensorflow_demo <= tensorflow_jni.cc\n> i686-linux-android-g++: error: unrecognized command line option '-mfpu=vfpv3-d16'\n> i686-linux-android-g++: error: unrecognized command line option '-mfloat-abi=softfp'\n> i686-linux-android-g++: error: unrecognized command line option '-mfpu=neon'\n> make[1]: *** [obj/local/x86/objs/tensorflow_demo/./tensorflow_jni.o] Error 1\n> make[1]: Leaving directory`/home/kaushalya/TensorFlowAndroidDemo-master/jni-build'\n> make: **\\* [build] Error 2\n> kaushalya@kaushalya-HP-350-G1:~/TensorFlowAndroidDemo-master/jni-build$ \n\nI think there should be change in **Android.mk** too. But I do not know how?\n", "It looks like you'll also need to remove the ARM-only flags being passed to the compiler. There are flags that only work for building x86, and likewise some that only work for arm.\n\nYou can take a look at tensorflow.bzl and try using the argument list from there with the exception of the mfpu one.\n", "Sorry for inconvenience with my project and let me investigate about this.\n\nBy the way this repository,\nhttps://github.com/miyosuda/TensorFlowAndroidDemo\nis using .a library files and headers from TensorFlow 0.6.0.\n\nI've recently upgraded another repository with MNIST sample\nhttps://github.com/miyosuda/TensorFlowAndroidMNIST\nwith latest TensorFlow r0.8.\n\nLet me upgrade TensorFlowAndroidDemo with r0.8 too.\n", "@CalmWaves \nI've prepared compiler options and .a library files for x86 and x86_64\nCould you try this branch?\n\nhttps://github.com/miyosuda/TensorFlowAndroidDemo/tree/x86\n\n(I've also upgraded to TensorFlow r0.8)\n\nWhen I tried this with x86 emulator, the app crashed with IllegalArgumentException at\nCameraManager.openCamera()\nbecause emulator does not have camera.\n", "Forgive me for late response, In my situation app closes after showing the camera input for 1s, I have used my web cam as the emulator camera,\nEmulator details:\n\n> Name: Nexus_5X_API_23\n> CPU/ABI: Google APIs Intel Atom (x86)\n> Path: /home/kaushalya/.android/avd/Nexus_5X_API_23.avd\n> Target: google_apis [Google APIs](API level 23)\n> Skin: nexus_5x\n> SD Card: /home/kaushalya/.android/avd/Nexus_5X_API_23.avd/sdcard.img\n> hw.dPad: no\n> runtime.network.speed: full\n> hw.accelerometer: yes\n> hw.device.name: Nexus 5X\n> vm.heapSize: 512\n> hw.device.manufacturer: Google\n> hw.gps: yes\n> image.androidVersion.api: 23\n> hw.audioInput: yes\n> image.sysdir.1: system-images/android-23/google_apis/x86/\n> tag.id: google_apis\n> hw.camera.back: webcam0\n> hw.mainKeys: no\n> AvdId: Nexus_5X_API_23\n> hw.camera.front: webcam0\n> hw.lcd.density: 420\n> runtime.scalefactor: auto\n> avd.ini.displayname: Nexus 5X API 23\n> hw.gpu.mode: auto\n> hw.device.hash2: MD5:1be89bc42ec9644d4b77968b23474980\n> hw.ramSize: 2048\n> hw.trackBall: no\n> hw.battery: yes\n> hw.sdCard: yes\n> tag.display: Google APIs\n> runtime.network.latency: none\n> hw.keyboard: yes\n> hw.sensors.proximity: yes\n> disk.dataPartition.size: 800M\n> hw.sensors.orientation: yes\n> avd.ini.encoding: UTF-8\n> hw.gpu.enabled: yes\n\nwhen I open the default emulator Camera app it works fine.\n\nWhen I open the TensorFlow_Demo, it shows the camera input for 1s  with blue band on the top and then closes automatically (without any crash message on the emulator screen).\n\nin logcat:\n\n> 04-25 21:17:05.594 2497-2503/org.tensorflow.tensorflowdemo E/art: Failed sending reply to debugger: Broken pipe \n> 04-25 19:29:20.492 3103-8992/org.tensorflow.tensorflowdemo E/BufferQueueProducer: [ImageReader-640x360f23m2-3103-5] dequeueBuffer: createGraphicBuffer failed\n> 04-25 19:29:20.492 3103-8992/org.tensorflow.tensorflowdemo E/Legacy-CameraDevice-JNI: LegacyCameraDevice_nativeProduceFrame: Error while producing frame Invalid argument (-22).\n> 04-25 19:29:20.550 3103-8992/org.tensorflow.tensorflowdemo E/BufferQueueProducer: [ImageReader-640x360f23m2-3103-5] dequeueBuffer: can't dequeue multiple buffers without setting the buffer count\n> 04-25 19:29:20.550 3103-8992/org.tensorflow.tensorflowdemo E/Legacy-CameraDevice-JNI: LegacyCameraDevice_nativeProduceFrame: Error while producing frame Function not implemented (-38).\n> 04-25 19:29:20.551 3103-8992/org.tensorflow.tensorflowdemo E/CameraDeviceGLThread-0: Received exception on GL render thread: \n>                                                                                      java.lang.UnsupportedOperationException: Unknown error -38\n>                                                                                          at android.hardware.camera2.legacy.LegacyExceptionUtils.throwOnError(LegacyExceptionUtils.java:69)\n>                                                                                          at android.hardware.camera2.legacy.LegacyCameraDevice.produceFrame(LegacyCameraDevice.java:636)\n>                                                                                          at android.hardware.camera2.legacy.SurfaceTextureRenderer.drawIntoSurfaces(SurfaceTextureRenderer.java:750)\n>                                                                                          at android.hardware.camera2.legacy.GLThreadManager$1.handleMessage(GLThreadManager.java:105)\n>                                                                                          at android.os.Handler.dispatchMessage(Handler.java:98)\n>                                                                                          at android.os.Looper.loop(Looper.java:148)\n>                                                                                          at android.os.HandlerThread.run(HandlerThread.java:61)\n> 04-25 19:29:24.537 3103-8991/org.tensorflow.tensorflowdemo E/RequestThread-0: Timed out while waiting for request to complete.\n> 04-25 19:29:24.537 3103-8991/org.tensorflow.tensorflowdemo E/CameraDeviceState: Cannot receive result while in state: 0\n> 04-25 19:29:24.537 3103-8991/org.tensorflow.tensorflowdemo E/CameraDeviceState: Cannot receive result while in state: 0\n> 04-25 19:29:24.537 3103-8991/org.tensorflow.tensorflowdemo E/CameraDeviceState: Cannot receive result while in state: 0\n> 04-25 19:29:24.539 3103-8992/org.tensorflow.tensorflowdemo E/Surface: getSlotFromBufferLocked: unknown buffer: 0xb4095560\n> 04-25 19:29:24.610 3103-3120/org.tensorflow.tensorflowdemo E/Surface: getSlotFromBufferLocked: unknown buffer: 0xb4093ce0\n\nWhat should I do? Have you tried emulator with webcam as the camera?. What was your detail configurations of the emulator in your case?\n", "I tested with emulator with the config given above except this emulator uses API 21 and it gives me in logcat:\n\n> 04-25 19:57:18.936 2313-2382/org.tensorflow.tensorflowdemo E/EGL_emulation: tid 2382: eglCreateContext(877): error 0x3005 (EGL_BAD_CONFIG)\n> 04-25 19:57:18.936 2313-2382/org.tensorflow.tensorflowdemo E/CameraDeviceGLThread-0: Received exception on GL render thread: \n>                                                                                      java.lang.IllegalStateException: eglCreateContext: EGL error: 0x3005\n>                                                                                          at android.hardware.camera2.legacy.SurfaceTextureRenderer.checkEglError(SurfaceTextureRenderer.java:487)\n>                                                                                          at android.hardware.camera2.legacy.SurfaceTextureRenderer.configureEGLContext(SurfaceTextureRenderer.java:386)\n>                                                                                          at android.hardware.camera2.legacy.SurfaceTextureRenderer.configureSurfaces(SurfaceTextureRenderer.java:600)\n>                                                                                          at android.hardware.camera2.legacy.GLThreadManager$1.handleMessage(GLThreadManager.java:87)\n>                                                                                          at android.os.Handler.dispatchMessage(Handler.java:98)\n>                                                                                          at android.os.Looper.loop(Looper.java:135)\n>                                                                                          at android.os.HandlerThread.run(HandlerThread.java:61)\n\ntensorflow demo app stays with empty blue band with totally black camera input area.\n\nAnother thing to notice is here default camera app on the emulator crashes. with the messages given below in the emulator message view:\n\n> emulator: ERROR: camera_device_start_capturing: VIDIOC_STREAMON on camera '/dev/video0' has failed: Timer expired\n> emulator: ERROR: _camera_client_query_start: Cannot start camera '/dev/video0' for NV21[640x480]: Inappropriate ioctl for device\n\nThis **Inappropriate ioctl for device** is seems due to not matching my webcam resolution with the emulator resolution, If so how do I fix that? I already tried changing the max vertical, max horizontal in emulator .ini file but message still the same. \nI have installed **tensorflow r0.8 only CPU version**. Is that have any effect on the issue I am facing?\nhelp me on this? If someone can drop API level of the app to 19 I can try it on my phone?\n", "The only thing preventing the demo from running on API 19 is that it uses the camera2 api. If you rewrite the relevant code to use android.hardware.Camera it will run fine on older devices.\n\nIn general I haven't found emulators and cameras to get along, unfortunately. If possible I'd suggest running on a real device or altering the code to take canned images (if you're just trying to test a model).\n", "Closing this now as your original issue has been addressed (thanks \n@miyosuda!) and there are complete instructions for adapting the TF demo to api level < 21 in https://github.com/tensorflow/tensorflow/issues/419 now.\n\nEmulator support for cameras is outside the scope of Tensorflow support (I'd try stackoverflow for that), but feel free to open any additional TF-specific issues you have with the demo.\n", "Thank you @andrewharp  and @miyosuda.\n"]}, {"number": 2034, "title": "segmentation fault", "body": "I install tensorflow use:sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl\n\nlinux version is :Linux version 3.10.0-229.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.2 20140120 (Red Hat 4.8.2-16) (GCC) ) #1 SMP Fri Mar 6 11:36:42 UTC 2015\n\nwhen I learn python, and import tensorflow,segmentation fault\n\n> > > import tensorflow\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n> > > Segmentation fault (core dumped)\n", "comments": ["Hi,\nCould you please provide a little more information:\n\nWhat is the hardware configuration of your machine and what versions of the NVIDIA software do you have installed?\n\nThanks,\nPaul\n", "I got the same problem under:\n\n<pre>sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl</pre>\n\nCuda: /usr/local/cuda-7.5/\nGPU: NVIDIA Geforce GTX 970M\nCUDNN: cudnn-7.0-linux-x64-v4.0-prod.tgz\nLinux: 4.2.0-30-generic #35-Ubuntu \n\nIf I import numpy or matplotlib before tensorflow, it won't crash. If I import tensorflow at the very beginning, it goes to segmentation fault. I guess your \"engineers\" simply forget to import some libs in this tensorflow release :laughing: \n\n<pre>\n~/>python3.4\nPython 3.4.3+ (default, Oct 14 2015, 16:03:50) \n[GCC 5.2.1 20151010] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nSegmentation fault (core dumped)\n~/>python3.4\nPython 3.4.3+ (default, Oct 14 2015, 16:03:50) \n[GCC 5.2.1 20151010] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import numpy\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n>>>#Everything good\n</pre>\n", "Yes I've got the same problem, and as @zhang8473 said, if I import numpy and matplotlib first, it won't crash.\nAnd I'm using the version without GPU\n\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow\n> > > Segmentation fault (core dumped)\n\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import numpy\n> > > import matplotlib\n> > > import tensorflow\n", "same problem here\nCUDA 7.5 with CUDNN V5(build 5004)\nPython 2.7.5 (default, Nov 20 2015, 02:00:19) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2\n", "I've got the same issue with the CPU version:\n`pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl`\n\nAll I have is imports in my test script, and I get the segfault:\n\n```\n# Import libraries for simulation\nimport tensorflow as tf\nimport numpy as np\nimport scipy.ndimage as nd\n```\n\nNo errors if I skip import of tensorflow, or if I put it as the final import instead of the first.\n\nI ran it through gdb and got the following backtrace:\n\n```\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff2d0e220 in PyArray_API () from /media/local/usr/lib64/python2.7/site-packages/numpy/core/multiarray.so\n(gdb) backtrace\n#0  0x00007ffff2d0e220 in PyArray_API () from /media/local/usr/lib64/python2.7/site-packages/numpy/core/multiarray.so\n#1  0x00007fffe0ed8e54 in _import_array ()\n    at /opt/_internal/cpython-2.7.11-ucs4/lib/python2.7/site-packages/numpy/core/include/numpy/__multiarray_api.h:1633\n#2  initspecfun () at build/src.linux-x86_64-2.7/scipy/special/specfunmodule.c:5830\n#3  0x00007ffff7b2fc0e in _PyImport_LoadDynamicModule () from /usr/lib64/libpython2.7.so.1.0\n#4  0x00007ffff7abf06c in ?? () from /usr/lib64/libpython2.7.so.1.0\n#5  0x00007ffff7abf2a4 in ?? () from /usr/lib64/libpython2.7.so.1.0\n#6  0x00007ffff7b01756 in PyImport_ImportModuleLevel () from /usr/lib64/libpython2.7.so.1.0\n#7  0x00007ffff7af276b in ?? () from /usr/lib64/libpython2.7.so.1.0\n#8  0x00007ffff7ad96f6 in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0\n#9  0x00007ffff7af2c80 in PyEval_CallObjectWithKeywords () from /usr/lib64/libpython2.7.so.1.0\n#10 0x00007ffff7af48ed in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\n#11 0x00007ffff7afa33e in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\n#12 0x00007ffff7b27142 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\n#13 0x00007ffff7b2efb0 in PyImport_ExecCodeModuleEx () from /usr/lib64/libpython2.7.so.1.0\n#14 0x00007ffff7b2f1ca in ?? () from /usr/lib64/libpython2.7.so.1.0\n#15 0x00007ffff7b0104f in ?? () from /usr/lib64/libpython2.7.so.1.0\n#16 0x00007ffff7b018d4 in PyImport_ImportModuleLevel () from /usr/lib64/libpython2.7.so.1.0\n#17 0x00007ffff7af276b in ?? () from /usr/lib64/libpython2.7.so.1.0\n#18 0x00007ffff7ad96f6 in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0\n#19 0x00007ffff7af2c80 in PyEval_CallObjectWithKeywords () from /usr/lib64/libpython2.7.so.1.0\n#20 0x00007ffff7af48ed in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\n#21 0x00007ffff7afa33e in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\n#22 0x00007ffff7b27142 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\n#23 0x00007ffff7b2efb0 in PyImport_ExecCodeModuleEx () from /usr/lib64/libpython2.7.so.1.0\n#24 0x00007ffff7b2f1ca in ?? () from /usr/lib64/libpython2.7.so.1.0\n#25 0x00007ffff7b2f870 in ?? () from /usr/lib64/libpython2.7.so.1.0\n#26 0x00007ffff7b0104f in ?? () from /usr/lib64/libpython2.7.so.1.0\n#27 0x00007ffff7b01624 in PyImport_ImportModuleLevel () from /usr/lib64/libpython2.7.so.1.0\n#28 0x00007ffff7af276b in ?? () from /usr/lib64/libpython2.7.so.1.0\n#29 0x00007ffff7ad96f6 in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0\n#30 0x00007ffff7af2c80 in PyEval_CallObjectWithKeywords () from /usr/lib64/libpython2.7.so.1.0\n#31 0x00007ffff7af48ed in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\n#32 0x00007ffff7afa33e in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\n#33 0x00007ffff7b27142 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\n#34 0x00007ffff7b2efb0 in PyImport_ExecCodeModuleEx () from /usr/lib64/libpython2.7.so.1.0\n#35 0x00007ffff7b2f1ca in ?? () from /usr/lib64/libpython2.7.so.1.0\n#36 0x00007ffff7b2f870 in ?? () from /usr/lib64/libpython2.7.so.1.0\n#37 0x00007ffff7b0104f in ?? () from /usr/lib64/libpython2.7.so.1.0\n#38 0x00007ffff7b01624 in PyImport_ImportModuleLevel () from /usr/lib64/libpython2.7.so.1.0\n#39 0x00007ffff7af276b in ?? () from /usr/lib64/libpython2.7.so.1.0\n#40 0x00007ffff7ad96f6 in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0\n#41 0x00007ffff7af2c80 in PyEval_CallObjectWithKeywords () from /usr/lib64/libpython2.7.so.1.0\n#42 0x00007ffff7af48ed in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\n#43 0x00007ffff7afa33e in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\n#44 0x00007ffff7b27142 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\n#45 0x00007ffff7b2efb0 in PyImport_ExecCodeModuleEx () from /usr/lib64/libpython2.7.so.1.0\n#46 0x00007ffff7b2f1ca in ?? () from /usr/lib64/libpython2.7.so.1.0\n#47 0x00007ffff7b0104f in ?? () from /usr/lib64/libpython2.7.so.1.0\n#48 0x00007ffff7b018d4 in PyImport_ImportModuleLevel () from /usr/lib64/libpython2.7.so.1.0\n#49 0x00007ffff7af276b in ?? () from /usr/lib64/libpython2.7.so.1.0\n#50 0x00007ffff7ad96f6 in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0\n#51 0x00007ffff7af2c80 in PyEval_CallObjectWithKeywords () from /usr/lib64/libpython2.7.so.1.0\n#52 0x00007ffff7af48ed in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\n#53 0x00007ffff7afa33e in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\n#54 0x00007ffff7b27142 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\n#55 0x00007ffff7b2efb0 in PyImport_ExecCodeModuleEx () from /usr/lib64/libpython2.7.so.1.0\n#56 0x00007ffff7b2f1ca in ?? () from /usr/lib64/libpython2.7.so.1.0\n#57 0x00007ffff7b2f870 in ?? () from /usr/lib64/libpython2.7.so.1.0\n#58 0x00007ffff7b0104f in ?? () from /usr/lib64/libpython2.7.so.1.0\n#59 0x00007ffff7b01624 in PyImport_ImportModuleLevel () from /usr/lib64/libpython2.7.so.1.0\n#60 0x00007ffff7af276b in ?? () from /usr/lib64/libpython2.7.so.1.0\n#61 0x00007ffff7ad96f6 in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0\n#62 0x00007ffff7af2c80 in PyEval_CallObjectWithKeywords () from /usr/lib64/libpython2.7.so.1.0\n#63 0x00007ffff7af48ed in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0\n#64 0x00007ffff7afa33e in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0\n#65 0x00007ffff7b27142 in PyEval_EvalCode () from /usr/lib64/libpython2.7.so.1.0\n#66 0x00007ffff7b338ad in ?? () from /usr/lib64/libpython2.7.so.1.0\n#67 0x00007ffff7ac16ad in PyRun_FileExFlags () from /usr/lib64/libpython2.7.so.1.0\n#68 0x00007ffff7ac2294 in PyRun_SimpleFileExFlags () from /usr/lib64/libpython2.7.so.1.0\n#69 0x00007ffff7ac9e63 in Py_Main () from /usr/lib64/libpython2.7.so.1.0\n#70 0x00007ffff7488b05 in __libc_start_main () from /lib64/libc.so.6\n#71 0x000000000040078e in _start ()\n(gdb)\n```\n\nThe issue seems very similar to [one that used to exist in numpy](https://github.com/numpy/numpy/issues/2521)\n", "I'm looking into this further and there seems to be a known issue related to this somehow in the [tf_session_helper.h](https://github.com/tensorflow/tensorflow/blob/e39d8feebb9666a331345cd8d960f5ade4652bba/tensorflow/python/client/tf_session_helper.h)\n\n```\n\n#ifdef PyArray_Type\n#error \"Numpy cannot be included before tf_session_helper.h.\"\n#endif\n\n// Disallow Numpy 1.7 deprecated symbols.\n#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n\n// We import_array in the tensorflow init function only.\n#define PY_ARRAY_UNIQUE_SYMBOL _tensorflow_numpy_api\n#ifndef TF_IMPORT_NUMPY\n#define NO_IMPORT_ARRAY\n#endif\n\n```\n\nNote the error check: **Numpy cannot be included before tf_session_helper.h**\n\nCould be that we need something similar for `#ifdef PyArray_API`\n", "Oh wait.. the problem appears if we put TensorFlow _before_ numpy... maybe not it\n", "The error sounds like \"import_array\" is not getting run. That's a function that sets up some global state and must be run before using numpy C API.\n\nWe used to have the following in tf_session.i\n\n```\n%include \"tensorflow/python/platform/numpy.i\"\n%init %{\nimport_array();\n%}\n```\n\nIt looks like it's got enhanced with some logic which I don't fully understand. @girving -- do you see any scenarios where \"import_array()\" isn't going to run?\n", "@tmsimont: I'm confused by your stacktrace.  Why is scipy involved?  Is it possible to get a reproduction case that doesn't involve scipy being the culprit?\n", "Ah, looks like there a few tests that do touch scipy, including one broken one that requires it or fails.  I will get the culprit to fix that one, but it's unrelated to this issue.\n", "It seems that in my case it wasn't just numpy...\n\nThis works:\n\n```\nimport tensorflow as tf\nimport numpy as np\n```\n\nThis fails\n\n```\nimport tensorflow as tf\nimport scipy.ndimage as nd\n```\n\nHowever, this does not fail:\n\n```\nimport tensorflow as tf\nimport scipy\n```\n\nSo it seems there is something in `scipy.ndimage` that causes a segmentation fault after tensorflow is loaded.\n\nThis does not fail\n\n```\nimport scipy.ndimage as nd\nimport tensorflow as tf\n```\n\nAlso -- FYI, I'm using scipy here as this comes from the [Mandlebrot example](https://www.tensorflow.org/versions/r0.8/tutorials/mandelbrot/index.html#mandelbrot-set)\n\nAfter running some more tests and comparing my issue to the OP I'm afraid I may have a different problem than what others in this thread have encountered.\n\n@mouendless, @chengdianxuezi  and @zhang8473 all seem to have a problem with just the single `import tensorflow` include. I apparently only have the issue when scipy.ndimage is imported after tensorflow...\n", "@vrv or @martinwicke: Do you know what version of numpy we're building against?\n@chengdianxuezi, @mouendless, @fxia22: What versions of numpy do you have?\n\nIf these versions don't match, numpy might decide to crash.\n", "@caisq for pip build info\n", "@girving I am using `numpy==1.11.0`\n", "I noticed something potentially relevant here:\n\n```\nINFO: From Compiling tensorflow/python/lib/core/py_func.cc:\nIn file included from third_party/py/numpy/numpy_include/numpy/ndarraytypes.h:17\n77:0,\n                 from third_party/py/numpy/numpy_include/numpy/ndarrayobject.h:1\n8,\n                 from third_party/py/numpy/numpy_include/numpy/arrayobject.h:4,\n                 from tensorflow/python/lib/core/py_func.cc:19:\nthird_party/py/numpy/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning:\n #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECA\nTED_API NPY_1_7_API_VERSION\" [-Wcpp]\n #warning \"Using deprecated NumPy API, disable it by \" \\\n  ^\n```\n\nThere's a warning about the numpy include using deprecated NumPy API.\n\nI tried to use `#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION` as it is used in other TensorFlow headers, but then the project fails to compile:\n\n```\ntensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ConvertNdarrayToTensor(PyObject*, tensorflow::Tensor*)':\ntensorflow/python/lib/core/py_func.cc:235:50: error: 'PyArrayObject' has no member named 'data'\n       memcpy(const_cast<char*>(p.data()), input->data, p.size());\n                                                  ^\ntensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::ConvertTensorToNdarray(const tensorflow::Tensor&, PyObject**)':\ntensorflow/python/lib/core/py_func.cc:330:61: error: 'PyArrayObject' has no member named 'data'\n     PyObject** out = reinterpret_cast<PyObject**>(np_array->data);\n                                                             ^\ntensorflow/python/lib/core/py_func.cc:345:22: error: 'PyArrayObject' has no member named 'data'\n     memcpy(np_array->data, p.data(), p.size());\n                      ^\n```\n\nIs TensorFlow trying to use two different versions of the NumPy API?\n", "It does look like an old numpy is creeping in from somewhere.\nOn Tue, Apr 26, 2016 at 06:48 Trevor Simonton notifications@github.com\nwrote:\n\n> I noticed something potentially relevant here:\n> \n> INFO: From Compiling tensorflow/python/lib/core/py_func.cc:\n> In file included from third_party/py/numpy/numpy_include/numpy/ndarraytypes.h:17\n> 77:0,\n>                  from third_party/py/numpy/numpy_include/numpy/ndarrayobject.h:1\n> 8,\n>                  from third_party/py/numpy/numpy_include/numpy/arrayobject.h:4,\n>                  from tensorflow/python/lib/core/py_func.cc:19:\n> third_party/py/numpy/numpy_include/numpy/npy_1_7_deprecated_api.h:15:2: warning:\n>  #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECA\n> TED_API NPY_1_7_API_VERSION\" [-Wcpp]\n>  #warning \"Using deprecated NumPy API, disable it by \" \\\n>   ^\n> \n> There's a warning about the numpy include using deprecated NumPy API.\n> \n> I tried to use #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION as it is\n> used in other TensorFlow headers, but then the project fails to compile:\n> \n> tensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ConvertNdarrayToTensor(PyObject_, tensorflow::Tensor_)':\n> tensorflow/python/lib/core/py_func.cc:235:50: error: 'PyArrayObject' has no member named 'data'\n>        memcpy(const_cast<char*>(p.data()), input->data, p.size());\n>                                                   ^\n> tensorflow/python/lib/core/py_func.cc: In function 'tensorflow::Status tensorflow::ConvertTensorToNdarray(const tensorflow::Tensor&, PyObject**)':\n> tensorflow/python/lib/core/py_func.cc:330:61: error: 'PyArrayObject' has no member named 'data'\n>      PyObject** out = reinterpret_cast<PyObject**>(np_array->data);\n>                                                              ^\n> tensorflow/python/lib/core/py_func.cc:345:22: error: 'PyArrayObject' has no member named 'data'\n>      memcpy(np_array->data, p.data(), p.size());\n>                       ^\n> \n> Is TensorFlow trying to use two different versions of the NumPy API?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-214750702\n", "@tmsimont: Can you remove `py_func.cc` entirely (or comment out the whole file) and see if that fixes the problem?  `py_func` should definitely be fixed, but I'm skeptical that it's the problem here (hopefully I'm wrong!).\n", "I'm fixing `py_func` to not roll it's own numpy import logic now in case that is the problem.\n", "With `py_func.cc` commented out the compiler warning goes away and so does the segmentation fault, but `import tensorflow` throws an error: `undefined symbol: _ZN10tensorflow22ConvertTensorToNdarrayERKNS_6TensorEPP7_object`. \n", "That's \"tensorflow::ConvertTensorToNdarray(tensorflow::Tensor const&, _object**)\" (used c++ filt)\n\nSo the library is being included through \"py_func_lib\", could you remove \"py_func_lib\" dependency here? \"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L1010\"\n", "https://github.com/tensorflow/tensorflow/pull/2114 fixes the `py_func` weirdness, but I still don't think it's related to the original issue.\n", "@caisq: Do you know what numpy version we're building against for the pip packages? \n", "I have faced same issue for the segmentation fault after compiling the 0.8 GPU version for tensor flow.\n\nWork around mentioned by @zhang8473 works perfectly,i.e import numpy then import tensor flow.\n\nIn addition to above workaround, i found something interesting. When building from source if i skip first step and directly go to build with GPU option, i am getting segmentation fault.\n\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n$======To build with GPU support:=======\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\n$ pip install /tmp/tensorflow_pkg/tensorflow-0.8.0-py2-none-linux_x86_64.whl\n\nOn other hand if i use all above mentioned commands in order then i do not get any segmentation fault.\n\nO/P if we skip the first step:\n\n![screen shot 2016-04-28 at 2 51 29 pm](https://cloud.githubusercontent.com/assets/12982311/14902573/baaba1ee-0d50-11e6-874e-efdbdc42f569.png)\n\nPardon my ignorance but can somebody tell me the correct way to compile the 0.8 GPU version. \n", "@kanwar2preet: Is it possible to get a stack trace from that crash by running Python inside gdb?  Also, unfortunately I didn't quite follow the set of commands that work and do not work from the above description.  Can you say which commands you mean again?\n", "@girving I posted a copy of my GDB run in #2129. Unfortunately, without debug symbols it's not much help. I did try compiling TF with \"-g\" on (manually removed the -g0 in the BUILD file), but debug symbols still didn't seem there, so I didn't bother.\n\nHere's some outputs of some strace runs though.\n\n```\n$ strace python -c \"import numpy; import tensorflow\" 2> tfnosegfault.txt\n$ strace python -c \"import tensorflow\" 2> tfsegfault.txt \n```\n\n[tfnosegfault.txt](https://github.com/tensorflow/tensorflow/files/241666/tfnosegfault.txt)\n[tfsegfault.txt](https://github.com/tensorflow/tensorflow/files/241665/tfsegfault.txt)\n", "In those outputs, my python installation is `/work/01813/roller/maverick/packages/python/`, which has `lib` and such under it.) What's interesting is all the calls to scipy and pandas and such. From what I can tell, they're picked up by the skflow module, which is imported somewhere...\n", "@girving @vrv The numpy version we use for current nightly builds and 0.8 release builds are:\nOn Mac: 1.11.0\nON Linux (in ubuntu:14.04 Docker images): 1.8.2\n\n1.8.2 is the version that comes with apt-get on ubuntu:14.04. See:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_deb_packages.sh#L41\n", "@stephenroller \"blaze build -c dbg <tensorflowtarget>\" to build with debug symbols\n", "@yaroslavvb thanks\n\nReally seems like the logic around _import_array() in initialization is wrong. Perhaps there's a missing `#define NO_IMPORT_ARRAY` somewhere? I've been messing around with adding it, moving `#include \"numpy.h\"` higher up before other includes, but no luck so far. Maybe only with Numpy 1.11?\n\n(Running CentOS 6.7, GCC 4.9.1 compiled by sysadmins, bazel self compiled, TF self compiled, Python self compiled, cuda 7.5, cudnn 5.0, Numpy 1.11.0 from pip install)\nStack trace:\n\n```\n#0  0x00002aaadcff3200 in PyArray_API ()\n   from /work/01813/roller/maverick/packages/python/lib/python2.7/site-packages/numpy/core/multiarray.so\n#1  0x00002aaaecd67e54 in _import_array ()\n    at /opt/_internal/cpython-2.7.11-ucs2/lib/python2.7/site-packages/numpy/core/include/numpy/__multiarray_api.h:1633\n#2  initspecfun () at build/src.linux-x86_64-2.7/scipy/special/specfunmodule.c:5830\n#3  0x00000000004d3858 in _PyImport_LoadDynamicModule ()\n#4  0x00000000004d0f89 in import_submodule ()\n#5  0x00000000004d1255 in ensure_fromlist ()\n#6  0x00000000004d2120 in PyImport_ImportModuleLevel ()\n#7  0x00000000004af654 in builtin___import__ ()\n#8  0x00000000004232ea in PyObject_Call ()\n#9  0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#10 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#11 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#12 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#13 0x00000000004d01de in load_source_module ()\n#14 0x00000000004d0f89 in import_submodule ()\n#15 0x00000000004d1f9f in PyImport_ImportModuleLevel ()\n#16 0x00000000004af654 in builtin___import__ ()\n#17 0x00000000004232ea in PyObject_Call ()\n#18 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#19 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#20 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#21 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#22 0x00000000004d01de in load_source_module ()\n#23 0x00000000004d14fc in load_package ()\n#24 0x00000000004d0f89 in import_submodule ()\n#25 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#26 0x00000000004af654 in builtin___import__ ()\n#27 0x00000000004232ea in PyObject_Call ()\n#28 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#29 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#30 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#31 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#32 0x00000000004d01de in load_source_module ()\n#33 0x00000000004d0f89 in import_submodule ()\n#34 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#35 0x00000000004af654 in builtin___import__ ()\n#36 0x00000000004232ea in PyObject_Call ()\n#37 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#38 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#39 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#40 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#41 0x00000000004d01de in load_source_module ()\n#42 0x00000000004d0f89 in import_submodule ()\n#43 0x00000000004d1f9f in PyImport_ImportModuleLevel ()\n#44 0x00000000004af654 in builtin___import__ ()\n#45 0x00000000004232ea in PyObject_Call ()\n#46 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#47 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#48 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#49 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#50 0x00000000004d01de in load_source_module ()\n#51 0x00000000004d14fc in load_package ()\n#52 0x00000000004d0f89 in import_submodule ()\n#53 0x00000000004d1f9f in PyImport_ImportModuleLevel ()\n#54 0x00000000004af654 in builtin___import__ ()\n#55 0x00000000004232ea in PyObject_Call ()\n#56 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#57 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#58 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#59 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#60 0x00000000004d01de in load_source_module ()\n#61 0x00000000004d0f89 in import_submodule ()\n#62 0x00000000004d1f9f in PyImport_ImportModuleLevel ()\n#63 0x00000000004af654 in builtin___import__ ()\n#64 0x00000000004232ea in PyObject_Call ()\n#65 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#66 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#67 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#68 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#69 0x00000000004d01de in load_source_module ()\n#70 0x00000000004d14fc in load_package ()\n#71 0x00000000004d0f89 in import_submodule ()\n#72 0x00000000004d1f9f in PyImport_ImportModuleLevel ()\n#73 0x00000000004af654 in builtin___import__ ()\n#74 0x00000000004232ea in PyObject_Call ()\n#75 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#76 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#77 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#78 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#79 0x00000000004d01de in load_source_module ()\n#80 0x00000000004d0f89 in import_submodule ()\n#81 0x00000000004d1255 in ensure_fromlist ()\n#82 0x00000000004d2120 in PyImport_ImportModuleLevel ()\n#83 0x00000000004af654 in builtin___import__ ()\n#84 0x00000000004232ea in PyObject_Call ()\n#85 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#86 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#87 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#88 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#89 0x00000000004d01de in load_source_module ()\n#90 0x00000000004d0f89 in import_submodule ()\n#91 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#92 0x00000000004af654 in builtin___import__ ()\n#93 0x00000000004232ea in PyObject_Call ()\n#94 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#95 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#96 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#97 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#98 0x00000000004d01de in load_source_module ()\n#99 0x00000000004d14fc in load_package ()\n#100 0x00000000004d0f89 in import_submodule ()\n#101 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#102 0x00000000004af654 in builtin___import__ ()\n#103 0x00000000004232ea in PyObject_Call ()\n#104 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#105 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#106 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#107 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#108 0x00000000004d01de in load_source_module ()\n#109 0x00000000004d14fc in load_package ()\n#110 0x00000000004d0f89 in import_submodule ()\n#111 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#112 0x00000000004af654 in builtin___import__ ()\n#113 0x00000000004232ea in PyObject_Call ()\n#114 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#115 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#116 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#117 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#118 0x00000000004d01de in load_source_module ()\n#119 0x00000000004d14fc in load_package ()\n#120 0x00000000004d0f89 in import_submodule ()\n#121 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#122 0x00000000004af654 in builtin___import__ ()\n#123 0x00000000004232ea in PyObject_Call ()\n#124 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#125 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#126 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#127 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#128 0x00000000004d01de in load_source_module ()\n#129 0x00000000004d14fc in load_package ()\n#130 0x00000000004d0f89 in import_submodule ()\n#131 0x00000000004d1255 in ensure_fromlist ()\n#132 0x00000000004d2120 in PyImport_ImportModuleLevel ()\n#133 0x00000000004af654 in builtin___import__ ()\n#134 0x00000000004232ea in PyObject_Call ()\n#135 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#136 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#137 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#138 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#139 0x00000000004d01de in load_source_module ()\n#140 0x00000000004d14fc in load_package ()\n#141 0x00000000004d0f89 in import_submodule ()\n#142 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#143 0x00000000004af654 in builtin___import__ ()\n#144 0x00000000004232ea in PyObject_Call ()\n#145 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#146 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#147 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#148 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#149 0x00000000004d01de in load_source_module ()\n#150 0x00000000004d14fc in load_package ()\n#151 0x00000000004d0f89 in import_submodule ()\n#152 0x00000000004d204b in PyImport_ImportModuleLevel ()\n#153 0x00000000004af654 in builtin___import__ ()\n#154 0x00000000004232ea in PyObject_Call ()\n#155 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#156 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#157 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#158 0x00000000004cfe98 in PyImport_ExecCodeModuleEx ()\n#159 0x00000000004d01de in load_source_module ()\n#160 0x00000000004d14fc in load_package ()\n#161 0x00000000004d0f89 in import_submodule ()\n#162 0x00000000004d1f9f in PyImport_ImportModuleLevel ()\n#163 0x00000000004af654 in builtin___import__ ()\n#164 0x00000000004232ea in PyObject_Call ()\n#165 0x00000000004b4b40 in PyEval_EvalFrameEx () at Python/ceval.c:4219\n#166 0x00000000004baf7b in PyEval_EvalCodeEx () at Python/ceval.c:3582\n#167 0x00000000004bb089 in PyEval_EvalCode () at Python/ceval.c:669\n#168 0x00000000004e430f in PyRun_SimpleStringFlags () at Python/pythonrun.c:1370\n#169 0x0000000000415b90 in Py_Main ()\n#170 0x000000389b41ed5d in __libc_start_main () from /lib64/libc.so.6\n#171 0x0000000000415179 in _start ()\n```\n", "The only thing I've figured out that commenting out the call to `tensorflow::ImportNumpy();` in `tf_session.i:33` keeps the segfault from occurring, but then inevitably get a segfault when trying to actually do anything in TF (like call someconstant.eval()).\n", "Hey, progress! I downgraded to NumPy 1.8.2 and the issue is no longer there.\n\nSo now the problem seems to be either:\n- Something about import_array1() broke between NP1.8 and NP1.11\n- OR, the sysadmins on my system have installed an older copy of numpy with headers in /usr/include, etc. The build process could be picking up those instead, and causing some gross conflict somewhere.\n\nI'm investigating each of these.\n", "No, it wasn't either. Turns out the benefit of downgrading was that I uninstalled scipy, not that I downgraded numpy.\n\nWith judicious pdb.set_trace()'s, I've determined:\n1. tensorflow imports tensorflow.contrib.learn\n2. tf.c.learn tries to load scikit-learn\n3. scikit-learn loads scipy.special for some optimization functions\n4. scipy.special loads a f2py module scipy.special.specfun\n5. scipy.special.specfun calls import_array, but _after_ tensorflow has already\n", "Thanks @stephenroller: I don't like the fact that importing tf automatically imports scikit-learn.  @martinwicke: Has that ship sailed?  For a long time we were explicitly working to keep tensorflow's scientific dependencies to just numpy.\n\n@stephenroller: Am I correct that the problem would go away if `tf.contrib.learn` explicitly imported numpy before scikit learn?   \n", "Yeah, it seems wrong that skflow is being automatically loaded. If nothing else, it's adding a whole bunch of unnecessary startup time to a submodule that may not ever be used...\n\nHowever, it's explicitly not a dependency: it's wrapped in an ImportError that has smart fallback behavior. The fallback behavior doesn't trigger the segfault, as it doesn't have these cascading imports to this obscure module.\n\nBut explicitly importing numpy in tf.contrib.learn wouldn't help; numpy is imported nearly a dozen times before then, by scikit-learn, scipy _and_ tf.contrib.learn.\n\nLooking at the specfun module, it calls import_array and doesn't properly define a PY_ARRAY_UNIQUE_SYMBOL like all c modules are supposed to do. And I _think_ TensorFlow is doing things in the wrong order, either in `numpy.c` or `numpy.h`. The result is that you have both modules slightly misbehaving about import_array and overwriting some function pointers somewhere, hence the segfault. I'm still confirming this though.\n", "+Illia Polosukhin ipolosukhin@google.com\n\nLet's disable that. It should automatically load the mock base class for\nestimator, I think that's the only one we need. We already have conditional\nloading there anyway (it only loads scipy if available).\n\nThat does seem like a pretty terrible bug in scipy though?\n\nOn Fri, Apr 29, 2016 at 11:45 AM Stephen Roller notifications@github.com\nwrote:\n\n> Yeah, it seems wrong that skflow is being automatically loaded. If nothing\n> else, it's adding a whole bunch of unnecessary startup time to a submodule\n> that may not ever be used...\n> \n> However, it's explicitly not a dependency: it's wrapped in an ImportError\n> that has smart fallback behavior. The fallback behavior doesn't trigger the\n> segfault, as it doesn't have these cascading imports to this obscure module.\n> \n> But explicitly importing numpy in tf.contrib.learn wouldn't help; numpy is\n> imported nearly a dozen times before then, by scikit-learn, scipy _and_\n> tf.contrib.learn.\n> \n> Looking at the specfun module, it calls import_array and doesn't properly\n> define a PY_ARRAY_UNIQUE_SYMBOL like all c modules are supposed to do. And\n> I _think_ TensorFlow is doing things in the wrong order, either in numpy.c\n> or numpy.h. The result is that you have both modules slightly misbehaving\n> about import_array and overwriting some function pointers somewhere, hence\n> the segfault. I'm still confirming this though.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-215845406\n", "Ok, I'll remove importing `sklearn` at all. And probably we can incorporate some of the functionality then into our own BaseEstimator later.\nThe only issue is with people checking stuff for things like `isinstance(obj, sklearn.BaseEstimator)` will stop working. I guess we are not very interested in supporting this anyway. I'll send a PR later today.\n", "@stephenroller: Can you expand on the fact that we're doing things in the wrong order?  I'd love to fix it if we're doing it wrong.  If it's just scikit-learn, we should probably file a bug for them as well. \n", "I don't know, this is really frustrating. It's definitely doesn't look like a scikit-learn bug: all scikit-learn does is try to import scipy.special for some highly optimized mathematical functions (like expit, digamma, etc), and falls back on naive implementations if scipy isn't installed. scipy.special imports scipy.special.specfun, which is a c module where the segfault occurs.\n\nThen the issue is the call to import_array in the specfun module. But the specfun module in question is actually fully, automatically generated from f2py: all of that logic about the import_array is occurring in f2py, not in the module causing the crash. And f2py is maintained by the NumPy people since 2007 and the logic around import_array is untouched for 6 years, and the scipy.special.specfun module which is causing a crash hasn't been touched in three years according to the commit logs. So I just feel these people are inclined to get that logic right, and if not, that bug would have surfaced sometime in the past 3 or 6 years.\n\nBut at the same time, try as I can, I can't figure out where the problem is in TensorFlow.\n", "I found where/why it's happening. I don't know the solution though.\n\nIf I modify [tensorflow/python/**init**.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/__init__.py#L37), starting at line 43:\n\nThis crashes:\n\n``` python\n_default_dlopen_flags = sys.getdlopenflags()\nsys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\nprint \"before import pywrap_tf\"\nfrom tensorflow.python import pywrap_tensorflow\nprint \"after import pywrap_tf\"\nsys.setdlopenflags(_default_dlopen_flags)\nfrom scipy.special import *\n\nprint \"Exiting gracefully.\"\nsys.exit(0)\n```\n\nSo the sys.exit(0) happens before any call to contrib, and therefore before scikit-learn. It's definitely not scikit-learn then.\n\nAfter trial and error, I figured out it's the call to setdlopenflags, which I have no idea what it does. Here's a minimum use case:\n\n```\n$ python\nPython 2.7.11 (default, Apr 25 2016, 20:02:17)\n[GCC 4.9.1] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sys, ctypes\n>>> flags = sys.getdlopenflags()\n>>> sys.setdlopenflags(flags | ctypes.RTLD_GLOBAL)\n>>> from scipy.special import *\nzsh: segmentation fault (core dumped)  python\n```\n\nSo the moment you've called import_array with the RTLD_GLOBAL flag up, something seems to get messed up royally. Maybe because scipy doesn't bother with a PY_ARRAY_UNIQUE_SYMBOL?\n\nI'm tempted to hack in a PY_ARRAY_UNIQUE_SYMBOL thingy into f2py and see what happens.\n", "Which brings it back to this issue, which someone else referenced https://github.com/numpy/numpy/issues/2521.\n\nEdit: Sorry for spamming everybody. I followed the advice of [this SO post](http://stackoverflow.com/questions/20849755/python-shared-libraries-rtld-global-segfault/21096879#21096879), which suggests recompiling scipy. This seems to entirely resolve the issue. @pulkitag, did you happen to reinstall scipy by rebuilding it somehow?\n\nSo to sum up: the bug occurs if you're using Linux (OS X doesn't have `sys.setdlopenflags`), you've compiled TensorFlow from source, but you are using binary installations of scipy. It hits the RedHat/CentOS people particularly hard because they all have to compile TF from source.\n", "The `setdlopenflags` is necessary to get imports of multiple Python packages using the same underlying C++ libraries to share copies of the libraries (in particular their global variables).\n\nIt's not optimal, but given that we don't want to depend on a fix to scipy: should we add an `import numpy` before we do our the pywrap import as a workaround?  If you add `import numpy` to the top of `tensorflow/python/__init__.py`, does it fix the problem?\n", "Also, @stephenroller: thank you so much for the detailed investigation! \n", "https://github.com/tensorflow/tensorflow/pull/2173 \n", "After I remove scikit-llean and scikit-image, tensorflow works fine with the mnist samples in this page:\nhttps://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#train-your-first-tensorflow-neural-net-model\n", "@fivejjs: Are you using the version of the code from git after this bug was fixed?\n", "After the issue closed. It was yesterday.\n\nBoth souce code compile and the pip GPU install got Segmentation fault (core dumped).\n\nSource compiled for cudnn V5 and cuda 7.5 then try cudnn v4 with cuda 7.5. \n", "Please file a separate bug and give us more information.  Does importing numpy before tensorflow fix your problem?  If it doesn't, your issue is unrelated to this bug.  A stack trace would be ideal.\n", "Thank you @girving \nI deleted all the installed code and changed the installation sequence to install again. This solved my problem. You can close #2248 now.\n", "I think I have the same problem:\n\n``` bash\n$ python\nPython 2.7.11+ (default, Apr 17 2016, 14:00:29) \n[GCC 5.3.1 20160413] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[1]    8944 segmentation fault (core dumped)  python\n```\n\n## My system\n- Ubuntu 16.04\n- Tensorflow 0.80\n- Cuda 7.5, CuDNN 4\n\nMore details:\n\n```\n$ uname -a\nLinux thinkpad 4.4.0-22-generic #39-Ubuntu SMP Thu May 5 16:53:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\n$ cat /etc/issue\nUbuntu 16.04 LTS \\n \\l\n\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\n\n$ cat /usr/include/cudnn.h | grep 'define CUDNN_MAJOR' -A 2\n#define CUDNN_MAJOR      4\n#define CUDNN_MINOR      0\n#define CUDNN_PATCHLEVEL 7\n\n$ pip show tensorflow\n---\nMetadata-Version: 2.0\nName: tensorflow\nVersion: 0.8.0\nSummary: TensorFlow helps the tensors flow\nHome-page: http://tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: opensource@google.com\nInstaller: pip\nLicense: Apache 2.0\nLocation: /usr/local/lib/python2.7/dist-packages\nRequires: numpy, six, protobuf, wheel\nClassifiers:\n  Development Status :: 4 - Beta\n  Intended Audience :: Developers\n  Intended Audience :: Education\n  Intended Audience :: Science/Research\n  License :: OSI Approved :: Apache Software License\n  Programming Language :: Python :: 2.7\n  Topic :: Scientific/Engineering :: Mathematics\n  Topic :: Software Development :: Libraries :: Python Modules\n  Topic :: Software Development :: Libraries\nEntry-points:\n  [console_scripts]\n  tensorboard = tensorflow.tensorboard.tensorboard:main\n\n$ glxinfo|egrep \"OpenGL vendor|OpenGL renderer\"\nOpenGL vendor string: NVIDIA Corporation\nOpenGL renderer string: GeForce 940MX/PCIe/SSE2\n```\n\n## Debugging\n\n```\n$ gdb python\nrun tf.py\n```\n\nwith `tf.py`:\n\n```\nimport tensorflow as tf\n```\n\ngives\n\n```\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffd6ee7700 (LWP 8658)]\n[New Thread 0x7fffd66e6700 (LWP 8659)]\n[New Thread 0x7fffd3ee5700 (LWP 8660)]\n[New Thread 0x7fffd16e4700 (LWP 8661)]\n[New Thread 0x7fffccee3700 (LWP 8662)]\n[New Thread 0x7fffca6e2700 (LWP 8663)]\n[New Thread 0x7fffc7ee1700 (LWP 8664)]\n\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x00007fffd9bc3220 in PyArray_API ()\n   from /home/moose/.local/lib/python2.7/site-packages/numpy/core/multiarray.so\n```\n\n## However\n\nImporting tensorflow by keras works:\n\n```\n$ python\nPython 2.7.11+ (default, Apr 17 2016, 14:00:29) \n[GCC 5.3.1 20160413] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import keras\nUsing TensorFlow backend.\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n>>> import tensorflow\n>>>\n```\n", "This bug is fixed.  If you have the same problem, please try with a version of TensorFlow after the fix.  If it is a different problem, please file a separate bug.\n", "While there was workaround for importing numpy before tensorflow, if you want a solution which works so you can run tensorboard.  (on ubuntu) `pip uninstall scipy` (and numpy) and then `apt-get install python-scipy` (and python-numpy).  This will cause compatible versions of both to be installed which match the binary file provided.\n", "Got the segmantation as well on running TensorBoard, updating numpy and scipy like @jrock08 said solved the issue for me. Thank you!\n", "Is there a solution for this when running in a virtualenv?\n", "I believe for virtualenv, you would need to install the correct numpy (1.8.2) and scipy (0.13.3) since those match the ubuntu repos.  (full disclosure: I haven't checked if this works)\n", "I had the same problem in Ubuntu 14.04, with TF 0.8.\nInstalling inside a virtualenv worked for me and it wasn't necessary to mess with numpy or scipy versions.\n", "@jrock08 @stephenroller  I meet the same problem and I tried to solve it by recompiling scipy and numpy, but the problem is still there. i.e. If I do `import tensorflow` before `import numpy` or `import scipy`, it gives me `segmentation fault`. But if I do `import tensorflow` after `import numpy` or `import scipy`. It works well. Any idea about this?\n\nFor your information, I install tensorflow0.8.0 by \n\n`pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl --user`  (I cannot do `sudo` install since I work on department computer)\n\nI reinstalled `scipy` and `numpy` by:\n`pip uninstall scipy`, `pip install scipy --upgrade --user` and the same for `numpy`,\nhowever, when I uninstall scipy, I met the following warning:\n`DEPRECATION: Uninstalling a distutils installed project (scipy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.`\nDoes it matter?\nAnd after reinstallation, I have scipy0.17.1 and numpy1.11.0\n\nMy system and python version are:\nPython 2.7.5 (default, Oct 11 2015, 17:47:16) \n[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2\n\n@girving I just installed tensorflow yesterday so I don't know why it still doesn't work as you said the bug is fixed...Did you mean the bug is fixed for other system versions?\n", "@gladys0313: I believe we haven't released a version since this fix.  @martinwicke: Is that correct?  \n", "Once before I have also encountered this problem. In my case it's related to the readline package. I reinstalled it from the readline-0.6.0.tar.gz. Then recompile the Python-2.7, then the error is gone. Maybe you can have a try.\n", "@girving This issue is in r0.8.\n\n@gladys0313: To fix everything you can match the numpy (1.8.2) and scipy (0.13.3) versions that ubuntu uses that would be `pip install numpy=1.8.2 scipy=0.13.3` or install from apt-get and don't install from pip.    The problem is due to updates in numpy and/or scipy  (as you've said you have scipy0.17.1 and numpy1.11.0) that changed some behavior, you have to use the right versions.  If you only care about running tensorflow, just import numpy (and scipy) before tensorflow and you are good.  If you want to use tensorboard, you have to match the numpy and scipy versions.\n", "We have not released since so unless you're using a nightly or building\nfrom source it's not surprising it's still an issue.\nOn Sun, May 22, 2016 at 09:36 zszhong notifications@github.com wrote:\n\n> Once before I have also encountered this problem. In my case it's related\n> to the readline package. I reinstalled it from the readline-0.6.0.tar.gz.\n> Then recompile the Python-2.7, then the error is gone.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-220842034\n", "@girving @martinwicke  Thanks for your clarification!\n", "@jrock08 Thanks for your clear explanation! Hmm...I may need some functions in higher version scipy so maybe I just import numpy/scipy before tensorflow at current stage...\n", "@zszhong  Hey, thanks for your comment! But are you sure about your readline version? I just found readline 2.4.2 as the oldest version. I tried by reinstalling the newest version but the error is still there, maybe in my case it doesn't work with readline... \n", "@gladys0313 I'm not sure if it is because of readline. But when I reinstall the readline-6.0.0 and use it to recompile the python. The error is gone. I didn't do anything else. Thus I doubt it is related to the readline. In my case, the error is when I run `python` in a terminal, it outputs `Segment fault`. And when I recompiled it, it can run OK. I use readline-6.0, did you try readline-6.0? not readline-6.3. In my case, reahat 6.5, gcc-4.8.4, python 2.7, readline-6.0\n", "@zszhong Okay I see..I will try later, I was just wondering how you realized that you should reinstall readline? For me it seems a very random discovery...\n", "@gladys0313 , I can't remember the details. Maybe after some googling, and got some hints. And then I tried, then it worked. \n", "Confirming that importing _numpy_ **before** _tensorflow_ fixes it. (ok, it's more of a workaround than a fix)\n\nFor me it started seg faulting after I installed scipy in an otherwise clean virtualenv on Ubuntu 15.04. \n", "In my case, `pip install scikit-image` fixed it.\n", "Probably because that updated scipy or numpy as a side effect.\nOn Sat, May 28, 2016 at 20:59 davebs notifications@github.com wrote:\n\n> In my case, _pip install scikits.ndimage_ fixed it.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2034#issuecomment-222341508,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_Y81C43-2fWqec_YERAKYwrI2jPvks5qGQ8agaJpZM4ILdja\n> .\n", "A quick-fix for this might be to install version 0.7:\n\n```\npip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp34-none-linux_x86_64.whl\n```\n\nor any other version found here: https://storage.googleapis.com/tensorflow/ \n\nThis worked for me, that is on 0.8.0 `python3 -c 'import tensorflow'` resulted in a segmentation fault, whereas 0.7.1 did not.\n", "For anyone who finds this thread: TensorFlow 0.9 has the workaround.\n", "Actually, I have the same issue in Tensorflow 0.9 :-1:\n", "@amineHorseman Are you sure you have the latest version (`print(tensorflow.__version__)`)?\nFor me, it works. I can now import tf without importing np before.\n", "@MartinThoma yes, I'm sure about the version.\nI installed tensorflow-0.9.0rc0 from source in conda environment and uninstalled the older version. The current configuration is: python 2.7.11, cuda 7.5, cudnn 5, OS X 11.10.5.\nEDIT: importing numpy or not didn't fix the problem\n", "@amineHorseman If importing numpy doesn't fix it, please file a different issue.  It is probably unrelated, unless you have reason to believe otherwise.\n", "Is there any update now?  I found there was no such problem in pyCharm IDE (with sklearn installed) \n\n> > import tensorflow as tf\n> > import numpy as np\n\nBut the same code cann't work in ipython, Dont know why\n", "For everyone using RHEL/Centos 6.x, you might have to build tensorflow from source by installing bazel first.\r\nHere's how I got past this issue on RHEL 6.8:\r\n\r\n# TensorFlow (Installation on RHEL/Centos 6.x):\r\n## 1) Enable Repos for Installing python2.7\r\n CentOS 6 `yum install centos-release-scl-rh`\r\n RHEL 6\t`yum-config-manager --enable rhel-server-rhscl-6-rpms`\r\n\r\nInstall Software Collections in Scientific Linux 6Shell\r\n\r\n`yum install \"http://ftp.scientificlinux.org/linux/scientific/6/external_products/softwarecollections/yum-conf-softwarecollections-1.0-1.el6.noarch.rpm\"`\r\n\r\nSet `gpgcheck=0` in /etc/yum.repos.d/softwarecollections.repo\r\n\r\n## 2) Install python2.7\r\n`yum install python27 python27-numpy swig python27-python-devel python27-python-wheel python27-pip`\r\n\r\n## 3) Install Developer Toolset\r\n`yum install devtoolset-4\r\n`\r\n## 4) Build and Install Bazel\r\nEnter Software Collection environment with Developer Toolset.\r\n`scl enable devtoolset-4 bash\r\n`\r\nPrepare Source Code\r\nclone source code repository\r\n```\r\ngit clone https://github.com/bazelbuild/bazel.git\r\ncd bazel\r\n```\r\nselect version\r\n`git checkout 0.3.1\r\n`\r\nBuild and Install\r\n\r\ncompile\r\n`./compile.sh\r\n`\r\ninstall\r\n```\r\nmkdir -p ~/bin\r\ncp output/bazel ~/bin/\r\n```\r\n\r\n exit from Software Collection environment\r\n`exit`\r\n## 5) Build TensorFlow with Bazel\r\nEnter Build Environment\r\n`scl enable devtoolset-4 python27 bash`\r\nPrepare Source Code\r\n clone source code repository\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n```\r\nselect version \r\n`git checkout v0.10.0`\r\n\r\n## 6) Since GNU C library version in CentOS 6 is less than 2.17, a slight modification needs to be applied before compilation.\r\n\r\nModify **tf_extension_linkopts** function in **tensorflow/tensorflow.bzl** **FROM**\r\n\r\n```\r\ndef tf_extension_linkopts():\r\n  return []  # No extension link opts\r\n```\r\n **TO**\r\n```\r\ndef tf_extension_linkopts():\r\n  return [\"-lrt\"]\r\n```\r\n\r\n## 7) Build\r\n configure workspace, just leave everything as default\r\n`./configure`\r\n build\r\n```\r\n~/bin/bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\n exit from Software Collection environment\r\n`exit\r\n`\r\n## 8) Install tensorflow via pip_package\r\n```\r\ncd /tmp/tensorflow_pkg\r\npip install --upgrade tensorflow-0.10.0-cp27-none-linux_x86_64.whl\r\n```\r\n## 9) Test tensorflow\r\n```\r\npython2.7\r\nPython 2.7.12 (default, Jun 28 2016, 17:49:40)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n\r\nexit()\r\n```\r\nHope this helps!!!", "I had a similar problem. And I've solved that problem by upgrading matplotlib.\r\n\r\n`sudo pip install --upgrade matplotlib`", "How to solve the similar problem as follow, I try to upgrade matplotlib and so on, but they are failed.\r\n[llliao@GPU-1-10 ~]$ python3\r\nPython 3.5.4 (default, Aug  8 2017, 11:09:21) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess=tf.Session()\r\n2018-01-13 20:00:56.735985: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-13 20:00:57.053726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 11.17GiB freeMemory: 4.29GiB\r\n2018-01-13 20:00:57.350308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 11.17GiB freeMemory: 2.58GiB\r\n2018-01-13 20:00:57.658419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 2 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.17GiB freeMemory: 6.35GiB\r\nSegmentation fault (core dumped)"]}, {"number": 2033, "title": "CudNN error running TensorFlow: Could not set cudnn filter descriptor: CUDNN_STATUS_BAD_PARAM", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -l /usr/local/cuda-7.5/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 16  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 59823168 Apr 19 15:15 /usr/local/cuda-7.5/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 59823168 Apr 19 15:15 /usr/local/cuda-7.5/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 59823168 Apr 19 15:15 /usr/local/cuda-7.5/lib64/libcudnn.so.5.0.4\n-rw-r--r-- 1 root root 58734618 Apr 19 15:15 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n```\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0rc0\n```\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n1. When I run the deep convolutional nn tutorial from TensorFlow's website, I get the following error:\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow dev\\\nice (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\nSuccessfully loaded: saved_networks/dqn-2920000\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not set cudnn filter d\\\nescriptor: CUDNN_STATUS_BAD_PARAM\n```\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["To use cudnn 5 you have to build from sources -- cudnn5 is not binary compatible with cudnn4, and our binaries use the latest official cudnn release (which is 4).\n", "Short form: \n@vrv I'd argue that the tensorflow.org should be update to reflect that, here: \nhttps://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#optional-install-cuda-gpus-on-linux\n\nThe following section is misleading, as it makes it sound like it should \"just work\" with other versions: \n\n> Download and install cuDNN\n> \n> https://developer.nvidia.com/cudnn\n> \n> Uncompress and copy the cuDNN files into the toolkit directory. Assuming the toolkit is installed in /usr/local/cuda, run the following commands (**edited to reflect the cuDNN version you downloaded**):\n> \n> tar xvzf cudnn-6.5-linux-x64-v2.tgz\n> sudo cp cudnn-6.5-linux-x64-v2/cudnn.h /usr/local/cuda/include\n> sudo cp cudnn-6.5-linux-x64-v2/libcudnn\\* /usr/local/cuda/lib64\n> sudo chmod a+r /usr/local/cuda/lib64/libcudnn*\n\nBolding for emphasis.   No indication that you need to specifically download cudnn4 for that. The rest of the page merely says cudnn > 2.0.  Also the versioning makes that more confusing than it needs to be (cudnn4 -> 6.5  cudnn5 -> 7.x, and the only way I know is by looking at the actual filenames in the download links) \n\nOn https://developer.nvidia.com/cudnn\n\ncudnn5 is now the first thing you're presented,  so you actually have to scroll down to get to cudnn4 (which you don't know you have to do anyway), so I expect more tensorflow beginners will wind up doing the same (Download cudnn5, follow install instructions, get obscure error message)\n\nSeems like something the documentation could be more clear. \n", "https://github.com/tensorflow/tensorflow/pull/2065 ?\n", "Excellent!\n\nOn Fri, Apr 22, 2016 at 10:15 AM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> #2065 https://github.com/tensorflow/tensorflow/pull/2065 ?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2033#issuecomment-213516628\n", "I ran into this issue as well. Very confusing, please update documentation.\n", "I have the same problem. \n", "Me too...\n\nUbuntu: 16.04\nCuda: 8.0.27 RC\ncudnn: 5.0.5 (thrust issue?)\ntensorflow: 0.9.0 (my tensorflow is certainly built from source)\n\nthe error message is like:\n\n>  tensorflow/stream_executor/cuda/cuda_dnn.cc:422] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n\nAny solutions ?\n\nThank you very much....\nPei\n", "I have similar problem.\n\nOS: Ubuntu 14.04\nCuda: 7.5\ncudnn: v4 for 7.0\ntensorflow: 0.10\n\nerror message: F tensorflow/stream_executor/cuda/cuda_dnn.cc:423] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n\nIt seems like every time 1 epoch training finishes, tensorflow will fail with this message.\n\nAny thought about this?\n\nThanks.\n", "I am having the same issue, however as far as I can tell it only shows up when I am using conv3d (conv2d works fine), after one epoch.\n\nOS: Ubuntu 16.04\nCUDA: 7.5\nCUDNN: Tried 4.0, 5.0 and 5.1 (and the corresponding tensorflow binaries)\n\nAny thoughts? I really would like to get this sorted out as quickly as possible.\n\nThanks.\n", "Same thing here:\nWith Ubuntu 14.04\nCuda: 7.5\nCUDNN: 5.0\nTrain on 16064 samples, validate on 1785 samples\nEpoch 1/10\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not set cudnn filter descriptor: CUDNN_STATUS_BAD_PARAM\nAborted (core dumped\n", "Similar for me.\r\nCuda 8.0, CUDNN 5.1, Tensorflow 0.12\r\n\r\nMulti-gpu version of my project has issues.\r\nSingle GPU version works fine.\r\n\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\n\r\nEven when I install cudnn v4, I get the following error\r\n_E tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 4007 (compatibility version 4000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration._\r\nF tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAborted (core dumped)\r\n\r\n", "If you have the error `CUDNN_STATUS_BAD_PARAM`, make sure that the problem does not come from your code. For example, it happened to me because I gave an empty array as an input to the tensorflow session after the SECOND epoch. My `next_batch()` function had a bug. This was the reason. I'm running v5 with TF 0.9 built from the sources and it works nicely now.\r\n@tongda @vguizilini @mpkuse @oakkas @achaiah @fancyerii ", "is this error still relevant for tensorflow 1.0.0?", "E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n\r\nMAC Tensorflow1.0 GTX650m  cudnn5.0 cuda8.0 . \r\nHow to slove this problem?>>", "Has anyone solved this problem ?", "Guys make sure your code is bug free. In most of the cases, we suspect this error to come from your code, rather than TF itself. I had this error because I was given an empty array to TF.", "To use cudnn 5 you have to build from sources -- cudnn5 is not binary compatible with cudnn4, and our binaries use the latest official cudnn release (which is 4).", "why not update to cudnn 5?\r\nI mean all of us will install new version cudnn when first install  tensorflow ", "I have the same problem too, can't someone really answer the question?", "Hi, I'm getting the same error.\r\n```\r\ntensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\r\n```\r\nI'm using 3D convolutions as well like @vguizilini . Also, the current documentation says cudnn 5.1 is supported.", "I tried, still getting it.\n\n2017-05-03 14:45 GMT+08:00 Syed Tousif Ahmed <notifications@github.com>:\n\n> Hi, I'm getting the same error.\n>\n> tensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\n>\n> I'm using 3D convolutions as well like @vguizilini\n> <https://github.com/vguizilini> . Also, the current documentation says\n> cudnn 5.1 is supported.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2033#issuecomment-298834171>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYWB4_MrXu8cSKAVNcbAcFXsbEsYbL_0ks5r2CJ3gaJpZM4ILXvL>\n> .\n>\n", "still happening on Windows 10 64 bit. GPU is GTX 1060 3Gb", "@supamonkey2000 run a very tensorflow script like MNIST CNN and check if you have this error.", "@philipperemy I ran the MNIST tutorial with no errors.", "@supamonkey2000 so the problem comes from your code. Check every line carefully.\r\n\r\nI had this error because I was given an empty array to the TF session.", "I had same error because of accidentally doing:\r\n`tf.layers.conv2d(net, filters=0)` ", "@philipperemy  was right, we didn't hit this issue when we just had one GPU, but when we trained our model with multiple GPUs, the error kicked in. You need to calculate your batch size for each GPU carefully. For example, if each batch size is 64 and you have 4 GPUs with 47 training samples, it will be split to 16, 16, 15, 0, the last batch will get zero sample. It's not obvious when there are many samples like our case.", "@nicolefinnie \r\nNicole is absolutely right. A safe strategy is to make your batch size dividable by the number of GPUs. ", "The ultimate solution i found was to reboot. It saves the fire. (2018 film Skyscraper)", "@cro888888 lol that's great!", "I was having this problem earlier trying to fit inception_v3 on cifar10. As noted in Keras' documentation, there is a minimum image size for each model. Make sure you're not fitting images that are too small for your selected model."]}, {"number": 2032, "title": "Fix Anaconda install instructions", "body": "Change to install python 3.4, and change title for consistency. Fixes #1990.\n", "comments": []}, {"number": 2031, "title": "python  trap divide error", "body": "# Environment info\n\nOperating System:CentOS 7\nGCC version:4.8.5\nCuda Toolkit:7.5\nCuda compiler driver:6.5\nBuilt with GPU\n# What have you tried?\n## I run /usr/lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py will print:Floating point exception\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: Tesla K40m\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\npciBusID 0000:04:00.0\nTotal memory: 11.25GiB\nFree memory: 3.08GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0)\nFailed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\nFloating point exception\n## I view the system message:\n\nApr 20 10:53:43 milan kernel: traps: python[66624] trap divide error ip:7fc0113304d2 sp:7fbf74ff0170 error:0 in _pywrap_tensorflow.so[7fc00f573000+a38a000]\n", "comments": ["\"Failed to get the number of CUDA devices: CUDA driver version is insufficient for CUDA runtime version\" from your logs sounds like a problem -- try to upgrade your cuda driver?\n", "(Or consider compiling from sources if you want to use an earlier CUDA runtime)\n"]}, {"number": 2030, "title": "Pass grid search params to TensorFlowEstimator custom model", "body": "GridSearchCV is a great way to test and optimize hyper-parameters automatically. I use it with TensorFlowEstimator to optimize learning_rate, batch_size, ...etc. It would be a great addition if I can also use it to customize other parameters in my custom model. \n\nFor example, say I have a custom model with a convnet and I want to optimize the stride value. This **pseudo code** explains what I'm trying to achieve. \n\n**I used a custom \"params\" input to the model function just as an example, not to imply that this is necessarily the right way to implement this feature.**\n\n```\n# My custom model. \n# Feature request: New params dict with values filled by GridSearchCV\ndef cnn_model(X, Y, params):\n  stride = params['stride']\n  ... custom model definition here ...\n\n# Create the Convnet classifier\ncnn_classifier = learn.TensorFlowEstimator(model_fn=cnn_model)\n\n# Grid search on different stride values.\nparameters = {'stride': [1, 2, 3],}\ngrid_searcher = GridSearchCV(cnn_classifier, parameters)\ngrid_searcher.fit(X, Y)\n```\n", "comments": ["It's on our TODO list. Just trying to figure out how to do it nicely to have a general way to pass hyper-parameters into the models.\n", "@ilblackdragon Any update on this?\n", "Model function has `params` argument. `TensorFlowEstimator` is deprecated, please use `Estimator` that takes `params` argument. This should work now, please re-open if this doesn't.\n"]}, {"number": 2029, "title": "question about LSTM cell state size", "body": "hi everyone:\nwith the config as below for a BasicLSTM cell:\n\n```\n   init_scale = 0.05\nlearning_rate = 0.01\nmax_grad_norm = 5\nnum_layers = 2\nnum_steps = 10\nhidden_size = 200\nkeep_prob = 0.5\nlr_decay = 0.8\nbatch_size = 30  \n```\n\nI use 2-hidden layers model:\n\nlstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\ncell = rnn_cell.MultiRNNCell([lstm_cell] \\* 2)\n\nThen: What is the cell.state_size?\nI got it as 30 x 800, but I can't understand how it come to?\n", "comments": []}, {"number": 2028, "title": "Upstream changes from internal", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2027, "title": "set cudnnSetFilter4dDescriptor format using filter layout for cudnn_v5", "body": "`CUDNN_TENSOR_NHWC` the only other cudnn filter format is still not supported since `kInputYXOutput` would correspond to `CUDNN_TENSOR_CHWN` which doesn\u2019t exist\n\nfor issue #1786 \n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins : test this please\n", "googlebot where are you?\n", "@googlebot, wakey wakey\n\n@willnorris in case you know why CLA check is not triggering anymore.\n", "I'll dig into it.  But in the meantime, this is fine to merge ([signcla/kashif](http://signcla/kashif))\n", "ah, looks like a race condition between when GitHub sent us the webhook and when we tried to fetch the commits for the pull request.\n\nAlso, I see that you have the CLA check required, so you can't just merge past this (probably not a good idea until we have better tools to manually override the CLA check).  In the meantime, @kashif would you mind leaving a comment here?  Doesn't matter what it is, but a comment from you will force the CLA check to run again.\n", "Actually I'm an admin so I can merge it forcefully -- I'd be happy to wait to get the official seal if you think it's for some reason better.\n", "We're already merging things we know are kosher (PRs auto-created from internal CLs, cherry-picks from another branch, ...), so I think there's no reason to wait if clabot has trouble re-checking this particular PR.\n", "Done.\n", "thanks @vrv \n"]}, {"number": 2026, "title": "One more version update for 0.8.0 final", "body": "", "comments": ["Are there other occurences of 0.8.0rc0? It's safe to assume they're all supposed to be converted (except in RELEASES.md).\n", "The timing is, as long as we don't deploy the website between this merge and the release, we're good.\n"]}, {"number": 2025, "title": "Constraint optimization?", "body": "Hello! Are there any plans for adding constraints to trainable variables? If not, I'd like to request it. Thanks!\n", "comments": ["You can do equality constraints by adding up gradients from 2 variables.\nIE, initialize them the same and do a+=(agrad+bgrad); b+=(agrad+bgrad)\n", "Closing due to lack of details/activity\n", "So it is like to use Lagrange Multipliers, just combine with new alphas.\n", "Is there a way to do proper constrained optimization? Besides using a Lagrangian relaxation?", "Maybe you would try KKT? (But I think it is also from Lagrangian....) ", "any beautiful solution?"]}, {"number": 2024, "title": "Fix wrong docstring", "body": "The keys in the _word2id are bytes literals.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2023, "title": "R0.8 tensorforest cherry-pick", "body": "This PR also contains a small amount of other changes pushed from internal.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Does this include the recent changes removing the flags?\n", "@martinwicke Yes.\n", "I see it does, never mind. Great.\n"]}, {"number": 2022, "title": "Fix wrong docstring", "body": "The keys in the _word2id are bytes literals.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 2021, "title": "bazel not respecting PYTHONPATH", "body": "### Environment info\n\nOperating System: Fedora 23\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nBuilt without GPU\n\nIf installed from sources, provide the commit hash:\nc303ff7\n### Steps to reproduce\n1.  bazel --output_base=/projects/.cache test //tensorflow/...\n### What have you tried?\n1.  starting python from command line and import tensorflow is ok.\n2.  the above command line produces tests shown below in the pastebin in the log section\n3.  echo $PYTHONPATH\n   /usr/local/lib/python3.4/site-packages:/projects/.cache/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorflow/tools/pip_package/build_pip_package.runfiles\n4.  I think there is an imports function recently added for bazel, would like some pointers to see if this is the best way to solve this problem and change all the bazel files (and not commit them to pull requests), or if there are better ways.\n   http://bazel.io/docs/be/python.html\n5. (EDIT) #1704 following comments did not work.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nhttp://pastebin.com/hTw53y2X\n", "comments": ["@davidzchen in case he knows what's going on here (or knows someone who might)\n", "Sorry for the delay.\n\nI have also seen this issue appear in the same setup -- building a target with Python protobuf while at the same time having Python protobuf installed locally on the machine. This seems to be the case even with the `imports` attribute set.\n\nI have opened bazelbuild/bazel#1193 to track this.\n", "This turned out to be a problem in the protobuf Bazel build setup. google/protobuf#1586 has been merged to fix this problem. Once tensorflow/tensorflow#1289 is merged to bump TensorFlow's dependency to the protobuf version that contains the fix, this problem will be fixed on the TensorFlow side as well.\n", "Trigger last updated\n"]}]