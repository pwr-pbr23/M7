[{"number": 48357, "title": "tensorflow-lite C++ example label_image failed for oemconfig.so and libhexagon_nn_skel.so", "body": "I compile tensorflow-lite and the example label_image in tensorflow-lite source code success,\r\nI could run success with delegates of NNAPI, XNNPACK, GPU and also CPU with ADB, but always fail with Hexagon.\r\nThe running comand for Hexagon is: ./label_image -m tflite_model_int8.tflite -i grace_hopper.bmp -l labels.txt -j 1\r\nThe CMD window only show following message and paused for long time:\r\nINFO: Loaded model tflite_model_int8.tflite\r\nINFO: resolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\nloaded libcdsprpc.so_\r\n\r\n\r\nThe log of \"adb logcat\" shows:\r\n04-07 14:52:43.607  3114  3114 I tflite  : Initialized TensorFlow Lite runtime.\r\n04-07 14:52:43.688  3114  3114 V ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1732: Successfully opened fastrpc_shell_3\r\n04-07 14:52:43.723  3114  3114 V ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1878: Successfully created user PD on domain 3 (attrs 0x0)\r\n04-07 14:52:43.724  3114  3117 V ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:277: FastRPC latency thread started for QoS\r\n04-07 14:52:43.728  3114  3115 E ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:745:Error 0x2: fopen failed for oemconfig.so. (No such file or directory)\r\n04-07 14:52:43.729  3114  3115 E ./label_image: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:745:Error 0x2: fopen failed for libhexagon_nn_skel.so. (No such file or directory)\r\n04-07 14:52:44.363   887  1096 E storaged: getDiskStats failed with result NOT_SUPPORTED and size 0\r\n04-07 14:52:46.371  1431  1431 E         : [eebbk_backup_restore]  get host by name error retry times: 245s!\r\n04-07 14:52:47.503  1433  1635 D BES     : AEEIOCTL_RT_MON_Kick IOCTL,cmd= 2147774474, lParam=300. \r\n\r\n\r\nI did copy an empty file \"oemconfig.so\" to the path of my executeable file label_image.\r\nAnd also I did download 3 different shared libraries \u201clibhexagon_nn_skel.so\u201d, \u201clibhexagon_nn_skel_v65.so\u201d, \u201clibhexagon_nn_skel_v66.so\u201d by hexagon_nn_skel.run and copy to the path of my executeable file label_image.\r\nShould I copy \"oemconfig.so\" and \u201clibhexagon_nn_skel.so\u201d to other path?\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 9.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BBK S5d (Chinese made pad for kid learning)\r\n- TensorFlow installed from (source or binary): build by myself\r\n- TensorFlow version (use command below): tensorflow-2.4.1\r\n- Python version: Python 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nCould anyone help? Or give a advice?\r\nThank you so much~\r\n", "comments": ["You don't need to copy oemconfig.so.\r\n\r\nThe error is here \r\n```\r\nintf/adsprpc/src/apps_std_imp.c:745:Error 0x2: fopen failed for libhexagon_nn_skel.so. (No such file or directory)\r\n```\r\n\r\nThis means you're not pushing the files to the correct location.\r\nTo run label_image example, you need to push the files to \"/data/local/tmp\"\r\n\r\n", "Closing. Please reopen or create new issue if you still have questions.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48357\">No</a>\n"]}, {"number": 48356, "title": "WGAN-GP Keras tutorial is slow to start the first epoch", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab and Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary in both cases\r\n- TensorFlow version (use command below): 2.4.1 in both cases\r\n- Python version: 3.9 on my computer, whatever google colab uses\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2.1/8.1.0.77 locally, whatever google colab uses\r\n- GPU model and memory: GeForce 1080, whatever google colab uses\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run the  [WGAN-GP tutorial from the Keras website](https://keras.io/examples/generative/wgan_gp/), I notice that the first epoch takes a really long time to start in both CPU and GPU, up to a minute.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe first epoch should start promptly, as it happens with the other tutorials. For example the [VAE](https://keras.io/examples/generative/vae/) and traditional [DCGAN](https://keras.io/examples/generative/dcgan_overriding_train_step/)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe [WGAN-GP colab](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/wgan_gp.ipynb). Choose a runtime accelerator and click on run all. The first epoch will always have a delay when starting training.\r\n\r\nThe [VAE colab](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb) and [DCGAN colab](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/dcgan_overriding_train_step.ipynb) do not exhibit this regression.\r\n\r\n**Other info**\r\n\r\nI can reproduce the same performance issue on my local computer, so this is not a colab issue.", "comments": ["Was able to reproduce the issue with TF v2.4, TF v2.5.0rc0 and TF-nightly. Please find the [gist](https://colab.research.google.com/gist/tilakrayal/01e5343d5c68634a3d9e38b436c46615/48356.ipynb) of it here. Thanks!", "@romanovzky I can reproduce the issue on my local but May be it is due to the size of the model (WGAN-GP). Did you notice this in any other models? Thanks!", "@jvishnuvardhan thanks for checking this out. I have no doubt that it might be not a bug, but just how heavy the model is. However, an equivalent model (also with gradient penalty) in pytorch is fast to start, so I'd just wanted to call the attention to this \"papercut\" from a user perspective (my workflow is TF, but my group has a growing interest in alternative frameworks and papercuts like this can easily shift the group's preference away from TF).", "@tomerk Any insights on the root-cause of \"slow-start\"? Thanks!", "@romanovzky As this is more related to keras-team/keras, I will close this issue here. Let's follow the progress there in keras-team/keras https://github.com/keras-team/keras/issues/14580. Thanks!"]}, {"number": 48355, "title": "Convert a model trained with TensorFlow1, and convert the model to TFLite with TensorFlow 2 ", "body": "Hi,\r\n\r\nIf the model is trained in TensorFlow1, is it possible to convert it to TensorFlow Lite model with the tool version 2.3.1 and be parsed with TFLite delegate parser?", "comments": ["@Rahn80643,\r\nIt is generally preferred to use the same TensorFlow version while building a model and converting it to `.tflite` format. Using different TensorFlow version for converting the model may result in unexpected behavior.\r\n\r\nHope this answers your query. Thanks!", "@amahendrakar \r\nThanks for your reply, I'm planning to modify my TensorFlow1 project to TensorFlow2.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48355\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48355\">No</a>\n"]}, {"number": 48354, "title": "Which cuda versions does tf-nightly 2.6 require?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tf-nightly-2.6.0.dev20210402\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Nvidia Driver: 460.39\r\n- CUDA version: cuda 11.0.2\r\n- cudNN version: libcudnn8=8.0.5.39-1+cuda11.0\r\n- GPU model and memory: GeForce RTX 3090 24GB\r\n\r\n**Describe the problem**\r\n\r\nWhen tensorflow attemps to load cuda libraries, it wants to load libcusolver.so.11. However cuda11.0 installs libcusolver.so.10.\r\nThe webpage of tf-nightly claims that cuda11.0 is the way to go. What are the correct dependencies? \r\n\r\nWhen using tensorflow 2.4.1 all libraries are loaded correctly. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n(venv) ...$ python3\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\\>>> import tensorflow as tf\r\n2021-04-07 09:26:18.837456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nINFO:tensorflow:Enabling eager execution\r\nINFO:tensorflow:Enabling v2 tensorshape\r\nINFO:tensorflow:Enabling resource variables\r\nINFO:tensorflow:Enabling tensor equality\r\nINFO:tensorflow:Enabling control flow v2\r\n\\>>> tf.\\_\\_version\\_\\_\r\n'2.6.0-dev20210402'\r\n\\>>> a = tf.constant(shape=(100), value=1)\r\n2021-04-07 09:26:41.014017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-04-07 09:26:41.062091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:21:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2021-04-07 09:26:41.062145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-04-07 09:26:41.064840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-04-07 09:26:41.064923: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-04-07 09:26:41.066001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-04-07 09:26:41.066298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-04-07 09:26:41.066485: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64\r\n2021-04-07 09:26:41.067051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-04-07 09:26:41.067192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-04-07 09:26:41.067209: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-04-07 09:26:41.067605: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-07 09:26:41.068940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-07 09:26:41.068989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \r\n", "comments": ["Ok. Found out myself with the command below. Would be nice to have this somewhere mentioned on the tf-nightly webpage.\r\nClosing the issue.\r\n\r\n\\>>> tf.sysconfig.get_build_info()\r\nOrderedDict([('cpu_compiler', '/usr/bin/gcc-5'), ('cuda_compute_capabilities', ['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']), ('cuda_version', '11.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48354\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48354\">No</a>\n"]}, {"number": 48353, "title": "Merge pull request #1 from tensorflow/master", "body": "Update on 9 Jan 2019", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48353) for more info**.\n\n<!-- need_sender_cla -->", "Closing this PR since no changes in files. Thanks!\r\ncc @mihaimaruseac"]}, {"number": 48352, "title": "Conv2d gives an error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):  9.3.0\r\n- CUDA/cuDNN version: 11.0 / 11.0.207\r\n- GPU model and memory: GeForce RTX 2060\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I try to run any model with Conv it raises an unexpected error. When I run the same code within an environment without gpu (cpu only) it works properly. \r\ntf.test.is_gpu_avialable returns True and it seems the gpu is installed properly. Other applications (ray rllib) can run on the gpu with no problem. \r\n\r\n**Describe the expected behavior**\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-13-4c2161bc3738> in <module>\r\n----> 1 model(pp)\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1010         with autocast_variable.enable_auto_cast_variables(\r\n   1011             self._compute_dtype_object):\r\n-> 1012           outputs = call_fn(inputs, *args, **kwargs)\r\n   1013 \r\n   1014         if self._activity_regularizer:\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)\r\n    422         a list of tensors if there are more than one outputs.\r\n    423     \"\"\"\r\n--> 424     return self._run_internal_graph(\r\n    425         inputs, training=training, mask=mask)\r\n    426 \r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)\r\n    558 \r\n    559         args, kwargs = node.map_arguments(tensor_dict)\r\n--> 560         outputs = node.layer(*args, **kwargs)\r\n    561 \r\n    562         # Update tensor_dict.\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1010         with autocast_variable.enable_auto_cast_variables(\r\n   1011             self._compute_dtype_object):\r\n-> 1012           outputs = call_fn(inputs, *args, **kwargs)\r\n   1013 \r\n   1014         if self._activity_regularizer:\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)\r\n    246       inputs = array_ops.pad(inputs, self._compute_causal_padding(inputs))\r\n    247 \r\n--> 248     outputs = self._convolution_op(inputs, self.kernel)\r\n    249 \r\n    250     if self.use_bias:\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in convolution_v2(input, filters, strides, padding, data_format, dilations, name)\r\n   1011     dilations=None,\r\n   1012     name=None):\r\n-> 1013   return convolution_internal(\r\n   1014       input,  # pylint: disable=redefined-builtin\r\n   1015       filters,\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in convolution_internal(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\r\n   1141         op = conv1d\r\n   1142 \r\n-> 1143       return op(\r\n   1144           input,\r\n   1145           filters,\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py in _conv2d_expanded_batch(input, filters, strides, padding, data_format, dilations, name)\r\n   2595     # We avoid calling squeeze_batch_dims to reduce extra python function\r\n   2596     # call slowdown in eager mode.  This branch doesn't require reshapes.\r\n-> 2597     return gen_nn_ops.conv2d(\r\n   2598         input,\r\n   2599         filter=filters,\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py in conv2d(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\r\n    930       return _result\r\n    931     except _core._NotOkStatusException as e:\r\n--> 932       _ops.raise_from_not_ok_status(e, name)\r\n    933     except _core._FallbackException:\r\n    934       pass\r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6860   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6861   # pylint: disable=protected-access\r\n-> 6862   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6863   # pylint: enable=protected-access\r\n   6864 \r\n\r\n~/.virtualenvs/trader/lib/python3.8/site-packages/six.py in raise_from(value, from_value)\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n# Create the base model from the pre-trained model ResNet50 V2\r\nIMG_SHAPE = (32, 32, 3)\r\nbase_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\r\n                                               include_top=False,\r\n                                               weights='imagenet')\r\nbase_model(sample)\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@rghelichi \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "here is the sample code:\r\nimport tensorflow as tf\r\n \r\n sample = tf.random.normal(shape=(32, 32, 3)) \r\n model = tf.keras.applications.ResNet50(input_shape=(32, 32, 3), include_top=False)\r\n print(model(sample[tf.newaxis, ...]))\r\n\r\nthis runs without gpu but with gpu gives me the error!", "@rghelichi,\r\n\r\nI ran the code shared on TF v2.4, TF v2.5.0rc0, TF nightly and do not see any error, please find the [gist here](https://colab.research.google.com/gist/tilakrayal/0a334cf74f39709bf9cbf0f2db7fc619/48352-2-4.ipynb).\r\n\r\nCould you please create a virtual environment and test your code again. It helps. Thanks!\r\n\r\n", "So the problem insisted I found on other issues \r\nhttps://github.com/tensorflow/tensorflow/issues/42738\r\na related error and when I tried one of the other users suggestions it worked\r\n\r\n```\r\ndevices = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(device[0], True)\r\n\r\n```\r\n\r\nwhat does this code do? is there sth I can do to avoid running these lines each time with my codes? ", "@rghelichi \r\nGlad the suggestion worked for you,  memory growth enables dynamic allotment of GPU memory, You only need to add these lines if your GPU memory size is falling short, It is not needed otherwise.\r\n\r\nPlease move this issue to closed status as it answers your question and for any further queries please open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48352\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48352\">No</a>\n"]}, {"number": 48351, "title": " [INTEL MKL] Curl upgrade to 7.76.0", "body": "", "comments": ["List of CVE's that are fixed in this release.\u00a0\r\n | Vulnerability | Date | CVSS v2 | CVSS v3 | Type\r\n\u00a0 | CVE-2020-8286 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n\u00a0 | CVE-2020-8231 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n\u00a0 | CVE-2020-8285 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n\u00a0 | CVE-2020-8169 | 2020/12/15 | 5.0 | 7.5 | Exact match\r\n\u00a0 | CVE-2020-8177 | 2020/12/15 | 4.6 | 7.1 | Exact match\r\n\u00a0 | CVE-2020-8284 | 2020/12/15 | 4.3 | 3.7 | Exact match\r\n\r\n\r\n\r\n\r\n", "FYI @penpornk ", "@rsketine Could you please help take a look at [Ubuntu CPU](https://source.cloud.google.com/results/invocations/902fa9fc-9be3-4d88-8d3d-ec5b4779097e/targets) and [Linux GPU](https://source.cloud.google.com/results/invocations/aed74cb9-6b0f-4c4d-a25b-09ed787f9163/targets) errors? Thank you! (Other CIs aren't done running yet.)", "@penpornk  added the change can you please retest."]}, {"number": 48350, "title": "divide_no_nan gives nans for small complex values in non-singleton tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.2.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`divide_no_nan` (and `divide`) give `nan` results when working with very small complex values in tensors with more than one element.\r\n\r\n**Describe the expected behavior**\r\nNeither `divide` nor `divide_no_nan` (but especially the latter) should give a `nan` when dividing a value by itself.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = tf.constant([1e-160, 1], tf.complex128)\r\ntf.math.divide_no_nan(x, x)\r\n```\r\nhttps://colab.research.google.com/drive/1I1vDbMhDYYncWMxX6jg4_nSyNmeb90z0?usp=sharing", "comments": ["@ymodak,\r\nI was able to reproduce the issue with TF v2.4.1, TF v2.5.0-rc0 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b34c7f06094d67479f37d8084e7804bd/48350.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48350\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48350\">No</a>\n"]}, {"number": 48349, "title": "What is the diference between NNAPI, GPU and Hexagon delegates", "body": "Could anyone describe the diference between NNAPI, NPU, GPU and Hexagon delegates?\r\nDoes it means NNAPI will use one delegate in NPU, GPU or Hexagon(DSP) according to device? If device only embedded GPU, NNAPI will use GPU; if device only embedded DSP, NNAPI will use DSP; and so on?\r\nOr does NNAPI only use NPU delegate?\r\n\r\nThank you so much~", "comments": ["@srjoglekar246 @karimnosseir could you take a look at this question?", "These are different types of TFLite delegates.\r\nI will try to explain below - and provide pointers. Please let me know if you have more questions.\r\n\r\n\r\n* What is TFLite Delegate ?\r\nTFLite delegates is a way to take part/all of the TFLite graph and execute it differently - in other words delegate the execution to something other than the default TFLite CPU kernels.\r\n\r\n* Why use TFLite Delegates ?\r\nRecently hardware vendors have specialized hardwares that can execute ML operations (or matrix operations) in an optimized way. Utilizing these specialized hardwares can benefit inference speed and power consumption.\r\n\r\n* What types of delegates does TFLite offers currently ?\r\nThere are types that are maintained by the TFLite team and others available open source (or closed source) that leverage TFLite Delegate framework.\r\nNote: You can search for types of delegates available online to use. Here below i will summarize the different delegates maintained by the TFLite team and you can find the code in the TF repo.\r\n\r\n\r\n- NNAPI Delegate:\r\nThis delegate is available only on Android devices starting Android 8.1 (API level 27).\r\nThis delegate uses Android Neural Network API which can accelerate the execution on different hardwares including GPU, DSP or NPU.\r\nNote that, although NNAPI can accelerate on GPU, or DSP it is not similar (exact) to the GPU delegate or Hexagon DSP delegate.\r\nFor more information and instructions on how to run it see the [guide](https://www.tensorflow.org/lite/performance/nnapi).\r\n\r\n- GPU Delegate:\r\nThis delegate supports running on OpenCL and OpenGL. It can run on Android or iOS.\r\nSee [guide](https://www.tensorflow.org/lite/performance/gpu) for more information on using it.\r\n\r\n- Hexagon Delegate:\r\nThis Delegate uses Hexagon NN framework which allows running ML operations on the Snapdragon Hexagon DSP.\r\nIt is available on Android and can be run also on non-android devices (e.g. Linux on Arm)\r\nSee the [guide](https://www.tensorflow.org/lite/performance/hexagon_delegate) for more information.\r\n\r\n- CoreML Delegate:\r\nThis delegate runs only on iOS since it leverages apple CoreML to accelerate on Apple's Neural engine.\r\nSee the [guide](https://www.tensorflow.org/lite/performance/coreml_delegate) for more information.\r\n\r\n\r\n\r\nEvery delegate has their own requirements, for example Hexagon DSP delegate runs only quantized models while GPU delegate can run floating point models.\r\nFor details about what each delegate support, please see this [section](https://www.tensorflow.org/lite/performance/delegates#delegates_by_model_type)\r\n\r\n\r\nHope that helps.\r\n\r\nThanks", "Hi karimnosseir, sorry to trouble you, that I have another question:\r\nI want to use Hexagon libraries in 64bit, but I could only get 32bit lib from [TFLite org link](https://www.tensorflow.org/lite/performance/hexagon_delegate?hl=zh-cn).\r\nI did download tflite_hexagon_nn_skel_v1.20.0.1.run, and use it to download Hexagon libraries.\r\nShould I download from other link, or just use 32bit in 64bit device?", "@karimnosseir  Thank you for your answer, it help me much.\r\nBy the way, if I want to use NPU, now I could only use NNAPI?", "@limdlh Yes, for newer Android devices that should be the case. ", "@srjoglekar246 What about XNNPACK ? What does it represent?\r\n", "@limdlh XNNPack is a delegate that implements some CPU optimizations, especially for sequences of heavy ops like MatMul, Conv etc. So its not exactly a hardware delegate, but runs on CPU. See [this blog post](https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html) for an explanation of how to try it.", "Hello @srjoglekar246 \r\nFrom this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md), it says that: \r\n**XNNPACK engine used by TensorFlow Lite interpreter uses a single thread for inference by default**\r\nCould you give some overview on how this is done? For example, if I want to run regular TensorFlowLite on single thread, will that be feasible? How is TFLite with xnnpack engine different from regular TFLite runtime that makes it easy to run on single thread?\r\n\r\nThanks. ", "We haven't enabled XNNPack by default yet. So you should be running TFLite w/ single-threaded CPU kernels by default as of now.\r\n\r\nWith XNNPack, some parts of the TFLite graph with CONV/FULLY_CONNECTED etc will get replaced by a single XNNPack kernel that performs the equivalent task but faster. XNNPack runs faster if you set num threads higher with TFLite.", "@srjoglekar246 I may be missing something here. Could you please clarify :\r\nI am running inference on CentOS 8 linux x86_64 architecture. And based on this [part](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md#enable-xnnpack-via-bazel-build-flags-recommended-on-desktop) \r\n```\r\nWhen building TensorFlow Lite with Bazel, add --define tflite_with_xnnpack=true, and the TensorFlow Lite interpreter \r\nwill use XNNPACK engine by default.\r\n```\r\n\r\nI did build TFlite with --define tflite_with_xnnpack=true. Will this not run on single thread?", "This will run w/ XNNPack & a single thread, yes. I did not realize you were building with that `--define`", "@srjoglekar246\r\nThat's good to know. Thanks.\r\n\r\nBut based on your previous comment : \r\n> We haven't enabled XNNPack by default yet. So you should be running TFLite w/ single-threaded CPU kernels by default as of now.\r\n\r\n \r\nDoes TFLite still run single threaded even if XNNPack is not enabled?\r\n\r\nI am asking this in context of a program that I am currently trying to run. It uses forking to create multiple processes to do inference. Currently, when I use default TFlite, it gets stuck in mutex lock. That's why I want to confine TFLite runtime to a single thread.\r\nNote it also uses flex delegate if that is relevant information to this problem.\r\n\r\n", "> Does TFLite still run single threaded even if XNNPack is not enabled?\r\n\r\nYup, atleast for TFLite builtin ops. However, I am not sure if flex uses multiple threads. @abattery do you have an idea?\r\n\r\nAlso, are you using the same Interpreter from multiple threads? TFLite doesn't support that, you will need a new Interpreter for each concurrent thread. It might be a good sanity check to run some mobilenet model in your framework (since it has only TFlite builtin ops) and see if things work without issues.", "> Also, are you using the same Interpreter from multiple threads? TFLite doesn't support that, you will need a new Interpreter for each concurrent thread.\r\n\r\nI am not sure I understand this completely. I specify only 1 thread when creating InterpreterBuilder, something like this : \r\n```\r\ntflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter, 1);\r\n```\r\nDo you mean every child process that is forked should have its own new Interpreter instance?\r\nMy model is, in fact, mobilenetv2 which is getting stuck in lock condition.\r\n ", "The Interpreter's kernels will use only single threaded inference in your case. However, you mentioned that you use multiple processes for inference:\r\n\r\n> I am asking this in context of a program that I am currently trying to run. It uses forking to create multiple processes to do inference.\r\n\r\nAre all of these processes using the same Interpreter instance? or different ones?\r\n\r\n", "@srjoglekar246 \r\nI am using a code written by someone else, but with some debugging I believe all the processes **are** using the same interpreter instance. And now I understand from your previous comment, that TFLite doesn't support that.\r\nSo I suppose you are suggesting me to create a new Interpreter instance for every process. That's very helpful to know."]}, {"number": 48348, "title": "[CHerryPick:r2.5] Include additional required headers in Windows libtensorflow releases.", "body": "PiperOrigin-RevId: 366494907\nChange-Id: I3abc6f432549275abb8a57ba0f2a5a5ed27250ff", "comments": ["There is one more change we need. I'm submitting it now."]}, {"number": 48347, "title": "r2.5-rc1 cherry-pick request: [Intel MKL] Fix MKL auto-mixed precision grappler pass in --config=cuda", "body": "MKL auto-mixed precision had failed some tests when built with `--config=cuda`, so we temporarily disabled it. This PR fixes the failures and re-enables MKL auto-mixed precision back.\r\n\r\nOriginal PR (merged into master today): #48286", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48347) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48347) for more info**.\n\n<!-- need_author_consent -->", "@mahmoud-abuzaina Could you please help reply to googlebot as well? Thank you!", "@googlebot I consent."]}, {"number": 48346, "title": "[ROCm] Adding ROCm support for SparseToDense Op", "body": "The following PR added CUDA support for SparseToDenseOp.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/47234\r\n\r\nThis PR/commit merely enables the same for the ROCm platform\r\n\r\n-------------------------------------------\r\n\r\n/cc @cheshire @chsigg ", "comments": []}, {"number": 48345, "title": "r2.5-rc1 cherry-pick request: [Intel MKL] Update oneDNN to v2.2 official release", "body": "oneDNN v2.2 wasn't released in time for the TF 2.5 branch cut, so we used an intermediate commit (after `v2.2-rc`) in the meanwhile. The final v2.2 release is out and has more fixes after the commit we initially used for TF 2.5.\r\n\r\nOriginal PR (merged into master yesterday): #48248 ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48345) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48345) for more info**.\n\n<!-- need_author_consent -->", "@cuixiaom Could you please help post `@googlebot I consent.` as well? Thank you!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48345) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent"]}, {"number": 48344, "title": "Allow specifying dtype to keras dataset generator", "body": "This change would allow image_dataset_from_directory to load images with\r\narbitary dtypes, like 16 bit IR imagery.", "comments": ["Looks Good to me, since this update the API interface, adding @fchollet as the API owner for review.", "Not sure if it's okay for me to modify the internal methods in the testing code, but I didn't want to just copy//paste stuff from _get_images and _prepare_directory into my test.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48344) for more info**.\n\n<!-- need_author_consent -->", "> > +36,982 \u22129,895\r\n> \r\n> It looks like your PR needs to be rebased. It contains many unrelated changes.\r\n\r\nFixed, sorry about that.", "@Cyniikal can you please check sanity build failures ?", "> @Cyniikal can you please check sanity build failures ?\r\n\r\nShould be good now. Do you guys want me to squash these commits?", "This PR wasn't merged. Is this the desired outcome?"]}, {"number": 48343, "title": "TFLite Object Detection Inference Limit 10", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2.4.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Jupyter Notebook\r\n\r\n**Describe the problem**\r\nTrying to make detections over an image with a tflite model. It executes correctly, but only detects 10 elements per image, no matter the image provided. \r\n\r\n\r\n**Reference** [**tutorial** ](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) - Export & run with TensorFlow Lite step- <br>\r\n**1.** Export model to tflite graph: \r\n\r\n```\r\npython models/research/object_detection/export_tflite_graph_tf2.py \\\r\n  --pipeline_config_path output/pipeline.config \\\r\n  --trained_checkpoint_dir output/checkpoint \\\r\n  --output_directory tflite\r\n```\r\n <br>\r\n\r\n`pipeline.config`: \r\n\r\n```\r\n# SSD with Mobilenet v2 FPN-lite (go/fpn-lite) feature extractor, shared box\r\n# predictor and focal loss (a mobile version of Retinanet).\r\n# Retinanet: see Lin et al, https://arxiv.org/abs/1708.02002\r\n# Trained on COCO, initialized from Imagenet classification checkpoint\r\n# Train on TPU-8\r\n#\r\n# Achieves 28.2 mAP on COCO17 Val\r\n\r\nmodel {\r\n  ssd {\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n    num_classes: 2\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.0\r\n        unmatched_threshold: 0.0\r\n        ignore_thresholds: true\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    encode_background_as_zeros: true\r\n    anchor_generator {\r\n      multiscale_anchor_generator {\r\n        min_level: 3\r\n        max_level: 7\r\n        anchor_scale: 4.0\r\n        aspect_ratios: [1.0, 2.0, 0.5]\r\n        scales_per_octave: 2\r\n      }\r\n    }\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 640\r\n        width: 640\r\n      }\r\n    }\r\n    box_predictor {\r\n      weight_shared_convolutional_box_predictor {\r\n        depth: 128\r\n        class_prediction_bias_init: -4.6\r\n        conv_hyperparams {\r\n          activation: RELU_6,\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00004\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              stddev: 0.01\r\n              mean: 0.0\r\n            }\r\n          }\r\n          batch_norm {\r\n            scale: true,\r\n            decay: 0.997,\r\n            epsilon: 0.001,\r\n          }\r\n        }\r\n        num_layers_before_predictor: 4\r\n        share_prediction_tower: true\r\n        use_depthwise: true\r\n        kernel_size: 3\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'ssd_mobilenet_v2_fpn_keras'\r\n      use_depthwise: true\r\n      fpn {\r\n        min_level: 3\r\n        max_level: 7\r\n        additional_layer_depth: 128\r\n      }\r\n      min_depth: 16\r\n      depth_multiplier: 1.0\r\n      conv_hyperparams {\r\n        activation: RELU_6,\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00004\r\n          }\r\n        }\r\n        initializer {\r\n          random_normal_initializer {\r\n            stddev: 0.01\r\n            mean: 0.0\r\n          }\r\n        }\r\n        batch_norm {\r\n          scale: true,\r\n          decay: 0.997,\r\n          epsilon: 0.001,\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n    }\r\n    loss {\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          alpha: 0.25\r\n          gamma: 2.0\r\n        }\r\n      }\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    normalize_loc_loss_by_codesize: true\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.0\r\n        max_detections_per_class: 200\r\n        max_total_detections: 200\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  fine_tune_checkpoint_version: V2\r\n  fine_tune_checkpoint: \"checkpoint/ckpt-0\"\r\n  fine_tune_checkpoint_type: \"detection\"\r\n  batch_size: 8\r\n  sync_replicas: true\r\n  startup_delay_steps: 0\r\n  replicas_to_aggregate: 8\r\n  num_steps: 50000\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    random_crop_image {\r\n      min_object_covered: 0.0\r\n      min_aspect_ratio: 0.75\r\n      max_aspect_ratio: 3.0\r\n      min_area: 0.75\r\n      max_area: 1.0\r\n      overlap_thresh: 0.0\r\n    }\r\n  }\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: .08\r\n          total_steps: 50000\r\n          warmup_learning_rate: .026666\r\n          warmup_steps: 1000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  max_number_of_boxes: 200\r\n  unpad_groundtruth_tensors: false\r\n}\r\n\r\ntrain_input_reader: {\r\n  label_map_path: \"/opt/ml/input/data/train/label_map.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"/opt/ml/input/data/train/train.records\"\r\n  }\r\n}\r\n\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n  batch_size: 1;\r\n}\r\n\r\neval_input_reader: {\r\n  label_map_path: \"/opt/ml/input/data/train/label_map.pbtxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: \"/opt/ml/input/data/train/validation.records\"\r\n  }\r\n}\r\n```\r\n\r\n**2.** Convert to model.tflite: \r\n<br>\r\n\r\n```\r\ntflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/model.tflite\r\n```\r\n\r\n**3.** Use .tflite model \r\n\r\n```python \r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nimport boto3\r\nimport cv2 \r\nfrom PIL import Image\r\nfrom six import BytesIO\r\nfrom object_detection.utils import config_util\r\n\r\nfrom object_detection.utils import config_util\r\nfrom object_detection.builders import model_builder\r\n\r\ndef reshape_img(image): \r\n\r\n    \"\"\"\r\n    image: numpy.ndarray\r\n    \"\"\"\r\n\r\n    width = 640\r\n    height = 640 \r\n    img = cv2.resize(image, (width,height))\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = np.expand_dims(img, axis=0)\r\n    return img\r\n\r\ndef images_reshaped(all_images):\r\n\r\n   \"\"\"\r\n   all_images: list\r\n   \"\"\"\r\n\r\n    i=0\r\n    im_reshaped = []\r\n    for im in all_images:\r\n            im_reshaped.append(reshape_img(im))\r\n        i += 1\r\n    return im_reshaped\r\n\r\ndef detect(interpreter, input_tensor, pipeline_config_path):\r\n\r\n    \"\"\"\r\n    interpreter: interpreter class\r\n    input_tensor: tensor class\r\n    pipeline_config_path: str\r\n    \"\"\"\r\n\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n\r\n    # We use the original model for pre-processing, since the TFLite model doesn't\r\n    # include pre-processing.\r\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\r\n    model_config = configs['model']\r\n    detection_model = model_builder.build(model_config=model_config, is_training=True)\r\n    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\r\n\r\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_image)\r\n    \r\n    interpreter.invoke()\r\n    boxes = interpreter.get_tensor(output_details[0]['index'])\r\n    classes = interpreter.get_tensor(output_details[1]['index'])\r\n    scores = interpreter.get_tensor(output_details[2]['index'])\r\n    print(f\"LEN {len(boxes.tolist()[0])}\")\r\n\r\n    return boxes, classes, scores\r\n\r\n\r\ndef inference_img(test_images_np, interpreter, pipeline_config_path):\r\n\r\n    \"\"\"\r\n    test_images_np: list \r\n    interpreter: interpreter class \r\n    pipeline_config_path: str\r\n    \"\"\"\r\n\r\n    label_id_offset = 1\r\n    for i in range(len(test_images_np)):\r\n        input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\r\n        print(f\"TENSOR SHAPE: {input_tensor.shape}\")\r\n        boxes, classes, scores = detect(interpreter, input_tensor, pipeline_config_path)\r\n\r\nif __name__ == \"__main__\":\r\n   interpreter = tf.lite.Interpreter(model_path=\"tflite/model.tflite\")\r\n   test_images_np = images_reshaped(all_images)\r\n   inference_img(test_images_np, interpreter, 'pipeline.config')\r\n```\r\n\r\n**OUTPUT**\r\n```\r\nLEN 10\r\nLEN 10\r\nLEN 10\r\nLEN 10\r\nLEN 10\r\n...\r\n\r\n```\r\n\r\n", "comments": ["Solved it by adding `--max_detections` flag to the `export_tflite_graph_tf2.py`:\r\n```\r\npython models/research/object_detection/export_tflite_graph_tf2.py \\\r\n  --pipeline_config_path output/pipeline.config \\\r\n  --trained_checkpoint_dir output/checkpoint \\\r\n  --output_directory tflite \\\r\n  --max_detections 200\r\n```"]}, {"number": 48342, "title": "Allow specifying image dtype on dataset creation", "body": "Simple change that allows the tensorflow.keras.preprocessing.image_dataset_from_directory function to allow imagery with dtypes other than uint8, such as 16 bit MW-IR imagery.", "comments": []}, {"number": 48341, "title": "[CherryPick:r2.5][tf.data] Apply options for a dataset before it is copied to another device in `tf.data.experimental.copy_to_device()`.", "body": "PiperOrigin-RevId: 366293759\nChange-Id: I2f38afd28f448cbd5a731a4a0261d74b3cc4ad0c", "comments": []}, {"number": 48340, "title": "Allow specifying image dtype on dataset creation", "body": "Simple change that allows the tensorflow.keras.preprocessing.image_dataset_from_directory function to allow imagery with dtypes other than uint8. I specifically needed this for 16 bit imagery.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48340) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48340) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it."]}, {"number": 48339, "title": "[CherryPick:r2.5] Rollback of rollback of [TF/XLA] Make sure that XLAControlFlowContext is injected on all paths of jit_compile=True", "body": "PiperOrigin-RevId: 366389161\nChange-Id: If4aca391be05315ccc875ac43acdcbc70bc64c67", "comments": []}, {"number": 48337, "title": "Keras: Replace trivial control flow ops with tf.where", "body": "This PR replaces the use of control flow ops in Keras with `tf.where` where using control flow is not needed.", "comments": []}, {"number": 48336, "title": " [ROCm] Enabling MLIR generated kernels by default on ROCm ", "body": "/cc @chsigg @cheshire @sanjoy ", "comments": []}, {"number": 48335, "title": "[CherryPick:r2.5] [TF/XLA] Fix the CollectiveReduceV2Op lowering by specifying the number of replicas needed at compile-time", "body": "PiperOrigin-RevId: 365890700\nChange-Id: I8c213c4ed469d7307d0a8be99a1a32bc6e06a1da", "comments": []}, {"number": 48334, "title": "some confuse about tf.shape(x) and x.shape", "body": "When I learning the [document](https://tensorflow.google.cn/tutorials/text/transformer) about transformer, I meet some different about tf.shape(x) and x.shape\r\n`tf.version == 2.3.0`\r\ncode:\r\n\r\n```\r\nclass Encoder(tf.keras.layers.Layer): \r\n    def call(self, x, training, mask):\r\n    ##########  it will raise a error when using x.shape[1]  ##########\r\n        seq_len = tf.shape(x)[1]         \r\n        x = self.embedding(x)\r\n        x *= tf.sqrt(tf.cast(self.d_model, tf.float32))\r\n        x += self.pos_encoding[:, :seq_len, :]\r\n        \r\n        x = self.dropout(x)\r\n        \r\n        for i in range(self.num_layers):\r\n            x = self.enc_layers[i](x, training, mask)\r\n\r\n        return x\r\n```\r\nthe error when using `x.shape[1]` to replace `tf.shape(x)[1]`:\r\n```\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Incompatible shapes: [64,38,128] vs. [1,8216,128]\r\n\t [[node transformer_14/encoder_19/add (defined at <ipython-input-104-62f8963562be>:19) ]]\r\n  (1) Invalid argument:  Incompatible shapes: [64,38,128] vs. [1,8216,128]\r\n\t [[node transformer_14/encoder_19/add (defined at <ipython-input-104-62f8963562be>:19) ]]\r\n\t [[gradient_tape/transformer_14/decoder_14/embedding_34/embedding_lookup/Reshape/_466]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_step_1459223]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node transformer_14/encoder_19/add:\r\n transformer_14/encoder_19/mul (defined at <ipython-input-104-62f8963562be>:18)\r\n\r\nInput Source operations connected to node transformer_14/encoder_19/add:\r\n transformer_14/encoder_19/mul (defined at <ipython-input-104-62f8963562be>:18)\r\n```\r\n**it waste a lot of time to find it, who can explain the different betweent them, thanks**", "comments": ["@Dragon-GCS \r\nI ran the code shared on tf nightly and do not see any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b9570f37557f93fb1d4f9b9fb6f73252/untitled583.ipynb).", "> @Dragon-GCS\r\n> I ran the code shared on tf nightly and do not see any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b9570f37557f93fb1d4f9b9fb6f73252/untitled583.ipynb).\r\n\r\nThe code in my question is not complete\uff0cyou can see the whole code in this [notebook](https://colab.research.google.com/drive/1tVnxGWiH54EL_NpcdlInqitG6FQiWcwS?usp=sharing). It\u2018s from this [document](https://tensorflow.google.cn/tutorials/text/transformer?hl=zh_cn). You can run all the cell in colab,and you wil find the error in the last cell output. This error occur in the `class Encoder`, when you change `x.shape[1]` to `tf.shape(x)[1]`, this program can run well.", "@Dragon-GCS \r\nPlease provide me with a  simplified colab gist with code that replicated the error.", "Here is the simplified code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef scaled_dot_product_attention(q, k, v):\r\n  matmul_qk = tf.matmul(q, k, transpose_b=True)\r\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\r\n  output = tf.matmul(attention_weights, v)\r\n  return output, attention_weights\r\n\r\n\r\nclass TestModel(tf.keras.Model):\r\n  def __init__(self, d_model=128, input_vocab_size=233,target_vocab_size=666):\r\n    super(TestModel, self).__init__()\r\n    self.embedding_inp = tf.keras.layers.Embedding(input_vocab_size, d_model)\r\n    self.embedding_tar = tf.keras.layers.Embedding(target_vocab_size, d_model)\r\n    self.pos_encoding_inp = tf.random.uniform([1,input_vocab_size,d_model])\r\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\r\n  \r\n  def call(self, inp, tar):\r\n    ##########      it will raise a error when using inp.shape[1]      ##########\r\n    #####  When use tf.shape or ignore adding self.pos_encoding_inp, it run well  #####\r\n    seq_len = inp.shape[1] # tf.shape(inp)[1]\r\n    inp = self.embedding_inp(inp)\r\n    inp += self.pos_encoding_inp[:, :seq_len, :]\r\n\r\n    tar = self.embedding_tar(tar)\r\n    tar, _ = scaled_dot_product_attention(v=inp, k=inp, q=tar)   ## raise error\r\n    out = self.final_layer(tar)\r\n    return out\r\n\r\ntestmodel = TestModel()\r\n\r\ntrain_step_signature = [tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n             tf.TensorSpec(shape=(None, None), dtype=tf.int64)]\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')   \r\n\r\n@tf.function(input_signature=train_step_signature)\r\ndef train_step(inp, tar):\r\n  tar_inp = tar[:, :-1]\r\n  tar_real = tar[:, 1:]\r\n  with tf.GradientTape() as tape:\r\n    predictions = testmodel(inp, tar_inp)\r\n    loss = loss_object(tar_real, predictions)\r\n  gradients = tape.gradient(loss, testmodel.trainable_variables)    \r\n  optimizer.apply_gradients(zip(gradients, testmodel.trainable_variables))\r\n  print('step over')\r\n\r\n\r\ninput = tf.constant(np.random.randint(0, 100,[64,40]),dtype=tf.int64)\r\ntarget = tf.constant(np.random.randint(0, 100,[64,39]),dtype=tf.int64)\r\ntrain_step(input, target)\r\n```", "I m able to replicate this issue on tf 2.3,2.5 and nightly, please find the gist [here](https://colab.research.google.com/gist/Saduf2019/04874623524313e08da482f0764acdcb/untitled589.ipynb) can you please refer to this comment based on [the error](https://github.com/keras-team/keras/issues/11749#issuecomment-804620973) reported.", "I read the link you mentioned, and I think my issue is not same as it. When I debug this error, x.shape and tf.shape(x) will rasie different value. So I think it will be a hidden danger and I mainly want to know why they are different.\r\nBTW, thanks for your patients and replys.", "@Dragon-GCS I didn't look at your code in detail but the main difference between x.shape (which results in static shape) and tf.shape (which results in dynamic shape) are described [here](https://github.com/tensorflow/tensorflow/issues/35798#issuecomment-574017006). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48333, "title": "TFLite object detection model inference returns 0.0", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Testing on computer with python \r\n\r\n**Describe the problem**\r\nAfter exporting a functional `saved_model.pb` to  a `model.tflite`  and try to make inferences with the `Interpreter` library, the results are 0.0 when tested with images that work over the `saved_model.pb`. \r\n\r\n\r\n**Reference** [**tutorial** ](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb) - Export & run with TensorFlow Lite step- <br>\r\n**1.** Export model to tflite graph: \r\n\r\n```\r\npython models/research/object_detection/export_tflite_graph_tf2.py \\\r\n  --pipeline_config_path output/pipeline.config \\\r\n  --trained_checkpoint_dir output/checkpoint \\\r\n  --output_directory tflite\r\n```\r\n <br>\r\n\r\n**3.** Convert to model.tflite: \r\n<br>\r\n```\r\ntflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/model.tflite\r\n```\r\n**3.** Test .tflite model \r\n\r\n```python \r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nimport boto3\r\nimport cv2 \r\nfrom PIL import Image\r\nfrom six import BytesIO\r\nfrom object_detection.utils import config_util\r\n\r\nfrom object_detection.utils import config_util\r\nfrom object_detection.builders import model_builder\r\n\r\ndef reshape_img(image): \r\n\r\n    \"\"\"\r\n    image: numpy.ndarray\r\n    \"\"\"\r\n\r\n    width = 640\r\n    height = 640 \r\n    img = cv2.resize(image, (width,height))\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n    img = np.expand_dims(img, axis=0)\r\n    return img\r\n\r\ndef images_reshaped(all_images):\r\n\r\n   \"\"\"\r\n   all_images: list\r\n   \"\"\"\r\n\r\n    i=0\r\n    im_reshaped = []\r\n    for im in all_images:\r\n            im_reshaped.append(reshape_img(im))\r\n        i += 1\r\n    return im_reshaped\r\n\r\ndef detect(interpreter, input_tensor, pipeline_config_path):\r\n\r\n    \"\"\"\r\n    interpreter: interpreter class\r\n    input_tensor: tensor class\r\n    pipeline_config_path: str\r\n    \"\"\"\r\n\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    print(f\"INPUT DETAILS:\\n{input_details}\")\r\n    print(f\"OUTPUT DETAILS:\\n{output_details}\")\r\n\r\n    # We use the original model for pre-processing, since the TFLite model doesn't\r\n    # include pre-processing.\r\n    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\r\n    model_config = configs['model']\r\n    detection_model = model_builder.build(model_config=model_config, is_training=True)\r\n    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\r\n\r\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_image)\r\n    \r\n    interpreter.invoke()\r\n    boxes = interpreter.get_tensor(output_details[0]['index'])\r\n    classes = interpreter.get_tensor(output_details[1]['index'])\r\n    scores = interpreter.get_tensor(output_details[2]['index'])\r\n  \r\n    print(f\"BOXES {boxes}\")\r\n    print(f\"CLASSES {classes}\")\r\n    print(f\"SCORES {scores}\")\r\n\r\n    return boxes, classes, scores\r\n\r\n\r\ndef inference_img(test_images_np, interpreter, pipeline_config_path):\r\n\r\n    \"\"\"\r\n    test_images_np: list \r\n    interpreter: interpreter class \r\n    pipeline_config_path: str\r\n    \"\"\"\r\n\r\n    label_id_offset = 1\r\n    for i in range(len(test_images_np)):\r\n        input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\r\n        print(f\"TENSOR SHAPE: {input_tensor.shape}\")\r\n        boxes, classes, scores = detect(interpreter, input_tensor, pipeline_config_path)\r\n\r\nif __name__ == \"__main__\":\r\n   interpreter = tf.lite.Interpreter(model_path=\"tflite/model.tflite\")\r\n   test_images_np = images_reshaped(all_images)\r\n   inference_img(test_images_np, interpreter, 'pipeline.config')\r\n```\r\n\r\n**OUTPUT:**\r\n```\r\nTENSOR SHAPE: (1, 640, 640, 3)\r\nINPUT DETAILS:\r\n[{'name': 'input', 'index': 12, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nOUTPUT DETAILS:\r\n[{'name': 'Identity', 'index': 0, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_1', 'index': 1, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_2', 'index': 4, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'Identity_3', 'index': 7, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nBOXES 0.0\r\nCLASSES 0.0\r\nSCORES 0.0\r\n```", "comments": ["I think it's related to the **cut-off threshold** because when making inference with `saved_model.pb` most of the allocated bounding boxes had a score lower than 0.4. Is there a way to lower cut-off threshold directly on the `Interpreter`? ", "Indeed, the problem was the **cut-off threshold**. For fixing it, I edited the `pipeline.config` file when exporting to tflite graph. For this, I changed `matched_threshold` and `unmatched_threshold` on `argmarx_matcher` and `iou_threshold` on `post_processing`, on the `pipeline.config` file.\r\n```\r\nmodel { \r\n...\r\n matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n...\r\npost_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 1e-8\r\n        iou_threshold: 0.1\r\n        max_detections_per_class: 200\r\n        max_total_detections: 200\r\n      }\r\n...\r\n}\r\n```"]}, {"number": 48332, "title": "Output shapes of then and else branches do not match  (s64[1,64]) vs. (s64[64])", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below):2.4\r\n- Python version:3.7.9\r\n- accelerator : TPU\r\n**Describe the current behavior**\r\nModel is not training on TPU but it is on GPU with same code \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n[](https://www.kaggle.com/prudhvi9999/oversampled?scriptVersionId=58909509)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nCode that throwing error when running on TPU\r\n`InvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-19-c05fea4c2943> in <module>\r\n      1 model.fit(train_ds,epochs=EPOCHS,validation_data=val_ds,\r\n----> 2           steps_per_epoch=TS,validation_steps=VS,callbacks=[lr_scheduler])\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1103               logs = tmp_logs  # No error, now safe to assign to logs.\r\n   1104               end_step = step + data_handler.step_increment\r\n-> 1105               callbacks.on_train_batch_end(end_step, logs)\r\n   1106               if self.stop_training:\r\n   1107                 break\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n    452     \"\"\"\r\n    453     if self._should_call_train_batch_hooks:\r\n--> 454       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n    455 \r\n    456   def on_test_batch_begin(self, batch, logs=None):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    294       self._call_batch_begin_hook(mode, batch, logs)\r\n    295     elif hook == 'end':\r\n--> 296       self._call_batch_end_hook(mode, batch, logs)\r\n    297     else:\r\n    298       raise ValueError('Unrecognized hook: {}'.format(hook))\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)\r\n    314       self._batch_times.append(batch_time)\r\n    315 \r\n--> 316     self._call_batch_hook_helper(hook_name, batch, logs)\r\n    317 \r\n    318     if len(self._batch_times) >= self._num_batches_for_timing_check:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)\r\n    354       hook = getattr(callback, hook_name)\r\n    355       if getattr(callback, '_supports_tf_logs', False):\r\n--> 356         hook(batch, logs)\r\n    357       else:\r\n    358         if numpy_logs is None:  # Only convert once.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n   1018 \r\n   1019   def on_train_batch_end(self, batch, logs=None):\r\n-> 1020     self._batch_update_progbar(batch, logs)\r\n   1021 \r\n   1022   def on_test_batch_end(self, batch, logs=None):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)\r\n   1082     if self.verbose == 1:\r\n   1083       # Only block async when verbose = 1.\r\n-> 1084       logs = tf_utils.to_numpy_or_python_type(logs)\r\n   1085       self.progbar.update(self.seen, list(logs.items()), finalize=False)\r\n   1086 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)\r\n    512     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n    513 \r\n--> 514   return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n    515 \r\n    516 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    657 \r\n    658   return pack_sequence_as(\r\n--> 659       structure[0], [func(*x) for x in entries],\r\n    660       expand_composites=expand_composites)\r\n    661 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    657 \r\n    658   return pack_sequence_as(\r\n--> 659       structure[0], [func(*x) for x in entries],\r\n    660       expand_composites=expand_composites)\r\n    661 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)\r\n    508   def _to_single_numpy_or_python_type(t):\r\n    509     if isinstance(t, ops.Tensor):\r\n--> 510       x = t.numpy()\r\n    511       return x.item() if np.ndim(x) == 0 else x\r\n    512     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1069     \"\"\"\r\n   1070     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1071     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1072     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1073 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1037       return self._numpy_internal()\r\n   1038     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1039       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1040 \r\n   1041   @property\r\n\r\n/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n  (1) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_35]]\r\n  (2) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_11]]\r\n  (3) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_5]]\r\n  (4) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_29]]\r\n  (5) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_47]]\r\n  (6) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_41]]\r\n  (7) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_17]]\r\n  (8) Invalid argument: {{function_node __inference_train_function_103841}} Compilation failure: Output shapes of then and else branches do not match: (s64[1,64]) vs. (s64[64])\r\n\t [[{{node cond}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3]]\r\n\t [[tpu_compile_succeeded_assert/_7380513076206390246/_3/_23]]\r\n0 successful operations.\r\n0 ... [truncated]`", "comments": ["@prudhvirajboddu,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here and also the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48332\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48332\">No</a>\n"]}, {"number": 48331, "title": "Support for VersionId in s3 file system", "body": "- TensorFlow version 2.4.1\r\n\r\nSupport **VersionId** argument in `s3_file_system` operations like reading (currently this is not supported by tensorflow even though it is supported by the underlying aws sdk).\r\n\r\nI want to be able to load a model with a specific **VersionId** from a versioned AWS S3-bucket in **tf-serving**. ", "comments": ["In the light of https://github.com/tensorflow/tensorflow/issues/48009#issuecomment-805965233 can you please post your issue on [tensorflow/io](https://github.com/tensorflow/io/issues) repo. Thanks!", "ok, I opened https://github.com/tensorflow/io/issues/1354#issue-852733649", "@sebastianschramm Let's close this issue and keep the conversation on https://github.com/tensorflow/io/issues/1354#issue-852733649"]}, {"number": 48330, "title": "Fix faulty link to numpy docs.", "body": "The links to the numpy documentation for `tensorflow.experimental.numpy.min()` and `tnp.max()` point to the documentation of `np.minimum()` and `np.maximum()` but this is wrong. `tnp.max()` roughly corresponds to `np.amax()` and `tnp.maximum()` to `np.maximum()`.", "comments": ["@SebastianJL Good catch! Thanks for the fix!", "Hope one day I also can catch some bugs too and become a tensorflow contributer! \ud83d\ude01"]}, {"number": 48329, "title": "model_main_tf2.py -> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbf in position 142: invalid start byte", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): example script\r\n- OS Platform and Distribution : win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): bin\r\n- TensorFlow version (use command below): tensorflow-cpu 2.4.1\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: intel integrated \r\n\r\n\r\n**Describe the current behavior**\r\n\r\n> (tensorflow) C:\\Users\\kacpe\\Desktop\\tensorflow\\models\\research\\object_detection>python model_main_tf2.py --pipeline_config_path=training/pipeline.config  --model_dir=training  --alsologtostderr\r\n> 2021-04-06 14:21:49.605417: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n> 2021-04-06 14:21:49.605814: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n> 2021-04-06 14:22:08.223915: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2021-04-06 14:22:08.229736: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n> 2021-04-06 14:22:08.230068: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n> 2021-04-06 14:22:08.242115: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-HO16S6U\r\n> 2021-04-06 14:22:08.242701: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-HO16S6U\r\n> 2021-04-06 14:22:08.244472: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2021-04-06 14:22:08.245897: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n> W0406 14:22:08.247780  9044 cross_device_ops.py:1321] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\n> INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n> I0406 14:22:08.247780  9044 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\r\n> INFO:tensorflow:Maybe overwriting train_steps: None\r\n> I0406 14:22:08.263412  9044 config_util.py:552] Maybe overwriting train_steps: None\r\n> INFO:tensorflow:Maybe overwriting use_bfloat16: False\r\n> I0406 14:22:08.263412  9044 config_util.py:552] Maybe overwriting use_bfloat16: False\r\n> I0406 14:22:08.325909  9044 ssd_efficientnet_bifpn_feature_extractor.py:142] EfficientDet EfficientNet backbone version: efficientnet-b1\r\n> I0406 14:22:08.325909  9044 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\r\n> I0406 14:22:08.325909  9044 ssd_efficientnet_bifpn_feature_extractor.py:145] EfficientDet BiFPN num iterations: 4\r\n> I0406 14:22:08.357155  9044 efficientnet_model.py:147] round_filter input=32 output=32\r\n> I0406 14:22:08.510153  9044 efficientnet_model.py:147] round_filter input=32 output=32\r\n> I0406 14:22:08.510153  9044 efficientnet_model.py:147] round_filter input=16 output=16\r\n> I0406 14:22:09.333078  9044 efficientnet_model.py:147] round_filter input=16 output=16\r\n> I0406 14:22:09.333078  9044 efficientnet_model.py:147] round_filter input=24 output=24\r\n> I0406 14:22:10.936553  9044 efficientnet_model.py:147] round_filter input=24 output=24\r\n> I0406 14:22:10.936553  9044 efficientnet_model.py:147] round_filter input=40 output=40\r\n> I0406 14:22:12.487046  9044 efficientnet_model.py:147] round_filter input=40 output=40\r\n> I0406 14:22:12.487046  9044 efficientnet_model.py:147] round_filter input=80 output=80\r\n> I0406 14:22:14.495594  9044 efficientnet_model.py:147] round_filter input=80 output=80\r\n> I0406 14:22:14.495594  9044 efficientnet_model.py:147] round_filter input=112 output=112\r\n> I0406 14:22:16.817428  9044 efficientnet_model.py:147] round_filter input=112 output=112\r\n> I0406 14:22:16.817428  9044 efficientnet_model.py:147] round_filter input=192 output=192\r\n> I0406 14:22:19.744267  9044 efficientnet_model.py:147] round_filter input=192 output=192\r\n> I0406 14:22:19.744267  9044 efficientnet_model.py:147] round_filter input=320 output=320\r\n> I0406 14:22:20.961132  9044 efficientnet_model.py:147] round_filter input=1280 output=1280\r\n> I0406 14:22:21.257415  9044 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\r\n> WARNING:tensorflow:From C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:545: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> rename to distribute_datasets_from_function\r\n> W0406 14:22:21.567980  9044 deprecation.py:333] From C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:545: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> rename to distribute_datasets_from_function\r\n> Traceback (most recent call last):\r\n>   File \"model_main_tf2.py\", line 113, in <module>\r\n>     tf.compat.v1.app.run()\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n>     _run_main(main, args)\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n>     sys.exit(main(argv))\r\n>   File \"model_main_tf2.py\", line 104, in main\r\n>     model_lib_v2.train_loop(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 545, in train_loop\r\n>     train_input = strategy.experimental_distribute_datasets_from_function(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 340, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1143, in experimental_distribute_datasets_from_function\r\n>     return self.distribute_datasets_from_function(dataset_fn, options)\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1134, in distribute_datasets_from_function\r\n>     return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py\", line 545, in _distribute_datasets_from_function\r\n>     return input_lib.get_distributed_datasets_from_function(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 161, in get_distributed_datasets_from_function\r\n>     return DistributedDatasetsFromFunction(dataset_fn, input_workers,\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1272, in __init__\r\n>     _create_datasets_from_function_with_input_context(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1936, in _create_datasets_from_function_with_input_context\r\n>     dataset = dataset_fn(ctx)\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 536, in train_dataset_fn\r\n>     train_input = inputs.train_input(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\inputs.py\", line 893, in train_input\r\n>     dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py\", line 210, in build\r\n>     decoder = decoder_builder.build(input_reader_config)\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\decoder_builder.py\", line 52, in build\r\n>     decoder = tf_example_decoder.TfExampleDecoder(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\data_decoders\\tf_example_decoder.py\", line 414, in __init__\r\n>     _ClassTensorHandler(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\data_decoders\\tf_example_decoder.py\", line 88, in __init__\r\n>     name_to_id = label_map_util.get_label_map_dict(\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\utils\\label_map_util.py\", line 201, in get_label_map_dict\r\n>     label_map = load_labelmap(label_map_path_or_proto)\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\utils\\label_map_util.py\", line 168, in load_labelmap\r\n>     label_map_string = fid.read()\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 117, in read\r\n>     self._preread_check()\r\n>   File \"C:\\Users\\kacpe\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 79, in _preread_check\r\n>     self._read_buf = _pywrap_file_io.BufferedInputStream(\r\n> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbf in position 142: invalid start byte\r\n> \r\n\r\n**Describe the expected behavior**\r\n\r\ntrain the model\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**pipeline.config :**\r\n\r\n`model {\r\n  ssd {\r\n    num_classes: 1\r\n    image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 640\r\n        max_dimension: 640\r\n        pad_to_max_dimension: true\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"ssd_efficientnet-b1_bifpn_keras\"\r\n      conv_hyperparams {\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 3.9999998989515007e-05\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            mean: 0.0\r\n            stddev: 0.029999999329447746\r\n          }\r\n        }\r\n        activation: SWISH\r\n        batch_norm {\r\n          decay: 0.9900000095367432\r\n          scale: true\r\n          epsilon: 0.0010000000474974513\r\n        }\r\n        force_use_bias: true\r\n      }\r\n      bifpn {\r\n        min_level: 3\r\n        max_level: 7\r\n        num_iterations: 4\r\n        num_filters: 88\r\n      }\r\n    }\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 1.0\r\n        x_scale: 1.0\r\n        height_scale: 1.0\r\n        width_scale: 1.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    box_predictor {\r\n      weight_shared_convolutional_box_predictor {\r\n        conv_hyperparams {\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 3.9999998989515007e-05\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              mean: 0.0\r\n              stddev: 0.009999999776482582\r\n            }\r\n          }\r\n          activation: SWISH\r\n          batch_norm {\r\n            decay: 0.9900000095367432\r\n            scale: true\r\n            epsilon: 0.0010000000474974513\r\n          }\r\n          force_use_bias: true\r\n        }\r\n        depth: 88\r\n        num_layers_before_predictor: 3\r\n        kernel_size: 3\r\n        class_prediction_bias_init: -4.599999904632568\r\n        use_depthwise: true\r\n      }\r\n    }\r\n    anchor_generator {\r\n      multiscale_anchor_generator {\r\n        min_level: 3\r\n        max_level: 7\r\n        anchor_scale: 4.0\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        scales_per_octave: 3\r\n      }\r\n    }\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 9.99999993922529e-09\r\n        iou_threshold: 0.5\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    loss {\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          gamma: 1.5\r\n          alpha: 0.25\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    encode_background_as_zeros: true\r\n    normalize_loc_loss_by_codesize: true\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n    add_background_class: false\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 8\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    random_scale_crop_and_pad_to_square {\r\n      output_size: 640\r\n      scale_min: 0.10000000149011612\r\n      scale_max: 2.0\r\n    }\r\n  }\r\n  sync_replicas: true\r\n  optimizer {\r\n    momentum_optimizer {\r\n      learning_rate {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.07999999821186066\r\n          total_steps: 300000\r\n          warmup_learning_rate: 0.0010000000474974513\r\n          warmup_steps: 2500\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.8999999761581421\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  fine_tune_checkpoint: \"C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/efficientdet_d1_coco17_tpu-32/checkpoint/ckpt-0\"\r\n  num_steps: 300000\r\n  startup_delay_steps: 0.0\r\n  replicas_to_aggregate: 8\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n  fine_tune_checkpoint_type: \"detection\"\r\n  use_bfloat16: true\r\n  fine_tune_checkpoint_version: V2\r\n}\r\ntrain_input_reader: {\r\n  label_map_path: \"C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/training/labelmap.bptxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/images/train.record\"\r\n  }\r\n}\r\n\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n  batch_size: 1;\r\n}\r\n\r\neval_input_reader: {\r\n  label_map_path: \"C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/training/labelmap.bptxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: \"C:/Users/kacpe/Desktop/tensorflow/models/research/object_detection/images/test.record\"\r\n  }\r\n}\r\n`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["@KacperKromka ,\r\n\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues). Thanks!", "@tilakrayal \r\n\r\nThanks for the reply! [post on models](https://github.com/tensorflow/models/issues/9875)"]}, {"number": 48328, "title": "saved model error", "body": "Following is my step:\r\n1. turn bert-base-chinese to saved model\r\n`\r\ninput_ids = tf.placeholder(shape=[None, 128], dtype=tf.int32, name=\"input_ids\")\r\ninput_mask = tf.placeholder(shape=[None, 128], dtype=tf.int32, name=\"input_mask\")\r\nsegment_ids = tf.placeholder(shape=[None, 128], dtype=tf.int32, name=\"segment_ids\")\r\n\r\nbert_config = modeling.BertConfig.from_json_file(\"chinese_L-12_H-768_A-12/bert_config.json\")\r\nmodel = modeling.BertModel(\r\n    config=bert_config,\r\n    is_training=False,\r\n    input_ids=input_ids,\r\n    input_mask=input_mask,\r\n    token_type_ids=segment_ids,\r\n    use_one_hot_embeddings=False\r\n)\r\n\r\ntvars = tf.trainable_variables()\r\ninit_checkpoint = \"chinese_L-12_H-768_A-12/bert_model.ckpt\"\r\n(assignment_map, initialized_variable_names) = \\\r\n    modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\r\ntf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    output = tf.placeholder(shape=[None, 768], dtype=tf.float32, name=\"bert/pooler/dense/Tanh\")\r\n    tf.saved_model.simple_save(\r\n        session=sess,\r\n        export_dir=\"./saved\",\r\n        inputs={\"inpu_ids\": input_ids,\r\n                \"inpu_mask\": input_mask,\r\n                \"segmen_ids\": segment_ids},\r\n        outputs={\"outpu\": output}\r\n    )\r\n`\r\n\r\n2. do inference on saved model using c++\r\n\r\nthen i got this error:\r\n\r\nE0406 11:25:56.438611   107 savedmodel_backend.cc:410] 2 root error(s) found.\r\n  (0) Invalid argument: You must feed a value for placeholder tensor 'bert/pooler/dense/Tanh_1' with dtype float and shape [?,768]\r\n         [[{{node bert/pooler/dense/Tanh_1}}]]\r\n         [[bert/pooler/dense/Tanh_1/_399]]\r\n  (1) Invalid argument: You must feed a value for placeholder tensor 'bert/pooler/dense/Tanh_1' with dtype float and shape [?,768]\r\n         [[{{node bert/pooler/dense/Tanh_1}}]]\r\n0 successful operations.\r\n0 derived errors ignored. \r\n\r\nIs there anything wrong about the way I turn ckpt to saved model?", "comments": ["this issue is caused by unfamiliarity with tensorflow. already fixed.\r\nreplace output = tf.placeholder(shape=[None, 768], dtype=tf.float32, name=\"bert/pooler/dense/Tanh\") with output = model.pooled_output can help"]}, {"number": 48327, "title": "I am unable to install Tensorflow on my mac without Virtual Environment, how can I fix this", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Prashant-Kesharwani \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\nYou can check the following as well:\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest microsoft [visual c++ redistributable from here](https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer similar issue #39007\r\n#41022, [link](https://github.com/tensorflow/tensorflow/issues/46362#issuecomment-759368982),#44291,#47140, #37525, #43459, #44130", "Thank you so much for your response, earlier when even I updated Python and then tried installing it, it didn't work.\r\nNow, after updating python it started working and finally I am able to install it,.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48327\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48327\">No</a>\n", "@Prashant-Kesharwani \r\nThank you for your response, glad your issue is resolved.\r\nCould you please share your platform information and tf version used."]}]