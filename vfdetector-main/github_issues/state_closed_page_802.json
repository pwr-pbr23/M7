[{"number": 29479, "title": "DLL", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 29478, "title": "Pix2Pix tutorial BatchNorm issue", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/alpha/tutorials/generative/pix2pix\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe `generate_images` function:\r\n\r\n```python\r\ndef generate_images(model, test_input, tar):\r\n  # the training=True is intentional here since\r\n  # we want the batch statistics while running the model\r\n  # on the test dataset. If we use training=False, we will get\r\n  # the accumulated statistics learned from the training dataset\r\n  # (which we don't want)\r\n  prediction = model(test_input, training=True)\r\n  plt.figure(figsize=(15,15))\r\n\r\n  display_list = [test_input[0], tar[0], prediction[0]]\r\n  title = ['Input Image', 'Ground Truth', 'Predicted Image']\r\n\r\n  for i in range(3):\r\n    plt.subplot(1, 3, i+1)\r\n    plt.title(title[i])\r\n    # getting the pixel values between [0, 1] to plot it.\r\n    plt.imshow(display_list[i] * 0.5 + 0.5)\r\n    plt.axis('off')\r\n  plt.show()\r\n```\r\n\r\ngenerate images using the generator model with the flag training=True. The problem is that in this way the batch normalization statistics (moving mean and moving variance) are updated using the test set statistics. \r\n\r\nThis is wrong. The original pix2pix paper asserts that they evaluate the model using the flag training=True but they do this in order to normalize using the minibatch (with batch size = 1) statistics. This is done only in the test phase and statistics should not be saved into the model.\r\n\r\nI think that a better approach is to visualize the data generated during training (not re-calling the generator but using the data generated in order to calculate the loss). \r\n\r\nOnce the training is finished we can evaluate the model.\r\n\r\nThe problem is that in tf 2.0 is not possible to use the minibatch statistics in the batch normalization layer. Every time we call BatchNorm()(input, training=True) the moving mean and moving variance are updated. I think that this can be managed in a better way by adding a flag that tells the layer whether to use the minibatch statistics. \r\n", "comments": ["Actually this is not an issue since the batch norm by default uses the statistics of the current batch during training. Since we never use the model with `training=False` this is not a problem.", "@EmanueleGhelfi This does matter. In every epoch, after making such a prediction, all the batch normalization layer are updated with the information from `test_input`, which is not the desired behavior.", "But since pix2pix always use `training=True` it always use batch statistics. So the moving mean and variance are not used anywhere.", "@EmanueleGhelfi With the code below\r\n\r\n```python\r\nfrom tensorflow.python.keras.layers import Input\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import BatchNormalization\r\nfrom tensorflow.python.keras.layers import Conv2D\r\n\r\ninput = Input( (3, 3, 1) )\r\nx = Conv2D( 1, 1, )(input)\r\nx = BatchNormalization(momentum=0.8)(x)\r\nx = Conv2D( 1, 1, activation='sigmoid' )(x)\r\nmodel = Model( input, x )\r\nmodel.compile( loss='mae', optimizer='adam' )\r\n\r\nimport numpy as np\r\na1 = np.random.random( (1, 3, 3, 1) )\r\na2 = np.random.random( (1, 3, 3, 1) )\r\nmodel.train_on_batch( a1, a2 )\r\n\r\nmodel.save( './model_pre.h5' )\r\n\r\nimport os\r\nos.system( '/bin/md5sum ./model_pre.h5' )\r\n\r\na = np.random.random( (1, 3, 3, 1) )\r\nb = model( np.asarray(a, dtype='float32'), training=True )\r\nmodel.save( './model_after.h5' )\r\n\r\nos.system( '/bin/md5sum ./model_after.h5' )\r\n\r\n```\r\n\r\nI got such output by running it twice:\r\n\r\n```\r\n feng@DDM5865 \ue0b0 ~/tmp/bn \ue0b0 python3 ./test.py\r\nWARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n2019-06-06 14:17:17.444135: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2809000000 Hz\r\n2019-06-06 14:17:17.444568: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x562041bffe50 executing computations on platform Host. Devices:\r\n2019-06-06 14:17:17.444598: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n4beaa9f45f17c7db87679092080ebf8c  ./model_pre.h5\r\n4beaa9f45f17c7db87679092080ebf8c  ./model_after.h5\r\n feng@DDM5865 \ue0b0 ~/tmp/bn \ue0b0 python3 ./test.py\r\nWARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n2019-06-06 14:17:22.777565: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2809000000 Hz\r\n2019-06-06 14:17:22.777901: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56502d9f7700 executing computations on platform Host. Devices:\r\n2019-06-06 14:17:22.777942: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\ndf0651ec886876f724a36ea1f5cf208b  ./model_pre.h5\r\ncfea66f48d5e93c2203512f730cd35be  ./model_after.h5\r\n```\r\n\r\nAnd I found for the first time, the model is not updated. But for the second one, the model did learn something new. I am confused....\r\n\r\n", "@fengwang yes, the model updates the moving mean and moving variance. You can also inspect them by accessing the correct attributes inside the BatchNormalization layer. The fact is that if the flag training is True the layer uses only the current mean and variance in order to calculate its output. In this way the moving mean and variance are not updated if I'm not wrong. You can also try this by inspecting the output of the layer with respect to its mean and its variance. I'll try it.", "I confirm my comment: \r\nif the flag training is True the layer uses only the current mean and variance in order to calculate its output. In this way the moving mean and variance are not updated.", "If this is not a problem anymore, @EmanueleGhelfi can you please close this.\r\n\r\nIf not, can you send us a PR (https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/pix2pix.ipynb). Any improvement to the docs is always appreciated :)", "Hi @yashk2810 I think the code has still problems right now. In the next days I'll try to submit a PR. ", "Hi! The model gives all NaNs when used with training = False on test images. Can anyone please explain this?", "@EmanueleGhelfi \r\nPlease post this issue on[ keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999"]}, {"number": 29477, "title": "Different Results on Different Machines", "body": "I am having a problem with the reproducibility of results. When I run the code with the same seed on two different machines I get different results. But on the same machine, it gives me the same results all the time. Are you guys aware of any such issue?", "comments": ["Are you sure that you are using same version of numpy and python on all machines? Maybe different versions have slight variation in randomization algorithm.", "Yes. The versions are exactly the same. But the results are not reproducible across machines.", "Okay, I figured out that the randomness was being produced by the following two functions in my code.\r\n\r\ntf.random_normal()\r\ntf.truncated_normal_initializer()\r\n\r\nWhen I leave out the functionality coming from these two functions, the results are exactly equal on the two machines. I do not set the seed argument of these functions explicitly during the function call. Rather, I set it globally using the tf.set_random_seed() function. My understanding is that the globally set seed from the tf.set_random_seed() function is incorporated when producing the distributions using the above two functions. Please correct me if I am wrong. This actually makes sense, because I get the same result through multiple runs on the same machine.\r\n\r\nNow my question is, does Tensorflow use any machine specific information within the functionalities of the above two functions?\r\n", "Hi,\r\n\r\nyes. There is an issue with global seed in tensorflow. this seems to be resolved in 2.0. refer below. \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/9171\r\n", "Hi,\r\n\r\nThanks @hegman12 for pointing me to the above issue."]}, {"number": 29476, "title": "There is no docker image with \"devel\" for version 1.13.1 and 1.14 in docker hub", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Docker image for tag '1.13.1-devel-gpu-py3' and for 1.14\r\n- TensorFlow version: 1.13.1 and 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Docker image unavailability issue\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1060 NVIDIA \r\n\r\n\r\n\r\n**Describe the problem**\r\nThere is no docker image with \"devel\" for version 1.13.1 and 1.14 in docker hub. I am not able to compile these versions from source and install. There are no tag with names \"1.13.1-devel-gpu-py3\" and \"1.14.0-devel-gpu-py3\".\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe issue is with unavailability of Image with \"devel\" tag for 1.13.1 and 1.14.0.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNA", "comments": ["That's correct. Following TF 1.12, we no longer provide `devel` containers based on specific versions, as `devel` tags are to be used for developing TensorFlow. If you would like to develop from source, please use the `devel-` tags instead, which are intended to support development for TensorFlow at master `HEAD`.\r\n\r\nPlease see the [Docker Hub TensorFlow page](https://hub.docker.com/r/tensorflow/tensorflow) for more details.", "without a devel version with bazel, how then is one supposed to run the speech_commands [example](https://www.tensorflow.org/tutorials/sequences/audio_recognition#streaming_accuracy)?", "@bjarthur, our `devel-` tags do contain Bazel. You should be able to use the existing `devel-` images to run the audio file generator.", "so to run the examples i have to be on master?  would prefer to be on a release branch.", "For the examples that use compiled source code, yes, that's correct. However, our development images do also contain a known-to-compile-commit in `/tensorflow_src`.", "Also note that once in the docker container other branches can also be checked out and built from. We don't guarantee that the code will compile, though in the majority of cases it should as the last commit on a release branch should also be the commit from which we generate the release associated docker tag. Thus, modulo some environment changes, only one `devel-` container should be needed to build on any release branch", "Hi @angerson\r\nIf TF 1.14.0 `devel` image will not be provided, Can you share how to reproduce speed of 1.12.0-devel-gpu with TF 1.14.0-gpu ?\r\nThere is  a speed difference between `devel` and `not devel` image during training phase. \r\nAlthough I don't develop from  source, I prefer to use 'devel' image for training\r\n<img width=\"427\" alt=\"tensorflow_dockerfile_steps_per_secs\" src=\"https://user-images.githubusercontent.com/5013450/66796453-abc1de00-ef42-11e9-8567-08d6931e51a0.png\">\r\n: In this experiment, there are about 3 steps difference per sec.\r\n", "I also tried to build a image of tensorflow/tensorflow:1.14.0-devel-gpu-py3 which has source compiled tf installed.\r\nBut I couldn't build the image which is faster to train than non-devel tag like 1.12.0-devel-gpu-py3.\r\n\r\nSo, I really want to know building the image like that tf team officially built devel tag until 1.12.0.", "my specific use case of running the speech commands examples was recently solved without the need of a devel container as the dependency on bazel was removed."]}, {"number": 29475, "title": "How to run  keras model using tensor flow JNI interface in java", "body": "This is python model which is working good in python:\r\n\r\nhttps://github.com/keras-team/keras/tree/master/examples\r\n\r\nlstm_seq2seq.py Trains a basic character-level sequence-to-sequence model.\r\nlstm_seq2seq_restore.py Restores a character-level sequence to sequence model from disk (saved by lstm_seq2seq.py) and uses it to generate predictions.\r\n\r\nDo you know if there is way to convert to java?\r\n\r\nI used below code to import python model -\r\n \r\nSavedModelBundle model = SavedModelBundle.load(\r\n                \"DIR\",\r\n                \"serve\");\r\n \r\ni got below exceptions:\r\norg.tensorflow.TensorFlowException: Could not find SavedModel .pb or .pbtxt at supplied export directory path.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide better support for such issues. Meanwhile you can have a look on this [link](https://towardsdatascience.com/deploying-keras-deep-learning-models-with-java-62d80464f34a). This might help. Thanks!\r\n", "DeepLearning4j is not working for me. If you know any way to convert below models to java Let me know.\r\n\r\nBelow are 2 models:\r\nhttps://github.com/keras-team/keras/tree/master/examples\r\n\r\nlstm_seq2seq.py Trains a basic character-level sequence-to-sequence model.\r\nlstm_seq2seq_restore.py Restores a character-level sequence to sequence model from disk (saved by lstm_seq2seq.py) and uses it to generate predictions.", "Since the issue at hand is not related to bug/build/feature request, we need to direct the same to[ StackOverflow](https://stackoverflow.com/questions/tagged//tensorflow) for better and faster resolution. However, you can have a look on this [link](https://stackoverflow.com/questions/35712182/running-a-python-program-in-java-using-jython) also which might be of some help to you. Thanks!"]}, {"number": 29474, "title": "Issue with tf.tpu.experimental.initialize_tpu_system in Google Colab", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): NA\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n```\r\nimport os\r\nimport pprint\r\n\r\nif 'COLAB_TPU_ADDR' not in os.environ:\r\n  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\r\nelse:\r\n  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n  print ('TPU address is', tpu_address)\r\n  \r\ntf.config.experimental_connect_to_host(tpu_address)\r\n\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\r\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n```\r\n\r\n**Describe the expected behavior**\r\nI get this error on the line - tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\n\r\n> NotFoundError: No registered 'ConfigureDistributedTPU' OpKernel for TPU_SYSTEM devices compatible with node {{node ConfigureDistributedTPU}}\r\n> \t.  Registered:  <no registered kernels>\r\n> \r\n> \t [[{{node ConfigureDistributedTPU}}]]\r\n> Additional GRPC error information:\r\n> {\"created\":\"@1559792051.046310667\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"No registered 'ConfigureDistributedTPU' OpKernel for TPU_SYSTEM devices compatible with node {{node ConfigureDistributedTPU}}\\n\\t.  Registered:  <no registered kernels>\\n\\n\\t [[{{node ConfigureDistributedTPU}}]]\",\"grpc_status\":5} [Op:__inference__tpu_init_fn_68]\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In addition, I get a similar error when I try to run it with GPU. With all GPU-specific changes necessary, I am getting this error when I run it with GPU:\r\n\r\n> NotFoundError: No registered 'TensorDataset' OpKernel for GPU devices compatible with node {{node TensorDataset}}\r\n\t.  Registered:  device='CPU'", "Hello, this error should be a basic issue that your Cloud TPU version is too old that does not contain the particular kernel you need. Could you try to contact Cloud TPU account team or raise an issue in https://github.com/tensorflow/tpu? I think you will need a tf 1.14 TPU at least (or tf-nightly), as it is close to tf 2.0 beta.", "I could reproduce the above issue with Tensorflow 2.0.0.alpha0 on colab. However, with TF2.0.0.beta0, the above code outputs expected output. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29474\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29474\">No</a>\n"]}, {"number": 29473, "title": "Fix the missing numpy import", "body": "", "comments": []}, {"number": 29472, "title": "[TF2.0]tf.lite.converter.convert() error:Cannot find the Placeholder op that is an input to the ReadVariableOp.   watch my second problem", "body": "I test this code for save model in  tf-nightly-2.0-gpu  in ubuntu 19.04,   tf.saved_model.save(model, saved_model_dir)       and get a error:\r\n\r\n\r\nAttributeError: 'TypeError' object has no attribute 'message'\r\n------------------------\r\nmy code like this:\r\n\r\n~~~~\r\n# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n\r\n\r\nimport tensorflow as tf\r\n\r\nimport os\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ntf.__version__\r\n\r\n\"\"\"## Setup Input Pipeline\r\n\r\nDownload the flowers dataset.\r\n\"\"\"\r\n\r\n\r\n\r\n\r\n\r\n_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\r\n\r\nzip_file = tf.keras.utils.get_file(origin=_URL, \r\n                                   fname=\"flower_photos.tgz\", \r\n                                   extract=True)\r\n\r\nbase_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')\r\n\r\n\"\"\"Use `ImageDataGenerator` to rescale the images.\r\n\r\nCreate the train generator and specify where the train dataset directory, image size, batch size.\r\n\r\nCreate the validation generator with similar approach as the train generator with the flow_from_directory() method.\r\n\"\"\"\r\n\r\nIMAGE_SIZE = 224\r\nBATCH_SIZE = 64\r\n\r\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n    rescale=1./255, \r\n    validation_split=0.2)\r\n\r\ntrain_generator = datagen.flow_from_directory(\r\n    base_dir,\r\n    target_size=(IMAGE_SIZE, IMAGE_SIZE),\r\n    batch_size=BATCH_SIZE, \r\n    subset='training')\r\n\r\nval_generator = datagen.flow_from_directory(\r\n    base_dir,\r\n    target_size=(IMAGE_SIZE, IMAGE_SIZE),\r\n    batch_size=BATCH_SIZE, \r\n    subset='validation')\r\n\r\nfor image_batch, label_batch in train_generator:\r\n  break\r\nimage_batch.shape, label_batch.shape\r\n\r\n\"\"\"Save the labels in a file which will be downloaded later.\"\"\"\r\n\r\nprint (train_generator.class_indices)\r\n\r\nlabels = '\\n'.join(sorted(train_generator.class_indices.keys()))\r\n\r\nwith open('labels.txt', 'w') as f:\r\n  f.write(labels)\r\n\r\n\r\n\"\"\"## Create the base model from the pre-trained convnets\r\n\r\nCreate the base model from the **MobileNet V2** model developed at Google, and pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.\r\n\r\nFirst, pick which intermediate layer of MobileNet V2 will be used for feature extraction. A common practice is to use the output of the very last layer before the flatten operation, the so-called \"bottleneck layer\". The reasoning here is that the following fully-connected layers will be too specialized to the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\r\n\r\nLet's instantiate an MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the `include_top=False` argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.\r\n\"\"\"\r\n\r\nIMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\r\n\r\n# Create the base model from the pre-trained model MobileNet V2\r\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n                                              include_top=False, \r\n                                              weights='imagenet')\r\n\r\n\"\"\"## Feature extraction\r\nYou will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\r\n\"\"\"\r\n\r\nbase_model.trainable = False\r\n\r\n\"\"\"### Add a classification head\"\"\"\r\n\r\nmodel = tf.keras.Sequential([\r\n  base_model,\r\n  tf.keras.layers.Conv2D(32, 3, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.GlobalAveragePooling2D(),\r\n  tf.keras.layers.Dense(5, activation='softmax')\r\n])\r\n\r\n\"\"\"### Compile the model\r\n\r\nYou must compile the model before training it.  Since there are two classes, use a binary cross-entropy loss.\r\n\"\"\"\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \r\n              loss='categorical_crossentropy', \r\n              metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nprint('Number of trainable variables = {}'.format(len(model.trainable_variables)))\r\n\r\n\"\"\"### Train the model\r\n\r\n<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->\r\n\"\"\"\r\n\r\nepochs = 2\r\n\r\nhistory = model.fit(train_generator, \r\n                    epochs=epochs, \r\n                    validation_data=val_generator)\r\n\r\n\"\"\"### Learning curves\r\n\r\nLet's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor.\r\n\"\"\"\r\n\r\nacc = history.history['accuracy']\r\nval_acc = history.history['val_accuracy']\r\n\r\nloss = history.history['loss']\r\nval_loss = history.history['val_loss']\r\n\r\nplt.figure(figsize=(8, 8))\r\nplt.subplot(2, 1, 1)\r\nplt.plot(acc, label='Training Accuracy')\r\nplt.plot(val_acc, label='Validation Accuracy')\r\nplt.legend(loc='lower right')\r\nplt.ylabel('Accuracy')\r\nplt.ylim([min(plt.ylim()),1])\r\nplt.title('Training and Validation Accuracy')\r\n\r\nplt.subplot(2, 1, 2)\r\nplt.plot(loss, label='Training Loss')\r\nplt.plot(val_loss, label='Validation Loss')\r\nplt.legend(loc='upper right')\r\nplt.ylabel('Cross Entropy')\r\nplt.ylim([0,1.0])\r\nplt.title('Training and Validation Loss')\r\nplt.xlabel('epoch')\r\nplt.show()\r\n\r\n\"\"\"## Fine tuning\r\nIn our feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were **not** updated during training.\r\n\r\nOne way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.\r\n\r\n### Un-freeze the top layers of the model\r\n\r\nAll you need to do is unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.\r\n\"\"\"\r\n\r\nbase_model.trainable = True\r\n\r\n# Let's take a look to see how many layers are in the base model\r\nprint(\"Number of layers in the base model: \", len(base_model.layers))\r\n\r\n# Fine tune from this layer onwards\r\nfine_tune_at = 100\r\n\r\n# Freeze all the layers before the `fine_tune_at` layer\r\nfor layer in base_model.layers[:fine_tune_at]:\r\n  layer.trainable =  False\r\n\r\n\"\"\"### Compile the model\r\n\r\nCompile the model using a much lower training rate.\r\n\"\"\"\r\n\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer = tf.keras.optimizers.Adam(1e-5),\r\n              metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\nprint('Number of trainable variables = {}'.format(len(model.trainable_variables)))\r\n\r\n\"\"\"### Continue Train the model\"\"\"\r\n\r\nhistory_fine = model.fit(train_generator, \r\n                         epochs=2,\r\n                         validation_data=val_generator)\r\n\r\n\"\"\"## Convert to TFLite\r\n\r\nSaved the model using `tf.saved_model.save` and then convert the saved model to a tf lite compatible format.\r\n\"\"\"\r\n\r\n#####  error code,save failed ............!!!!!!!!!!!!!!!!!!!!!!!!!1\r\nsaved_model_dir = 'save/fine_tuning'\r\ntf.saved_model.save(model, saved_model_dir)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n\"\"\"Download the converted model and labels\"\"\"\r\n\r\n# from google.colab import files\r\n\r\n# files.download('model.tflite')\r\n# files.download('labels.txt')\r\n\r\n\r\nacc = history_fine.history['accuracy']\r\nval_acc = history_fine.history['val_accuracy']\r\n\r\nloss = history_fine.history['loss']\r\nval_loss = history_fine.history['val_loss']\r\n\r\nplt.figure(figsize=(8, 8))\r\nplt.subplot(2, 1, 1)\r\nplt.plot(acc, label='Training Accuracy')\r\nplt.plot(val_acc, label='Validation Accuracy')\r\nplt.legend(loc='lower right')\r\nplt.ylabel('Accuracy')\r\nplt.ylim([min(plt.ylim()),1])\r\nplt.title('Training and Validation Accuracy')\r\n\r\nplt.subplot(2, 1, 2)\r\nplt.plot(loss, label='Training Loss')\r\nplt.plot(val_loss, label='Validation Loss')\r\nplt.legend(loc='upper right')\r\nplt.ylabel('Cross Entropy')\r\nplt.ylim([0,1.0])\r\nplt.title('Training and Validation Loss')\r\nplt.xlabel('epoch')\r\nplt.show()\r\n\r\n\r\n~~~~\r\n\r\nand  this is  mistake:\r\n\r\n\r\n~~~~\r\nW0606 11:03:53.153013 140230779684672 saved_model.py:748] Skipping full serialization of Keras layer <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f894c11be80>, because it does not have an input spec defined.\r\nW0606 11:03:53.165678 140230779684672 saved_model.py:748] Skipping full serialization of Keras layer <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f89a64fdba8>, because it does not have an input spec defined.\r\nTraceback (most recent call last):\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 712, in serialize_all_attributes\r\n    save_model_default_signature)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 850, in _wrap_layer_functions\r\n    fn.get_concrete_function()\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 681, in get_concrete_function\r\n    self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 359, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1401, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1689, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1582, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 728, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 309, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 994, in call_and_return_conditional_losses\r\n    return layer_call(inputs, training=training), layer.get_losses_for(inputs)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 651, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\", line 533, in _fused_batch_norm\r\n    self.add_update(mean_update)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1117, in add_update\r\n    updates = [process_update(x) for x in updates]\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1117, in <listcomp>\r\n    updates = [process_update(x) for x in updates]\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1113, in process_update\r\n    reachable = tf_utils.get_reachable_from_inputs(relevant_inputs, [update])\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 134, in get_reachable_from_inputs\r\n    raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\r\nTypeError: Expected Operation, Variable, or Tensor, got None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mint/ai/tensorflow/flowerlite/flowers_tf_lite.py\", line 204, in <module>\r\n    tf.saved_model.save(model, saved_model_dir)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 812, in save\r\n    checkpoint_graph_view)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py\", line 65, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 139, in list_functions\r\n    self._serialization_cache)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2234, in _list_functions_for_serialization\r\n    fns = (saved_model.serialize_all_attributes(self, serialization_cache)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 712, in serialize_all_attributes\r\n    save_model_default_signature)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 818, in _wrap_layer_functions\r\n    original_attrs = _replace_child_layer_functions(layer, serialization_cache)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 891, in _replace_child_layer_functions\r\n    layer_fns = (serialize_all_attributes(child_layer, serialization_cache)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 712, in serialize_all_attributes\r\n    save_model_default_signature)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 818, in _wrap_layer_functions\r\n    original_attrs = _replace_child_layer_functions(layer, serialization_cache)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 891, in _replace_child_layer_functions\r\n    layer_fns = (serialize_all_attributes(child_layer, serialization_cache)\r\n  File \"/home/mint/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 716, in serialize_all_attributes\r\n    'message: {}'.format(layer, e.message))\r\nAttributeError: 'TypeError' object has no attribute 'message'\r\n\r\n~~~~\r\n\r\nthe save code is ok at 2.0alpha0 ,but dismiss some  function like this :tf.lite.TFLiteConverter.from_saved_model\r\n\r\n\r\nthis is my tf2.0 version :\r\n\r\n\r\n~~~\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.0.0-dev20190605\r\n\r\n~~~\r\n\r\nI think it's bug?is it ?", "comments": ["Have tried on Colab with TF version 2.0.0-dev20190605 and was able to get mentioned output.", "@tms2003 I don't see any issue in saved_model as it ran through without any errors. Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/8415f5b8b3f5f8f1ed4f7470437cdb9f/tf_29472_saved_model.ipynb). Please provide a small reproducible code if there are any more issues. Thanks!", "> I test this code for save model in tf-nightly-2.0-gpu in ubuntu 19.04, tf.saved_model.save(model, saved_model_dir) and get a error:\r\n> \r\n\r\n> the save code is ok at 2.0alpha0 ,but dismiss some function like this :tf.lite.TFLiteConverter.from_saved_model\r\n> \r\n> this is my tf2.0 version :\r\n> \r\n> ```\r\n> >>> import tensorflow as tf\r\n> >>> print(tf.__version__)\r\n> 2.0.0-dev20190605\r\n> ```\r\n> \r\n> I think it's bug?is it ?\r\n\r\nIt's ok in beta0,but  I get error when covert:\r\nmy code:\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\n\r\nand error:\r\n\r\n~~~\r\ntraceback (most recent call last):\r\n  File \"D:/aisource/tf/flowertflite/flowers_tf_lite.py\", line 253, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"D:\\ProgramData\\Miniconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 348, in convert\r\n    self._funcs[0])\r\n  File \"D:\\ProgramData\\Miniconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\framework\\convert_to_constants.py\", line 166, in convert_variables_to_constants_v2\r\n    raise ValueError(\"Cannot find the Placeholder op that is an input \"\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.\r\n~~~\r\nThe same error in :\r\nhttps://colab.research.google.com/gist/tms2003/e65a1e3bc1c5e978a4436ad377b0a92a/tf_29472_saved_model.ipynb\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-5d4cfeefed96> in <module>()\r\n      1 \r\n----> 2 tflite_model = converter.convert()\r\n      3 \r\n      4 \r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func)\r\n    164         input_name = get_name(map_name_to_node[input_name].input[0])\r\n    165       if map_name_to_node[input_name].op != \"Placeholder\":\r\n--> 166         raise ValueError(\"Cannot find the Placeholder op that is an input \"\r\n    167                          \"to the ReadVariableOp.\")\r\n    168       # Build a map of Placeholder ops that are inputs to ReadVariableOps to the\r\n\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.\r\n\r\nso,anybody can help me?\r\n", "@tms2003 I can reproduce the same error as you listed. Could you edit the title accordingly. \r\nThanks!", "Hi, I see the same error when trying to convert a RNN model to tflite but not when converting a fully connected or CNN model? Does anyone know if tf.keras.layers.LSTM and tf.keras.layers.GRU ops have issues with conversion to TFLite models?\r\n\r\nWhen the tflite converter encounters the lstm node, it find a \"Enter\" op instead of \"Placeholder\" op and causes the conversion to fail. For my case the node and the corresponding op where is fails are:\r\n\r\n**node**\r\nname: \"lstm/while/ReadVariableOp_1\"\r\nop: \"ReadVariableOp\"\r\ninput: \"lstm/while/ReadVariableOp/Enter\"\r\ninput: \"^lstm/while/Identity\"\r\nattr {\r\n  key: \"dtype\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\n\r\n**input op**\r\n\r\nname: \"lstm/while/ReadVariableOp/Enter\"\r\nop: \"Enter\"\r\ninput: \"lstm/recurrent_kernel\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_RESOURCE\r\n  }\r\n}\r\nattr {\r\n  key: \"frame_name\"\r\n  value {\r\n    s: \"lstm/while/while_context\"\r\n  }\r\n}\r\nattr {\r\n  key: \"is_constant\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nattr {\r\n  key: \"parallel_iterations\"\r\n  value {\r\n    i: 32\r\n  }\r\n}\r\n\r\nIs there a remedy around this or tf.keras.layers.LSTM are just not usable with TFLite?  ", "There is currently limited support for LSTMs in TFLite. The documented path is available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/g3doc/README.md). We are working on improving our support of control flow based operations and models.", "i am getting this error while saving my model and converting it into tfLite :\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-31-214edbbd817b> in <module>\r\n      1 saved_model_dir = 'Documents/fine_tuning'\r\n----> 2 tf.saved_model.save(model, saved_model_dir)\r\n      3 \r\n      4 converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n      5 tflite_model = converter.convert()\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py in save(obj, export_dir, signatures)\r\n    820   object_saver = util.TrackableSaver(checkpoint_graph_view)\r\n    821   asset_info, exported_graph = _fill_meta_graph_def(\r\n--> 822       meta_graph_def, saveable_view, signatures)\r\n    823   saved_model.saved_model_schema_version = (\r\n    824       constants.SAVED_MODEL_SCHEMA_VERSION)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py in _fill_meta_graph_def(meta_graph_def, saveable_view, signature_functions)\r\n    508   resource_initializer_ops = []\r\n    509   with exported_graph.as_default():\r\n--> 510     object_map, resource_map, asset_info = saveable_view.map_resources()\r\n    511     for resource_initializer_function in resource_initializer_functions:\r\n    512       asset_dependencies = []\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py in map_resources(self)\r\n    243             and capture not in self.captured_tensor_node_ids):\r\n    244           copied_tensor = constant_op.constant(\r\n--> 245               tensor_util.constant_value(capture))\r\n    246           node_id = len(self.nodes)\r\n    247           node = _CapturedConstant(\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    244   \"\"\"\r\n    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 246                         allow_broadcast=True)\r\n    247 \r\n    248 \r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    282       tensor_util.make_tensor_proto(\r\n    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 284           allow_broadcast=allow_broadcast))\r\n    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    286   const_tensor = g.create_op(\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\hello-tf\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    452   else:\r\n    453     if values is None:\r\n--> 454       raise ValueError(\"None values not supported.\")\r\n    455     # if dtype is provided, forces numpy array to be the type\r\n    456     # provided if possible.\r\n\r\nValueError: None values not supported.\r\n\r\n\r\n\r\n", "I am also getting the same error\r\n\r\nopt/conda/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in from_keras_model_file(cls, model_file, input_arrays, input_shapes, output_arrays, custom_objects)\r\n    760     _set_tensor_shapes(input_tensors, input_shapes)\r\n    761 \r\n--> 762     graph_def = _freeze_graph(sess, input_tensors, output_tensors)\r\n    763     return cls(graph_def, input_tensors, output_tensors)\r\n    764 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/lite/python/util.py in freeze_graph(sess, input_tensors, output_tensors)\r\n    236     output_arrays = [get_tensor_name(tensor) for tensor in output_tensors]\r\n    237     return tf_graph_util.convert_variables_to_constants(sess, graph_def,\r\n--> 238                                                         output_arrays)\r\n    239   else:\r\n    240     return sess.graph_def\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py in convert_variables_to_constants(sess, input_graph_def, output_node_names, variable_names_whitelist, variable_names_blacklist)\r\n    300         source_op_name = get_input_name(map_name_to_node[source_op_name])\r\n    301       if map_name_to_node[source_op_name].op != \"VarHandleOp\":\r\n--> 302         raise ValueError(\"Cannot find the variable that is an input \"\r\n    303                          \"to the ReadVariableOp.\")\r\n    304 \r\n\r\nValueError: Cannot find the variable that is an input to the ReadVariableOp.\r\n\r\nDoes anyone has any solution?", "@tms2003 Can you try `TF2.0` and let us know. I ran it in `TF2.0` and I don't see any error. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/8dc40269e8223c9eb66f286f94e1d621/tf_29472_saved_model.ipynb) is the gist. I just changed epochs from 2 to 1 just to save some TF runtime. \r\n\r\nIf you think the issue was resolved by `TF2.0` then please close this issue. Thanks!  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29472\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29472\">No</a>\n", "> Hi, I see the same error when trying to convert a RNN model to tflite but not when converting a fully connected or CNN model? Does anyone know if tf.keras.layers.LSTM and tf.keras.layers.GRU ops have issues with conversion to TFLite models?\r\n> \r\n> When the tflite converter encounters the lstm node, it find a \"Enter\" op instead of \"Placeholder\" op and causes the conversion to fail. For my case the node and the corresponding op where is fails are:\r\n> \r\n> **node**\r\n> name: \"lstm/while/ReadVariableOp_1\"\r\n> op: \"ReadVariableOp\"\r\n> input: \"lstm/while/ReadVariableOp/Enter\"\r\n> input: \"^lstm/while/Identity\"\r\n> attr {\r\n> key: \"dtype\"\r\n> value {\r\n> type: DT_FLOAT\r\n> }\r\n> }\r\n> \r\n> **input op**\r\n> \r\n> name: \"lstm/while/ReadVariableOp/Enter\"\r\n> op: \"Enter\"\r\n> input: \"lstm/recurrent_kernel\"\r\n> attr {\r\n> key: \"T\"\r\n> value {\r\n> type: DT_RESOURCE\r\n> }\r\n> }\r\n> attr {\r\n> key: \"frame_name\"\r\n> value {\r\n> s: \"lstm/while/while_context\"\r\n> }\r\n> }\r\n> attr {\r\n> key: \"is_constant\"\r\n> value {\r\n> b: true\r\n> }\r\n> }\r\n> attr {\r\n> key: \"parallel_iterations\"\r\n> value {\r\n> i: 32\r\n> }\r\n> }\r\n> \r\n> Is there a remedy around this or tf.keras.layers.LSTM are just not usable with TFLite?\r\n\r\n@mrudulaathi You able to resolve this one ?", "@gunjanddave You should be able to resolve your conversion issues by setting the flag `experimental_new_converter` to `True`. If that does not resolve your issues, please file a new bug with your error and details on how to reproduce your error.", "> @gunjanddave You should be able to resolve your conversion issues by setting the flag `experimental_new_converter` to `True`. If that does not resolve your issues, please file a new bug with your error and details on how to reproduce your error.\r\n\r\nPerfect it worked. Thanks."]}, {"number": 29471, "title": "can't log my accuracy using tf.estimator", "body": "I can't log during evalution. It seems tf.train.LoggingTensorHook can only log during training. The tf.train.LoggingTensorHook export the prototxt into my terminal when evalutation.\r\nHere's some of my code\r\n\r\n```\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ntrain_tensors_to_log = {\r\n    \"total_loss\": \"summary/total_loss\",\r\n    \"box_loss\": \"summary/box_loss\",\r\n    \"cls_loss\": \"summary/cls_loss\",\r\n    \"reg_loss\": \"summary/reg_loss\"\r\n}\r\ntrain_logging_hook = tf.train.LoggingTensorHook(tensors=train_tensors_to_log, at_end=True)\r\n\r\neval_tensors_to_log = {\r\n    \"total_loss\": \"summary/total_loss\",\r\n    \"box_loss\": \"summary/box_loss\",\r\n    \"cls_loss\": \"summary/cls_loss\",\r\n    \"reg_loss\": \"summary/reg_loss\",\r\n    \"mAP\": \"eval_summary/mAP\"\r\n}\r\neval_logging_hook = tf.train.LoggingTensorHook(tensors=eval_tensors_to_log, every_n_iter=10)\r\n\r\nestimator = tf.estimator.Estimator(model_fn, config=run_config)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(\r\n    input_fn = train_input_fn,\r\n    max_steps = 100000,\r\n    hooks=[train_logging_hook]\r\n)\r\neval_spec = tf.estimator.EvalSpec(\r\n    input_fn = eval_input_fn,\r\n    steps = 100,\r\n    hooks=[eval_logging_hook],\r\n    throttle_secs=60\r\n)\r\ntf.estimator.train_and_evaluate(\r\n    estimator,\r\n    train_spec,\r\n    eval_spec\r\n)\r\n\r\n```\r\n\r\nHere's my terminal export:\r\n```\r\n\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x153\\x88\\xf0?', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15\\t;A@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\xab \\xa2@'\r\nINFO:tensorflow:Evaluation [10/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15B\\xa5\\x08@', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15M\\x14r@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\xe2\\xbd\\xc2@' (2.541 sec)\r\nINFO:tensorflow:Evaluation [20/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15z \\x0c@', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15\\x1b;<@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\xe4\\x8e\\xa9@' (1.154 sec)\r\nINFO:tensorflow:Evaluation [30/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15\\xf4w\\x00@', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15\\\\\\xca+@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15B\\x82\\x9b@' (1.198 sec)\r\nINFO:tensorflow:Evaluation [40/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15\\xa2\\xcf\\x0f@', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15wt{@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15&\\x03\\xcb@' (1.140 sec)\r\nINFO:tensorflow:Evaluation [50/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15 \\x85\\x02@', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15s\\x9c\\x1e@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\xe4\\xf1\\x95@' (1.120 sec)\r\nINFO:tensorflow:Evaluation [60/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15\\x11\\x80\\xf7?', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15\\xfa3[@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\x1b\\xdb\\xb0@' (1.125 sec)\r\nINFO:tensorflow:Evaluation [70/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15\\xd9>\\xf2?', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15fk\\x1e@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\x83&\\x91@' (1.167 sec)\r\nINFO:tensorflow:Evaluation [80/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15\\xc4\\xe1\\xcf?', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15\\x00^\\x08@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\x15\\x11{@' (1.152 sec)\r\nINFO:tensorflow:Evaluation [90/100]\r\nINFO:tensorflow:box_loss = b'\\n\\x17\\n\\x10summary/box_loss\\x15\\xb4\\x8d\\x03@', cls_loss = b'\\n\\x17\\n\\x10summary/cls_loss\\x15s\\x15Y@', mAP = b'\\n\\x17\\n\\x10eval_summary/mAP\\x15\\x00\\x00\\x00\\x00', reg_loss = b'\\n\\x17\\n\\x10summary/reg_loss\\x157#,>', total_loss = b'\\n\\x19\\n\\x12summary/total_loss\\x15\\xae\\xb2\\xb3@' (1.158 sec)\r\nINFO:tensorflow:Evaluation [100/100]\r\nINFO:tensorflow:Finished evaluation at 2019-06-06-02:27:09\r\nINFO:tensorflow:Saving dict for global step 3000: global_step = 3000, loss = 5.306427\r\n\r\n```\r\n", "comments": ["@mttbx Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: 7.5, 24GB\r\n- exact command to reproduce: not know yet, but I've show my code to you.\r\n\r\n", "@mttbx Code snippet looks incomplete to reproduce the issue. Please provide the complete code to reproduce. Thanks!", "The code below should reproduce the result, as matter of fact, this issue should be produced with any dataset and any net. It seems LoggingTensorHook can't work in estimator when evaluation\r\n```\r\nimport sys\r\nimport os\r\nimport pdb\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom dataset import dataset\r\nfrom basemodel import net\r\n\r\nbatch_size = 4\r\nimsz = [512, 512]\r\n\r\n\r\ndef train_input_fn():\r\n    dataset = dataset(batch_size, imsz, './dataset/train.tfrecord')\r\n    return dataset.get()\r\n\r\ndef eval_input_fn():\r\n    dataset = dataset(batch_size, imsz, './dataset/test.tfrecord')\r\n    return dataset.get()\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    is_training = mode == tf.estimator.ModeKeys.TRAIN\r\n    imgs = features\r\n    gts = labels\r\n    \r\n    model = net(batch_size, imsz, is_training)\r\n    model.predict(imgs)\r\n    total_loss, subloss = \\\r\n        model.loss(gts)\r\n    \r\n\r\n    with tf.name_scope('summary'):\r\n        tf.summary.scalar('subloss', subloss)\r\n        tf.summary.scalar('total_loss', total_loss)\r\n\r\n\r\n    train_op=None\r\n    if is_training:\r\n        with tf.name_scope('train'):\r\n            global_step = tf.train.get_or_create_global_step()\r\n            trainable_variables = tf.trainable_variables()\r\n\r\n            optimizer = tf.train.AdamOptimizer(\r\n                learning_rate=0.0001,\r\n                beta1=0.9,\r\n                beta2=0.999,\r\n                epsilon=1e-08\r\n            )\r\n            \r\n            up_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n            with tf.control_dependencies(up_ops):\r\n                train_op = optimizer.minimize(total_loss, global_step = global_step)\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        loss=total_loss,\r\n        train_op=train_op)\r\n    )\r\n\r\nif __name__ == '__main__':\r\n\r\n    run_config = tf.estimator.RunConfig(\r\n        model_dir='./model',\r\n        tf_random_seed=None,\r\n        save_summary_steps=100,\r\n        save_checkpoints_steps=100,\r\n        session_config=None,\r\n        keep_checkpoint_max=5,\r\n        keep_checkpoint_every_n_hours=1,\r\n    )\r\n\r\n\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n    train_tensors_to_log = {\r\n        \"subloss\": \"summary/subloss\",\r\n        \"total_loss\": \"summary/total_loss\",\r\n    }\r\n    train_logging_hook = tf.train.LoggingTensorHook(tensors=train_tensors_to_log, at_end=True)\r\n\r\n    eval_tensors_to_log = {\r\n        \"subloss\": \"summary/subloss\",\r\n        \"total_loss\": \"summary/total_loss\",\r\n    }\r\n    eval_logging_hook = tf.train.LoggingTensorHook(tensors=eval_tensors_to_log, every_n_iter=1)\r\n\r\n\r\n    estimator = tf.estimator.Estimator(model_fn, config=run_config)\r\n\r\n    train_spec = tf.estimator.TrainSpec(\r\n        input_fn = eval_input_fn,\r\n        max_steps = 1000000000,\r\n        hooks=[train_logging_hook]\r\n    )\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        input_fn = eval_input_fn,\r\n        steps = 10,\r\n        hooks=[eval_logging_hook],\r\n        throttle_secs=1\r\n    )\r\n    tf.estimator.train_and_evaluate(\r\n        estimator,\r\n        train_spec,\r\n        eval_spec\r\n    )\r\n\r\n            \r\n            \r\n            \r\n            \r\n            \r\n            \r\n            \r\n        \r\n```", "@mttbx Please provide a standalone code to reproduce the issue. You could provide a GitHub gist or any other way to share simple standalone code. Current code cannot run as it is due to import issues other errors. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "The same problem!  Has anyone solved this\uff1f", "@yaronhe Please create a new issue with a standalone code to reproduce the issue? Thanks!"]}, {"number": 29470, "title": "Add an auto-clustering heuristic for RNN performance", "body": "    Add an auto-clustering heuristic for RNN performance.\r\n\r\n     - The heuristic is to avoid creating dependency between while loop cond and\r\n       body computations. This unnecessary, artificial dependency can greatly hurt\r\n       the while loop performance.\r\n", "comments": ["@sanjoy @jlebar \r\n\r\nThis PR resolves half of the issue in #28890. That is, it removes the extra dependency that is introduced between the (while) loop condition and body computations due to auto-clustering. This dependency incurs great host execution overhead because it hinders the progression of the loop condition computation.\r\n\r\nTo address the issue, one (relatively) heavy way is to decluster all the ops related to the loop condition computation. However, I made an observation that, typically, the dependency is created through a special identity node (which is in a cycle of the loop condition computation) that drives const nodes in the loop body. So, a simple yet effective approach is to (just) not cluster this identity node to avoid the unwanted dependency. See the code and comments in the PR for details.\r\n\r\nThis is the best approach I've come up with. But as a nature of heuristic, I understand there may be room for discussions. Please help to take a look when you have a moment.\r\n\r\nThanks for your review and valuable feedback as usual.\r\n"]}, {"number": 29469, "title": "Tensorflow Lite model accuracy page not found", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/accuracy/README.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn post_training_quantization page (https://tensorflow.google.cn/lite/performance/post_training_quantization), the link to _TensorFlow Lite model accuracy_ page is invalid, it changes to new address after check (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/accuracy/ilsvrc/README.md), Please fix it\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29469\">No</a>\n"]}, {"number": 29468, "title": "Tensorflow 2.0 failed to run ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04 LTE\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\n2019-06-06 02:20:57.389862: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-06 02:20:57.394683: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-06-06 02:20:57.394945: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x3d792a0 executing computations on platform Host. Devices:\r\n2019-06-06 02:20:57.395053: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"tfrecord.py\", line 75, in <module>\r\n    main()\r\n  File \"tfrecord.py\", line 70, in main\r\n    download(file1, offset1, length1)\r\n  File \"tfrecord.py\", line 50, in download\r\n    image_masked = tf.boolean_mask(image, image > i_min+2)\r\n  File \"/home/aashishkumar/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 876, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/home/aashishkumar/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 383, in add\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node.\r\nNode: {{node Add}}\r\nAll kernels registered for op Add :\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n [Op:Add] name: add/\r\n```", "comments": ["@ak-org In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29467, "title": "404", "body": "TF 2.0 Alpha page 404", "comments": ["https://www.tensorflow.org/beta/", "beta also 404"]}, {"number": 29466, "title": "Add an auto-clustering heuristic for RNN performance.", "body": " - The heuristic is to avoid creating dependency between while loop cond and\r\n   body computations. This unnecessary, artificial dependency can greatly hurt\r\n   the while loop performance.", "comments": []}, {"number": 29465, "title": "Yet another performance improvement of LRNGrad on CPU builds", "body": "This improves the performance of the CPU version of LRNGrad by\r\nskipping the innermost loop altogether when the gradient to be applied\r\nat the end is *explicitly* zero (and thus nothing would change in the\r\noutput tensor). It turns out a lot of loops have 0 as value, and this\r\ngives us a ~3% speed up on CIFAR-10 training on a Intel(R) Core(TM)\r\ni9-7900X CPU @ 3.30GHz, with 20 threads (and remember that IEEE 754\r\ndefines the comparison 0.0==-0.0 to be true). Training accuracy on that\r\nsame model has been check with/without the changes and nothing\r\nchanges on that regard.\r\n\r\nThis was sitting in my hard drive for a while now, finally making its way up to here.", "comments": ["Thank you too! Any merger for that?"]}, {"number": 29464, "title": "Final cherrypick for keras saved models", "body": "Needed to cherrypick a few others as well to resolve conflicts, and fix test breakages.", "comments": []}, {"number": 29463, "title": "AttributeError: 'RefVariable' object has no attribute 'numpy'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): anaconda 3.7 \r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): na\r\n- GCC/Compiler version (if compiling from source): na\r\n- CUDA/cuDNN version: 9.0/7.6.0\r\n- GPU model and memory: 2 NVIDIA GeForceGTX 1080 Ti, 64.0 GB RAM\r\n\r\n\r\n\r\n**Describe the problem**\r\nWe are trying to run our own object detection using tensorflow and pretrained models. If you any advice to solve the below issues, pls help.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThis is the command that we are running from the anaconda command line:\r\n`python legacy\\train.py --logtostderr --train_dir=training\\ --pipeline_config_path=training\\ssd_mobilenet_v1_pets.config`\r\n\r\n**Any other info / logs**\r\nThis is the errors and warnings the command line outputs after running the above command. The main error we are dealing with is AttributeError: 'RefVariable' object has no attribute 'numpy'. \r\n\r\n`WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py:125: main (from __main__) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse object_detection/model_main.py.\r\n2019-06-05 16:42:56.976238: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2019-06-05 16:42:57.379496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 11.00GiB freeMemory: 9.11GiB\r\n2019-06-05 16:42:57.541412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 11.00GiB freeMemory: 9.11GiB\r\n2019-06-05 16:42:57.555091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1\r\n2019-06-05 16:43:00.519542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-05 16:43:00.528620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1\r\n2019-06-05 16:43:00.534150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N\r\n2019-06-05 16:43:00.540969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N\r\n2019-06-05 16:43:00.548760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2019-06-05 16:43:00.566108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From C:\\Users\\nnucs\\Documents\\Object Detection API\\models\\research\\object_detection\\legacy\\trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From C:\\Users\\nnucs\\Documents\\Object Detection API\\models\\research\\object_detection\\builders\\dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.experimental.parallel_interleave(...)`.\r\nWARNING:tensorflow:From C:\\Users\\nnucs\\Documents\\Object Detection API\\models\\research\\object_detection\\core\\preprocessor.py:188: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nWARNING:tensorflow:From C:\\Users\\nnucs\\Documents\\Object Detection API\\models\\research\\object_detection\\core\\batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\input.py:784: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/biases/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/biases/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/biases/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/weights/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/weights/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_0/ClassPredictor/weights/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 6]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/biases/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/biases/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/biases/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/weights/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/weights/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_1/ClassPredictor/weights/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/biases/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/biases/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/biases/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/weights/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/weights/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_2/ClassPredictor/weights/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/biases/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/biases/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/biases/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/weights/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/weights/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_3/ClassPredictor/weights/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/biases/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/biases/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/biases/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/weights/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/weights/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_4/ClassPredictor/weights/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/biases/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/biases/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/biases/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/weights/ExponentialMovingAverage] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/weights/RMSProp] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [BoxPredictor_5/ClassPredictor/weights/RMSProp_1] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 12]]. This variable will not be initialized from the checkpoint.\r\nWARNING:root:Variable [global_step] is not available in checkpoint\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py:737: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2019-06-05 16:43:14.953615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1\r\n2019-06-05 16:43:14.959366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-05 16:43:14.967292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1\r\n2019-06-05 16:43:14.974644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N\r\n2019-06-05 16:43:14.980817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N\r\n2019-06-05 16:43:14.987571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n2019-06-05 16:43:15.003633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from training\\model.ckpt-0\r\nINFO:tensorflow:Restoring parameters from training\\model.ckpt-0\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file utilities to get mtimes.\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file utilities to get mtimes.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Starting Session.\r\nINFO:tensorflow:Starting Session.\r\nINFO:tensorflow:Error reported to Coordinator: 'RefVariable' object has no attribute 'numpy'\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 485, in run\r\n    self.start_loop()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\", line 1067, in start_loop\r\n    self._sess, self._step_counter)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py\", line 68, in global_step\r\n    return int(global_step_tensor.numpy())\r\nAttributeError: 'RefVariable' object has no attribute 'numpy'\r\nINFO:tensorflow:Error reported to Coordinator: 'RefVariable' object has no attribute 'numpy'\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 485, in run\r\n    self.start_loop()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\", line 1067, in start_loop\r\n    self._sess, self._step_counter)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py\", line 68, in global_step\r\n    return int(global_step_tensor.numpy())\r\nAttributeError: 'RefVariable' object has no attribute 'numpy'\r\nINFO:tensorflow:Saving checkpoint to path training\\model.ckpt\r\nINFO:tensorflow:Saving checkpoint to path training\\model.ckpt\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:Finished training! Saving model to disk.\r\nINFO:tensorflow:Finished training! Saving model to disk.\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\r\n  warnings.warn(\"Attempting to use a closed FileWriter. \"\r\nTraceback (most recent call last):\r\n  File \"legacy\\train.py\", line 184, in <module>\r\n    tf.app.run()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"legacy\\train.py\", line 180, in main\r\n    graph_hook_fn=graph_rewriter_fn)\r\n  File \"C:\\Users\\nnucs\\Documents\\Object Detection API\\models\\research\\object_detection\\legacy\\trainer.py\", line 416, in train\r\n    saver=saver)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\slim\\python\\slim\\learning.py\", line 785, in train\r\n    ignore_live_threads=ignore_live_threads)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\", line 832, in stop\r\n    ignore_live_threads=ignore_live_threads)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\six.py\", line 693, in reraise\r\n    raise value\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 485, in run\r\n    self.start_loop()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\", line 1067, in start_loop\r\n    self._sess, self._step_counter)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py\", line 68, in global_step\r\n    return int(global_step_tensor.numpy())\r\nAttributeError: 'RefVariable' object has no attribute 'numpy'`\r\n", "comments": ["I am also having this issue.", "@aleeshachavez : Please provide minimal code snippet that can reproduce the issue. It will be easier for us to debug to know whether it is a bug or not. Meanwhile, the warnings suggest that those functions will be deprecated in the latest version of TensorFlow 2.0 as for example contrib is not present in TF 2.0. Thanks!   ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29462, "title": "updated tf.identity and tf.zeros_like", "body": "Updated tf.identity and tf.zeros_like with examples", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29462) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 29461, "title": "Possible race condition when calling custom op with control dependencies", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI was trying to study the behavior of tf.control_dependencies when training Resnet50 on distributed CPU, with the benchmark script (https://github.com/tensorflow/benchmarks/tree/cnn_tf_v1.12_compatible/scripts/tf_cnn_benchmarks). \r\nI have modified this script to load the same dataset on each run, and removed the random params from the model (for predictable repro). \r\nI have written a dummy custom C++ op where I'm simply printing the input tensor and returning.\r\nI have also modified the benchmark_cnn.py in the above repo to call my custom op, instead of hvd.allreduce. I have also added control dependency between the layers to ensure that my custom op gets called one layer after another. Following are the code changes for this:\r\n\r\n```\r\n        pred = None\r\n        for i in range(0, len(grads), 1):\r\n            grad = grads[i]\r\n\r\n            import re\r\n            if re.search(\"batch.*norm\", params[i].name, re.IGNORECASE) != None:\r\n              grads[i] = grad\r\n            else:\r\n              tensor_name = 'HorovodAllreduce_%s' % hvd_normalize_name(grad.name)\r\n              if pred != None:\r\n                with tf.control_dependencies([pred]):\r\n                  grad = custom_op_cpp(grad, horovod_device, tensor_name, hvd.rank())\r\n              else:\r\n                grad = custom_op_cpp(grad, horovod_device, tensor_name, hvd.rank())\r\n\r\n              pred = grad\r\n              grads[i] = grad\r\n```\r\n\r\nAnd custom_op_cpp() function is defined as follows:\r\n```\r\ncustom_module = tf.load_op_library('./custom_op_cpp/custom_op.so')\r\n\r\ndef custom_op_cpp(t_data, dense_device_name, tensor_name, my_rank):\r\n    result = custom_module.dummy_func(t_data, tensor_name, my_rank)\r\n    return result\r\n```\r\n\r\nCustom C++ op is defined as:\r\n```\r\nREGISTER_OP(\"DummyFunc\")\r\n    .Input(\"input: float32\")\r\n    .Input(\"tname: string\")\r\n    .Input(\"rank: int32\")\r\n    .Output(\"output: float32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n        c->set_output(0, c->input(0));\r\n        return Status::OK();\r\n    });\r\n\r\nclass DummyFuncOp : public OpKernel {\r\n    explicit DummyFuncOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n    void Compute(OpKernelContext* context) override {\r\n        ....\r\n        const Tensor& input_tensor = context->input(0);\r\n        float *inp_tensor = (float *)(input_tensor.tensor_data().data());\r\n        size_t inp_tensor_size = (size_t)input_tensor.shape().num_elements();\r\n          \r\n        PrintTensorData(inp_tensor, inp_tensor_size);\r\n\r\n       //Some code to return copy the input tensor into output and return\r\n    }\r\n}\r\n```\r\n\r\nThis works as expected, I see the same gradients for each run (1 iteration), since I'm loading the same dataset each time. \r\nNow, If I reverse the control dependencies from the benchmark script as follows:\r\n`for i in reversed(range(0, len(grads), 1)):`\r\nThen I see that the gradients coming into the dummy op are different for some of the layers, and they are random from run to run. However, if I add a `sleep(4)` in my dummy op's Compute function, then this issue is no longer seen. Hence, I'm suspecting that there may be a race condition, wherein dummy op is getting invoked even before the gradients are properly updated in the backprop phase. \r\n\r\n**Describe the expected behavior**\r\nWhether default or reverse control dependency is used between layers should not have any implications on the gradients seen in dummy c++ op for a given layer, since we are running a predictable model, with same datasets. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@chandanjc In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@gadagashwini Do you need me to share the code for c++ op or all of the Resnet50 benchmark script changes to make it deterministic? ", "@chandanjc Will it be possible to provide us the minimal full code snippet that includes all your operations. So that we can reproduce it on our environment. Thanks! ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29460, "title": "Add python syntax formatting for code samples, returns value and errors raised", "body": "", "comments": []}, {"number": 29459, "title": "updated tf.identity and tf.zeros_like", "body": "Updated tf.identity and tf.zeros_like with examples", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 29458, "title": "Deprecate `ModelCheckpoint.__init__`'s `load_weights_on_restart` argument and provide a warning message if used.", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 29457, "title": "Marks Keras set_session as compat.v1 only. Also moves some renames to\u2026", "body": "\u2026 the manual renames that had been incorrectly placed in the auto-generated symbol mappings.\r\n\r\nPiperOrigin-RevId: 251708447", "comments": []}, {"number": 29456, "title": "Marks Keras set_session as compat.v1 only. Also moves some renames to\u2026", "body": "\u2026 the manual renames that had been incorrectly placed in the auto-generated symbol mappings.\r\n\r\nPiperOrigin-RevId: 251708447", "comments": []}, {"number": 29455, "title": "Make default Keras ConfigProto use tf.config", "body": "PiperOrigin-RevId: 251659257", "comments": []}, {"number": 29454, "title": "Fix https://github.com/tensorflow/tensorflow/issues/29432", "body": "", "comments": []}, {"number": 29453, "title": "Make default Keras ConfigProto use tf.config", "body": "PiperOrigin-RevId: 251659257", "comments": ["Closing as Gaurav will submit a PR containing this as well as PiperOrigin-RevId: 251708447"]}, {"number": 29452, "title": "Automated rollback of commit 9b450b1ef19ab4640bb0ea8b1ebabf14e710a0b9", "body": "PiperOrigin-RevId: 250449705", "comments": []}, {"number": 29451, "title": "Added missing documentation sections to make_saveable_from_iterator, description corrected.", "body": "Added returns and raises section to make_saveable_from_iterator function #29406\r\n\r\n@rthadur \r\nPull request is now against master.\r\nRecommitted for easier merging into master.", "comments": ["requested changes were made.", "@jsimsa Sorry about the title, It has been corrected."]}, {"number": 29450, "title": "[Intel MKL] Fixing MKL support for secure build gcc flags and v2.0", "body": "", "comments": []}]