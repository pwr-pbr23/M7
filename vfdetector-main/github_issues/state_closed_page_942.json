[{"number": 25176, "title": "Cannot build Tensorflow 1.12 On Windows 10 and CUDA 9.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): MSVC2015 \r\n- CUDA/cuDNN version: 9.0.176 / 7.4.2\r\n- GPU model and memory: Nvidia Geforce GTX 750 Ti/ 2GB DDR5\r\nCpu : AMD Athlon II x4 631\r\n\r\n\r\n**Describe the problem**\r\nSo I've been using Anaconda to build Tensorflow 1.12, since my cpu is old and doesn't support AVX (i want tensorflow to run on gpu) i cant use the pre-built pip packages. \r\nI've installed bazel and msys64 correctly and added them to path directory. After configuring the configure.py and when trying to build, i get errors posted below.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n> WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nf:\\tensorflow\\tensorflow/.bazelrc\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: ce75c25f-4ec0-436f-91bb-da2ed1092a50\r\nINFO: Build option --define has changed, discarding analysis cache.\r\nWARNING: F:/tensorflow/tensorflow/tensorflow/python/BUILD:3099:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: F:/tensorflow/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: F:/tensorflow/tensorflow/tensorflow/python/BUILD:100:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: F:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: F:/tensorflow/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: F:/tensorflow/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: F:/tensorflow/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 14335 targets configured).\r\nINFO: Found 1 target...\r\nERROR: F:/tensorflow/tensorflow/tensorflow/core/BUILD:2497:1: ProtoCompile tensorflow/core/example/example_pb2.py failed (Exit -1073741795): protoc failed: error executing command\r\n  cd C:/users/albis/_bazel_albis/a6so2axr/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=F:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n    SET PYTHON_BIN_PATH=F:/Anaconda3/envs/Tensorflow/python.exe\r\n    SET PYTHON_LIB_PATH=F:/Anaconda3/envs/Tensorflow/lib/site-packages\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  bazel-out/x64_windows-opt/bin/external/protobuf_archive/protoc --python_out=bazel-out/x64_windows-opt/genfiles/ -I. -I. -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python -Iexternal/protobuf_archive/python -Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/python tensorflow/core/example/example.proto tensorflow/core/example/feature.proto tensorflow/core/framework/allocation_description.proto tensorflow/core/framework/api_def.proto tensorflow/core/framework/attr_value.proto tensorflow/core/framework/cost_graph.proto tensorflow/core/framework/device_attributes.proto tensorflow/core/framework/function.proto tensorflow/core/framework/graph.proto tensorflow/core/framework/graph_transfer_info.proto tensorflow/core/framework/kernel_def.proto tensorflow/core/framework/log_memory.proto tensorflow/core/framework/node_def.proto tensorflow/core/framework/op_def.proto tensorflow/core/framework/reader_base.proto tensorflow/core/framework/remote_fused_graph_execute_info.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/step_stats.proto tensorflow/core/framework/summary.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_description.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor_slice.proto tensorflow/core/framework/types.proto tensorflow/core/framework/variable.proto tensorflow/core/framework/versions.proto tensorflow/core/protobuf/config.proto tensorflow/core/protobuf/cluster.proto tensorflow/core/protobuf/debug.proto tensorflow/core/protobuf/device_properties.proto tensorflow/core/protobuf/queue_runner.proto tensorflow/core/protobuf/rewriter_config.proto tensorflow/core/protobuf/tensor_bundle.proto tensorflow/core/protobuf/saver.proto tensorflow/core/util/event.proto tensorflow/core/util/memmapped_file_system.proto tensorflow/core/util/saved_tensor_slice.proto tensorflow/core/example/example_parser_configuration.proto tensorflow/core/protobuf/checkpointable_object_graph.proto tensorflow/core/protobuf/control_flow.proto tensorflow/core/protobuf/meta_graph.proto tensorflow/core/protobuf/named_tensor.proto tensorflow/core/protobuf/saved_model.proto tensorflow/core/protobuf/tensorflow_server.proto tensorflow/core/protobuf/transport_options.proto tensorflow/core/util/test_log.proto\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 18.245s, Critical Path: 4.43s\r\nINFO: 2 processes: 2 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["Can you upgrade your Python version to 3.6.1 and check?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25175, "title": "Dropout layer is broken", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10\r\n- TensorFlow installed from (source or binary): conda default channel\r\n- TensorFlow version (use command below): 1.12 (conda version mkl_py36h69b6ba0_0)\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nThe Dropout layer always acts as if it is in testing mode.\r\n\r\n**Describe the expected behavior**\r\nThe Dropout layer switches between training and testing.\r\n\r\n**Code to reproduce the issue**\r\nThe tutorial code available from the [hompage](https://www.tensorflow.org/tutorials/):\r\n\r\n    import tensorflow as tf\r\n    mnist = tf.keras.datasets.mnist\r\n    \r\n    (x_train, y_train),(x_test, y_test) = mnist.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n    \r\n    model = tf.keras.models.Sequential([\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n      tf.keras.layers.Dropout(0.2),\r\n      tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n    ])\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    \r\n    model.fit(x_train, y_train, epochs=5)\r\n    model.evaluate(x_test, y_test)\r\n\r\nSetting the droprate to 0.999 yields the same performance as 0.2 (well above 95% acc). This is not possible, it means that the training/training switch inside the Dropout layer is broken, it is always returning the `inputs`, even during the training phase. \r\n\r\nWith tensorflow 1.10, the tutorial script gives  the \"correct\" result which is ~11% accuracy with 99.9% droprate.\r\n\r\n\r\n**Other info / logs**\r\nIt seems that either `tensorflow.keras.backend.learning_phase` is at the root of the problem, or `model.fit` doesn't correctly sets the training flag.\r\n", "comments": ["@randolf-scholz I test it on tf-nightly(1.13.0.dev20190124). The results are in below and look correct.\r\n\r\nWhen drop_rate = 0.99\r\n```\r\nEpoch 1/5\r\n60000/60000==============================] - 5s 76us/sample - loss: 2.2957 - acc: 0.1542\r\nEpoch 2/5\r\n60000/60000==============================] - 4s 74us/sample - loss: 2.1568 - acc: 0.1891\r\nEpoch 3/5\r\n60000/60000==============================] - 4s 73us/sample - loss: 2.1143 - acc: 0.2052\r\nEpoch 4/5\r\n60000/60000==============================] - 4s 73us/sample - loss: 2.0843 - acc: 0.2174\r\nEpoch 5/5\r\n60000/60000==============================] - 4s 73us/sample - loss: 2.0794 - acc: 0.2218\r\n10000/10000==============================] - 0s 24us/sample - loss: 1.1996 - acc: 0.8066\r\n```\r\nWhen drop_rate = 0.2\r\n```\r\nEpoch 1/5\r\n60000/60000==============================] - 5s 84us/sample - loss: 0.2183 - acc: 0.9349\r\nEpoch 2/5\r\n60000/60000==============================] - 5s 82us/sample - loss: 0.0980 - acc: 0.9704\r\nEpoch 3/5\r\n60000/60000==============================] - 5s 79us/sample - loss: 0.0669 - acc: 0.9789\r\nEpoch 4/5\r\n60000/60000==============================] - 5s 80us/sample - loss: 0.0527 - acc: 0.9829\r\nEpoch 5/5\r\n60000/60000==============================] - 5s 79us/sample - loss: 0.0433 - acc: 0.9856\r\n10000/10000==============================] - 0s 27us/sample - loss: 0.0697 - acc: 0.9801\r\n```", "+1. The results are correct with tf-nightly. Please test against tf-nightly. Feel free to reopen if still running into problems. Thanks!", "@aselle I don't know when this was fixed, but this may be important enough to cherry-pick into 1.13 if it wasn't fixed before the branch cut.", "Also run the tests on ` tensorflow==1.13.0rc0`. The results look right.", "Thanks @feihugis! ", "@martinwicke tested dropout with 1.12 (from PyPI), works fine. We have unit tests for this, always had.\r\n", "Hmm... strange thing to be broken by a conda rebuild. But then... who knows.", "@fchollet @martinwicke \r\n\r\nThe problems seems to appear in `tf.keras`, but not in the standalone `keras` package. The issue also seems affect the  `keras.backend.in_train_phase`  function. I tested it with the following files:\r\n\r\n- [https://pastebin.com/SvTwrSC8](https://pastebin.com/SvTwrSC8)\r\n\r\n- [https://pastebin.com/VtBwL5mz](https://pastebin.com/VtBwL5mz)\r\n", "@martinwicke tested dropout with 1.12 (from PyPI and in colab), with the above code. Does not work fine.\r\n\r\n", "Lets leave the issue open while we figure out more details and provide updates here.", "@randolf-scholz: thank you for taking the time to report this.\r\n\r\nTo be fair I have not done a full investigation and I have not looked at what was wrong with the script posted, or at which commit its behavior might have started changing.\r\n\r\nHere is the test script I have used. I have only tested it with 1.13 and 1.12 on CPU. I have tested it in eager and graph mode. The script rigorously test that the Dropout layer injects ones in the forward pass activations of the Dense layer below it.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint('tf.version:', tf.__version__)  # 1.12.0\r\n\r\n# tf.enable_eager_execution()  # try with eager or not\r\n\r\ninputs = keras.Input(shape=(100,))\r\nx = keras.layers.Dense(100,\r\n                       kernel_initializer='zeros',\r\n                       bias_initializer='ones',\r\n                       trainable=False)(inputs)\r\nx = keras.layers.Dropout(0.5)(x)\r\nmodel = keras.Model(inputs, x)\r\nmodel.compile(tf.train.RMSPropOptimizer(0.1), 'mse')\r\n\r\nprint(model.train_on_batch(np.ones((1, 100)), np.ones((1, 100))))  # Positive loss\r\nmodel.fit(np.ones((1, 100)), np.ones((1, 100)))  # Positive loss\r\n\r\nprint(model.test_on_batch(np.ones((1, 100)), np.ones((1, 100))))  # 0 loss\r\nmodel.evaluate(np.ones((1, 100)), np.ones((1, 100)))  # 0 loss\r\n```\r\n\r\nIn addition to this, we have correctness tests for dropout in our test suite (multiple in fact, because we typically use a dropout layer as a way to test whether the learning phase was correctly set, which is something we have to test for a range of model types and situations).\r\n\r\nTo be the best I can tell tf.keras.layers.Dropout() is working normally in TF 1.12 in training and inference modes with `fit`, `train_on_batch`, `test_on_batch`, `evaluate`.", "I have now done a full investigation of the issue with the script posted. I confirm that incorrect behavior is present in 1.11 and 1.12.\r\n\r\nThis behavior is entirely unrelated to either the `Dropout` layer, or to the `in_train_phase` backend utility. The `Dropout` layer works completely fine.\r\n\r\nThe bug is an issue that occurs when using a `Sequential` model in \"deferred mode\". Deferred mode is a recently-introduce way to use `Sequential` without passing an `input_shape` argument as first layer. It's looking like the learning phase value was incorrectly set in this case.\r\n\r\nI think this would not have affected many users since \"deferred mode\" for `Sequential` is new, and not widely used, because hardly any Keras example use it (none on keras.io). It is unfortunate that some of our tutorials on tensorflow.org have started using it.\r\n\r\nI'm contacting devrel to make sure that the code examples are updated to add `input_shape` arguments in our Sequential models (which is better practice anyway since it allows for static layer compatibility checks).\r\n\r\nYou can fix the issue in the script by simply passing `input_shape=(28, 28)` to the first flatten layer.\r\n\r\nThis bug has been fixed some time ago on the TF side."]}, {"number": 25174, "title": "Why does tensorflow element not forget origin after using .eval()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: Python 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am using tensorflow_probability (0.5.0) to sample points (random walk metropolis algorithm) that I want to use as inputs for my neural network. The sample points are Tensor(\"mcmc_sample_chain/current_state:0\", shape=(), dtype=float32) so I use tf.eval() to get the actual float number. After multiplying the input with my first layer I get as an object Tensor(\"mcmc_sample_chain/mh_bootstrap_results/rwm_bootstrap_results/cond/Add:0\", shape=(1, 16), dtype=float32).\r\n\r\n\r\n**Describe the expected behavior**\r\nI would expect that tf.eval() gives me a numpy float/array and therefore the ouput after multiplying with my first layer should result in something like Tensor(\"Add_12:0\", shape=(1, 16), dtype=float32). Why does the numpy array/float \"remember its origin\"?\r\n", "comments": ["@timudk Could you provide a code to reproduce the bug? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 25173, "title": "ImportError: DLL load failed: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Windows 10 64bits\r\n\r\n- TensorFlow installed from (source or binary): anaconda\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.5 | Acaconda INC\r\n- Installed using virtualenv? pip? conda?:conda\r\n\r\n- CUDA/cuDNN version: none, there is no card board\r\n- GPU model and memory: none\r\n\r\n\r\n\r\n**Describe the problem**\r\nI can\u00b4t run keras installed by anaconda in a pc without car board.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nrunfile('C:/Users/CAST/Desktop/train/IA_2.py', wdir='C:/Users/CAST/Desktop/train')\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-1706b5d23745>\", line 1, in <module>\r\n    runfile('C:/Users/CAST/Desktop/train/IA_2.py', wdir='C:/Users/CAST/Desktop/train')\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/CAST/Desktop/train/IA_2.py\", line 16, in <module>\r\n    import keras\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["I suspect you have installed TF - GPU version with cuda compatibility issue underneath. Can you please confirm? Also can you please describe the steps you followed to install TF? Thanks!", "Ymodak, how can I check that?\n\nAbout the installation process I used anaconda and selected the TensorFlow and Keras package for installation. ", "Can you reinstall keras and check if you can import tensorflow successfully,\r\n> conda install -c conda-forge keras ", "ymodak this is the result:\r\n\r\n> Solving environment: done\r\n> \r\n> \r\n> ==> WARNING: A newer version of conda exists. <==\r\n>   current version: 4.5.12\r\n>   latest version: 4.6.2\r\n> \r\n> Please update conda by running\r\n> \r\n>     $ conda update -n base -c defaults conda\r\n> \r\n> \r\n> \r\n> ## Package Plan ##\r\n> \r\n>   environment location: C:\\ProgramData\\Anaconda3\r\n> \r\n>   added / updated specs:\r\n>     - keras\r\n> \r\n> \r\n> The following NEW packages will be INSTALLED:\r\n> \r\n>     libgpuarray:         0.7.6-hfa6e2cd_1003       conda-forge\r\n>     mako:                1.0.7-py_1                conda-forge\r\n>     pygpu:               0.7.6-py36h452e1ab_1000   conda-forge\r\n>     theano:              1.0.4-py36h6538335_1000   conda-forge\r\n>     vs2015_win-64:       14.0.25123-h17c34da_3     conda-forge\r\n> \r\n> The following packages will be UPDATED:\r\n> \r\n>     certifi:             2018.4.16-py36_0                      --> 2018.4.16-py36_0          conda-forge\r\n>     conda:               4.5.12-py36_0                         --> 4.6.2-py36_0              conda-forge\r\n> \r\n> The following packages will be DOWNGRADED:\r\n> \r\n>     keras:               2.2.4-0                               --> 2.2.2-py36_0              conda-forge\r\n>     keras-applications:  1.0.6-py36_0                          --> 1.0.4-py_1                conda-forge\r\n>     keras-base:          2.2.4-py36_0                          --> 2.2.2-py36_0\r\n>     keras-preprocessing: 1.0.5-py36_0                          --> 1.0.2-py_1                conda-forge\r\n>     tensorboard:         1.12.0-py36he025d50_0                 --> 1.10.0-py36_0             conda-forge\r\n>     tensorflow:          1.12.0-gpu_py36ha5f9131_0             --> 1.10.0-py36_0             conda-forge\r\n>     tensorflow-base:     1.12.0-gpu_py36h6e53903_0             --> 1.10.0-gpu_py36h6e53903_0\r\n> \r\n> Proceed ([y]/n)? y\r\n> \r\n> Preparing transaction: done\r\n> Verifying transaction: done\r\n> Executing transaction: done\r\n\r\n\r\n", "It shows that you have TF-GPU installed on your system. Since you don't have a compatible gpu config on your system you can uninstall the tf-gpu version and install regular tf version.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25172, "title": "Reason for big performance difference between TFLite and TF Mobile app? Protobuf and .tflite file as reason?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No I used the DetectorActivity of android mobile demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) and the TensorFlow Lite demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04.1 LTS and LineageOS Android 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5\r\n- TensorFlow installed from (source or binary): compiled from source with cuda 10 and bazel\r\n- TensorFlow version ( use command below): in Python 2.7 (command python): (tf.GIT_VERSION, tf.VERSION) = ('v1.12.0-0-ga6d8ffae09', '1.12.0') in Python 3.6.7 (command python3): b'v1.9.0-rc2-5108-g4e06be5f8f' 1.12.0-rc0 \r\n- Python version: Python 2.7.15rc1 and Python 3.6.7\r\n- Bazel version (if compiling from source): either 0.20.0 or 0.21.0 (I don't know with which version I compiled the tensorflow installed in python but I used 0.20.0 in order to execute the bazel commands for the transform of the protobuf to the tflite file - I think it wasn't possible to use bazel 0.21 therefore)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: Cuda 10.0 und cuDNN 7.3.1\r\n- GPU model and memory:  GeForce GTX 970, RAM total: 16345820 and RAM swap: 2097148\r\n\r\n**Describe the current behavior**\r\n\r\nBecause the calculation of  the classification and the bounding boxes in the DetectorActivity (from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) with the model ssd_mobilenet_v1_android_export.pb (https://www.dropbox.com/home/Modelfile?preview=ssd_mobilenet_v1_android_export.pb) took 200ms - I rely to the runtime of the native method run in the TensorFlowInferenceInterface.class - I compared the runtime of the native method run() in the Interpreter.class of the classifier (no bounding boxes) from the tensorflow lite example app under https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo with the models \"mobilenet_v1_1.0_224.tflite\" and \"mobilenet_v1_1.0_224_quant.tflite\" that were autodonwloaded into asserts. So I assumed that that huge performance is because of the fact that the TFLite example app uses a model ending with .tflite and the mobile app a protobuf file. \r\nIs this a possible reason or is bounding box calculation such a expensive operation? On model zoo the ssd_mobilenet_v1 model should be provide results within ~30 ms. \r\n\r\nOn https://www.tensorflow.org/lite/apis#loading_a_model_2 under the heading Running Model Inference I found a hint to https://www.tensorflow.org/lite/convert/cmdline_examples. This points out that you have to optimize you model. Can the fact that the .pb files aren't optimezed be another reason for the bad performance? \r\n\r\n**Describe the expected behavior**\r\nI expected that the runtime for classification (and bounding box calculation) in Tensorflow lite and Tensorflow mobile demo app doesn't differ so much!\r\n**Code to reproduce the issue**\r\n1) download the two demo apps and import them to Android Studio\r\n2) print you a timestamp before and after the execution of the following methods:\r\n- Tensorflow Lite demo app: In the ImageClassifier.java in the method classifyFrame \r\n\r\n` long startTime = SystemClock.uptimeMillis();\r\n    runInference();\r\n    long endTime = SystemClock.uptimeMillis();\r\n    Log.d(TAG, \"Timecost to run model inference: \" + Long.toString(endTime - startTime));`\r\n\r\n- Tensorflow Mobile demo app: in DetectorActivity in the method processImage() runInBackground {} put the code \r\n`                        final long startTime = SystemClock.uptimeMillis();\r\n                        final List<Classifier.Recognition> results = detector.recognizeImage(croppedBitmap);\r\n                        lastProcessingTimeMs = SystemClock.uptimeMillis() - startTime;` \r\naround the recognizeImage() method.\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["Why do you expect to be able to compare the performance of different systems without using the same model?", "The reason why I did this is the fact that I get the error described in #25171 when I include a .tflite model generated from a protobuf file. Apart from that, I couldn't find a .tflite and the suiting .pb file in the model zoo and the web. ", "Is it possible that the calculation of the bounding box detection of the ssd_mobinenet (e.g. ssd_mobilenet_v1_quantized_coco and ssd_mobilenet_v2_quantized_coco) models takes on a Android phone with 8 CPU cores about 300ms? Are the 30 ms calculated on a high-performance-cluster? This might be an explanation for the runtime difference from above.\r\n@Arjuna197 \r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md", "@defaultUser3214 : Those numbers are probably not on device numbers. You can use one of the [benchmarking tools](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark) to benchmark the model if you want to test the model latency outside your app."]}, {"number": 25171, "title": "Runtime error: failed to find input Node 'image_tensor' after conversion from protobuf to tflite model file", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No I used the DetectorActivity of android mobile demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) and the TensorFlow Lite demo app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 18.04.1 LTS and LineageOS Android 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5\r\n- TensorFlow installed from (source or binary): compiled from source with cuda 10 and bazel\r\n- TensorFlow version ( use command below): in Python 2.7 (command python): (tf.GIT_VERSION, tf.VERSION) = ('v1.12.0-0-ga6d8ffae09', '1.12.0') in Python 3.6.7 (command python3): b'v1.9.0-rc2-5108-g4e06be5f8f' 1.12.0-rc0 \r\n- Python version: Python 2.7.15rc1 and Python 3.6.7\r\n- Bazel version (if compiling from source): either 0.20.0 or 0.21.0 (I don't know with which version I compiled the tensorflow installed in python but I used 0.20.0 in order to execute the bazel commands for the transform of the protobuf to the tflite file - I think it wasn't possible to use bazel 0.21 therefore)\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: Cuda 10.0 und cuDNN 7.3.1\r\n- GPU model and memory:  GeForce GTX 970, RAM total: 16345820 and RAM swap: 2097148\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nBecause I assumed that protobuf files were much slower than .tflite files I tried to converted a .pb to a .tflite:\r\nThus I downloaded the r1.95 branch of Tensorflow and converted the frozen_inference_graph.pb from (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) to a .tflite using https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630 and https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-456853685. This worked well!\r\n\r\nThe .pb file worked well with my Android App, but after copying the .tflite model to the app/assets directory of the TensorFlow Mobile demo App (from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) and replacing the .pb file in the code with\r\n`   private static final DetectorMode MODE = DetectorMode.TF_OD_API;\r\n   \r\n    private static final int TF_OD_API_INPUT_SIZE = 300;\r\n    private static final String TF_OD_API_MODEL_FILE =\r\n            \"file:///android_asset/frozen_inference_graph.pb\";\r\n    private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/coco_labels_list.txt\";`\r\n\r\nthe following runtime error appears:\r\n\r\n`\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: myPackage.myProcess, PID: 15491\r\n    **java.lang.RuntimeException: Failed to find input Node 'image_tensor'**\r\n        at myPackage.myProcess.myClass.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:106)\r\n        at myPackage.myProcess.myClass.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:146)\r\n        at myPackage.myProcess.myClass.CameraActivity$5.onPreviewSizeChosen(CameraActivity.java:370)\r\n        at myPackage.myProcess.myClass.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:412)\r\n        at myPackage.myProcess.myClass.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:419)\r\n        at myPackage.myProcess.myClass.CameraConnectionFragment.access$000(CameraConnectionFragment.java:66)\r\n        at myPackage.myProcess.myClass.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:97)\r\n        at android.view.TextureView.getHardwareLayer(TextureView.java:390)\r\n`\r\nI think there is a similar issue #22565 .\r\n\r\n**Describe the expected behavior**\r\nI would have expected that the .tflite version works because the .pb version of the same ssd_model works well!\r\n**Code to reproduce the issue**\r\n1) Download the Tensorflow Mobile Demo App from the Link from above\r\n2) Compile the tensorflow 1.12 version from source using the mentioned cuda settings\r\n3) Download the model from model zoo\r\n4) Execute the bazel commands from https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630 and https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-456853685\r\n\r\n\r\n# Download and extract SSD MobileNet model\r\nwget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\r\ntar -xvf ssd_mobilenet_v1_coco_2017_11_17.tar.gz \r\nDETECT_PB=$PWD/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb\r\nSTRIPPED_PB=$PWD/frozen_inference_graph_stripped.pb\r\nDETECT_FB=$PWD/tensorflow/contrib/lite/examples/android/assets/mobilenet_ssd.tflite\r\n\r\n# Strip out problematic nodes before even letting TOCO see the graphdef\r\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n--alsologtostderr\r\n\r\n# Run TOCO conversion.\r\nbazel run tensorflow/lite/toco:toco -- \\\r\n--input_file=$STRIPPED_PB --output_file=$DETECT_FB \\\r\n--input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n--input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub \\\r\n--output_arrays=concat,concat_1 --inference_type=FLOAT --logtostderr\r\n\r\n# Build and install the demo\r\nbazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo\r\nadb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\r\n\r\n\r\n5) Then you have to copy the .tflite model to you apps app/assert directory and refer to it in you code like mentioned above\r\n6) After the gradle build and running the app on you phone you might probably get the runtime error I did. \r\n\r\n**Other info / logs**\r\n\r\n[bug_tracker_bazel_run_warnings.txt](https://github.com/tensorflow/tensorflow/files/2792167/bug_tracker_bazel_run_warnings.txt)\r\n[bug_tracker_runtime_error.txt](https://github.com/tensorflow/tensorflow/files/2792169/bug_tracker_runtime_error.txt)\r\n\r\n", "comments": ["I really cannot tell what the app you tried to use is TF Mobile one or TFLite one. If what you want to try is the TFLite one, you should read [this article](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) first.", "Thanks, the bazel build command in that article solved my problem. Do you know a method how to get the output dimensions of the mobile_ssd_v2_coco file ( found under  https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/mobile_ssd_v2_float_coco.tflite) that is referred at the tutorial https://www.tensorflow.org/lite/performance/gpu#supported_models_and_ops? I would like to know how my outputMap has to be structured:\r\n   `tflite.runForMultipleInputsOutputs(inputArray,outputMap);`\r\n\r\nI have tried to use \r\n\r\n`\r\nbazel run tensorflow/tools/graph_transforms:summarize_graph -- --in_graph=/abolutePath/src/main/assets/mobile_ssd_v2_float_coco.tflite\r\n`\r\n\r\nNo inputs spotted.\r\nNo variables spotted.\r\nNo outputs spotted.\r\nFound 0 (0) const parameters, 0 (0) variable parameters, and 0 control_edges\r\nOp types used: \r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=/home/stephie/Documents/BA/Tensorflow-stable-v1.12.0/tensorflow/tensorflow/lite/java/demo/app/src/main/assets/mobile_ssd_v2_float_coco.tflite --show_flops --input_layer= --input_layer_type= --input_layer_shape= --output_layer=\r\n\r\nWhen I analyse a .pb file the command even doesn'tell my how my outputMap has to be formed.\r\n\r\nThank you so much!!", "`summary_graph` is for TensorFlow's .pb files. It doesn't handle TFLite FlatBuffer file. One line python script like this, let's call it `foo.py`, could print output nodes\r\n```python\r\nimport sys\r\nfrom tensorflow.lite.python import interpreter as interpreter_wrapper\r\n\r\ninterpreter = interpreter_wrapper.Interpreter(model_path=sys.argv[1])\r\nprint(interpreter.get_output_details())\r\n```\r\nRun it, \r\n```\r\n> python /tmp/foo.py /tmp/mobile_ssd_v2_float_coco.tflite\r\n{'index': 307, 'shape': array([   1, 2034,    4], dtype=int32), 'quantization': (0.0, 0L), 'name': 'raw_outputs/box_encodings', 'dtype': <type 'numpy.float32'>}, {'index': 308, 'shape': array([   1, 2034,   91], dtype=int32), 'quantization': (0.0, 0L), 'name': 'raw_outputs/class_predictions', 'dtype': <type 'numpy.float32'>}\r\n```\r\n\r\nIt seems the model doesn't come with post-processing nodes.", "@freedomtan Thank you so much for you answer, which so super helpful!! Could you please tell me where I can find some documentation about pre-/ post-processing nodes of models that are useable in Tensorflow Lite? I have already searched but only found stuff describing that post-processing of bounding boxes is drawing them. That was not so helpful.", "@freedomtan the image_tensor error from above only appears in the tfMobile App but in the tfLite demo app it could be resolved by using your link. Is tfMobile only able to use protobuf files and this is the reason for the error?", "@defaultUser3214 \r\nwere you able to find anything for the post-processing of the object detection model used with tflite gpu snippet ? ", "@defaultUser3214 Surely TFLite doesn't have image_tensor problem, because there is no such node if you follow [the article](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) I mentioned. I don't know if there is any documentation on preprocessing and postprocessing of the model. I guess most people figure it out by reading paper and source code.", "@freedomtan Is this question regarding the CPU execution or the GPU execution? The CPU execution was specified in  the [article](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)", "@achowdhery I don't have question. I was trying to answer @defaultUser3214's question :-) And, yes, I think it's CPU execution question.", ">  the image_tensor error from above only appears in the tfMobile App but in the tfLite demo app it could be resolved by using your link. Is tfMobile only able to use protobuf files and this is the reason for the error?\r\n\r\nTFMobile consumes the frozen graphs (.pb) files, whereas TFLite consumes converted flatbuffer (.tflite) files. They are incompatible and not interchangeable. Do you have a specific question for TensorFlow Lite execution?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 25170, "title": "TFLite model returns completely wrong results.", "body": "**System information**\r\n- Have I written custom code: YES\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip (`pip install tf-gpu-nightly`)\r\n- TensorFlow version (use command below): b'v1.12.0-6574-gee5c4383f9' 1.13.0-dev20190124\r\n(Installed `tf-gpu-nigtly` version about two weeks ago, so decided to test everything again in current nigly version and problem remains).\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version:\r\n   CUDA: 10.0.130  (but have also 9.0.176 installed)\r\n   cuDNN: 7.4.2\r\n- GPU model and memory:  GeForce GTX 1070 \r\n\r\n**Intro**\r\n\r\nHi , \r\n\r\nI try to train resnet using `tf.keras` API, convert it to `Lite` version and then later use it on embedded system. \r\nI have struggled to convert model to Lite version, but finally when I managed to do it, the model converted seems to return completely different results. Below I provide simple example to reproduce the problem. I also highlited one particular line `tf.keras.backend.set_learning_phase(1)` and described there why it is important\r\n\r\n**Toy example to reproduce the problem**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom scipy import signal\r\n\r\n\r\nDATA_SHAPE = (50, 100, 1)  # single channel image\r\n\r\n\r\ndef prepare_simple_resnet(n_filters=10):\r\n\r\n    parameters_input = tf.keras.layers.Input(shape=DATA_SHAPE)\r\n\r\n    x = parameters_input\r\n    res_node = tf.keras.layers.Conv2D(n_filters, kernel_size=(3, 3), padding='same', use_bias=False)(x)\r\n    res_node = tf.keras.layers.ReLU()(res_node)\r\n    res_node = tf.keras.layers.BatchNormalization()(res_node)\r\n    res_node = tf.keras.layers.Conv2D(n_filters, kernel_size=(3, 3), padding='same', use_bias=False)(res_node)\r\n    res_node = tf.keras.layers.ReLU()(res_node)\r\n    res_node = tf.keras.layers.BatchNormalization()(res_node)\r\n\r\n    x = tf.keras.layers.add([res_node, x])\r\n\r\n    x = tf.keras.layers.Lambda(lambda z: tf.math.reduce_mean(z, axis=[1, 2]))(x)   # mean for each filter\r\n    x = tf.keras.layers.Dense(2)(x)  # output two classes\r\n\r\n    model = tf.keras.models.Model(inputs=parameters_input, outputs=x)\r\n\r\n    return model\r\n\r\n\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    \"\"\" Generates random images, which may or may not be blurred\"\"\"\r\n\r\n    def __init__(self, batch_size=32, random_state=None):\r\n        self.batch_size = batch_size\r\n        if random_state is None:\r\n            self.random_state = np.random.RandomState()\r\n        else:\r\n            self.random_state = random_state\r\n\r\n        self.averaging_filter = np.ones((3, 3))\r\n\r\n    def __len__(self):  # returns number of batches per epoch, not dataset size\r\n        return 50\r\n\r\n    def __getitem__(self, index):  # returns batch of data\r\n        batch = []\r\n        labels = []\r\n        for i in range(self.batch_size):\r\n            single_data, label = self.generate_single_data_sample()\r\n            batch.append(single_data)\r\n            labels.append(label)\r\n        return np.array(batch), np.array(labels)\r\n\r\n    def generate_single_data_sample(self):\r\n        is_blurred = self.random_state.randint(0, 2)\r\n        data_sample = self.random_state.rand(*DATA_SHAPE[0:2])\r\n        if is_blurred:\r\n            data_sample = signal.convolve2d(data_sample, self.averaging_filter, mode='same')\r\n            label = np.array([0, 1])\r\n        else:\r\n            label = np.array([1, 0])\r\n        data_sample = np.expand_dims(data_sample, -1).astype(np.float32)\r\n\r\n        return data_sample, label\r\n\r\n\r\nif __name__ == '__main__':\r\n    checkpoint_path = '/tmp/test_checkpoint.ckpt'\r\n    tflite_file_path = '/tmp/tflite_model.tflite'\r\n    random_state = np.random.RandomState(seed=0)\r\n\r\n    with tf.device(\"/gpu:0\"):\r\n\r\n        ############################################################################################\r\n        # LINE BELOW IS IMPORTANT (FOR SOME REASON)\r\n        #  - if set to:\r\n        #        tf.keras.backend.set_learning_phase(0)\r\n        #    model of course is not training, but Lite model returns almost same results as tf.keras model\r\n        #  - if set to:\r\n        #        tf.keras.backend.set_learning_phase(1)\r\n        #    model is training, good acc, but lite model returns completely different results\r\n        #  - if commented/removed\r\n        #    model is training, but there is error in converting model to Lite version\r\n        tf.keras.backend.set_learning_phase(1)\r\n        ############################################################################################\r\n\r\n        # train tf.keras model\r\n        data_generator = DataGenerator(random_state=random_state)\r\n        tfkeras_resnet = prepare_simple_resnet()\r\n\r\n        tfkeras_resnet.compile(\r\n            optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9, decay=0.00001),\r\n            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n            metrics=['accuracy'],\r\n        )\r\n        cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=False,\r\n                                                         save_best_only=True, verbose=1)\r\n        tfkeras_resnet.fit_generator(generator=data_generator, validation_data=data_generator,\r\n                                     use_multiprocessing=False, epochs=5, callbacks=[cp_callback])\r\n\r\n        # test some random data before and after changing learning phase\r\n        random_data = random_state.rand(1, *DATA_SHAPE).astype(np.float32)\r\n        result_tfkeras_train = tfkeras_resnet.predict(random_data)\r\n        tf.keras.backend.set_learning_phase(0)\r\n        result_tfkeras_test = tfkeras_resnet.predict(random_data)\r\n\r\n        # Convert to tensorflow lite\r\n        session = tf.keras.backend.get_session()\r\n        converter = tf.lite.TFLiteConverter.from_session(session, [tfkeras_resnet.input],\r\n                                                         [tfkeras_resnet.outputs[0]])\r\n        tflite_model = converter.convert()\r\n        with open(tflite_file_path, 'wb') as f:\r\n            f.write(tflite_model)\r\n\r\n        interpreter = tf.lite.Interpreter(model_path=tflite_file_path)\r\n        interpreter.allocate_tensors()\r\n\r\n        # Get input and output tensors.\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n\r\n        # Test model on random input data.\r\n        input_shape = input_details[0]['shape']\r\n        input_data = np.array(random_data, dtype=np.float32)\r\n        interpreter.set_tensor(input_details[0]['index'], input_data)\r\n        interpreter.invoke()\r\n        result_tflite = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n        print(\"TFKeras Train: {}\".format(result_tfkeras_train))\r\n        print(\"TFKeras Test: {}\".format(result_tfkeras_test))\r\n        print(\"TFLite: {}\".format(result_tflite))\r\n\r\n```", "comments": ["I have similar problem to yours and no idea how to solve it. ", "Can you try using `from_keras_model_file` instead of `from_session` for your conversion process and see if that resolves your issue? Documentation is [here](https://www.tensorflow.org/lite/convert/python_api#exporting_a_tfkeras_file_).", "I was trying to do that earlier but had some other problems now I don't remember. Why there are so many ways of doing things while none of them works? :(\r\n\r\nI found that the problem was certainly with Batch Normalization layer. \r\n\r\nI solved my problem by:\r\n1) creating separate test and train net\r\n2) creating custom Batch Normalization layer for test network (comprising only multiplication and add bias layers)\r\n3) copying weights from train to test net, for all layers except BN\r\n4) as for BN, using weights of trained BN to compute weights of multiplication and bias (it is not straightforward, you need to take a look at BN paper)\r\n\r\nI can provide fixed example I posted earlier.", "I have the same problem. A keras model converted to tflite model, returned different resluts. After one day debugging, I just removed Batch Normalization layer and retrained keras model,  convert to tflite , the results are correct.  So as dankal444 said, the problem was certainly with Batch Normalization layer.", "> I have the same problem. A keras model converted to tflite model, returned different resluts. After one day debugging, I just removed Batch Normalization layer and retrained keras model, convert to tflite , the results are correct. So as dankal444 said, the problem was certainly with Batch Normalization layer.\r\n\r\nI'm having this problem too, what is thesolution to it?", "> > I have the same problem. A keras model converted to tflite model, returned different resluts. After one day debugging, I just removed Batch Normalization layer and retrained keras model, convert to tflite , the results are correct. So as dankal444 said, the problem was certainly with Batch Normalization layer.\r\n> \r\n> I'm having this problem too, what is thesolution to it?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/25170#issuecomment-472613815\r\n\r\nBut I just removed BN, the accuracy didn't drop too much for my model.", "> > > I have the same problem. A keras model converted to tflite model, returned different resluts. After one day debugging, I just removed Batch Normalization layer and retrained keras model, convert to tflite , the results are correct. So as dankal444 said, the problem was certainly with Batch Normalization layer.\r\n> > \r\n> > \r\n> > I'm having this problem too, what is thesolution to it?\r\n> \r\n> [#25170 (comment)](https://github.com/tensorflow/tensorflow/issues/25170#issuecomment-472613815)\r\n> \r\n> But I just removed BN, the accuracy didn't drop too much for my model.\r\n\r\nYou removed BN from training code?", "> > > > I have the same problem. A keras model converted to tflite model, returned different resluts. After one day debugging, I just removed Batch Normalization layer and retrained keras model, convert to tflite , the results are correct. So as dankal444 said, the problem was certainly with Batch Normalization layer.\r\n> > > \r\n> > > \r\n> > > I'm having this problem too, what is thesolution to it?\r\n> > \r\n> > \r\n> > [#25170 (comment)](https://github.com/tensorflow/tensorflow/issues/25170#issuecomment-472613815)\r\n> > But I just removed BN, the accuracy didn't drop too much for my model.\r\n> \r\n> You removed BN from training code?\r\n\r\nyep"]}, {"number": 25169, "title": "Replace deprecated usage of np.asscalar with np.ndarray.item()", "body": "[`numpy.asscalar()` is deprecated since version 1.16](https://github.com/numpy/numpy/blob/master/numpy/lib/type_check.py#L519-L548).\r\n\r\nThis PR replaces its usage with [`numpy.ndarray.item()`](https://www.numpy.org/devdocs/reference/generated/numpy.ndarray.item.html)", "comments": ["@revan It looks like you reverted this PR in 51c7409562e917fd3fe2012ae57699547d0aebec\r\n\r\nIs there anything I can do to get this change accepted?", "Some tests failed on our nightly windows build.\n\nOn Mon, Jan 28, 2019 at 9:10 AM Lukas Geiger <notifications@github.com>\nwrote:\n\n> @revan <https://github.com/revan> It looks like you reverted this PR in\n> 51c7409\n> <https://github.com/tensorflow/tensorflow/commit/51c7409562e917fd3fe2012ae57699547d0aebec>\n>\n> Is there anything I can do to get this change accepted?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25169#issuecomment-458216240>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWSNbc1VDMW8nORddJEF7VaFE7wYks5vHy8egaJpZM4aQ01F>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 25168, "title": "Error while calling TFLite interpreter - cannot import name cloud", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Stretch \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Applicable\r\n- TensorFlow installed from (source or binary):whl file\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: Pip3\r\n- Bazel version (if compiling from source): Not applicable\r\n- GCC/Compiler version (if compiling from source): Not applicable\r\n- CUDA/cuDNN version: Not applicable\r\n- GPU model and memory: Not applicable\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm using a Raspberry Pi 3B+, with Raspbian Stretch as my OS.\r\n\r\nI installed Tensorflow 1.12 using [this guide](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-on-the-Raspberry-Pi), I downloaded the latest versions of TensorFlow and Protobuf, but as I was trying out [this guide](https://github.com/freedomtan/tensorflow/blob/deeplab_tflite_python/tensorflow/contrib/lite/examples/python/object_detection.py), I hit an error:\r\n\r\n`File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 39, in <module>\r\n    from tensorflow.contrib import cloud\r\nImportError: cannot import name 'cloud'` \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe steps that I had followed are based off of the guide linked above, I had used the updated compiled TensorFlow wheels and the updated Protobuf as well.\r\n\r\nHere are the exact steps I took:\r\nTensorFlow Lite Guide\r\n\r\nYou may do the following in the RPi itself, or connect via SSH\r\n\r\n\r\nIf you encounter the \u201cRemote Host Identification has changed\u201d message, you may have to edit the ~/.ssh/known_hosts file.\r\nsudo nano ~/.ssh/known_hosts\r\n\r\nremove the line for the IP Address of the RPi\r\n\r\n1. Delete bloatware\r\n$ sudo apt-get purge wolfram-engine\r\n$ sudo apt-get purge libreoffice*\r\n$ sudo apt-get clean\r\n$ sudo apt-get autoremove\r\n\r\n2. Install Dependencies\r\n$ sudo apt-get update\r\n$ sudo apt-get dist-upgrade\r\n\r\n3. Install Tensorflow\r\n$ mkdir tf && cd tf\r\n\r\ncheck: https://github.com/lhelontra/tensorflow-on-arm/releases \r\nfor latest tensorflow releases\r\n\r\n$ wget https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.12.0/tensorflow-1.12.0-cp35-none-linux_armv7l.whl\r\n\r\n$ sudo pip3 install /home/pi/tf/tensorflow-1.12.0-cp35-none-linux_armv7l.whl\r\n\r\n$ sudo apt-get install libatlas-base-dev\r\n$ sudo pip3 install pillow lxml jupyter matplotlib cython\r\n$ sudo apt-get install python-tk\r\n\r\n3. Install OpenCV\r\n$ sudo apt-get install libjpeg-dev libtiff5-dev libjasper-dev libpng12-dev\r\n$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev\r\n$ sudo apt-get install libxvidcore-dev libx264-dev\r\n$ sudo apt-get install qt4-dev-tools\r\n$ pip3 install opencv-python\r\n\r\n4. Install Protobuf\r\n$ sudo apt-get install autoconf automake libtool curl\r\ncheck https://github.com/protocolbuffers/protobuf/releases/ \r\nfor latest releases\r\n$ wget https://github.com/google/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz\r\n$ tar -zxvf protobuf-all-3.6.1.tar.gz\r\n$ cd protobuf-3.6.1\r\n$ ./configure\r\n$ make\r\n$ make check\r\n$ sudo make install\r\n$ cd python\r\n$ export LD_LIBRARY_PATH=../src/.libs\r\n$ python3 setup.py build --cpp_implementation \r\n$ python3 setup.py test --cpp_implementation\r\n$ sudo python3 setup.py install --cpp_implementation\r\n$ export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=cpp\r\n$ export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION=3\r\n$ sudo ldconfig\r\n$ protoc\r\n$ sudo reboot now\r\n\r\n$ cd ~\r\n$ mkdir tensorflow1 && cd tensorflow1\r\n$ git clone --recurse-submodules https://github.com/tensorflow/models.git\r\n\r\n$ sudo nano ~/.bashrc\r\nAdd: export PYTHONPATH=$PYTHONPATH:/home/pi/tensorflow1/models/research:/home/pi/tensorflow1/models/research/slim\r\n\r\n$ cd /home/pi/tensorflow1/models/research\r\n$ protoc object_detection/protos/*.proto --python_out=.\r\n\r\n$ cd /home/pi/tensorflow1/models/research/object_detection\r\n$ wget http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\r\n$ tar -xzvf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\r\n\r\nThen I ran:\r\n$ python3\r\n>> from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper\r\n But I hit an error, as shown below:\r\n\r\n**Any other info / logs**\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 47, in <module>\r\n    import numpy as np\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/numpy/__init__.py\", line 187, in <module>\r\n    from .testing import Tester\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/numpy/testing/__init__.py\", line 10, in <module>\r\n    from unittest import TestCase\r\n  File \"/usr/lib/python3.5/unittest/__init__.py\", line 64, in <module>\r\n    from .main import TestProgram, main\r\n  File \"/usr/lib/python3.5/unittest/main.py\", line 7, in <module>\r\n    from . import loader, runner\r\n  File \"/usr/lib/python3.5/unittest/runner.py\", line 8, in <module>\r\n    from .signals import registerResult\r\n  File \"/usr/lib/python3.5/unittest/signals.py\", line 1, in <module>\r\n    import signal\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/signal/__init__.py\", line 42, in <module>\r\n    from tensorflow.contrib.signal.python.ops.mel_ops import linear_to_mel_weight_matrix\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 39, in <module>\r\n    from tensorflow.contrib import cloud\r\nImportError: cannot import name 'cloud'\r\n\r\nI've checked the __init__.py code, and I'm kind of clueless as to what or how to approach this. I'm not sure if it's safe to just comment the line out. \r\n\r\nHas anyone solved this issue? \r\n\r\n", "comments": ["Unfortunately we can't support external builds like https://github.com/lhelontra/tensorflow-on-arm/releases, since we're not in control of how they're built. ", "Hi @Zeit42,\r\n\r\n>File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py\", line 39, in <module> from >tensorflow.contrib import cloud ImportError: cannot import name 'cloud'\r\n\r\nI have the same error, would you please kindly share your insights? Thanks.\r\n\r\nMy system infor\r\ntensorflow-1.13.1-cp35-none-linux_aarch64.whl\r\nLinux firefly 4.4.52 #16 SMP Thu Apr 27 17:56:29 HKT 2017 aarch64 aarch64 aarch64 GNU/Linux\r\nrk3399\r\n\r\nHi  @petewarden,\r\n> can't support external builds like https://github.com/lhelontra/tensorflow-on-arm/releases, \r\n\r\nNoted. Thanks.\r\n\r\n`\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=tflitefile)\r\n  File \"/media/root/usb16g1/venv/tf1.13/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n    module = self._load()\r\n  File \"/media/root/usb16g1/venv/tf1.13/lib/python3.5/site-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/media/root/usb16g1/venv/tf1.13/lib/python3.5/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 665, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/media/root/usb16g1/venv/tf1.13/lib/python3.5/site-packages/tensorflow/contrib/__init__.py\", line 30, in <module>\r\n    from tensorflow.contrib import cloud\r\nImportError: cannot import name 'cloud'\r\n`\r\n"]}, {"number": 25167, "title": "what is the FFT size of the tensorflow.audio_contrib.audio_spectrogram in input_audio.py?", "body": "in input_audio.py, is the FFT size equal to window_size?\r\nspectrogram = contrib_audio.audio_spectrogram(\r\n          background_clamp,\r\n          window_size=model_settings['window_size_samples'],\r\n          stride=model_settings['window_stride_samples'],\r\n          magnitude_squared=True)\r\n\r\nif it is, should the window size be 2's power? because most of time the FFT size is equal to 2's power.\r\n", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 25166, "title": " Text generation tutorial issue", "body": "**My issues relate to the following tutorial \"Text generation using a RNN with eager execution\"**: \r\n- https://www.tensorflow.org/tutorials/sequences/text_generation\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n\r\n(disclaimer: I am fairly new to using tensorflow, so please let me know if I am misunderstanding something!)\r\n\r\n**1) Questions about the 'Try the Model' section**\r\n`sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()`\r\n\r\nWhen I run this code above, my program would produce an error message:\r\n`AttributeError: module 'tensorflow._api.v1.random' has no attribute 'categorical'`\r\n\r\nI checked the Tensorflow API for 1.12, and indeed random does not have a 'categorical' attribute. So as per suggestion by a Stack Overflow comment, I am using the 'multinomial' attribute (namely  tf.random.multinomial instead of  tf.random.categorical) : https://www.tensorflow.org/api_docs/python/tf/random/multinomial\r\n\r\n**My questions:** Is this an adequate solution? Am I missing something, or does random indeed not have the categorical attribute? If so, would it be possible to update the tensorflow tutorial to prevent future users from running into this issue?\r\n\r\n**2) Questions about the 'Train the Model' section**\r\n`def loss(labels, logits):`\r\n                   `return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)`\r\n\r\n`example_batch_loss  = loss(target_example_batch, example_batch_predictions)`\r\n`print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") `\r\n`print(\"scalar_loss:      \", example_batch_loss.numpy().mean())`\r\n\r\nWhen I run this above code, my program would produce an error message:\r\n`sparse_categorical_crossentropy() got an unexpected keyword argument 'from_logits'`\r\n\r\nI checked the Tensorflow API for 1.12, and indeed tf.keras.LOSSES.sparse_categorical_crossentropy() (alias for metrics which I linked to) only has two parameters (neither of which are 'from_logits'):\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_categorical_crossentropy\r\n\r\nWhereas tf.keras.BACKEND.sparse_categorical_crossentropy() has four paramaters, one of which is\r\n'from_logits`: https://www.tensorflow.org/api_docs/python/tf/keras/backend/sparse_categorical_crossentropy\r\n\r\nSo I changed my program to use tf.keras.backend.sparse_categorical_crossentropy() instead of  tf.keras.losses.sparse_categorical_crossentropy().\r\n\r\n**My questions:** Is this an adequate solution? If the tutorial is indeed mistaken for using losses instead of backend, would it be possible to update it with this as well?\r\n\r\nThank you!\r\n\r\n\r\n\r\n\r\n", "comments": ["The tutorial installs tf-nightly build which contains TF 1.13rc0\r\n1) [tf.random.multinomial](https://github.com/tensorflow/tensorflow/blob/9f30ec59fda6e9134dff2c7d9cc42b3fb455f8b7/tensorflow/python/ops/random_ops.py#L330) will be deprecated starting from TF 1.13rc0 thus can use tf.random.categorical instead.\r\nThe work around will be to use tf.random.multinomial if using TF 1.12 or lower. \r\n2) Your workaround is correct.", "Thank you for your confirming my workarounds! \r\n\r\nThis all makes sense now, since I think I am using TF 1.12. Just one more question:\r\n\r\nWhen I run:\r\n`!pip install -q tf-nightly`\r\nand then :\r\n`!pip show tensorflow`\r\n\r\nI get the following output:\r\n`Name: tensorflow`\r\n`Version: 1.12.0`\r\n`Summary: TensorFlow is an open source machine learning framework for everyone.`\r\n`Home-page: https://www.tensorflow.org/`\r\n`Author: Google Inc.`\r\n`Author-email: opensource@google.com`\r\n`License: Apache 2.0`\r\n`Location: /usr/local/lib/python3.6/dist-packages`\r\n`Requires: gast, six, termcolor, keras-preprocessing, tensorboard, grpcio, absl-py, numpy, protobuf,` `keras-applications, astor, wheel`\r\n`Required-by: stable-baselines, magenta, fancyimpute`\r\n\r\nFor the version, shouldn't I be getting :\r\n`Version: 1.13rc0`\r\n\r\nsince I am using tf-nightly, as you described?", "I suspect you forgot to restart the runtime after tf-nightly installation. Can you please confirm?\r\nAlternatively, you can also try,\r\n>pip install tensorflow==1.13rc0", "Restarting the runtime didn't fix it, but running `pip install tensorflow==1.13rc0` did! Thank you for the help! ", "You are welcome! Closing this issue since its solved.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25165, "title": "Contrib package initialization is broken in 1.13.0.rc0", "body": "When I try to use any functionality from `contrib` package I get an exception.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 17.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **-**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **b'v1.13.0-rc0-0-g6ce86799c8' 1.13.0-rc0**\r\n- Python version: **3.6.3**\r\n- Bazel version (if compiling from source): **-**\r\n- GCC/Compiler version (if compiling from source): **-**\r\n- CUDA/cuDNN version: **-**\r\n- GPU model and memory: **-**\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to run `tensorflow/contrib/ignite/python/tests/ignite_dataset_test.py` (and any other code that imports from `contrib`) I get the following exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"ignite_dataset_test.py\", line 23, in <module>\r\n    from tensorflow.contrib.ignite import IgniteDataset\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib import distribute\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py\", line 33, in <module>\r\n    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py\", line 27, in <module>\r\n    from tensorflow.contrib.tpu.python.ops import tpu_ops\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py\", line 73, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\", line 62, in <module>\r\n    from tensorflow.contrib.tpu.python.tpu import tpu\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu.py\", line 24, in <module>\r\n    from tensorflow.contrib.compiler import xla\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/compiler/xla.py\", line 28, in <module>\r\n    from tensorflow.python.estimator import model_fn as model_fn_lib\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/__init__.py\", line 26, in <module>\r\n    from tensorflow_estimator.python import estimator\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/__init__.py\", line 25, in <module>\r\n    import tensorflow_estimator.python.estimator.estimator_lib\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator_lib.py\", line 54, in <module>\r\n    from tensorflow_estimator.python.estimator.mode_keys import ModeKeysV2\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/mode_keys.py\", line 22, in <module>\r\n    from tensorflow.python.training.mode_keys import ModeKeys\r\nModuleNotFoundError: No module named 'tensorflow.python.training.mode_keys'\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo exception\r\n\r\n**Code to reproduce the issue**\r\nInstall last released version of TensorFlow, `pip3 install tensorflow=1.13.0rc0`. Then run `tensorflow/contrib/ignite/python/tests/start_ignite.sh` to start server. Then run `python3 tensorflow/contrib/ignite/python/tests/ignite_dataset_test.py` and get an exception.\r\n\r\n**Other info / logs**\r\nSee above.\r\n", "comments": ["Folks, seriously, release `1.13` just broke the whole `contrib` package, all functionality because of error in `contrib/__init__.py`. Can somebody have a look at this issue?\r\n\r\nI guess the problem is that this commit https://github.com/tensorflow/tensorflow/commit/8bcc801a7cd748e7d9d47f0f5d7ebd84a2f2eaea (Create v2 ModeKeys class that's shared among TensorFlow...) is not included into `1.13`, but there is some code that relies on `tensorflow-estimator` in `contrib` package. At the same time, `tensorflow-estimator` has been built from `master` instead of `1.13` branch and pushed into central repository (if you have a look at the package you'll find there files that are not present in `1.13`).\r\n\r\nBTW, manual installation of `tensorflow-estimator==1.10.12` solves this problem.", "Hi, @case540, could you please have a look at this issue? It looks like a versions conflict you might be aware of.", "I opened this one: https://github.com/tensorflow/tensorflow/issues/25089\r\nwhich I think is the root of the problem.", "FYI I just closed #25089 . It looks like the Estimator 1.13 branch no longer has https://github.com/tensorflow/estimator/commit/f7640c1aa61e798fdde5001ed42237ab7f941667  and perhaps has been rebased altogether.", "@dmitrievanthony, we are aware the estimator integration is broken in rc0, and we are working to fix it. There is a new rc release of estimator 1.13. When installing python you should usually use \"--upgrade \" to make sure packages are updated to meet dependencies of tensorflow package. However, tensorflow 1.13 RC0 doesn't correctly call out the right version of estimator it needs which is what we are fixing in RC1.\r\n\r\n@case540, do you have anything to add?", "pip install tensorflow==1.13rc0\r\npip install tensorflow_estimator==1.13rc0\r\n\r\ndoesn't seem to have this issue. Issue should be resolved in rc1 when we update dependency in TF to install correct Estimator package version.\r\n\r\nSo yeah, this issue might be able to come up if you install tf-nightly then somehow downgrade tensorflow package without downgrading estimator package at the same time.", "Okay, @aselle, @case540 thanks for the explanation.", "Please do upgrade to a latest Tensorflow version.\r\nSince `contrib` has been depreciated in `Tensorflow 2.x`, you can find the `ignite` under `tfio`.\r\nFor more details please visit the documentation on [tfio.ignite](https://www.tensorflow.org/io/api_docs/python/tfio/ignite).\r\nFeel free to open a new issue, if you face any errors."]}, {"number": 25164, "title": "os.environ[\"CUDA_DEVICE_ORDER\"] ='0' not work in tensorflow ", "body": "I have two GPU, I want to create 2 graph in 2 GPU, first graph in first GPU, second graph in second GPU.\r\n\r\nwhen I create first graph in first GPU, I use os.environ[\"CUDA_DEVICE_ORDER\"] ='0'\r\n2.when I create second graph in second GPU, I use os.environ[\"CUDA_DEVICE_ORDER\"] = '1', but second graph still create on first GPU, I try many different ways, but it still not work.\r\nIt's a bug?  Please help me. \r\n@ry @jmhodges @eggie5 @bmabey @djones ", "comments": ["duplicate #25152"]}, {"number": 25163, "title": "TF Contrib typo-error fix", "body": "Typo error fixed", "comments": []}, {"number": 25162, "title": "Cant convert my model to tflite. Show the following error. (included code).", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):anaconda\r\n- TensorFlow version (use command below):latest\r\n- Python version:3.6.5\r\n- Bazel version (if compiling from source):No\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:No\r\n- GPU model and memory:920m\r\n\r\ncode used to convert the file:\r\nfrom tensorflow.contrib import lite\r\nkeras_file = \"movie_classifier.h5\"\r\nkeras.models.save_model(model, keras_file)\r\nconverter = lite.TocoConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()\r\nopen(\"linear.h5\" , wb).write(tflite_model)\r\n\r\n\r\nError is as follows:\r\nRuntimeError: TOCO failed see console for info.\r\nb'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\h5py\\\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\\r\\n  from ._conv import register_converters as _register_converters\\r\\nTraceback (most recent call last):\\r\\n  File \"C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"C:\\\\ProgramData\\\\Anaconda3\\\\Scripts\\\\toco_from_protos-script.py\", line 6, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python.toco_from_protos import main\\r\\n  File \"C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone", "comments": ["Can you try like below\r\n`from tensorflow import lite`\r\n\r\ninstead of\r\n`from tensorflow.contrib import lite`", "> Can you try like below\r\n> `from tensorflow import lite`\r\n> \r\n> instead of\r\n> `from tensorflow.contrib import lite`\r\n\r\n@ANSHUMAN87 \r\n\r\nI tried but its giving the following error\r\n\r\nImportError: cannot import name 'lite'", "Try to follow the steps in below link:\r\nhttps://www.tensorflow.org/install/pip\r\n\r\nI suspect because you are using Python 3.7, the package released does not support Python 3.7 yet.\r\n![image](https://user-images.githubusercontent.com/32511895/51676448-a3182900-1ffc-11e9-8db0-c49345797fb4.png)\r\n\r\nTry to use respective versions and check once.\r\nSuggest try with Python 3.6.\r\nPlease do let me know your outcomes.", "> Try to follow the steps in below link:\r\n> https://www.tensorflow.org/install/pip\r\n> \r\n> I suspect because you are using Python 3.7, the package released does not support Python 3.7 yet.\r\n> ![image](https://user-images.githubusercontent.com/32511895/51676448-a3182900-1ffc-11e9-8db0-c49345797fb4.png)\r\n> \r\n> Try to use respective versions and check once.\r\n> Suggest try with Python 3.6.\r\n> Please do let me know your outcomes.\r\n\r\nI thought the same but I am using python 3.6.5.\r\nwhat should be the problem.\r\nThanks for putting up with me . ", "Can you try,\r\n>converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)\r\nAlso can you print the current TF version you are using?", "> converter = tf.contrib.lite.TocoConverter.from_keras_model_file(keras_file)\r\n\r\nI have tried it out.\r\nIt is giving the same error as before. ", "Can you try with tf-nightly build?", "> Can you try with tf-nightly build?\r\n\r\nI just did it didn't help either. Sorry for the delay.", "Duplicate of #23678.", "why", "hello\r\nmy code is\r\n\r\n\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file( r'/content/drive/My Drive/inceptionv3-transfer-learning__fine_tune.h5') # Your model's name\r\nmodel = converter.convert()\r\nfile = open( 'model.tflite' , 'wb' )\r\nfile.write( model )\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n\r\nwhat can ido?", "@akinpelu746, that's a separate issue, you need to specify the inputs and input shapes for inputs that use placeholder shapes (see https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter#from_keras_model_file).", "how can  I stop my model from predicting what is not trained for?\r\nI trained my model based on tomato leaves but if I feed in any picture apart from tomato leaves my model will still classifier it?\r\nwhat can I do?", "> converter = lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n\r\n\r\nWhy are there not any updates after this...??", "@ashishmagar600 could you be elaborate more?", "The error still persists...\r\n\r\n`ImportError: cannot import name lite\r\n`\r\n\r\nPS: for ubuntu 16.04", "@ashishmagar600  please upload the screenshot of your terminal\r\n"]}, {"number": 25161, "title": "replace initialize_all_variables()", "body": "replace tf.initialize_all_variables() to tf.global_variables_initializer()", "comments": []}, {"number": 25160, "title": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12, 1.11\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:7.4.2, 7.3.1\r\n- GPU model and memory: CUDA 9.0\r\n\r\n**Describe the current behavior**\r\nI have installed TF using pip, I have tested and it was able to detect the GPU, but when start to train, it throws the error below:\r\n\r\n> UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n>          [[node FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d (defined at C:\\Users\\bahra\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:2777)  = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_1a_7x7/separable_conv2d/depthwise, FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights/read/_165)]]\r\n>          [[{{node BatchMultiClassNonMaxSuppression/map/while/Exit_6/_76}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1252_BatchMultiClassNonMaxSuppression/map/while/Exit_6\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\n**Note**\r\nI have tried TF 1.12, 1.11, and 1.8.0 all have the same problem.\r\nWhy it throwing this error and how to solve?\r\n\r\nBefore this error, I was able to train, and it was successfully worked, but when to start the second time training then this error happens.\r\n", "comments": ["duplicate #24828 \r\nClosing this issue so that we can focus on one thread. Thanks!", "@ymodak the possible solution which I found can be found [here](https://github.com/tensorflow/tensorflow/issues/24828).", "Read your solution on the thread. Thanks a lot for sharing it, will keep a note of it moving forward.", "anaconda Python 3.7  cuda10.0.130 cudnn 7.5.1  the same error \r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node Conv2D_12 (defined at <ipython-input-14-8e7270518a76>:174) ]]", "Caused by op 'Conv2D_12', defined at:\r\n  File \"/home/wtl/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/asyncio/base_events.py\", line 528, in run_forever\r\n    self._run_once()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1764, in _run_once\r\n    handle._run()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\r\n    ret = callback()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\r\n    self.run()\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\r\n    yielded = self.gen.send(value)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\r\n    return runner(coro)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-14-8e7270518a76>\", line 318, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"<ipython-input-14-8e7270518a76>\", line 208, in main\r\n    logits = model(train_data_node, True)\r\n  File \"<ipython-input-14-8e7270518a76>\", line 174, in model\r\n    padding='SAME')\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1026, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/wtl/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nUnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node Conv2D_12 (defined at <ipython-input-14-8e7270518a76>:174) ]]", "I am using TF container and this also happened. Before this error, I was able to train, and it was successfully worked, but when to start the second time training then this error happens. What is the problem here?", "Using cudnn-7.4.2 and cuda-10.0. Both should be usable by tensorflow 2.0 https://www.tensorflow.org/install/source#tested_build_configurations\r\nbut I'm running into the same issue as described above.", "I was able to resolve this by installing an updated version of the libcudnn library (7.6.5 - I am using cuda 10.0 on Ubuntu 16.04) from the  [NVIDIA developer page](https://developer.nvidia.com/cudnn). ", "I managed to get it running with cudnn/7.6.3/cuda-10.0 on CentOS Linux 7.6.1810", "**Same error i got , The Reason of getting this error is due to the mismatch of the version of the cudaa/cudnn with your tensorflow version there are two methods to solve this:**\r\n\r\n1.    Either you Downgrade your Tensorflow Version\r\n   ` pip install --upgrade tensorflowgpu==1.8.0`\r\n\r\n2.   Or You can follow the steps at Here.\r\n\r\n    tip:  Choose your ubuntu version and follow the steps.:-)\r\n\r\n", "Workaround:\r\nFresh install TF 2.0 and ran a simple Minst tutorial, it was alright, opened another notebook, tried to run and encountered this issue.\r\nI existed all notebooks and restarted Jupyter and open only one notebook, ran it successfully\r\nIssue seems to be either memory or running more than one notebook on GPU\r\n\r\nThanks", "@roebel https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-630420518\r\n@kabylan https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-641978924\r\n\r\n\r\nI think it's related to GPU memory as well, I was trying to run https://keras.io/examples/rl/deep_q_network_breakout/ before 2020/06/17 in the following environment:\r\n\r\nGPU: GeForce RTX 2060 Super\r\n| OS/Driver/Lib  | Version |\r\n| ------------- | ------------- |\r\n| Ubuntu  | 18.04.4 LTS  |\r\n| GPU Driver  | 450.36.06  |\r\n| CUDA | 11.0 |\r\n| Tensorflow | 2.2.0 |\r\n| Keras | 2.3.1 |\r\n\r\nBefore running the code, the GPU status is:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 206...  On   | 00000000:01:00.0 Off |                  N/A |\r\n| 41%   33C    P8     8W / 175W |      1MiB /  7979MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nAfter running the code, I got the error:\r\n```\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model_1/conv2d/Conv2D (defined at <ipython-input-4-aa1698769333>:87) ]] [Op:__inference_predict_function_229]\r\n\r\nFunction call stack:\r\npredict_function\r\n```\r\nAnd the GPU memory is almost full:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 206...  On   | 00000000:01:00.0 Off |                  N/A |\r\n| 41%   33C    P2    40W / 175W |   7902MiB /  7979MiB |      9%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      6753      C   ...Apps/anaconda3/bin/python     7899MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nI tried\r\n```\r\ntf.config.experimental.set_memory_growth = True\r\n```\r\nbut still had the same issue.\r\n\r\n---\r\nAlthough the code has never raised GPU/CUDA memory errors, observed that the GPU memory was almost full(7902MiB/7979MiB), one way to sovle this issue is by **limiting the GPU memory usage manually** in my case:\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\r\n    try:\r\n        tf.config.experimental.set_virtual_device_configuration(\r\n            gpus[0],\r\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 2)])\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n        # Virtual devices must be set before GPUs have been initialized\r\n        print(e)\r\n```\r\nCheck how to limit GPU memory: https://www.tensorflow.org/guide/gpu\r\n\r\nProbably this is because cuDNN tried to initialize but there isn't enough GPU memory left. Setting a hard GPU memory limit explicitly would let it know the extent to which it can use GPU memory to initialize GPU-mem-used variables, then the Tensorflow backend will figure out how much and when to allocate proper GPU memory for certain GPU-mem-used variables. Though this argument needs further verified.\r\n\r\n---\r\nNote that the code of https://keras.io/examples/rl/deep_q_network_breakout/ has been updated on 2020/06/17 and the memory filling issue no longer exists.", "@BryanBo-Cao \r\nI had the same error with:\r\n\r\nGPU: GeForce RTX 2060\r\nOS/Driver/Lib | Version\r\n-- | --\r\nUbuntu | 18.04.4 LTS\r\nGPU Driver | 435.21\r\nCUDA | 10.1\r\nTensorflow | 2.2.0\r\nKeras | 2.3.0\r\n\r\nAnd your solution for limiting the GPU usage is the only one that has worked to me.", "> @BryanBo-Cao\r\n> I had the same error with:\r\n> \r\n> GPU: GeForce RTX 2060\r\n> \r\n> OS/Driver/Lib\tVersion\r\n> Ubuntu\t18.04.4 LTS\r\n> GPU Driver\t435.21\r\n> CUDA\t10.1\r\n> Tensorflow\t2.2.0\r\n> Keras\t2.3.0\r\n> And your solution for limiting the GPU usage is the only one that has worked to me.\r\n\r\nIf this doesn't fix your problem, then upgrade your cuDNN, it works for me.", "SOLUTION:\r\nGo to train.py add these codes\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\nThis way worked for me.", "> SOLUTION:\r\n> Go to train.py add these codes\r\n> \r\n> from tensorflow.compat.v1 import ConfigProto\r\n> from tensorflow.compat.v1 import InteractiveSession\r\n> \r\n> config = ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> session = InteractiveSession(config=config)\r\n> \r\n> This way worked for me.\r\n\r\nthanks mate, works here too"]}, {"number": 25159, "title": "eager.function.defun bypasses Autograph even when autograph=True", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly (1.13.0-dev20190118)\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: NVIDIA TITAN Xp (12196MB)\r\n\r\n**Describe the current behavior**\r\n\r\nFrom tensorflow 1.13, there is integration between autograph and tf.eager.function.defun (alias of tf.function in tf2.0 maybe?) \r\nThe logic flow is if programmer enables `autograph` option as `True` in `defun` decorator,\r\n`func_graph_from_py_func` function in `tf.python.framework.func_graph.py` will be called with `autograph=True`.\r\nInside that function, it will call `converted_call` which is defined in `autograph.impl.api.py` and convert py_function to graph_funcion via autograph lib\r\n\r\nHowever, inside `converted_call`, it should shows some log information about conversion in current version, but in my simplest example, it does not in actual execution.\r\nMoreover, if I add some redundant print statement inside converted_call, it prints nothing.\r\nI think rewrapping autograph wrapper in line 445-446 inside of `tf.python.framework.func_graph.py` does not wrap autograph's `converted_call`\r\n\r\n**Describe the expected behavior**\r\n\r\nThe program should print some logging information in autograph conversion\r\ne.g. autograph converted code information\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python import autograph\r\nfrom tensorflow.python.eager.function import defun\r\n\r\ntf.enable_eager_execution()\r\n\r\n@defun(autograph=True)\r\ndef add(x, y):\r\n    return x + y\r\n\r\nprint(add(tf.constant(4), tf.constant(3))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI think this issue occurs because\r\nInside `unwrap` function in `tf.python.util.tf_decorator.py`, it does nothing if `innermost_decorator` is `None`\r\nThus, if I change line 445-446 in `tf.python.framework.func_graph.py` as \r\n```\r\nconverted_func = tf_decorator.make_decorator(original_func, wrapper)\r\nif not hasattr(python_func, '_tf_decorator'):\r\n    python_func = converted_func\r\nelse:\r\n    tf_decorator.rewrap(python_func, original_func, converted_func)\r\n```\r\nit works.\r\n", "comments": ["Thank you for the detailed report and tracking down the fix! Sorry for the delay.\r\n\r\nYour suspicion is right, `defun` is a precursor of `function` in 2.0. In integrates with a stable version of autograph, one which converts control flow only.\r\nWe recently changed the logging mechanism in Autograph to make it easier to control. It is also quite verbose now. You can now enable it either by calling [tf.autograph.set_verbosity](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/set_verbosity) or by setting the `AUTOGRAPH_VERBOSITY` environment variable. Levels are 0-4, with 0 being no logging and 1 showing the generated code.\r\nUnfortunately, logging doesn't seem to redirect to the cell output in Jupyter - that is a bug we're working to fix. But it should show up in the kernel logs.\r\n\r\nI'm looking to replicate the bug, but it seems to be gone. Here are the commands I ran:\r\n\r\n```\r\n!pip install tf-nightly\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n```\r\n\r\n1.13.0-dev20190207\r\n\r\n```\r\nfrom tensorflow.python import autograph\r\nfrom tensorflow.python.eager.function import defun\r\n\r\ntf.enable_eager_execution()\r\n\r\n@defun(autograph=True)\r\ndef add(x, y):\r\n    return x + y\r\n\r\nprint(add(tf.constant(4), tf.constant(3)))\r\n```\r\n\r\ntf.Tensor(7, shape=(), dtype=int32)\r\n\r\nPlease reopen the issue if it still occurs on your side of if you have further questions.", "After a bit more digging into the logging, it seems that defun indeed does not call Autograph! My apologies. I'll look into applying the fix you suggested."]}, {"number": 25158, "title": "Updated README.md", "body": "Typo error fixed", "comments": []}, {"number": 25157, "title": "Updated LIMITATIONS.md for typo error", "body": "Updated one Typo error", "comments": []}, {"number": 25156, "title": "Update README.md", "body": "Updated one typo error in the file", "comments": []}, {"number": 25155, "title": "How to measure every node flops form tflite model\uff1f", "body": "benchmark_model  or profile only have run time and memory\uff0ci  also want to get flops from node", "comments": ["if what you want is estimating FLOPS, you can check what TensorFlow benchmark_mode does, check [its code](\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc#L148-L212) does.", "Great\uff01enough for me to rough estimation", "Closing since its resolved. Thanks!"]}, {"number": 25154, "title": "Is there tf_cuda_cc_binary rule in TF?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  RHEL 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.4\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.9.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: k40m\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI want to put a simple cuda cupti program(Eg, the official CUPTI samples) in TF for some tests. But I can't find any `tf_cuda_cc_binary` rule in TF. So, my question is: how could I put such raw cuda souce in TF, and compile them by bazel?\r\n", "comments": ["You should not need a separate target. You can depend on these targets in your cc_binary to pull in cupti: `tensorflow/tensorflow/core/profiler/internal/gpu:cupti_wrapper` I think we have targets for all of these under stream executor, and you can just depend on those as well, like `tensorflow/stream_executor/cuda:cupti_stub`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25154\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25154\">No</a>\n"]}, {"number": 25153, "title": "replace initialize_all_variables()", "body": "This fix is to replace tf.initialize_all_variables() to tf.global_variables_initializer() in `test_file_v0_11.py`\r\n", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25152, "title": "os.environ[\"CUDA_DEVICE_ORDER\"]  not work", "body": "I have two GPU, I want to create 2 graph in 2 GPU, first graph in first GPU, second graph in second GPU.\r\n1. when I create first graph in first GPU, I use os.environ[\"CUDA_DEVICE_ORDER\"] ='0'\r\n2.when I create second graph in second GPU, I use os.environ[\"CUDA_DEVICE_ORDER\"] = '1', but second graph still create on first GPU, I try many different ways, but it still not work.\r\n\r\nIt's a bug?\r\n", "comments": ["can anyone help me, Please", "You want specify  visible devices not the device order. Use\n`os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"` before the tensorflow import", "I will try, thank you ", "@DavidWiesner this is my code, but it still not work, it occur same condition, can you help me?\r\n<img width=\"1006\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51754174-9677fb80-20f6-11e9-85c6-c2e143d194e1.png\">\r\n<img width=\"997\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51754197-a68fdb00-20f6-11e9-8262-aee25cd725e6.png\">\r\n<img width=\"1006\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51754211-b1e30680-20f6-11e9-9d45-b016d1a1f630.png\">\r\n<img width=\"1012\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51754225-befff580-20f6-11e9-9be1-8f13faeed197.png\">\r\n<img width=\"1005\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51754246-caebb780-20f6-11e9-8878-114b9e24a7fb.png\">\r\n<img width=\"1002\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51754270-d5a64c80-20f6-11e9-99c2-7d5ba589b1a0.png\">\r\n<img width=\"1001\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51754289-dccd5a80-20f6-11e9-8130-fbf7208d1b11.png\">\r\n", "Bert is also importing tensorflow so put your environment variables before import bert", "@DavidWiesner but I change my code, it still occur same condition, this is my code, after change,\r\n\r\n<img width=\"1280\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51759509-59fecc80-2103-11e9-950c-e896b10c9784.png\">\r\n<img width=\"1277\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51759530-62570780-2103-11e9-9fbb-f26524a430b8.png\">\r\n<img width=\"1270\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51759549-6c790600-2103-11e9-8596-1ff5e09ec0cb.png\">\r\n", "@DavidWiesner this is first page, it lossed\r\n<img width=\"888\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51759713-d1346080-2103-11e9-851f-3549ed8fdfbb.png\">\r\n", "this is running time log\r\n<img width=\"903\" alt=\"image\" src=\"https://user-images.githubusercontent.com/30991932/51759805-03de5900-2104-11e9-87fe-2ddf7103c257.png\">\r\n", "@DavidWiesner please help me, this is very important to me, I was perplex by this problem  2 days.\r\nthank you very much!", "It is working. In the log you see only one gpu will be used ", "@DavidWiesner sorry, It took me so long to get back to you, But in my code, I set different gpu number, when I create class instance, I create 4 class instance, I want first instance work in fist gpu, second instance work in second gpu, third instance work in third gpu, fourth instance work in fourth gpu, but in my log, four instance still work in first gpu, I try many time, but it it same.", "@omalleyt12 Can you help me?", "@guptapriya Can you help me\uff1f", "hi @policeme - have you tried creating your graph under a `with tf.device('/device:GPU:1'):` context? \r\nGiven you're trying to use different GPUs in the same process, that is likely a better way to specify GPUs than the environment variables. (which you can then leave to be default and let TF detect all of them).\r\nThis page has more information on how to use this API: https://www.tensorflow.org/guide/using_gpu\r\n\r\nHope this helps. ", "Hi @guptapriya , but it wasn't work, this is my code and running time log, please help me.\r\n![image](https://user-images.githubusercontent.com/30991932/51815918-1375c180-22ff-11e9-997c-7364556c2e7e.png)\r\n![image](https://user-images.githubusercontent.com/30991932/51815924-1f618380-22ff-11e9-9d90-8982ec73e33c.png)\r\n![image](https://user-images.githubusercontent.com/30991932/51815928-27b9be80-22ff-11e9-98c4-6dc2b3e6eaf1.png)\r\n![image](https://user-images.githubusercontent.com/30991932/51815930-2c7e7280-22ff-11e9-92b3-9c00519cd306.png)\r\n![image](https://user-images.githubusercontent.com/30991932/51815938-343e1700-22ff-11e9-96e6-37a026abf853.png)\r\n", "Can you try a simple example first, such as the one from the guide and see if there you're able to get ops on different GPUs? I wonder if TF is not detecting the multiple GPUs at all for some reason. It would be easier to debug with a small simple code snippet.", "@guptapriya in this program, it work, I have 1 GPU, when I set GPU:1, it will occur exception, but I don't know why my first  program doesn't work, can you tell me why.\r\n![image](https://user-images.githubusercontent.com/30991932/51828643-fc4aca00-2326-11e9-9b0a-7a9091e3dfab.png)\r\n", "![image](https://user-images.githubusercontent.com/30991932/51828804-55b2f900-2327-11e9-954c-98a59a360518.png)\r\n![image](https://user-images.githubusercontent.com/30991932/51828816-5b104380-2327-11e9-8c2c-f44b694bbcb9.png)\r\n![image](https://user-images.githubusercontent.com/30991932/51828826-5fd4f780-2327-11e9-9f6d-f812741aed7a.png)\r\n", "@guptapriya  this is running time log.", "Do you only have 1 GPU? If yes, do you mean why your original program didn't throw exception when you tried to use other GPUs? ", "yes", "if you want original code, I can put my code into github", "can you check what is the value of `allow_soft_placement` in both the cases? if it is true,  then everything will be placed on GPU:0 (when you have 1 GPU). See more details on the same page i mentioned before: https://www.tensorflow.org/guide/using_gpu\r\nand here: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L379", "but when I removed allow_soft_placement in my code, I set my graph in first GPU, it appeared this exception.\r\n![image](https://user-images.githubusercontent.com/30991932/51881068-b8060b00-23b4-11e9-957a-c28129ae3aaf.png)\r\n", "That's a different error because you're trying to place the saver ops on GPU which will not work. \r\n\r\nAlso, it doesn't look like there is a bug here being reported, so I will close this ticket now. I believe stack overflow is a better venue for clarifications on usage, such as this. ", "please do not close this issue, how is this not a bug ? all these people having this issue, the least you can do is to help or leave this open. ", "@jaingaurav can you help look into this, and see if there is a bug, and maybe provide the recommended APIs?", "@policeme  Hi, Have you solved this problem?  I also encountered this problem.", "no I haven't\n\n\nCally Ma (\u9a6c\u96c4)\nChongqing University of Post and Telecommunications\nMobile: +86 15025700935\nE-mail: mx15025700935@aliyun.com\n\n\n\n\n------------------------------------------------------------------\n\u53d1\u4ef6\u4eba\uff1alingdavid <notifications@github.com>\n\u53d1\u9001\u65f6\u95f4\uff1a2019\u5e749\u670830\u65e5(\u661f\u671f\u4e00) 10:55\n\u6536\u4ef6\u4eba\uff1atensorflow/tensorflow <tensorflow@noreply.github.com>\n\u6284\u3000\u9001\uff1aCally <mx15025700935@aliyun.com>; Mention <mention@noreply.github.com>\n\u4e3b\u3000\u9898\uff1aRe: [tensorflow/tensorflow] os.environ[\"CUDA_DEVICE_ORDER\"] not work (#25152)\n\n@policeme Hi, Have you solved this problem? I also encountered this problem.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread. ", "I am facing the similar issue using Pytorch.\r\n`os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu` this somehow works on some code and some code would fail.\r\n\r\nYou may change your running command from\r\n```\r\n# this might fails with os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\r\npython main.py\r\n\r\n# you can use this\r\nCUDA_VISIBLE_DEVICES=\"3,4,5\" python main.py\r\n```\r\n\r\nHowever, I don't know why some codes could work some codes don't.", "@shamangary  This solves the problem https://discuss.pytorch.org/t/why-setting-cuda-visible-devices-within-the-code-doesn-t-work/31826/2?u=ht_wang", "@shamangary You need to set that before the first use of cuda rather than after that. The following works well for me.\r\n![Snipaste_2020-06-29_16-53-34](https://user-images.githubusercontent.com/26649267/85993723-59214700-ba29-11ea-9ccc-fd437c775e2f.jpg)\r\n"]}, {"number": 25151, "title": "version 1.13.0rc0 build failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.0rc0\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?:  no\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):  7.3.0\r\n- CUDA/cuDNN version: 10.0/ 7.4\r\n- GPU model and memory:  GTX1080Ti GDDR5X 11GB X 7\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nbazel build failed\r\n\r\nERROR: /home/wmind/repo/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 72, in <module>\r\n    from tensorflow.python.ops.standard_ops import *\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/ops/standard_ops.py\", line 25, in <module>\r\n    from tensorflow.python import autograph\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/__init__.py\", line 37, in <module>\r\n    from tensorflow.python.autograph.core.converter import ConversionOptions\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/core/converter.py\", line 74, in <module>\r\n    from tensorflow.python.autograph.pyct import cfg\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/cfg.py\", line 41, in <module>\r\n    from tensorflow.python.autograph.pyct import compiler\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/compiler.py\", line 30, in <module>\r\n    import astor\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/__init__.py\", line 14, in <module>\r\n    from .code_gen import to_source  # NOQA\r\n  File \"/home/wmind/.cache/bazel/_bazel_wmind/a0a15f0a32d3688786619e94912711cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python_api_gen_v1.runfiles/astor_archive/astor/code_gen.py\", line 311\r\n    def visit_FunctionDef(self, node, async=False):\r\n                                          ^\r\nSyntaxError: invalid syntax\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2248.641s, Critical Path: 409.07s\r\nINFO: 15004 processes: 15004 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Python 3.7 is not supported in 1.13.0rc0 yet. However we can expect it in [rc2 or official](https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-457185528).", "It probably will be for official release.."]}, {"number": 25150, "title": "Update profiler ui documentation.", "body": "", "comments": ["Nagging Reviewer @MarkDaoust: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25149, "title": "could you teach me how to config the openssl and icu lib\uff1f\uff1f\uff1f\uff1f\uff1f\uff1f", "body": "tensorflow 1.12\r\nwin10\r\npython 3.5\r\nvs 2015 update3\r\ncompile tensorflow for tensorflow.dll and tensorflow.lib\r\ncould you teach me how to config the openssl and icu lib\uff1f\uff1f\uff1f\uff1f\uff1f\uff1f\r\nhow to package openssl and icu lib to tensorflow.dll and tensorflow.lib", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 25148, "title": "TypeError when using PReLU activations with TPUs", "body": "I'm updating a Keras Functional API SRGAN model, so that it will run on a TPU instead of GPU (i'm using colaboratory).\r\nEverything is now OK, with the exception of being able to use PReLU Activation.  When I Fit a model that has a PReLU layer:\r\n\r\n`x = tf.keras.layers.PReLU(alpha_initializer='zeros',alpha_regularizer=None,alpha_constraint=None,shared_axes=[1,2])(x)`\r\n\r\nI get the following error:\r\n\r\n`TypeError: bad operand type for unary -: 'ReplicatedVariable'`\r\n\r\nIf I swap the PReLU with:\r\n\r\n`x = tf.keras.layers.Activation('relu')(x)`\r\n\r\nThe model runs with no error on the TPU.  Has anyone seen this problem?\r\n", "comments": ["How do I stop receiving emails from tensorflow\n\nOn Wed, Jan 23, 2019, 6:42 PM ctmckee <notifications@github.com wrote:\n\n> Get one more error when I compile the full adversarial network:\n>\n> `/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\n> in from_config(cls, config, custom_objects)\n> 1289\n> 1290 # First, we create all layers and enqueue nodes to be processed\n> -> 1291 for layer_data in config['layers']:\n> 1292 process_layer(layer_data)\n> 1293 # Then we process nodes in order of layer depth.\n>\n> KeyError: 'layers'`\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25148#issuecomment-457022046>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ArBbKuYWvwcM2OW65STPvRxe0xB1qvfzks5vGQGKgaJpZM4aP3qY>\n> .\n>\n", "In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Here is the model, compile, and fit_generator code which leads to the TypeError:\r\n![image](https://user-images.githubusercontent.com/40606780/51883356-73885a00-2337-11e9-9c15-dd939489325c.png)\r\n\r\nIf I switch out all the PReLU activations for relu the fit_generator works fine\r\n\r\n```\r\nimport keras\r\nimport tensorflow as tf \r\nfrom tensorflow.keras import layers\r\n\r\n#clean out old data \r\ntf.reset_default_graph\r\ntf.keras.backend.clear_session()\r\n\r\n#some variables\r\nGen_init = tf.initializers.he_normal()\r\naxis = -1 \r\n\r\ninput_to_generator = tf.keras.Input(shape=(64,64,3))\r\n\r\n#pre-res_block\r\nx = layers.Conv2D(filters=64, kernel_size=(3,3),kernel_initializer=Gen_init,strides=(1,1),padding='same')(input_to_generator)\r\nx_input_res_block=layers.PReLU(alpha_initializer='zeros',alpha_regularizer=None,alpha_constraint=None,shared_axes=[1,2])(x)\r\nx = x_input_res_block\r\n\r\n#create B number of residual blocks\r\n#for i in range(B):\r\n#    x = res_block(x)\r\n\r\n#this is the res_block() function, I've put it here so you can see it once\r\nx = layers.Conv2D(64,kernel_size=(3,3),kernel_initializer=Gen_init,strides=(1,1),padding='same',activation=None,use_bias=False)(x)\r\nx = layers.BatchNormalization(axis=axis)(x)\r\nx = layers.PReLU(alpha_initializer = 'zeros', alpha_regularizer = None, alpha_constraint = None, shared_axes=[1,2])(x)\r\nx = layers.Conv2D(64,kernel_size=(3,3),kernel_initializer=Gen_init,strides=(1,1),padding='same',activation=None,use_bias=False)(x)\r\nx = layers.BatchNormalization(axis=axis)(x)\r\nx = layers.Add()([x,x_input_res_block])\r\n\r\n#post the res_block\r\nx = layers.Conv2D(64,kernel_size=(3,3),kernel_initializer=Gen_init,strides=(1,1),padding='same',activation=None,use_bias = False)(x)\r\nx = layers.BatchNormalization(axis=axis)(x)\r\n\r\n#skip connection\r\nx = layers.Add()([x,x_input_res_block])\r\n\r\n# two upscale blocks, because downscale is equal to 4\r\nx = layers.UpSampling2D(size=(2,2))(x)\r\nx = layers.Conv2D(256,kernel_size=(3,3),kernel_initializer=Gen_init,strides(1,1),padding='same',activation=None, use_bias=False)(x)\r\nx = layers.PReLU(alpha_initializer='zeros',alpha_regularizer=None,alpha_constraint=None,shared_axes=shared_axis)(x)\r\n\r\nx = layers.UpSampling2D(size=(2,2))(x)\r\nx = layers.Conv2D(256,kernel_size=(3,3),kernel_initializer=Gen_init,strides(1,1),padding='same',activation=None, use_bias=False)(x)\r\nx = layers.PReLU(alpha_initializer='zeros',alpha_regularizer=None,alpha_constraint=None,shared_axes=shared_axis)(x)\r\n\r\n\r\n\r\n#final conv layer needs to use activation with tanh:  the output will be in the range [-1,1]\r\noutput_generator = layers.Conv2D(3,kernel_size=(9,9),kernel_initializer=Gen_init,strides=(1,1),activation='tanh',use_bias=False,padding='same')(x)\r\n\r\n#create the model\r\ngenerator = tf.keras.Model(inputs=input_to_generator,outputs=output_generator)\r\n\r\n#convert keras model to tpu model\r\ntpu_generator = tf.contrib.tpu.keras_to_tpu_model(\r\n    generator,\r\n    strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n        tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n    )\r\n)\r\n\r\n#compile the model\r\ntpu_generator.compile(loss='mse',optimizer=tf.train.AdamOptimizer(learning_rate=0.000015))\r\n\r\n#I use an hdf5 file in combination with keras sequence generator to train model\r\nwith h5py.File('/tmp/smallersizeGAN.hdf5') as f:\r\n    HR = f['HRtrain_img'] \r\n    LR = f['LRtrain_img']\r\n    \r\n    train_gen = DataGenerator(LR,HR)  \r\n        \r\n    tpu_generator.fit_generator(generator=train_gen,\r\n                                steps_per_epoch = (LR.shape[0]//32),  \r\n                                epochs = 1,\r\n                                verbose = 1)\r\n```\r\n\r\n", "Here is the DataGenerator\r\n\r\n```\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n   \r\n    def __init__(self,X,y,batch_size=32):   \r\n    \r\n        self.X = X\r\n        self.y = y\r\n        self.batch_size = batch_size\r\n        \r\n    def __len__(self):\r\n        return int(np.floor(len(self.X) / self.batch_size))\r\n    \r\n    def __getitem__(self, idx):\r\n        L = idx*self.batch_size\r\n        R = L+self.batch_size\r\n        batch_x = self.X[L:R]\r\n        batch_y = self.y[L:R]\r\n        \r\n        return batch_x,batch_y\r\n```\r\n\r\n", "Looks like this issue is relatively stale. If this problem can still be reproduced on the latest versions of TensorFlow, please do open a new issue. Thanks! -Brennan"]}, {"number": 25147, "title": "Profiler hook causes exception", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo. I am using 1.12.0\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14. Docker 16.\r\n- TensorFlow installed from (source or binary): binary whl\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: Repros on both local 1080ti + TitanV and 8xV100 on gcloud\r\n\r\n\r\n```\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nINFO:tensorflow:loss = 0.28926048, step = 0\r\n2019-01-23 22:36:02.026095: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally\r\npure virtual method called\r\nterminate called without an active exception\r\nAborted (core dumped)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n        hooks = []\r\n        if self.params.profile:\r\n            profile_destination = '/'.join([self.params.job_dir, 'profiler'])\r\n            tf.logging.info(\"Profiling session to: {}\".format(profile_destination))\r\n            profiler_hook = tf.train.ProfilerHook(save_steps=30,\r\n                                                    output_dir=profile_destination,\r\n                                                    show_memory=True)\r\n            hooks.append(profiler_hook)\r\n\r\n        # Build train and eval specs\r\n        train_spec = tf.estimator.TrainSpec(\r\n            input_fn=lambda: self.model.input_tf_dataset(DataSplitKey.TRAIN),\r\n            max_steps=self.params.train_steps,\r\n            hooks=hooks,\r\n        )\r\n```\r\n", "comments": ["cc @ssdrdp", "Any tips on debugging this would be much appreciated. I am using the mirrored strategy.", "Installing these fixes the issue. There seems to be a cuda incompatibility even in the gcloud ml engine docker container.\r\n\r\n```\r\nsudo apt install libcudnn7=7.2.1.38-1+cuda9.0\r\nsudo apt install libnccl2=2.2.13-1+cuda9.0\r\n```", "Closing this issue since its resolved. Thanks for the workaround."]}]