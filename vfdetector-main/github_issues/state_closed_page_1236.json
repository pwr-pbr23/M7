[{"number": 16080, "title": "MKL-DNN: fix a concat issue which is related to negative concat_dim", "body": "For a negative concat_dim input, the actual concat_dim should be N + concat_dim with N \r\nbeing the dims of input tensors. \r\n\r\nThis PR fixes an issue of setting N properly. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16079, "title": "Branch 181765083", "body": "", "comments": ["@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please."]}, {"number": 16078, "title": "Update important RELNOTES and author list", "body": null, "comments": []}, {"number": 16077, "title": "Update LICENSE", "body": "This commit updates an instance of 2017 to 2018.", "comments": ["I was told by people who are lawyers to just leave these alone. Don't worry about changing the years, so I'll close this PR. Thanks!"]}, {"number": 16076, "title": "Using self.test_session() in setUp() in tf.test.TestCase runs setUp but not tearDown", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: From binary with pip\r\n- **TensorFlow version (use command below)**: v1.4.1\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: 1080Ti\r\n- **Exact command to reproduce**: python mwe.py\r\n\r\n### Describe the problem\r\nhttps://github.com/tensorflow/tensorflow/blob/1cc4ec4c5a08cadae87fa02222c5b5c3e81dedbb/tensorflow/python/framework/test_util.py#L874\r\n\r\nIf you use `tf.test.TestCase.test_session()` in `setUp()`, there will be one \"skipped\" test for which `setUp()` is run, but not `tearDown()`. I believe this has to do with `tf.TestCase.test_session()` trying to take care of automatic test discovery on the line above, but I'm not sure.\r\n\r\n### Source code / logs\r\n\r\nIn the following MWE (mwe.py), after running this test there will be a tmp.txt file left in the current directory.\r\n\r\nMWE:\r\n\r\n    import tensorflow as tf\r\n\r\n    import os\r\n    import unittest\r\n\r\n    class FooTest(tf.test.TestCase):\r\n        def setUp(self):\r\n            with open(\"tmp.txt\", \"w\") as f:\r\n                f.write(\"Hello\")\r\n\r\n            with self.test_session() as sess:\r\n                pass\r\n\r\n        def tearDown(self):\r\n            os.unlink(\"tmp.txt\")\r\n\r\n        def testExample(self):\r\n            self.assertEqual(1, 1)\r\n\r\n    if __name__==\"__main__\":\r\n        unittest.main()\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I have updated the issue with the new issue template.", "@martinwicke can you comment or refer further, please? Looks like a bug.", "It is a bug. Realistically, since it doesn't seem to be hurting too badly, not sure when someone will look at it, but I'd love a PR with a fix.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "This should be addressed by 9962eb5e84b15e309410071b06c2ed2d6148ed44 which deprecates test_session() in favor of new methods session() and cached_session()."]}, {"number": 16075, "title": "optimize_for_inference_lib.fold_batch_norms() preserves data_format", "body": "`fold_batch_norms()` currently breaks graphs containing convolutions using NCHW data format. The function replaces a BiasAdd operation with a new one, while not preserving the data format of the original operation. As a result, the new operation always has NHWC data format, and the execution of the resulting graph fails because of mismatching dimensions.\r\n\r\nThe proposed resolution is to copy the `data_format` property from the original operation.\r\n\r\nThe patch fixes https://github.com/tensorflow/tensorflow/issues/15034.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Could you please add a test that fails for master branch and pass for this PR code?", "Signed.", "CLAs look good, thanks!\n\n<!-- ok -->", "@qmick Could you have a look at the added test?", "Test looks fine. \r\n\r\nIt is what we want? Ping @petewarden for the assignment of #15034, feel free to ignore this ping if it's not appropriate.\r\n\r\n@yegord This PR should close #15034, right? It is strange that the keyword \"fix #15034\" did not ask for a close. Could you try to make it work?", "Well, I have added the \"Fixes NNN\" line to the commit message.\r\nGitHub should close the issue as soon as this commit is merged.\r\nIf you have other ideas about what I should do, please let me know.", "@yegord Could you please check if this change still needed? This might have been caused by a bug in graph optimization that is now fixed.", "@rmlarsen The test from this PR still fails on master (55c1e4ae66ddb1866804c1d20ddc20aead12b392). In master `fold_batch_norm()` still replaces NCHW BiasAdd with NHWC BiasAdd. This still needs to be fixed. The change from this PR is still needed."]}, {"number": 16074, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16073, "title": "Feature request: (optionally) return all audio streams in tf.contrib.ffmpeg.decode_audio", "body": "I'm trying to read [musdb18](https://sigsep.github.io/musdb.html) with `tf.data` and it comes in the form of mp4 files with multiple audio streams so `ffmpeg -map` is needed.\r\n\r\n`tf.contrib.ffmpeg.decode_audio` cannot be configured to choose the audio stream. I wonder if we could have a `tf.contrib.ffmpeg.decode_audios` that returns every audio stream in the file, or if we could have a new argument in `tf.contrib.ffmpeg.decode_audio` for choosing streams.\r\n\r\nBeing able to choose the audio stream is also important for getting the right language in a movie file's audio, and similarly `tf.contrib.ffmpeg.decode_video` could need the same extension (though multiple video streams is not as common AFAIK).", "comments": ["Maybe @faroit has thoughts on this?", "@vrv do you have any comments on this too or redirect to who would be the best person?", "@fredbertsch I think knows most about ffmpeg ops", "Added a PR #16101 so that it is possible to provide an optional `stream=\"0\"` for ffmpeg to select. Please take a look.", "@carlthome this is great!", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 16072, "title": "Dynamic Bi-directional RNN vs Dynamic RNN.- Not working as expected.", "body": "I am trying to use Bidirectional RNN and pass the output through a CNN for text classification. However, I am getting all sorts of shape errors with bidirectional RNN. Although, If I use two dynamic rnn with reverse op in the second layer, it appears to work fine:\r\n\r\nHere is bidirectional RNN code that DOES NOT work for me:\r\n\r\n```\r\n    # Bidirectional LSTM layer\r\n    with tf.name_scope(\"bidirectional-lstm\"):\r\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n\r\n        self.lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\r\n            lstm_fw_cell, \r\n            lstm_bw_cell, \r\n            self.embedded_chars, \r\n            sequence_length=self.seqlen, \r\n            dtype=tf.float32)\r\n        self.lstm_outputs = tf.concat(self.lstm_outputs, axis=2)\r\n```\r\n\r\n\r\nHere is the two layer dynamic rnn that DOES work for me:\r\n\r\n\r\n```\r\n  # Bidirectional LSTM layer\r\n    with tf.name_scope(\"bidirectional-lstm\"):\r\n        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)\r\n    with tf.variable_scope(\"lstm-output-fw\"):\r\n        self.lstm_outputs_fw, _ = tf.nn.dynamic_rnn(\r\n            lstm_fw_cell, \r\n            self.embedded_chars, \r\n            sequence_length=self.seqlen, \r\n            dtype=tf.float32)\r\n\r\n    with tf.variable_scope(\"lstm-output-bw\"):\r\n        self.embedded_chars_rev = array_ops.reverse_sequence(self.embedded_chars, seq_lengths=self.seqlen, seq_dim=1)\r\n        tmp, _ = tf.nn.dynamic_rnn(\r\n            lstm_bw_cell, \r\n            self.embedded_chars_rev, \r\n            sequence_length=self.seqlen, \r\n            dtype=tf.float32)\r\n        self.lstm_outputs_bw = array_ops.reverse_sequence(tmp, seq_lengths=self.seqlen, seq_dim=1)\r\n\r\n    Concatenate outputs\r\n    self.lstm_outputs = tf.add(self.lstm_outputs_fw, self.lstm_outputs_bw, name=\"lstm_outputs\")\r\n```\r\n\r\nI am passing the output of this to CNN and error occurs when computing the\r\n\r\nHere is the rest of the code:\r\n\r\n# Convolution + maxpool layer for each filter size\r\n        pooled_outputs = []\r\n        for i, filter_size in enumerate(filter_sizes):\r\n            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, hidden_size, 1, num_filters]\r\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\r\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\r\n\r\n                conv = tf.nn.conv2d(\r\n                    self.lstm_outputs_expanded, \r\n                    W,\r\n                    strides=[1, 1, 1, 1], \r\n                    padding=\"VALID\",\r\n                    name=\"conv\")\r\n\r\n                # Apply nonlinearity\r\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\r\n\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(\r\n                    h, \r\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\r\n                    strides=[1, 1, 1, 1], \r\n                    padding='VALID',\r\n                    name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n\r\n        # Combine all the pooled features\r\n        num_filters_total = num_filters * len(filter_sizes)\r\n        self.h_pool = tf.concat(axis=3, values=pooled_outputs)\r\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\r\n\r\n\r\n        # Dropout layer\r\n        with tf.name_scope(\"dropout\"):\r\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\r\n\r\n\r\n```\r\n        # Final (unnormalized) scores and predictions\r\n        with tf.name_scope(\"output\"):\r\n            # Standard output weights initialization\r\n            W = tf.get_variable(\r\n                \"W\", \r\n                shape=[num_filters_total, num_classes], \r\n                initializer=tf.contrib.layers.xavier_initializer())\r\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\r\n\r\n            # # Initialized output weights to 0.0, might improve accuracy\r\n            # W = tf.Variable(tf.constant(0.0, shape=[num_filters_total, num_classes]), name=\"W\")\r\n            # b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\r\n\r\n            l2_loss += tf.nn.l2_loss(W)\r\n            l2_loss += tf.nn.l2_loss(b)\r\n\r\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\r\n\r\n            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\r\n\r\n        # Calculate mean cross-entropy loss\r\n        with tf.name_scope(\"loss\"):\r\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\r\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\r\n\r\n        # Accuracy\r\n        with tf.name_scope(\"accuracy\"):\r\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\r\n```\r\n\r\n\r\nhere are the errors I am getting.\r\n\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_upgraded.py\", line 209, in <module>\r\n    train_step(x_batch, seqlen_batch, y_batch)\r\n  File \"train_upgraded.py\", line 177, in train_step\r\n    feed_dict)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\n\r\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\r\n  File \"train_upgraded.py\", line 87, in <module>\r\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\r\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\r\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\r\n    name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_upgraded.py\", line 209, in <module>\r\n    train_step(x_batch, seqlen_batch, y_batch)\r\n  File \"train_upgraded.py\", line 177, in train_step\r\n    feed_dict)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\n\r\nCaused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:\r\n  File \"train_upgraded.py\", line 87, in <module>\r\n    l2_reg_lambda=FLAGS.l2_reg_lambda)\r\n  File \"/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py\", line 138, in __init__\r\n    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1783, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4364, in _softmax_cross_entropy_with_logits\r\n    name=name)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]\r\n         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/Reshape, loss/Reshape_1)]]\r\n```", "comments": ["@hkhatod I'm no expert in bidirectional LSTMs, but the error lines about the shapes of labels and logits make me wonder if you're somehow presenting the concatenation of all sequence steps as logits (7550x2), which mismatches with much narrower labels (50x2). It's particularly interesting that 50 evenly divides 7550--might your sequence length for the first example be 151?\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]. \r\n\r\nCan you trace back through your code and look at the shapes of self.scores and self.h_drop, and convince yourself that they're doing the expected thing in the failing case?\r\n\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "That was it. Shape issue."]}, {"number": 16071, "title": "fix comments and code matches", "body": "", "comments": ["Can one of the admins verify this patch?", "cc @keveman (original author of the edit distance op?) I can no longer assign you as a reviewer. So assigning the PR to @agarwal-ashish. Please re-assign if appropriate."]}, {"number": 16070, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16069, "title": "Key generator/encoder_8/conv/filter not found in checkpoint", "body": "I'm using python 3.6.3 win 10 64bit and tensorflow 1.2.1 and now I'm working on https://github.com/datitran/face2face-demo project and **part 4.export model** I'm taking this error:NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint \r\n\r\nhow can I solve this problem ?\r\n\r\nwhat I run\r\n`C:\\Users\\hajum>python C:\\Users\\hajum\\Desktop\\face2face-demo-master\\reduce_model.py --model-input C:\\Users\\hajum\\Desktop\\face2face-model --model-output C:\\Users\\hajum\\Desktop\\face2face-reduced-model`\r\n\r\nsame folder names with project but I have my own models\r\n\r\nwhat it shows\r\n```\r\n2018-01-12 13:24:05.337267: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.337407: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.338476: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.338779: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.339070: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.339369: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.339659: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.339962: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2018-01-12 13:24:05.779044: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_8/conv/filter not found in checkpoint\r\n2018-01-12 13:24:05.779482: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_2/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.779562: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_1/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.780339: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_3/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.780354: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.781063: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.781066: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.781015: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.784971: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.785703: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.785849: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.785928: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_4/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.787052: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.787160: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.787346: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_5/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.787687: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.791739: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.793255: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.793508: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_6/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.794303: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.795123: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.795823: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.796067: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_7/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.797352: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.798112: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_7/conv/filter not found in checkpoint\r\n2018-01-12 13:24:05.800556: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_1/conv/filter not found in checkpoint\r\n2018-01-12 13:24:05.801703: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.801868: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/decoder_8/deconv/filter not found in checkpoint\r\n2018-01-12 13:24:05.801974: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.801977: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_2/conv/filter not found in checkpoint\r\n2018-01-12 13:24:05.804154: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.805983: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.806160: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.807834: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.808628: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_3/conv/filter not found in checkpoint\r\n2018-01-12 13:24:05.808808: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.809721: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.809836: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_4/conv/filter not found in checkpoint\r\n2018-01-12 13:24:05.810842: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.811998: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_5/conv/filter not found in checkpoint\r\n2018-01-12 13:24:05.812062: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.812846: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/offset not found in checkpoint\r\n2018-01-12 13:24:05.812889: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/scale not found in checkpoint\r\n2018-01-12 13:24:05.813195: W c:\\l\\tensorflow_1501918863922\\work\\tensorflow-1.2.1\\tensorflow\\core\\framework\\op_kernel.cc:1158] Not found: Key generator/encoder_6/conv/filter not found in checkpoint\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1139, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1121, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint\r\n         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hajum\\Desktop\\face2face-demo-master\\reduce_model.py\", line 215, in <module>\r\n    saver.restore(sess, checkpoint)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1548, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint\r\n         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]\r\n\r\nCaused by op 'save/RestoreV2_43', defined at:\r\n  File \"C:\\Users\\hajum\\Desktop\\face2face-demo-master\\reduce_model.py\", line 213, in <module>\r\n    saver = tf.train.Saver()\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1139, in __init__\r\n    self.build()\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1170, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\r\n    restore_sequentially, reshape)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 640, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\hajum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint\r\n         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]\r\n```", "comments": ["I'm having the same error running arch linux 64bit, python 3.6.4 and tensorflow 1.4.1. datitran/face2face-demo#13", "Same error running Windows 10, python 3.6.2 and tensorflow 1.4.0 ", "The issue is solved by using an older version of affinelayer/pix2pix-tensorflow.\r\n```\r\ngit clone https://github.com/affinelayer/pix2pix-tensorflow\r\ncd pix2pix-tensorflow\r\n# Reset to april version\r\ngit reset --hard d6f8e4ce00a1fd7a96a72ed17366bfcb207882c7\r\n```\r\nAnd then retrain your model and everything should work fine.\r\n\r\nThanks to @qz517 for the hint.", "Since this seems to be related to code in an external repository, I'm closing for now.", "lionawurscht its working thanks"]}, {"number": 16068, "title": "Empty tensorflow/python/tools directory after build tf 1.5 from source", "body": "I rebuild Tensorflow-GPU Version 1.5.0rc0 from source with Bazel Version 0.9.0 with CUDA 9 and cuDNN 7 to see changes from my previous version 1.4 also build from source with same prerequesites.\r\n\r\nBut now the tensorflow/python/tools directory is empty? Has anybody experienced the same?\r\nOr Did these scripts get moved to another directory?", "comments": ["@GustavZ do you mean this directory: https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/python/tools\r\n", "yes exactly this one.\r\nI am now switching back to 1.4 for multiple reasons.", "@GustavZ seems reasonable. I'll close for now--it's not clear that this is widespread. Please reopen if you (or others) encounter this again. "]}, {"number": 16067, "title": "[XLA] Separate out the dynamic slice wrapping tests", "body": "This is a set of changes to allow disabling of bfloat16 tests, for backends which don't support bfloat16.  Originally it was a change to the same set of tests to allow the wrapping behaviour tests to be disabled.  That change was made obsolete by some parallel work.\r\n\r\n---\r\nThe original text was:\r\n\r\nThe XLA documentation says that the behaviour of dynamic slice and dynamic update slice is undefined when the indices wrap.\r\n\r\nThis separates out the tests which check for wrapping behaviour, so that they can be ignored for backends which don't exhibit the test's expected results.\r\n", "comments": ["Can one of the admins verify this patch?", "@broune\r\n@alinas \r\n\r\nHi - this is a small change to allow disabling of the bfloat16 tests.  I notice that my earlier change (separating out the wrapping behaviour into different tests) was already done separately.\r\n\r\n\r\n\r\n", "thanks ", "i think someone from google needs to tell the CI system to do a test run.", "@tensorflow-jenkins test this please"]}, {"number": 16066, "title": "the loss is nan", "body": "when i training the facenet(build by myself) the loss is normal on the first iteration, but on the second and following iteration ,the loss became nan, i don't know what happened, please help me, Thanks!!!", "comments": ["Can one of the admins verify this patch?", "This should be opened as an issue (or Stack Overfllow question), not a pull request.\r\nClosing the PR."]}, {"number": 16065, "title": "export tflite::Intepreter's  UseNNAPI() and setNumThreads() to java", "body": "Export tflite::Intepreter's UseNNAPI() and SetNumThreads() to Java\r\nand modify the Android TfLiteCameraDemo app to use them.", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "CL Looks great, thanks for the contribution!", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@andrehentz @aselle Thanks", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks..\r\n\r\nI am getting below error after compiling..\r\nError:(147, 15) error: cannot find symbol method setUseNNAPI(boolean)\r\nError:(152, 15) error: cannot find symbol method setNumThreads(int)\r\nError:Execution failed for task ':app:compileDebugJavaWithJavac'.\r\n> Compilation failed; see the compiler error output for details.", "@deval1491 How did you build your apk? I tested it with bazel, e.g.,\r\n```\r\nbazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n```", "I am using Android studio..\r\nWhat you suggest?", "I don't really understand how the library in use is built when you use Android Studio. There probably is a problem.", "@deval1491 when you build the app with Android Studio, prebuilt AAR is used, which caused the problem.", "I am getting error using bazel also..\r\n\r\n--------------------------------------------------------------------\r\n\r\ndeval@deval-H97M-D3H:~/tf_src/tensorflow/tensorflow/contrib/lite$ bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo\r\nWARNING: ignoring http_proxy in environment.\r\nWARNING: /home/deval/.cache/bazel/_bazel_deval/6c287c6d7af4adf32e94338284090b89/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/deval/.cache/bazel/_bazel_deval/6c287c6d7af4adf32e94338284090b89/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nINFO: Analysed target //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/deval/tf_src/tensorflow/tensorflow/contrib/lite/java/BUILD:36:1: Building tensorflow/contrib/lite/java/libtensorflowlite_java.jar (6 source files) failed (Exit 1)\r\ntensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java:214: error: cannot find symbol\r\n    wrapper.setNumThreads(num_threads);\r\n           ^\r\n  symbol:   method setNumThreads(int)\r\n  location: variable wrapper of type NativeInterpreterWrapper\r\nTarget //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 0.987s, Critical Path: 0.55s\r\nFAILED: Build did NOT complete successfully\r\n--------------------------------------------------------------------\r\n", "@deval1491 I probably screwed my patch a bit. Please get latest one and try again.", "Thanks again.. seems the code is building. But now i am using android on Intel platform.. \r\nand i am getting \r\n6703 KB/s (5322802 bytes in 0.775s)\r\nFailure [INSTALL_FAILED_NO_MATCHING_ABIS: Failed to extract native libraries, res=-113]\r\n\r\nwhat commands do i use to build code for x86 target platform?", "try either\r\n`\r\nbazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config android --cpu=x86 --fat_apk_cpu=x86\r\n`\r\nor \r\n`\r\nbazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config android --cpu=x86_64 --fat_apk_cpu=x86_64\r\n`", "Thanks fro quick response..\r\nNow i am not able to run this app on x86 arch\r\n\r\n11-12 12:35:04.389  2823  8172 I ActivityManager: Start proc 8475:com.example.android.tflitecamerademo/u0a89 for activity com.example.android.tflitecamerademo/.CameraActivity\r\n11-12 12:35:04.489  2375  2536 E iahwcomposer: ReleaseGraphicsBuffer: Failed to close gem handle ErrorCode: -1 PrimeFD: 31 HWCBuffer: 0 GemHandle: 1\r\n11-12 12:35:04.562  8475  8475 D AndroidRuntime: Shutting down VM\r\n--------- beginning of crash\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime: FATAL EXCEPTION: main\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 8475\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime: java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.android.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.NullPointerException: Attempt to invoke virtual method 'void android.widget.CompoundButton.setOnCheckedChangeListener(android.widget.CompoundButton$OnCheckedChangeListener)' on a null object reference\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2780)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2856)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.ActivityThread.access$1300(ActivityThread.java:176)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1589)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:164)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.ActivityThread.main(ActivityThread.java:6494)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at java.lang.reflect.Method.invoke(Native Method)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:438)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:807)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime: Caused by: java.lang.NullPointerException: Attempt to invoke virtual method 'void android.widget.CompoundButton.setOnCheckedChangeListener(android.widget.CompoundButton$OnCheckedChangeListener)' on a null object reference\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at com.example.android.tflitecamerademo.Camera2BasicFragment.onViewCreated(Camera2BasicFragment.java:299)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1289)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.moveFragmentToExpectedState(FragmentManager.java:1557)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1618)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.BackStackRecord.executeOps(BackStackRecord.java:807)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.executeOps(FragmentManager.java:2386)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.executeOpsTogether(FragmentManager.java:2181)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.removeRedundantOperationsAndExecute(FragmentManager.java:2136)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:2043)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.dispatchMoveToState(FragmentManager.java:3032)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentManagerImpl.dispatchActivityCreated(FragmentManager.java:2979)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.FragmentController.dispatchActivityCreated(FragmentController.java:178)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.Activity.performCreate(Activity.java:7005)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.Activity.performCreate(Activity.java:6990)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2731)\r\n11-12 12:35:04.563  8475  8475 E AndroidRuntime:        ... 9 more\r\n11-12 12:35:04.567  2823  5139 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity\r\n11-12 12:35:04.571  8475  8475 I Process : Sending signal. PID: 8475 SIG: 9\r\n11-12 12:35:04.580  3702  3704 E GPTP    : Error (TX) timestamping PDelay request, error=-72\r\n11-12 12:35:04.580  3702  3704 E GPTP    : *** Unsuccessful Sync timestamp\r\n11-12 12:35:04.593  2823  6140 I ActivityManager: Process com.example.android.tflitecamerademo (pid 8475) has died: fore TOP\r\n11-12 12:35:04.593  2823  2864 W zygote64: kill(-8475, 9) failed: No such process\r\n11-12 12:35:04.593  2823  2864 I zygote64: Successfully killed process cgroup uid 10089 pid 8475 in 0ms\r\n11-12 12:35:04.608  2823  6140 D WindowManager: relayoutVisibleWindow: Window{cd2be30 u0 com.android.car.mapsplaceholder/com.android.car.mapsplaceholder.MapsPlaceholderActivity EXITING} mAnimatingExit=true, mRemoveOnExit=false, mDestroying=false\r\n11-12 12:35:04.611  2552  2552 D CRASHLOG: sdcard_allowed : Current crashlog mode is NOMINAL MODE - SDCard storage disabled.\r\n11-12 12:35:04.615  2823  2861 W BroadcastQueue: Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.stats.service.DropBoxEntryAddedReceiver\r\n11-12 12:35:04.615  2823  2861 W BroadcastQueue: Background execution not allowed: receiving Intent { act=android.intent.action.DROPBOX_ENTRY_ADDED flg=0x10 (has extras) } to com.google.android.gms/.chimera.GmsIntentOperationService$PersistentTrustedReceiver\r\n11-12 12:35:04.621  2389  2538 E EvsApp  : POWER_STATE available from vehicle: 3\r\n11-12 12:35:04.621  2389  2538 I EvsApp  : EVS: GearValue: 1\r\n11-12 12:35:04.621  2389  2538 D EvsApp  : EVS: desiredState: 0\r\n11-12 12:35:04.646  2552  2552 E CRASHLOG: CRASH   3c84c22c2cea50830170  2011-11-12/12:35:04  JAVACRASH /data/logs/crashlog5_3c84c22c2cea50830170\r\n11-12 12:35:04.646  2552  2552 I CRASHLOG: start_dumpstate_srv: Can't launch dumpstate for /data/logs/crashlog5_3c84c22c2cea50830170, already running.\r\n11-12 12:35:04.649  2375  2536 E iahwcomposer: ReleaseGraphicsBuffer: Failed to close gem handle ErrorCode: -1 PrimeFD: 58 HWCBuffer: 0 GemHandle: 9\r\n11-12 12:35:04.685  7214  7214 I PhoneDoctor: CrashReport: init of parser container\r\n", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I apologize, but an internal developer did this independently. So I don't think this change is still needed:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/049dfd5e070cfa84c82eea71c6c746a70cba4a3f/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L207\r\n", "OK, I'll rebase this", "I just cloned the tensorflow project and tried to run the Tensorflow lite demo but I had a problem with the \"setNumThreads\" method.\r\n\r\nProblem : cannot resolve method \"setNumThreads (int)\"\r\n\r\nIt seems that the method is missing from the interpreter class. I'm using the latest version of tflite at the moment : 0.1.7\r\n\r\nWhich version should I use for now that works and can run the demo without errors. Thanks in advance"]}, {"number": 16064, "title": "MKL DNN: fixed a bug related to feature column test in python", "body": "The fix only affects one file, mkl_aggregate.cc. It fixes both MKL ML and MKL DNN unit test test case. The test case is: python/feature_column:feature_column_test unit test. \r\n\r\nWith the test case, now the test case passes.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 16063, "title": "`tools/ci_builds/pi/build_raspberry_pi.sh`: '__PTHREAD_SPINS' was not declared in this scope", "body": "Hi, \r\nI'm trying to build TensorFlow for RasperryPi from my computer (Ubuntu 16:04) \r\nWhen I'm running `tensorflow/tools/ci_builds/pi/build_raspberry_pi.sh`\r\nfrom the docker container build from `tensorflow/tools/ci_builds/Dockerfile.pi-python3`, some compilation commands from `bazel build` failed ! \r\n\r\nMy goal is to use TensorFlow on my RaspberryPi (with a PiCamera) with the C++ API (compiled with bazel ? or makefile ?) ! I started to run some code on my RPi, but the compilation step is so long (6hours to compile/install TensorFlow from sources with Bazel !). I would like to save time, by compiling my code on my laptop, then sending it to the RPi for execution ! \r\n\r\nHere my commands : \r\n\r\n```bash\r\n# get tensorflow 1.5\r\ngit clone ......\r\ncd tensorflow \r\ngit checkout r1.5\r\ncd tensorflow/tools/ci_build\r\n\r\n# this docker file was added with `r1.5`!\r\ndocker build -t tf_ci_buid/pi-py3 -f Dockerfile.pi-python3 .\r\ncd ../../../\r\n\r\n# run the docker image\r\ndocker run -it -v \"$PWD\":/workspace -w /workspace tf_ci_buid/pi-py3:latest\r\n\r\n# then, from the docker container, run the *.sh code \r\n# this script contains the `bazel build` command ! \r\nroot@88db37534dff:/workspace# ./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh \r\n```\r\n\r\nBut during the execution of `bazel build`, the compilation of code from `external/highwayhash/highwayhash` seems to fail ... (see error message bellow)\r\n\r\nSomeone has already encountered this issue ? Or it's due to the new version (r1.5) ?\r\n\r\nError message : \r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python2.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.d '-frandom-seed=bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o' -iquote external/highwayhash -iquote bazel-out/armeabi-opt/genfiles/external/highwayhash -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/highwayhash/highwayhash/sip_hash.cc -o bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o)\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nIn file included from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/arm-linux-gnueabihf/bits/gthr-default.h:35:0,\r\n                 from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/arm-linux-gnueabihf/bits/gthr.h:148,\r\n                 from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/ext/atomicity.h:35,\r\n                 from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/memory:73,\r\n                 from external/highwayhash/highwayhash/state_helpers.h:23,\r\n                 from external/highwayhash/highwayhash/sip_hash.h:25,\r\n                 from external/highwayhash/highwayhash/sip_hash.cc:15:\r\n/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/ext/concurrence.h:122:34: error: '__PTHREAD_SPINS' was not declared in this scope\r\n     __gthread_mutex_t _M_mutex = __GTHREAD_MUTEX_INIT;\r\n                                  ^\r\n/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/ext/concurrence.h:177:44: error: '__PTHREAD_SPINS' was not declared in this scope\r\n     __gthread_recursive_mutex_t _M_mutex = __GTHREAD_RECURSIVE_MUTEX_INIT;\r\n                                            ^\r\nINFO: Elapsed time: 275.112s, Critical Path: 9.55s\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@JGuillaumin I had the same issue a few days ago, but using current master 283f03c825312efd3319cb37dc1a412288a536ec, and an additional\r\n```apt install libpython2.7-dev``` in the docker container before calling the build script, it worked for me.\r\n\r\nI was also able to generate `libtensorflow_framework.so` by changing the build target to `//tensorflow:libtensorflow_framework.so` in `./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`. I haven't tested it on a PI yet, though.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@sbrunk : Thanks for the advice, hopefully that helped @JGuillaumin .\r\n \r\nClosing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 16062, "title": "Which CUDA/cuDNN version should I chose for tf-1.50", "body": "![eo hqbj __vt _49a35ztc](https://user-images.githubusercontent.com/30284428/34855697-10dd4c5e-f77c-11e7-8174-3d1925170698.png)\r\n", "comments": ["If installing using pip then cuda 9.0 and cudnn 7.0. ", "Did that means both cudnn 7.0.4 and 7.0.5 are ok?", "I have not used pip package but, i think both option should work but cuda 9.0 is required for pip installation. But if you want to build tensorflow from source, you can  use CUDA Toolkit 9.1 with cuDNN 7.0.5 following the tutorial at: www.python36.com/install-tensorflow141-gpu/", "OK\uff01THANKS\uff01"]}, {"number": 16061, "title": "OSError: [Errno 12] Cannot allocate memory on deep Q learning model", "body": "I was having fun with the Deep Q learning model, which came from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN. Then after 750 episodes this error popped up.\r\n![1](https://user-images.githubusercontent.com/20869223/34855218-6ae8778a-f779-11e7-9a7f-bbe7670f47b0.png)\r\n![2](https://user-images.githubusercontent.com/20869223/34855219-6b1bc496-f779-11e7-9183-24306a413c01.png)\r\n![3](https://user-images.githubusercontent.com/20869223/34855220-6b4a6436-f779-11e7-84de-e66f157d5a68.png)\r\n![4](https://user-images.githubusercontent.com/20869223/34855221-6b77d754-f779-11e7-90c8-2b700fd10d81.png)\r\n![5](https://user-images.githubusercontent.com/20869223/34855222-6baf6d18-f779-11e7-9963-6a860225a0f2.png)\r\n![6](https://user-images.githubusercontent.com/20869223/34855224-6bdfce68-f779-11e7-8a9a-16ddc709895e.png)\r\n![7](https://user-images.githubusercontent.com/20869223/34855225-6c110014-f779-11e7-8738-c60e50059330.png)\r\nI was running this code on Ubuntu 14.04 with a 8G graphic card(GTX 1070). I have had libav-tools installed before I ran the code.\r\nI will try to reduce the batch size to see whether similar error will pop up again.\r\nPlease help me solve this problem.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I suggest using `nvidia-smi` to monitor the GPU memory. If you find this is not a bug of TensorFlow, you can close this issue.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 16060, "title": "Branch 181679271", "body": "Merging internal changes", "comments": ["@zheng-xq, @gunan, FYI, this is now largely duplicate with a push PR that just got merged: https://github.com/tensorflow/tensorflow/pull/16047\r\n\r\nWe should probably close this PR and push again."]}, {"number": 16059, "title": "[Intel MKL] Fixes for various MKLDNN unit test failures", "body": "1. MklLayout pass changes\r\n\r\n   Making workspace type uint8 for MaxPool; Handling duplicate control edge insertion\r\n\r\n   1) Handles case of inserting duplicate control edge (fixing Mkl layout graph\r\n   pass unit test)\r\n   2) Enables uint8 as workspace tensor type (makes consistent with LRN workspace\r\n   handling)\r\n\r\n   Workspace tensor type change is also performed in MaxPool and MaxPoolGrad\r\n   operators.\r\n\r\n2. Handling MklReshape failing case\r\n\r\n   MklReshape was failing on a unit test when Mkl layout and Tensorflow layout for\r\n   input tensors were same, but shape of input tensor and output tensor was\r\n   different. No reorder is required in such case, but reshape is needed. Before\r\n   this fix, we were asserting that reorder is performed.\r\n\r\n3. Adding support for empty input/filter tensors in Convolution backprop operators", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 16058, "title": "How to initialize embeddings layer within Estimator API?", "body": "I'm trying to use existing embeddings within tensorflow model, the size of embedding is greater than 2Gb and this makes my original try of doing this unsuccessful:\r\n\r\n```\r\nembedding_var = tf.get_variable(\r\n        \"embeddings\", \r\n        shape=GLOVE_MATRIX.shape, \r\n        initializer=tf.constant_initializer(np.array(GLOVE_MATRIX))\r\n)\r\n```\r\nWhich gave me this error:\r\n\r\n` Cannot create a tensor proto whose content is larger than 2GB.`\r\n\r\nI'm using AWS SageMaker, which based on the Estimator API, and the actual running of the graph in session happens behind the scene, so I'm not sure how to initialize some placeholders for embedding given that. Would be helpful if someone will be able to share the way how to do such initialization in term of EstimatorAPI.\r\n\r\n--------------------------\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I think this would normally be a \"send to StackOverflow\" (standard response appended below) kind of issue, but the 2GB limit seems like it's within range of a bug or a feature request. \r\n\r\n@martinwicke  @ispirmustafa any suggestions?\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I think it's related to graph size limit. using constant_initializer embeds the GLOVE_MATRIX into the graph which increases the graph size.\r\nCould you please try to use non constant initializer?", "looks like there the right way to initialize variables with embeddings would be to use [tf.train.Scaffold](https://www.tensorflow.org/api_docs/python/tf/train/Scaffold). Here is more information regarding this on [stackoverflow](https://stackoverflow.com/questions/48217599/how-to-initialize-embeddings-layer-within-estimator-api/48243086#48243086)"]}, {"number": 16057, "title": "Make platform a proper module", "body": "This fixes an issue where the nice error about importing tensorflow from the TF source directory is not displayed in Python 2.7\r\n\r\nFixes #16019 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@droidicus Is this still needed?", "It looks like this has already been fixed on master in another way, closing MR and associated issue."]}, {"number": 16056, "title": "Apply 1.5-rc1 cherry-picks.", "body": null, "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Whoops, there are a bunch of cherrypicks missing (I thought we had less). Not ready for review yet.", "I've added the rest of the cherry-picks: most are documentation changes and the other portion of the contributions from NVIDIA.", "CLA ok, as this is cherrypicking from master\r\n"]}, {"number": 16055, "title": "MKL: Fixed 3 bugs picked up by the unit tests", "body": "- There were 2 kinds of registrations for MatMul - with and without the 'eigen' label. Re-added the registrations with the 'eigen' label when MKL is used. \r\n- Removed the ifdef that removed the check for the label when MKL was used. The eigen op should be called when the eigen label is used.\r\n- In the selective registration header test, unicode strings aren't handled correctly, so there's a \"u\" before the kernel class string that is compared to the hardcoded string. This has been fixed.\r\n```\r\n- [('BiasAdd', 'BiasOp<CPUDevice, float>'), \r\n+ [('BiasAdd', u'BiasOp<CPUDevice, float>'), \r\n```", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 16053, "title": "java.lang.IndexOutOfBoundsException: Invalid index 0, size is 0, TensorFlow on Android", "body": "Hello,\r\n\r\nUpdated the following info:\r\nHave I written custom code No its modification of the code from here https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample \r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: anaconda\r\nTensorFlow version: 1.2.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\nI created  my custom model in keras to recognize happy faces and loaded the model into android and ran into this issue of \r\njava.lang.IndexOutOfBoundsException: Invalid index 0, size is 0 at runtime and my app crashed. I have modified the code from here https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample \r\nto suit my model. Is it the problem with protobuf file creation? I have tested my model and it works well in python. Below is the log file and the source code. Please help with this issue? Thanks!\r\n\r\n### Source code / logs\r\n\r\n\r\n01-11 16:21:12.508 18038-18078/com.sridhar.deepak.objectdetection D/OpenGLRenderer: endAllStagingAnimators on 0xab6c06f0 (ListView) with handle 0xab7000d8\r\n01-11 16:21:18.135 18038-18038/com.sridhar.deepak.objectdetection E/TensorFlowInferenceInterface: Failed to run TensorFlow session: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                                                    device='GPU'; T in [DT_STRING]\r\n                                                                                                    device='GPU'; T in [DT_BOOL]\r\n                                                                                                    device='GPU'; T in [DT_INT32]\r\n                                                                                                    device='GPU'; T in [DT_FLOAT]\r\n                                                                                                    device='CPU'; T in [DT_FLOAT]\r\n                                                                                                    device='CPU'; T in [DT_INT32]\r\n                                                                                                  \r\n                                                                                                  \t [[Node: bn0/cond/Switch = Switch[T=DT_BOOL](bn0/keras_learning_phase, bn0/keras_learning_phase)]]\r\n01-11 16:21:18.135 18038-18038/com.sridhar.deepak.objectdetection D/AndroidRuntime: Shutting down VM\r\n01-11 16:21:18.136 18038-18038/com.sridhar.deepak.objectdetection E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                                    Process: com.sridhar.deepak.objectdetection, PID: 18038\r\n                                                                                    java.lang.IndexOutOfBoundsException: Invalid index 0, size is 0\r\n                                                                                        at java.util.ArrayList.throwIndexOutOfBoundsException(ArrayList.java:255)\r\n                                                                                        at java.util.ArrayList.get(ArrayList.java:308)\r\n                                                                                        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.getTensor(TensorFlowInferenceInterface.java:473)\r\n                                                                                        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.readNodeIntoFloatBuffer(TensorFlowInferenceInterface.java:320)\r\n                                                                                        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.readNodeFloat(TensorFlowInferenceInterface.java:275)\r\n                                                                                        at com.sridhar.deepak.objectdetection.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:161)\r\n                                                                                        at com.sridhar.deepak.objectdetection.HappyFaceDetector$2.onPictureTaken(HappyFaceDetector.java:82)\r\n                                                                                        at com.flurgle.camerakit.CameraView$CameraListenerMiddleWare.onPictureTaken(CameraView.java:296)\r\n                                                                                        at com.flurgle.camerakit.Camera1$2.onPictureTaken(Camera1.java:185)\r\n                                                                                        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1118)\r\n                                                                                        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n                                                                                        at android.os.Looper.loop(Looper.java:154)\r\n                                                                                        at android.app.ActivityThread.main(ActivityThread.java:5527)\r\n                                                                                        at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                                        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:739)\r\n                                                                                        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:629)\r\n01-11 16:21:18.136 18038-18038/com.sridhar.deepak.objectdetection E/MQSEventManagerDelegate: failed to get MQSService.\r\n01-11 16:21:19.470 18038-18038/com.sridhar.deepak.objectdetection I/Process: Sending signal. PID: 18038 SIG: 9\r\n\r\nTensorFlowImageClassifier file\r\n\r\n    @Override\r\n    public List<Recognition> recognizeImage(final Bitmap bitmap,int s) {\r\n        // Log this method so that it can be analyzed with systrace.\r\n        Trace.beginSection(\"recognizeImage\");\r\n\r\n        Trace.beginSection(\"preprocessBitmap\");\r\n        // Preprocess the image data from 0-255 int to normalized float based\r\n        // on the provided parameters.\r\n        if (s==1) {\r\n            bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n            for (int i = 0; i < intValues.length; ++i) {\r\n                final int val = intValues[i];\r\n                floatValues[i * 3 + 0] = (((val >> 16) & 0xFF) - imageMean) / imageStd;\r\n                floatValues[i * 3 + 1] = (((val >> 8) & 0xFF) - imageMean) / imageStd;\r\n                floatValues[i * 3 + 2] = ((val & 0xFF) - imageMean) / imageStd;\r\n            }\r\n        }else {\r\n            bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n            for (int i = 0; i < intValues.length; ++i) {\r\n                final int val = intValues[i];\r\n                floatValues[i * 3 + 0] = (((val >> 16) & 0xFF))/imageStd;\r\n                floatValues[i * 3 + 1] = (((val >> 8) & 0xFF))/imageStd;\r\n                floatValues[i * 3 + 2] = ((val & 0xFF))/imageStd;\r\n                floatValues[i * 3 + 0] = floatValues[i * 3 + 0] - 1;\r\n                floatValues[i * 3 + 1] = floatValues[i * 3 + 1] - 1;\r\n                floatValues[i * 3 + 2] = floatValues[i * 3 + 2] - 1;\r\n\r\n            }\r\n        }\r\n        Trace.endSection();\r\n\r\n        // Copy the input data into TensorFlow.\r\n        Trace.beginSection(\"fillNodeFloat\");\r\n        inferenceInterface.fillNodeFloat(\r\n                inputName, new int[]{1, inputSize, inputSize, 3}, floatValues);\r\n        Trace.endSection();\r\n\r\n        // Run the inference call.\r\n        Trace.beginSection(\"runInference\");\r\n        inferenceInterface.runInference(outputNames);\r\n        Trace.endSection();\r\n\r\n        // Copy the output Tensor back into the output array.\r\n        Trace.beginSection(\"readNodeFloat\");\r\n        inferenceInterface.readNodeFloat(outputName, outputs);\r\n        Trace.endSection();\r\n\r\nMainActivity\r\n    private static final int INPUT_SIZE = 64;\r\n    private static final int IMAGE_MEAN = 128;\r\n    private static final float IMAGE_STD = 128;\r\n    private static final String INPUT_NAME = \"input_1\";\r\n    private static final String OUTPUT_NAME = \"fc/Sigmoid\";\r\n\r\n    private static final String MODEL_FILE = \"file:///android_asset/happy_model.pb\";\r\n    private static final String LABEL_FILE =\r\n            \"file:///android_asset/face_label.txt\";\r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler @andrewharp Hi, I have updated the info. Please help with the issue. Thanks! ", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@DeepakSridhar Hi ! Have you solved the problem? I get the same problem when inference on Android ", "@Mr-Better Hi, What model are you using? Are you using the TensorFlowImageClassifier class from the tensorflow demo app?\r\nI solved the problem. I updated the TensorFlowImageClassifier class similar to the tensorflow demo app. ", "@DeepakSridhar Hi, Thanks for reply! I am using a simple model which I designed for test, so I didn't use the TensorFlowImageClassifier class from the tensorflow demo app, and today the code works well, I don't know why even myself. Thanks again for reply!"]}, {"number": 16052, "title": "Feature Request: Setting the shape of a tf.data.Dataset if it cannot be inferred", "body": "Hello, I have really liked the new `tf.data.Dataset` api, and had a feature request. \r\nI need to often make data transformations that require third-party libraries, and use `Dataset.map` along with a `tf.py_func` command as shown in the Importing Data tutorial. In the process of doing this, Tensorflow is not able to infer the shape of the numpy arrays that are returned by the py_func-based functions, and so the output_shapes attribute of the dataset returns something like `(TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None))`\r\n\r\nTo address this, I have been adding a new map function after that calls set_shape on each tensor to enforce the shape requirement. For example, I have code that looks something like this:\r\n\r\n```\r\ndataset = dataset.map(lambda strings, labels: tuple(tf.py_func(_featurize, [strs, labels], [tf.int32, tf.float64, tf.int32, tf.int32, labels.dtype])))\r\ndataset = dataset.map(_set_shapes)\r\n```\r\nwhere \r\n```\r\ndef _set_shapes(af, pf, split, atp, labels):\r\n    af.set_shape([None, 75])\r\n    pf.set_shape([None, 14])\r\n    split.set_shape([None, ])\r\n    atp.set_shape([None, 2])\r\n    labels.set_shape([None, ])\r\n    return af, pf, split, atp, labels\r\n```\r\n\r\nCould this be simplified by adding a new `tf.data.Dataset` member function called \"set_dataset_shape\" which essentially just implements the above _set_shapes method?\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: AWS Deep Learning AMI\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: AWS Deep Learning AMI-based\r\n- **GPU model and memory**: NVIDIA K80\r\n- **Exact command to reproduce**: N/A\r\n\r\n", "comments": ["@mrry seem reasonable to you? Should we ask @ankitvgupta for a PR?", "Yes, this could make sense as a transformation called `tf.contrib.data.assert_element_shape()`. The [internal `_RestructuredDataset` class](https://github.com/tensorflow/tensorflow/blob/07e497b0ad3f0e102e167a50de2b543a4a238fde/tensorflow/contrib/data/python/ops/batching.py#L260) could be useful when implementing it. (It's currently used in `tf.contrib.data.batch_and_drop_remainder()` to achieve a similar effect.)", "Cool, that makes sense to me. Would you recommend implementing it similarly to `tf.contrib.data.batch_and_drop_remainder()` where we use `dataset.apply` to apply the function?", "Yes, that would be best for now. We prefer to try out new transformations in the `tf.contrib.data` namespace using `dataset.apply()` before they graduate to the core API.", "Great, will do.", "Any update/pr on this in contrib? /cc @lenlen", "Hi, I have open a PR #17480  to fix the issue since it has been a long time with no activity.", "Hi @mrry @facaiy , I just tried the above PR, and believe it has some issues. For one, the test cases don't include a case where one of the TensorShapes you pass to the assert is TensorShape(None). When I do that, I'm getting the following error:\r\n```\r\n  File \"/home/ubuntu/anaconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 578, in __len__\r\n    raise ValueError(\"Cannot take the length of Shape with unknown rank.\")\r\nValueError: Cannot take the length of Shape with unknown rank\r\n```\r\n\r\nFurthermore, this implementation also doesn't allow for the setting of partial shapes. For example, I would like to be able to set the shape to be [None, 5] for a tensor. However, when I do this, I get this error:\r\n```   File \"/home/ubuntu/anaconda3/envs/reverie_env/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 238, in _tensor_shape_tensor_conversion_function\r\n    \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 5)\r\n ```\r\n\r\nThis is also not a situation that the current test cases cover. I believe this is due to limitations with \r\n `tf.contrib.framework.with_shape`, which cannot take partially known TensorShapes.", "@ankitvgupta Thanks for bring this to our attention. I think it's a problem. However because the method is `assert_element_shape`, rather than `set_element_sahpe`, the behavior seems expected.\r\n\r\n@mrry How do you think about unknown shape and partial shape?", "Supporting partial shapes would make sense, and we'd welcome contributions to do that.", "OK, I'll look into it later.", "Hi, I create a PR #21702 to fix partial shape. Could you take a look?", "@mrry @facaiy \r\n\r\nIs there an update to this, because this is still an active problem with `tf.numpy_function` \r\n\r\nhttps://colab.research.google.com/drive/1DeGMPxb8cyHm5QLpqJ9-cB2sbF8PC3vA#scrollTo=qGiGN5rl4s2f&line=22&uniqifier=1\r\n\r\n", "Is this, like, actually fixed? It says closed but I don't see anything that resembles a resolution...", "> Is this, like, actually fixed? It says closed but I don't see anything that resembles a resolution...\r\n\r\nNo, it's unfortunately not."]}, {"number": 16051, "title": "Fix crash on GPU (out of GPU memory) for `softmax_cross_entropy_with_logits`", "body": "This fix tries to address the issue raised in #6766 where `softmax_cross_entropy_with_logits` will trigger the crash on GPU (out of GPU memory) if the first dimension is 0.\r\n\r\nThis fix fixes #6766.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16050, "title": "Eigen assertion when running on GPU with debug enabled ", "body": "I used r1.5 release version to compile in debug mode. The build command is \r\n```\r\nbazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nI tested the `tutorial/mnist/mnist_deep.py` and it got assertion below. I searched the forum and it seems that there is no clear answer for it. Thanks.\r\n\r\n===========================\r\nAnswer the questions below:\r\nHave I written custom code:No\r\nOS Platform and Distribution:ubuntu 16.04\r\nTensorFlow installed from: official\r\nTensorFlow version: r1.5\r\nBazel version: 0.8\r\nCUDA/cuDNN version: 9.0 / 7.0\r\nGPU model and memory: P100, 16GB\r\nExact command to reproduce: as above\r\n\r\n\r\n===========================\r\n```\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nSaving graph to: /tmp/tmpis6Bjq\r\n2018-01-11 11:45:43.003071: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-01-11 11:45:43.377683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\r\n2018-01-11 11:45:43.737737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-01-11 11:45:43.738343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties:\r\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 15.89GiB freeMemory: 15.60GiB\r\n2018-01-11 11:45:43.738437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix\r\n2018-01-11 11:45:43.738512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1\r\n2018-01-11 11:45:43.738527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y N\r\n2018-01-11 11:45:43.738535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   N Y\r\n2018-01-11 11:45:43.738573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)\r\n2018-01-11 11:45:43.738589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)\r\nstep 0, training accuracy 0.08\r\npython: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_sum_op<float, float>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_product_op<float, float>, const Eigen::TensorBroadcastingOp<const Eigen::array<long int, 1ul>, const Eigen::TensorReshapingOp<const Eigen::Sizes<1l>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<const float>, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> > > >, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer> > > > >; bool Vectorizable = true]: Assertion `**cudaGetLastError() == cudaSuccess'** failed.\r\nAborted (core dumped)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have updated in the problem description. Thank.", "Thank you for reporting that! It could be indicative of a bug in our code.", "@rmlarsen WDYT?", "I have the same issue", "I have the same issue, too.\r\nI use the r1.5 release and build TF from source with the debugging compilation mode.\r\nThe `mnist_deep` program fails at the same point with the same cuda assertion message.\r\nThe same problem runs successfully when I recompile TF with the default compilation mode.", "I also have this problem on tf1.6. ", "i have same issue.\r\ni use the r1.4 and bazel build code is \"bazel build --config=opt --config=cuda -c dbg //tensorflow/tools/pip_package:build_pip_package\".\r\n", "This issue seems to be still present at the master", "Repros on r1.9 Ubuntu 16.04 when trying to run an example built with:\r\n\r\n`bazel build -c opt --config=cuda -c dbg --strip=never tensorflow/examples/multibox_detector/...`\r\n", "I got a similar error using r1.11 on Ubuntu 16.04, using the C++ api to run a pre-trained model from this repo https://github.com/tensorflow/models/tree/master/research/object_detection (got the same error with several different models)\r\n\r\nbuilt with\r\n```\r\n  bazel build \\\r\n    --config=opt \\\r\n    --config=cuda \\\r\n    --config=monolithic \\\r\n    //tensorflow:libtensorflow_cc.so \\\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nerror:\r\n```\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<float, float>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, int>, 16, Eigen::MakePointer>, const Eigen::TensorBroadcastingOp<const Eigen::array<long int, 2ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, int>, 16, Eigen::MakePointer> > > >; bool Vectorizable = true]: Assertion `cudaGetLastError() == cudaSuccess' failed.\r\n```\r\n", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=16050\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=16050\">No</a>\n"]}]