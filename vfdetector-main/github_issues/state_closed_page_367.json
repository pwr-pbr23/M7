[{"number": 43042, "title": "tensorflow converter.convert() failure", "body": "I built model in tf==2.3.0 and I saved my model in SavedModel format.\r\n\r\nHere is my conversion operation:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./binary_bert_model')\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\nHere is my model summary, which is a typical text classification model.\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_3 (InputLayer)            [(None, 1)]          0\r\n__________________________________________________________________________________________________\r\ntext_vectorization_7 (TextVecto (None, 128)          0           input_3[0][0]\r\n__________________________________________________________________________________________________\r\ntoken_and_position_embedding_2  (None, 128, 128)     1252608     text_vectorization_7[0][0]\r\n__________________________________________________________________________________________________\r\ntransformer_block_2 (Transforme (None, 128, 128)     99584       token_and_position_embedding_2[0]\r\n__________________________________________________________________________________________________\r\nembedding_4 (Embedding)         (None, 128, 128)     1236224     text_vectorization_7[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_average_pooling1d_10 (Gl (None, 128)          0           transformer_block_2[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_8 (Conv1D)               (None, 125, 128)     65664       embedding_4[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_9 (Conv1D)               (None, 126, 128)     49280       embedding_4[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_10 (Conv1D)              (None, 127, 128)     32896       embedding_4[0][0]\r\n__________________________________________________________________________________________________\r\nconv1d_11 (Conv1D)              (None, 128, 128)     16512       embedding_4[0][0]\r\n__________________________________________________________________________________________________\r\ndropout_10 (Dropout)            (None, 128)          0           global_average_pooling1d_10[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_average_pooling1d_11 (Gl (None, 128)          0           conv1d_8[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_max_pooling1d_8 (GlobalM (None, 128)          0           conv1d_8[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_average_pooling1d_12 (Gl (None, 128)          0           conv1d_9[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_max_pooling1d_9 (GlobalM (None, 128)          0           conv1d_9[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_average_pooling1d_13 (Gl (None, 128)          0           conv1d_10[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_max_pooling1d_10 (Global (None, 128)          0           conv1d_10[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_average_pooling1d_14 (Gl (None, 128)          0           conv1d_11[0][0]\r\n__________________________________________________________________________________________________\r\nglobal_max_pooling1d_11 (Global (None, 128)          0           conv1d_11[0][0]\r\n__________________________________________________________________________________________________\r\ndense_20 (Dense)                (None, 64)           8256        dropout_10[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_4 (Concatenate)     (None, 1024)         0           global_average_pooling1d_11[0][0]\r\n                                                                 global_max_pooling1d_8[0][0]\r\n                                                                 global_average_pooling1d_12[0][0]\r\n                                                                 global_max_pooling1d_9[0][0]\r\n                                                                 global_average_pooling1d_13[0][0]\r\n                                                                 global_max_pooling1d_10[0][0]\r\n                                                                 global_average_pooling1d_14[0][0]\r\n                                                                 global_max_pooling1d_11[0][0]\r\n__________________________________________________________________________________________________\r\ndropout_11 (Dropout)            (None, 64)           0           dense_20[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate_5 (Concatenate)     (None, 1088)         0           concatenate_4[0][0]\r\n                                                                 dropout_11[0][0]\r\n__________________________________________________________________________________________________\r\nmlp1 (Dense)                    (None, 512)          557568      concatenate_5[0][0]\r\n__________________________________________________________________________________________________\r\nmlp2 (Dense)                    (None, 1)            513         mlp1[0][0]\r\n==================================================================================================\r\nTotal params: 2,082,881\r\nTrainable params: 2,082,881\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```\r\n\r\n\r\nHere is the failure errors:\r\n\r\n> ---------------------------------------------------------------------------\r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n>     496         results = c_api.TF_GraphImportGraphDefWithResults(\r\n> --> 497             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n>     498         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n> \r\n> InvalidArgumentError: Input 3 of node StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/transformer_encoder/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/input/_622:0 incompatible with expected resource.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-7-ec5379be901e> in <module>\r\n> ----> 1 converter.convert()\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>    1074         Invalid quantization parameters.\r\n>    1075     \"\"\"\r\n> -> 1076     return super(TFLiteConverterV2, self).convert()\r\n>    1077\r\n>    1078\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>     876     frozen_func, graph_def = (\r\n>     877         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n> --> 878             self._funcs[0], lower_control_flow=False))\r\n>     879\r\n>     880     input_tensors = [\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n>    1107\r\n>    1108   frozen_func = _construct_concrete_function(func, output_graph_def,\r\n> -> 1109                                              converted_input_indices)\r\n>    1110   return frozen_func, output_graph_def\r\n>    1111\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)\r\n>     999   new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n>    1000                                                    new_input_names,\r\n> -> 1001                                                    new_output_names)\r\n>    1002\r\n>    1003   # Manually propagate shape for input tensors where the shape is not correctly\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)\r\n>     648     importer.import_graph_def(graph_def, name=\"\")\r\n>     649\r\n> --> 650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n>     651   import_graph = wrapped_import.graph\r\n>     652   return wrapped_import.prune(\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)\r\n>     626           signature=signature,\r\n>     627           add_control_dependencies=False,\r\n> --> 628           collections={}),\r\n>     629       variable_holder=holder,\r\n>     630       signature=signature)\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n>     984         _, original_func = tf_decorator.unwrap(python_func)\r\n>     985\r\n> --> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n>     987\r\n>     988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)\r\n>      85\r\n>      86   def __call__(self, *args, **kwargs):\r\n> ---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n>      88\r\n>      89   def call_with_variable_creator_scope(self, fn):\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)\r\n>      91     def wrapped(*args, **kwargs):\r\n>      92       with variable_scope.variable_creator_scope(self.variable_creator_scope):\r\n> ---> 93         return fn(*args, **kwargs)\r\n>      94\r\n>      95     return wrapped\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in _imports_graph_def()\r\n>     646\r\n>     647   def _imports_graph_def():\r\n> --> 648     importer.import_graph_def(graph_def, name=\"\")\r\n>     649\r\n>     650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n>     505                 'in a future version' if date is None else ('after %s' % date),\r\n>     506                 instructions)\r\n> --> 507       return func(*args, **kwargs)\r\n>     508\r\n>     509     doc = _add_deprecated_arg_notice_to_docstring(\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)\r\n>     403       return_elements=return_elements,\r\n>     404       name=name,\r\n> --> 405       producer_op_list=producer_op_list)\r\n>     406\r\n>     407\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n>     499       except errors.InvalidArgumentError as e:\r\n>     500         # Convert to ValueError for backwards compatibility.\r\n> --> 501         raise ValueError(str(e))\r\n>     502\r\n>     503     # Create _DefinedFunctions for any imported functions.\r\n> \r\n> ValueError: Input 3 of node StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/transformer_encoder/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/input/_622:0 incompatible with expected resource.", "comments": ["Can you share `./binary_bert_model`?\r\n", "Sorry. TextVectorization layer is not supported by TFLite yet. The layer requires MutableHashtable support. TFLite currently is not able to support it.", "@abattery Do you think that it can fail with a more \"readable\" exception? \r\nIf not I suppose that other tickets like this will be opened again.", "@bhack Thanks for your suggestion. We are working on having more readability in the conversion errors. TextVectorization is kind of a new thing, which is under experimental phase. Sorry for that :-)", "@abattery Thanks it is just to not overload triaging activities :wink: ", "Yes, I use TextVectorization which adapts to the corpus and convert the text of string into word ids. \r\n```\r\nmaxlen = 128\r\nvectorizer = TextVectorization(max_tokens=10000, output_sequence_length=maxlen)\r\ntext_ds = tf.data.Dataset.from_tensor_slices(df_all['content'].tolist() + df_val['content'].tolist()).batch(128)\r\nvectorizer.adapt(text_ds)\r\n```\r\n", "I replace the textvectorization process with the traditional tokenizer,\r\n\r\nhowever, there is still errors when I try to use tf.lite convert:\r\n\r\n> [---------------------------------------------------------------------------\r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n>     496         results = c_api.TF_GraphImportGraphDefWithResults(\r\n> --> 497             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n>     498         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n> \r\n> InvalidArgumentError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-56-6718a11fca73> in <module>\r\n>       1 converter = tf.lite.TFLiteConverter.from_saved_model('./trans_model')\r\n>       2 converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n> ----> 3 tflite_quant_model = converter.convert()\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>    1074         Invalid quantization parameters.\r\n>    1075     \"\"\"\r\n> -> 1076     return super(TFLiteConverterV2, self).convert()\r\n>    1077\r\n>    1078\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>     876     frozen_func, graph_def = (\r\n>     877         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n> --> 878             self._funcs[0], lower_control_flow=False))\r\n>     879\r\n>     880     input_tensors = [\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n>    1107\r\n>    1108   frozen_func = _construct_concrete_function(func, output_graph_def,\r\n> -> 1109                                              converted_input_indices)\r\n>    1110   return frozen_func, output_graph_def\r\n>    1111\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)\r\n>     999   new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n>    1000                                                    new_input_names,\r\n> -> 1001                                                    new_output_names)\r\n>    1002\r\n>    1003   # Manually propagate shape for input tensors where the shape is not correctly\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)\r\n>     648     importer.import_graph_def(graph_def, name=\"\")\r\n>     649\r\n> --> 650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n>     651   import_graph = wrapped_import.graph\r\n>     652   return wrapped_import.prune(\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)\r\n>     626           signature=signature,\r\n>     627           add_control_dependencies=False,\r\n> --> 628           collections={}),\r\n>     629       variable_holder=holder,\r\n>     630       signature=signature)\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n>     984         _, original_func = tf_decorator.unwrap(python_func)\r\n>     985\r\n> --> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n>     987\r\n>     988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)\r\n>      85\r\n>      86   def __call__(self, *args, **kwargs):\r\n> ---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n>      88\r\n>      89   def call_with_variable_creator_scope(self, fn):\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)\r\n>      91     def wrapped(*args, **kwargs):\r\n>      92       with variable_scope.variable_creator_scope(self.variable_creator_scope):\r\n> ---> 93         return fn(*args, **kwargs)\r\n>      94\r\n>      95     return wrapped\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in _imports_graph_def()\r\n>     646\r\n>     647   def _imports_graph_def():\r\n> --> 648     importer.import_graph_def(graph_def, name=\"\")\r\n>     649\r\n>     650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n>     505                 'in a future version' if date is None else ('after %s' % date),\r\n>     506                 instructions)\r\n> --> 507       return func(*args, **kwargs)\r\n>     508\r\n>     509     doc = _add_deprecated_arg_notice_to_docstring(\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)\r\n>     403       return_elements=return_elements,\r\n>     404       name=name,\r\n> --> 405       producer_op_list=producer_op_list)\r\n>     406\r\n>     407\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n>     499       except errors.InvalidArgumentError as e:\r\n>     500         # Convert to ValueError for backwards compatibility.\r\n> --> 501         raise ValueError(str(e))\r\n>     502\r\n>     503     # Create _DefinedFunctions for any imported functions.\r\n> \r\n> ValueError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.](url)", "Here is the data preparation step:\r\n\r\n```\r\nmaxlen = 100\r\ndef get_keras_data(df): \r\n    content_ids_list = tok.texts_to_sequences(df['content'].tolist())\r\n    content_pad_array = tf.keras.preprocessing.sequence.pad_sequences(content_ids_list, maxlen=maxlen, padding='post', truncating='post',value=0)\r\n    return [content_pad_array], df['label'].values\r\n\r\n\r\nx_train, y_train = get_keras_data(df_all)\r\nx_val, y_val = get_keras_data(df_val)\r\n\r\n```", "For the tokenize version, I try to use the command line method to convert.\r\nIt seems fine.\r\n\r\n```\r\ntflite_convert \\\r\n  --saved_model_dir=trans_model \\\r\n  --output_file=trans_model_tflite\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/26405281/92549512-03b8b000-f28c-11ea-97f5-54b3f1cff09d.png)\r\n"]}, {"number": 43041, "title": "TFLite C API problem inferencing", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI'm using TFLite C API to run inference repeatedly, and got the following error. This error does not happen right away, but only randomly after many different runs\r\n```\r\n2020-09-08 17:45:35.676 11696-25578/? E/tflite: Unsupported data type: 0\r\n2020-09-08 17:45:35.676 11696-25578/? E/tflite: Node number 0 (POW) failed to invoke.\r\n2020-09-08 17:45:38.469 25588-25588/? A/DEBUG:       #03 pc 0000000000011e50  /system/lib64/libtensorflowlite_c.so (TfLiteInterpreterDelete+36)\r\n2020-09-08 17:51:45.920 26100-26312/? E/tflite: tensorflow/lite/kernels/add.cc:373 Type NOTYPE is unsupported by op Add.\r\n2020-09-08 17:51:45.921 26100-26312/? E/tflite: Node number 9 (ADD) failed to invoke.\r\n2020-09-08 17:51:48.680 26320-26320/? A/DEBUG:       #03 pc 0000000000011e50  /system/lib64/libtensorflowlite_c.so (TfLiteInterpreterDelete+36)\r\n2020-09-08 17:52:02.371 26405-26405/? A/DEBUG:       #00 pc 0000000000011f04  /system/lib64/libtensorflowlite_c.so (TfLiteInterpreterGetInputTensor+16)\r\n```\r\n\r\nIs this related to failure in copying data to `TfLiteTensorCopyToBuffer`? How could we try catch this problem?\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Can you share a very minimal but runnable code snippet to reproduce this?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43041\">No</a>\n"]}, {"number": 43040, "title": "Overview sanilty check fails on pylint upgrade", "body": "This is just to have an overview of the sanity checks that will fail with a `pytlint` upgrade:\r\n\r\nTheoretically fix: https://github.com/tensorflow/tensorflow/issues/43038\r\n\r\n", "comments": ["To shortcut triage routing /cc @mihaimaruseac", "@bhack  This PR is in draft, any update on this? Please. Thanks!", "@gbaned Check https://github.com/tensorflow/tensorflow/issues/43038#issuecomment-688817162", "Also in the meantime pylint was just partially updated but with another version by https://github.com/tensorflow/tensorflow/pull/42938 approved by @angerson ", "@bhack  Can you please resolve conflicts? Thanks!", "Hmm, sanity is still failing :(", "@mihaimaruseac Yes this was opened to have an overview of what it is failing with an updated version of pylint.", "Let me know what is the plan. Do we want to change pylint configuration? Do we want to fix something else? Etc..", "I don't think there is any plan at the moment. We have the issue that we have to make sure any changes to pylint don't go against internal lint rules, but otherwise I think we can attempt an upgrade", "@bhack  Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "> @bhack  Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!\n\nI don't understand cause it seems to me that there Is still any plan on your side.", "@mihaimaruseac We already fixed this internally, right? Is this PR still necessary?", "I am unsure. Our pylint version is still old and needs to be upraded (config too). There's at least a discrepancy between formatting and linting code around Python type annotations.", "@angerson Any update on this PR? Please. Thanks!", "@angerson  Any update on this PR? Please. Thanks!", "@mihaimaruseac Why it was ready to pull and merged? What is the plan?", "It looked like it was passing all external CI when I approved. It is failing internal sanity with \r\n\r\n```\r\nFAIL: Found 6088 non-allowlisted pylint errors:\r\ntensorflow/lite/python/convert.py:112: [E0307(invalid-str-returned), OpsSet.__str__] __str__ does not return str\r\n...\r\n```\r\n\r\nWill probably revert and try fixing from inside now.", "> It looked like it was passing all external CI when I approved. It is failing internal sanity with \n> \n> ```\n> FAIL: Found 6088 non-allowlisted pylint errors:\n> tensorflow/lite/python/convert.py:112: [E0307(invalid-str-returned), OpsSet.__str__] __str__ does not return str\n> ...\n> ```\n> \n> Will probably revert and try fixing from inside now.\n\nOk thanks", "We had a rollback of this with https://github.com/tensorflow/tensorflow/commit/420ce899d013b7ac60482a66edacb458cc6a6bf1"]}, {"number": 43039, "title": "MultiWorkerMirroredStrategy.experimental_distribute_dataset return an empty DistributedDataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  docker hub image: tensorflow/tensorflow:2.3.0-gpu\r\n- TensorFlow version (use command below): 2.3.0-gpu\r\n- Python version: 3\r\n- CUDA/cuDNN version: 7.6.4.38-1\r\n- GPU model and memory:GeForce GTX 1080 Ti , 12G memory\r\n\r\n**Describe the current behavior**\r\nI use `MultiWorkerMirroredStrategy` with three machines, one GPU on each. And create Dataset from tfrecord, then wrap it with `experimental_distribute_dataset`. I want to iterator the returned DistributedDataset, but encounter `tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence` error. It means the DistributedDataset is empty. But I succeed to print the tensors in original dataset.\r\n\r\n**Describe the expected behavior**\r\nPrint the DistributedDataset elements properly.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\ntfrecord file [link:](https://drive.google.com/file/d/1bvqxBHB3jiZewDpKt85tT3pjau6AvEpQ/view?usp=sharing)\r\n\r\nYou should change the `dataset_path` in dict `cfg` to proper location after you download the tfrecord file.\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nfrom absl import app, flags, logging\r\n\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom itertools import product as product\r\n\r\n\r\n###############################################################################\r\n#   Tensorflow / Numpy Priors                                                 #\r\n###############################################################################\r\ndef prior_box(image_sizes, min_sizes, steps, clip=False):\r\n    \"\"\"prior box\"\"\"\r\n    feature_maps = [\r\n        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\r\n        for step in steps]\r\n\r\n    anchors = []\r\n    for k, f in enumerate(feature_maps):\r\n        for i, j in product(range(f[0]), range(f[1])):\r\n            for min_size in min_sizes[k]:\r\n                s_kx = min_size / image_sizes[1]\r\n                s_ky = min_size / image_sizes[0]\r\n                cx = (j + 0.5) * steps[k] / image_sizes[1]\r\n                cy = (i + 0.5) * steps[k] / image_sizes[0]\r\n                anchors += [cx, cy, s_kx, s_ky]\r\n\r\n    output = np.asarray(anchors).reshape([-1, 4])\r\n\r\n    if clip:\r\n        output = np.clip(output, 0, 1)\r\n\r\n    return output\r\n\r\n\r\ndef prior_box_tf(image_sizes, min_sizes, steps, clip=False):\r\n    \"\"\"prior box\"\"\"\r\n    image_sizes = tf.cast(tf.convert_to_tensor(image_sizes), tf.float32)\r\n    feature_maps = tf.math.ceil(\r\n        tf.reshape(image_sizes, [1, 2]) /\r\n        tf.reshape(tf.cast(steps, tf.float32), [-1, 1]))\r\n\r\n    anchors = []\r\n    for k in range(len(min_sizes)):\r\n        grid_x, grid_y = _meshgrid_tf(tf.range(feature_maps[k][1]),\r\n                                      tf.range(feature_maps[k][0]))\r\n        cx = (grid_x + 0.5) * steps[k] / image_sizes[1]\r\n        cy = (grid_y + 0.5) * steps[k] / image_sizes[0]\r\n        cxcy = tf.stack([cx, cy], axis=-1)\r\n        cxcy = tf.reshape(cxcy, [-1, 2])\r\n        cxcy = tf.repeat(cxcy, repeats=tf.shape(min_sizes[k])[0], axis=0)\r\n\r\n        sx = min_sizes[k] / image_sizes[1]\r\n        sy = min_sizes[k] / image_sizes[0]\r\n        sxsy = tf.stack([sx, sy], 1)\r\n        sxsy = tf.repeat(sxsy[tf.newaxis],\r\n                         repeats=tf.shape(grid_x)[0] * tf.shape(grid_x)[1],\r\n                         axis=0)\r\n        sxsy = tf.reshape(sxsy, [-1, 2])\r\n\r\n        anchors.append(tf.concat([cxcy, sxsy], 1))\r\n\r\n    output = tf.concat(anchors, axis=0)\r\n\r\n    if clip:\r\n        output = tf.clip_by_value(output, 0, 1)\r\n\r\n    return output\r\n\r\n\r\ndef _meshgrid_tf(x, y):\r\n    \"\"\" workaround solution of the tf.meshgrid() issue:\r\n        https://github.com/tensorflow/tensorflow/issues/34470\"\"\"\r\n    grid_shape = [tf.shape(y)[0], tf.shape(x)[0]]\r\n    grid_x = tf.broadcast_to(tf.reshape(x, [1, -1]), grid_shape)\r\n    grid_y = tf.broadcast_to(tf.reshape(y, [-1, 1]), grid_shape)\r\n    return grid_x, grid_y\r\n\r\n\r\n###############################################################################\r\n#   Tensorflow Encoding                                                       #\r\n###############################################################################\r\ndef encode_tf(labels, priors, match_thresh, ignore_thresh,\r\n              variances=[0.1, 0.2]):\r\n    \"\"\"tensorflow encoding\"\"\"\r\n    assert ignore_thresh <= match_thresh\r\n    priors = tf.cast(priors, tf.float32)\r\n    bbox = labels[:, :4]\r\n    landm = labels[:, 4:-1]\r\n    landm_valid = labels[:, -1]  # 1: with landm, 0: w/o landm.\r\n\r\n    # jaccard index\r\n    overlaps = _jaccard(bbox, _point_form(priors))\r\n\r\n    # (Bipartite Matching)\r\n    # [num_objects] best prior for each ground truth\r\n    best_prior_overlap, best_prior_idx = tf.math.top_k(overlaps, k=1)\r\n    best_prior_overlap = best_prior_overlap[:, 0]\r\n    best_prior_idx = best_prior_idx[:, 0]\r\n\r\n    # [num_priors] best ground truth for each prior\r\n    overlaps_t = tf.transpose(overlaps)\r\n    best_truth_overlap, best_truth_idx = tf.math.top_k(overlaps_t, k=1)\r\n    best_truth_overlap = best_truth_overlap[:, 0]\r\n    best_truth_idx = best_truth_idx[:, 0]\r\n\r\n    # ensure best prior\r\n    def _loop_body(i, bt_idx, bt_overlap):\r\n        bp_mask = tf.one_hot(best_prior_idx[i], tf.shape(bt_idx)[0])\r\n        bp_mask_int = tf.cast(bp_mask, tf.int32)\r\n        new_bt_idx = bt_idx * (1 - bp_mask_int) + bp_mask_int * i\r\n        bp_mask_float = tf.cast(bp_mask, tf.float32)\r\n        new_bt_overlap = bt_overlap * (1 - bp_mask_float) + bp_mask_float * 2\r\n        return tf.cond(best_prior_overlap[i] > match_thresh,\r\n                       lambda: (i + 1, new_bt_idx, new_bt_overlap),\r\n                       lambda: (i + 1, bt_idx, bt_overlap))\r\n    _, best_truth_idx, best_truth_overlap = tf.while_loop(\r\n        lambda i, bt_idx, bt_overlap: tf.less(i, tf.shape(best_prior_idx)[0]),\r\n        _loop_body, [tf.constant(0), best_truth_idx, best_truth_overlap])\r\n\r\n    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\r\n    matches_landm = tf.gather(landm, best_truth_idx)  # [num_priors, 10]\r\n    matches_landm_v = tf.gather(landm_valid, best_truth_idx)  # [num_priors]\r\n\r\n    loc_t = _encode_bbox(matches_bbox, priors, variances)\r\n    landm_t = _encode_landm(matches_landm, priors, variances)\r\n    landm_valid_t = tf.cast(matches_landm_v > 0, tf.float32)\r\n    conf_t = tf.cast(best_truth_overlap > match_thresh, tf.float32)\r\n    conf_t = tf.where(\r\n        tf.logical_and(best_truth_overlap < match_thresh,\r\n                       best_truth_overlap > ignore_thresh),\r\n        tf.ones_like(conf_t) * -1, conf_t)    # 1: pos, 0: neg, -1: ignore\r\n\r\n    return tf.concat([loc_t, landm_t, landm_valid_t[..., tf.newaxis],\r\n                      conf_t[..., tf.newaxis]], axis=1)\r\n\r\n\r\ndef _encode_bbox(matched, priors, variances):\r\n    \"\"\"Encode the variances from the priorbox layers into the ground truth\r\n    boxes we have matched (based on jaccard overlap) with the prior boxes.\r\n    Args:\r\n        matched: (tensor) Coords of ground truth for each prior in point-form\r\n            Shape: [num_priors, 4].\r\n        priors: (tensor) Prior boxes in center-offset form\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        encoded boxes (tensor), Shape: [num_priors, 4]\r\n    \"\"\"\r\n\r\n    # dist b/t match center and prior's center\r\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\r\n    # encode variance\r\n    g_cxcy /= (variances[0] * priors[:, 2:])\r\n    # match wh / prior wh\r\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\r\n    g_wh = tf.math.log(g_wh) / variances[1]\r\n    # return target for smooth_l1_loss\r\n    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]\r\n\r\n\r\ndef _encode_landm(matched, priors, variances):\r\n    \"\"\"Encode the variances from the priorbox layers into the ground truth\r\n    boxes we have matched (based on jaccard overlap) with the prior boxes.\r\n    Args:\r\n        matched: (tensor) Coords of ground truth for each prior in point-form\r\n            Shape: [num_priors, 10].\r\n        priors: (tensor) Prior boxes in center-offset form\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        encoded landm (tensor), Shape: [num_priors, 10]\r\n    \"\"\"\r\n\r\n    # dist b/t match center and prior's center\r\n    matched = tf.reshape(matched, [tf.shape(matched)[0], 5, 2])\r\n    priors = tf.broadcast_to(\r\n        tf.expand_dims(priors, 1), [tf.shape(matched)[0], 5, 4])\r\n    g_cxcy = matched[:, :, :2] - priors[:, :, :2]\r\n    # encode variance\r\n    g_cxcy /= (variances[0] * priors[:, :, 2:])\r\n    # g_cxcy /= priors[:, :, 2:]\r\n    g_cxcy = tf.reshape(g_cxcy, [tf.shape(g_cxcy)[0], -1])\r\n    # return target for smooth_l1_loss\r\n    return g_cxcy\r\n\r\n\r\ndef _point_form(boxes):\r\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\r\n    representation for comparison to point form ground truth data.\r\n    Args:\r\n        boxes: (tensor) center-size default boxes from priorbox layers.\r\n    Return:\r\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\r\n    \"\"\"\r\n    return tf.concat((boxes[:, :2] - boxes[:, 2:] / 2,\r\n                      boxes[:, :2] + boxes[:, 2:] / 2), axis=1)\r\n\r\n\r\ndef _intersect(box_a, box_b):\r\n    \"\"\" We resize both tensors to [A,B,2]:\r\n    [A,2] -> [A,1,2] -> [A,B,2]\r\n    [B,2] -> [1,B,2] -> [A,B,2]\r\n    Then we compute the area of intersect between box_a and box_b.\r\n    Args:\r\n      box_a: (tensor) bounding boxes, Shape: [A,4].\r\n      box_b: (tensor) bounding boxes, Shape: [B,4].\r\n    Return:\r\n      (tensor) intersection area, Shape: [A,B].\r\n    \"\"\"\r\n    A = tf.shape(box_a)[0]\r\n    B = tf.shape(box_b)[0]\r\n    max_xy = tf.minimum(\r\n        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\r\n        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\r\n    min_xy = tf.maximum(\r\n        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\r\n        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\r\n    inter = tf.maximum((max_xy - min_xy), tf.zeros_like(max_xy - min_xy))\r\n    return inter[:, :, 0] * inter[:, :, 1]\r\n\r\n\r\ndef _jaccard(box_a, box_b):\r\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\r\n    is simply the intersection over union of two boxes.  Here we operate on\r\n    ground truth boxes and default boxes.\r\n    E.g.:\r\n        A \u2229 B / A \u222a B = A \u2229 B / (area(A) + area(B) - A \u2229 B)\r\n    Args:\r\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\r\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\r\n    Return:\r\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\r\n    \"\"\"\r\n    inter = _intersect(box_a, box_b)\r\n    area_a = tf.broadcast_to(\r\n        tf.expand_dims(\r\n            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\r\n        tf.shape(inter))  # [A,B]\r\n    area_b = tf.broadcast_to(\r\n        tf.expand_dims(\r\n            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\r\n        tf.shape(inter))  # [A,B]\r\n    union = area_a + area_b - inter\r\n    return inter / union  # [A,B]\r\n\r\n\r\n###############################################################################\r\n#   Tensorflow Decoding                                                       #\r\n###############################################################################\r\ndef decode_tf(labels, priors, variances=[0.1, 0.2]):\r\n    \"\"\"tensorflow decoding\"\"\"\r\n    bbox = _decode_bbox(labels[:, :4], priors, variances)\r\n    landm = _decode_landm(labels[:, 4:14], priors, variances)\r\n    landm_valid = labels[:, 14][:, tf.newaxis]\r\n    conf = labels[:, 15][:, tf.newaxis]\r\n\r\n    return tf.concat([bbox, landm, landm_valid, conf], axis=1)\r\n\r\n\r\ndef _decode_bbox(pre, priors, variances=[0.1, 0.2]):\r\n    \"\"\"Decode locations from predictions using priors to undo\r\n    the encoding we did for offset regression at train time.\r\n    Args:\r\n        pre (tensor): location predictions for loc layers,\r\n            Shape: [num_priors,4]\r\n        priors (tensor): Prior boxes in center-offset form.\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        decoded bounding box predictions\r\n    \"\"\"\r\n    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]\r\n    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])\r\n\r\n    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)\r\n\r\n\r\ndef _decode_landm(pre, priors, variances=[0.1, 0.2]):\r\n    \"\"\"Decode landm from predictions using priors to undo\r\n    the encoding we did for offset regression at train time.\r\n    Args:\r\n        pre (tensor): landm predictions for loc layers,\r\n            Shape: [num_priors,10]\r\n        priors (tensor): Prior boxes in center-offset form.\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        decoded landm predictions\r\n    \"\"\"\r\n    landms = tf.concat(\r\n        [priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\r\n         priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\r\n         priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\r\n         priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\r\n         priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:]], axis=1)\r\n    return landms\r\n\r\ndef _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,\r\n                    using_encoding, priors, match_thresh, ignore_thresh,\r\n                    variances):\r\n    def parse_tfrecord(tfrecord):\r\n        features = {\r\n            'image/img_name': tf.io.FixedLenFeature([], tf.string),\r\n            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark0/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark0/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark1/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark1/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark2/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark2/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark3/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark3/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark4/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark4/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark/valid': tf.io.VarLenFeature(tf.float32)}\r\n        if using_bin:\r\n            features['image/encoded'] = tf.io.FixedLenFeature([], tf.string)\r\n            x = tf.io.parse_single_example(tfrecord, features)\r\n            img = tf.image.decode_jpeg(x['image/encoded'], channels=3)\r\n        else:\r\n            features['image/img_path'] = tf.io.FixedLenFeature([], tf.string)\r\n            x = tf.io.parse_single_example(tfrecord, features)\r\n            image_encoded = tf.io.read_file(x['image/img_path'])\r\n            img = tf.image.decode_jpeg(image_encoded, channels=3)\r\n\r\n        labels = tf.stack(\r\n            [tf.sparse.to_dense(x['image/object/bbox/xmin']),\r\n             tf.sparse.to_dense(x['image/object/bbox/ymin']),\r\n             tf.sparse.to_dense(x['image/object/bbox/xmax']),\r\n             tf.sparse.to_dense(x['image/object/bbox/ymax']),\r\n             tf.sparse.to_dense(x['image/object/landmark0/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark0/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark1/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark1/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark2/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark2/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark3/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark3/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark4/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark4/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark/valid'])], axis=1)\r\n\r\n        img, labels = _transform_data(\r\n            img_dim, using_flip, using_distort, using_encoding, priors,\r\n            match_thresh, ignore_thresh, variances)(img, labels)\r\n\r\n        return img, labels\r\n    return parse_tfrecord\r\n\r\n\r\ndef _transform_data(img_dim, using_flip, using_distort, using_encoding, priors,\r\n                    match_thresh, ignore_thresh, variances):\r\n    def transform_data(img, labels):\r\n        img = tf.cast(img, tf.float32)\r\n\r\n        # randomly crop\r\n        img, labels = _crop(img, labels)\r\n\r\n        # padding to square\r\n        img = _pad_to_square(img)\r\n\r\n        # resize\r\n        img, labels = _resize(img, labels, img_dim)\r\n\r\n        # randomly left-right flip\r\n        if using_flip:\r\n            img, labels = _flip(img, labels)\r\n\r\n        # distort\r\n        if using_distort:\r\n            img = _distort(img)\r\n\r\n        # encode labels to feature targets\r\n        if using_encoding:\r\n            labels = encode_tf(labels=labels, priors=priors,\r\n                               match_thresh=match_thresh,\r\n                               ignore_thresh=ignore_thresh,\r\n                               variances=variances)\r\n\r\n        return img, labels\r\n    return transform_data\r\n\r\n\r\ndef load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\r\n                          using_bin=True, using_flip=True, using_distort=True,\r\n                          using_encoding=True, priors=None, match_thresh=0.45,\r\n                          ignore_thresh=0.3, variances=[0.1, 0.2],\r\n                          shuffle=True, buffer_size=10240):\r\n    \"\"\"load dataset from tfrecord\"\"\"\r\n    if not using_encoding:\r\n        assert batch_size == 1  # dynamic data len when using_encoding\r\n    else:\r\n        assert priors is not None\r\n\r\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\r\n    raw_dataset = raw_dataset.repeat()\r\n    if shuffle:\r\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\r\n    dataset = raw_dataset.map(\r\n        _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,\r\n                        using_encoding, priors, match_thresh, ignore_thresh,\r\n                        variances),\r\n        num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.batch(batch_size, drop_remainder=True)\r\n    dataset = dataset.prefetch(\r\n        buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n    return dataset\r\n\r\n\r\n###############################################################################\r\n#   Data Augmentation                                                         #\r\n###############################################################################\r\ndef _flip(img, labels):\r\n    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\r\n\r\n    def flip_func():\r\n        flip_img = tf.image.flip_left_right(img)\r\n        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\r\n                                1 - labels[:, 0],  labels[:, 3],\r\n                                1 - labels[:, 6],  labels[:, 7],\r\n                                1 - labels[:, 4],  labels[:, 5],\r\n                                1 - labels[:, 8],  labels[:, 9],\r\n                                1 - labels[:, 12], labels[:, 13],\r\n                                1 - labels[:, 10], labels[:, 11],\r\n                                labels[:, 14]], axis=1)\r\n\r\n        return flip_img, flip_labels\r\n\r\n    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],\r\n                          default=lambda: (img, labels))\r\n\r\n    return img, labels\r\n\r\n\r\ndef _crop(img, labels, max_loop=250):\r\n    shape = tf.shape(img)\r\n\r\n    def matrix_iof(a, b):\r\n        \"\"\"\r\n        return iof of a and b, numpy version for data augenmentation\r\n        \"\"\"\r\n        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\r\n        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\r\n\r\n        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\r\n            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\r\n        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\r\n        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\r\n\r\n    def crop_loop_body(i, img, labels):\r\n        valid_crop = tf.constant(1, tf.int32)\r\n\r\n        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\r\n        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\r\n        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\r\n        h = w = tf.cast(scale * short_side, tf.int32)\r\n        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\r\n        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\r\n        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\r\n        roi = tf.cast(roi, tf.float32)\r\n\r\n        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\r\n        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\r\n                             lambda: valid_crop, lambda: 0)\r\n\r\n        centers = (labels[:, :2] + labels[:, 2:4]) / 2\r\n        mask_a = tf.reduce_all(\r\n            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\r\n            axis=1)\r\n        labels_t = tf.boolean_mask(labels, mask_a)\r\n        valid_crop = tf.cond(tf.reduce_any(mask_a),\r\n                             lambda: valid_crop, lambda: 0)\r\n\r\n        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\r\n        h_offset = tf.cast(h_offset, tf.float32)\r\n        w_offset = tf.cast(w_offset, tf.float32)\r\n        labels_t = tf.stack(\r\n            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\r\n             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\r\n             labels_t[:, 4] - w_offset,  labels_t[:, 5] - h_offset,\r\n             labels_t[:, 6] - w_offset,  labels_t[:, 7] - h_offset,\r\n             labels_t[:, 8] - w_offset,  labels_t[:, 9] - h_offset,\r\n             labels_t[:, 10] - w_offset, labels_t[:, 11] - h_offset,\r\n             labels_t[:, 12] - w_offset, labels_t[:, 13] - h_offset,\r\n             labels_t[:, 14]], axis=1)\r\n\r\n        return tf.cond(valid_crop == 1,\r\n                       lambda: (max_loop, img_t, labels_t),\r\n                       lambda: (i + 1, img, labels))\r\n\r\n    _, img, labels = tf.while_loop(\r\n        lambda i, img, labels: tf.less(i, max_loop),\r\n        crop_loop_body,\r\n        [tf.constant(-1), img, labels],\r\n        shape_invariants=[tf.TensorShape([]),\r\n                          tf.TensorShape([None, None, 3]),\r\n                          tf.TensorShape([None, 15])])\r\n\r\n    return img, labels\r\n\r\n\r\ndef _pad_to_square(img):\r\n    height = tf.shape(img)[0]\r\n    width = tf.shape(img)[1]\r\n\r\n    def pad_h():\r\n        img_pad_h = tf.ones([width - height, width, 3]) * \\\r\n            tf.reduce_mean(img, axis=[0, 1], keepdims=True)\r\n        return tf.concat([img, img_pad_h], axis=0)\r\n\r\n    def pad_w():\r\n        img_pad_w = tf.ones([height, height - width, 3]) * \\\r\n            tf.reduce_mean(img, axis=[0, 1], keepdims=True)\r\n        return tf.concat([img, img_pad_w], axis=1)\r\n\r\n    img = tf.case([(tf.greater(height, width), pad_w),\r\n                   (tf.less(height, width), pad_h)], default=lambda: img)\r\n\r\n    return img\r\n\r\n\r\ndef _resize(img, labels, img_dim):\r\n    w_f = tf.cast(tf.shape(img)[1], tf.float32)\r\n    h_f = tf.cast(tf.shape(img)[0], tf.float32)\r\n    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\r\n                     labels[:, 2] / w_f,  labels[:, 3] / h_f,\r\n                     labels[:, 4] / w_f,  labels[:, 5] / h_f,\r\n                     labels[:, 6] / w_f,  labels[:, 7] / h_f,\r\n                     labels[:, 8] / w_f,  labels[:, 9] / h_f,\r\n                     labels[:, 10] / w_f, labels[:, 11] / h_f,\r\n                     labels[:, 12] / w_f, labels[:, 13] / h_f], axis=1)\r\n    locs = tf.clip_by_value(locs, 0, 1)\r\n    labels = tf.concat([locs, labels[:, 14][:, tf.newaxis]], axis=1)\r\n\r\n    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\r\n\r\n    def resize(method):\r\n        def _resize():\r\n            return tf.image.resize(\r\n                img, [img_dim, img_dim], method=method, antialias=True)\r\n        return _resize\r\n\r\n    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\r\n                   (tf.equal(resize_case, 1), resize('area')),\r\n                   (tf.equal(resize_case, 2), resize('nearest')),\r\n                   (tf.equal(resize_case, 3), resize('lanczos3'))],\r\n                  default=resize('bilinear'))\r\n\r\n    return img, labels\r\n\r\n\r\ndef _distort(img):\r\n    img = tf.image.random_brightness(img, 0.4)\r\n    img = tf.image.random_contrast(img, 0.5, 1.5)\r\n    img = tf.image.random_saturation(img, 0.5, 1.5)\r\n    img = tf.image.random_hue(img, 0.1)\r\n\r\n    return img\r\n\r\ndef load_dataset(cfg, priors, shuffle=True, buffer_size=10240):\r\n    \"\"\"load dataset\"\"\"\r\n    logging.info(\"load dataset from {}\".format(cfg['dataset_path']))\r\n    dataset = load_tfrecord_dataset(\r\n        tfrecord_name=cfg['dataset_path'],\r\n        batch_size=cfg['batch_size'],\r\n        img_dim=cfg['input_size'],\r\n        using_bin=cfg['using_bin'],\r\n        using_flip=cfg['using_flip'],\r\n        using_distort=cfg['using_distort'],\r\n        using_encoding=True,\r\n        priors=priors,\r\n        match_thresh=cfg['match_thresh'],\r\n        ignore_thresh=cfg['ignore_thresh'],\r\n        variances=cfg['variances'],\r\n        shuffle=shuffle,\r\n        buffer_size=buffer_size)\r\n    return dataset\r\n\r\ndef set_memory_growth():\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    if gpus:\r\n        try:\r\n            # Currently, memory growth needs to be the same across GPUs\r\n            for gpu in gpus:\r\n                tf.config.experimental.set_memory_growth(gpu, True)\r\n                logical_gpus = tf.config.experimental.list_logical_devices(\r\n                    'GPU')\r\n                logging.info(\r\n                    \"Detect {} Physical GPUs, {} Logical GPUs.\".format(\r\n                        len(gpus), len(logical_gpus)))\r\n        except RuntimeError as e:\r\n            # Memory growth must be set before GPUs have been initialized\r\n            logging.info(e)\r\n\r\ndef prior_box(image_sizes, min_sizes, steps, clip=False):\r\n    \"\"\"prior box\"\"\"\r\n    feature_maps = [\r\n        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\r\n        for step in steps]\r\n\r\n    anchors = []\r\n    for k, f in enumerate(feature_maps):\r\n        for i, j in product(range(f[0]), range(f[1])):\r\n            for min_size in min_sizes[k]:\r\n                s_kx = min_size / image_sizes[1]\r\n                s_ky = min_size / image_sizes[0]\r\n                cx = (j + 0.5) * steps[k] / image_sizes[1]\r\n                cy = (i + 0.5) * steps[k] / image_sizes[0]\r\n                anchors += [cx, cy, s_kx, s_ky]\r\n\r\n    output = np.asarray(anchors).reshape([-1, 4])\r\n\r\n    if clip:\r\n        output = np.clip(output, 0, 1)\r\n\r\n    return output\r\n\r\ncfg = {\r\n    # general setting\r\n    \"batch_size\": 8,\r\n    \"input_size\": 640,  # (h,w)\r\n\r\n    \"backbone_type\": 'ResNet50',  # 'ResNet50', 'MobileNetV2'\r\n    \"sub_name\": 'retinaface_res50',\r\n\r\n    # training dataset\r\n    \"dataset_path\": '/mnt/retinaface-tf2/data/widerface_test_bin.tfrecord',  # 'dataset/trainval_mask.tfrecord'\r\n    \"testing_dataset_path\": './data/widerface/val',  #\r\n    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\r\n    \"val_len\": 1839,\r\n    \"using_crop\": True,\r\n    \"using_bin\": True,\r\n    \"using_flip\": True,\r\n    \"using_distort\": True,\r\n    \"using_normalizing\": True,\r\n    \"labels_list\": ['background', 'mask', 'unmask'],  # xml annotation\r\n\r\n    # anchor setting\r\n    # \"min_sizes\": [[(9, 7), (24, 20), (39, 35)], [(54, 41), (65, 61), (81, 66)],\r\n    #               [(94, 86), (113, 95), (131, 122)], [(137, 128), (172, 162), (176, 210)]],\r\n    \"min_sizes\": [[16, 32], [64, 128], [256, 512]],\r\n    \"steps\": [8, 16, 32],\r\n    \"match_thresh\": 0.45,\r\n    \"ignore_thresh\": 0.3,\r\n    \"variances\": [0.1, 0.2],\r\n    \"clip\": False,\r\n\r\n    # network\r\n    \"out_channel\": 256,\r\n\r\n    # training setting\r\n    \"resume\": False,  # if False,training from scratch\r\n    \"epoch\": 100,\r\n    \"init_lr\": 1e-2,\r\n    \"lr_decay_epoch\": [50, 68],\r\n    \"lr_rate\": 0.1,\r\n    \"warmup_epoch\": 5,\r\n    \"min_lr\": 1e-3,\r\n    \"pretrain\": True,\r\n\r\n    \"save_steps\": 2000,\r\n\r\n    \"weights_decay\": 5e-4,\r\n    \"momentum\": 0.9,\r\n    \"save_freq\": 1, #frequency of save model weights\r\n\r\n    # inference\r\n    \"score_threshold\": 0.5,\r\n    \"nms_threshold\": 0.4,\r\n    \"max_number_keep\": 200\r\n}\r\n\r\ndef main(_):\r\n\r\n    multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    # init\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n\r\n    logger = tf.get_logger()\r\n    logger.disabled = True\r\n    set_memory_growth()\r\n    priors = prior_box((cfg['input_size'], cfg['input_size']),\r\n                           cfg['min_sizes'],  cfg['steps'], cfg['clip'])\r\n    dataset = load_dataset(cfg, priors, shuffle=True)\r\n    train_iter = iter(dataset)\r\n    print(next(train_iter))\r\n    dist_dataset = multiworker_strategy.experimental_distribute_dataset(dataset)\r\n    dist_train_iter = iter(dist_dataset)\r\n    print(next(dist_train_iter))\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(main)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```bash\r\n2020-09-08 10:19:16.853430: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-08 10:19:18.227937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-08 10:19:18.254671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:82:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-08 10:19:18.254730: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-08 10:19:18.257145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-08 10:19:18.259279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-08 10:19:18.259620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-08 10:19:18.262046: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-08 10:19:18.263487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-08 10:19:18.268749: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-08 10:19:18.271254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-08 10:19:18.271693: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-08 10:19:18.283629: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199875000 Hz\r\n2020-09-08 10:19:18.286517: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x597c600 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-08 10:19:18.286539: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-08 10:19:19.151667: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59e8480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-08 10:19:19.151723: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-09-08 10:19:19.154118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:82:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-08 10:19:19.154177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-08 10:19:19.154212: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-08 10:19:19.154246: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-08 10:19:19.154274: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-08 10:19:19.154302: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-08 10:19:19.154328: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-08 10:19:19.154352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-08 10:19:19.158471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-08 10:19:19.158536: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-08 10:19:19.793164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-08 10:19:19.793223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-08 10:19:19.793233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-08 10:19:19.795000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\r\n2020-09-08 10:19:19.800484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:82:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-08 10:19:19.800568: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-08 10:19:19.800618: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-08 10:19:19.800650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-08 10:19:19.800681: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-08 10:19:19.800711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-08 10:19:19.800741: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-08 10:19:19.800768: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-08 10:19:19.803844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-08 10:19:19.803916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-08 10:19:19.803935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-08 10:19:19.803953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-08 10:19:19.806763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)\r\n2020-09-08 10:19:19.814394: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> dist-data-worker-0.t9k-sample.svc:2222, 1 -> dist-data-worker-1.t9k-sample.svc:2222, 2 -> dist-data-worker-2.t9k-sample.svc:2222}\r\n2020-09-08 10:19:19.816567: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://dist-data-worker-0.t9k-sample.svc:2222\r\nINFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0', '/job:worker/replica:0/task:0/device:XLA_GPU:0', '/job:worker/replica:0/task:0/device:GPU:0']\r\nI0908 10:19:19.817300 139928804230976 collective_all_reduce_strategy.py:329] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0', '/job:worker/replica:0/task:0/device:XLA_GPU:0', '/job:worker/replica:0/task:0/device:GPU:0']\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0/device:GPU:0',)\r\nI0908 10:19:19.818905 139928804230976 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:0/device:GPU:0',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['dist-data-worker-0.t9k-sample.svc:2222', 'dist-data-worker-1.t9k-sample.svc:2222', 'dist-data-worker-2.t9k-sample.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nI0908 10:19:19.819303 139928804230976 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['dist-data-worker-0.t9k-sample.svc:2222', 'dist-data-worker-1.t9k-sample.svc:2222', 'dist-data-worker-2.t9k-sample.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nI0908 10:19:19.819926 139928804230976 dist_data.py:600] Physical devices cannot be modified after being initialized\r\nI0908 10:19:19.850645 139928804230976 dist_data.py:569] load dataset from /mnt/retinaface-tf2/data/widerface_test_bin.tfrecord\r\n(<tf.Tensor: shape=(8, 640, 640, 3), dtype=float32, numpy=\r\narray([[[[ 1.24728775e+02,  1.22055222e+02,  1.15999634e+02],\r\n         [ 1.22892578e+02,  1.20219025e+02,  1.14163445e+02],\r\n         [ 1.22099586e+02,  1.19426033e+02,  1.13370453e+02],\r\n         ...,\r\n         [ 8.73357391e+01,  8.56875610e+01,  7.93239059e+01],\r\n         [ 8.93529053e+01,  8.77047119e+01,  8.13410645e+01],\r\n         [ 9.24133301e+01,  9.07651443e+01,  8.44014969e+01]],\r\n\r\n        [[ 1.09426826e+02,  1.06753273e+02,  1.00697693e+02],\r\n         [ 1.08657150e+02,  1.05983597e+02,  9.99280167e+01],\r\n         [ 1.08476242e+02,  1.05802689e+02,  9.97471085e+01],\r\n         ...,\r\n         [ 9.34566040e+01,  9.18084183e+01,  8.54447632e+01],\r\n         [ 9.55525665e+01,  9.39043808e+01,  8.75407257e+01],\r\n         [ 9.91462860e+01,  9.74981003e+01,  9.11344452e+01]],\r\n\r\n        [[ 9.72546539e+01,  9.45811005e+01,  8.85255203e+01],\r\n         [ 9.70970535e+01,  9.44235001e+01,  8.83679199e+01],\r\n         [ 9.78483429e+01,  9.51747971e+01,  8.91192169e+01],\r\n         ...,\r\n         [ 9.82655640e+01,  9.66173859e+01,  9.02537308e+01],\r\n         [ 9.94642181e+01,  9.78160324e+01,  9.14523773e+01],\r\n         [ 1.02227783e+02,  1.00579597e+02,  9.42159424e+01]],\r\n\r\n        ...,\r\n\r\n        [[ 1.03908783e+02,  1.07768463e+02,  7.75826416e+01],\r\n         [ 1.02116661e+02,  1.05932289e+02,  7.63713684e+01],\r\n         [ 9.66357956e+01,  1.00444916e+02,  7.09763565e+01],\r\n         ...,\r\n         [ 9.68215866e+01,  9.82557907e+01,  7.19535065e+01],\r\n         [ 1.00859833e+02,  1.02294022e+02,  7.59917374e+01],\r\n         [ 9.64176331e+01,  9.78518372e+01,  7.15495605e+01]],\r\n\r\n        [[ 1.06085648e+02,  1.09945328e+02,  7.97595062e+01],\r\n         [ 1.04293526e+02,  1.08109146e+02,  7.85482330e+01],\r\n         [ 9.88126526e+01,  1.02621773e+02,  7.31532135e+01],\r\n         ...,\r\n         [ 9.66639557e+01,  9.80981598e+01,  7.17958755e+01],\r\n         [ 1.00690552e+02,  1.02124748e+02,  7.58224716e+01],\r\n         [ 9.57151184e+01,  9.71493149e+01,  7.08470383e+01]],\r\n\r\n        [[ 1.06697701e+02,  1.10557381e+02,  8.03715591e+01],\r\n         [ 1.04905586e+02,  1.08721207e+02,  7.91602936e+01],\r\n         [ 9.94247055e+01,  1.03233826e+02,  7.37652664e+01],\r\n         ...,\r\n         [ 9.55974274e+01,  9.70316315e+01,  7.07293549e+01],\r\n         [ 9.99996796e+01,  1.01433876e+02,  7.51315994e+01],\r\n         [ 9.44909668e+01,  9.59251709e+01,  6.96228943e+01]]],\r\n\r\n\r\n       [[[ 1.79020119e+01,  2.19689369e+01,  2.99244499e+00],\r\n         [ 1.58624716e+01,  2.06704369e+01,  0.00000000e+00],\r\n         [ 1.51764307e+01,  1.90834465e+01,  0.00000000e+00],\r\n         ...,\r\n         [ 8.16278076e+01,  9.86631927e+01,  6.32296028e+01],\r\n         [ 6.73102951e+01,  8.42869415e+01,  4.91317062e+01],\r\n         [ 6.12751694e+01,  7.79278717e+01,  4.47397270e+01]],\r\n\r\n        [[ 1.95028820e+01,  2.17408943e+01,  7.81370163e-01],\r\n         [ 1.96296520e+01,  2.13537941e+01,  0.00000000e+00],\r\n         [ 2.09664173e+01,  2.15310116e+01,  0.00000000e+00],\r\n         ...,\r\n         [ 8.06434631e+01,  9.79421234e+01,  6.09631004e+01],\r\n         [ 5.99489441e+01,  7.71724243e+01,  4.05182495e+01],\r\n         [ 5.25240326e+01,  6.93798370e+01,  3.49581909e+01]],\r\n\r\n        [[ 2.45029068e+01,  2.24879875e+01,  0.00000000e+00],\r\n         [ 2.51510925e+01,  2.26621895e+01,  0.00000000e+00],\r\n         [ 2.70248432e+01,  2.31384068e+01,  0.00000000e+00],\r\n         ...,\r\n         [ 7.55968781e+01,  9.31968842e+01,  5.44395905e+01],\r\n         [ 4.63827667e+01,  6.39156876e+01,  2.53827095e+01],\r\n         [ 3.69936066e+01,  5.42851677e+01,  1.72176933e+01]],\r\n\r\n        ...,\r\n\r\n        [[-1.93025589e+00, -1.93025589e+00, -1.93025589e+00],\r\n         [-2.22468567e+00, -2.22468567e+00, -2.22468567e+00],\r\n         [-3.08513641e+00, -3.08513641e+00, -3.08513641e+00],\r\n         ...,\r\n         [ 7.91158600e+01,  9.09350739e+01,  3.32669144e+01],\r\n         [ 7.68767090e+01,  8.71371689e+01,  2.85244370e+01],\r\n         [ 7.47488556e+01,  8.26550827e+01,  2.33686600e+01]],\r\n\r\n        [[-6.81667328e-01, -6.81667328e-01, -6.81667328e-01],\r\n         [-1.40288925e+00, -1.40288925e+00, -1.40288925e+00],\r\n         [-2.36134720e+00, -2.36134720e+00, -2.36134720e+00],\r\n         ...,\r\n         [ 8.02405701e+01,  9.04674072e+01,  3.20591087e+01],\r\n         [ 7.76517258e+01,  8.66896591e+01,  2.68999023e+01],\r\n         [ 7.68033295e+01,  8.36673737e+01,  2.28854980e+01]],\r\n\r\n        [[-1.89395905e-01, -1.89395905e-01, -1.89395905e-01],\r\n         [-7.97077179e-01, -7.97077179e-01, -7.97077179e-01],\r\n         [-1.66807556e+00, -1.66807556e+00, -1.66807556e+00],\r\n         ...,\r\n         [ 8.23338089e+01,  9.02438660e+01,  3.09343338e+01],\r\n         [ 7.98906250e+01,  8.67535095e+01,  2.59786530e+01],\r\n         [ 7.89819565e+01,  8.43535233e+01,  2.31105156e+01]]],\r\n\r\n\r\n       [[[ 9.29103622e+01,  8.38115387e+01,  9.26200790e+01],\r\n         [ 9.21722794e+01,  8.30734787e+01,  9.18819885e+01],\r\n         [ 9.14833374e+01,  8.23845596e+01,  9.11930695e+01],\r\n         ...,\r\n         [ 5.40838509e+01,  4.53541336e+01,  4.25934219e+00],\r\n         [ 5.80340424e+01,  4.97181396e+01,  1.29117203e+00],\r\n         [ 5.84229126e+01,  4.98948250e+01,  0.00000000e+00]],\r\n\r\n        [[ 8.68602219e+01,  7.79978180e+01,  8.60694427e+01],\r\n         [ 8.78025208e+01,  7.89401245e+01,  8.70117950e+01],\r\n         [ 8.75972595e+01,  7.87348328e+01,  8.68065033e+01],\r\n         ...,\r\n         [ 5.68775215e+01,  4.34964104e+01,  3.66986465e+00],\r\n         [ 5.98933830e+01,  4.68066788e+01,  0.00000000e+00],\r\n         [ 6.04419479e+01,  4.84969749e+01,  0.00000000e+00]],\r\n\r\n        [[ 9.39529495e+01,  8.52325211e+01,  9.65406189e+01],\r\n         [ 9.61381149e+01,  8.74176636e+01,  9.87257614e+01],\r\n         [ 9.64990234e+01,  8.77786179e+01,  9.90867004e+01],\r\n         ...,\r\n         [ 5.81260529e+01,  4.53354836e+01,  5.51031113e+00],\r\n         [ 6.04816360e+01,  4.78775520e+01,  8.11939240e-01],\r\n         [ 6.11883049e+01,  4.94758110e+01,  0.00000000e+00]],\r\n\r\n        ...,\r\n\r\n        [[ 1.24503052e+02,  1.62149078e+02,  8.95797729e-01],\r\n         [ 1.24353889e+02,  1.59983643e+02,  0.00000000e+00],\r\n         [ 1.25073418e+02,  1.57816925e+02,  0.00000000e+00],\r\n         ...,\r\n         [ 1.29795639e+02,  1.66753372e+02,  2.55889435e+01],\r\n         [ 1.27766533e+02,  1.65338638e+02,  2.42820740e+01],\r\n         [ 1.29873688e+02,  1.64272003e+02,  2.05228882e+01]],\r\n\r\n        [[ 1.24627296e+02,  1.61203949e+02,  0.00000000e+00],\r\n         [ 1.24903076e+02,  1.59984085e+02,  0.00000000e+00],\r\n         [ 1.24675163e+02,  1.57494263e+02,  0.00000000e+00],\r\n         ...,\r\n         [ 1.32041168e+02,  1.68460938e+02,  2.15579376e+01],\r\n         [ 1.33513550e+02,  1.69203796e+02,  2.11235657e+01],\r\n         [ 1.32195358e+02,  1.66914062e+02,  1.70703583e+01]],\r\n\r\n        [[ 1.28943268e+02,  1.65691986e+02,  0.00000000e+00],\r\n         [ 1.28879425e+02,  1.64225494e+02,  1.00944519e+00],\r\n         [ 1.29246002e+02,  1.62467010e+02,  3.57754517e+00],\r\n         ...,\r\n         [ 1.34914566e+02,  1.71236755e+02,  1.62087860e+01],\r\n         [ 1.34592255e+02,  1.68047272e+02,  1.38260651e+01],\r\n         [ 1.31932831e+02,  1.66052216e+02,  8.41160583e+00]]],\r\n\r\n\r\n       ...,\r\n\r\n\r\n       [[[ 7.57140045e+01,  1.32128372e+02,  1.51906891e+02],\r\n         [ 7.55561981e+01,  1.31970551e+02,  1.51749100e+02],\r\n         [ 7.58803101e+01,  1.32294662e+02,  1.52073212e+02],\r\n         ...,\r\n         [ 7.63204269e+01,  1.38002457e+02,  1.55694473e+02],\r\n         [ 7.63204346e+01,  1.38002441e+02,  1.55694458e+02],\r\n         [ 7.63204346e+01,  1.38002441e+02,  1.55694458e+02]],\r\n\r\n        [[ 7.60969543e+01,  1.32511337e+02,  1.52289871e+02],\r\n         [ 7.55828400e+01,  1.31997223e+02,  1.51775757e+02],\r\n         [ 7.59586639e+01,  1.32373047e+02,  1.52151581e+02],\r\n         ...,\r\n         [ 7.68187943e+01,  1.38500839e+02,  1.56192856e+02],\r\n         [ 7.68188095e+01,  1.38500854e+02,  1.56192841e+02],\r\n         [ 7.68188095e+01,  1.38500854e+02,  1.56192841e+02]],\r\n\r\n        [[ 7.60515213e+01,  1.32465881e+02,  1.52244431e+02],\r\n         [ 7.59733200e+01,  1.32387695e+02,  1.52166214e+02],\r\n         [ 7.63387985e+01,  1.32753159e+02,  1.52531708e+02],\r\n         ...,\r\n         [ 7.67181396e+01,  1.38400146e+02,  1.56092163e+02],\r\n         [ 7.67181244e+01,  1.38400162e+02,  1.56092178e+02],\r\n         [ 7.67181320e+01,  1.38400177e+02,  1.56092163e+02]],\r\n\r\n        ...,\r\n\r\n        [[ 9.98160553e+01,  1.20287552e+02,  3.95508881e+01],\r\n         [ 1.03795059e+02,  1.25791283e+02,  4.36515121e+01],\r\n         [ 1.05114639e+02,  1.28117142e+02,  4.40065994e+01],\r\n         ...,\r\n         [ 1.06390022e+02,  1.25751274e+02,  4.62075577e+01],\r\n         [ 1.05510178e+02,  1.24841278e+02,  4.33839493e+01],\r\n         [ 9.98391418e+01,  1.19300049e+02,  3.51180725e+01]],\r\n\r\n        [[ 9.31870270e+01,  1.14661629e+02,  3.25725708e+01],\r\n         [ 1.07921799e+02,  1.29681992e+02,  4.67424469e+01],\r\n         [ 1.07605896e+02,  1.30248184e+02,  4.56777954e+01],\r\n         ...,\r\n         [ 1.00495163e+02,  1.19859558e+02,  4.13804169e+01],\r\n         [ 9.99863281e+01,  1.19170776e+02,  3.83058929e+01],\r\n         [ 9.57622681e+01,  1.15001633e+02,  3.31129532e+01]],\r\n\r\n        [[ 1.01265877e+02,  1.22496277e+02,  4.03028183e+01],\r\n         [ 1.12068802e+02,  1.33676376e+02,  5.00867157e+01],\r\n         [ 1.08309929e+02,  1.31249863e+02,  4.57988663e+01],\r\n         ...,\r\n         [ 9.07576447e+01,  1.09978981e+02,  3.21097488e+01],\r\n         [ 8.99604492e+01,  1.09427834e+02,  2.96014786e+01],\r\n         [ 9.18549347e+01,  1.10977539e+02,  3.00187531e+01]]],\r\n\r\n\r\n       [[[ 1.91680695e+02,  2.38228241e+02,  2.35225067e+02],\r\n         [ 1.92582962e+02,  2.39130493e+02,  2.36127335e+02],\r\n         [ 1.93529877e+02,  2.40061951e+02,  2.37057846e+02],\r\n         ...,\r\n         [ 1.72848755e+02,  1.60696655e+02,  1.06524742e+02],\r\n         [ 1.72897552e+02,  1.60745483e+02,  1.06573517e+02],\r\n         [ 1.72923737e+02,  1.60771683e+02,  1.06599754e+02]],\r\n\r\n        [[ 1.90356064e+02,  2.37042603e+02,  2.34052338e+02],\r\n         [ 1.91232483e+02,  2.37919006e+02,  2.34928741e+02],\r\n         [ 1.92235733e+02,  2.38913666e+02,  2.35919846e+02],\r\n         ...,\r\n         [ 1.73280212e+02,  1.61128143e+02,  1.06956184e+02],\r\n         [ 1.72987717e+02,  1.60835648e+02,  1.06663681e+02],\r\n         [ 1.72798050e+02,  1.60645981e+02,  1.06474014e+02]],\r\n\r\n        [[ 1.88877411e+02,  2.35250122e+02,  2.32223495e+02],\r\n         [ 1.89737762e+02,  2.36111267e+02,  2.33084335e+02],\r\n         [ 1.90798828e+02,  2.37171631e+02,  2.34137512e+02],\r\n         ...,\r\n         [ 1.73966415e+02,  1.61814331e+02,  1.07642403e+02],\r\n         [ 1.73338882e+02,  1.61186813e+02,  1.07014870e+02],\r\n         [ 1.72980103e+02,  1.60828033e+02,  1.06656113e+02]],\r\n\r\n        ...,\r\n\r\n        [[ 1.62996979e+01,  9.67771759e+01,  1.01026886e+02],\r\n         [ 1.63509140e+01,  9.67883759e+01,  1.01029572e+02],\r\n         [ 1.63950500e+01,  9.67980347e+01,  1.01031891e+02],\r\n         ...,\r\n         [ 1.03893127e+01,  1.00420944e+02,  1.02773575e+02],\r\n         [ 1.03893280e+01,  1.00420883e+02,  1.02773560e+02],\r\n         [ 1.03893280e+01,  1.00420883e+02,  1.02773560e+02]],\r\n\r\n        [[ 1.56857376e+01,  9.51733170e+01,  1.01032059e+02],\r\n         [ 1.56471558e+01,  9.51697311e+01,  1.01032059e+02],\r\n         [ 1.56139221e+01,  9.51666565e+01,  1.01032059e+02],\r\n         ...,\r\n         [ 1.06467667e+01,  9.76320877e+01,  1.02251343e+02],\r\n         [ 1.06467667e+01,  9.76320877e+01,  1.02251343e+02],\r\n         [ 1.06467667e+01,  9.76320877e+01,  1.02251343e+02]],\r\n\r\n        [[ 1.49750595e+01,  9.38345718e+01,  1.01032059e+02],\r\n         [ 1.49750671e+01,  9.38345947e+01,  1.01032059e+02],\r\n         [ 1.49750671e+01,  9.38345947e+01,  1.01032059e+02],\r\n         ...,\r\n         [ 1.08568802e+01,  9.53557205e+01,  1.01825073e+02],\r\n         [ 1.08568802e+01,  9.53557205e+01,  1.01825073e+02],\r\n         [ 1.08568802e+01,  9.53557205e+01,  1.01825073e+02]]],\r\n\r\n\r\n       [[[ 7.83921051e+01,  1.24869156e+02,  7.64564514e+01],\r\n         [ 7.83407288e+01,  1.22990463e+02,  7.69286194e+01],\r\n         [ 9.65125351e+01,  1.37986038e+02,  9.52618561e+01],\r\n         ...,\r\n         [ 1.18475189e+02,  1.53988434e+02,  1.06111961e+02],\r\n         [ 1.15520340e+02,  1.49667419e+02,  1.04680801e+02],\r\n         [ 1.05429680e+02,  1.39591858e+02,  9.70172577e+01]],\r\n\r\n        [[ 8.45359344e+01,  1.31023071e+02,  8.38709183e+01],\r\n         [ 8.28325424e+01,  1.27507378e+02,  8.16269608e+01],\r\n         [ 9.68063507e+01,  1.38501221e+02,  9.59045181e+01],\r\n         ...,\r\n         [ 1.09455978e+02,  1.43719650e+02,  9.77291107e+01],\r\n         [ 1.13780060e+02,  1.46499756e+02,  1.03052628e+02],\r\n         [ 1.12625420e+02,  1.45448212e+02,  1.03386864e+02]],\r\n\r\n        [[ 9.08953476e+01,  1.36272064e+02,  9.05691833e+01],\r\n         [ 8.86048050e+01,  1.32403290e+02,  8.71840515e+01],\r\n         [ 1.01192871e+02,  1.42970306e+02,  1.00444214e+02],\r\n         ...,\r\n         [ 1.06194847e+02,  1.38833054e+02,  9.51865082e+01],\r\n         [ 1.17479927e+02,  1.49704147e+02,  1.07861160e+02],\r\n         [ 1.11618668e+02,  1.43836151e+02,  1.02608093e+02]],\r\n\r\n        ...,\r\n\r\n        [[ 4.97378540e+01,  6.34630661e+01,  3.84943619e+01],\r\n         [ 4.56276321e+01,  5.83987885e+01,  3.39005966e+01],\r\n         [ 4.70007172e+01,  5.92259178e+01,  3.53177795e+01],\r\n         ...,\r\n         [ 1.30571686e+02,  1.43362213e+02,  1.07660027e+02],\r\n         [ 1.26607735e+02,  1.35462082e+02,  1.00648926e+02],\r\n         [ 1.20941162e+02,  1.26461502e+02,  9.25070038e+01]],\r\n\r\n        [[ 5.56412773e+01,  6.93217773e+01,  4.46427994e+01],\r\n         [ 5.28044205e+01,  6.67666626e+01,  4.17459335e+01],\r\n         [ 5.03333588e+01,  6.32095261e+01,  4.01224670e+01],\r\n         ...,\r\n         [ 1.30266159e+02,  1.45737137e+02,  1.06612762e+02],\r\n         [ 1.25733429e+02,  1.37645782e+02,  1.00882072e+02],\r\n         [ 1.19405090e+02,  1.28468903e+02,  9.46394958e+01]],\r\n\r\n        [[ 6.66966553e+01,  8.03032684e+01,  5.69682922e+01],\r\n         [ 6.34833031e+01,  7.71879883e+01,  5.36312714e+01],\r\n         [ 5.69458885e+01,  6.96463470e+01,  4.78098984e+01],\r\n         ...,\r\n         [ 1.32660233e+02,  1.49680664e+02,  1.07838509e+02],\r\n         [ 1.26706856e+02,  1.41510147e+02,  1.03348465e+02],\r\n         [ 1.19581390e+02,  1.32401215e+02,  9.78807983e+01]]]],\r\n      dtype=float32)>, <tf.Tensor: shape=(8, 16800, 16), dtype=float32, numpy=\r\narray([[[  5.612871  ,  68.04673   ,  -5.210217  , ...,  -2.4999998 ,\r\n           0.        ,   0.        ],\r\n        [  2.8064356 ,  34.023365  ,  -8.675953  , ...,  -1.2499999 ,\r\n           0.        ,   0.        ],\r\n        [  0.6128713 ,  68.04673   ,  -5.210217  , ...,  -2.4999998 ,\r\n           0.        ,   0.        ],\r\n        ...,\r\n        [-11.308971  ,  -9.982915  , -22.538898  , ..., -12.187499  ,\r\n           0.        ,   0.        ],\r\n        [-23.867945  , -19.96583   , -19.07316   , ..., -24.374998  ,\r\n           0.        ,   0.        ],\r\n        [-11.933972  ,  -9.982915  , -22.538898  , ..., -12.187499  ,\r\n           0.        ,   0.        ]],\r\n\r\n       [[ 26.56724   ,  30.905638  ,   8.800869  , ...,  55.820175  ,\r\n           1.        ,   0.        ],\r\n        [ 13.28362   ,  15.452819  ,   5.3351336 , ...,  27.910088  ,\r\n           1.        ,   0.        ],\r\n        [ 21.56724   ,  30.905638  ,   8.800869  , ...,  55.820175  ,\r\n           1.        ,   0.        ],\r\n        ...,\r\n        [-10.654148  , -11.143574  ,  -8.52781   , ..., -10.364994  ,\r\n           1.        ,   0.        ],\r\n        [-22.558296  , -22.287148  ,  -5.062074  , ..., -20.729988  ,\r\n           1.        ,   0.        ],\r\n        [-11.279148  , -11.143574  ,  -8.52781   , ..., -10.364994  ,\r\n           1.        ,   0.        ]],\r\n\r\n       [[ 38.281437  ,  85.41209   ,   2.6807168 , ...,  -2.4999998 ,\r\n           0.        ,   0.        ],\r\n        [ 19.140718  ,  42.706043  ,  -0.78501904, ...,  -1.2499999 ,\r\n           0.        ,   0.        ],\r\n        [ 33.281437  ,  85.41209   ,   2.6807168 , ...,  -2.4999998 ,\r\n           0.        ,   0.        ],\r\n        ...,\r\n        [-10.288079  ,  -9.440247  , -14.647963  , ..., -12.187499  ,\r\n           0.        ,   0.        ],\r\n        [-21.826159  , -18.880493  , -11.182227  , ..., -24.374998  ,\r\n           0.        ,   0.        ],\r\n        [-10.913079  ,  -9.440247  , -14.647963  , ..., -12.187499  ,\r\n           0.        ,   0.        ]],\r\n\r\n       ...,\r\n\r\n       [[337.60413   , 152.18748   ,  -0.9116125 , ..., 155.5406    ,\r\n           1.        ,   0.        ],\r\n        [168.80206   ,  76.09374   ,  -4.3773484 , ...,  77.7703    ,\r\n           1.        ,   0.        ],\r\n        [332.60413   , 152.18748   ,  -0.9116125 , ..., 155.5406    ,\r\n           1.        ,   0.        ],\r\n        ...,\r\n        [ -0.93424535,  -7.3535156 , -18.240292  , ...,  -7.2487307 ,\r\n           1.        ,   0.        ],\r\n        [ -3.118491  , -14.707031  , -14.774556  , ..., -14.497461  ,\r\n           1.        ,   0.        ],\r\n        [ -1.5592455 ,  -7.3535156 , -18.240292  , ...,  -7.2487307 ,\r\n           1.        ,   0.        ]],\r\n\r\n       [[298.7106    , 101.13196   ,  14.91776   , ..., 126.88885   ,\r\n           1.        ,   0.        ],\r\n        [149.3553    ,  50.56598   ,  11.4520235 , ...,  63.444424  ,\r\n           1.        ,   0.        ],\r\n        [293.7106    , 101.13196   ,  14.91776   , ..., 126.88885   ,\r\n           1.        ,   0.        ],\r\n        ...,\r\n        [ -2.1496675 ,  -8.949     ,  -2.4109201 , ...,  -8.144098  ,\r\n           1.        ,   0.        ],\r\n        [ -5.549335  , -17.898     ,   1.0548158 , ..., -16.288197  ,\r\n           1.        ,   0.        ],\r\n        [ -2.7746675 ,  -8.949     ,  -2.4109201 , ...,  -8.144098  ,\r\n           1.        ,   0.        ]],\r\n\r\n       [[193.08823   , 121.7647    ,  10.969757  , ..., 152.64705   ,\r\n           1.        ,   0.        ],\r\n        [ 96.54411   ,  60.88235   ,   7.5040207 , ...,  76.323524  ,\r\n           1.        ,   0.        ],\r\n        [188.08821   , 121.7647    ,  10.969757  , ..., 152.64705   ,\r\n           1.        ,   0.        ],\r\n        ...,\r\n        [ -5.4503675 ,  -8.304227  ,  -6.3589225 , ...,  -7.339154  ,\r\n           1.        ,   0.        ],\r\n        [-12.150735  , -16.608454  ,  -2.8931866 , ..., -14.678308  ,\r\n           1.        ,   0.        ],\r\n        [ -6.0753675 ,  -8.304227  ,  -6.3589225 , ...,  -7.339154  ,\r\n           1.        ,   0.        ]]], dtype=float32)>)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 2102, in execution_mode\r\n    yield\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 758, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2610, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 649, in __next__\r\n    return self.get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 721, in get_next\r\n    strict=True,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1210, in cond\r\n    result = false_fn()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 720, in <lambda>\r\n    lambda: out_of_range_fn(i, device),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 702, in out_of_range_fn\r\n    data = self._iterators[worker_index].get_next(device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1457, in get_next\r\n    return self._iterator.get_next(device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 576, in get_next\r\n    return self._device_iterators[index].get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 825, in get_next\r\n    return self._next_internal()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 764, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 2105, in execution_mode\r\n    executor_new.wait()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py\", line 67, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/retinaface-tf2/dist_data.py\", line 701, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/mnt/retinaface-tf2/dist_data.py\", line 697, in main\r\n    print(next(dist_train_iter))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 651, in __next__\r\n    raise StopIteration\r\nStopIteration\r\n2020-09-08 10:19:30.991376: W tensorflow/core/common_runtime/eager/context.cc:566] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n\r\n```\r\n", "comments": ["Hi @wulikai1993, this is a lot of custom code. Can you please trim it down and provide a minimal reproduction? Looks like the majority of the code here is not relevant to the specific issue you're facing.\r\n\r\nFrom the error message, sounds to me like the problem here is not having enough data in each batch to distribute evenly across your replicas. Can you try running this in nightly and let me know if the error persists? You can also try making your batch size evenly divisible by multiworker_strategy.num_replicas_in_sync. ", "@nikitamaia The custom code is mostly used in Dataset.map. In the main function, I succeed in printing the original Dataset's elements, which means the custom code is working correctly in the original Dataset. And you can see the element's shape is (8, 640, 640, 3), which means the batch size is 8. I used 3 workers with 1 gpu on each, so the elements should be enough for each worker.\r\n\r\nThen I just wrap the original Dataset as following\r\n```bash\r\ndist_dataset = multiworker_strategy.experimental_distribute_dataset(dataset)\r\n```\r\n\r\nIf normally, each worker should print the tensor with 2 or 3 batch sizes. The strange thing I want to say is the wrapped `DistributedDataset` seems like to be empty. Because the `End of sequence` error demonstrates there is nothing in it.", "I can try and reproduce this on my end if you provide minimal reproducible code. Having minimal code helps a lot in expediting the troubleshooting process. Thanks!", "@nikitamaia I try my best to trim down redundant code. The reserved code is used to parse the tfrecord file, mainly to crop images so that they can be batched together. In my opinion, now that I can iterate `Dataset` successfully, I can iterate `DistributedDataset` easily. But the fact is not so, that's the most confusing part for me. Thank you for your patience!\r\n\r\n```bash\r\nimport os\r\nimport tensorflow as tf\r\nfrom absl import app, flags, logging\r\n\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom itertools import product as product\r\n\r\n\r\n###############################################################################\r\n#   Tensorflow / Numpy Priors                                                 #\r\n###############################################################################\r\ndef prior_box(image_sizes, min_sizes, steps, clip=False):\r\n    \"\"\"prior box\"\"\"\r\n    feature_maps = [\r\n        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\r\n        for step in steps]\r\n\r\n    anchors = []\r\n    for k, f in enumerate(feature_maps):\r\n        for i, j in product(range(f[0]), range(f[1])):\r\n            for min_size in min_sizes[k]:\r\n                s_kx = min_size / image_sizes[1]\r\n                s_ky = min_size / image_sizes[0]\r\n                cx = (j + 0.5) * steps[k] / image_sizes[1]\r\n                cy = (i + 0.5) * steps[k] / image_sizes[0]\r\n                anchors += [cx, cy, s_kx, s_ky]\r\n\r\n    output = np.asarray(anchors).reshape([-1, 4])\r\n\r\n    if clip:\r\n        output = np.clip(output, 0, 1)\r\n\r\n    return output\r\n\r\n\r\n###############################################################################\r\n#   Tensorflow Encoding                                                       #\r\n###############################################################################\r\ndef encode_tf(labels, priors, match_thresh, ignore_thresh,\r\n              variances=[0.1, 0.2]):\r\n    \"\"\"tensorflow encoding\"\"\"\r\n    assert ignore_thresh <= match_thresh\r\n    priors = tf.cast(priors, tf.float32)\r\n    bbox = labels[:, :4]\r\n    landm = labels[:, 4:-1]\r\n    landm_valid = labels[:, -1]  # 1: with landm, 0: w/o landm.\r\n\r\n    # jaccard index\r\n    overlaps = _jaccard(bbox, _point_form(priors))\r\n\r\n    # (Bipartite Matching)\r\n    # [num_objects] best prior for each ground truth\r\n    best_prior_overlap, best_prior_idx = tf.math.top_k(overlaps, k=1)\r\n    best_prior_overlap = best_prior_overlap[:, 0]\r\n    best_prior_idx = best_prior_idx[:, 0]\r\n\r\n    # [num_priors] best ground truth for each prior\r\n    overlaps_t = tf.transpose(overlaps)\r\n    best_truth_overlap, best_truth_idx = tf.math.top_k(overlaps_t, k=1)\r\n    best_truth_overlap = best_truth_overlap[:, 0]\r\n    best_truth_idx = best_truth_idx[:, 0]\r\n\r\n    # ensure best prior\r\n    def _loop_body(i, bt_idx, bt_overlap):\r\n        bp_mask = tf.one_hot(best_prior_idx[i], tf.shape(bt_idx)[0])\r\n        bp_mask_int = tf.cast(bp_mask, tf.int32)\r\n        new_bt_idx = bt_idx * (1 - bp_mask_int) + bp_mask_int * i\r\n        bp_mask_float = tf.cast(bp_mask, tf.float32)\r\n        new_bt_overlap = bt_overlap * (1 - bp_mask_float) + bp_mask_float * 2\r\n        return tf.cond(best_prior_overlap[i] > match_thresh,\r\n                       lambda: (i + 1, new_bt_idx, new_bt_overlap),\r\n                       lambda: (i + 1, bt_idx, bt_overlap))\r\n    _, best_truth_idx, best_truth_overlap = tf.while_loop(\r\n        lambda i, bt_idx, bt_overlap: tf.less(i, tf.shape(best_prior_idx)[0]),\r\n        _loop_body, [tf.constant(0), best_truth_idx, best_truth_overlap])\r\n\r\n    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\r\n    matches_landm = tf.gather(landm, best_truth_idx)  # [num_priors, 10]\r\n    matches_landm_v = tf.gather(landm_valid, best_truth_idx)  # [num_priors]\r\n\r\n    loc_t = _encode_bbox(matches_bbox, priors, variances)\r\n    landm_t = _encode_landm(matches_landm, priors, variances)\r\n    landm_valid_t = tf.cast(matches_landm_v > 0, tf.float32)\r\n    conf_t = tf.cast(best_truth_overlap > match_thresh, tf.float32)\r\n    conf_t = tf.where(\r\n        tf.logical_and(best_truth_overlap < match_thresh,\r\n                       best_truth_overlap > ignore_thresh),\r\n        tf.ones_like(conf_t) * -1, conf_t)    # 1: pos, 0: neg, -1: ignore\r\n\r\n    return tf.concat([loc_t, landm_t, landm_valid_t[..., tf.newaxis],\r\n                      conf_t[..., tf.newaxis]], axis=1)\r\n\r\n\r\ndef _encode_bbox(matched, priors, variances):\r\n    \"\"\"Encode the variances from the priorbox layers into the ground truth\r\n    boxes we have matched (based on jaccard overlap) with the prior boxes.\r\n    Args:\r\n        matched: (tensor) Coords of ground truth for each prior in point-form\r\n            Shape: [num_priors, 4].\r\n        priors: (tensor) Prior boxes in center-offset form\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        encoded boxes (tensor), Shape: [num_priors, 4]\r\n    \"\"\"\r\n\r\n    # dist b/t match center and prior's center\r\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\r\n    # encode variance\r\n    g_cxcy /= (variances[0] * priors[:, 2:])\r\n    # match wh / prior wh\r\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\r\n    g_wh = tf.math.log(g_wh) / variances[1]\r\n    # return target for smooth_l1_loss\r\n    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]\r\n\r\n\r\ndef _encode_landm(matched, priors, variances):\r\n    \"\"\"Encode the variances from the priorbox layers into the ground truth\r\n    boxes we have matched (based on jaccard overlap) with the prior boxes.\r\n    Args:\r\n        matched: (tensor) Coords of ground truth for each prior in point-form\r\n            Shape: [num_priors, 10].\r\n        priors: (tensor) Prior boxes in center-offset form\r\n            Shape: [num_priors,4].\r\n        variances: (list[float]) Variances of priorboxes\r\n    Return:\r\n        encoded landm (tensor), Shape: [num_priors, 10]\r\n    \"\"\"\r\n\r\n    # dist b/t match center and prior's center\r\n    matched = tf.reshape(matched, [tf.shape(matched)[0], 5, 2])\r\n    priors = tf.broadcast_to(\r\n        tf.expand_dims(priors, 1), [tf.shape(matched)[0], 5, 4])\r\n    g_cxcy = matched[:, :, :2] - priors[:, :, :2]\r\n    # encode variance\r\n    g_cxcy /= (variances[0] * priors[:, :, 2:])\r\n    # g_cxcy /= priors[:, :, 2:]\r\n    g_cxcy = tf.reshape(g_cxcy, [tf.shape(g_cxcy)[0], -1])\r\n    # return target for smooth_l1_loss\r\n    return g_cxcy\r\n\r\n\r\ndef _point_form(boxes):\r\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\r\n    representation for comparison to point form ground truth data.\r\n    Args:\r\n        boxes: (tensor) center-size default boxes from priorbox layers.\r\n    Return:\r\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\r\n    \"\"\"\r\n    return tf.concat((boxes[:, :2] - boxes[:, 2:] / 2,\r\n                      boxes[:, :2] + boxes[:, 2:] / 2), axis=1)\r\n\r\n\r\ndef _intersect(box_a, box_b):\r\n    \"\"\" We resize both tensors to [A,B,2]:\r\n    [A,2] -> [A,1,2] -> [A,B,2]\r\n    [B,2] -> [1,B,2] -> [A,B,2]\r\n    Then we compute the area of intersect between box_a and box_b.\r\n    Args:\r\n      box_a: (tensor) bounding boxes, Shape: [A,4].\r\n      box_b: (tensor) bounding boxes, Shape: [B,4].\r\n    Return:\r\n      (tensor) intersection area, Shape: [A,B].\r\n    \"\"\"\r\n    A = tf.shape(box_a)[0]\r\n    B = tf.shape(box_b)[0]\r\n    max_xy = tf.minimum(\r\n        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\r\n        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\r\n    min_xy = tf.maximum(\r\n        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\r\n        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\r\n    inter = tf.maximum((max_xy - min_xy), tf.zeros_like(max_xy - min_xy))\r\n    return inter[:, :, 0] * inter[:, :, 1]\r\n\r\n\r\ndef _jaccard(box_a, box_b):\r\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\r\n    is simply the intersection over union of two boxes.  Here we operate on\r\n    ground truth boxes and default boxes.\r\n    E.g.:\r\n        A \u2229 B / A \u222a B = A \u2229 B / (area(A) + area(B) - A \u2229 B)\r\n    Args:\r\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\r\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\r\n    Return:\r\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\r\n    \"\"\"\r\n    inter = _intersect(box_a, box_b)\r\n    area_a = tf.broadcast_to(\r\n        tf.expand_dims(\r\n            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\r\n        tf.shape(inter))  # [A,B]\r\n    area_b = tf.broadcast_to(\r\n        tf.expand_dims(\r\n            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\r\n        tf.shape(inter))  # [A,B]\r\n    union = area_a + area_b - inter\r\n    return inter / union  # [A,B]\r\n\r\n\r\ndef _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,\r\n                    using_encoding, priors, match_thresh, ignore_thresh,\r\n                    variances):\r\n    def parse_tfrecord(tfrecord):\r\n        features = {\r\n            'image/img_name': tf.io.FixedLenFeature([], tf.string),\r\n            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark0/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark0/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark1/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark1/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark2/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark2/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark3/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark3/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark4/x': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark4/y': tf.io.VarLenFeature(tf.float32),\r\n            'image/object/landmark/valid': tf.io.VarLenFeature(tf.float32)}\r\n        if using_bin:\r\n            features['image/encoded'] = tf.io.FixedLenFeature([], tf.string)\r\n            x = tf.io.parse_single_example(tfrecord, features)\r\n            img = tf.image.decode_jpeg(x['image/encoded'], channels=3)\r\n        else:\r\n            features['image/img_path'] = tf.io.FixedLenFeature([], tf.string)\r\n            x = tf.io.parse_single_example(tfrecord, features)\r\n            image_encoded = tf.io.read_file(x['image/img_path'])\r\n            img = tf.image.decode_jpeg(image_encoded, channels=3)\r\n\r\n        labels = tf.stack(\r\n            [tf.sparse.to_dense(x['image/object/bbox/xmin']),\r\n             tf.sparse.to_dense(x['image/object/bbox/ymin']),\r\n             tf.sparse.to_dense(x['image/object/bbox/xmax']),\r\n             tf.sparse.to_dense(x['image/object/bbox/ymax']),\r\n             tf.sparse.to_dense(x['image/object/landmark0/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark0/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark1/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark1/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark2/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark2/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark3/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark3/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark4/x']),\r\n             tf.sparse.to_dense(x['image/object/landmark4/y']),\r\n             tf.sparse.to_dense(x['image/object/landmark/valid'])], axis=1)\r\n\r\n        img, labels = _transform_data(\r\n            img_dim, using_flip, using_distort, using_encoding, priors,\r\n            match_thresh, ignore_thresh, variances)(img, labels)\r\n\r\n        return img, labels\r\n    return parse_tfrecord\r\n\r\n\r\ndef _transform_data(img_dim, using_flip, using_distort, using_encoding, priors,\r\n                    match_thresh, ignore_thresh, variances):\r\n    def transform_data(img, labels):\r\n        img = tf.cast(img, tf.float32)\r\n\r\n        # randomly crop\r\n        img, labels = _crop(img, labels)\r\n\r\n        # padding to square\r\n        img = _pad_to_square(img)\r\n\r\n        # resize\r\n        img, labels = _resize(img, labels, img_dim)\r\n\r\n        # encode labels to feature targets\r\n        if using_encoding:\r\n            labels = encode_tf(labels=labels, priors=priors,\r\n                               match_thresh=match_thresh,\r\n                               ignore_thresh=ignore_thresh,\r\n                               variances=variances)\r\n\r\n        return img, labels\r\n    return transform_data\r\n\r\n\r\ndef load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\r\n                          using_bin=True, using_flip=True, using_distort=True,\r\n                          using_encoding=True, priors=None, match_thresh=0.45,\r\n                          ignore_thresh=0.3, variances=[0.1, 0.2],\r\n                          shuffle=True, buffer_size=10240):\r\n    \"\"\"load dataset from tfrecord\"\"\"\r\n    if not using_encoding:\r\n        assert batch_size == 1  # dynamic data len when using_encoding\r\n    else:\r\n        assert priors is not None\r\n\r\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\r\n    raw_dataset = raw_dataset.repeat()\r\n    if shuffle:\r\n        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\r\n    dataset = raw_dataset.map(\r\n        _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,\r\n                        using_encoding, priors, match_thresh, ignore_thresh,\r\n                        variances),\r\n        num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    dataset = dataset.batch(batch_size, drop_remainder=True)\r\n    dataset = dataset.prefetch(\r\n        buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n    return dataset\r\n\r\n\r\n###############################################################################\r\n#   Data Augmentation                                                         #\r\n###############################################################################\r\n\r\n\r\ndef _crop(img, labels, max_loop=250):\r\n    shape = tf.shape(img)\r\n\r\n    def matrix_iof(a, b):\r\n        \"\"\"\r\n        return iof of a and b, numpy version for data augenmentation\r\n        \"\"\"\r\n        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\r\n        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\r\n\r\n        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\r\n            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\r\n        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\r\n        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\r\n\r\n    def crop_loop_body(i, img, labels):\r\n        valid_crop = tf.constant(1, tf.int32)\r\n\r\n        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\r\n        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\r\n        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\r\n        h = w = tf.cast(scale * short_side, tf.int32)\r\n        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\r\n        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\r\n        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\r\n        roi = tf.cast(roi, tf.float32)\r\n\r\n        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\r\n        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\r\n                             lambda: valid_crop, lambda: 0)\r\n\r\n        centers = (labels[:, :2] + labels[:, 2:4]) / 2\r\n        mask_a = tf.reduce_all(\r\n            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\r\n            axis=1)\r\n        labels_t = tf.boolean_mask(labels, mask_a)\r\n        valid_crop = tf.cond(tf.reduce_any(mask_a),\r\n                             lambda: valid_crop, lambda: 0)\r\n\r\n        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\r\n        h_offset = tf.cast(h_offset, tf.float32)\r\n        w_offset = tf.cast(w_offset, tf.float32)\r\n        labels_t = tf.stack(\r\n            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\r\n             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\r\n             labels_t[:, 4] - w_offset,  labels_t[:, 5] - h_offset,\r\n             labels_t[:, 6] - w_offset,  labels_t[:, 7] - h_offset,\r\n             labels_t[:, 8] - w_offset,  labels_t[:, 9] - h_offset,\r\n             labels_t[:, 10] - w_offset, labels_t[:, 11] - h_offset,\r\n             labels_t[:, 12] - w_offset, labels_t[:, 13] - h_offset,\r\n             labels_t[:, 14]], axis=1)\r\n\r\n        return tf.cond(valid_crop == 1,\r\n                       lambda: (max_loop, img_t, labels_t),\r\n                       lambda: (i + 1, img, labels))\r\n\r\n    _, img, labels = tf.while_loop(\r\n        lambda i, img, labels: tf.less(i, max_loop),\r\n        crop_loop_body,\r\n        [tf.constant(-1), img, labels],\r\n        shape_invariants=[tf.TensorShape([]),\r\n                          tf.TensorShape([None, None, 3]),\r\n                          tf.TensorShape([None, 15])])\r\n\r\n    return img, labels\r\n\r\n\r\ndef _pad_to_square(img):\r\n    height = tf.shape(img)[0]\r\n    width = tf.shape(img)[1]\r\n\r\n    def pad_h():\r\n        img_pad_h = tf.ones([width - height, width, 3]) * \\\r\n            tf.reduce_mean(img, axis=[0, 1], keepdims=True)\r\n        return tf.concat([img, img_pad_h], axis=0)\r\n\r\n    def pad_w():\r\n        img_pad_w = tf.ones([height, height - width, 3]) * \\\r\n            tf.reduce_mean(img, axis=[0, 1], keepdims=True)\r\n        return tf.concat([img, img_pad_w], axis=1)\r\n\r\n    img = tf.case([(tf.greater(height, width), pad_w),\r\n                   (tf.less(height, width), pad_h)], default=lambda: img)\r\n\r\n    return img\r\n\r\n\r\ndef _resize(img, labels, img_dim):\r\n    w_f = tf.cast(tf.shape(img)[1], tf.float32)\r\n    h_f = tf.cast(tf.shape(img)[0], tf.float32)\r\n    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\r\n                     labels[:, 2] / w_f,  labels[:, 3] / h_f,\r\n                     labels[:, 4] / w_f,  labels[:, 5] / h_f,\r\n                     labels[:, 6] / w_f,  labels[:, 7] / h_f,\r\n                     labels[:, 8] / w_f,  labels[:, 9] / h_f,\r\n                     labels[:, 10] / w_f, labels[:, 11] / h_f,\r\n                     labels[:, 12] / w_f, labels[:, 13] / h_f], axis=1)\r\n    locs = tf.clip_by_value(locs, 0, 1)\r\n    labels = tf.concat([locs, labels[:, 14][:, tf.newaxis]], axis=1)\r\n\r\n    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\r\n\r\n    def resize(method):\r\n        def _resize():\r\n            return tf.image.resize(\r\n                img, [img_dim, img_dim], method=method, antialias=True)\r\n        return _resize\r\n\r\n    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\r\n                   (tf.equal(resize_case, 1), resize('area')),\r\n                   (tf.equal(resize_case, 2), resize('nearest')),\r\n                   (tf.equal(resize_case, 3), resize('lanczos3'))],\r\n                  default=resize('bilinear'))\r\n\r\n    return img, labels\r\n\r\n\r\ndef load_dataset(cfg, priors, shuffle=True, buffer_size=10240):\r\n    \"\"\"load dataset\"\"\"\r\n    logging.info(\"load dataset from {}\".format(cfg['dataset_path']))\r\n    dataset = load_tfrecord_dataset(\r\n        tfrecord_name=cfg['dataset_path'],\r\n        batch_size=cfg['batch_size'],\r\n        img_dim=cfg['input_size'],\r\n        using_bin=True,\r\n        using_flip=False,\r\n        using_distort=False,\r\n        using_encoding=True,\r\n        priors=priors,\r\n        match_thresh=cfg['match_thresh'],\r\n        ignore_thresh=cfg['ignore_thresh'],\r\n        variances=cfg['variances'],\r\n        shuffle=shuffle,\r\n        buffer_size=buffer_size)\r\n    return dataset\r\n\r\ndef set_memory_growth():\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    if gpus:\r\n        try:\r\n            # Currently, memory growth needs to be the same across GPUs\r\n            for gpu in gpus:\r\n                tf.config.experimental.set_memory_growth(gpu, True)\r\n                logical_gpus = tf.config.experimental.list_logical_devices(\r\n                    'GPU')\r\n                logging.info(\r\n                    \"Detect {} Physical GPUs, {} Logical GPUs.\".format(\r\n                        len(gpus), len(logical_gpus)))\r\n        except RuntimeError as e:\r\n            # Memory growth must be set before GPUs have been initialized\r\n            logging.info(e)\r\n\r\ndef prior_box(image_sizes, min_sizes, steps, clip=False):\r\n    \"\"\"prior box\"\"\"\r\n    feature_maps = [\r\n        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\r\n        for step in steps]\r\n\r\n    anchors = []\r\n    for k, f in enumerate(feature_maps):\r\n        for i, j in product(range(f[0]), range(f[1])):\r\n            for min_size in min_sizes[k]:\r\n                s_kx = min_size / image_sizes[1]\r\n                s_ky = min_size / image_sizes[0]\r\n                cx = (j + 0.5) * steps[k] / image_sizes[1]\r\n                cy = (i + 0.5) * steps[k] / image_sizes[0]\r\n                anchors += [cx, cy, s_kx, s_ky]\r\n\r\n    output = np.asarray(anchors).reshape([-1, 4])\r\n\r\n    if clip:\r\n        output = np.clip(output, 0, 1)\r\n\r\n    return output\r\n\r\ncfg = {\r\n    # general setting\r\n    \"batch_size\": 8,\r\n    \"input_size\": 640,  # (h,w)\r\n\r\n    \"backbone_type\": 'ResNet50',  # 'ResNet50', 'MobileNetV2'\r\n    \"sub_name\": 'retinaface_res50',\r\n\r\n    # training dataset\r\n    \"dataset_path\": '/mnt/retinaface-tf2/data/widerface_test_bin.tfrecord',  # 'dataset/trainval_mask.tfrecord'\r\n    \"testing_dataset_path\": './data/widerface/val',  #\r\n    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\r\n    \"val_len\": 1839,\r\n    \"labels_list\": ['background', 'mask', 'unmask'],  # xml annotation\r\n\r\n    # anchor setting\r\n    \"min_sizes\": [[16, 32], [64, 128], [256, 512]],\r\n    \"steps\": [8, 16, 32],\r\n    \"match_thresh\": 0.45,\r\n    \"ignore_thresh\": 0.3,\r\n    \"variances\": [0.1, 0.2],\r\n    \"clip\": False,\r\n\r\n    # network\r\n    \"out_channel\": 256,\r\n\r\n}\r\n\r\ndef main(_):\r\n\r\n    multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    # init\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n\r\n    logger = tf.get_logger()\r\n    logger.disabled = True\r\n    set_memory_growth()\r\n    priors = prior_box((cfg['input_size'], cfg['input_size']),\r\n                           cfg['min_sizes'],  cfg['steps'], cfg['clip'])\r\n    dataset = load_dataset(cfg, priors, shuffle=True)\r\n    train_iter = iter(dataset)\r\n    for _ in range(10):\r\n        print(next(train_iter)[0].shape)\r\n    dist_dataset = multiworker_strategy.experimental_distribute_dataset(dataset)\r\n    dist_train_iter = iter(dist_dataset)\r\n    for _ in range(10):\r\n        print(next(dist_train_iter)[0].shape)\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(main)\r\n\r\n```\r\n\r\nThe log as following:\r\n\r\n```bash\r\n2020-09-10 07:37:16.503276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-10 07:37:17.829609: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-10 07:37:18.105548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-10 07:37:18.105661: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-10 07:37:18.109466: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-10 07:37:18.112812: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-10 07:37:18.113402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-10 07:37:18.116045: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-10 07:37:18.117442: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-10 07:37:18.122760: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-10 07:37:18.141044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-10 07:37:18.141716: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-10 07:37:18.156381: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199875000 Hz\r\n2020-09-10 07:37:18.158897: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6029f90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-10 07:37:18.158941: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-10 07:37:18.640931: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6095e10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-10 07:37:18.641017: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-09-10 07:37:18.643033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-10 07:37:18.643099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-10 07:37:18.643141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-10 07:37:18.643172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-10 07:37:18.643202: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-10 07:37:18.643232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-10 07:37:18.643261: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-10 07:37:18.643287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-10 07:37:18.646399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-10 07:37:18.646451: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-10 07:37:19.301344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-10 07:37:19.301386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-10 07:37:19.301399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-10 07:37:19.303428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2020-09-10 07:37:19.327745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-09-10 07:37:19.327814: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-10 07:37:19.327857: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-10 07:37:19.327887: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-10 07:37:19.327914: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-10 07:37:19.327940: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-10 07:37:19.327964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-10 07:37:19.327990: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-10 07:37:19.331434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-10 07:37:19.331512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-10 07:37:19.331538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-10 07:37:19.331559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-10 07:37:19.334657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2020-09-10 07:37:19.342618: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> dist-data-worker-0.t9k-sample.svc:2222, 1 -> dist-data-worker-1.t9k-sample.svc:2222, 2 -> dist-data-worker-2.t9k-sample.svc:2222}\r\n2020-09-10 07:37:19.345112: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://dist-data-worker-0.t9k-sample.svc:2222\r\nINFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0', '/job:worker/replica:0/task:0/device:XLA_GPU:0', '/job:worker/replica:0/task:0/device:GPU:0']\r\nI0910 07:37:19.346995 140561422595904 collective_all_reduce_strategy.py:329] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0', '/job:worker/replica:0/task:0/device:XLA_GPU:0', '/job:worker/replica:0/task:0/device:GPU:0']\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0/device:GPU:0',)\r\nI0910 07:37:19.348705 140561422595904 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:0/device:GPU:0',)\r\nINFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['dist-data-worker-0.t9k-sample.svc:2222', 'dist-data-worker-1.t9k-sample.svc:2222', 'dist-data-worker-2.t9k-sample.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nI0910 07:37:19.349095 140561422595904 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['dist-data-worker-0.t9k-sample.svc:2222', 'dist-data-worker-1.t9k-sample.svc:2222', 'dist-data-worker-2.t9k-sample.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nI0910 07:37:19.349557 140561422595904 dist_data.py:469] Physical devices cannot be modified after being initialized\r\nI0910 07:37:19.374378 140561422595904 dist_data.py:438] load dataset from /mnt/retinaface-tf2/data/widerface_test_bin.tfrecord\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\n(8, 640, 640, 3)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 2102, in execution_mode\r\n    yield\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 758, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2610, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 649, in __next__\r\n    return self.get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 721, in get_next\r\n    strict=True,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1210, in cond\r\n    result = false_fn()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 720, in <lambda>\r\n    lambda: out_of_range_fn(i, device),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 702, in out_of_range_fn\r\n    data = self._iterators[worker_index].get_next(device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1457, in get_next\r\n    return self._iterator.get_next(device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 576, in get_next\r\n    return self._device_iterators[index].get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 825, in get_next\r\n    return self._next_internal()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 764, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\", line 2105, in execution_mode\r\n    executor_new.wait()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py\", line 67, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/retinaface-tf2/dist_data.py\", line 545, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/mnt/retinaface-tf2/dist_data.py\", line 541, in main\r\n    print(next(dist_train_iter)[0].shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 651, in __next__\r\n    raise StopIteration\r\nStopIteration\r\n2020-09-10 07:37:29.088617: W tensorflow/core/common_runtime/eager/context.cc:566] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown. Thank you for your patience!\r\n\r\n```", "Can you remove any of the augmentation functions? The tf.record doesn't have to be loaded in the exact format you ultimately want it to be in as long as we can reproduce the error. I understand that some of the preprocessing code is necessary, but just trying to throw out whatever we don't need (while still preserving the error message).", "Can you also try setting the `tf.data.experimental.AutoShardPolicy` to `DATA` ?\r\n\r\n[From the docs](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset), the policy is by default AUTO:\r\n`AUTO: This is the default option which means an attempt will be made to shard by FILE. The attempt to shard by FILE fails if a file-based dataset is not detected. tf.distribute will then fall back to sharding by DATA. Note that if the input dataset is file-based but the number of files is less than the number of workers, an error will be raised.`\r\nSo since you have more than 1 worker, but only one input file, that could be causing the problem.", "Thanks a lot ! It works for me ! I add following codes:\r\n\r\n```bash\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\ndataset = dataset.with_options(options)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43039\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43039\">No</a>\n", "> Thanks a lot ! It works for me ! I add following codes:\r\n> \r\n> ```shell\r\n> options = tf.data.Options()\r\n> options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n> dataset = dataset.with_options(options)\r\n> ```\r\n\r\nThank you! Adding that `with_options(options)` to all my datasets solved my problem as well. I am training in 2 nodes and was getting:\r\n```\r\n2021-06-04 18:26:24.529773: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 0 of dimension 0 out of bounds.\r\n2021-06-04 18:26:24.530388: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at reduction_ops_common.h:148 : Invalid argument: Invalid reduction dimension (-1 for input with 0 dimension(s)\r\n2021-06-04 18:26:24.531465: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 0 of dimension 0 out of bounds.\r\n2021-06-04 18:26:24.531598: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at reduction_ops_common.h:148 : Invalid argument: Invalid reduction dimension (-1 for input with 0 dimension(s)\r\n2021-06-04 18:26:24.531675: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 0 of dimension 0 out of bounds.\r\n2021-06-04 18:26:24.531790: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at reduction_ops_common.h:148 : Invalid argument: Invalid reduction dimension (-1 for input with 0 dimension(s)\r\n2021-06-04 18:26:24.531848: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index 0 of dimension 0 out of bounds.\r\n2021-06-04 18:26:24.531931: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at reduction_ops_common.h:148 : Invalid argument: Invalid reduction dimension (-1 for input with 0 dimension(s)\r\n```\r\nmore erros:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 5 root error(s) found.\r\n  (0) Invalid argument:  slice index 0 of dimension 0 out of bounds.\r\n\t [[node replica_3/strided_slice (defined at /threading.py:916) ]]\r\n\t [[cond_9/then/_196/cond_9/LAMB/LAMB/group_deps/NoOp_1/_805]]\r\n  (1) Invalid argument:  slice index 0 of dimension 0 out of bounds.\r\n\t [[node replica_3/strided_slice (defined at /threading.py:916) ]]\r\n  (2) Invalid argument:  slice index 0 of dimension 0 out of bounds.\r\n\t [[node replica_3/strided_slice (defined at /threading.py:916) ]]\r\n\t [[cond_8/else/_181/cond_8/Maximum/_670]]\r\n  (3) Invalid argument:  slice index 0 of dimension 0 out of bounds.\r\n\t [[node replica_3/strided_slice (defined at /threading.py:916) ]]\r\n\t [[cond_8/then/_180/cond_8/GreaterEqual/_680]]\r\n  (4) Invalid argument:  slice index 0 of dimension 0 out of bounds.\r\n\t [[node replica_3/strided_slice (defined at /threading.py:916) ]]\r\n\t [[cond_9/then/_196/cond_9/LAMB/Cast/ReadVariableOp/_638]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_99090]\r\n```\r\n\r\nSince it says \"slice index 0 of dimension 0 out of bounds\" so I guess once of the worker is getting a batch size of 0? But I do wish the error message could be more descriptive.\r\n"]}, {"number": 43038, "title": "Pylint incorrectly identifies tensorflow public API functions in tensorflow 2.2+", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Apart from the example below, no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04/OSX 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: :x:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2+\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): :x:\r\n- GCC/Compiler version (if compiling from source): :x:\r\n- CUDA/cuDNN version: :x:\r\n- GPU model and memory: :x:\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using pylint, a lot of functions from the public API are misidentified.\r\nFor example, the public api function `tf.split`, which is publicly defined as `tensorflow/python/ops/array_ops.split` is misidentified (I think as `tensorflow/python/ops/gen_array_ops.split`).\r\n\r\nOther examples include `tf.random.uniform`, `tf.concat` and the list goes on.\r\n\r\nLet's take the code snipped below:\r\n\r\n`example.py`:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntensor = tf.random.uniform((2, 4), minval=0, maxval=256)\r\ntf.split(tensor, num_or_size_splits=2, axis=-1)\r\n```\r\n\r\nThis is perfectly fine code, it runs as expected.\r\n\r\nHowever, when running pylint both functions are misidentified and lots of linting errors are raised.\r\nRunning pylint on the module (`pylint -E example.py`) gives:\r\n```\r\nexample.py:3:9: E1123: Unexpected keyword argument 'minval' in function call (unexpected-keyword-arg)\r\nexample.py:3:9: E1123: Unexpected keyword argument 'maxval' in function call (unexpected-keyword-arg)\r\nexample.py:3:9: E1120: No value for argument 'dtype' in function call (no-value-for-parameter)\r\nexample.py:4:0: E1123: Unexpected keyword argument 'num_or_size_splits' in function call (unexpected-keyword-arg)\r\nexample.py:4:0: E1124: Argument 'axis' passed by position and keyword in function call (redundant-keyword-arg)\r\nexample.py:4:0: E1120: No value for argument 'value' in function call (no-value-for-parameter)\r\nexample.py:4:0: E1120: No value for argument 'num_split' in function call (no-value-for-parameter)\r\n```\r\n\r\n**Describe the expected behavior**\r\nRunning `pylint -E example.py` should not give any errors.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ntensor = tf.random.uniform((2, 4), minval=0, maxval=256)\r\ntf.split(tensor, num_or_size_splits=2, axis=-1)\r\n```\r\n\r\n**Other info / logs**\r\nThis occurs with `tensorflow>=2.2.0`.\r\nPrevious versions of tensorflow `2.X` (e.g. tensorflow `2.1.X` and tensorflow `2.0.X`) do not have these problems.\r\n\r\nI have used `pylint==2.6.0` for the example, but previous versions have the same behaviour.\r\n\r\nOne of the things that might have caused this (just a guess) is the upgrade to a new version of `gast` that occurred in tensorflow `2.2`, where they went from `gast==0.2.2` to `gast==0.3.3`.\r\n\r\nNow, I know that this issue is not a code breaking issue, but it is a workflow breaking issue when using tensorflow in a professional setting. For example, one of the requirements for passing all steps in the CI may be running pylint, which now fails. Pylint allow disabling errors for specific third-party packages, so really the only solution is to add a `pylint: disable=...` comment every time you use a tensorflow function which is misidentified or to disable pylint for the project all together. Both options aren't desirable.\r\n\r\nThis issue was also raised in the `pylint` repo (https://github.com/PyCQA/pylint/issues/3613) and probably also related to https://github.com/PyCQA/pylint/issues/3596. But I don't think these issues belong in the `pylint` repo` (or `astroid` for that matter), but here in the tensorflow repo as it's probably caused by the import structure of tensorflow.\r\n\r\nOne lead might be that a wildstar import overwrites functions. An examples might be the wildstar import in https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_ops.py#L40 which overwrites `tensorflow/python/ops/array_ops.split` with the starred import `tensorflow/python/ops/gen_array_ops.split`. I'm not sure, but not performing wildstar imports might solve this linter problem.", "comments": ["This is true with newer pylint versions. As you see currently in master we are on `1.6.4`:\r\n```\r\ntensorflow/tools/ci_build/rel/ubuntu/sanity.sh:sudo python2 -m pip install pylint==1.6.4\r\ntensorflow/tools/ci_build/rel/ubuntu/sanity.sh:sudo python3 -m pip install pylint==1.6.4\r\ntensorflow/tools/ci_build/presubmit/ubuntu_16/sanity/build.sh:  sudo python3 -m pip install pylint==1.6.4\r\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:pip2 install pylint==1.6.4\r\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:pip3 install pylint==1.6.4\r\ntensorflow/tools/ci_build/install/install_pip_packages.sh:# pylint==1.6.4 requires python-astroid (>= 1.4.5) requires lazy-object-proxy\r\ntensorflow/tools/ci_build/install/install_pip_packages.sh:pip2 install pylint==1.6.4\r\ntensorflow/tools/ci_build/install/install_pip_packages.sh:pip3 install pylint==1.6.4\r\ntensorflow/tools/ci_build/release/ubuntu_16/sanity/build.sh:sudo python2 -m pip install pylint==1.6.4\r\ntensorflow/tools/ci_build/release/ubuntu_16/sanity/build.sh:sudo python3 -m pip install pylint==1.6.4\r\n```", "Ahh, I see. I can indeed verify that when using `pylint==1.6.4`, there are no linting errors in the `example.py` I mentioned.\r\n\r\nAre there plans to support newer versions of pylint? `pylint==1.6.4` was released on July 19th 2016 and numerous improvements have been made since.\r\n\r\nAlso, if this _bug_ was introduced in tensorflow `2.2`, are people working on finding the cause of this?", "I don't know just let see what kind of sanity checks will fails with https://github.com/tensorflow/tensorflow/pull/43040.", "I did some more digging with older versions of pylint.\r\n\r\nNo errors on `example.py`:\r\n* last `1.x` release (`1.9.5`): \u2705  \r\n* `2.0.1` up until `2.2.3` gave an error that probably has something to do with the astroid dependency https://github.com/PyCQA/astroid/issues/649\r\n* `2.3.3`: \u2705 \r\n* `2.4.4`: \u2705 \r\n* `2.5.3`: \u274c \r\n* `2.6.0`: \u274c \r\n\r\nNow these only check the minimal example above, but might give an indication of what might still work.\r\n\r\nI saw that 69a975e6a35279a91035f601b36f6771331d6619 upgrades to `2.6.0` and the CI failed. Maybe we could try upgrading to one of the versions above?", "With `2.6.0` the list of things to fix or ignore it is quite long:\r\n\r\nhttps://source.cloud.google.com/results/invocations/65c2a3ec-39f8-4d43-823d-ba791a96cbb2/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/log", "I can imagine that the list is quite large. There are a lot of new error code introductions along the way to newer versions.\r\n\r\nWould you (or the tensorflow team) be interested in bumping one minor pylint version at a time and fixing/disabling issues along the way? I can image that fixing (at least some of them) would require backwards-compatibility breaking changes.", "@ScaleRunner Yes let's see what they want to do after having an overview on the LOG.", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2e2f2330c3a3c9786a9994e4c4d0b5a4/43038.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43038\">No</a>\n", "@mihaimaruseac Please reopen this.", "@mihaimaruseac I've maintained the related PR branch so let know if we want to handle this with another PR or do you want to push this internally.", "If you want to work on it that should be good, I'll handle off on fixing this internally.\r\n\r\nDo you want the full list of error messages?", "@mihaimaruseac I think the problem Is more on the policy point of view and I don't know where we want to discuss this cause we need some feedbacks from the internal team. Also with the current pylint versiin we are in a mix of accepted pylint error that we ignore and things that we care. \r\nOn what we want to expand the allowed-list and what instrad we need to fix? So we Need to collect an  internal feedback somewhere.", "The ignored list was seeds with errors that the internal python linting and formatting tools would allow but fail in OSS. Then stuff got added to the ignored list to fix breakages but these would need to be removed in time.\r\n\r\nWe also have inline annotations that at some point will have to be cleared if not relevant anymore but there's no timeline on that.", "> The ignored list was seeds with errors that the internal python linting and formatting tools would allow but fail in OSS. Then stuff got added to the ignored list to fix breakages but these would need to be removed in time.\r\n> \r\n> We also have inline annotations that at some point will have to be cleared if not relevant anymore but there's no timeline on that.\r\n\r\nI think that it is a little bit different cause we have  a specific error list to fail:\r\nhttps://github.com/offscale/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh#L203-L213\r\n\r\nAnd a general allow list:\r\nhttps://github.com/offscale/tensorflow/blob/626a13aafa15d2a0d51c1cf9251a074a6b65ae3b/tensorflow/tools/ci_build/ci_sanity.sh#L100-L121\r\n\r\nPlus inlines.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43038\">No</a>\n"]}, {"number": 43037, "title": "Error \"failed to connect to all addresses\" when training on TPU with Colab", "body": "Hello!\r\n\r\n**Describe the current behavior**\r\nWhen running the my Colab I get the following error:\r\n```\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1599555147.064735819\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1599555147.064732799\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n```\r\nWhen I try to iterator over my dataset:\r\n```\r\n<ipython-input-5-9732b6b5faf1> in train(self)\r\n     42 \r\n     43             for epoch_iter in range(1, 5):\r\n---> 44                 for step, batch in enumerate(train_ds):\r\n     45                     self.global_step = iterations.numpy()\r\n     46 \r\n```\r\n\r\n**Describe the expected behavior**\r\nBeing able to iterate over the dataset.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe full Colab can be found here https://colab.research.google.com/drive/1jZaFmrDmBGbGkg8oKEJ78cL2PgIB7UF5?usp=sharing\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/23e04d7ef7e70a5ee6eaf93b0b57e0e8/untitled408.ipynb)", "@jplu Can you check https://github.com/tensorflow/tensorflow/issues/40035#issuecomment-663173209?", "I get the exact same error also when loading data from a Google Storage bucket.", "You can replace:\r\n```\r\nds, info = tfds.load(\"glue/mrpc\", split=\"train\", with_info=True, try_gcs=True)\r\n```\r\nBy\r\n```\r\nds, info = tfds.load(\"glue/mrpc\", split=\"train\", with_info=True, try_gcs=True, data_dir=<gs bucket>)\r\n```\r\nIn your [gist ](https://colab.research.google.com/gist/Saduf2019/23e04d7ef7e70a5ee6eaf93b0b57e0e8/untitled408.ipynb) and you will get the exact same error.", "It seems to me that on @Saduf2019 tf-nightly gist there is another issue about Graphdef versioning:\r\n\r\n> InvalidArgumentError: Converting GraphDef to Graph has failed. The binary trying to import the GraphDef was built when GraphDef version was 440. The GraphDef was produced by a binary built when GraphDef version was 517. The difference between these versions is larger than TensorFlow's forward compatibility guarantee. The following error might be due to the binary trying to import the GraphDef being too old: NodeDef mentions attr 'ram_budget' not in Op<name=ModelDataset; signature=input_dataset:variant -> handle:variant; attr=algorithm:int,default=0; attr=cpu_budget:int,default=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>; NodeDef: {{node ModelDataset/_21}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.). [Op:OptimizeDatasetV2]\r\n\r\n/cc @jhseu", "Indeed the difference comes from the fact to use `tf-nightly version 2.4.0-dev20200908` instead of the 2.3 version.", "@jplu Yes there is no visibility about your issue reproducibility in the @Saduf2019 gist cause this new issue could be the only one (in this case your issue It Is already solved in nightly) or just an early issue that doesn't let to trigger your own.\n\nThe only thing to \"quickly\" disambiguate the status on nightly Is to resolve this new Graphdef incompatibilty.", "Hi @jplu were you able to try the suggestion from @bhack?", "What was the suggestion? What I understood is that we have to find out if in nightly is issue comes after because the issue in 2.3 has been solved or if it comes before and the nightly brings another issue. I don't know what to do to know that.", "@nikitamaia The problem is that @jplu is using in the submitted example an external library (https://huggingface.co/transformers/) that probably is using something pretrained that is incompatible to test with `tf-nightly`", "unfortunately I stumbled upon this issue when trying with generator. I tried an example from [docs here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) and it returns same error. This happens when I initialize the TPU first, and then dataset later.\r\n\r\n```\r\nimport tensorflow as tf\r\ntry:\r\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\n  tf.config.experimental_connect_to_cluster(tpu)\r\n  tf.tpu.experimental.initialize_tpu_system(tpu)\r\n  strategy = tf.distribute.TPUStrategy(tpu)\r\nexcept ValueError:\r\n  print('WARNING: TPU not available!')\r\n  strategy = tf.distribute.get_strategy()\r\n\r\nimport itertools\r\n\r\ndef gen():\r\n  for i in itertools.count(1):\r\n    yield (i, [1] * i)\r\n\r\ndataset = tf.data.Dataset.from_generator(\r\n     gen,\r\n     (tf.int64, tf.int64),\r\n     (tf.TensorShape([]), tf.TensorShape([None])))\r\n\r\nlist(dataset.take(3).as_numpy_iterator())\r\n```\r\n\r\nerror:\r\n\r\n```\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1606047177.389355931\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3948,\"referenced_errors\":[{\"created\":\"@1606047176.622134062\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n```\r\n\r\nIt's on Google Colab btw", "@Smankusors: I believe the root cause for the error you are encountering is different from that of this initial bug report. \r\n\r\nAs for your case, could you instead materialize the dataset (instead of using `from_generator` API) ?\r\ntf.data.Dataset.from_generator() API is not yet supported on TPU's and I believe this is the reason for the failure. ", "> As for your case, could you instead materialize the dataset (instead of using `from_generator` API) ?\r\n\r\nno, my Google Colab session crashes when doing this \ud83d\ude06 \r\n\r\nprobably protobuf hard limit I guess\r\n\r\n> tf.data.Dataset.from_generator() API is not yet supported on TPU's and I believe this is the reason for the failure.\r\n\r\nhuh... I see... then the documentation needs to be updated about this?", "@jplu It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.5 and let us know if the issue still persists? Thanks!", "@sushreebarsa  Still the case with TF 2.5. Error from the same place:\r\n```\r\n<ipython-input-4-9732b6b5faf1> in train(self)\r\n     42 \r\n     43             for epoch_iter in range(1, 5):\r\n---> 44                 for step, batch in enumerate(train_ds):\r\n     45                     self.global_step = iterations.numpy()\r\n```\r\nWith the same message:\r\n```\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:\r\n:{\"created\":\"@1628525695.735776411\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":5420,\"referenced_errors\":[{\"created\":\"@1628525695.735773104\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":398,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n```", "@jplu Do you have the same error with `pip install tensorflow==2.6.0rc2` ?", "I now get a new error, the same than https://github.com/keras-team/keras/issues/15044 . The piece of code that raises this issue is located before the one that raises the previous error. Then I cannot say if the error is still here or not because now this piece of code is not anymore reached.", "FYI, if you are using tf-nightly with TPUs you may need to make the TPU runtime version match with the currently installed TF version. To do this:\r\n\r\n```\r\n!pip install cloud_tpu_client\r\nfrom cloud_tpu_client import Client\r\nc = Client(tpu='')\r\nc.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\r\n```", "@bfontain We really need to find a better place to expose this tip.", "Ideally a colab user should not need to do this. We could detect a tf version change after running a !pip or %pip or something similar and automatically make the runtime match if possible. An alternative would be to enhance %tensorflow_version (which does support TPU runtime version changing) to support more than just a handful of versions. In particular it doesn't support nightly.", "I was not using tf-nightly but tensorflow==2.6.0rc2. But thanks for the tip it is always good to know!", "IIRC, we don't have 2.6.0 runtime for TPUs available yet. If you need something feature added beyond 2.5.0 you should try the nightly.", "2.6.0 was just released", " The TensorFlow release may be done, but it may be a few days before the TPU runtime release is available. You can check the runtime version your associated TPU is on via the Client() object above,\r\n\r\n```\r\n!pip install cloud_tpu_client\r\nfrom cloud_tpu_client import Client\r\nc = Client(tpu='')\r\nprint(c.runtime_version())\r\n```", "I think as well, the default TPU runtime will match the default version of TensorFlow installed in colab, if you change the TF version (even to non-nightly) it is advised to configure the TPU runtime version as mentioned above. The caveat for non-nightly versions is that we generally only have TPU runtimes for non rc releases. In some cases (e.g. 2.5.1), we don't have a TPU runtime release, and it is most likely safe to use TensorFlow 2.5.1 with TPU runtime 2.5.0 as most point release don't require any TPU runtime side changes.", "I think with the current TF release policy patch releases are just for security fixes.\n\nFor bug triaging it is important to work with nightly as we often test on Colab nightly versions to check if It something the is fixed in master.", "Awesome!! The @bfontain's tip made it works!! I had to adapt my code to take into account some new changes and now it works like a charm with TF 2.6.0 and the TPU client set to this same version with:\r\n```\r\nfrom cloud_tpu_client import Client\r\nc = Client(tpu='')\r\nc.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\r\n```\r\n\r\nBTW when I run `print(c.run_time_version())` I get the error `AttributeError: 'Client' object has no attribute 'run_time_version'` with `cloud_tpu_client==0.10`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43037\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43037\">No</a>\n", "Great to hear! With regards to `cloud_tpu_client`, I made a typo (now corrected), the correct call would be `print(c.runtime_version())` with `runtime` as one word.", "Also, I believe the 2.6.0 runtime may now be available on Cloud TPU as well if you don't want to use nightly, but you may still need the `cloud_tpu_client` code until default TF and TPU runtime version for colab is updated."]}, {"number": 43036, "title": "I want to minimize 'cost' with variable 'h0'", "body": "M=np.array([[1,0],[0,0]])\r\nM=np.kron(M,np.identity(32))\r\nM=tf.constant(M,dtype=tf.float32)\r\nM=tf.complex(M,tf.zeros([64,64],tf.float32))\r\nh0=tf.Variable(tf.random.normal([64,64],0,1,tf.float32),dtype=tf.float32,shape=[64,64])\r\nh0_T=tf.transpose(h0)\r\nih = tf.complex(h0 - h0_T, h0 + h0_T)\r\nu = tf.linalg.expm(ih)\r\nu_dagger=tf.linalg.expm(-ih)\r\nro=tf.constant(ro[0],dtype=tf.float32)\r\nro=tf.complex(ro,tf.zeros([64,64],tf.float32))\r\ncost= lambda: tf.math.real(tf.linalg.trace(u*ro*u_dagger*M))\r\nopt = tf.keras.optimizers.SGD(learning_rate=0.001)\r\ntrain=opt.minimize(cost, var_list=[h0]).numpy()\r\n\r\n\r\n\r\nThis is my code, and i error occurs like below\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-0f648ef2f92b> in <module>\r\n     12 cost= lambda: tf.math.real(tf.linalg.trace(u*ro*u_dagger*M))\r\n     13 opt = tf.keras.optimizers.SGD(learning_rate=0.001)\r\n---> 14 train=opt.minimize(cost, var_list=[h0]).numpy()\r\n     15 with tf.Session() as sess:\r\n     16     sess.run(tf.global_variables_initializer())\r\n\r\n~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)\r\n    375         loss, var_list=var_list, grad_loss=grad_loss)\r\n    376 \r\n--> 377     return self.apply_gradients(grads_and_vars, name=name)\r\n    378 \r\n    379   def _clip_gradients(self, grads):\r\n\r\n~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)\r\n    511       ValueError: If none of the variables have gradients.\r\n    512     \"\"\"\r\n--> 513     grads_and_vars = _filter_grads(grads_and_vars)\r\n    514     var_list = [v for (_, v) in grads_and_vars]\r\n    515 \r\n\r\n~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in _filter_grads(grads_and_vars)\r\n   1268   filtered = tuple(filtered)\r\n   1269   if not filtered:\r\n-> 1270     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n   1271                      ([v.name for _, v in grads_and_vars],))\r\n   1272   if vars_with_empty_grads:\r\n\r\nValueError: No gradients provided for any variable: ['Variable:0'].\r\n\r\n\r\nWhat is problem? Thank you very much", "comments": ["@MaarHybrid \r\n\r\nI tried reproducing the issue in colab with TF 2.3 and i am seeing the error message(`NameError: name 'ro' is not defined`).\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This is better asked on StackOverflow as it has no issue with TF code", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43036\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43036\">No</a>\n", "import tensorflow as tf\r\nimport numpy as np\r\nfrom scipy.linalg import expm, sinm, cosm\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow_addons as tfa\r\ntfa.options.TF_ADDONS_PY_OPS = True\r\n\r\n#Load MNIST datasets from Keras\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n#Rescale the images from [0, 255] to the [0.0, 1.0] range.\r\nx_train, x_test = x_train[..., np.newaxis]*np.pi/255.0, x_test[..., np.newaxis]*np.pi/255.0\r\n\r\n#Filter out specific type of images\r\ntrain_filter = np.where((y_train == 0 ))\r\ntest_filter = np.where((y_test == 0))\r\n\r\n#Put filters\r\nx_train, y_train = x_train[train_filter], y_train[train_filter]\r\nx_test, y_test = x_test[test_filter], y_test[test_filter]\r\n\r\n\r\n# In[4]:\r\n\r\n\r\n#Downscale loaded MNIST images to n*n images\r\nx_train_downscale = tf.image.resize(x_train, (8,8)).numpy()\r\nx_test_downscale = tf.image.resize(x_test, (8,8)).numpy()\r\n\r\nflat = []\r\nfor i in range (0,39):\r\n    flat.append(x_train_downscale[i].flatten().reshape(1,64))\r\nro=[]\r\nfor i in range (0,39):\r\n    ro.append(np.matmul(flat[i].transpose(),flat[i]))\r\n\r\nM=np.array([[1,0],[0,0]])\r\nM=np.kron(M,np.identity(32))\r\nM=tf.constant(M,dtype=tf.float32)\r\nM=tf.complex(M,tf.zeros([64,64],tf.float32))\r\nh0=tf.Variable(tf.random.normal([64,64],0,1,tf.float32),dtype=tf.float32,shape=[64,64])\r\nh0_T=tf.transpose(h0)\r\nih = tf.complex(h0 - h0_T, h0 + h0_T)\r\nu = tf.linalg.expm(ih)\r\nu_dagger=tf.linalg.expm(-1*ih)\r\nro=tf.constant(ro[0],dtype=tf.float32)\r\nro=tf.complex(ro,tf.zeros([64,64],tf.float32))\r\ncost= lambda: tf.math.real(tf.linalg.trace(u*ro*u_dagger*M))\r\nopt = tf.keras.optimizers.SGD(learning_rate=0.001)\r\ntrain=opt.minimize(cost, var_list=[h0]).numpy()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for step in range(2001):\r\n        _, cost_val,h_val = sess.run([train, cost, h])\r\n        if step % 20 == 0:\r\n            print(step, cost_val, h_val)\r\n\r\nThis is whole code"]}, {"number": 43034, "title": "XLA Asynchronous compilation", "body": "XLA Asynchronous compilation\r\n    1) add option to opt into asynchronous compilation\r\n    2) asynchronous compilation uses a dedicated number of threads\r\n    to start a cluster instance compilation while the fallback path\r\n    is executed\r\n    3) limit number of ongoing compilations to a fixed threshold\r\n\r\nchange some VLOG levels to make level 2 less verbose", "comments": ["@bas-aarts  Can you please check @sanjoy's comments and keep us posted ? Thanks!", "@bas-aarts  Can you please check @sanjoy's comments and keep us posted ? Thanks!", "@bas-aarts Any update on this PR? Please. Thanks!", "This PR is still being worked on\r\n", "@bas-aarts Can you please check @sanjoy's comments and keep us posted ? Thanks!", "will also add a test", "Are you still working on this or do you want me to review this now?", "> Are you still working on this or do you want me to review this now?\r\n\r\nHaven't gotten to the test yet. Will ping you when it's there.", "added a test", "investigating the failures", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43034) for more info**.\n\n<!-- need_author_cla -->", "@sanjoy please advise how to fix the cla issue. I only have one account, and don't know how to address\r\n\r\n", "@googlebot I fixed it.\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43034) for more info**.\n\n<!-- need_author_cla -->", "> @sanjoy please advise how to fix the cla issue. I only have one account, and don't know how to address\r\n\r\nThe f234816e414e1842a7cdbcd2962d1b97dde2e080 commit has a different icon on github, can you please verify if that one is also from your NVIDIA email address?", "the last commit came from on machine where I did not set my email address (git config user.email is empty).\r\nHow do I fix it at this point?", "> the last commit came from on machine where I did not set my email address (git config user.email is empty).\r\n> How do I fix it at this point?\r\n\r\nI suspect editing the commit to have the right email and then force pushing should do the trick?", "@googlebot I fixed it.\r\n", "> > the last commit came from on machine where I did not set my email address (git config user.email is empty).\r\n> > How do I fix it at this point?\r\n> \r\n> I suspect editing the commit to have the right email and then force pushing should do the trick?\r\n\r\nyep. Thanks. My last change should fix the errors in the CI", "Just noticed this change has not yet been merged. Any reason for this?", "> Just noticed this change has not yet been merged. Any reason for this?\r\n\r\nNot clear, @gbaned do you know what's going on here?", "> > Just noticed this change has not yet been merged. Any reason for this?\r\n> \r\n> Not clear, @gbaned do you know what's going on here?\r\n\r\n@sanjoy  Internal test failures are appearing in CL. Can you please take a look. Thank you!"]}, {"number": 43033, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "Name: tensorflow\r\nVersion: 2.3.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\user\\anaconda3\\lib\\site-packages\r\nRequires: numpy, wrapt, gast, protobuf, google-pasta, keras-preprocessing, tensorflow-estimator, termcolor, scipy, opt-einsum, tensorboard, absl-py, astunparse, wheel, grpcio, six, h5py\r\nRequired-by: \r\n\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-ac9bda315bf2> in <module>\r\n      1 # Importing the required libraries\r\n----> 2 import tensorflow as tf\r\n      3 \r\n      4 \r\n      5 # Import MNIST data\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@rrajkumar77 \r\n\r\nPlease installed the latest distribution of Visual C++ from [this link](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads). and refer to below duplicate issues for the same error and let us know.\r\n#41895 #42809 #43003 #42372 #42058 ", "Even after installing Visual c++ i getting the same error.\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-7-2c56a628b4dd> in <module>\r\n      1 # Importing the required libraries\r\n----> 2 import tensorflow as tf\r\n      3 \r\n      4 # Import MNIST data\r\n      5 mnist = tf.keras.datasets.mnist\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "@rrajkumar77 \r\nPlease confirm you have verified with the issues shared, compatibility issues and missing dll files in case.\r\n\r\nYou might be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python\r\nYour CPU does not support AVX instructions. Please share the make and model of your CPU, so that we can verify this.\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\nYou need to install the MSVC 2019 redistributable\r\nThere is a library that is in a different location/not installed on your system that cannot be loaded.\r\n\r\nPlease verify with existing issues, as this is a reported issue.", "\r\nOS Name\tMicrosoft Windows 10 Pro\r\nVersion\t10.0.19041 Build 19041\r\nOther OS Description \tNot Available\r\nOS Manufacturer\tMicrosoft Corporation\r\nSystem Name\tDESKTOP-RAJ\r\nSystem Manufacturer\tHewlett-Packard\r\nSystem Model\tHP EliteBook 840 G2\r\nSystem Type\tx64-based PC\r\nSystem SKU\tN2C61US#ABA\r\nProcessor\tIntel(R) Core(TM) i7-5600U CPU @ 2.60GHz, 2601 Mhz, 2 Core(s), 4 Logical Processor(s)\r\nBIOS Version/Date\tHewlett-Packard M71 Ver. 01.18, 9/26/2016\r\nSMBIOS Version\t2.7\r\nEmbedded Controller Version\t150.91\r\nBIOS Mode\tLegacy\r\nBaseBoard Manufacturer\tHewlett-Packard\r\nBaseBoard Product\t2216\r\nBaseBoard Version\tKBC Version 96.5B\r\nPlatform Role\tMobile\r\nSecure Boot State\tUnsupported\r\nPCR7 Configuration\tBinding Not Possible\r\nWindows Directory\tC:\\WINDOWS\r\nSystem Directory\tC:\\WINDOWS\\system32\r\nBoot Device\t\\Device\\HarddiskVolume1\r\nLocale\tIndia\r\nHardware Abstraction Layer\tVersion = \"10.0.19041.423\"\r\nUser Name\tDESKTOP-RAJ\\user\r\nTime Zone\tIndia Standard Time\r\nInstalled Physical Memory (RAM)\t8.00 GB\r\nTotal Physical Memory\t7.88 GB\r\nAvailable Physical Memory\t943 MB\r\nTotal Virtual Memory\t13.9 GB\r\nAvailable Virtual Memory\t2.67 GB\r\nPage File Space\t6.00 GB\r\nPage File\tC:\\pagefile.sys\r\nKernel DMA Protection\tOff\r\nVirtualization-based security\tNot enabled\r\nDevice Encryption Support\tReasons for failed automatic device encryption: PCR7 binding is not supported, Hardware Security Test Interface failed and device is not Modern Standby, Disabled by policy\r\nHyper-V - VM Monitor Mode Extensions\tYes\r\nHyper-V - Second Level Address Translation Extensions\tYes\r\nHyper-V - Virtualization Enabled in Firmware\tNo\r\nHyper-V - Data Execution Protection\tYes\r\n\r\nMSVC 2019 redistributable installed\r\n", "@rrajkumar77\r\nCan you please let me know if this is an anaconda installation? if yes, installation issues within the Anaconda environment are tracked in the [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues).\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "Yes, I use anaconda \r\n#12028 raised for anaconda issue through the link provided", "Moving this to closed status as issue is created in anaconda repo.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43033\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43033\">No</a>\n"]}, {"number": 43032, "title": " Neural machine translation with attention issue :Translate function give Keyerror with Tunisian dialect", "body": "\r\ni 'm working with attention model for Tunisian dialect _Standard arabic translation , with google colab ,\r\n\r\n![image](https://user-images.githubusercontent.com/53334878/92415130-6b1d1580-f14f-11ea-8ffd-c56e417a2901.png)\r\n", "comments": ["@zeyneb-chiha \r\n\r\nPlease, let us know which TF version you are using.\r\nPlease,fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I have the same issue, and I'm stuck at it!"]}, {"number": 43031, "title": "AttributeError: module 'tensorflow' has no attribute 'Dimension'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nMy list of packages:\r\n\r\nc:\\Dev\\Projects\\TooGAN\\stylegan>pip list\r\nPackage                  Version\r\n------------------------ ------------\r\nabsl-py                  0.9.0\r\nappdirs                  1.4.4\r\nasgiref                  3.2.5\r\nastor                    0.8.1\r\nastroid                  2.3.3\r\nattrs                    19.3.0\r\naudioread                2.1.8\r\nbackcall                 0.1.0\r\nbleach                   3.1.3\r\ncachetools               4.0.0\r\ncertifi                  2019.11.28\r\ncffi                     1.14.0\r\nchardet                  3.0.4\r\ncolorama                 0.4.3\r\ncv                       1.0.0\r\ncycler                   0.10.0\r\ndecorator                4.4.2\r\ndefusedxml               0.6.0\r\ndistlib                  0.3.1\r\nDjango                   3.0.4\r\ndjango-crispy-forms      1.9.0\r\ndnspython                1.16.0\r\ndtw                      1.4.0\r\nentrypoints              0.3\r\nfake-useragent           0.1.11\r\nffmpeg                   1.4\r\nfilelock                 3.0.12\r\ngast                     0.2.2\r\ngoogle-auth              1.11.3\r\ngoogle-auth-oauthlib     0.4.1\r\ngoogle-images-download   2.8.0\r\ngoogle-pasta             0.2.0\r\ngrpcio                   1.27.2\r\nh5py                     2.10.0\r\nidna                     2.9\r\nimageio                  2.8.0\r\nimportlib-metadata       1.5.0\r\nimutils                  0.5.3\r\nipykernel                5.1.4\r\nipython                  7.13.0\r\nipython-genutils         0.2.0\r\nipywidgets               7.5.1\r\nisort                    4.3.21\r\njedi                     0.16.0\r\nJinja2                   2.11.1\r\njoblib                   0.14.1\r\njsonschema               3.2.0\r\njupyter                  1.0.0\r\njupyter-client           6.1.0\r\njupyter-console          6.1.0\r\njupyter-core             4.6.3\r\nKeras-Applications       1.0.8\r\nKeras-Preprocessing      1.1.0\r\nkiwisolver               1.1.0\r\nlazy-object-proxy        1.4.3\r\nlibrosa                  0.7.2\r\nllvmlite                 0.31.0\r\nMarkdown                 3.2.1\r\nMarkupSafe               1.1.1\r\nmatplotlib               3.2.1\r\nmccabe                   0.6.1\r\nmido                     1.2.9\r\nmistune                  0.8.4\r\nmysql-connector-python   8.0.19\r\nmysqlclient              1.4.6\r\nnbconvert                5.6.1\r\nnbformat                 5.0.4\r\nnotebook                 6.0.3\r\nnumba                    0.48.0\r\nnumpy                    1.18.2\r\noauthlib                 3.1.0\r\nopencv-python            4.4.0.42\r\nopt-einsum               3.2.0\r\npandocfilters            1.4.2\r\nparso                    0.6.2\r\npickleshare              0.7.5\r\nPillow                   7.1.1\r\npip                      20.2.2\r\nprogressbar              2.5\r\nprometheus-client        0.7.1\r\nprompt-toolkit           3.0.4\r\nprotobuf                 3.11.3\r\npyasn1                   0.4.8\r\npyasn1-modules           0.2.8\r\npycparser                2.20\r\npydub                    0.23.1\r\nPygments                 2.6.1\r\npylint                   2.4.4\r\npyparsing                2.4.6\r\npyrsistent               0.15.7\r\npython-dateutil          2.8.1\r\npytz                     2019.3\r\npywin32                  227\r\npywinpty                 0.5.7\r\nPyYAML                   5.3.1\r\npyzmq                    19.0.0\r\nqtconsole                4.7.1\r\nQtPy                     1.9.0\r\nrequests                 2.23.0\r\nrequests-oauthlib        1.3.0\r\nresampy                  0.2.2\r\nrsa                      4.0\r\nscikit-learn             0.22.2.post1\r\nscipy                    1.4.1\r\nselenium                 3.141.0\r\nSend2Trash               1.5.0\r\nsetuptools               41.2.0\r\nsix                      1.14.0\r\nSoundFile                0.10.3.post1\r\nSpeechRecognition        3.8.1\r\nsqlparse                 0.3.1\r\ntensorboard              2.1.1\r\ntensorflow-estimator     2.1.0\r\ntensorflow-gpu           2.1.0\r\ntensorflow-gpu-estimator 2.1.0\r\ntermcolor                1.1.0\r\nterminado                0.8.3\r\ntestpath                 0.4.4\r\ntornado                  6.0.4\r\ntraitlets                4.3.3\r\ntyped-ast                1.4.1\r\nurllib3                  1.25.8\r\nvirtualenv               20.0.31\r\nwcwidth                  0.1.8\r\nwebencodings             0.5.1\r\nWerkzeug                 1.0.0\r\nwheel                    0.34.2\r\nwidgetsnbextension       3.5.1\r\nwrapt                    1.11.2\r\nzipp                     3.1.0\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI am trying to run StyleGan library\r\n\r\nhttps://github.com/NVlabs/stylegan\r\n\r\nRunning \"python pretrained_example.py\"\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Cause `tf.Dimension` is deprecated and is `tf.compat.v1.Dimension`", "Also generally (NVlabs) Stylegan and Stylegan2 require `TensorFlow 1.14 or 1.15 with GPU support.`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43030, "title": "Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Education\r\n- TensorFlow version: tf-nightly-2.4.0.dev20200907\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 11.0/8.0.2\r\n- GPU model and memory: GeForce GTX 1050, 4GB\r\n\r\n**Describe the problem**\r\ncupti64_110.dll cannot be found, and the only CUPTI .dll file I can find in the CUDA 11.0 files is C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\extras\\CUPTI\\lib64\\cupti64_2020.1.1.dll\r\n\r\nAm I missing some files/part of the installation? \r\n\r\nThanks\r\n", "comments": ["I suppose CUDA 11.0 Update 1 renamed the library but TF code it is not ready.", "Check https://github.com/JuliaGPU/CUDA.jl/issues/300", "I also encountered this problem", "add this in your code\r\n`import tensorflow as tf`\r\n`config = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))`\r\n`sess = tf.compat.v1.Session(config=config)`", "This is a known issue in TF that was caught very late in the 2.4 release process.  So for TF 2.4 you'll have to use a workaround -- copy cupti64_2020.1.1.dll to cupti64_110.dll to use the profiler on Windows.  We'll fix this for TF 2.5.", "can confirm solution works, thank you @sanjoy !", "Can you please give a link to the file ", "@king398: as @sanjoy said, you can create a copy of the file \"cupti64_2020.1.1.dll\" (which is already in the folder CUPTI) and then change the file's name to \"cupti64_110.dll \"\r\n", "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 2.5\r\n- Python version: 3.9.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 11.3/8.2\r\n- GPU model and memory: GeForce GTX 1070\r\n\r\nI changed the name \"cupti64_2021.1.1.0.dll\" to \"cupti64_110.dll \" but now I am getting\r\n```\r\n2021-05-13 23:40:36.457597: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti64_112.dll'; dlerror: cupti64_112.dll not found\r\n2021-05-13 23:40:36.458637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n2021-05-13 23:40:36.458923: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1661] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-05-13 23:40:36.459260: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\r\n2021-05-13 23:40:36.459430: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1752] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\r\n```", "I fixed it by copying all the \"cupti64_*.dll\" from \"C:\\Program Files\\NVIDIA Corporation\\Nsight Systems 2021.1.3\\target-windows-x64\" into \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\extras\\CUPTI\\lib64\" \r\nFound it in https://stackoverflow.com/questions/56860180/tensorflow-cuda-cupti-error-cupti-could-not-be-loaded-or-symbol-could-not-be/65847699#65847699", "Had the same issue @jairoMolina9, tried what you suggested but it didn't work. I also added the CUPTI/lib64 directory to PATH, still the same thing. \r\n\r\nWindows 10\r\nPython 3.8.5 \r\nTensorflow 2.5.0\r\nCuda 11.2\r\n\r\nI'm having a hard time checking the cudnn version though.", "> Had the same issue @jairoMolina9, tried what you suggested but it didn't work. I also added the CUPTI/lib64 directory to PATH, still the same thing. \r\n> \r\n> \r\n> \r\n> Windows 10\r\n> \r\n> Python 3.8.5 \r\n> \r\n> Tensorflow 2.5.0\r\n> \r\n> Cuda 11.2\r\n> \r\n> \r\n> \r\n> I'm having a hard time checking the cudnn version though.\r\n\r\nHaving the exact same issue except that my Python is 3.7 and not 3.8\r\nMy CUDNN folder was cudnn-11.2-windows-x64-v8.1.1.33 ", "I have the same issue:\r\nWindows 10\r\nPython 3.6.1\r\nTensorFlow 2.5.0-gpu\r\nCUDA 11.2\r\ncuDNN 8.1\r\n\r\n```\r\n2021-05-28 09:23:53.958843: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 1 GPUs\r\n2021-05-28 09:23:53.960936: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti64_112.dll'; dlerror: cupti64_112.dll not found\r\n2021-05-28 09:23:53.961837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n2021-05-28 09:23:53.962304: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1661] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-05-28 09:23:54.039846: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\r\n2021-05-28 09:23:54.040074: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1752] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-05-28 09:23:54.052862: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \r\n2021-05-28 09:23:54.057927: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\r\n```", "still having this issue", "As a Windows User, I solved this issue by using a Python installed from [the official repository](https://www.python.org/downloads/). Previously my Python was installed (naively) using the Microsoft Store. \r\n\r\nAs these were the exact same versions for Windows I can't explain why. My guess would be that the Python installed by the Microsoft store wasn't given enough permission to see the dlls maybe ?", "> As a Windows User, I solved this issue by using a Python installed from the official repository. Previously my Python was installed (naively) using the Microsoft Store.\r\nAs these were the exact same versions for Windows I can't explain why. My guess would be that the Python installed by the Microsoft store wasn't given enough permission to see the dlls maybe ?\r\n\r\nYes you will find this reported in multiple ticket, you need to use python from the official repository. /cc @MarkDaoust Do we have this info somewhere in the official Doc?\r\n\r\n", "Not that I know of.  It seems like the relevant page would be: https://www.tensorflow.org/install/pip\r\n\r\nOr under \"windows setup\" on the install GPU page: https://www.tensorflow.org/install/gpu#windows_setup\r\n\r\nI'm not sure I understand the problem well enough to suggest an update. What's this \"official repository\"?\r\n\r\n@lamberta owns the install pages", "@MarkDaoust see https://github.com/tensorflow/tensorflow/issues/36111#issuecomment-619156137", "> \r\n> \r\n> I have the same issue:\r\n> Windows 10\r\n> Python 3.6.1\r\n> TensorFlow 2.5.0-gpu\r\n> CUDA 11.2\r\n> cuDNN 8.1\r\n> \r\n> ```\r\n> 2021-05-28 09:23:53.958843: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 1 GPUs\r\n> 2021-05-28 09:23:53.960936: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti64_112.dll'; dlerror: cupti64_112.dll not found\r\n> 2021-05-28 09:23:53.961837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n> 2021-05-28 09:23:53.962304: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1661] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n> 2021-05-28 09:23:54.039846: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\r\n> 2021-05-28 09:23:54.040074: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1752] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\r\n> 2021-05-28 09:23:54.052862: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 0 callback api events and 0 activity events. \r\n> 2021-05-28 09:23:54.057927: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\r\n> ```\r\n\r\nI had an almost identical setup and the dll is still named cupti64_2020.3.1.dll  \r\nHowever, the renaming \"trick\" works  cupti64_2020.3.1.dll  -> cupti64_112.dll  \r\nI put all the files from the \"Extras\" folder in the proper Cuda (lib to lib, include to include etc...)  rather than add to my path.\r\nWin 10\r\nTF-gpu 2.5.0\r\nCuDNN 8.1.1\r\nCUDA 11.2.1\r\npython 3.8.7", "```\r\n2021-07-26 19:00:59.081093: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 1 GPUs\r\n2021-07-26 19:00:59.086072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti64_112.dll'; dlerror: cupti64_112.dll not found\r\n2021-07-26 19:00:59.092193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n2021-07-26 19:00:59.097167: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1661] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-07-26 19:01:00.126079: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\r\n20\r\n```\r\n\r\n\r\nas suggested by @rdslater, renaming `cupti64_2021.1.1.dll -> cupti64_112.dll` worked for me as well.\r\n![Capture](https://user-images.githubusercontent.com/7473349/127002211-32ad4adf-7db5-473a-aa93-16b8ed9be684.PNG)\r\n\r\nTf 2.5 GPU\r\nWindows 10\r\nCUDA 11.3\r\n\r\n", "TF 2.5 GPU\r\nCUDA v11.4\r\nWindows 10\r\nCuda compilation tools V11.4.48\r\n\r\nI renamed cupti64_2021.2.0.dll into cupti64_112.dll\r\nand do not have the error:\r\n2021-08-06 10:30:20.552092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cupti64_112.dll'; dlerror: cupti64_112.dll not found\r\n\r\nYet, I still have the following:\r\ncupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-08-06 10:41:41.747407: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\r\n2021-08-06 10:41:41.747421: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1752] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.", "My settings: \r\n   - Wins 10\r\n   - TF 2.5\r\n   - CUDA v11.1\r\n   - GPU GTX 3090\r\n\r\nI have tried several methods but it still did not work. Then, I tried using the \"cudatoolkit\" and \"cudnn\" from conda and it works.\r\nYou can try by opening the terminal of your virtual environment and type:\r\n`conda install cudatoolkit`\r\n`conda install cudnn`\r\n\r\nI know that it is not the absolute solution, but hope it works for you\r\n", "@thunguyenth\r\nthanks for your answer but only problems with CUPTI not loading some libraries for tensorboard.\r\n\r\nIn any case, I solved it: the path to C:\\Program Files\\NVIDIA Corporation\\Nsight Systems 2021.2.4\\target-windows-x64\r\nwas not set while there was a path to\r\nC:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2021.2.0\r\nSo I added the path to \r\nC:\\Program Files\\NVIDIA Corporation\\Nsight Systems 2021.2.4\\target-windows-x64\r\n", "Hi, I had **the exact same issue** as @zainulabidin302 suffered from the cupti dll issue. \r\n\r\nMy configuration \r\n```\r\n- Tensor 2.6\r\n- CUDA 11.4\r\n- Nvidia Geforce 1050 (not ti)\r\n- Windows 10 \r\n- Python 3.8.8 (Setup with Anaconda)\r\n- Visual Studio Code\r\n```\r\n\r\nHere is what helps me, \r\n\r\n1.  Add path `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\extras\\CUPTI\\lib64` to the System path as below \r\n- ![image](https://user-images.githubusercontent.com/49303847/136801826-ba488fab-650d-495a-9fda-08ff93654f76.png)\r\n\r\n2. Rename `cupti65_2022_xx.dll` to `cupti64_112.dll`\r\n\r\n![image](https://user-images.githubusercontent.com/49303847/136802076-f72ef341-8950-46ba-bf3c-4d5a508e3791.png)\r\n\r\n3. **Make sure to reopen your code editor.** and build your code to see if there is any error occured. \r\n\r\n", "I can confirm @rjtp5670 's advice works. Thanks for the help. \r\n", "> at was caught very late in the 2.4 release process. So for TF 2.4 you'll have to use a workaround -- copy cupti64_2020.1.1.dll to cupti64_110.dll to use the profiler on Windows. We'll\r\n\r\nI have used TF 2.5 but it did not fix yet", "\u60a8\u597d\uff0c\u60a8\u7684\u90ae\u4ef6\u5df2\u6536\u5230\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u6d59\u6c5f\u5927\u5b66\u2014\u2014\u97e9\u5174\u4f73", "@IchiruTake,\r\nRenaming `cupti64_2020.1.1.dll` -> `cupti64_112.dll`  should solve your proble. Thanks!", "\u60a8\u597d\uff0c\u60a8\u7684\u90ae\u4ef6\u5df2\u6536\u5230\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u6d59\u6c5f\u5927\u5b66\u2014\u2014\u97e9\u5174\u4f73", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43030\">No</a>\n"]}, {"number": 43029, "title": "TensorFlow2 tutorials Actor-Critic method", "body": "## URL(s) with the issue:\r\n\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nAt the second cell, we want to install additional packages for visualization.\r\nMaybe should be added \"sudo apt-get update\" before the line \"sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\".\r\nLet the xvfb package can be installed normally.\r\n\r\n### Usage example\r\nCorrect the second cell as below:\r\n%%bash\r\n\r\nsudo apt-get update\r\nsudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\r\npip install pyvirtualdisplay > /dev/null 2>&1\r\npip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\r\n", "comments": ["Closing this issue now since the colab executes successfully end-to-end.\r\nFor more info see the the associated PR thread. Thank you."]}, {"number": 43028, "title": "Installation error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: I use PC\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: \r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: NVIDIA GeForce GTX 750 Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands/steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info/logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n--MY QUESTION STARTS HERE--\r\nWhenever I do  pip install tensor flow, it runs code then it shows this\r\n\r\nInstalling collected packages: tensorflow\r\nERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\minad\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python38\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\client_channel\\\\lb_policy\\\\grpclb\\\\client_load_reporting_filter.h'\r\n\r\nplease help", "comments": ["From the stack trace it looks like you are hitting windows path length limit.\n   * Try to disable path length limit on Windows 10.\n     * Refer [disable path length limit instructions guide.](https://mspoweruser.com/ntfs-260-character-windows-10/)\n\nPlease let us know if this helps.\n", "@XDKaioden,\r\nPlease take a look at the above comment by tensorflow-butler and also these similar StackOverflow issues [issue#1](https://stackoverflow.com/a/63712528) and [issue#2](https://stackoverflow.com/a/63438359) and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43028\">No</a>\n", "I have the same problem, my python binary has been installed from the windows store and I have the path length limit disabled.\r\n\r\nOS Platform and Distribution: _Windows 11 21H2_\r\nDevice: _PC_\r\nTensorFlow version: _2.7.0_\r\nPython version: _3.9.8_\r\nInstalled using..?: _pip 21.3.1_\r\nCUDA/cuDNN version: _N/A_\r\nGPU model: NVIDIA GeForce RTX 2080 Super"]}, {"number": 43027, "title": "Please update the tflite_convert -h command line help ", "body": "What I see when I run \r\n\r\ntflite_convert -h\r\n\r\n\r\n```\r\nusage: tflite_convert [-h] --output_file OUTPUT_FILE [--saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE]\r\n                      [--enable_v1_converter] [--experimental_new_converter [EXPERIMENTAL_NEW_CONVERTER]]\r\n\r\nCommand line tool to run TensorFlow Lite Converter.\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  --output_file OUTPUT_FILE\r\n                        Full filepath of the output file.\r\n  --saved_model_dir SAVED_MODEL_DIR\r\n                        Full path of the directory containing the SavedModel.\r\n  --keras_model_file KERAS_MODEL_FILE\r\n                        Full filepath of HDF5 file containing tf.Keras model.\r\n  --enable_v1_converter\r\n                        Enables the TensorFlow V1 converter in 2.0\r\n  --experimental_new_converter [EXPERIMENTAL_NEW_CONVERTER]\r\n                        Experimental flag, subject to change. Enables MLIR-based conversion instead of TOCO conversion. (default\r\n                        True)\r\n```\r\n\r\n\r\nand what I see from python is \r\n\r\n```\r\n\r\nusage: tflite_convert.py [-h] --output_file OUTPUT_FILE\r\n                          (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)\r\n                          [--output_format {TFLITE,GRAPHVIZ_DOT}]\r\n                          [--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n                          [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\r\n                          [--input_arrays INPUT_ARRAYS]\r\n                          [--input_shapes INPUT_SHAPES]\r\n                          [--output_arrays OUTPUT_ARRAYS]\r\n                          [--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n                          [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\r\n                          [--std_dev_values STD_DEV_VALUES]\r\n                          [--mean_values MEAN_VALUES]\r\n                          [--default_ranges_min DEFAULT_RANGES_MIN]\r\n                          [--default_ranges_max DEFAULT_RANGES_MAX]\r\n                          [--drop_control_dependency DROP_CONTROL_DEPENDENCY]\r\n                          [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]\r\n                          [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]\r\n                          [--allow_custom_ops ALLOW_CUSTOM_OPS]\r\n\r\n```\r\n\r\n\r\nIf the above is available for tflite_convert using the command line could the help file reflect that ability.\r\n\r\n", "comments": ["FYI, the first help message of the command line `tflite_convert` is coming from TF v2 while the later help message is coming from TF v1. They co-exist in the TF v2.x package.", "> FYI, the first help message of the command line `tflite_convert` is coming from TF v2 while the later help message is coming from TF v1. They co-exist in the TF v2.x package.\r\n\r\nSo do the command line quantized commands only work with TF V1 or can they still be used with TF 2?", "We actually recommend using TFLite converter Python API for either version, which supports quantization features. https://www.tensorflow.org/lite/convert/quantization", "> We actually recommend using TFLite converter Python API for either version, which supports quantization features. https://www.tensorflow.org/lite/convert/quantization\r\n\r\nI agree, but I am trying something different. We can close this issue if you wish. I will take it up on stack overflow. Any idea why Tf downplays the command line. So many powerful things can be done with inputs to a bash file. ", "@hpssjellis Could you leave the stack overflow link in here after you uploaded it? Thanks.", "Here is my stack overflow question\r\n\r\n[https://stackoverflow.com/questions/63785445/please-post-working-tflite-convert-command-line-examples](https://stackoverflow.com/questions/63785445/please-post-working-tflite-convert-command-line-examples)\r\n\r\n", "You will be able to run V1 converter command line when you set the environment variable like the below:\r\n\r\nTF2_BEHAVIOR=0 tflite_convert ...", "@hpssjellis \r\nplease update as per above comment, along with the tf version used.", "Thanks @Saduf2019 . I have no interest running V1 I am only interested in V2. I would like to see help files for running tflite_convert using the command line. The typical process is to use Python, but I come from a Tensorflowjs background teaching High School students and only wish to use the command line or Javascript. I am having a lot of success moving TensorflowJS Version 2 models to TensorFlowLite then TensorflowMicro, before running them on an Arduino but I have not been able to quantize the models either using [tensorflowjs_converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter) or using tflite_convert.\r\n\r\nAny help would be appreciated. Command line examples would be a sensible addition.", "HI @jvishnuvardhan would you like some clarification of the issue?", "@hpssjellis The [TFLite Converter documentation](https://www.tensorflow.org/lite/convert) has been recently updated and we recommend using the Python API (simple conversions are still possible using the command line) for quantization related tooling. We supported quantization via command line in TF1 but have deprecated/removed this in TF2.\r\n\r\n[TF2 Python API](https://www.tensorflow.org/lite/convert#python_api_):\r\nFor TF1 models, use the ` tf.compat.v1.lite.TFLiteConverter` API. You can use [colab.research.google.com](colab.research.google.com) for free to quickly upload your model, convert, and download the converted TFLite model. During conversion, you can perform [post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization).\r\n\r\n[TF2 CommandLine tool](https://www.tensorflow.org/lite/convert#command_line_tool_):\r\n*Assumption: TensorFlow 2.x has been installed via pip*\r\nFor TF1 models, you can use the `enable_v1_converter` flag as follows:\r\n```\r\n$ tflite_convert --enable_v1_converter \\\r\n--graph_def_file=/tmp/frozen_cifarnet.pb \\\r\n--output_file=/tmp/quantized_cifarnet.tflite \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--output_arrays=CifarNet/Predictions/Softmax \\\r\n--input_arrays=input \\\r\n--mean_values 121 \\\r\n--std_dev_values 64\r\n```\r\nTo see what flags you can use and how to use them, refer to the documentation [cmdline_reference.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_reference.md), and [cmdline_examples.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/cmdline_examples.md) \r\n\r\nFinally, we are looking into the issue you pointed out in the first comment (discrepancy between `python ... tflite_convert` and `tflite_convert` help output) and will resolve it soon. Thanks for pointing it out! :)\r\n\r\nMarking this issue as resolved. Feel free to open it again it's unresolved."]}, {"number": 43026, "title": "ImportError: DLL load failed: The specified procedure could not be found.", "body": "\r\n**System information**\r\nOS Name\tMicrosoft Windows 10 Pro\r\nVersion\t10.0.17134 Build 17134\r\nSystem Type\tx64-based PC\r\n\r\n**Describe the problem**\r\nI was following this [guide](https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606)\r\nSo I installed the dependencies in the terminal:\r\n\r\n`pip3 install tensorflow==1.13.1 `\r\n`pip3 install opencv-python`\r\n`pip3 install keras==2.2.4`\r\n`pip3 install numpy==1.16.1`\r\n`pip3 install imageai --upgrade`\r\n\r\n**Python Version:**\r\n```\r\nC:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36>python --version\r\nPython 3.6.0\r\n```\r\n**Some more info:**\r\n```\r\nC:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36>python -m pip list\r\nPackage              Version\r\n-------------------- ---------\r\nabsl-py              0.10.0\r\nastor                0.8.1\r\ncertifi              2020.6.20\r\ncycler               0.10.0\r\ngast                 0.4.0\r\ngoogle-pasta         0.2.0\r\ngrpcio               1.31.0\r\nh5py                 2.10.0\r\nimageai              2.1.5\r\nimportlib-metadata   1.7.0\r\nKeras                2.2.4\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.2\r\nkiwisolver           1.2.0\r\nMarkdown             3.2.2\r\nmatplotlib           3.3.1\r\nmock                 4.0.2\r\nnumpy                1.16.1\r\nopencv-python        4.4.0.42\r\nPillow               7.2.0\r\npip                  20.2.2\r\nprotobuf             3.13.0\r\npyparsing            2.4.7\r\npython-dateutil      2.8.1\r\nPyYAML               5.3.1\r\nscipy                1.5.2\r\nsetuptools           50.3.0\r\nsix                  1.15.0\r\ntensorboard          1.13.1\r\ntensorflow           1.13.1\r\ntensorflow-estimator 1.13.0\r\ntermcolor            1.1.0\r\nWerkzeug             1.0.1\r\nwheel                0.35.1\r\nwrapt                1.12.1\r\nzipp                 3.1.0\r\n```\r\n**The code Im running** (the one on the guide i mentioned):\r\n\r\n```\r\nfrom imageai.Detection import ObjectDetection\r\nimport os\r\n\r\nexecution_path = os.getcwd()\r\n\r\ndetector = ObjectDetection()\r\ndetector.setModelTypeAsRetinaNet()\r\ndetector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\r\ndetector.loadModel()\r\ndetections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , \"image.jpg\"), output_image_path=os.path.join(execution_path , \"imagenew.jpg\"))\r\n\r\nfor eachObject in detections:\r\n    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )\r\n```\r\n\r\n**The error I get:**\r\n```\r\n\r\nC:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\Scripts\\python.exe C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\document_extraction.py\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\document_extraction.py\", line 1, in <module>\r\n    from imageai.Detection import ObjectDetection\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\imageai\\Detection\\__init__.py\", line 2, in <module>\r\n    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\imageai\\Detection\\keras_retinanet\\models\\resnet.py\", line 19, in <module>\r\n    import keras\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\danieltsilva\\Documents\\Python\\document_segmentaton36\\venv\\new_env\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 48, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\nI don\u00b4t really know what's failing. If there is a similar issue pls hit me up (i haven\u00b4t found it), or if you have any tip tho share with me. Thank you <3", "comments": ["@danieltarita \r\n\r\nCan you please refer the [link](https://stackoverflow.com/questions/52092810/tensorflow-error-dll-load-failed-the-specified-procedure-could-not-be-found) and see if it helps you. Thanks!", "Thank you, it worked! Problem solved.\r\nJust had to run\r\n`pip install protobuf==3.6.0`", "@danieltarita \r\n\r\nPlease, close this thread as your issue was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43026\">No</a>\n"]}, {"number": 43024, "title": "[mlir] Update the usage of buffer placement", "body": "This PR prepares Tensorflow for down streaming of [recent changes in mlir:BufferPlacement](https://reviews.llvm.org/D87079).", "comments": ["This landed as part of revision f84dfcb7be1db6cc5e001a8537448847e82ef45b as it had to be merged atomically with an LLVM integrate."]}, {"number": 43023, "title": "Clarified the DispatchServer creation process in tf.data distribute service docs", "body": "This PR adds the documentation to give the user some clarity on the `DisptachServer` creation process while using the `tf.data.experimental.service.distribute` API.", "comments": ["@kvignesh1420  Can you please resolve conflicts? Thanks!", "@gbaned there was a small error while resolving the conflicts, can we delete the request for review by:\r\nalextp, annarev, caisq, jaingaurav and MarkDaoust? ", "Something went wrong in that merge. This is registering as 500+ files changed. We can't merge it like that. \r\n\r\nYou probably want to reapply your changes directly onto  `master`.  You can probably just cherry-pick d96e762", "Yes, fixing it now.", "@gbaned @MarkDaoust fixed it. Sorry for the trouble!", "cc: @rachellim @MarkDaoust, pinging for review. Let me know, thanks.", "@rachellim, this seems like a big/important enough topic that we should have a notebook guide for it on tensorflow.org, is someone working on that? That could cover all the higher level questions that don't quite fit in a docstring. It could also clarify the the relationship between this and the [distributed-input totorial](https://www.tensorflow.org/tutorials/distribute/input). WDYT? \r\n\r\n@kvignesh1420, thanks for the PR, and thanks for the ping.\r\n\r\nThis looks good to me since it makes the example code much more **runnable**.\r\n\r\nIt would be good to include two workers there so that you can see that something different is happening.\r\n\r\n```\r\nworker1 = tf.data.experimental.service.WorkerServer(\r\n           port=5001, dispatcher_address=dispatcher_address)\r\n\r\nworker2 = tf.data.experimental.service.WorkerServer(\r\n           port=5002, dispatcher_address=dispatcher_address)\r\n```\r\n\r\nIf you also need to update the distribute service address on both examples to `grpc://localhost:5000`. I don't think this is how you would typcally **use** it, so a note explaining that would be good.\r\n\r\nIf you can make it runnable we may be able to use doctest format, `>>>`, instead of code fences, ` ``` `, so that this code gets tested. \r\n\r\nThen you could sort the dataset and show the result:\r\n\r\n```\r\n>>> sorted(dataset)\r\n[<tf.Tensor: shape=(), dtype=int64, numpy=1>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=1>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=2>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=2>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=5>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=5>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=10>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=10>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=17>,\r\n <tf.Tensor: shape=(), dtype=int64, numpy=17>]\r\n```", "Thanks for the ping. @aaudiber - can you review?", "@aaudiber, I have made the requested changes. Also as @MarkDaoust mentioned, a working example which demonstrates the integration of `tf.distribute.Strategy.experimental_distribute_dataset` with a dataset created using `tf.data.experimental.service.distribute` seems to be missing. We have a [docstring](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/distribute), but I believe it's not sufficient for the users to understand.\r\n\r\nLet me know if this is already in progress, else I can try to create a notebook and we can review the PR for a better way of demonstrating the functionality.", "@aaudiber made the changes. Also, thanks for the info."]}, {"number": 43022, "title": "Building and testing CPU Ops, ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.5\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\nIn Custom Ops, I'm trying to build the example files. When I run the build commands I just get an error saying:\r\n\r\n\"Executing genrule @local_config_tf//:tf_header_include failed (Exit 35584)\"\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n$ ./configure.sh\r\nDo you want to build ops again TensorFlow CPU pip package? Y or enter for CPU (tensorflow), N for GPU (tensorflow-gpu). [Y/n] y\r\nBuild with CPU pip package.\r\nOn windows, skipping toolchain flags..\r\nAre you building against TensorFlow 2.1(including RCs) or newer?[Y/n] y\r\nBuild against TensorFlow 2.1 or newer.\r\nUsing installed tensorflow\r\n2020-09-07 15:46:38.915193: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-07 15:46:38.915211: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-09-07 15:46:40.468711: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-07 15:46:40.468729: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\n$ bazel build tensorflow_zero_out:python/ops/_zero_out_ops.so\r\nLoading:\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow_zero_out:python/ops/_zero_out_ops.so (0 packages loaded, 0 targets configured)\r\nINFO: Analyzed target //tensorflow_zero_out:python/ops/_zero_out_ops.so (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\n[0 / 5] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\nERROR: C:/users/patrik.veges/_bazel_patrik.veges/gdil2b47/external/local_config_tf/BUILD:17:8: Executing genrule @local_config_tf//:tf_header_include failed (Exit 35584)\r\n      0 [] bash 287 cygwin_exception::open_stackdumpfile: Dumping stack trace to bash.exe.stackdump\r\nTarget //tensorflow_zero_out:python/ops/_zero_out_ops.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: C:/users/patrik.veges/_bazel_patrik.veges/gdil2b47/external/local_config_tf/BUILD:3:11 Executing genrule @local_config_tf//:tf_header_include failed (Exit 35584)\r\nINFO: Elapsed time: 0.899s, Critical Path: 0.80s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n-\r\n", "comments": ["@Patrikvs94 \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/29831#issuecomment-504704980) and let us know if it helps.\r\nverify with existing issues:\r\n#40804 #36111 #31018 [for cuda version] #42436 [without gpu]", "I'm not following why the cudart64.dll files would cause my issues. Why would that affect it? It feels like there's something off about the include stuff. I'm trying to do this with CPU only.", "@Patrikvs94 are your Nvidia and CUDA requirements satisfied? https://www.tensorflow.org/install/gpu#software_requirements", "Do they need to be satisfied if I'm only doing CPU only stuff?", "How did you install Tensorflow?", "I just did a \"$pip install tensorflow\" and it installed tensorflow 2.3\r\n", "Can you try instead with `pip install tensorflow-cpu`?", "I did, but the configure file from the \"Custom Ops\" repository actually installs tensorflow for me. (Aka Tensorflow 2.3)", "Yes there is a bug with new package naming  at https://github.com/tensorflow/custom-op/blob/master/configure.sh#L89.\n\nCan you close this ticket and open a new one on that repository?\n\nThanks.", "Please close this it is already handled in https://github.com/tensorflow/custom-op/issues/74", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43022\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43022\">No</a>\n"]}, {"number": 43021, "title": "TF_TensorToPyArray is hard coded for equality, limits performance enhancements ", "body": "Hi,\r\n\r\nI am running one CNN model which has convolution and FC layers. \r\nFor getting optimal performance i am using buffers allocated with allocate_persistent() API and I am reusing these buffers multiple times as Network is pretty sequential. \r\nFor achieving this i pre-allocate big chunks with allocate_persistent() API and for every Op execution i use the same buffers by setting with different shape. \r\n\r\nWith convolutions it worked fine and i see some performance gains. However once FC layer gets executed it throws below exception. Because of this exception, I am not able to run my network. \r\n\r\n_INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes\r\nI0907 13:07:13.556839 140574226294592 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes_\r\n\r\nAfter looking into code, I found function **TF_TensorToPyArray()** gets called, which compare the ndarray bytes with TF_tensor bytes, which is fine, But why does it need to be exact match?\r\nIt should have been below comparison for throwing error, instead of exact match.  \r\n_PyArray_NBYTES(py_array)) >\r\n             TF_TensorByteSize(tensor.get()_\r\n\r\nThis way same buffers can be used for Op execution, just we need to change the shape.\r\n\r\nFor experiment i changed \r\nset_shape(const TensorShape& shape)  to Public scope, looking for some alternate-native for this too. ", "comments": ["@avinashcpandey \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "I created new issue with right template and shared detailed info.\r\nhttps://github.com/tensorflow/tensorflow/issues/43044\r\n\r\nWe can close this and work on that.", "@avinashcpandey \r\n\r\nAs per your comments i am closing this issue. We can track the issue in #43044. Thanks!"]}, {"number": 43020, "title": "Arduino tensorflow", "body": "- Mac OS Catalina Version 10.15.6\r\n- Arduino 1.8.13\r\n- Tensorflow Library \"Arduino_TensorFlowLite\" version 2.1.0-ALPHA-precompiled\r\n- Arduino Nano BLE 33\r\n\r\nFollowing the Arduino instructions as listed on the [tensorflow github](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection#running-on-arduino\r\n) the default person detection demonstration fails on compilation looking for `cmsis_compiler.h`\r\n\r\nI was able to simply copy v5.01 of the `cmsis_compiler_h` to the `../Arduino_TensorFlowLite/src/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include` directory and the demonstration compiled successfully and worked.\r\n", "comments": ["@LiamRandall It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.5 and let us know if the issue still persists? Thanks!", "Yes, I will try to complete this with the latest version and confirm that this works as expected.", "@LiamRandall Could you please let us know if you have tried in the latest stable version of TF 2.6.0 and if the issue still persists ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43020\">No</a>\n"]}, {"number": 43019, "title": "action_env ignored in some cc_library calls", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): 8.3\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build TensorFlow with TF_SYSTEM_LIBS and need to set `$CPATH` via `--action_env`. However for some instances of `cc_library` this env variable is not passed through to the compiler invocation.   \r\nI observed the issue for tensorflow/python/tools/BUILD:226:10 C++ compilation of rule '//tensorflow/core/platform/default:mutex'. The definition for that is at https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/platform/default/BUILD#L206\r\n\r\nLog output:\r\n\r\n```\r\n  (cd /tmp/easybuild-tmp/eb-tHWBp_/tmphugHLy-bazel-build/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=<redacted> \\\r\n    PATH=<redacted> \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/ppc-opt-exec-50AE0418/bin/tensorflow/core/platform/default/_objs/mutex/mutex.d '-frandom-seed=bazel-out/ppc-opt-exec-50AE0418/bin/tensorflow/core/platform/default/_objs/mutex/mutex.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/ppc-opt-exec-50AE0418/bin -iquote external/com_google_absl -iquote bazel-out/ppc-opt-exec-50AE0418/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/ppc-opt-exec-50AE0418/bin/external/nsync -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 '-std=c++14' -c tensorflow/core/platform/default/mutex.cc -o bazel-out/ppc-opt-exec-50AE0418/bin/tensorflow/core/platform/default/_objs/mutex/mutex.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\n```\r\n\r\nHowever it doesn't always occur:  E.g. for `SUBCOMMAND: # @com_google_absl//absl/strings:strings [action 'Compiling external/com_google_absl/absl/strings/internal/memutil.cc', configuration: cab848d308c51b34c791977a9ba0d73e541cabe37f3e6da7b163b34d8bf29b6b, execution platform: @local_execution_config_platform//:platform]` I see all my action_env variables being passed and that is also added with cc_library: https://github.com/abseil/abseil-cpp/blob/df3ea785d8c30a9503321a3d35ee7d35808f190d/absl/strings/BUILD.bazel#L30\r\n\r\nCross-posting this from https://github.com/bazelbuild/bazel/issues/12059 as I'm not sure this is a Bazel or TF issue as it seems to only happens sometimes I'd guess it is on the TF side.\r\n\r\nSame issue (I guess) reported at https://github.com/tensorflow/tensorflow/issues/37861#issuecomment-686418236\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- `TF_SYSTEM_LIBS=icu,jsoncpp_git,lmdb,nasm,pcre,org_sqlite,swig,curl,double_conversion,flatbuffers,gif,hwloc,libjpeg_turbo,png,nsync,com_google_protobuf,pybind11,snappy,zlib,absl_py,astor_archive,astunparse_archive,cython,enum34_archive,functools32_archive,gast_archive,opt_einsum_archive,pasta,six_archive,termcolor_archive,wrapt\" ./configure`\r\n- `bazel --output_base=<...> --install_base=/<...> --output_user_root=<...> build --compilation_mode=opt --config=opt --subcommands --verbose_failures --config=noaws --jobs=64 --copt=\"-fPIC\" --action_env=CPATH='<...>' --action_env=LIBRARY_PATH='<...>' --action_env=PYTHONPATH --action_env=PYTHONNOUSERSITE=1 --distinct_host_configuration=false  //tensorflow/tools/pip_package:build_pip_package`", "comments": ["/cc @angerson ", "Tagging @perfinion", "It is probably upstream as it seems connected also to https://github.com/bazelbuild/bazel/issues/12049.\r\nWe could evaluate to close this.", "I'm not fully convinced it is an upstream (Bazel) issue because many actions (is this the right term?) do use the action_env. All use the (seemingly) same cc_library macro(?) but some don't get my variables. I don't really understand the Bazel architecture (and frankly I don't like the \"clean env\" decision at all) so I'm asking if there is anything in TF that might change the behavior of those cc_library invocations? Is there something like a \"toolchain\" or so which gets set for certain folders which makes subsequent rules ignore action_env? Some override of cc_library?", "I just patched the TF sources to allow Bazel 2.0.0 and tried that again (worked for 2.2.0) and it still fails. So that makes it very much look like something in TF changed which causes the new behavior", "I bisected the the TF changes between 2.2 and 2.3 and found the commit which introduced the failure: https://github.com/tensorflow/tensorflow/commit/f827c023906e7d30f0e5f2992b111ab34153310a\r\n\r\nLooks like exec_tools is worse than tools. Could that be reverted? From https://docs.bazel.build/versions/master/be/general.html#genrule.exec_tools it sounds like a good idea anyway:\r\n\r\n> Note that eventually the host configuration will be replaced by the execution configuration. When that happens, this attribute will be deprecated in favor of tools. Until then, this attribute allows users to selectively migrate dependencies to the execution configuration. ", "@gunan This commit was hidden by @tensorflower-gardener proxy. Do you know the internal author?", "@Flamefire Thank you very much for going through the trouble of bisecting!\r\nThis was unfortunately an internal cleanup done by other teams.\r\nSince it was submitted a long time ago, I suspect reverting it may break things.\r\n@av8ramit could you look into this? Maybe we can reach out to bazel team for help with understanding and resolving this?", "FWIW: I created a patch which does revert this and it seems to work: https://github.com/easybuilders/easybuild-easyconfigs/blob/8b7342cb08b1e44f91241655ccd58cd3ec4f3687/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.3.0_revert-tools-to-exectools-renaming.patch\r\n\r\nI could open a PR with that so you can run it through CI, if that helps", "Taking a look now, and will triage it to bazel. I second @gunan, thank you @Flamefire for bisecting. @Flamefire I think opening a PR is not a bad idea, I don't know if we will decide to merge it though. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43019\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43019\">No</a>\n"]}, {"number": 43018, "title": "Wrong Copyright/license info in core/lib/io files, originated in LevelDB", "body": "Duplicite with issue #43015\r\n--------------------------------\r\n\r\nHi TensorFlow authors,\r\nMy name is Petr Prokop, I serve in NXP as a Trusted Advisor \u2013 person in charge of SW licenses compliance.\r\nNXP adapts TensorFlow for its i.MX processors. Such releases are subject of license-compliance check.\r\n\r\nUnfortunately, our scanning tool (Black Duck Protex by Synopsis) has reported following license breach:\r\nTensorFlow file https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache.cc is very similar to\r\nLevelDB file https://github.com/google/leveldb/blob/master/util/cache.cc.\r\n\r\nLikewise, files\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache.h\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache_test.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/table.cc\r\nare reported to be similar to\r\nhttps://github.com/google/leveldb/blob/master/include/leveldb/cache.h\r\nhttps://github.com/google/leveldb/blob/master/util/cache_test.cc\r\nhttps://github.com/google/leveldb/blob/master/table/table.cc.\r\n\r\nAs many SW tools, Black Duck Protex has only limited efficiency, so I\u2019ve found some further files in core/lib/io directory during my investigation manually.\r\n\r\nMy understanding is, that TensorFlow reuses LevelDB files, that is pretty fine according original LevelDB license (BSD-3-Clause). However, changing both copyright and license breaches claim #1 of BSD. I wouldn\u2019t dare to advise you anything, but described state is an obstacle for using TF by NXP. Simultaneously, our internal policy prohibits to change 3rd party copyrights, even if they were wrong.\r\n\r\nCan I ask you to allow update of *.h, *.cc and addition of Authors/License_LevelDB.txt files in your repository?\r\n\r\nFeel free change exact wording of \u201cModification\u201d clause and/or formatting.\r\n\r\nRegards,\r\nPetr", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43018) for more info**.\n\n<!-- need_sender_cla -->", "@PetrProkopNXP Thank you for your contribution. Can you please sign CLA? Thanks!", "@PetrProkopNXP Any update on this PR? Please. Thanks!", "Based on #43015 I think this can be closed.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen! "]}, {"number": 43017, "title": "TFLu: Support Ethos-U fast memory scratch", "body": "Register fast memory area with Ethos-U driver and pass tensor sizes to Ethos-U invoke function.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43016, "title": "Dataset.from_generator breaks on last element", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from: Docker Image\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6.9\r\n\r\nRunning the attached gist throws InvalidArgumentError: Argument 0 is out of range. [[{{node args_0}}]].\r\n\r\n**Describe the expected behavior**\r\nIt should run through the dataset, and once completed exit gracefully.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/optiluca/f97a34237837d19f9643b54ef17f8a76\r\n\r\n**Other info / logs**\r\nThis issue goes away if I do any one of the following:\r\n- If I don't batch the data ( commenting line 41)\r\n- If I don't run _transform_after_windowing (commenting line 38)\r\n\r\nI noticed this issue when running a similar pipeline and feeding it to a tf.keras model.predict().  It broke with the aforementioned error.", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200907`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d83f0868204690814a2cc0890364b803/untitled320.ipynb). Thanks!", "Is 1 Batch of data(8*5) if more than the items number(17)?", "You can replace the 17 with e..g. 99 and it still breaks (while also printing some OK messages suggesting that the first batches were output correctly)", "The issue is fixed in Tensorflow version 2.5, your code no longer breaks, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7c30a74189bf5b5f40b9c5e51357d88f/43016.ipynb). \r\nClosing this issue since it is resolved, feel free to reopen. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43016\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43016\">No</a>\n"]}, {"number": 43015, "title": "Wrong licenses / license breach in TensorFlow", "body": "Hi TensorFlow authors,\r\nMy name is Petr Prokop, I serve in NXP as a Trusted Advisor \u2013 person in charge of SW licenses compliance.\r\nNXP adapts TensorFlow for its i.MX processors. Such releases are subject of license-compliance check.\r\n\r\nUnfortunately, our scanning tool (Black Duck Protex by Synopsis) has reported following license breach:\r\nTensorFlow file https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache.cc is very similar to\r\nLevelDB file https://github.com/google/leveldb/blob/master/util/cache.cc.\r\n\r\nLikewise, files\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache.h\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache_test.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/table.cc\r\nare reported to be similar to \r\nhttps://github.com/google/leveldb/blob/master/include/leveldb/cache.h\r\nhttps://github.com/google/leveldb/blob/master/util/cache_test.cc\r\nhttps://github.com/google/leveldb/blob/master/table/table.cc.\r\n\r\nAs many SW tools, Black Duck Protex has only limited efficiency, so I\u2019ve found some further files in core/lib/io directory during my investigation manually. \r\n\r\nMy understanding is, that TensorFlow reuses LevelDB files, that is pretty fine according original LevelDB license (BSD-3-Clause). However, changing both copyright and license breaches claim#1 of BSD. I wouldn\u2019t dare to advise you anything, but described state is an obstacle for using TensorFlow by NXP. Simultaneously, our internal policy prohibits to change 3rd party copyrights, even if they were wrong.\r\n\r\nCan I ask you to allow update of *.h, *.cc and addition of Authors/License_LevelDB.txt files in your repository?\r\nMy modification are prepared in https://github.com/PetrProkopNXP/tensorflow, based on original TensorFlow master branch.\r\n\r\nFeel free change exact wording of \u201cModification\u201d clause and/or formatting.\r\n\r\nRegards,\r\nPetr", "comments": ["@PetrProkopNXP I think the right way might be to vendor leveldb as a third party package through bazel, and then tensorflow's core code will use vendor'ed leveldb package?\r\n\r\nThat will resolve any license issues. It will also be easier in the future to update leveldb in case there are any bug fixes (from leveldb side).\r\n\r\n/cc @martinwicke ", "Sorry @yongtang , I don\u2019t understand your answer (verb \u201cto vendor\u201d, \u201cbazel\u201d)\r\n\r\nIf you suggest I should take original LevelDB sources and refactor them for Tensorflow purposes, it is exactly what I really has done, as it depends on point of view: Consider the change, as I took original LevelDB sources with LevelDB copyright headers and moved there all TensorFlow changes that have been already done. Then I renamed original LevelDB LICENSE and AUTHORS files, both in directory and in license header reference, because in TensorFlow environment they don\u2019t express outgoing license and whole-project authors, but relates to only one component.\r\n\r\nRegardless any need of NXP, through breaching license to one particular SW component, TensorFlow project now violates copyright law. Proposed change heals the issue.\r\n\r\nRegards,\r\nPetr\r\n", "I think that @yongtang's solution is correct.", "When including the code from leveldb, we did relicensed the original code, which was created by Google, under Apache2 for TensorFlow. \r\n\r\nHence, the license headers are correct. \r\n\r\nI understand that it is confusing to automated systems to find similar code under a different license.", "Hi @martinwicke,\r\nSorry for my insisting. I can imagine code licensed once under BSD may be relicensed by its copyright owner.\r\nSimilarly, BSD licensed component might be _used_ in Apache licensed project by anyone.\r\nBut I have no idea what legal title could empower a user to relicense someone else\u2019s BSD code, not giving attribution to original author, in environment of open-source.\r\n\r\nYou are right, that finding similar code is tricky issue. However, I checked the tensorflow vs. levelDb code manually: in modified (_similar_) code remained very same comments. Weird, isn\u2019t it?\r\n\r\nI don't stick on form of solution I've offered initially - more complex solution was offered by @yongtang in PR [43204](https://github.com/tensorflow/tensorflow/pull/43204). But the PR was closed after close of this issue.", "LevelDB is composed either entirely of a) contributions Google received under CLA, or b) Google-employee authored code. a) gives us a \"sublicense right\", which means we can apply any license we choose to the code.", "@PetrProkopNXP if you need any attorney communication about this please drop me an email at ewj at google.com ", "Hi Edd,\r\nThank you for your offer of explanation.\r\n\r\nA notice at beginning: our release is already done. We removed files in question, as developers have said that LevelDB component is not necessary for them.\r\n\r\nBut I\u2019m still curious how the legal things work in this case (and I\u2019d like to be ready when developers say the LevelDB component become necessary): it seems to me that @martinwicke thinks Google CLA overcomes license of particular project (LevelDB here).\r\nI can\u2019t oppose to this (although seems to me that adding an individual license to project on GitHub.com would make lesser sense than I thought sooner). However Google CLA does contain not only clause 2 (right to \u2026 sublicense\u2026), but clause 5 (declaration of original creation, indication of 3rd party code), 6 (no warranty of title and non-infringement -> this would make US responsible!) and 8 (commitment to notify Google).\r\nFrom this perspective, I\u2019m still persuaded there is not everything OK  in the way how LevelDB was reused in this particular case.\r\n\r\nPlease, let me know your point of view to the github licensing, or some link to explanation. For example, I wonder what happens if GPL licensed code, containing 3rd party GPL components, is distributed on GitHub.\r\n\r\nBest Regards,\r\nPetr Prokop\r\nSoftware Engineer, Trusted Advisor\r\nNXP Semiconductors\r\nSochorova 3232/36\r\n616 00 Brno - Zabovresky\r\nCzech Republic\r\nPhone: +420 511 152 414\r\nEmail: petr.prokop@nxp.com<mailto:petr.prokop@nxp.com>\r\n[cid:image005.png@01D1328E.35DBD800]\r\n\r\n\r\nFrom: Edd Wilder-James <notifications@github.com>\r\nSent: Tuesday, September 29, 2020 8:02 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Petr Prokop <petr.prokop@nxp.com>; Mention <mention@noreply.github.com>\r\nSubject: [EXT] Re: [tensorflow/tensorflow] Wrong licenses / license breach in TensorFlow (#43015)\r\n\r\n\r\nCaution: EXT Email\r\n\r\n@PetrProkopNXP<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FPetrProkopNXP&data=02%7C01%7Cpetr.prokop%40nxp.com%7Cb73a270487e946312d6708d864a1bd18%7C686ea1d3bc2b4c6fa92cd99c5c301635%7C0%7C0%7C637369993122730014&sdata=%2FFmEou3mx7fEPIE8NdvFargttlZEAib5y7aQGSY7IWk%3D&reserved=0> if you need any attorney communication about this please drop me an email at ewj at google.com\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F43015%23issuecomment-700884127&data=02%7C01%7Cpetr.prokop%40nxp.com%7Cb73a270487e946312d6708d864a1bd18%7C686ea1d3bc2b4c6fa92cd99c5c301635%7C0%7C0%7C637369993122730014&sdata=%2FPrRAtOs6TY8dl9uiBQZyaOUbjf8xZHvVjGSlctpfL0%3D&reserved=0>, or unsubscribe<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAQNTHJO6PVBTCPKZREV5N4TSIIOIZANCNFSM4Q6AYIRA&data=02%7C01%7Cpetr.prokop%40nxp.com%7Cb73a270487e946312d6708d864a1bd18%7C686ea1d3bc2b4c6fa92cd99c5c301635%7C0%7C0%7C637369993122740002&sdata=vOZEYlFlX2FcGH%2BDUtwyoQ7sGBmnb9rrbrGqk8BmNMc%3D&reserved=0>.\r\n"]}, {"number": 43014, "title": "tf.estimator.train_and_evaluate  \"loss\" meaning in \"saving dict for global step\"", "body": "when i run tf.estimator.train_and_evaluate, log prints that:\r\n[INFO:tensorflow:Saving dict for global step 1000: eval_loss = 0.0055662687, global_step = 1000, loss = 3.3700151]\r\nINFO:tensorflow:global_steps = 1000, loss = 0.05528491 (1202.815 sec)\r\n\r\nquestion:\r\nwhat's meaning of the \"loss = 3.3700151\"\r\n\r\n=====================log=====================\r\n```\r\n2020-09-07 12:33:03.540827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-09-07 12:33:03.542687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-07 12:33:03.542709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-09-07 12:33:03.542718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-09-07 12:33:03.542826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22853 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt.\r\n2020-09-07 12:33:26.224454: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\nINFO:tensorflow:loss = 96.96749, step = 0\r\nINFO:tensorflow:global_steps = 0, loss = 96.96749\r\nINFO:tensorflow:global_step/sec: 0.803899\r\nINFO:tensorflow:loss = 4.264659, step = 100 (124.394 sec)\r\nINFO:tensorflow:global_step/sec: 0.682575\r\nINFO:tensorflow:loss = 3.211001, step = 200 (146.504 sec)\r\nINFO:tensorflow:global_step/sec: 0.442842\r\nINFO:tensorflow:loss = 0.8785575, step = 300 (225.815 sec)\r\nINFO:tensorflow:global_step/sec: 0.388806\r\nINFO:tensorflow:loss = 1.20301, step = 400 (257.197 sec)\r\nINFO:tensorflow:global_step/sec: 0.389973\r\nINFO:tensorflow:loss = 0.50117, step = 500 (256.429 sec)\r\nINFO:tensorflow:global_steps = 500, loss = 0.50117 (1010.338 sec)\r\nINFO:tensorflow:global_step/sec: 0.431264\r\nINFO:tensorflow:loss = 0.52161276, step = 600 (231.876 sec)\r\nINFO:tensorflow:global_step/sec: 0.42097\r\nINFO:tensorflow:loss = 0.32268906, step = 700 (237.547 sec)\r\nINFO:tensorflow:global_step/sec: 0.409772\r\nINFO:tensorflow:loss = 0.28711706, step = 800 (244.038 sec)\r\nINFO:tensorflow:global_step/sec: 0.398877\r\nINFO:tensorflow:loss = 0.32013908, step = 900 (250.704 sec)\r\nINFO:tensorflow:Saving checkpoints for 1000 into /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nI:NER Training:[ber:mod:384]:*** Features ***\r\nI:NER Training:[ber:mod:386]:  name = input_ids, shape = (?, 202)\r\nI:NER Training:[ber:mod:386]:  name = input_mask, shape = (?, 202)\r\nI:NER Training:[ber:mod:386]:  name = label_ids, shape = (?, 202)\r\nI:NER Training:[ber:mod:386]:  name = segment_ids, shape = (?, 202)\r\nWARNING:tensorflow:From /home/panyinghao/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2020-09-07T05:10:03Z\r\nINFO:tensorflow:Graph was finalized.\r\n2020-09-07 13:10:03.661048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-09-07 13:10:03.661119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-07 13:10:03.661130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-09-07 13:10:03.661137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-09-07 13:10:03.661247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22853 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:From /home/panyinghao/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1000\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [10/100]\r\nINFO:tensorflow:Evaluation [20/100]\r\nINFO:tensorflow:Finished evaluation at 2020-09-07-05:10:18\r\nINFO:tensorflow:Saving dict for global step 1000: eval_loss = 0.0055662687, global_step = 1000, loss = 3.3700151\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1000\r\nINFO:tensorflow:global_step/sec: 0.419043\r\nWARNING:tensorflow:From /home/panyinghao/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse eager execution and: \r\n`tf.data.TFRecordDataset(path)`\r\nINFO:tensorflow:loss = 0.05528491, step = 1000 (238.650 sec)\r\nINFO:tensorflow:global_steps = 1000, loss = 0.05528491 (1202.815 sec)\r\nINFO:tensorflow:global_step/sec: 0.66271\r\nINFO:tensorflow:loss = 0.20649494, step = 1100 (150.885 sec)\r\nINFO:tensorflow:global_step/sec: 0.681622\r\nINFO:tensorflow:loss = 0.21718065, step = 1200 (146.709 sec)\r\nINFO:tensorflow:global_step/sec: 0.684914\r\nINFO:tensorflow:loss = 0.12876259, step = 1300 (146.004 sec)\r\nINFO:tensorflow:global_step/sec: 0.680007\r\nINFO:tensorflow:loss = 0.100298606, step = 1400 (147.057 sec)\r\nINFO:tensorflow:global_step/sec: 0.676314\r\nINFO:tensorflow:loss = 0.12061991, step = 1500 (147.873 sec)\r\nINFO:tensorflow:global_steps = 1500, loss = 0.12061991 (738.528 sec)\r\nINFO:tensorflow:global_step/sec: 0.642209\r\nINFO:tensorflow:loss = 0.0042267526, step = 1600 (155.699 sec)\r\nINFO:tensorflow:global_step/sec: 0.637592\r\nINFO:tensorflow:loss = 0.0035796848, step = 1700 (156.841 sec)\r\nINFO:tensorflow:Saving checkpoints for 1793 into /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt.\r\nINFO:tensorflow:Calling model_fn.\r\nI:NER Training:[ber:mod:384]:*** Features ***\r\nI:NER Training:[ber:mod:386]:  name = input_ids, shape = (?, 202)\r\nI:NER Training:[ber:mod:386]:  name = input_mask, shape = (?, 202)\r\nI:NER Training:[ber:mod:386]:  name = label_ids, shape = (?, 202)\r\nI:NER Training:[ber:mod:386]:  name = segment_ids, shape = (?, 202)\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2020-09-07T05:30:20Z\r\nINFO:tensorflow:Graph was finalized.\r\n2020-09-07 13:30:21.334671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2020-09-07 13:30:21.334728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-07 13:30:21.334737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2020-09-07 13:30:21.334743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2020-09-07 13:30:21.835343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22853 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\nINFO:tensorflow:Restoring parameters from /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1793\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Evaluation [10/100]\r\nINFO:tensorflow:Evaluation [20/100]\r\nINFO:tensorflow:Finished evaluation at 2020-09-07-05:30:38\r\nINFO:tensorflow:Saving dict for global step 1793: eval_loss = 0.005485246, global_step = 1793, loss = 4.2882214\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 1793: /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1793\r\nINFO:tensorflow:Loss for final step: 0.12153285.\r\n```", "comments": ["@haozaiiii \r\nPlease fill in issue template for us to replicate the issue faced.", "> @haozaiiii\r\n> Please fill in issue template for us to replicate the issue faced.\r\n\r\nwhere am i to find the issue template", "@haozaiiii \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nPlease feel free to close the issue if you do not have any other query."]}, {"number": 43013, "title": "Why is the quantified TF_LITE slower than the unquantified TF_LITE test time on the PC side\uff1f", "body": "\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport cv2\r\nimport os\r\nimport time\r\nfrom PIL import Image\r\n\r\n\r\nmodel = \"D:/sfz/tf_sfz/uint9.tflite\"\r\ninterpreter = tf.lite.Interpreter(model_path=model)\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(str(input_details), '\\n', str(output_details))\r\nimg = Image.open(\u201cD:/data/000001.jpg\u201d)\r\n        num = img.size[0] * 32 // img.size[1]\r\n        if num > max_width:\r\n            num = max_width\r\n        if num < 5:\r\n            num = 5\r\n        out_mid = img.resize((num, 32))\r\n        out = img.resize((max_width, 32))\r\n        out_crop = out_mid.crop((num - 1, 0, out_mid.size[0], out_mid.size[1]))\r\n        out.paste(out_mid, (0, 0))\r\n        while num < max_width:\r\n            out.paste(out_crop, (num, 0))\r\n            num += 1\r\n        img = cv2.cvtColor(np.asarray(out), cv2.COLOR_RGB2BGR)\r\n        inputImg=(img/255).astype(np.float32)\r\n        interpreter = tf.lite.Interpreter(model_path=model)\r\n        interpreter.allocate_tensors()\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n        interpreter.set_tensor(input_details[0]['index'], [inputImg])\r\n        a = time.time()\r\n        # print(a)\r\n        interpreter.invoke()\r\n        b = time.time()\r\n        # print(b)\r\n        print((b - a) * 1000)\r\n\r\n", "comments": ["@huafeihuayu \r\n\r\nCan you please fill issue template.\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localization issue faster.Thanks!", "> @huafeihuayu\r\n> \r\n> Can you please fill issue template.\r\n> Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localization issue faster.Thanks!\r\n\r\n@ravikyram \r\nI've provided the code", "@huafeihuayu \r\nSince you are using python, I am supposed you are running on PC.\r\n\r\nCould you try to  compare the result on mobile. You can profile your model using one of following approaches:\r\nhttps://www.tensorflow.org/lite/performance/best_practices#profile_your_model", "> @huafeihuayu\r\n> \u56e0\u4e3a\u60a8\u4f7f\u7528\u7684\u662fpython\uff0c\u6240\u4ee5\u6211\u8ba4\u4e3a\u60a8\u6b63\u5728PC\u4e0a\u8fd0\u884c\u3002\r\n> \u91cf\u5316\u6a21\u578b\u5728PC\u4e0a\u6ca1\u6709\u592a\u5927\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u6ca1\u6709\u4e13\u95e8\u7684\u8bf4\u660e\u3002\r\n> \r\n> \u60a8\u53ef\u4ee5\u5c1d\u8bd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6bd4\u8f83\u7ed3\u679c\u5417\uff1f\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5\u4e4b\u4e00\u6765\u5206\u6790\u6a21\u578b\uff1a[https](https://www.tensorflow.org/lite/performance/best_practices#profile_your_model) :\r\n> [//www.tensorflow.org/lite/performance/best_practices#profile_your_model](https://www.tensorflow.org/lite/performance/best_practices#profile_your_model)\r\n\r\n\r\n> @huafeihuayu\r\n> Since you are using python, I am supposed you are running on PC.\r\n> The quantized model don't get much performance benefit on PC since it does not have specialized instruction for that.\r\n> \r\n> Could you try to compare the result on mobile. You can profile your model using one of following approaches:\r\n> https://www.tensorflow.org/lite/performance/best_practices#profile_your_model\r\n\r\n@thaink @ravikyram \r\nimport tensorflow as tf\r\nsaved_model_dir='D:/tf_ocr/tf_resnet7'\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.supported_types = [tf.lite.constants.INT8]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nTflite_quanit_model = converter.convert()\r\nopen(\"D:/sfz/tf_sfz/model10.tflite\",\"wb\").write(Tflite_quanit_model)\r\n\r\nSince it is trainable quantized, using savermodel.pb to quantify the TF_lite model input/output or Float32 instead of UINT8, does this decrease the reasoning speed on the move?\r\n\r\n\r\n", "What do you mean by \"trainable quantized\"?\r\nIf you use quantization aware training, the conversion should be as simple as:\r\nhttps://www.tensorflow.org/model_optimization/guide/quantization/training_example#create_quantized_model_for_tflite_backend", "> What do you mean by \"trainable quantized\"?\r\n> If you use quantization aware training, the conversion should be as simple as:\r\n> https://www.tensorflow.org/model_optimization/guide/quantization/training_example#create_quantized_model_for_tflite_backend\r\n\r\n@thaink @ravikyram \r\nimport tensorflow as tf\r\nsaved_model_dir='D:/sfz/tf_sfz'\r\ndef representative_data_gen():\r\nfor input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\nyield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model_quant = converter.convert()\r\nopen(\"D:/sfz/tf_sfz/model25.tflite\",\"wb\").write(tflite_model_quant)\r\n\r\nthe train_images don't know what it is,I only know to select a part of the representative data, I don't know what to do next?The training data is JPG file and TXT file\uff0cThey are in a folder, how do I make train_images?What type of train_images is it?Can you fix the code a little bit", "You don't need to have accuracy data for representative_dataset so just images are enough.\r\nAssumes image_paths is the list of your image files. You can take a look at following example:\r\n```python\r\ndef representative_dataset_gen(image_paths):\r\n\timage_paths_tensor = tf.convert_to_tensor(image_paths, dtype=tf.string)\r\n\tdataset = tf.data.Dataset.from_tensor_slices(image_paths_tensor)\r\n\r\n\tdef map_fn(path):\r\n\t    image = tf.image.decode_jpeg(tf.read_file(path))\r\n\t    image = tf.image.resize_images(out_shape)\r\n\t    image = tf.to_float(image) /127.5 - 127.5\r\n\t    return image\r\n\r\n\tdataset = dataset.map(map_fn)\r\n\tdataset = dataset.batch(batch_size).take(100)\r\n\r\n\titerator = dataset.make_one_shot_iterator()\r\n\tfor _ in range(number_of_calibration_steps):\r\n\t    images = iterator.get_next()\r\n\t    yield [images]\r\n```\r\nPlease make sure the normalization step in map_fn is correct.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 43012, "title": "[MLIR][KERNEL_GEN] Add a library to lower kernels with the host side.", "body": "* Unified TF->GPU Binary and TF->Kernel_with_host side lowering in\r\n`kernel_creator.h|cc`\r\n* Added a pass that attaches GPU binary blob to GPUModuleOp\r\n* Refactored most of the code.\r\n* Added tf_to_kernel binary that emits obj file\r\n", "comments": []}, {"number": 43011, "title": "Same variable name for tf.feature_column.embedding_column embedding tensors in tf 2.3.0", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: 64 bit Linux\r\n- TensorFlow installed from source\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0 \r\n- Python version: 3.6.10\r\n- Cuda compilation tools, release 10.0, V10.0.130\r\n\r\n**Describe the current behavior**\r\nIn TF 2.1.0, when I use tf.feature_column.embedding_column, the variable names of the embedding variables are derived from the categorical column name.\r\nExample: if I create an embedding column using a categorical column\r\nfeature_alpha = tf.feature_column.categorical_column_with_hash_bucket('feature_alpha', 100, dtype=tf.dtypes.string)\r\nalpha_emb = tf.feature_column.embedding_column(feature_alpha, dimension=10)\r\n\r\nfeature_beta = tf.feature_column.categorical_column_with_hash_bucket('feature_beta', 200, dtype=tf.dtypes.string)\r\nbeta_emb = tf.feature_column.embedding_column(feature_beta, dimension=20)\r\n\r\nthe embedding variable for 'feature_alpha' is named 'dense_features/feature_alpha_embedding/embedding_weights:0'\r\nthe embedding variable for 'feature_beta' is named 'dense_features/feature_beta_embedding/embedding_weights:0'\r\n\r\nthe variable names contain the categorical column name, 'feature_alpha...' and 'feature_beta...'\r\n\r\nIn TF 2.3.0\r\n\r\nthe embedding variable for 'feature_alpha' is named 'dense_features/embedding_weights:0'.\r\nthe embedding variable for 'feature_beta' is named 'dense_features/embedding_weights:0'\r\n\r\nThis works fine if there is only a single feature column. but when there are multiple feature columns.\r\nThe embedding variables are assigned the same name, which creates problems while saving the model.\r\nI get the error \"RuntimeError: Unable to create link (name already exists)\" while saving the model, since the embedding variables for 'feature_alpha' and 'feature_beta' have the same variable name.\r\n\r\n**Describe the expected behavior**\r\n\r\nthe expected behaviour is that what we observe in TF 2.1.0.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis code works in TF 2.1.0 but fails in TF 2.3.0 with the error \"RuntimeError: Unable to create link (name already exists)\" due to the same variable names.\r\n```\r\nimport tensorflow as tf\r\ninputs = {'feature_alpha' : tf.keras.layers.Input(name='feature_alpha', \r\n                                                 shape=(None,), \r\n                                                 sparse=True, \r\n                                                 dtype=tf.dtypes.string),\r\n         'feature_beta' : tf.keras.layers.Input(name='feature_beta', \r\n                                                 shape=(None,), \r\n                                                 sparse=True, \r\n                                                 dtype=tf.dtypes.string)}\r\ndef gen_model(inputs):\r\n    feature_alpha = tf.feature_column.categorical_column_with_hash_bucket('feature_alpha', 100, dtype=tf.dtypes.string)\r\n    feature_beta = tf.feature_column.categorical_column_with_hash_bucket('feature_beta', 200, dtype=tf.dtypes.string)    \r\n    \r\n    alpha_emb = tf.feature_column.embedding_column(feature_alpha, dimension=10)\r\n    beta_emb = tf.feature_column.embedding_column(feature_beta, dimension=20)\r\n    out = tf.keras.layers.DenseFeatures([alpha_emb, beta_emb])(inputs)\r\n\r\n    out = tf.keras.layers.Dense(64, activation='relu')(out)\r\n\r\n    model = tf.keras.Model(inputs, out)\r\n\r\n    return model        \r\n\r\nmodel = gen_model(inputs)\r\n\r\nprint(model.trainable_variables)\r\nmodel.save('mdl.h5')\r\n```", "comments": ["I've tried this code on Colab and it is not failing with Tensorflow 2.3.0", "is there any way I can help reproduce the error?\r\nI'm getting the following error in tensorflow 2.3.0\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-161fd8a33deb> in <module>\r\n     25 \r\n     26 print(model.trainable_variables)\r\n---> 27 model.save('mdl.h5')\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1977     \"\"\"\r\n   1978     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n-> 1979                     signatures, options)\r\n   1980 \r\n   1981   def save_weights(self,\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    129           'or using `save_weights`.')\r\n    130     hdf5_format.save_model_to_hdf5(\r\n--> 131         model, filepath, overwrite, include_optimizer)\r\n    132   else:\r\n    133     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)\r\n    117     model_weights_group = f.create_group('model_weights')\r\n    118     model_layers = model.layers\r\n--> 119     save_weights_to_hdf5_group(model_weights_group, model_layers)\r\n    120 \r\n    121     # TODO(b/128683857): Add integration tests between tf.keras and external\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py in save_weights_to_hdf5_group(f, layers)\r\n    638     save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\r\n    639     for name, val in zip(weight_names, weight_values):\r\n--> 640       param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)\r\n    641       if not val.shape:\r\n    642         # scalar\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds)\r\n    137             dset = dataset.Dataset(dsid)\r\n    138             if name is not None:\r\n--> 139                 self[name] = dset\r\n    140             return dset\r\n    141 \r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj)\r\n    371 \r\n    372             if isinstance(obj, HLObject):\r\n--> 373                 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)\r\n    374 \r\n    375             elif isinstance(obj, SoftLink):\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5o.pyx in h5py.h5o.link()\r\n\r\nRuntimeError: Unable to create link (name already exists)\r\n```", "Ok now I see better your error and it is reproducible saving with h5 format.\r\n\r\n", "Can you test the same example with `pip install tf_nightly-cpu` or `pip install tf_nightly`?", "@akshayjagatap,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a72465c7e3d740127444b03ede6ed4f2/43011.ipynb). \r\n\r\nHowever, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/ca4c228b5ec181db4b9580ad8fccb96d/43011-tf-nightly.ipynb). Please find the attached gist. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43011\">No</a>\n", "I have met the same issue. A work around way is to create multiple DenseFeature layer with different name for embedding column, after that concatenate the result after those DenseFeature layer. This works for me in TF 2.3 (in case you don't want to upgrade your TF version to nightly)"]}]