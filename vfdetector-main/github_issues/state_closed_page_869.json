[{"number": 27431, "title": "Using layer classes as attribute throw an exception", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: `2.0.0.dev20190402`\r\n- Python version: 2.7.12\r\n\r\n**Describe the current behavior**\r\n\r\nWhen a layer class is used as attribute, the code will throw a `TypeError` exception when calling `self._gather_children_attribute`. It appears that the layer class is tracked.\r\n\r\n**Describe the expected behavior**\r\n\r\nOnly layer instances should be tracked, not classes.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Layer(tf.keras.layers.Layer):\r\n\r\n    def __init__(self):\r\n        super(Layer, self).__init__()\r\n        self.layer_fn = tf.keras.layers.Dense\r\n\r\nlayer = Layer()\r\nprint(layer.variables)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"tf2/class.py\", line 10, in <module>\r\n    print(layer.variables)\r\n  File \"/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1330, in variables\r\n    return self.weights\r\n  File \"/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 708, in weights\r\n    return self.trainable_weights + self.non_trainable_weights\r\n  File \"/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 687, in trainable_weights\r\n    nested = self._gather_children_attribute('trainable_weights')\r\n  File \"/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1850, in _gather_children_attribute\r\n    getattr(layer, attribute) for layer in nested_layers))\r\nTypeError: 'property' object is not iterable\r\n```", "comments": ["@qlzh727 this is a better project for someone working on TF Keras. I am realistically never going to get to it. (It's near code I've modified, but I don't think those modifications made the situation any worse.)", "Ack, I will take it from here.", "Should be fixed by https://github.com/tensorflow/tensorflow/commit/9d724a8e6034d321e97cdc9972d4d6e7adb3e3ca now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27431\">No</a>\n"]}, {"number": 27430, "title": "[Feature] Store shared library name in tf.sysconfig", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1 or 2.0.0\r\n- Are you willing to contribute it (Yes/No): Probably not the best person\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs of #22797 the tensorflow shared library is now versioned. This makes linking to the shared library difficult (at least with my knowledge of bazel). The recommended method in https://github.com/tensorflow/custom-op links a [static file name](https://github.com/tensorflow/custom-op/blob/master/tf/BUILD.tpl#L12)\r\n\r\n**Will this change the current api? How?**\r\nAdd a new shared library name to tf.sysconfig \r\n\r\n**Who will benefit with this feature?**\r\nThe community who is building upon TensorFlow. Given the push for modularity it's an important thing to figure out.\r\n\r\n\r\n**Any Other info.**\r\nThis stems from https://github.com/tensorflow/addons/pull/130\r\n", "comments": ["cc @yifeif as a change may need to be made to custom-op", "As discussed in today's SIG Build meeting ... a more short term fix (that wouldn't require API review) would be if there is a proper symlink from `libtensorflow_framework` to whatever the current version is. ", "@perfinion I think some internal tests may be broken due to the missing symlink:\r\n\r\n```\r\n==================== Test output for //tensorflow/tools/lib_package:libtensorflow_test:\r\n+ CC=\r\n+ TAR=\r\n+ '[' -z '' ']'\r\n+ CC=/usr/bin/gcc\r\n+ '[' -z '' ']'\r\n+ TAR=tar\r\n+ TARFILE=/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/lib_package/libtensorflow_test.runfiles/org_tensorflow/tensorflow/tools/lib_package/libtensorflow.tar.gz\r\n+ CFILE=/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/lib_package/libtensorflow_test.runfiles/org_tensorflow/tensorflow/tools/lib_package/libtensorflow_test.c\r\n+ cd /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/_tmp/3302b1ea63f567e726bad68a4f03cae9\r\n+ mkdir tensorflow\r\n+ tar -xzf /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/lib_package/libtensorflow_test.runfiles/org_tensorflow/tensorflow/tools/lib_package/libtensorflow.tar.gz -Ctensorflow\r\n+ /usr/bin/gcc /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/lib_package/libtensorflow_test.runfiles/org_tensorflow/tensorflow/tools/lib_package/libtensorflow_test.c -Itensorflow/include -Ltensorflow/lib -ltensorflow_framework -ltensorflow -oa.out\r\n/usr/bin/ld: cannot find -ltensorflow_framework\r\ncollect2: error: ld returned 1 exit status\r\n```", "I opened https://github.com/tensorflow/tensorflow/pull/27493 to add back the unversioned .so first.\r\nThat should fix this quickly then we can take a look at adding something to tf.sysconfig more carefully.", "@seanpmorgan this should work for you right? https://github.com/tensorflow/tensorflow/pull/27694", "Yes that will work for us... we've already swapped to linking on versioned soname", "I'm getting an error when trying to build TF Addons on MacOS:\r\n```\r\nERROR: /private/var/tmp/.../external/local_config_tf/BUILD:3800:1: Executing genrule\r\n@local_config_tf//:libtensorflow_framework.so.2 failed (Exit 1) bash failed: error\r\nexecuting command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\n```\r\n\r\nThe `libtensorflow_framework.so.2` file is missing?\r\n\r\nLooking into that directory, I see that the library's extension is dylib. So maybe the hardcoded name should be depend on the platform?\r\n\r\n```bash\r\n$ ls /Users/.../lib/python3.6/site-packages/tensorflow/\r\n__init__.py\r\n__pycache__\r\n_api\r\ncompiler\r\ncore\r\ninclude\r\nlibtensorflow_framework.2.0.0.dylib\r\nlibtensorflow_framework.2.dylib\r\nlite\r\npython\r\ntools\r\n```", "Yes, on macos they're `.dylib`s, not `.so`s", "@ageron Thanks! See #27865 ", "Solved by #27694 and #27865 ... Thanks for the help!"]}, {"number": 27429, "title": "ValueError: Invalid tensors 'Mul' were found.", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\nPython:2.7\r\ntensorflow:1.9\r\n\r\npython script\r\n`input_arrays= [\"Mul\"]\r\noutput_arrays=[\"final_result\"]\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(graph_def_file,input_arrays,output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"model.tflite\", \"wb\").write(tflite_model)`\r\n\r\nIt was working fine a month before.Now it shows the following error\r\n\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert_saved_model.py\", line 205, in get_tensors_from_tensor_names\r\n    \",\".join(invalid_tensors)))\r\nValueError: Invalid tensors 'Mul' were found.\r\n", "comments": ["@jennings1716 You mentioned it was working well a month before. What was the change you made recently? Did you upgrade tensorflow or python or any other dependencies?Could you provide a code to reproduce bug? Could you try it in Google [colab](https://colab.sandbox.google.com) to verify whether it is working or not. Can you also try to upgrade your code to latest TF. Thanks! ", "Hi @jvishnuvardhan It was working fine... When I use the same input and output tensor names for the tflite conversion i got the error... Then I changed the tensor names to \r\ninput array = \"lambda_1/Mul\"\r\noutput array = \"model_2/model_1/dense_2/Softmax\"\r\nIt works with above names... I wonder why...?", "@jennings1716 Could you check with TF1.13.1 and see whether the bug persists with latest version. You could also try TF2.0. Thanks!", "Hi @jvishnuvardhan It is working for python 3.5( Mul,final_result) But for Python 2.7 there remains the bug", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27429\">No</a>\n"]}, {"number": 27428, "title": "Tensorflow 2.0.0 multiple GPU output zero for non-root GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo.\r\n- TensorFlow installed from (source or binary):\r\nSource.\r\n- TensorFlow version (use command below):\r\nv2.0.0-alpha0-0-g2c319fb415 2.0.0-alpha0\r\n- Python version:\r\n3.7\r\n- Bazel version (if compiling from source):\r\n0.23.0\r\n- GCC/Compiler version (if compiling from source):\r\nGCC 4.8\r\n- CUDA/cuDNN version:\r\nCUDA 10.0, cuDNN 7\r\n- GPU model and memory:\r\nA simple model can reproduce.\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nWhen I run the `tf_env_collect.sh` script, tensorflow just hangs there forever. I couldn't even kill it. What the hell is going on?\r\n\r\n```bash\r\n(base) \u279c  ~ ./tfenv.sh \r\nCollecting system information...\r\n2019-04-02 22:07:13.977020: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-04-02 22:07:15.731652: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x56368643d070 executing computations on platform CUDA. Devices:\r\n2019-04-02 22:07:15.731716: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2\r\n2019-04-02 22:07:15.731733: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (1): GeForce GTX TITAN X, Compute Capability 5.2\r\n2019-04-02 22:07:15.731751: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (2): TITAN X (Pascal), Compute Capability 6.1\r\n2019-04-02 22:07:15.731765: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (3): TITAN X (Pascal), Compute Capability 6.1\r\n2019-04-02 22:07:15.731780: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (4): TITAN X (Pascal), Compute Capability 6.1\r\n2019-04-02 22:07:15.731795: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (5): GeForce GTX TITAN X, Compute Capability 5.2\r\n2019-04-02 22:07:15.731810: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (6): GeForce GTX TITAN X, Compute Capability 5.2\r\n2019-04-02 22:07:15.757870: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299805000 Hz\r\n2019-04-02 22:07:15.761353: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x563686573540 executing computations on platform Host. Devices:\r\n2019-04-02 22:07:15.761414: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-04-02 22:07:15.762076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 11.93GiB freeMemory: 11.81GiB\r\n2019-04-02 22:07:15.762510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 1 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 11.93GiB freeMemory: 11.81GiB\r\n2019-04-02 22:07:15.762924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 2 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:82:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.76GiB\r\n2019-04-02 22:07:15.763343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 3 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:85:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.76GiB\r\n2019-04-02 22:07:15.763634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 4 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:86:00.0\r\ntotalMemory: 11.91GiB freeMemory: 5.41GiB\r\n2019-04-02 22:07:15.764061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 5 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:89:00.0\r\ntotalMemory: 11.93GiB freeMemory: 11.81GiB\r\n2019-04-02 22:07:15.764483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 6 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:8a:00.0\r\ntotalMemory: 11.93GiB freeMemory: 11.81GiB\r\n2019-04-02 22:07:15.770500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6\r\n2019-04-02 22:07:15.770587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n```\r\n\r\n**Describe the current behavior**\r\nRun the test code below. For GPU 0 the behavior is normal, but for GPU 1 the output becomes zero.\r\n**Describe the expected behavior**\r\nThe output of GPU 1 should be the same as GPU 0.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nrun `python test.py`.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\n\r\ndef linear(name, input, output_dim, reuse=False):\r\n    with tf.variable_scope(name, reuse=reuse):\r\n        w = tf.get_variable(\"weight\", shape=[input.get_shape()[-1], output_dim], initializer=tf.orthogonal_initializer)\r\n        b = tf.get_variable(\"bias\", [output_dim], initializer=tf.constant_initializer(.0))\r\n\r\n        x = tf.matmul(input, w) + b\r\n        with tf.device(\"/device:CPU:0\"):\r\n            x = tf.Print(x, [tf.reduce_sum(tf.abs(w)), tf.reduce_sum(tf.abs(b))], name + \"/weight_bias: \")\r\n    \r\n    return x\r\n\r\ndef tower(x, reuse=False):\r\n    rec_tensor = []\r\n    rec_name = []\r\n    x = linear(\"fc1\", x, 10, reuse)\r\n    rec_tensor.append(x); rec_name.append(\"fc1\")\r\n    x = linear(\"fc2\", x, 1, reuse)\r\n    rec_tensor.append(x); rec_name.append(\"fc2\")\r\n    return x, rec_tensor, rec_name\r\n\r\nx = tf.placeholder(tf.float32, [None, 3])\r\nx_data = np.random.rand(5, 3)\r\nfeed_dict = {x: x_data}\r\nys = []\r\nrec_xs = []\r\nrec_names = []\r\n\r\nfor i in range(2):\r\n    with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\r\n        y_, v_, n_ = tower(x, i>0)\r\n        ys.append(y_)\r\n        rec_xs.append(v_)\r\n        rec_names.append(n_)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nconfig.allow_soft_placement = False\r\nsess = tf.Session(config=config)\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\nprint(\"=> Check forward\")\r\nfor i in range(2):\r\n    print(\"=> Check GPU %d\" % i)\r\n    for j in range(len(rec_xs[i])):\r\n        t = sess.run(rec_xs[i][j], feed_dict)[0]\r\n        l1norm = np.sum(np.abs(t))\r\n        print(\"=> %s: %.5f\" % (rec_names[i][j], l1norm))\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nMy running log of `test.py`:\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0402 21:57:00.129247 139765947877184 deprecation.py:506] From /home/atlantix/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py:883: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0402 21:57:00.150027 139765947877184 deprecation.py:323] From test.py:15: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\r\nInstructions for updating:\r\nUse tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\r\n```python\r\n    sess = tf.Session()\r\n    with sess.as_default():\r\n        tensor = tf.range(10)\r\n        print_op = tf.print(tensor)\r\n        with tf.control_dependencies([print_op]):\r\n          out = tf.add(tensor, tensor)\r\n        sess.run(out)\r\n    ```\r\nAdditionally, to use tf.print in python 2.7, users must make sure to import\r\nthe following:\r\n\r\n  `from __future__ import print_function`\r\n\r\n2019-04-02 21:57:00.195153: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-04-02 21:57:00.626301: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55bf73aa0640 executing computations on platform CUDA. Devices:\r\n2019-04-02 21:57:00.626354: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX TITAN X, Compute Capability 5.2\r\n2019-04-02 21:57:00.626368: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (1): GeForce GTX TITAN X, Compute Capability 5.2\r\n2019-04-02 21:57:00.649925: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299805000 Hz\r\n2019-04-02 21:57:00.653093: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55bf73a9fad0 executing computations on platform Host. Devices:\r\n2019-04-02 21:57:00.653157: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-04-02 21:57:00.653809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 11.93GiB freeMemory: 11.81GiB\r\n2019-04-02 21:57:00.654269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 1 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 11.93GiB freeMemory: 11.81GiB\r\n2019-04-02 21:57:00.654961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1\r\n2019-04-02 21:57:00.655060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-04-02 21:57:01.477672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-04-02 21:57:01.477727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 \r\n2019-04-02 21:57:01.477757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N Y \r\n2019-04-02 21:57:01.477767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   Y N \r\n2019-04-02 21:57:01.478354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11422 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:08:00.0, compute capability: 5.2)\r\n2019-04-02 21:57:01.478791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11422 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\n2019-04-02 21:57:01.487682: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n=> Check forward\r\n=> Check GPU 0\r\n2019-04-02 21:57:01.527837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\nfc1/weight_bias: [8.01475811][0]\r\n=> fc1: 1.84453\r\nfc1/weight_bias: [8.01475811][0]\r\nfc2/weight_bias: [2.51356649][0]\r\n=> fc2: 0.19745\r\n=> Check GPU 1\r\nfc1/weight_bias: [8.01475811][0]\r\n=> fc1: 0.00000\r\nfc1/weight_bias: [8.01475811][0]\r\nfc2/weight_bias: [2.51356649][0]\r\n=> fc2: 0.00000\r\n```\r\n\r\nAs for my GPUs, the driver version is `410.104`, installed according to official instructions.\r\n\r\nHope it helps.\r\nCould anybody help me ASAP? I am almost crazy with TF's multiple GPU. \r\nThank you very much!", "comments": ["why are you trying to use v2.0.0-alpha0-0-g2c319fb415 2.0.0-alpha0. if your code seems to be TF 1", "I am writing code like this because I want to see what the math operation is as clearly as possible. As for using TF2, it is just following up with the latest version.", "I can confirm you're not using the version of TF you claim to be using. The current 1.x nightly shows v1.12.0-11729-g98c3cfbf74 1.14.1-dev20190404 as the version string while you have the one from the 2.x alpha.\r\n\r\nIt also looks like your cuda installation is borked somehow. Have you tried using an nvidia provided docker image?", "\r\nI have tried nightly build and it seems to work:\r\n\r\n=> Check forward\r\n=> Check GPU 0\r\n2019-04-04 17:40:19.590663: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1284] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-04-04 17:40:19.591831: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\nfc1/weight_bias: [8.02037048][0]\r\n=> fc1: 2.59844\r\nfc1/weight_bias: [8.02037048][0]\r\nfc2/weight_bias: [2.84391737][0]\r\n=> fc2: 0.13365\r\n=> Check GPU 1\r\nfc1/weight_bias: [8.02037048][0]\r\n=> fc1: 2.59844\r\nfc1/weight_bias: [8.02037048][0]\r\nfc2/weight_bias: [2.84391737][0]\r\n=> fc2: 0.13365\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27428\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27428\">No</a>\n"]}, {"number": 27427, "title": "Do not use symlink", "body": "Closes https://github.com/tensorflow/tensorflow/issues/27282", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27426, "title": "Fix bug for sample_weights being specialized to None", "body": "@anj-s, reference to #24226.\r\n\r\nFirst, Thanks for your help, I'm not sure I was in the right direction, please take a look, and I'll write some test about it then. ", "comments": ["Thanks for the PR! The change looks to be correct. Can you add a correctness test for this? Thanks!", "@anj-s , Thanks for your feedback, I have changed some logic in this PR, and I think I met some problems to add a test here, could you give me some reference?\r\n\r\nHere is My original thought, I couldn't find a efficient way to verify the output after calling function `_prepare_feed_values`, which means I don't know how to check whether the return `ins` contain the not none `sample_weights`. Is there any different behaviour result  the not none sample_weights will lead to? And then I could only check whether the result is match our expectation.", "Sorry for the late response. There are a couple of things you need in order to check correctness:\r\n1. A model where you can initialize weights such that output = input (y = x). \r\n2. Set a slice of your output labels to not be equal to your input data.\r\n3. Let your sample weights array zero out all the indices where the output is not equal to the input.\r\n4. If sample_weights works as expected you should see a 0 loss otherwise your loss will be > 0. The idea being sample weights will zero out all the samples where the output is not equal to the input. \r\nLet me know if this helps. Thanks!", "@anj-s, Thanks for you kind help, appreciate it very much. I have added the test within `test_sample_weights`, I'm not sure whether it match your saying above, please take a look at it, and then I will try to pass  the unit test.", "@anj-s, Thanks for you kind help. Already tackle the conflicts.", "The tests on this PR fail due to more recent changes to the codebase. Closing this PR since the fix for this issue has been submitted.\r\nhttps://github.com/tensorflow/tensorflow/commit/98a0c57c44d9c4f1a1ad2f7003c83dbe2802f7ce"]}, {"number": 27425, "title": "Unity3D ml-agents not working properly in TF 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win 10\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0.0-alpha0\r\n- Python version:3.6\r\n\r\n**Describe the current behavior**\r\nThe value estimate became an array instead of scalar float after 8000+ steps\r\n\r\n**Describe the expected behavior**\r\nThe value estimate should be a scalar float\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/SetoKaiba/ml-agents/tree/tf2\r\n\r\n**Other info / logs**\r\nAfter some deep dig, I found the problem is here. It uses the tf session to estimate the value. The value became an array instead of scalar float after 8000+ steps\r\nhttps://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/ppo/policy.py#L198\r\n", "comments": ["https://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/ppo/policy.py#L199-L200\r\n```python\r\n        value_estimate = self.sess.run(self.model.value, feed_dict)\r\n        if not isinstance(value_estimate, float):\r\n            value_estimate = value_estimate[0][0]\r\n```\r\n\r\nI finally get it working by some trick.\r\nWhy does it become an array? \r\nThe model.value is defined by the code here.\r\n```python\r\n        value = tf.layers.dense(hidden_value, 1, activation=None)\r\n        self.value = tf.identity(value, name=\"value_estimate\")\r\n```\r\n\r\nhttps://github.com/SetoKaiba/ml-agents/blob/tf2/ml-agents/mlagents/trainers/models.py#L298-L299", "Anyone helps? Is it a bug of TF 2.0?", "Is it read by Tensorflow official? Is it a bug? The code below sometime generate an ndarray instead of a float.\r\n```Python\r\n        value = tf.layers.dense(hidden_value, 1, activation=None)\r\n        self.value = tf.identity(value, name=\"value_estimate\")\r\n```", "Can any one reply or help?", "Closing the issue since it is a duplicate of #27669 . Thank you! "]}, {"number": 27424, "title": "Api doc updated for BatchToSpaceND", "body": "Tensor shape matched with the example", "comments": []}, {"number": 27423, "title": "Compute F1 score for multilabel classifier #27171", "body": "Compute F1 score for multilabel classifier #27171 ", "comments": ["Added a function to calculate minor precision.", "The contrib metrics are deprecated and slated to be removed in tf v2, so closing this.\r\n\r\nPlease make the change to the tf.keras.metrics instead (and switch to using those)."]}, {"number": 27422, "title": "TfLite one_hot int8 and unit8 feature support", "body": "1- int8 and uint8 data type support change\r\n2- supported data type test coverage added", "comments": ["Have you verified that this works in a real quantized model? The on/off value semantics may work differently for quantized models.", "@suharshs how should we treat these ops for quantization?", "@suharshs can you please review this? Thanks!", "Sorry for the delay, am discussing the general approach for operations like this. Will update when we have concensus.", "Can one of the admins verify this patch?", "@suharshs  Any update on this PR, please. ", "Since one-hot just using int for the backing values anyways."]}, {"number": 27421, "title": "TfLite one_hot int16 feature fix", "body": "1- int16 data type support fix\r\n2- int16 and int64 data type test coverage added", "comments": ["Is there a specific model you had in mind that requires this? Our int16 (and int64) paths right now are more of an intermediate solution, purely need-based, rather than adding full int16/int64 support to all kernels.", "Let's avoid adding int16 unless you have a model which requires it."]}, {"number": 27420, "title": "Official website is causing high cpu usage in firefox and chrome when scrolled to top", "body": "Website is causing high cpu usage in firefox and chrome when scrolled to top", "comments": ["The website is working as expected. Please give it a try again. Thanks!", "If showing the animation on the front page taking 15% of the CPU is expected, then it is indeed working correctly\r\n"]}, {"number": 27419, "title": "Update License to 2019", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27419) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27419) for more info**.\r\n\r\ni signed it", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27419) for more info**.\n\n<!-- ok -->", "Our legal team advises me that it makes no difference updating the copyright line in the license file, so we prefer to leave it untouched, out of an abundance of caution. Thank you for your suggestion, though!"]}, {"number": 27418, "title": "Minor \"Error\" in Tutorial", "body": "", "comments": ["@kiunthmo Thanks for bringing this to our notice. We will take a look and resolve it. Thanks!"]}, {"number": 27417, "title": "tf.gather shows an error in presence of different dtype tensor under tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Source (1st April 2019)\r\n- TensorFlow version (use command below): v2.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0, 7.5.0\r\n- GPU model and memory: Zotac GTX 1060, 6GB\r\n\r\n**Describe the current behavior**\r\ntf.gather throws error in presence of another independent tensor inside tf.function\r\n\r\n**Describe the expected behavior**\r\nRun smoothly\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n\r\n    a = tf.Variable(tf.random.normal([10]))\r\n    b = tf.Variable(tf.random.uniform([10], minval=-1, maxval=1, dtype=tf.dtypes.int32))\r\n    value = tf.Variable(tf.random.normal([10]))\r\n\r\n    print(a.numpy)\r\n    print(b.numpy)\r\n\r\n    @tf.function\r\n    def run():\r\n        for j in tf.range(10):\r\n\r\n            var = tf.gather(a, j)\r\n            tf.print(var)\r\n\r\n            var_int = tf.gather(b, j)\r\n            tf.print(var_int)\r\n\r\n    run()\r\n\r\n**Information**\r\nWhen you comment out the float variable, it functions correctly with a warning. When both the int32 and float 32 are present, tf.gather shows an error as shown below.\r\n\r\n**Other info / logs**\r\nFile \"test.py\", line 20, in <module>\r\n    run()\r\n  File \"/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 437, in __call__\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 559, in _filtered_call\r\n    (t for t in nest.flatten((args, kwargs))\r\n  File \"/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 633, in _call_flat\r\n    outputs = self._inference_function.call(ctx, args)\r\n  File \"/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 416, in call\r\n    ctx=ctx)\r\n  File \"/home/caissalover/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0. The edge src node is while/exit/_27 , and the dst node is while [Op:__inference_run_98]\r\n", "comments": ["Thank you for reaching out to us. Can you please provide minimum reproducible code snippet to help us proceed further and verify what is going wrong", "@achandraa I have updated the error and I'm sure it's much better than the earlier post. Let me know. Thanks.", "@caissalover I ran your script in google colab with TF2.0 and it worked as expected. Please check the output below. Could you try running it in colab.sandbox.google.com? You could also upgrade TF2.0 and check whether the bug persists or not. Thanks!\r\n\r\n<bound method ResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=\r\narray([-0.7555967 , -1.0448577 , -0.08942685,  0.09107728,  0.25068387,\r\n        1.6492767 , -0.40207562, -0.7407482 , -1.7115227 , -0.6381878 ],\r\n      dtype=float32)>>\r\n<bound method ResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(10,) dtype=int32, numpy=array([ 0, -1,  0,  0,  0,  0, -1,  0,  0, -1], dtype=int32)>>\r\n-0.755596697\r\n0\r\n-1.04485774-1\r\n\r\n-0.0894268528\r\n0\r\n00.091077283\r\n\r\n0.250683874\r\n001.64927673\r\n\r\n\r\n-0.402075619\r\n-1\r\n-0.7407482270\r\n\r\n-1.7115227\r\n0\r\n-0.638187826\r\n-1", "Colab shows no warning. Maybe someone fixed the bug after 1st April. Will update and let you know.", "Yes, this is resolved. Please close this. Thanks!", "Closing this issue as it was resolved. Thanks!"]}, {"number": 27416, "title": "Unable to use FeatureColumn with Keras Functional API", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below):  2.0.0-alpha0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI followed the guide: [Classify structured data](https://www.tensorflow.org/alpha/tutorials/keras/feature_columns). The author used TF Feature Column and TF Data alongside a Sequential model, which worked out just fine. \r\n\r\nThen, I've tried implementing the same using Keras Functional API, but was greeted by the error message below:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-19-9647c70de900> in <module>\r\n----> 1 inputs = layers.Input(tensor=feature_layer, name='features')\r\n      2 x = layers.Dense(128, activation='relu')(inputs)\r\n      3 x = layers.Dense(64, activation='relu')(x)\r\n      4 \r\n      5 baggage_pred = layers.Dense(1, activation='sigmoid', name='baggage')(x)\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_layer.py in Input(shape, batch_size, name, dtype, sparse, tensor, **kwargs)\r\n    231       dtype=dtype,\r\n    232       sparse=sparse,\r\n--> 233       input_tensor=tensor)\r\n    234   # Return tensor including `_keras_history`.\r\n    235   # Note that in this case train_output and test_output are the same pointer.\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_layer.py in __init__(self, input_shape, batch_size, dtype, input_tensor, sparse, name, **kwargs)\r\n     77         dtype = backend.floatx()\r\n     78       else:\r\n---> 79         dtype = backend.dtype(input_tensor)\r\n     80     elif input_tensor is not None and input_tensor.dtype != dtype:\r\n     81       raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in dtype(x)\r\n   1025   ```\r\n   1026   \"\"\"\r\n-> 1027   return x.dtype.base_dtype.name\r\n   1028 \r\n   1029 \r\n\r\nAttributeError: 'str' object has no attribute 'base_dtype'\r\n```\r\n\r\n**Describe the expected behavior**\r\nIf I understood correctly, TF Keras is supposed to be interoperable with Feature Column. And the way to achieve that is to wrap a list of feature columns with `tf.keras.layers.DenseFeatures()` and parse it to the input layer as a tensor like so:\r\n\r\n`feature_layer = tf.keras.layers.DenseFeatures(feature_columns)`\r\n`inputs = layers.Input(tensor=feature_layer, name='features')`\r\n\r\n**Code to reproduce the issue**\r\nHere's the code to reproduce the error:\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n#!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ndataframe.head()\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nage = feature_column.numeric_column(\"age\")\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nthal_embedding = feature_column.embedding_column(thal, dimension=8)\r\nthal_hashed = feature_column.categorical_column_with_hash_bucket(\r\n      'thal', hash_bucket_size=1000)\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\n\r\nfeature_columns = []\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n\r\n# bucketized cols\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\n# indicator cols\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\n\r\n# embedding cols\r\nthal_embedding = feature_column.embedding_column(thal, dimension=8)\r\nfeature_columns.append(thal_embedding)\r\n\r\n# crossed cols\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\ncrossed_feature = feature_column.indicator_column(crossed_feature)\r\nfeature_columns.append(crossed_feature)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\ninputs = layers.Input(tensor=feature_layer, name='features')\r\nx = layers.Dense(128, activation='relu')(inputs)\r\nx = layers.Dense(64, activation='relu')(x)\r\n\r\nbaggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = keras.Model(inputs=inputs,outputs=baggage_pred)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n```\r\n\r\n**Other info / logs**\r\nN/A\r\n", "comments": ["i've been struggling with this as well. \r\nI found 2 workarounds but hope there is easier way..\r\n\r\n\r\nFirst method is you subclass `tf.keras.models.Model`  there, you have `inputs` parameter in `def call` method, and you feed it to `DenseFeatures`\r\nBut subclassing keras model has shortcommings in that it can't do `model.save()`\r\n\r\n\r\nSecond method is you define input tensors for all your feature (`Input(name=\"feature_name\")`) when you use functional api to build keras model. and build `feature_inputs = {\"feature_name\" : input_tensor}` dictionary, and feed the dictionary to `tf.keras.layers.DenseFeatures(feature_columns)(feature_inputs)` . \r\n\r\n\r\nSo you have to give input tensors to `feature_layer` ...  not the other way around\r\n\r\nIt would be much easier if one could create `input tensors` from `feature_columns`\r\n", "@littlehome-eugene : Taking on the second method you've suggested, I've referred to the [documentation of DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) and came across the example:\r\n\r\n```\r\nprice = numeric_column('price')\r\nkeywords_embedded = embedding_column(\r\n    categorical_column_with_hash_bucket(\"keywords\", 10K), dimensions=16)\r\ncolumns = [price, keywords_embedded, ...]\r\nfeature_layer = DenseFeatures(columns)\r\n\r\nfeatures = tf.parse_example(..., features=make_parse_example_spec(columns))\r\ndense_tensor = feature_layer(features)\r\nfor units in [128, 64, 32]:\r\n  dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)\r\nprediction = tf.layers.dense(dense_tensor, 1).\r\n```\r\nIt seems `features`, in this regard, refers to the input tensor. It is fed to the feature layer in `dense_tensor = feature_layer(features)`, and finally to the model `dense_tensor = tf.layers.dense(dense_tensor, units, tf.nn.relu)`.\r\n\r\nI have tried the same by defining:\r\n```\r\n...\r\nfeature_layer = tf.keras.layers.DenseFeatures(get_cols()) \r\n# get_cols() returns a list of feature columns\r\n\r\nfeatures = tf.io.parse_example(features=tf.feature_column.make_parse_example_spec(get_cols()))\r\ndense_tensor = feature_layer(features)\r\n\r\ninputs = keras.layers.Input(tensor=dense_tensor, name='features')\r\nx = layers.Dense(128, activation='relu')(inputs)\r\nx = layers.Dense(64, activation='relu')(x)\r\n...\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-60a3dbfaf487> in <module>\r\n      1 feature_layer = tf.keras.layers.DenseFeatures(get_cols())\r\n      2 \r\n----> 3 features = tf.io.parse_example(features=tf.feature_column.make_parse_example_spec(get_cols()))\r\n      4 dense_tensor = feature_layer(features)\r\n      5 \r\n\r\nTypeError: parse_example_v2() missing 1 required positional argument: 'serialized'\r\n```\r\nI'm having trouble defining `serialized` in `tf.io.parse_example`. Any pointers?", "oh, I was just saying I'm trying to solve the same problem, I do have partial solutions as I outlined. \r\nSo you could try what I did before tensorflow has the definitive answer to this.\r\n\r\nWith my limited knowledge of tensorflow (have worked with only a few weeks), \r\nbut I don't think `parse_example` would help here, the `serialized` arguments is the actual data like training data, so you can't use it to define your functional api-based model.  (I had hard time trying to find what it is due to their lack of explanation in the api guide myself :()\r\n\r\n", "@littlehome-eugene: No issue. :) \r\nOkay, so `parse_example` was only meant for demo purposes. \r\n\r\nDigging deeper:\r\n\r\n`features = tf.feature_column.make_parse_example_spec(get_cols())`\r\n`print(features)`\r\n\r\nreturns me a schema/placeholder:\r\n\r\n```\r\n{'aaa': VarLenFeature(dtype=tf.string),\r\n 'bbb': VarLenFeature(dtype=tf.string),\r\n 'count': VarLenFeature(dtype=tf.int64),\r\n...\r\n 'prob_aaa': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None)}\r\n```\r\nI was thinking if it is possible to congregate these placeholders and use it as the input to `Input(shape=...)` instead...\r\n\r\nps. I'm planning to use `Dataset` in batched reading fashion for training. So it wouldn't make sense to parse a tensor directly to the model..", "I think I found another way.. \r\nit's hacky.. but you can try\r\n\r\nI extracted symbolic tensor ( I think it's tensor definition not associated with a data) \r\nand gave them to keras.Input() as a parameter\r\n\r\nhere's a sample\r\n\r\n```\r\n\r\n import copy\r\n import tensorflow as tf\r\n import pandas as pd\r\n from tensorflow.keras.layers import Input, Embedding, concatenate, Dense, Flatten\r\n from tensorflow import feature_column\r\n from tensorflow.python.keras.engine import training_utils\r\n\r\n # tf.compat.v1.disable_eager_execution()\r\n\r\n def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n   dataframe = dataframe.copy()\r\n   labels = dataframe.pop('target')\r\n   d = dict(dataframe)\r\n\r\n   ds = tf.data.Dataset.from_tensor_slices((d, labels))\r\n   if shuffle:\r\n     ds = ds.shuffle(buffer_size=len(dataframe))\r\n   ds = ds.batch(batch_size)\r\n   return ds\r\n\r\n\r\n # import pdb; pdb.set_trace()\r\n # feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\n\r\n class FeatureModel(tf.keras.Model):\r\n\r\n   def __init__(self, *args, **kwargs):\r\n     feature_columns = kwargs.pop('feature_columns', None)\r\n     super().__init__(*args, **kwargs)\r\n\r\n     self.feature_columns = feature_columns\r\n     self.feature_layer = tf.keras.layers.DenseFeatures(self.feature_columns)\r\n     self.dense1 = Dense(1, activation='sigmoid')\r\n\r\n   def call(self, inputs, training=False):\r\n       a_inputs = {k:v for k, v in inputs.items() if k in ['child_month_young', 'child_gender_young']}\r\n       model_inputs = training_utils.ModelInputs(a_inputs)\r\n       feature_inputs = model_inputs.get_symbolic_inputs()\r\n\r\n       self.feature_inputs = feature_inputs\r\n\r\n       features = self.feature_layer(inputs)\r\n\r\n       o = self.dense1(features)\r\n       return o\r\n\r\n\r\n feature_columns = []\r\n child_month_young = feature_column.numeric_column(\"child_month_young\")\r\n kid_age_youngest_buckets = feature_column.bucketized_column(child_month_young, boundaries=[0, 12, 24, 36, 72, 96])\r\n feature_columns.append(kid_age_youngest_buckets)\r\n\r\n child_gender_young = feature_column.categorical_column_with_vocabulary_list(\r\n   'child_gender_young', ['boy', 'girl', ''])\r\n child_gender_young_one_hot = feature_column.indicator_column(child_gender_young)\r\n feature_columns.append(child_gender_young_one_hot)\r\n\r\n class FModel(object):\r\n     def get_model(self, feature_inputs):\r\n         first_input = Input(shape = (1,), name='first_input')\r\n         second_input = Input(shape = (1,), name='second_input' )\r\n\r\n         feature_inputs_d = {}\r\n         for name, tensor in feature_inputs.items():\r\n             input_t = Input(shape=tensor.shape, name=name, tensor=tensor)\r\n             setattr(self, name, input_t)\r\n\r\n             feature_inputs_d[name] = input_t\r\n\r\n         embedding_layer = Embedding(input_dim=10, output_dim=3, input_length=1)\r\n\r\n         first_input_encoded = embedding_layer(first_input)\r\n         first_input_encoded = tf.keras.layers.Reshape((3,))(first_input_encoded)\r\n         second_input_encoded = embedding_layer(second_input)\r\n         second_input_encoded = tf.keras.layers.Reshape((3,))(second_input_encoded)\r\n\r\n         feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n\r\n         features = feature_layer(feature_inputs_d)\r\n         o = concatenate([first_input_encoded, second_input_encoded, features])\r\n         o = Dense(1)(o)\r\n\r\n         inputs = [first_input, second_input] + list(feature_inputs_d.values())\r\n         model = tf.keras.models.Model(inputs=inputs, outputs=o)\r\n         return model\r\n\r\n\r\n df = pd.DataFrame(\r\n     [[3,4,5, 'boy', 0],\r\n     [2,6,7, 'girl', 1]], columns=['first_input', 'second_input', 'child_month_young', 'child_gender_young', 'target'])\r\n\r\n\r\n def get_feature_inputs(train_ds):\r\n\r\n     feature_model = FeatureModel(feature_columns=feature_columns)\r\n     feature_model.compile(optimizer='adam',\r\n                           loss='binary_crossentropy',\r\n                           metrics=['accuracy'])\r\n\r\n\r\n     feature_model.fit(train_ds, epochs=1)\r\n\r\n\r\n     return dict(feature_model.feature_inputs)\r\n\r\n\r\n\r\n train_ds = df_to_dataset(df)\r\n val_ds = df_to_dataset(df)\r\n\r\n # feature_inputs, _, _ = training_utils.extract_tensors_from_dataset(train_ds)\r\n # feature_inputs, _, _ = training_utils.unpack_iterator_input(train_ds)\r\n\r\n feature_inputs = get_feature_inputs(train_ds)\r\n f = FModel()\r\n model = f.get_model(feature_inputs)\r\n\r\n model.compile(optimizer='adam',\r\n               loss='binary_crossentropy',\r\n               metrics=['accuracy'])\r\n\r\n\r\n model.fit(\r\n     train_ds,\r\n     validation_data=val_ds,\r\n     epochs=1\r\n )\r\n\r\n```", "`DenseFeatures.dtype` is None, so it can't be passed into `Input` in existing code.\r\n\r\nnice workaround @littlehome-eugene !\r\n\r\n```\r\n         feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n         features = feature_layer(feature_inputs_d)\r\n```\r\nany syntax sugar to create `map[string]Input feature_inputs_d` per column or from `feature_column` directly?\r\nthen it doesn't need to fit FeatureModel in this way", "@ymodak\r\n\r\nI am having the same issue. Is there any plan to make feature columns work with Keras functional API? Because Keras functional API preserves the static nature of a computation graph, it is more efficient performance-wise.\r\n\r\n\r\n", "Hello. Has there been any progress on this issue? I wanted to use the Feature Columns with the Functional API as well. Is there a good workaround to this issue?", "I also wanted to use the Feature Columns with the Functional API. Waiting for a good workaround to this issue.", "Apologies for the delay. Will take a look today.\n\nOn Thu, Jun 13, 2019 at 1:18 AM admu <notifications@github.com> wrote:\n\n> I also wanted to use the Feature Columns with the Functional API. Waiting\n> for a good workaround to this issue.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27416?email_source=notifications&email_token=AKEVL2AFAV4GS6SNAN2E5R3P2H7EJA5CNFSM4HC55GZKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXS4PTI#issuecomment-501598157>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKEVL2HA25XU7SY6EYU2QETP2H7EJANCNFSM4HC55GZA>\n> .\n>\n", "[How to build a wide-and-deep model using Keras in TensorFlow 2.0 (Using Feature columns with the Keras Functional API)](https://towardsdatascience.com/how-to-build-a-wide-and-deep-model-using-keras-in-tensorflow-2-0-2f7a236b5a4b)", "I am also trying to make feature column work with Keras function API. waiting for a reasonable solution.", "As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:\r\n\r\n```python\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n#!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ndataframe.head()\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nage = feature_column.numeric_column(\"age\")\r\n\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n  feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n\r\n# bucketized cols\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\n# indicator cols\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\nfeature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n\r\n# embedding cols\r\nthal_embedding = feature_column.embedding_column(thal, dimension=8)\r\nfeature_columns.append(thal_embedding)\r\n\r\n# crossed cols\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\ncrossed_feature = feature_column.indicator_column(crossed_feature)\r\nfeature_columns.append(crossed_feature)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\r\n\r\nx = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\nx = layers.Dense(64, activation='relu')(x)\r\n\r\nbaggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(train_ds)\r\n```", "@tanzhenyu \r\nThank you so much for the code example. Appreciate it!\r\nI noticed that inside of the code, embedding columns and crossed columns are not added to feature_layer_inputs, is that intentional?\r\nif feature_layer_inputs doesn't include the two left-out columns, the respective data won't flow through the network, right?\r\n\r\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\r\n", "Right, that's why I mentioned it needs to be \"originating columns\", i.e. those which is actually your input that you will feed in. embedding is derived from you categorical column, and your input (from your data pipeline or dataframe in this case) doesn't contain that, so no need to create input for it.", "> @tanzhenyu\r\n> Thank you so much for the code example. Appreciate it!\r\n> I noticed that inside of the code, embedding columns and crossed columns are not added to feature_layer_inputs, is that intentional?\r\n> if feature_layer_inputs doesn't include the two left-out columns, the respective data won't flow through the network, right?\r\n> \r\n> feature_layer_outputs = feature_layer(feature_layer_inputs)\r\n\r\nTo add on to @tanzhenyu's response, note the below line:\r\n\r\n`feature_layer = tf.keras.layers.DenseFeatures(feature_columns)`\r\n\r\nThis handles the feature column transformation required during model definition. Meanwhile, during model compilation:\r\n\r\n`keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)`\r\n\r\nfeature_layer_inputs passes in the actual input (original features) to the model.\r\n\r\nHope it helps ;)\r\n\r\nThank you @tanzhenyu for the codes! I shall try this out when I have the chance to. Kudos!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27416\">No</a>\n", "I saved and loaded it using:\r\n`model.save('./model/structed_data_model.h5')`\r\n`new_model = keras.models.load_model('./model/structed_data_model.h5')`\r\nIt prompt error\uff1a\r\n`ValueError: Unknown layer: DenseFeatures`\r\n[https://github.com/tensorflow/tensorflow/issues/27008](url)\r\nSo,how can i load it correctly", "@admu thanks for the heads up. It is going to be a blocking issue if we are not able to save/load models.\r\n\r\nAnother issue I hit is that after update tensorflow installation to 2.0.0-beta1, the code example from @tanzhenyu is broken with the following errors. Does this smell like a bug? \r\nTensorflow team, could you please comment on this breakage? Thank you.\r\n\r\n******\r\n49 validation examples\r\n61 test examples\r\nTraceback (most recent call last):\r\n  File \"/tmp/zeppelin_pyspark-5282677307998123760.py\", line 326, in <module>\r\n    exec(code)\r\n  File \"<stdin>\", line 44, in <module>\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 662, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 166, in wrapper\r\n    ), args, kwargs)\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 340, in converted_call\r\n    if inspect_utils.isbuiltin(f):\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 84, in isbuiltin\r\n    if f in six.moves.builtins.__dict__.values():\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/pandas/core/generic.py\", line 1478, in __nonzero__\r\n    .format(self.__class__.__name__))\r\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"/tmp/zeppelin_pyspark-5282677307998123760.py\", line 333, in <module>\r\n    raise Exception(traceback.format_exc())\r\nException: Traceback (most recent call last):\r\n  File \"/tmp/zeppelin_pyspark-5282677307998123760.py\", line 326, in <module>\r\n    exec(code)\r\n  File \"<stdin>\", line 44, in <module>\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 662, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 166, in wrapper\r\n    ), args, kwargs)\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 340, in converted_call\r\n    if inspect_utils.isbuiltin(f):\r\n  File \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 84, in isbuiltin\r\n    if f in six.moves.builtins.__dict__.values():\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/pandas/core/generic.py\", line 1478, in __nonzero__\r\n    .format(self.__class__.__name__))\r\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().", "Since this issue is closed. I opened a new one for discussing the compatibility issue: https://github.com/tensorflow/tensorflow/issues/29923", "Given the model trained this way, did anyone successfully feed data into model.predict()? Could you share the code snippet as well.\r\n\r\nThe following code snippet doesn't seem to work:\r\n```\r\ndef df_to_dataset_for_inference(dataframe, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n  \r\nmodel.predict(df_to_dataset_for_inference(test))\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/zeppelin_pyspark-669659483191650312.py\", line 331, in <module>\r\n    exec(code)\r\n  File \"<stdin>\", line 6, in <module>\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1143, in predict\r\n    callbacks=callbacks)\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 257, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 531, in predict_on_batch\r\n    return model.predict_on_batch(x)\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1368, in predict_on_batch\r\n    ops.convert_to_tensor(val, dtype=K.floatx()) for val in inputs]\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1368, in <listcomp>\r\n    ops.convert_to_tensor(val, dtype=K.floatx()) for val in inputs]\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1050, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1108, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in internal_convert_to_tensor\r\n    value = _TensorTensorConversionFunction(value, dtype=dtype)\r\n  File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 984, in _TensorTensorConversionFunction\r\n    (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'tf.Tensor(\\n[[2]\\n [2]\\n [3]\\n [1]\\n [1]\\n [2]\\n [1]\\n [1]\\n [2]\\n [2]\\n [1]\\n [2]\\n [2]\\n [1]\\n [1]\\n [2]\\n [1]\\n [2]\\n [1]\\n [2]\\n [1]\\n [2]\\n [2]\\n [1]\\n [2]\\n [1]\\n [1]\\n [1]\\n [2]\\n [2]\\n [3]\\n [1]], shape=(32, 1), dtype=int32)'\r\n\r\n\r\n", "> As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:\r\n> \r\n> ```python\r\n> from __future__ import absolute_import, division, print_function\r\n> \r\n> import numpy as np\r\n> import pandas as pd\r\n> \r\n> #!pip install tensorflow==2.0.0-alpha0\r\n> import tensorflow as tf\r\n> \r\n> from tensorflow import feature_column\r\n> from tensorflow import keras\r\n> from tensorflow.keras import layers\r\n> from sklearn.model_selection import train_test_split\r\n> \r\n> URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\n> dataframe = pd.read_csv(URL)\r\n> dataframe.head()\r\n> \r\n> train, test = train_test_split(dataframe, test_size=0.2)\r\n> train, val = train_test_split(train, test_size=0.2)\r\n> print(len(train), 'train examples')\r\n> print(len(val), 'validation examples')\r\n> print(len(test), 'test examples')\r\n> \r\n> # A utility method to create a tf.data dataset from a Pandas Dataframe\r\n> def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n>   dataframe = dataframe.copy()\r\n>   labels = dataframe.pop('target')\r\n>   ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n>   if shuffle:\r\n>     ds = ds.shuffle(buffer_size=len(dataframe))\r\n>   ds = ds.batch(batch_size)\r\n>   return ds\r\n> \r\n> batch_size = 5 # A small batch sized is used for demonstration purposes\r\n> train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> \r\n> age = feature_column.numeric_column(\"age\")\r\n> \r\n> feature_columns = []\r\n> feature_layer_inputs = {}\r\n> \r\n> # numeric cols\r\n> for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n>   feature_columns.append(feature_column.numeric_column(header))\r\n>   feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n> \r\n> # bucketized cols\r\n> age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\n> feature_columns.append(age_buckets)\r\n> \r\n> # indicator cols\r\n> thal = feature_column.categorical_column_with_vocabulary_list(\r\n>       'thal', ['fixed', 'normal', 'reversible'])\r\n> thal_one_hot = feature_column.indicator_column(thal)\r\n> feature_columns.append(thal_one_hot)\r\n> feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n> \r\n> # embedding cols\r\n> thal_embedding = feature_column.embedding_column(thal, dimension=8)\r\n> feature_columns.append(thal_embedding)\r\n> \r\n> # crossed cols\r\n> crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\n> crossed_feature = feature_column.indicator_column(crossed_feature)\r\n> feature_columns.append(crossed_feature)\r\n> \r\n> batch_size = 32\r\n> train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> \r\n> feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n> feature_layer_outputs = feature_layer(feature_layer_inputs)\r\n> \r\n> x = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\n> x = layers.Dense(64, activation='relu')(x)\r\n> \r\n> baggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n> \r\n> model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)\r\n> \r\n> model.compile(optimizer='adam',\r\n>               loss='binary_crossentropy',\r\n>               metrics=['accuracy'])\r\n> \r\n> model.fit(train_ds)\r\n> ```\r\n\r\n@tanzhenyu\r\nThe solution works in TF2.0 beta, except it has two major problems:\r\n1. You cannot save the model. Adding ```model.save('wide_n_deep')``` to the end of the code fails and gives you this error:\r\n```\r\n    TypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: model/dense_features/age_bucketized_X_thal_indicator/thal_lookup/Const:0\r\n```\r\n\r\n2. You cannot run it in non-eager mode: ```tf.compat.v1.disable_eager_execution()```. It gives this error:\r\n```\r\n2019-06-24 19:02:02.217642: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at lookup_table_op.cc:809 : Failed precondition: Table not initialized.\r\nTraceback (most recent call last):\r\n  File \"fc_test.py\", line 91, in <module>\r\n    model.fit(train_ds)\r\n  File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 664, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 294, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3353, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.\r\n\t [[{{node dense_features/age_bucketized_X_thal_indicator/hash_table_Lookup/LookupTableFindV2}}]]\r\n```\r\n\r\nDo you have recommendations on how to fix these?", "> Given the model trained this way, did anyone successfully feed data into model.predict()? Could you share the code snippet as well.\r\n> \r\n> The following code snippet doesn't seem to work:\r\n> \r\n> ```\r\n> def df_to_dataset_for_inference(dataframe, batch_size=32):\r\n>   dataframe = dataframe.copy()\r\n>   ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\r\n>   ds = ds.batch(batch_size)\r\n>   return ds\r\n>   \r\n> model.predict(df_to_dataset_for_inference(test))\r\n> ```\r\n> \r\n> Traceback (most recent call last):\r\n> File \"/tmp/zeppelin_pyspark-669659483191650312.py\", line 331, in \r\n> exec(code)\r\n> File \"\", line 6, in \r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1143, in predict\r\n> callbacks=callbacks)\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 257, in model_iteration\r\n> batch_outs = batch_function(*batch_data)\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 531, in predict_on_batch\r\n> return model.predict_on_batch(x)\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1368, in predict_on_batch\r\n> ops.convert_to_tensor(val, dtype=K.floatx()) for val in inputs]\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1368, in \r\n> ops.convert_to_tensor(val, dtype=K.floatx()) for val in inputs]\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1050, in convert_to_tensor\r\n> return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1108, in convert_to_tensor_v2\r\n> as_ref=False)\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in internal_convert_to_tensor\r\n> value = _TensorTensorConversionFunction(value, dtype=dtype)\r\n> File \"/usr/lib/envs/env-2007-ver-2424-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 984, in _TensorTensorConversionFunction\r\n> (dtype.name, t.dtype.name, str(t)))\r\n> ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'tf.Tensor(\\n[[2]\\n [2]\\n [3]\\n [1]\\n [1]\\n [2]\\n [1]\\n [1]\\n [2]\\n [2]\\n [1]\\n [2]\\n [2]\\n [1]\\n [1]\\n [2]\\n [1]\\n [2]\\n [1]\\n [2]\\n [1]\\n [2]\\n [2]\\n [1]\\n [2]\\n [1]\\n [1]\\n [1]\\n [2]\\n [2]\\n [3]\\n [1]], shape=(32, 1), dtype=int32)'\r\n\r\nThis should get fixed, can you try install tf 2.0 nightly?", "> > As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:\r\n> > ```python\r\n> > from __future__ import absolute_import, division, print_function\r\n> > \r\n> > import numpy as np\r\n> > import pandas as pd\r\n> > \r\n> > #!pip install tensorflow==2.0.0-alpha0\r\n> > import tensorflow as tf\r\n> > \r\n> > from tensorflow import feature_column\r\n> > from tensorflow import keras\r\n> > from tensorflow.keras import layers\r\n> > from sklearn.model_selection import train_test_split\r\n> > \r\n> > URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\n> > dataframe = pd.read_csv(URL)\r\n> > dataframe.head()\r\n> > \r\n> > train, test = train_test_split(dataframe, test_size=0.2)\r\n> > train, val = train_test_split(train, test_size=0.2)\r\n> > print(len(train), 'train examples')\r\n> > print(len(val), 'validation examples')\r\n> > print(len(test), 'test examples')\r\n> > \r\n> > # A utility method to create a tf.data dataset from a Pandas Dataframe\r\n> > def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n> >   dataframe = dataframe.copy()\r\n> >   labels = dataframe.pop('target')\r\n> >   ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n> >   if shuffle:\r\n> >     ds = ds.shuffle(buffer_size=len(dataframe))\r\n> >   ds = ds.batch(batch_size)\r\n> >   return ds\r\n> > \r\n> > batch_size = 5 # A small batch sized is used for demonstration purposes\r\n> > train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> > val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> > test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> > \r\n> > age = feature_column.numeric_column(\"age\")\r\n> > \r\n> > feature_columns = []\r\n> > feature_layer_inputs = {}\r\n> > \r\n> > # numeric cols\r\n> > for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n> >   feature_columns.append(feature_column.numeric_column(header))\r\n> >   feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n> > \r\n> > # bucketized cols\r\n> > age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\n> > feature_columns.append(age_buckets)\r\n> > \r\n> > # indicator cols\r\n> > thal = feature_column.categorical_column_with_vocabulary_list(\r\n> >       'thal', ['fixed', 'normal', 'reversible'])\r\n> > thal_one_hot = feature_column.indicator_column(thal)\r\n> > feature_columns.append(thal_one_hot)\r\n> > feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n> > \r\n> > # embedding cols\r\n> > thal_embedding = feature_column.embedding_column(thal, dimension=8)\r\n> > feature_columns.append(thal_embedding)\r\n> > \r\n> > # crossed cols\r\n> > crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\n> > crossed_feature = feature_column.indicator_column(crossed_feature)\r\n> > feature_columns.append(crossed_feature)\r\n> > \r\n> > batch_size = 32\r\n> > train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> > val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> > test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> > \r\n> > feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n> > feature_layer_outputs = feature_layer(feature_layer_inputs)\r\n> > \r\n> > x = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\n> > x = layers.Dense(64, activation='relu')(x)\r\n> > \r\n> > baggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n> > \r\n> > model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)\r\n> > \r\n> > model.compile(optimizer='adam',\r\n> >               loss='binary_crossentropy',\r\n> >               metrics=['accuracy'])\r\n> > \r\n> > model.fit(train_ds)\r\n> > ```\r\n> \r\n> @tanzhenyu\r\n> The solution works in TF2.0 beta, except it has two major problems:\r\n> \r\n> 1. You cannot save the model. Adding `model.save('wide_n_deep')` to the end of the code fails and gives you this error:\r\n> \r\n> ```\r\n>     TypeError: An op outside of the function building code is being passed\r\n>     a \"Graph\" tensor. It is possible to have Graph tensors\r\n>     leak out of the function building context by including a\r\n>     tf.init_scope in your function building code.\r\n>     For example, the following function will fail:\r\n>       @tf.function\r\n>       def has_init_scope():\r\n>         my_constant = tf.constant(1.)\r\n>         with tf.init_scope():\r\n>           added = my_constant * 2\r\n>     The graph tensor has name: model/dense_features/age_bucketized_X_thal_indicator/thal_lookup/Const:0\r\n> ```\r\n> \r\n> 1. You cannot run it in non-eager mode: `tf.compat.v1.disable_eager_execution()`. It gives this error:\r\n> \r\n> ```\r\n> 2019-06-24 19:02:02.217642: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at lookup_table_op.cc:809 : Failed precondition: Table not initialized.\r\n> Traceback (most recent call last):\r\n>   File \"fc_test.py\", line 91, in <module>\r\n>     model.fit(train_ds)\r\n>   File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n>   File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 664, in fit\r\n>     steps_name='steps_per_epoch')\r\n>   File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 294, in model_iteration\r\n>     batch_outs = f(actual_inputs)\r\n>   File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3353, in __call__\r\n>     run_metadata=self.run_metadata)\r\n>   File \"/temp/v36_test/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n>     run_metadata_ptr)\r\n> tensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.\r\n> \t [[{{node dense_features/age_bucketized_X_thal_indicator/hash_table_Lookup/LookupTableFindV2}}]]\r\n> ```\r\n> \r\n> Do you have recommendations on how to fix these?\r\n\r\nThe first one is very likely a bug, can you create a new github issue for this? Thanks,\r\nThe second one requires you to call sess.run(tf.tables_initializer())", "I use the following hack to solve the prediction issue:\r\n1) Use DenseFeatures layer as a feature transformation that takes inputs from feature columns.\r\n2) Use outputs of DenseFeatures layer as the input to the model.\r\n\r\nAs a result, model load/save as well as predict API works. The following is the complete example:\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.data import Dataset\r\nfrom tensorflow.keras import layers\r\n\r\nprint(\"numpy version: \", np.__version__)\r\nprint(\"pandas version: \", pd.__version__)\r\nprint(\"Tensorflow version: \", tf.__version__)\r\n\r\n\r\ndef train_test_split(data: pd.DataFrame, test_fraction: float = 0.2) -> (pd.DataFrame, pd.DataFrame):\r\n    assert 0.0 < test_fraction < 1.0\r\n    split = int(data.shape[0] * (1.0 - test_fraction))\r\n    return data[:split], data[split:]\r\n\r\n\r\ndef load_google_heart_example_data() -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\r\n    \"\"\" Loads google heart data and split the data into train/validation/test sets.\"\"\"\r\n\r\n    URL = \"https://storage.googleapis.com/applied-dl/heart.csv\"\r\n    dataframe = pd.read_csv(URL)\r\n    train, test = train_test_split(dataframe, test_fraction=0.1)\r\n    train, val = train_test_split(train, test_fraction=0.1)\r\n    return train, val, test\r\n\r\n\r\ndef df_to_dataset(features: np.ndarray, labels: np.ndarray, shuffle=True, batch_size=32) -> Dataset:\r\n    ds = Dataset.from_tensor_slices(({\"feature\": features}, {\"target\": labels}))\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(features))\r\n    ds = ds.batch(batch_size)\r\n    return ds\r\n\r\n\r\ndef get_feature_transform() -> layers.DenseFeatures:\r\n    \"\"\" builds a DenseFeatures layer as feature transformation.\r\n\r\n    The function handles all feature transformation such as bucketizing, vectorizing (one-hot encoding), etc.\r\n    \"\"\"\r\n    feature_columns = []\r\n\r\n    # Numeric columns.\r\n    for feature in [\"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"slope\", \"ca\"]:\r\n        feature_columns.append(feature_column.numeric_column(feature))\r\n\r\n    # Bucketized column.\r\n    age = feature_column.numeric_column(\"age\")\r\n    age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\n    feature_columns.append(age_buckets)\r\n\r\n    # Indicator columns.\r\n    thal = feature_column.categorical_column_with_vocabulary_list(\r\n        \"thal\", [\"fixed\", \"normal\", \"reversible\"])\r\n    thal_one_hot = feature_column.indicator_column(thal)\r\n    feature_columns.append(thal_one_hot)\r\n\r\n    return layers.DenseFeatures(feature_columns)\r\n\r\n\r\ndef get_model(input_shape: tuple, learning_rate: float = 0.01) -> keras.Model:\r\n    \"\"\" Constructs a model using various layers and compiles the model with proper optimizer/loss/metrics.\"\"\"\r\n\r\n    inputs = keras.Input(shape=input_shape, name=\"feature\")\r\n    x = layers.Dense(8, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_layer_1\")(inputs)\r\n    x = layers.Dropout(0.2, name=\"dropout_1\")(x)\r\n    x = layers.Dense(6, kernel_initializer=\"normal\", activation=\"relu\", name=\"hidden_layer_2\")(x)\r\n    x = layers.Dropout(0.2, name=\"dropout_2\")(x)\r\n    baggage_pred = layers.Dense(1, activation=\"sigmoid\", name=\"target\")(x)\r\n\r\n    model = keras.Model(inputs=inputs, outputs=baggage_pred, name=\"keras/tensorflow_example\")\r\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\r\n                  loss=\"binary_crossentropy\",\r\n                  metrics=[\"accuracy\"])\r\n    return model\r\n\r\n\r\ndef main() -> None:\r\n    # Loads data from internet.\r\n    train, val, test = load_google_heart_example_data()\r\n    print(\"Sample training data:\")\r\n    print(train.head(10))\r\n\r\n    # Transforms all features into dense tensors.\r\n    feature_transform = get_feature_transform()\r\n    train_features = feature_transform(dict(train)).numpy()\r\n    val_features = feature_transform(dict(val)).numpy()\r\n    test_features = feature_transform(dict(test)).numpy()\r\n\r\n    # Defines datasets on the input data.\r\n    batch_size = 8\r\n    train_ds = df_to_dataset(train_features, train[\"target\"].values, batch_size=batch_size)\r\n    val_ds = df_to_dataset(val_features, val[\"target\"].values, shuffle=False, batch_size=batch_size)\r\n    test_ds = df_to_dataset(test_features, test[\"target\"].values, shuffle=False, batch_size=batch_size)\r\n\r\n    # Compiles a model, prints the model summary, and saves the model diagram into a png file.\r\n    model = get_model(input_shape=(train_features.shape[1],), learning_rate=0.01)\r\n    model.summary()\r\n    keras.utils.plot_model(model, \"keras_model.png\", show_shapes=True)\r\n\r\n    # Trains the model.\r\n    history = model.fit(train_ds, epochs=10, validation_data=val_ds)\r\n    print(\"Training history\")\r\n    print(history.history)\r\n\r\n    # Evaluates on test data.\r\n    test_results = model.evaluate(test_ds)\r\n    print(\"test result\")\r\n    print(test_results)\r\n\r\n    # Runs prediction on test data.\r\n    predictions = model.predict({\"feature\": test_features})\r\n    print(\"Predictions on test data:\")\r\n    print(predictions)\r\n\r\n    # Saves and reloads model.\r\n    model.save(\"./model.h5\")\r\n    model_from_saved = keras.models.load_model(\"./model.h5\")\r\n    model_from_saved.summary()\r\n\r\n    # Runs test data through the reloaded model to make sure the results are same.\r\n    predictions_from_saved = model_from_saved.predict({\"feature\": test_features})\r\n    np.testing.assert_array_equal(predictions_from_saved, predictions)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```", "@HubberDev  Your solution works, but it confuses tensors and numpy arrays when transforming features. It is generally fine since TF 2.0 enables Eager mode by default.\r\n\r\nHowever, turning off the Eager mode, TF will complains about this line\r\n```train_features = feature_transform(dict(train)).numpy()```\r\nbecause the result of ```feature_transform``` is a tensor, but we need it in ndarray form to make a dataset later from it.\r\n\r\nAlso, saving in hd5 works fine, but if you remove the ```.h5``` extension to try to save the model in the SavedModel format, it won't save it correctly. When you load the SavedModel and try to print ```model.summary()``` it will complain:\r\n```\r\nValueError: You tried to call `count_params` on feature, but the layer isn't built. You can build it manually via: `feature.build(batch_input_shape)`.\r\n```\r\nI think in general there is a disconnect between the FeatureColumn API and Keras API. So far all the examples I worked on which uses these two APIs fail in the non-eager mode. ", "@tanzhenyu \r\n1. For the saving issue, I am hesitant to create a new issue, and prefer to wait for the status of these issues: #26835 and #28923 which I think are highly correlated.\r\n\r\n2. For running the example in the non-eager mode, you recommended to add ```sess.run(tf.tables_initializer())``` to the code. How do you write that in TF 2.0? I thought we are not supposed to use Sessions in this version.", "@mahzoon Since you already did tf.compat.v1.disable_eager_execution() you are already in graph mode so you can also use tf.compat.v1.Session.", "Thanks, this solved the table initialization problem in non-eager mode:\r\n```python\r\nwith tf.compat.v1.Session() as sess:\r\n    sess.run(tf.compat.v1.initialize_all_tables())\r\n    model.fit(train_ds)\r\n```\r\n", "Unable to use FeatureColumn with Keras Functional API\u00ab\r\nHave a better solution\r\n\uff1f\uff1f\uff1f", "Any news about the saving issue?", "What issue is remaining regarding this? ", "@tanzhenyu  when I try to save this model:\r\n\r\n```\r\n       input = [v for v in feature_layer_inputs.values()]\r\n        x = Dense(2048, activation='relu')(feature_layer)\r\n        x = Dropout(0.5)(x)\r\n        x = Dense(1024, activation='relu')(x)\r\n        x = Dropout(0.5)(x)\r\n        x = Dense(512, activation='relu')(x)\r\n        x = Dropout(0.5)(x)\r\n        x = Dense(256, activation='relu')(x)\r\n        x = Dropout(0.5)(x)\r\n        out = Dense(1, activation='sigmoid')(x)\r\n        self.model = Model(inputs=[input], outputs=out)\r\n        self.model.summary()\r\n        self.model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])\r\n```\r\n\r\n\r\nI obtain the following error:\r\n\r\n```Traceback (most recent call last):\r\n  File \"classifier.py\", line 165, in <module>\r\n    main()\r\n  File \"classifier.py\", line 148, in main\r\n    c.save_model(filename, stats)\r\n  File \"classifier.py\", line 108, in save_model\r\n    self.model.save('{}.model'.format(filename))\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/models.py\", line 65, in save\r\n    self.model.save(file_model)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1157, in save\r\n    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 107, in save_model\r\n    saved_model_save.save(model, filepath, overwrite, include_optimizer)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py\", line 85, in save\r\n    save_lib.save(model, filepath)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 855, in save\r\n    meta_graph_def, saveable_view, signatures)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 585, in _fill_meta_graph_def\r\n    signatures = _generate_signatures(signature_functions, resource_map)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 459, in _generate_signatures\r\n    function, mapped_inputs, resource_map)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 411, in _call_function_with_mapped_captures\r\n    function.graph.captures, resource_map)\r\n  File \"/home/sartiano/projects/suspicious-network-event-recognition-2019/.env/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 333, in _map_captures_to_created_tensors\r\n    .format(interior))\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_106:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\n\r\n\r\n", "That is not related to the original issue. Can you file another github issue for this?", "For anyone landing here using tf 2.0 ... DO NOT START with the keras functional API if you are doing anything other than vanilla regression. Subclass API is first class now and much simpler. Start there.\r\n\r\nI'm looking for tf.feature_columns examples in that context. Landed here from google search. Post refs for the next person if you have any.", "> i've been struggling with this as well.\r\n> I found 2 workarounds but hope there is easier way..\r\n> \r\n> First method is you subclass `tf.keras.models.Model` there, you have `inputs` parameter in `def call` method, and you feed it to `DenseFeatures`\r\n> But subclassing keras model has shortcommings in that it can't do `model.save()`\r\n> \r\n> Second method is you define input tensors for all your feature (`Input(name=\"feature_name\")`) when you use functional api to build keras model. and build `feature_inputs = {\"feature_name\" : input_tensor}` dictionary, and feed the dictionary to `tf.keras.layers.DenseFeatures(feature_columns)(feature_inputs)` .\r\n> \r\n> So you have to give input tensors to `feature_layer` ... not the other way around\r\n> \r\n> It would be much easier if one could create `input tensors` from `feature_columns`\r\n\r\nThis is quit the only two ways I found....since there has been 6 months since you post this comment, is there any other solution?", "@littlebeandog I was able to find a workaround to get past the above error (but I'm still figuring out how to initialize the table in TF2.0) I think this is a bit cleaner, but maybe still suboptimal.\r\n\r\n```\r\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n    features,labels = tf.data.experimental.get_single_element(training_dataset.take(1))\r\n    \r\n    dense_inputs = feature_layer(features)\r\n```\r\n\r\nThe inputs to the keras model would then be dense_inputs.", "The workarounds above don't cover the case where the `FeatureColumn`s contain trainable parameters which require the gradients during model training; e.g. embedding columns. In the case of an embedding, gradients are needed to refine the embedding during training, and precomputing via a `DenseFeatures` layer is not a workable solution for this use -- if I am understanding correctly, the above examples would just generate randomly-initialized embeddings that are never actually learning anything during the training iterations.\r\n\r\nAre there any solutions which don't require placing the `DenseFeatures` outside the `Model`?", "@jpgard I believe that the above workarounds actually include the DenseFeatures params in the model despite the funky calling. Running model.summary() on tanzhenyu's workaround gives me the output shown below. Note that the DenseFeatures layers are in the middle and have trainable params as desired. \r\n\r\n```\r\nModel: \"model_2\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\nage (InputLayer)                [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nca (InputLayer)                 [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nchol (InputLayer)               [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\noldpeak (InputLayer)            [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nslope (InputLayer)              [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nthal (InputLayer)               [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nthalach (InputLayer)            [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\ntrestbps (InputLayer)           [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\ndense_features_2 (DenseFeatures (None, 1029)         24          age[0][0]                        \r\n                                                                 ca[0][0]                         \r\n                                                                 chol[0][0]                       \r\n                                                                 oldpeak[0][0]                    \r\n                                                                 slope[0][0]                      \r\n                                                                 thal[0][0]                       \r\n                                                                 thalach[0][0]                    \r\n                                                                 trestbps[0][0]                   \r\n__________________________________________________________________________________________________\r\ndense_6 (Dense)                 (None, 128)          131840      dense_features_2[0][0]           \r\n__________________________________________________________________________________________________\r\ndense_7 (Dense)                 (None, 64)           8256        dense_6[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_8 (Dense)                 (None, 1)            65          dense_7[0][0]                    \r\n==================================================================================================\r\nTotal params: 140,185\r\nTrainable params: 140,185\r\nNon-trainable params: 0\r\n```", "Ah, I see, thanks @JoshEZiegler ", "> As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:\r\n> \r\n> ```python\r\n> from __future__ import absolute_import, division, print_function\r\n> \r\n> import numpy as np\r\n> import pandas as pd\r\n> \r\n> #!pip install tensorflow==2.0.0-alpha0\r\n> import tensorflow as tf\r\n> \r\n> from tensorflow import feature_column\r\n> from tensorflow import keras\r\n> from tensorflow.keras import layers\r\n> from sklearn.model_selection import train_test_split\r\n> \r\n> URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\n> dataframe = pd.read_csv(URL)\r\n> dataframe.head()\r\n> \r\n> train, test = train_test_split(dataframe, test_size=0.2)\r\n> train, val = train_test_split(train, test_size=0.2)\r\n> print(len(train), 'train examples')\r\n> print(len(val), 'validation examples')\r\n> print(len(test), 'test examples')\r\n> \r\n> # A utility method to create a tf.data dataset from a Pandas Dataframe\r\n> def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n>   dataframe = dataframe.copy()\r\n>   labels = dataframe.pop('target')\r\n>   ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n>   if shuffle:\r\n>     ds = ds.shuffle(buffer_size=len(dataframe))\r\n>   ds = ds.batch(batch_size)\r\n>   return ds\r\n> \r\n> batch_size = 5 # A small batch sized is used for demonstration purposes\r\n> train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> \r\n> age = feature_column.numeric_column(\"age\")\r\n> \r\n> feature_columns = []\r\n> feature_layer_inputs = {}\r\n> \r\n> # numeric cols\r\n> for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n>   feature_columns.append(feature_column.numeric_column(header))\r\n>   feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n> \r\n> # bucketized cols\r\n> age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\n> feature_columns.append(age_buckets)\r\n> \r\n> # indicator cols\r\n> thal = feature_column.categorical_column_with_vocabulary_list(\r\n>       'thal', ['fixed', 'normal', 'reversible'])\r\n> thal_one_hot = feature_column.indicator_column(thal)\r\n> feature_columns.append(thal_one_hot)\r\n> feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n> \r\n> # embedding cols\r\n> thal_embedding = feature_column.embedding_column(thal, dimension=8)\r\n> feature_columns.append(thal_embedding)\r\n> \r\n> # crossed cols\r\n> crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\n> crossed_feature = feature_column.indicator_column(crossed_feature)\r\n> feature_columns.append(crossed_feature)\r\n> \r\n> batch_size = 32\r\n> train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> \r\n> feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n> feature_layer_outputs = feature_layer(feature_layer_inputs)\r\n> \r\n> x = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\n> x = layers.Dense(64, activation='relu')(x)\r\n> \r\n> baggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n> \r\n> model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)\r\n> \r\n> model.compile(optimizer='adam',\r\n>               loss='binary_crossentropy',\r\n>               metrics=['accuracy'])\r\n> \r\n> model.fit(train_ds)\r\n> ```\r\n\r\nthank you so much. I also want to ask you a question\uff0cWhen I deploy the model in this way on tf-serving\uff0cI find that every request takes hundreds of milliseconds\uff0cI don't think it's normal. Do you have any other methods other than your one? For example, use a layer of example to accept all feature columns\uff1f", "To all:\r\n\r\nThe interaction between feature columns and Keras is painful. To address this issue, we have proposed Keras preprocessing layers. If you're interested, please put your comment under this [RFC](https://github.com/tensorflow/community/pull/188)", "> As unfortunate as this is, for now you will have to make it work by assigning tf.keras.Input to each original feature column that you have, i.e., those numeric feature column and categorical feature columns. Here's the code snippet that will make it work:\r\n> \r\n> ```python\r\n> from __future__ import absolute_import, division, print_function\r\n> \r\n> import numpy as np\r\n> import pandas as pd\r\n> \r\n> #!pip install tensorflow==2.0.0-alpha0\r\n> import tensorflow as tf\r\n> \r\n> from tensorflow import feature_column\r\n> from tensorflow import keras\r\n> from tensorflow.keras import layers\r\n> from sklearn.model_selection import train_test_split\r\n> \r\n> URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\n> dataframe = pd.read_csv(URL)\r\n> dataframe.head()\r\n> \r\n> train, test = train_test_split(dataframe, test_size=0.2)\r\n> train, val = train_test_split(train, test_size=0.2)\r\n> print(len(train), 'train examples')\r\n> print(len(val), 'validation examples')\r\n> print(len(test), 'test examples')\r\n> \r\n> # A utility method to create a tf.data dataset from a Pandas Dataframe\r\n> def df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n>   dataframe = dataframe.copy()\r\n>   labels = dataframe.pop('target')\r\n>   ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n>   if shuffle:\r\n>     ds = ds.shuffle(buffer_size=len(dataframe))\r\n>   ds = ds.batch(batch_size)\r\n>   return ds\r\n> \r\n> batch_size = 5 # A small batch sized is used for demonstration purposes\r\n> train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> \r\n> age = feature_column.numeric_column(\"age\")\r\n> \r\n> feature_columns = []\r\n> feature_layer_inputs = {}\r\n> \r\n> # numeric cols\r\n> for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n>   feature_columns.append(feature_column.numeric_column(header))\r\n>   feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n> \r\n> # bucketized cols\r\n> age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\n> feature_columns.append(age_buckets)\r\n> \r\n> # indicator cols\r\n> thal = feature_column.categorical_column_with_vocabulary_list(\r\n>       'thal', ['fixed', 'normal', 'reversible'])\r\n> thal_one_hot = feature_column.indicator_column(thal)\r\n> feature_columns.append(thal_one_hot)\r\n> feature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n> \r\n> # embedding cols\r\n> thal_embedding = feature_column.embedding_column(thal, dimension=8)\r\n> feature_columns.append(thal_embedding)\r\n> \r\n> # crossed cols\r\n> crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\n> crossed_feature = feature_column.indicator_column(crossed_feature)\r\n> feature_columns.append(crossed_feature)\r\n> \r\n> batch_size = 32\r\n> train_ds = df_to_dataset(train, batch_size=batch_size)\r\n> val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\n> test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n> \r\n> feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n> feature_layer_outputs = feature_layer(feature_layer_inputs)\r\n> \r\n> x = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\n> x = layers.Dense(64, activation='relu')(x)\r\n> \r\n> baggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n> \r\n> model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)\r\n> \r\n> model.compile(optimizer='adam',\r\n>               loss='binary_crossentropy',\r\n>               metrics=['accuracy'])\r\n> \r\n> model.fit(train_ds)\r\n> ```\r\n\r\nI noticed that in this post when you create your model you get the values of feature_layer_inputs with .values() function. I think this could be dangerous because you cannot gaurantee that the values are in the correct order. I think you should probably use an OrderedDict no?", "I don't believe the above workaround is effective if using this inside an estimator\r\n```\r\ndef mlp_fn(features, labels, mode, params):\r\n    \r\n    feature_layer_inputs = {}\r\n    feature_layer_inputs['dense_input'] = tf.keras.Input(shape=(4,), name='dense_input', dtype=tf.string)\r\n\r\n    \r\n    feature_columns = [\r\n        tf.feature_column.numeric_column('dense_input')\r\n    ]\r\n    \r\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\r\n\r\n    \r\n    # --- create NN ---\r\n    \r\n    # input\r\n    input_layer = tf.keras.layers.DenseFeatures(feature_layer_outputs)\r\n    \r\n    # output\r\n    logits = tf.keras.layers.Dense(units=3, activation=tf.nn.relu)(input_layer)\r\n    output = tf.keras.layers.Softmax(logits)\r\n\r\n    \r\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=output)\r\n```\r\n\r\nIn this case the error \r\n`OperatorNotAllowedInGraphError: iterating over tf.Tensor is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.` \r\nis thrown\r\n", "Any update on this thread?", "@sivaharsh \r\n\r\nPlease review:\r\nhttps://github.com/keras-team/governance/blob/master/rfcs/20190502-preprocessing-layers.md\r\nand\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md \r\n\r\nThe work product of which would 'replace feature columns and tf.keras.layers.DenseFeatures with proposed layers'. \r\n\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/", "any update then?", "Closing this issue. For anyone working on Keras + feature columns, please note that we have now replaced it with Keras Preprocessing Layers (links above)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27416\">No</a>\n"]}, {"number": 27415, "title": "How to improve efficiency when sess.run", "body": "When I test my model using Tensorflow, I wonder how to imporve the speed (or reduce running time) when I execute  session like:\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    model.saver.restore(sess, conf[\"init_model\"])\r\n    feed = { \r\n        model.data: mydata,  \r\n        model.label: mylabel\r\n        }\r\n    scores = sess.run(model.logits, feed_dict = feed)\r\n    losses = sess.run(model.loss, feed_dict = feed)\r\n\r\nAs my model size and number of parameter being fixed (after training), How can I impove the speed when I test model with only little batch (even one batch would be slow) to get scores and losses.\r\nI wonder if taking more GPUs could be help. I am not sure.\r\nDoes there any other idea to solve it? Thank you !", "comments": ["@zysNLP This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@zysNLP Please take a look at the [tutorial](https://www.tensorflow.org/tutorials) section in TF website and most importantly look at the tutorials under \"ML at production scale\". There are several approaches to optimize ML model and there is vast amount of information available (TF website, stackoverflow, github, medium, kaggle, .......).\r\nI am closing the issue here as it is not related to Bug or performance related to TF. GitHub repo is mainly for addressing bugs in installation and performance. Thanks!", "OK, sorry and thankyou very much!"]}, {"number": 27414, "title": "tf.data.Iterator won't release memory when reinitialized", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.10.1\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: NVIDIA GTX1080Ti, 11GB GPU/ 64 GB RAM\r\n\r\n**Describe the current behavior**\r\nWhen testing my model, I had several batches of images with batch size equals to 10. As the images are large, I loaded them from hard drive every time. My code is structured as <br />\r\n```python\r\niterator = tf.data.Iterator.from_structure(output_types=tf.float32, output_shapes=[1536, 1536, 2])\r\noutput  = model_fn(iterator.get_next())\r\nfor images in files:\r\n    dataset = tf.data.Dataset.from_tensor_slices(tf.constant(images))\r\n    sess.run(iterator.make_initializer(dataset))\r\n    output_value = []\r\n    for _ in range(10):\r\n        output_value.append(sess.run(output))\r\n``` \r\nHere each of my `images` is a numpy array with a size of [10, 1536, 1536, 2].<br />\r\nWhile the code running, the memory usage is growing fast -- every image set it read in stayed there in the memory.\r\n\r\n\r\n**Describe the expected behavior**\r\nI'm expecting the iterator release the memory each time it reinitialized.\r\n\r\n\r\n**Code to reproduce the issue**\r\nFor a reproducible test, I also tried this, to see if the memory leakage is due to something else:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\niterator = tf.data.Iterator.from_structure(output_types=tf.float64, output_shapes=[1536,1536,2])\r\nsess = tf.InteractiveSession()\r\nfor i in range(100):\r\n    data = tf.constant(np.random.rand(10,1536,1536,2))\r\n    dataset = tf.data.Dataset.from_tensor_slices(data)\r\n    ite_data = iterator.make_initializer(dataset)\r\n    sess.run(ite_data)\r\n```\r\nI can still see a huge memory usage when executing this. \r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/23904", "@yluo16 visit this issue you will get little bit help.", "> @yluo16 visit this issue you will get little bit help.\r\n\r\nThanks for advising. I'm sorry for raising a existing issue... Maybe I should upgrade to TF 2.0.", "Closing since #23904 captures the explanation of behavior observed in this issue. Feel free to reopen if still have further problems. Thanks!"]}, {"number": 27413, "title": "TfLite squeeze_test extend test case added", "body": "1- Refactor existing test case to simplified version to support additional test case\r\n2- Additional data type test coverage added", "comments": ["@miaout17 Could you PTAL and approve.", "@Dayananda-V Did you get a chance to look on reviewer comments? Please let us know on the update. Thanks!", "Can one of the admins verify this patch?", "@Dayananda-V Gentle ping to check the reviewer comments. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27412, "title": "[XLA] Reduce the logging level for two utilities", "body": "This change reduces the visual clutter by reducing the log level for two of the utility classes in XLA.  It seems to me that log level 1 should be for a high level tracing of the whole system, and the messages that these modules are producing are more like debug trace.", "comments": []}, {"number": 27411, "title": "Lite: Add_n Op refactored", "body": "1:> Dynamic memory allocation moved from Eval to Prepare.\r\n2:> VectorOfTensors refactored to eliminate the limitation of \r\n\r\n> \"can be used only in Eval\"\r\n\r\n.", "comments": ["@shahzadlone : Your comments are addressed, please check, Thanks!", "@haozha111 : Thanks a lot for your valuable comments, all are handled now, please check, Thanks!", "@haozha111 Could you PTAL and approve.", "@haozha111, @shahzadlone Can you please review this PR? Thanks!", "@miaout17 , @haozha111 : Gentle Reminder!!!", "@miaout17 : Thank you for your valuable comments! I have resolved all the comments except one. Please find my comment inline. ", "Can one of the admins verify this patch?", "@miaout17 Can you please take a look on this PR? Thanks!", "@miaout17 , @gbaned : Gentle Reminder!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27410, "title": "TF Lite added missing test cases in comparisons_test", "body": "Missing test cases for data type Int8 are added", "comments": []}, {"number": 27409, "title": "Invalid argument: Conv2DSlowBackpropInput: input and out_backprop must have the same batch size", "body": "I want to use tensorflow to train my model  \r\nfirstly, I use conv2d_transpose in order to realize upsampling, then, I use Batch_normalization after each conv2d_transpose layer, and then I use conv2d and maxpool.  \r\nBut when I train my model , I met a problem, such like this:  \r\n\r\nInvalid argument: Conv2DSlowBackpropInput: input and out_backprop must have the same batch size  input batch: 64  outbackprop batch: 56 batch_dim: 0  \r\n\r\nwhat makes me more puzzled is the training can run some steps, my batch_size=64, when the steps arrives at 101 steps, the training stopped!!!!  \r\n\r\nhere is my deconv functions:  \r\n```python\r\ndef deconv2d(input_, output_shape,\r\n             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\r\n             name=\"deconv2d\", with_w=False):\r\n    with tf.variable_scope(name):\r\n\r\n        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\r\n                            initializer=tf.random_normal_initializer(stddev=stddev))\r\n\r\n        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\r\n                                        strides=[1, d_h, d_w, 1])\r\n\r\n        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\r\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\r\n\r\n        if with_w:\r\n            return deconv, w, biases\r\n        else:\r\n            return deconv\r\n```\r\nand this is how I use this deconv2d fucntion:  \r\n```python\r\nh1 = deconv2d(h0, [batch_size, s_h8, s_w8, f_dim * 4], name=\"deconv1\")\r\n        h1 = tf.nn.relu(d_bn_1(h1))\r\n```\r\n\r\nand this is my batch_normalization fuction:  \r\n```python\r\nclass BatchNorm(object):\r\n    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\r\n        # with tf.variable_scope(name):\r\n        self.epsilon = epsilon\r\n        self.momentum = momentum\r\n        self.name = name\r\n\r\n\r\n    def __call__(self, x, train=True):\r\n        return tf.contrib.layers.batch_norm(x,\r\n                                            decay=self.momentum,\r\n                                            updates_collections=None,\r\n                                            epsilon=self.epsilon,\r\n                                            scale=True,\r\n                                            is_training=train,\r\n                                            scope=self.name)\r\n```\r\nI have tried some methods , such as use tf.reshape() right after the conv2d_transpose.  \r\nBut it didn't work!\r\n\r\nI hope anyone else can help me resolve this problem.  \r\nThank you very much!!!", "comments": ["@liyunfei1994 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error.  Thanks!\r\n\r\n\r\n", "Thanks a lot for helping me!!  \r\nLet me explain my  model firstly.  \r\nMy input is  pressure data. I use TFRecords and tf.data to put the pressure data into the model.  \r\nAnd my label is the flow field pictures. So you can see, I use the supervised learning.  \r\n\r\nIn order to achieve  upsampling, I use conv2d_transpose to convert the pressure data into a picture  4D Tensor.  \r\nSo, I use 4 times  conv2d_transpose, and after each conv2d_transpose, I add a BN layer.  \r\nAfter that, I use 4 times conv2d and max_pool  \r\nfinally\uff0cI add two fully connected layer.  \r\nThis is my code.  \r\n```python\r\nCONV1_SIZE = 2\r\nCONV1_DEPTH = 16\r\n\r\nCONV2_SIZE = 2\r\nCONV2_DEPTH = 32\r\n\r\nCONV3_SIZE = 2\r\nCONV3_DEPTH = 64\r\n\r\nCONV4_SIZE = 2\r\nCONV4_DEPTH = 128\r\n\r\nFC_NODE = 512\r\n\r\n\r\nf_dim = 64\r\nchannel_dim = 3\r\n\r\ndef conv_out_size_same(size, stride):\r\n    return int(math.ceil(float(size) / float(stride)))\r\n\r\n# s_h = 252, 400\r\ns_h, s_w = IMG_HEIGHT, IMG_WIDTH\r\nprint(\"s_h = \", s_h, \"s_w = \", s_w)\r\n# s_h2 = 126, s_w2 = 200\r\ns_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\r\nprint(\"s_h2 = \", s_h2, \"s_w2 = \", s_w2)\r\n# s_h4 = 63, s_h4 = 100\r\ns_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\r\nprint(\"s_h4 = \", s_h4, \"s_w4 = \", s_w4)\r\n# s_h8 = 32, s_h8 = 50\r\ns_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\r\nprint(\"s_h8 = \", s_h8, \"s_w8 = \", s_w8)\r\n# s_h16 = 16, s_h16 = 25\r\ns_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\r\nprint(\"s_h16 = \", s_h16, \"s_w16 = \", s_w16)\r\nprint(\"=\" * 20)\r\n\r\nd_bn_0 = BatchNorm(name=\"d_bn_0\")\r\nd_bn_1 = BatchNorm(name=\"d_bn_1\")\r\nd_bn_2 = BatchNorm(name=\"d_bn_2\")\r\nd_bn_3 = BatchNorm(name=\"d_bn_3\")\r\nd_bn_4 = BatchNorm(name=\"d_bn_4\")\r\n\r\nc_bn_0 = BatchNorm(name=\"c_bn_0\")\r\n\r\n\r\ndef inference(input_tensor, train, regularizer):\r\n\r\n    with tf.variable_scope('input_reshape'):\r\n\r\n        \"convert the pressure data 2-D  to  4-D Tensor\"\r\n        z = linear(input_=input_tensor, output_size=f_dim * 8 * s_h16 * s_w16)\r\n        h0 = tf.reshape(z, [-1, s_h16, s_w16, f_dim * 8])\r\n        h0 = tf.nn.relu(d_bn_0(h0))\r\n        # [None, 16, 25, 512]\r\n        print(\"h0.shape\", h0.get_shape().as_list())\r\n\r\n        # h1 = upsampling(h0, s_h8, s_w8)\r\n        h1 = deconv2d(input_=h0, output_shape=[batch_size, s_h8, s_w8, f_dim * 4], name=\"deconv1\")\r\n        h1 = d_bn_1(h1)\r\n        # [None, 32, 50, 512]\r\n        print(\"h1.shape\", h1.get_shape().as_list())\r\n        # h1 = conv2d(input_=h1, output_dim=f_dim * 4, d_h=1, d_w=1, name=\"conv_upsample1\")\r\n        # [None, 32, 50, 256]\r\n        # print(\"h1.shape\", h1.get_shape().as_list())\r\n        # h2 = upsampling(h1, s_h4, s_w4)\r\n        h2 = deconv2d(input_=h1, output_shape=[batch_size, s_h4, s_w4, f_dim * 2], name=\"deconv2\")\r\n        h2 = d_bn_2(h2)\r\n        # [None, 63, 100, 256]\r\n        print(\"h2.shape\", h2.get_shape().as_list())\r\n        # h2 = conv2d(input_=h2, output_dim=f_dim * 2, d_h=1, d_w=1, name=\"conv_upsample2\")\r\n        # # [None, 63, 100, 128]\r\n        # print(\"h2.shape\", h2.get_shape().as_list())\r\n        # h3 = upsampling(h2, s_h2, s_w2)\r\n        # [None, 126, 200, 128]\r\n        h3 = deconv2d(input_=h2, output_shape=[batch_size, s_h2, s_w2, f_dim], name=\"deconv3\")\r\n        h3 = d_bn_3(h3)\r\n        print(\"h3.shape\", h3.get_shape().as_list())\r\n        # h3 = conv2d(input_=h3, output_dim=f_dim, d_h=1, d_w=1, name=\"conv_upsample3\")\r\n        # # [None, 126, 200, 64]\r\n        # print(\"h3.shape\", h3.get_shape().as_list())\r\n        # h4 = upsampling(h3, s_h, s_w)\r\n        # [None, 252, 400, 64]\r\n        h4 = deconv2d(input_=h3, output_shape=[batch_size, s_h, s_w, channel_dim], name=\"deconv4\")\r\n        h4 = d_bn_4(h4)\r\n        # print(\"h4.shape\", h4.get_shape().as_list())\r\n        # h4 = conv2d(input_=h4, output_dim=f_dim//2, d_h=1, d_w=1, name=\"conv_upsample4\")\r\n        # # [None, 252, 400, 32]\r\n        print(\"h4.shape\", h4.get_shape().as_list())\r\n\r\n\r\n    print(\"=\" * 20)\r\n\r\n    conv1 = c_bn_0(conv2d(input_=h4, output_dim=CONV1_DEPTH, name=\"conv1\"))\r\n\r\n    with tf.variable_scope(\"pool1\"):\r\n        pool1 = tf.nn.max_pool(\r\n            conv1, ksize=[\r\n                1, 2, 2, 1], strides=[\r\n                1, 2, 2, 1], padding=\"SAME\")\r\n        # POOL1 shape [64,63, 100, 16]\r\n        print(\"POOL1 shape\", pool1.get_shape().as_list())\r\n\r\n    conv2 = conv2d(input_=pool1, output_dim=CONV2_DEPTH, name=\"conv2\")\r\n\r\n    with tf.variable_scope(\"pool2\"):\r\n        pool2 = tf.nn.max_pool(\r\n            conv2, ksize=[\r\n                1, 2, 2, 1], strides=[\r\n                1, 2, 2, 1], padding=\"SAME\")\r\n        # POOL2 shape [64, 16, 25, 32]\r\n        print(\"POOL2 shape\", pool2.get_shape().as_list())\r\n\r\n    conv3 = conv2d(input_=pool2, output_dim=CONV3_DEPTH, name=\"conv3\")\r\n\r\n    with tf.variable_scope(\"pool3\"):\r\n        pool3 = tf.nn.max_pool(\r\n            conv3, ksize=[\r\n                1, 2, 2, 1], strides=[\r\n                1, 2, 2, 1], padding=\"SAME\")\r\n        # POOL3 shape [64, 4, 7, 64]\r\n        print(\"POOL3 shape\", pool3.get_shape().as_list())\r\n\r\n    conv4 = conv2d(input_=pool3, output_dim=CONV4_DEPTH, name=\"conv4\")\r\n\r\n    with tf.variable_scope(\"fullly_reshape\"):\r\n        pool_shape = conv4.get_shape().as_list()\r\n        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\r\n        # [batch_size, 2*4*128]\r\n        reshaped = tf.reshape(conv4, [-1, nodes])\r\n        # reshaped shape [64, 1024]\r\n        print(\"reshaped shape\", reshaped.get_shape().as_list())\r\n\r\n    with tf.variable_scope(\"fully1\"):\r\n\r\n        with tf.name_scope(\"weights\"):\r\n            fc1_weights = tf.get_variable(\r\n                name=\"fc1_weights\",\r\n                shape=[\r\n                    nodes,\r\n                    FC_NODE],\r\n                initializer=tf.truncated_normal_initializer(\r\n                    stddev=0.1))\r\n            print(\"fc1_weights\", fc1_weights.get_shape())\r\n            variable_summaries(fc1_weights, \"fully1\" + \"/weights\")\r\n        if regularizer is not None:\r\n            tf.add_to_collection('losses', regularizer(fc1_weights))\r\n\r\n        with tf.name_scope(\"biases\"):\r\n            fc1_biases = tf.get_variable(\r\n                name=\"fc1_biases\",\r\n                shape=[FC_NODE],\r\n                initializer=tf.constant_initializer(0.1))\r\n            variable_summaries(fc1_biases, \"fully1\" + \"/biases\")\r\n        # [batch_size, FC_NODE]\r\n        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\r\n\r\n    with tf.variable_scope(\"fully2\"):\r\n\r\n        with tf.name_scope(\"weights\"):\r\n            fc2_weights = tf.get_variable(\r\n                name=\"fc2_weights\",\r\n                shape=[\r\n                    FC_NODE,\r\n                    NUM_LABELS],\r\n                initializer=tf.truncated_normal_initializer(\r\n                    stddev=0.1))\r\n            print(\"fc2_weights\", fc2_weights.get_shape())\r\n            variable_summaries(fc2_weights, \"fully2\" + \"/weights\")\r\n        if regularizer is not None:\r\n            tf.add_to_collection(\"losses\", regularizer(fc2_weights))\r\n\r\n        with tf.name_scope(\"biases\"):\r\n            fc2_biases = tf.get_variable(\r\n                name=\"fc2_biases\",\r\n                shape=[NUM_LABELS],\r\n                initializer=tf.constant_initializer(0.1))\r\n            variable_summaries(fc2_biases, \"fully2\" + \"/biases\")\r\n        # [batch_size, 252*400*3]\r\n        fc2 = tf.matmul(fc1, fc2_weights)+fc2_biases\r\n\r\n    with tf.variable_scope(\"output_reshape\"):\r\n        output_array = tf.reshape(fc2, [-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL])\r\n        print(\"output_shape\", output_array.get_shape().as_list())\r\n\r\n    tf.summary.histogram(\"output_array\", output_array)\r\n    \"\"\"[64, 252, 400, 3]\"\"\"\r\n    return output_array\r\n\r\n```\r\n\r\nAnd this is my deconv2d and linear and conv2d functions.  \r\n```python\r\nclass BatchNorm(object):\r\n    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\r\n        with tf.variable_scope(name):\r\n            self.epsilon = epsilon\r\n            self.momentum = momentum\r\n            self.name = name\r\n\r\n\r\n    def __call__(self, x, train=True):\r\n        return tf.contrib.layers.batch_norm(x,\r\n                                            decay=self.momentum,\r\n                                            updates_collections=None,\r\n                                            epsilon=self.epsilon,\r\n                                            scale=True,\r\n                                            is_training=train,\r\n                                            scope=self.name)\r\n\r\ndef linear(input_, output_size, scope=\"Linear\", stddev=0.1, bias_start=0.1, with_w=False):\r\n\r\n    shape = input_.get_shape().as_list()\r\n\r\n    with tf.variable_scope(scope):\r\n        try:\r\n            matrix = tf.get_variable(name=\"weights\", shape=[shape[1], output_size], dtype=tf.float32,\r\n                                     initializer=tf.truncated_normal_initializer(stddev=stddev))\r\n            variable_summaries(matrix, scope+\"weights\")\r\n        except ValueError as err:\r\n            msg = \"NOTE: Usually, this is due to an issue with the image dimensions. \" \\\r\n                  \" Did you correctly set '--crop' or '--input_height' or '--output_height'?\"\r\n            err.args = err.args + (msg,)\r\n            raise\r\n        bias = tf.get_variable(\"bias\", [output_size],\r\n                               initializer=tf.constant_initializer(bias_start))\r\n        variable_summaries(bias, scope+\"bias\")\r\n        # print(\"with_w\u4e3aFalse,\u53ea\u8fd4\u56de\u4e86\u77e9\u9635\u76f8\u4e58\u7684\u7ed3\u679c\")\r\n        return tf.matmul(input_, matrix) + bias\r\n\r\ndef conv2d(input_, output_dim,\r\n        k_h=2, k_w=2, d_h=2, d_w=2, stddev=0.1,\r\n        name=\"conv2d\"):\r\n    with tf.variable_scope(name):\r\n        w = tf.get_variable('weights', [k_h, k_w, input_.get_shape()[-1], output_dim],\r\n              initializer=tf.truncated_normal_initializer(stddev=stddev))\r\n        variable_summaries(w, name+\"weights\")\r\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\r\n\r\n        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\r\n        variable_summaries(biases, name+\"biases\")\r\n        conv = tf.nn.relu(tf.nn.bias_add(conv, biases))\r\n        print(name+\".shape\", conv.get_shape().as_list())\r\n\r\n    return conv\r\n\r\ndef deconv2d(input_, output_shape,\r\n             k_h=2, k_w=2, d_h=2, d_w=2, stddev=0.1,\r\n             name=\"deconv2d\"):\r\n    with tf.variable_scope(name):\r\n        # filter : [height, width, output_channels, in_channels]\r\n        w = tf.get_variable(name='w', shape=[k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\r\n                            initializer=tf.random_normal_initializer(stddev=stddev))\r\n        variable_summaries(w, name+\"_weights\")\r\n        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\r\n                                        strides=[1, d_h, d_w, 1])\r\n        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.1))\r\n        variable_summaries(biases, name + \"_bias\")\r\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\r\n\r\n        return deconv\r\n```\r\nAnd this is the Error:  \r\nAfter 1 training steps, loss on training batch is 7995.41\r\nAfter 101 training steps, loss on training batch is 13560203.00\r\nAfter 201 training steps, loss on training batch is 13308757.00\r\nAfter 301 training steps, loss on training batch is 13153199.00\r\n2019-04-06 11:19:38.114520: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_grad_input_ops.cc:733 : Invalid argument: Conv2DSlowBackpropInput: input and out_backprop must have the same batch size  input batch: 32  outbackprop batch: 24 batch_dim: 0  \r\nThe training can run some steps which is so puzzled!!!  \r\nThanks a lot!!  \r\nBecause I can't resolve the conv2d_transpose problem, I have to use the tf.image to achieve upsampling, but I know it's not a good methods.  \r\nThank you !!  ", "@muddham ", "To clarify: the code snippet should be the minimal code required to replicate the reported issue.", "This is my ops.py  \r\nThese are some fuctions will be used in my model  \r\nInclude deconv2d and conv2d and Batch_normalization and Linear function  \r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef variable_summaries(var, name):\r\n\r\n    # \u5c06\u751f\u6210\u76d1\u63a7\u4fe1\u606f\u7684\u64cd\u4f5c\u653e\u5728\u540c\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u4e0b\r\n    with tf.name_scope(\"summaries\"):\r\n        # \u901a\u8fc7tf.summary.histogram\u8bb0\u5f55\u5f20\u91cf\u4e2d\u5143\u7d20\u7684\u53d6\u503c\u5206\u5e03\r\n        tf.summary.histogram(name, var)\r\n        # \u8ba1\u7b97\u53d8\u91cf\u7684\u5e73\u5747\u503c\r\n        mean = tf.reduce_mean(var)\r\n        tf.summary.scalar('mean/' + name, mean)\r\n        # \u8ba1\u7b97\u53d8\u91cf\u7684\u6807\u51c6\u5dee\r\n        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\r\n        tf.summary.scalar('stddev/' + name, stddev)\r\n\r\n\r\ndef upsampling(x, height, width):\r\n    # \u4e34\u754c\u70b9\u63d2\u503c\u8fdb\u884c\u56fe\u7247\u7684\u653e\u5927\uff0c\u4e0a\u91c7\u6837\r\n    with tf.name_scope(\"image.resize\"):\r\n        return tf.image.resize_nearest_neighbor(x, [height, width])\r\n\r\n\r\nclass BatchNorm(object):\r\n    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\r\n        with tf.variable_scope(name):\r\n            self.epsilon = epsilon\r\n            self.momentum = momentum\r\n            self.name = name\r\n\r\n\r\n    def __call__(self, x, train=True):\r\n        return tf.contrib.layers.batch_norm(x,\r\n                                            decay=self.momentum,\r\n                                            updates_collections=None,\r\n                                            epsilon=self.epsilon,\r\n                                            scale=True,\r\n                                            is_training=train,\r\n                                            scope=self.name)\r\n\r\ndef linear(input_, output_size, scope=\"Linear\", stddev=0.1, bias_start=0.1, with_w=False):\r\n\r\n    shape = input_.get_shape().as_list()\r\n\r\n    with tf.variable_scope(scope):\r\n        try:\r\n            matrix = tf.get_variable(name=\"weights\", shape=[shape[1], output_size], dtype=tf.float32,\r\n                                     initializer=tf.truncated_normal_initializer(stddev=stddev))\r\n            variable_summaries(matrix, scope+\"weights\")\r\n        except ValueError as err:\r\n            msg = \"NOTE: Usually, this is due to an issue with the image dimensions. \" \\\r\n                  \" Did you correctly set '--crop' or '--input_height' or '--output_height'?\"\r\n            err.args = err.args + (msg,)\r\n            raise\r\n        bias = tf.get_variable(\"bias\", [output_size],\r\n                               initializer=tf.constant_initializer(bias_start))\r\n        variable_summaries(bias, scope+\"bias\")\r\n        # print(\"with_w\u4e3aFalse,\u53ea\u8fd4\u56de\u4e86\u77e9\u9635\u76f8\u4e58\u7684\u7ed3\u679c\")\r\n        return tf.matmul(input_, matrix) + bias\r\n\r\ndef conv2d(input_, output_dim,\r\n        k_h=2, k_w=2, d_h=2, d_w=2, stddev=0.1,\r\n        name=\"conv2d\"):\r\n    with tf.variable_scope(name):\r\n        w = tf.get_variable('weights', [k_h, k_w, input_.get_shape()[-1], output_dim],\r\n              initializer=tf.truncated_normal_initializer(stddev=stddev))\r\n        variable_summaries(w, name+\"weights\")\r\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\r\n\r\n        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\r\n        variable_summaries(biases, name+\"biases\")\r\n        conv = tf.nn.relu(tf.nn.bias_add(conv, biases))\r\n        print(name+\".shape\", conv.get_shape().as_list())\r\n\r\n    return conv\r\n\r\ndef deconv2d(input_, output_shape,\r\n             k_h=2, k_w=2, d_h=2, d_w=2, stddev=0.1,\r\n             name=\"deconv2d\"):\r\n    with tf.variable_scope(name):\r\n        # filter : [height, width, output_channels, in_channels]\r\n        w = tf.get_variable(name='w', shape=[k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\r\n                            initializer=tf.random_normal_initializer(stddev=stddev))\r\n        variable_summaries(w, name+\"_weights\")\r\n        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\r\n                                        strides=[1, d_h, d_w, 1])\r\n        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.1))\r\n        variable_summaries(biases, name + \"_bias\")\r\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\r\n\r\n        return deconv\r\n```\r\nAnd this is my Network.py  \r\nHere I define my network structure  \r\n```python\r\nimport tensorflow as tf\r\nimport math\r\nfrom ops_CNN import *\r\nfrom train_read_from_tfrecords import batch_size\r\nfrom tfrecords_train import IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL\r\n\r\n\r\nNUM_LABELS = IMG_HEIGHT * IMG_WIDTH * IMG_CHANNEL\r\n\r\n# \"\u5b66\u4e0d\u5230\u6bd4\u8f83\u7ec6\u5c0f\u7684\u7279\u5f81\uff0c\u662f\u4e0d\u662f\u6211\u8fd9\u4e2a\u5377\u79ef\u6838\u7684\u5c3a\u5bf8\u7ed9\u5927\u4e86\uff0c\u5c40\u90e8\u611f\u53d7\u91ce\u592a\u5927\uff0c\u5b66\u4e0d\u5230\u7ec6\u5c0f\u7684\u7279\u5f81\uff1f\uff1f\uff1f\"\r\n\r\nCONV1_SIZE = 2\r\nCONV1_DEPTH = 16\r\n\r\nCONV2_SIZE = 2\r\nCONV2_DEPTH = 32\r\n\r\nCONV3_SIZE = 2\r\nCONV3_DEPTH = 64\r\n\r\nCONV4_SIZE = 2\r\nCONV4_DEPTH = 128\r\n\r\nFC_NODE = 512\r\n\r\n\r\nf_dim = 64\r\nchannel_dim = 3\r\n\r\ndef conv_out_size_same(size, stride):\r\n    return int(math.ceil(float(size) / float(stride)))\r\n\r\n# s_h = 252, 400\r\ns_h, s_w = IMG_HEIGHT, IMG_WIDTH\r\nprint(\"s_h = \", s_h, \"s_w = \", s_w)\r\n# s_h2 = 126, s_w2 = 200\r\ns_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\r\nprint(\"s_h2 = \", s_h2, \"s_w2 = \", s_w2)\r\n# s_h4 = 63, s_h4 = 100\r\ns_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\r\nprint(\"s_h4 = \", s_h4, \"s_w4 = \", s_w4)\r\n# s_h8 = 32, s_h8 = 50\r\ns_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\r\nprint(\"s_h8 = \", s_h8, \"s_w8 = \", s_w8)\r\n# s_h16 = 16, s_h16 = 25\r\ns_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\r\nprint(\"s_h16 = \", s_h16, \"s_w16 = \", s_w16)\r\nprint(\"=\" * 20)\r\n\r\nd_bn_0 = BatchNorm(name=\"d_bn_0\")\r\nd_bn_1 = BatchNorm(name=\"d_bn_1\")\r\nd_bn_2 = BatchNorm(name=\"d_bn_2\")\r\nd_bn_3 = BatchNorm(name=\"d_bn_3\")\r\nd_bn_4 = BatchNorm(name=\"d_bn_4\")\r\n\r\nc_bn_0 = BatchNorm(name=\"c_bn_0\")\r\n\r\n\r\ndef inference(input_tensor, train, regularizer):\r\n\r\n    with tf.variable_scope('input_reshape'):\r\n\r\n        \"convert the pressure data 2-D  to  4-D Tensor\"\r\n        z = linear(input_=input_tensor, output_size=f_dim * 8 * s_h16 * s_w16)\r\n        h0 = tf.reshape(z, [-1, s_h16, s_w16, f_dim * 8])\r\n        h0 = tf.nn.relu(d_bn_0(h0))\r\n        # [None, 16, 25, 512]\r\n        print(\"h0.shape\", h0.get_shape().as_list())\r\n\r\n        # h1 = upsampling(h0, s_h8, s_w8)\r\n        h1 = deconv2d(input_=h0, output_shape=[batch_size, s_h8, s_w8, f_dim * 4], name=\"deconv1\")\r\n        h1 = d_bn_1(h1)\r\n        # [None, 32, 50, 512]\r\n        print(\"h1.shape\", h1.get_shape().as_list())\r\n        # h1 = conv2d(input_=h1, output_dim=f_dim * 4, d_h=1, d_w=1, name=\"conv_upsample1\")\r\n        # [None, 32, 50, 256]\r\n        # print(\"h1.shape\", h1.get_shape().as_list())\r\n        # h2 = upsampling(h1, s_h4, s_w4)\r\n        h2 = deconv2d(input_=h1, output_shape=[batch_size, s_h4, s_w4, f_dim * 2], name=\"deconv2\")\r\n        h2 = d_bn_2(h2)\r\n        # [None, 63, 100, 256]\r\n        print(\"h2.shape\", h2.get_shape().as_list())\r\n        # h2 = conv2d(input_=h2, output_dim=f_dim * 2, d_h=1, d_w=1, name=\"conv_upsample2\")\r\n        # # [None, 63, 100, 128]\r\n        # print(\"h2.shape\", h2.get_shape().as_list())\r\n        # h3 = upsampling(h2, s_h2, s_w2)\r\n        # [None, 126, 200, 128]\r\n        h3 = deconv2d(input_=h2, output_shape=[batch_size, s_h2, s_w2, f_dim], name=\"deconv3\")\r\n        h3 = d_bn_3(h3)\r\n        print(\"h3.shape\", h3.get_shape().as_list())\r\n        # h3 = conv2d(input_=h3, output_dim=f_dim, d_h=1, d_w=1, name=\"conv_upsample3\")\r\n        # # [None, 126, 200, 64]\r\n        # print(\"h3.shape\", h3.get_shape().as_list())\r\n        # h4 = upsampling(h3, s_h, s_w)\r\n        # [None, 252, 400, 64]\r\n        h4 = deconv2d(input_=h3, output_shape=[batch_size, s_h, s_w, channel_dim], name=\"deconv4\")\r\n        h4 = d_bn_4(h4)\r\n        # print(\"h4.shape\", h4.get_shape().as_list())\r\n        # h4 = conv2d(input_=h4, output_dim=f_dim//2, d_h=1, d_w=1, name=\"conv_upsample4\")\r\n        # # [None, 252, 400, 32]\r\n        print(\"h4.shape\", h4.get_shape().as_list())\r\n\r\n\r\n    print(\"=\" * 20)\r\n\r\n    conv1 = c_bn_0(conv2d(input_=h4, output_dim=CONV1_DEPTH, name=\"conv1\"))\r\n\r\n    with tf.variable_scope(\"pool1\"):\r\n        pool1 = tf.nn.max_pool(\r\n            conv1, ksize=[\r\n                1, 2, 2, 1], strides=[\r\n                1, 2, 2, 1], padding=\"SAME\")\r\n        # POOL1 shape [64,63, 100, 16]\r\n        print(\"POOL1 shape\", pool1.get_shape().as_list())\r\n\r\n    conv2 = conv2d(input_=pool1, output_dim=CONV2_DEPTH, name=\"conv2\")\r\n\r\n    with tf.variable_scope(\"pool2\"):\r\n        pool2 = tf.nn.max_pool(\r\n            conv2, ksize=[\r\n                1, 2, 2, 1], strides=[\r\n                1, 2, 2, 1], padding=\"SAME\")\r\n        # POOL2 shape [64, 16, 25, 32]\r\n        print(\"POOL2 shape\", pool2.get_shape().as_list())\r\n\r\n    conv3 = conv2d(input_=pool2, output_dim=CONV3_DEPTH, name=\"conv3\")\r\n\r\n    with tf.variable_scope(\"pool3\"):\r\n        pool3 = tf.nn.max_pool(\r\n            conv3, ksize=[\r\n                1, 2, 2, 1], strides=[\r\n                1, 2, 2, 1], padding=\"SAME\")\r\n        # POOL3 shape [64, 4, 7, 64]\r\n        print(\"POOL3 shape\", pool3.get_shape().as_list())\r\n\r\n    conv4 = conv2d(input_=pool3, output_dim=CONV4_DEPTH, name=\"conv4\")\r\n\r\n    with tf.variable_scope(\"fullly_reshape\"):\r\n        pool_shape = conv4.get_shape().as_list()\r\n        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\r\n        # [batch_size, 2*4*128]\r\n        reshaped = tf.reshape(conv4, [-1, nodes])\r\n        # reshaped shape [64, 1024]\r\n        print(\"reshaped shape\", reshaped.get_shape().as_list())\r\n\r\n    with tf.variable_scope(\"fully1\"):\r\n\r\n        with tf.name_scope(\"weights\"):\r\n            fc1_weights = tf.get_variable(\r\n                name=\"fc1_weights\",\r\n                shape=[\r\n                    nodes,\r\n                    FC_NODE],\r\n                initializer=tf.truncated_normal_initializer(\r\n                    stddev=0.1))\r\n            print(\"fc1_weights\", fc1_weights.get_shape())\r\n            variable_summaries(fc1_weights, \"fully1\" + \"/weights\")\r\n        if regularizer is not None:\r\n            tf.add_to_collection('losses', regularizer(fc1_weights))\r\n\r\n        with tf.name_scope(\"biases\"):\r\n            fc1_biases = tf.get_variable(\r\n                name=\"fc1_biases\",\r\n                shape=[FC_NODE],\r\n                initializer=tf.constant_initializer(0.1))\r\n            variable_summaries(fc1_biases, \"fully1\" + \"/biases\")\r\n        # [batch_size, FC_NODE]\r\n        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\r\n\r\n    with tf.variable_scope(\"fully2\"):\r\n\r\n        with tf.name_scope(\"weights\"):\r\n            fc2_weights = tf.get_variable(\r\n                name=\"fc2_weights\",\r\n                shape=[\r\n                    FC_NODE,\r\n                    NUM_LABELS],\r\n                initializer=tf.truncated_normal_initializer(\r\n                    stddev=0.1))\r\n            print(\"fc2_weights\", fc2_weights.get_shape())\r\n            variable_summaries(fc2_weights, \"fully2\" + \"/weights\")\r\n        if regularizer is not None:\r\n            tf.add_to_collection(\"losses\", regularizer(fc2_weights))\r\n\r\n        with tf.name_scope(\"biases\"):\r\n            fc2_biases = tf.get_variable(\r\n                name=\"fc2_biases\",\r\n                shape=[NUM_LABELS],\r\n                initializer=tf.constant_initializer(0.1))\r\n            variable_summaries(fc2_biases, \"fully2\" + \"/biases\")\r\n        # [batch_size, 252*400*3]\r\n        fc2 = tf.matmul(fc1, fc2_weights)+fc2_biases\r\n\r\n    with tf.variable_scope(\"output_reshape\"):\r\n        output_array = tf.reshape(fc2, [-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL])\r\n        print(\"output_shape\", output_array.get_shape().as_list())\r\n\r\n    tf.summary.histogram(\"output_array\", output_array)\r\n    \"\"\"[64, 252, 400, 3]\"\"\"\r\n    return output_array\r\n\r\n```\r\nIn order to put the pressure data and the picture data into the model  \r\nI use the TFrecords and tf.data  \r\nHere is my Input.py  \r\n```python\r\nimport tensorflow as tf\r\nfrom tfrecords_train import IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL\r\n\r\nbuffer_size = 100000\r\nbatch_size = 32\r\nnum_epochs = 500\r\nfilename = [\"train.tfrecords\"]\r\n\r\n\r\ndef parser(record):\r\n    features = tf.parse_single_example(\r\n        record,\r\n        features={\r\n            \"images_raw\": tf.FixedLenFeature([], tf.string),\r\n            \"features\": tf.FixedLenFeature([10], tf.float32)\r\n        }\r\n    )\r\n\r\n    # \"\u8fd9\u91cc\u89e3\u6790\u7684\u65f6\u5019\u56fe\u50cf\u7684\u6570\u636e\u7c7b\u578b\u4e00\u5b9a\u8981\u662fuint8!!!!!\"\r\n    decoded_image = tf.decode_raw(features['images_raw'], tf.uint8)\r\n    decoded_image = tf.reshape(decoded_image, [IMG_HEIGHT,IMG_WIDTH,IMG_CHANNEL])\r\n    # decoded_images.shape [252, 400, 3]\r\n    # \"\u4f46\u662f\u53ef\u4ee5\u5728\u8fd9\u91cc\u8fdb\u884c\u6570\u636e\u7c7b\u578b\u7684\u4fee\u6539\uff01\uff01\uff01\"\r\n    decoded_image = tf.cast(decoded_image, tf.float32)\r\n    print(\"decoded_images.shape\", decoded_image.get_shape().as_list())\r\n\r\n    feature = features['features']\r\n    # feature.shape [10]\r\n    print(\"feature.shape\", feature.get_shape().as_list())\r\n    return decoded_image, feature\r\n\r\n\r\nwith tf.name_scope('input_data') as scope:\r\n    Dataset = tf.data.TFRecordDataset(filename)\r\n    print(\"=\" * 10)\r\n    print(\"batch_size=\", batch_size)\r\n    Dataset = Dataset.map(parser).shuffle(buffer_size=buffer_size).batch(batch_size=batch_size).repeat(num_epochs)\r\n    train_iterator = Dataset.make_initializable_iterator()\r\n    train_next_element = train_iterator.get_next()\r\n```\r\n\r\nThanks a lot!!", "@robieta @muddham @tensorflowbutler @jvishnuvardhan ", "After many attempts, finally I find where the mistake is!!!   \r\nWell, we need to use tf.reshape() after the conv2d_transpose, but it's not enough, we also need to use tf.reshape() after the conv2d.  \r\nThe reason why we need tf.shape() after the conv2d_transpose and conv2d is maybe the shape of Tensor will be lost in the training.  \r\nThen, the problem is settled!!!  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27409\">No</a>\n", "@liyunfei1994 Thanks for posting solution also. ", "@liyunfei1994 Could you share your code before modify and after,I want to see the differencies,Thank you!"]}, {"number": 27408, "title": "Added Activation,padding and stride scenarios.", "body": "This is one of the TODO in the file.", "comments": ["@shahzadlone , thanks for the review, i have updated the code as per your suggestion, kindly check and approve.\r\n\r\nRegards\r\nAmit", "@suharshs, can you please review the PR.\r\n\r\nRegards \r\nAmit ", "@rthadur this PR is approved by @shahzadlone , can you please change the status to Approved.\r\n\r\nRegards\r\nAmit", "> @rthadur this PR is approved by @shahzadlone , can you please change the status to Approved.\r\n> \r\n> Regards\r\n> Amit\r\n\r\n@suharshs gentle ping to approve this PR, @amitsrivastava78 thanks for your contribution, waiting for a googler to approve this changes.", "Closing this PR since similar is already approved\r\n\r\nRegards\r\nAmit"]}, {"number": 27407, "title": "Added different activation types, bias, padding", "body": "Added sceanrios like different activation types, bias, padding types\r\nstride values.", "comments": ["@jdduke , thanks for the comments, i have updated the code as per your suggestion, kindly check and approve.\r\n\r\nRegards\r\nAmit"]}, {"number": 27406, "title": "Added Variable shape and lookup errors TCs.", "body": "This is one of the TODO in the file.", "comments": ["@miaout17 Could you PTAL and approve.", "@miaout17 , thanks for approving the PR, could you please help to get this merged.\r\n\r\n\r\nRegards\r\nAmit", "Yes this should be merged soon", "Can one of the admins verify this patch?", "@amitsrivastava78 can you please fix build failures ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27405, "title": "Added Support for different data types.", "body": "Added different data types along with test cases.", "comments": ["@miaout17 Could you PTAL and approve.", "@miaout17 could you please review and approve this ?", "Can one of the admins verify this patch?", "Closing as a duplicate of https://github.com/tensorflow/tensorflow/pull/26934"]}, {"number": 27404, "title": "Removed the dynamic allocations for optmization.", "body": "This is one of the TODO in the file.", "comments": ["We are in the process of optimizing this op and releasing a newer version. We'll go ahead and benchmark this, thanks for the proposal."]}, {"number": 27403, "title": "Reorganised the code and shifted common code.", "body": "This is one of the TODO item.", "comments": ["We prefer not to include std::initializer_list<values> in our kernel_util binary.", "We'll go ahead and remove the TODO, thanks anyway for the proposal."]}, {"number": 27402, "title": "Build TFLite Model Benchmark Tool with GPU delegate?", "body": "**System information**\r\n- TensorFlow version (you are using):nightly-build\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm trying to build TFLite Model Benchmark Tool with GPU delegate on Android and iOS devices.\r\nBut unfortunately, It seems to be too difficult for me especially on iOS.\r\nWould you like to provide bazel script to build TFLite Model Benchmark Tool with GPU delegate?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using the TFLite.\r\n\r\n**Any Other info.**\r\n", "comments": ["for Android, it's quite easy. I had a quick hack couple days ago. I don't think it would be much harder on iOS. See https://github.com/freedomtan/tensorflow/commit/04996cac4a2400523c6e85a19ebcb590ea23ff5a. And if you just want to check the performance of GPU Delegate, I had quick-and-dirty GPU Delegate enabled apps for [android](https://github.com/freedomtan/glDelegateBench/) and [iOS](https://github.com/freedomtan/glDelegateBenchmark/)", "@freedomtan \r\nI appreciate your reply.\r\nYour grateful patches including label_image_gl_delegate are quite helpful for me.\r\nBy the way, do you have any plan for submitting pull requests?", "Patches for label_image, yes, I'll do it later. For benchmark_model, need to find a better way, current one is too hacky :-)", "@stakemura I sent PR for label_image https://github.com/tensorflow/tensorflow/pull/27464.", "Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!"]}]