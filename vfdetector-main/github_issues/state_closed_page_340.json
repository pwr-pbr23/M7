[{"number": 43932, "title": "Use TensorShape.assert_is_compatible_with instead of merge_with", "body": "`TensorShape.merge_with()` is currently used in many places to assert for compatible shapes. This PR replaces it with `TensorShape.assert_is_compatible_with()` for improved readability where possible.", "comments": []}, {"number": 43931, "title": "Slow inference with libopencm3 when compared to Mbed ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10\r\n- TensorFlow installed from: source\r\n- Tensorflow version: 2.3.0, commit b36436b\r\n- Target platform: Building for Nucleo-F767ZI dev board, comparing performance between Mbed and libopencm3\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello,\r\n\r\nI have been having this problem for a while now, but only recently made enough progress so I can ask for help.\r\n\r\nI'm trying to make TensorFlow Lite for Microcontrollers (TFMicro) library to work with libopencm3, which is a open-source firmware library for various ARM Cortex-M microcontrollers. \r\nWhen this is done it should be quick to run TensorFlow on any micro that libopencm3 supports.\r\nRight now I'm extensively testing this on Nucleo-F767ZI dev board which has STM32F767ZI micro, with 2MB of flash and 1M of SRAM, 216 MHz.\r\n\r\nTFMicro port is working as it should, I tested it with several different models, everthing compiles and I also get same outputs as compared to TFLite python interpreter.\r\n\r\nHowever, on device inference is **very slow**. It takes about 1465 ms for one inference, with -O3 flag\r\nI managed to get the same setup working with a generated Mbed project. With -O3 flag I get inference time of 486 ms, almost a second faster.\r\n\r\nMbed has a option to export a makefile of the project, that you usually need to compile with mbed command line tool.\r\nI exported the makefile and with some small changes the project compiled and inference time was again 486 ms.\r\n\r\nI hoped that with a exported mbed makefile and my makefile I could compare the differences and come down to the core fo the problem, but so far I have not managed to so.\r\n\r\nSo far I can tell that the problem is not in the tensorflow code or in the model, both setups use the same things. I also copied exported mbed makefile into my project and started changing out pieces. I managed get to a setup where I am using all compile flags from mbed makefile and a libopencm archive file, linker file and startup file, but inference is stilll around 1465 ms.\r\n\r\nBoth setups set the clock frequency of a micro to 216MHz, I am timing inference in exactly the same way, with a DWT counter which increments for every clock cycle, to get to milliseconds I do a bit of calculation that is the same in both cases.\r\n\r\nBoth setups also use cmsis-nn kernel implementations, i had to make manually sure of that.\r\n\r\nFor now I can not tell what exactly is the problem, but the things that are different are:\r\n* linker script file\r\n* linker flags\r\n* startup routine\r\n\r\nI can see that libopencm needs linker with -nostartfiles, -specs=nano.specs, -specs=nosys.specs, while mbed makefile just passes many -Wl,--wrap flags. Judging by the look of it it uses crt0.s for some startup work, but this is way over my head.\r\n\r\nWhat can make microcontroller run slower, even though the clock is the same in both examples? Can a incorrect linker script slow down a micro, or are there some settings that make loading a model from flash slow? \r\n\r\nAny help or a suggested path how to continue wiht this problem would be extremly appreciated.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nBoth setups are available on my github.\r\n\r\n**Project with fast mbed example:**\r\nhttps://github.com/MarkoSagadin/tensorflow_mbed_test \r\nTo load the dependencies this code requires, run:\r\n`mbed config root .`\r\n`mbed deploy`\r\n\r\nTo compile this for any platform supported by mbed:\r\n`mbed compile -m auto -t GCC_ARM -f --profile my_profile.json -f`\r\n\r\nTo compile this for Nucleo_F767ZI dev board you can run makefile:\r\n`make flash -j4`\r\n\r\nor with mbed:\r\n`mbed compile -m NUCLEO_F767ZI -t GCC_ARM -f --profile my_profile.json -f`\r\n\r\nInspect serial output with `minicom -b 9600`\r\n\r\nLocation of main file:\r\nhttps://github.com/MarkoSagadin/tensorflow_mbed_test/tree/master/tensorflow/lite/micro/examples/hello_world\r\n\r\nLinker file that is used can be found here:\r\nhttps://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/STM32F767xI.ld\r\n\r\nStartup file that is used can be found here:\r\nhttps://github.com/ARMmbed/mbed-os/blob/master/targets/TARGET_STM/TARGET_STM32F7/TARGET_STM32F767xI/TOOLCHAIN_GCC_ARM/startup_stm32f767xx.S\r\n\r\n**Project with slow libopencm3 example:**\r\nhttps://github.com/MarkoSagadin/MicroML/tree/mbed_makefile\r\nThe setup for my project takes more time and more commands to setup, as im using tensorflow and libopencm as submodules. Please note that im using branch mbed_makefile to showcase my problem. To get everthing setup you should just copy commands below. \r\n```\r\ngit clone --recurse-submodules https://github.com/SkobecSlo/MicroML.git\r\ngit checkout mbed_makefile\r\ncd tensorflow\r\nsudo make -f tensorflow/lite/micro/tools/make/Makefile hello_world\r\ncd ..\r\nmake -C libopencm3\r\nmbed config root .\r\nmbed deploy\r\n```\r\n\r\nTo compile and flash to Nucelo board:\r\n`make flash -j4`\r\n\r\nInspect serial output with `minicom -b 115200`\r\n\r\nLocation of main file and linker script that is used:\r\nhttps://github.com/MarkoSagadin/MicroML/tree/mbed_makefile/projects/mbed_test\r\n", "comments": ["I managed to find the issue: mbed-os by default, before program enter main configures few important settings:\r\n* It enables I-chache\r\n* It enabled D-chache\r\n* It enables Flash ART accelator \r\n* It enables Flash prefetch\r\n\r\nAlthough libopencm3 does not support these functions,or even registers, it was trivial to copy and implement them in my project.\r\n\r\nWith this settings combined my inference dropped down to 357 ms, which makes me extremly happy. I hope that this issue might help someone in the future."]}, {"number": 43930, "title": "Add additional example to timeseries_dataset_from_array", "body": "Motivation: https://github.com/keras-team/keras/issues/11293\r\n\r\nAdd additional example for many-to-many architectures (left: many-to-one, right: many to many):\r\n\r\n![image](https://user-images.githubusercontent.com/12762439/95662529-e94a4e80-0b37-11eb-8d08-700f75bb4536.png)\r\n\r\nThis dataset type is especially interesting in engineering domain, where one wants to mimic [state space system](https://en.wikipedia.org/wiki/State-space_representation).\r\n\r\nAdditionally prettify the documentation, see Example 2 (no bold text) on https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array \r\n\r\n", "comments": ["@kopytjuk  Can you please resolve conflicts? Thanks!"]}, {"number": 43928, "title": "Using Orthogonal initializer in dense layer with backend set to fp16 gets stuck", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 64 bit\r\n- TensorFlow installed from binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: 2060 super, 8GB\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUsing Orthogonal kernel initializer in dense layer with backend set to fp16 execution gets stuck during initialization. CPU runs at 100% and nothing happens. Changing initializer to Zeros fixes this. The same Orthogonal initializer in convolutional layer works fine.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass SNetwork(tf.keras.Model):\r\n    def __init__(self):\r\n        super(SNetwork, self).__init__()\r\n\r\n        self.conv1 = tf.keras.layers.Conv2D(filters=32,\r\n                                            kernel_size=(8, 8),\r\n                                            strides=(4, 4),\r\n                                            padding=\"same\",\r\n                                            input_shape=(96, 96, 3),\r\n                                            kernel_initializer=tf.keras.initializers.Orthogonal(np.sqrt(2)),\r\n                                            bias_initializer=tf.keras.initializers.Zeros(),\r\n                                            activation=\"linear\")\r\n\r\n        self.flatten = tf.keras.layers.Flatten()\r\n\r\n        self.a_dense = tf.keras.layers.Dense(512,\r\n                                             kernel_initializer=tf.keras.initializers.Orthogonal(np.sqrt(2)),\r\n                                             bias_initializer=tf.keras.initializers.Zeros(),\r\n                                             activation=\"relu\")\r\n\r\n        self.a_out = tf.keras.layers.Dense(9,\r\n                                           bias_initializer=tf.keras.initializers.Zeros(),\r\n                                           activation=\"softmax\")\r\n\r\n    def call(self, inputs):\r\n        x = self.conv1(inputs)\r\n        x = tf.keras.layers.LeakyReLU()(x)\r\n        x = self.flatten(x)\r\n        x = self.a_dense(x)\r\n        x = self.a_out(x)\r\n        return x\r\n\r\ntf.keras.backend.set_floatx(\"float16\")\r\ntmp = np.zeros((1, 96, 96, 3), dtype=\"float16\")\r\nnet = SNetwork()\r\nprint(net(tmp))\r\n```", "comments": ["Was able to reproduce the issue. With [TF v2.3](https://colab.research.google.com/gist/amahendrakar/8a6989ea636f62bdd78f1589db82cbde/43928.ipynb), code runs indefinitely. \r\n\r\nWhereas with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/eeb7fa71b0c69bab167eb1dc1e310baf/43928-tf-nightly.ipynb), it throws an error stating `NotFoundError: Could not find device for node: {{node Qr}} = Qr[T=DT_HALF, full_matrices=false]`. Please find the attached gist. Thanks!", "Facing some error in both [Tf Nightly](https://colab.research.google.com/gist/sachinprasadhs/0f3578b6267419585defd81441e304ed/43928.ipynb) and [Tensorflow 2.5](https://colab.research.google.com/gist/sachinprasadhs/a8cb8bc968de2127bca02d8c834cfceb/43928.ipynb) version while trying to reproduce your issue, please find the attached gist. Thanks! ", "> Facing some error in both [Tf Nightly](https://colab.research.google.com/gist/sachinprasadhs/0f3578b6267419585defd81441e304ed/43928.ipynb) and [Tensorflow 2.5](https://colab.research.google.com/gist/sachinprasadhs/a8cb8bc968de2127bca02d8c834cfceb/43928.ipynb) version while trying to reproduce your issue, please find the attached gist. Thanks!\r\n\r\nI think that is the expected behavior. You get the exception saying that you can\u2019t use orthogonal initializer with half precision computation. Because there is no HALF kernel for QR decomposition. The problem with TF version 2.3 was that there was no exception. It would just get stuck in an infinite loop.", "If it is the expected behavior, can we close this issue then. Thanks!", "I don't know who is responsible for writing kernels. It would be nice to be able to use orthogonal with half precision. But I'm guessing this is not a bug any more. So yes. You can close this issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43928\">No</a>\n"]}, {"number": 43927, "title": "Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04 -Training performance low - GPU Util% keep changing - Process Id section empty", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): tensorflow-gpu ==1.15.0\r\nPython version: python 3.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 10.2\r\nGPU model and memory: 4 GPU - Model : V100 Memory: 16160 MB\r\n\r\ncollected some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5359214/tf_env.txt)\r\n\r\nEnvironment:\r\nDocker Image : cuda:10.0-cudnn7-devel-ubuntu16.04\r\nTotal GPUs : 4 nos Tesla V100 - GPU Memory 16.2 GB\r\nCUDA: 10.2\r\nTensorflow-gpu - 1.15\r\nkeras: 2.1.3\r\n\r\n**Describe the current behavior**\r\n\r\nnvidia-smi shows as below - that is all GPUs Utilisation is above 90% for few seconds \r\n\r\n![gpu all above 90 %](https://user-images.githubusercontent.com/2168986/95658306-00d50780-0b37-11eb-833b-a45d9214e6db.png)\r\n\r\nThen nvidia-smi window shows as follows, that is all GPUs Utilisation is 0% for few seconds\r\n![Gpu 0%](https://user-images.githubusercontent.com/2168986/95658335-472a6680-0b37-11eb-80ee-a817391f3d58.png)\r\n\r\n\r\nThen nvidia-smi window shows as follows, that is  Utilisation% is random in all GPUs Utilisation for few seconds\r\n![gpu random %](https://user-images.githubusercontent.com/2168986/95658620-6c1fd900-0b39-11eb-9d21-c6a5e43b9afd.png)\r\n\r\nI have noticed Training performance is low and taking longer duration in Docker container mentioned here. Same code is working fine in Tesla K80 2 GPUs with CUDA 10.0 on a Dedicated server as shown below.\r\n\r\n![Screen Shot 2020-10-10 at 9 34 06 PM](https://user-images.githubusercontent.com/2168986/95659895-d177c800-0b41-11eb-8c72-91dc83ef8d8a.png)\r\n\r\n\r\nBut in Docker container, Why GPU utilization% is keep changing unusually and why process list section is empty? Why CUDA version is shown a 10.2 in the cuda:10.0-cudnn7-devel-ubuntu16.04 Image refer first three nvidia-smi schreen shots? I have not installed CUDA 10.2 toolkit and CuDNN libraries in the Image. How can i solve this issue?\r\n\r\nps aux command shows processids but nvidia-smi doesn\u2019t show\r\n", "comments": ["@suresh-s \r\nPlease note we do not have support for 1.x, could you please upgrade to 2.x and let us know in case you face any issues.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43926, "title": "To compile the code need portpicker. What for? is it safe?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): no\r\n- TensorFlow version: master branch\r\n- Python version: 3.8.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): bazelisk\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11/8\r\n- GPU model and memory: 1070 ti 8 Gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nTo compile the code,  need portpicker. What for? is it safe?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Expert73 \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "We are using it internally for building.", "thanks for answer!!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43926\">No</a>\n"]}, {"number": 43925, "title": "Avoiding data copy when using tf.py_func in Python", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): b'v1.13.0-rc2-5-g6612da8' 1.13.1\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla V100 32GB\r\n\r\n**Describe the current behavior**\r\nThere may exists data copy from numpy array to tensorflow tensor when using ``tf.py_func``, and it costs extra time.\r\n\r\n**Describe the expected behavior**\r\nAvoid the data copy.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\narr = np.random.rand(100000000)\r\n\r\n\r\ndef get_arr():\r\n    return arr\r\n\r\n\r\nget = tf.py_func(get_arr, [], arr.dtype)\r\n\r\nsess = tf.Session()\r\n\r\nwhile 1:\r\n    t = time.time()\r\n    sess.run(get)\r\n    print(time.time() - t)  # it outputs ~0.5s in my case\r\n```\r\n\r\nAs it is mentioned [here](https://github.com/tensorflow/docs/blob/r1.13/site/en/api_docs/python/tf/py_func.md#args), \r\n\r\n> Important Note: Input and output numpy ndarrays of func are not guaranteed to be copies. In some cases their underlying memory will be shared with the corresponding TensorFlow tensors.\r\n\r\nso in what cases do the numpy arrays share memory with tensors?\r\n\r\nOr how to avoid the data copy?\r\n\r\nThanks for help!!\r\n", "comments": ["@zhm9484,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43924, "title": "Build from source wheel can not be used in another cpu", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:1.14.1\r\n- Python version:3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n- CUDA/cuDNN version:10.2\r\n- GPU model and memory:2080ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nI build the wheel on one CPU(Xeon) and can not be used in another CPU(i7-8700). I've tried with AVX enabled and disabled, but all lead to the same problem. \r\nCPU used to build:\r\n```\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              48\r\nOn-line CPU(s) list: 0-47\r\nThread(s) per core:  2\r\nCore(s) per socket:  12\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) Gold 6226 CPU @ 2.70GHz\r\nStepping:            7\r\nCPU MHz:             1199.554\r\nBogoMIPS:            5400.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            19712K\r\nNUMA node0 CPU(s):   0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46\r\nNUMA node1 CPU(s):   1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts pku ospke avx512_vnni md_clear flush_l1d arch_capabilities\r\n```\r\nCPU which caused the problem:\r\n```\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              12\r\nOn-line CPU(s) list: 0-11\r\nThread(s) per core:  2\r\nCore(s) per socket:  6\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               158\r\nModel name:          Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\r\nStepping:            10\r\nCPU MHz:             899.941\r\nCPU max MHz:         4600.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            6399.96\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            256K\r\nL3 cache:            12288K\r\nNUMA node0 CPU(s):   0-11\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d\r\n```\r\nwhen `import tensorflow` caused `Illegal instruction (core dumped)`  \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1.  Build tensorflow as instructed by https://www.tensorflow.org/install/source, with `--config=opt --config=cuda`  \r\n2. pip install built wheel   \r\n3. `python -q -X faulthandler -c \"import tensorflow as tf\"`\r\n\r\n**Any other info / logs**\r\n```\r\nFatal Python error: Illegal instruction\r\nCurrent thread 0x00007f598de0f740 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 922 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 571 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 658 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 684 in _load\r\n  File \"/opt/conda/lib/python3.6/imp.py\", line 343 in load_dynamic\r\n  File \"/opt/conda/lib/python3.6/imp.py\", line 243 in load_module\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24 in swig_import_helper\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 678 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 665 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 955 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 971 in _find_and_load\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 678 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 665 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 955 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 971 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1023 in _handle_fromlist\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 678 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 665 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 955 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 971 in _find_and_load\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/__init__.py\", line 28 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 678 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 665 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 955 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 971 in _find_and_load\r\n  File \"<string>\", line 1 in <module>\r\nIllegal instruction (core dumped)\r\n```\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@ArtificialNotImbecile \r\nCan you please refer to these resolved issues with same error and let us know: #40978 #39532 [link](https://github.com/tensorflow/tensorflow/issues/580#issuecomment-211169699)\r\n\r\nPlease note 1.x is not officially supported, please try fresh build on 2.x if the above resolved issues do not help and let us know if you face any issues with 2.x.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Maybe the issue is caused by using the wrong [Compute Capability](https://developer.nvidia.com/cuda-gpus#compute) config. The default is `3.5, 7.0` but 2080Ti should use `7.5`. But the Xeon machine with 2080Ti can use `Compute=3.5, 7.0` wheel, which is weird. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43924\">No</a>\n"]}, {"number": 43923, "title": "Fixing missing flatbuffer headers in lite", "body": "fixing cannot find \"flatbuffers/flatbuffers.h\" head file. Missing headers ", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/43923\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43923) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 43922, "title": "R2.1", "body": "Fixing cannot find \"flatbuffers/flatbuffers.h\" head file", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43922) for more info**.\n\n<!-- need_sender_cla -->", "> \r\n> \r\n> Fixing cannot find \"flatbuffers/flatbuffers.h\" head file\r\n\r\n", "We never merge release branches back into master"]}, {"number": 43921, "title": "Compare  Update Pull ", "body": "Update pull request @nairb774 @jmhodges  help build this repository better", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43921) for more info**.\n\n<!-- need_sender_cla -->", "@digitalvirtuallife Can you please sign CLA. Thanks!", "This looks like a spam PR."]}, {"number": 43920, "title": "[INTEL MKL] Added  bf16 support for Sparse_Xent_op.", "body": "Added bf16 support for Sparse_Xent_op.", "comments": ["@penpornk Hello Penporn, Can you please provide review on this PR? Please let me know, if there is any concern.", "@vishakha-nervana  Can you please resolve conflicts? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43920) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43920) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43920) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43920) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43920) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 43919, "title": "Docker cuda:10.0-cudnn7-devel-ubuntu16.04 - 3 out of 4 GPUs not utilized", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Docker Image : cuda:10.0-cudnn7-devel-ubuntu16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): tensorflow-gpu ==1.15.0\r\n- Python version: python 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 4 GPU - Model : V100 Memory: 16160 MB\r\n\r\nEnvironment:\r\nDocker Image : cuda:10.0-cudnn7-devel-ubuntu16.04\r\nTotal GPUs : 4 nos Tesla V100 - GPU Memory 16.2 GB\r\nCUDA: 10.2\r\nTensorflow-gpu - 1.15\r\nkeras: 2.2.5\r\n\r\n**Describe the current behavior**\r\n\r\nI have started the training. Logs shows all 4 GPU devices are prepared and allocated. but nvidia-smi command shows as below\r\n\r\n![3 GPUs is not Utilized at all](https://user-images.githubusercontent.com/2168986/95613566-c90c8800-0a82-11eb-9b45-348bb7a144a3.png)\r\n\r\nPlease refer the screenshot, One GPU is 63% utilization and other three GPUs are 0 % Utilization. But There is no process id list.\r\n\r\nWhat is the issue here? Why three GPUs are not utilized and empty process list?\r\n\r\n", "comments": ["@suresh-s \r\n\r\nPlease, refer tested build configurations from [here](https://www.tensorflow.org/install/source#gpu)..Can you please upgrade to TF 2.3 and check for GPU devices. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43918, "title": "Allow custom image size for NASNet models", "body": "Running `model = NASNetMobile(input_shape=(400, 400, 3), include_top=False, weights=\"imagenet\")` results in a ValueError: When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (224, 224, 3). This PR aims to fix the above error.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43918) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thanks for the PR. I suspect there was a reason why `require_flatten` was set to True. Please add a unit test to test this change.", "Unfortunately we had to revert this change because it was breaking internal tests. It's looking like there was indeed a reason why `require_flatten` was set to True. I still think this would be a useful change, but making it would require further testing to make sure we don't break backwards compatibility."]}, {"number": 43917, "title": "tensorflow/core/grappler/optimizers/meta_optimizer.cc:581", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nMore details on bug :  function_optimizer failed: Invalid argument: Input 0 of node zeros_like_270 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant\r\n\r\n**System information**\r\n- Have I written custom code : yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10  (with 1080Ti)\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.3.0 \r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX 1660 Ti computeCapability: 7.5\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\ntf version : v2.3.0-rc2-23-gb36436b087 2.3.0\r\n\r\n**Describe the current behavior**\r\n\r\nError messages \r\n\r\n2020-10-09 16:42:47.403851: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_270 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n2020-10-09 16:42:47.524417: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_240 was passed float from StatefulPartitionedCall/sequential/lstm_3/PartitionedCall:6 incompatible with expected variant.\r\n2020-10-09 16:42:52.806817: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_190 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n2020-10-09 16:42:52.899388: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_160 was passed float from StatefulPartitionedCall/sequential/lstm_3/PartitionedCall:6 incompatible with expected variant.\r\n2020-10-09 16:42:54.497530: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_150 was passed float from StatefulPartitionedCall/sequential/lstm/PartitionedCall:6 incompatible with expected variant.\r\n2020-10-09 16:42:54.584999: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] function_optimizer failed: Invalid argument: Input 0 of node zeros_like_120 was passed float from StatefulPartitionedCall/sequential/lstm_3/PartitionedCall:6 incompatible with expected variant.\r\n\r\n**Describe the expected behavior**\r\nI don't see why the errors are happening\r\n\r\n\r\n\r\n**Other info / logs** \r\nI am using ipopt for optimization alongwith tensorflow (and tf.function and tf.gradTape). \r\n", "comments": ["Can you share a very, very minimal standalone code example that we could copy, past and run or a colab to reproduce your error?", "@bhack  I cannot share any part of the script due to IP protocols. Can you suggest any possible solution that I can check for ?", "Can you try with tf-nightly?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43916, "title": "Keras Functional API does not support tf.custom_gradient() with tf.numpy_function()", "body": "**System information**\r\n\r\n- I have written custom code (see below)\r\n- Red Hat Enterprise Linux Server release 7.7 (Maipo)\r\n- TensorFlow installed from: binary (pip)\r\n- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0 AND v1.12.1-42951-ge8766a151d 2.4.0-dev20201005\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nFunctions with custom gradients (i.e. decorated with `tf.custom_gradient()`) using numpy functions internally (with `tf.numpy_function()`) cannot be used directly with the Keras Functional API (plain crash when calling these for example with `tf.keras.layers.Input(shape=(2,))` Tensor).\r\n\r\n**Describe the expected behavior**\r\n\r\nCalling these functions on Tensors originating from a `tf.keras.layers.Input()` Tensor should return another symbolic Tensor usable with the Keras Functional API.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\n>>> import tensorflow as tf, numpy as np\r\n>>> (tf.__version__, np.__version__)\r\n('2.3.0', '1.18.5')\r\n\r\n>>> x = tf.keras.layers.Input(shape=(2,))\r\n>>> x\r\n<tf.Tensor 'input_1:0' shape=(None, 2) dtype=float32>\r\n\r\n>>> @tf.custom_gradient\r\n... def my_sin_with_gradient_using_numpy_function(x):\r\n...     def _sin_and_gradient_numpy(x):\r\n...         return (np.sin(x), np.cos(x))\r\n...     (y, dy_dx) = tf.numpy_function(_sin_and_gradient_numpy, [x], [tf.float32, tf.float32])\r\n...     def grad_fn(grad_y):\r\n...         return grad_y * dy_dx\r\n...     return (y, grad_fn)\r\n\r\n>>> my_sin_with_gradient_using_numpy_function(x)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 264, in __call__\r\n    return self._d(self._f, a, k)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 218, in decorated\r\n    return _eager_mode_decorator(wrapped, args, kwargs)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 412, in _eager_mode_decorator\r\n    result, grad_fn = f(*args, **kwargs)\r\n  File \"<stdin>\", line 5, in my_sin_with_gradient_using_numpy_function\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 632, in numpy_function\r\n    return py_func_common(func, inp, Tout, stateful=True, name=name)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 519, in py_func_common\r\n    result = func(*[np.array(x) for x in inp])\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 519, in <listcomp>\r\n    result = func(*[np.array(x) for x in inp])\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 848, in __array__\r\n    \" a NumPy call, which is not supported\".format(self.name))\r\nNotImplementedError: Cannot convert a symbolic Tensor (input_1:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis ticket is related to #43542 where I mention the fact that `tf.numpy_function()` does not support symbolic Tensors as of TF 2.3. It looks like this is going to be fixed in TF 2.4 (tested with tf_nightly), but the crash here still happens in a somewhat different fashion.\r\n\r\nBelow are some observations/tests with TF 2.3 and tf_nightly.\r\n\r\n```python\r\n>>> tf.__version__\r\n'2.3.0' # TF 2.3\r\n'2.4.0-dev20201005' # tf_nightly\r\n\r\n\r\n# Let us define the following implementations of the sin() function:\r\n\r\n# sin() with custom gradient, using tensorflow ops:\r\n>>> @tf.custom_gradient\r\n... def my_sin_with_gradient_using_tensorflow_ops(x):\r\n...     def grad_fn(grad_y):\r\n...         return grad_y * tf.cos(x)\r\n...     return (tf.sin(x), grad_fn)\r\n\r\n# sin() without custom gradient, using numpy function:\r\n>>> def my_sin_without_gradient_using_numpy_function(x):\r\n...     return tf.numpy_function(np.sin, [x], np.float32)\r\n\r\n# sin() with custom gradient, using numpy function:\r\n# This is the function of interest here.\r\n>>> @tf.custom_gradient\r\n... def my_sin_with_gradient_using_numpy_function(x):\r\n...     def _sin_and_gradient_numpy(x):\r\n...         return (np.sin(x), np.cos(x))\r\n...     (y, dy_dx) = tf.numpy_function(_sin_and_gradient_numpy, [x], [tf.float32, tf.float32])\r\n...     def grad_fn(grad_y):\r\n...         return grad_y * dy_dx\r\n...     return (y, grad_fn)\r\n\r\n\r\n# All these variants return the expected values and gradients with eager Tensors:\r\n\r\n>>> x = tf.constant([0.5])\r\n>>> x\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)> # TF 2.3 and tf_nightly\r\n\r\n>>> my_sin_with_gradient_using_tensorflow_ops(x)\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47942555], dtype=float32)> # TF 2.3 and tf_nightly\r\n>>> tf.test.compute_gradient(my_sin_with_gradient_using_tensorflow_ops, [x])\r\n((array([[0.87758255]], dtype=float32),), (array([[0.877574]], dtype=float32),)) # TF 2.3 and tf_nightly\r\n\r\n>>> my_sin_without_gradient_using_numpy_function(x)\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47942555], dtype=float32)> # TF 2.3 and tf_nightly\r\n>>> tf.test.compute_gradient(my_sin_without_gradient_using_numpy_function, [x])\r\n((array([[0.]], dtype=float32),), (array([[0.877574]], dtype=float32),)) # TF 2.3 and tf_nightly\r\n\r\n>>> my_sin_with_gradient_using_numpy_function(x)\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47942555], dtype=float32)> # TF 2.3 and tf_nightly\r\n>>> tf.test.compute_gradient(my_sin_with_gradient_using_numpy_function, [x])\r\n((array([[0.87758255]], dtype=float32),), (array([[0.877574]], dtype=float32),)) # TF 2.3 and tf_nightly\r\n\r\n\r\n# However, both fail with Keras Functional API:\r\n\r\n\r\n>>> x = tf.keras.layers.Input(shape=(2,))\r\n>>> x\r\n<tf.Tensor 'input_1:0' shape=(None, 2) dtype=float32> # TF 2.3\r\n<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'input_1')> # tf_nightly\r\n\r\n# tf.custom_gradient() no longer works with Functional API with new TF version (new bug).\r\n>>> my_sin_with_gradient_using_tensorflow_ops(x)\r\n<tf.Tensor 'Identity_3:0' shape=(None, 2) dtype=float32> # TF 2.3 (ok)\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. # tf_nightly\r\n\r\n# tf.numpy_function() works with Functional API with new TF version (fixed bug, see issue #43542).\r\n>>> my_sin_without_gradient_using_numpy_function(x)\r\nNotImplementedError: Cannot convert a symbolic Tensor (input_1:0) to a numpy array # TF 2.3\r\n<KerasTensor: shape=<unknown> dtype=float32 (created by layer 'tf.numpy_function_8')> # tf_nightly\r\n\r\n# tf.custom_gradient() used with tf.numpy_function() does not work with Functional API.\r\n>>> my_sin_with_gradient_using_numpy_function(x)\r\nNotImplementedError: Cannot convert a symbolic Tensor (input_1:0) to a numpy array # TF 2.3\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array # tf_nightly\r\n\r\n# As with issue #43542, wrapping this function into tf.keras.layers.Lambda() is a workaround but the root issue remains.\r\n>>> tf.keras.layers.Lambda(my_sin_with_gradient_using_numpy_function)(x)\r\n<tf.Tensor 'lambda/IdentityN:0' shape=<unknown> dtype=float32> # TF 2.3\r\n<KerasTensor: shape=<unknown> dtype=float32 (created by layer 'lambda_1')> # tf_nightly\r\n```\r\n\r\n\r\n\r\n", "comments": ["@loic-ehrhardt \r\nI ran the code shared above its difficult to replicate due to indentation issues can you please share the same in a colab gist.", "Hi @Saduf2019. I created a github gist [here](https://colab.research.google.com/gist/loic-ehrhardt/2f0827698eaec79f245a3932356c5ae0/untitled0.ipynb). The minimal code to reproduce is in section \"Standalone code to reproduce the issue\" and what follows in the notebook are the extra information not necessary to reproduce it (but to help debugging this).", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d3ac43343b72d7e2a45dfd423386b5cb/untitled436.ipynb).", "Hi, do you have more information regarding this issue? Should we assign this ticket to some tensorflower?", "Hi,\r\n\r\nThe issue still exists in TensorFlow 2.4.1:\r\n\r\n```python3\r\nimport tensorflow as tf\r\n\r\n@tf.custom_gradient\r\ndef identity(x):\r\n    def grad_fn(grad_y):\r\n        return grad_y\r\n    return (x, grad_fn)\r\n\r\nprint(\"-\" * 20, \"Eager Tensor\", \"-\" * 20)\r\neager_tensor = tf.constant([4.0])\r\nprint(identity(eager_tensor))\r\n\r\nprint(\"-\" * 20, \"Keras Tensor\", \"-\" * 20)\r\nkeras_tensor = tf.keras.layers.Input(shape=(2))\r\nprint(identity(keras_tensor))\r\n```\r\n\r\nwhich gives:\r\n```\r\n$ python3 main.py \r\n-------------------- Eager Tensor --------------------\r\ntf.Tensor([4.], shape=(1,), dtype=float32)\r\n-------------------- Keras Tensor --------------------\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 15, in <module>\r\n    print(identity(keras_tensor))\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 261, in __call__\r\n    return self._d(self._f, a, k)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 215, in decorated\r\n    return _eager_mode_decorator(wrapped, args, kwargs)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 457, in _eager_mode_decorator\r\n    flat_result = [gen_array_ops.identity(x) for x in flat_result]\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 457, in <listcomp>\r\n    flat_result = [gen_array_ops.identity(x) for x in flat_result]\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3938, in identity\r\n    input, name=name, ctx=_ctx)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3957, in identity_eager_fallback\r\n    _attr_T, (input,) = _execute.args_to_matching_eager([input], ctx, [])\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 274, in args_to_matching_eager\r\n    t, dtype, preferred_dtype=default_dtype, ctx=ctx)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 339, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 265, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\n  File \"/home/loic/venv/tf/lib64/python3.6/site-packages/tensorflow/python/keras/engine/keras_tensor.py\", line 274, in __array__\r\n    'Cannot convert a symbolic Keras input/output to a numpy array. '\r\nTypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.\r\n```", "@loic-ehrhardt Based on error description, if you disable eager model by `tf.compat.v1.disable_eager_execution()` , then your code runs without an issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2d956509d3dec4298d2c493b8df90966/untitled.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43916\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43916\">No</a>\n"]}, {"number": 43915, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Nope\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: It didn't happen on mobile\r\n- TensorFlow installed from (source or binary): I used pip\r\n- TensorFlow version (use command below): tensorflow==2.3.1\r\n- Python version: Python 3.8.2\r\n- Bazel version (if compiling from source): I didn't use Bazel\r\n- GCC/Compiler version (if compiling from source): I did not compile it from source\r\n- CUDA/cuDNN version: I didn't install tensorflow for GPU, just tensorflow\r\n- GPU model and memory: I don't have one\r\n\r\n**Describe the current behavior**\r\n The current behavior is that I try to import tensorflow using `import tensorflow`\r\nAnd I get\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\M.Sahal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflo\r\nw\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dyna\r\nmic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\M.Sahal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflo\r\nw\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\M.Sahal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflo\r\nw\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\M.Sahal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflo\r\nw\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\M.Sahal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflo\r\nw\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\M.Sahal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflo\r\nw\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\M.Sahal\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflo\r\nw\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dyna\r\nmic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n \r\n**Describe the expected behavior**\r\nI expected for it to load normally.\r\n\r\n**Standalone code to reproduce the issue**\r\n`import tensorflow`\r\n\r\n**Other info / logs**\r\nI tried to install the newest version of Microsoft Visual C++ but that didn't solve it. I tried installing Keras too but that didn't fix it either.\r\n\r\nStack overflow question: https://stackoverflow.com/questions/64282592/failed-to-load-the-native-tensorflow-runtime-windows-8-1", "comments": ["Please @ravikyram can you help me out ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43915\">No</a>\n", "@sahal-mulki \r\n\r\nYou might be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python or 32-bit OS\r\nYou have not installed the [Microsoft Visual C++ Redistributable package\r\n](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\nYour CPU does not support AVX instructions.\r\nPlease take a look at the [system requirements ](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nPlease, refer duplicate issue #43130\r\nThanks!", "Thank you for responding, I do have Microsoft Visual C++ Redistributable package installed : \r\n![c++redistributables](https://user-images.githubusercontent.com/56467460/95814028-6ccb9180-0d2a-11eb-97ba-356e1c6200a4.png)\r\nAnd using [Coreinfo](url), I got told that yes, AVX is supported,\r\n\r\nAnd no my Python Installation isn't 32 bit and neither is my OS.\r\n\r\nAnd I wanted to say one thing, I had tensorflow installed before and one day for no reason\r\nit threw a error when I imported tensorflow, I tried everything to fix ,but in the end I ended up reinstalling it \r\nand python too, but now it's giving me this DLL error\r\n", "@sahal-mulki \r\n\r\nCan you create a new virtual environment and try install from scratch and see if you are seeing the same behavior. Thanks!", "I have tried that but I'm not on Linux, so `source` to activate the environment doesn't work, and for some reason neither does the `.virtualenviron\\Scripts\\activate` which should work on windows.", "@sahal-mulki \r\n\r\n@sahal-mulki,\r\nIn Windows, you can change your current directory to `../Scripts/` and then run the command `../Scripts/activate` to activate your virtual environment. Please, refer this [SO comment](https://stackoverflow.com/a/4528057) for more information.Thanks!", "Thanks, I'll try that.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Ok, @ravikyram  I tried installing and importing tensorflow in a virtual enviroment but it gave the same error", "@sahal-mulki I had similar issue (with Windows10) when I tried to install newer version of Tensorflow. I found that the reason was that i didn't completely removed folders related to older TF. \r\n\r\nCan you please find folders related to older TF--> delete those folders completely --> restart the computer -->follow the instructions [here](https://www.tensorflow.org/install/source_windows) --> install TF.\r\n\r\nAlso, I had similar issue with Windows10 but not sure how Windows8.1 and python3.8.2. Maybe try installing with python3.7. Please let us know how it progresses. Thanks!", "Ok, I'll try that, so just to clarify, I should remove all tensorflow folders and then install tensorflow?", "@sahal-mulki Correct. remove all folders related to Tensorflow, RESTART computer/laptop, install fresh TF (instructions in the link above), and test. Thanks! ", "I tried that but it still doesn't work. I installed tensorflow using pip after I deleted everything. It's possible that I didn't delete everything, but I am very sure I did.", "@sahal-mulki Difficult to find root-cause. I am not sure how to resolve. I would say keep trying to find any traces of tensorflow folders, env PATH, etc and delete them. You could also try Anaconda (we don't support) or Google colab. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43915\">No</a>\n", "\r\n@sahal-mulki have you solved your problem. I have the same problem.", "No but I was due to get a new laptop, when I got my new laptop and installed tensorflow and the problem wasnt there"]}, {"number": 43914, "title": "Internal launch timed out and terminated", "body": "2020-10-09 20:00:22.213230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-10-09 20:00:22.243996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.244526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 920M computeCapability: 3.5\r\ncoreClock: 0.954GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 14.92GiB/s\r\n2020-10-09 20:00:22.244886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-09 20:00:22.246939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-10-09 20:00:22.248872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-10-09 20:00:22.249439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-10-09 20:00:22.251486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-09 20:00:22.252804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-09 20:00:22.257173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-09 20:00:22.257405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.258014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.258479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n(4834, 21, 5)\r\n(538, 21, 5)\r\n2020-10-09 20:00:22.663431: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\n2020-10-09 20:00:22.663497: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs\r\n2020-10-09 20:00:22.664240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\r\n2020-10-09 20:00:22.665489: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1479] CUPTI activity buffer flushed\r\n2020-10-09 20:00:22.678966: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-10-09 20:00:22.684542: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2400500000 Hz\r\n2020-10-09 20:00:22.684806: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558bf28f53e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-09 20:00:22.684835: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-09 20:00:22.685094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.685482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 920M computeCapability: 3.5\r\ncoreClock: 0.954GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 14.92GiB/s\r\n2020-10-09 20:00:22.685564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-09 20:00:22.685607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-10-09 20:00:22.685631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-10-09 20:00:22.685653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-10-09 20:00:22.685676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-09 20:00:22.685698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-09 20:00:22.685721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-09 20:00:22.685813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.686221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.686534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-10-09 20:00:22.686597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-09 20:00:22.765993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-09 20:00:22.766040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-10-09 20:00:22.766058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-10-09 20:00:22.766352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.766933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.767337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-09 20:00:22.767749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1757 MB memory) -> physical GPU (device: 0, name: GeForce 920M, pci bus id: 0000:01:00.0, compute capability: 3.5)\r\n2020-10-09 20:00:22.769944: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558bf65bd9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-09 20:00:22.769984: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 920M, Compute Capability 3.5\r\n2020-10-09 20:00:25.217767: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_TIMEOUT: the launch timed out and was terminated\r\n2020-10-09 20:00:25.217849: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\nAborted (core dumped)\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro 20.1.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1.243/7.6\r\n- GPU model and memory: Nvidia 920m / 2GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43914\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43914\">No</a>\n", "Issue resolved by downgrading the display driver to v435"]}, {"number": 43913, "title": "keras.utils.Sequence instance can't be deleted from memory after training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 x64\r\n- TensorFlow version (use command below):2.3.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN : 10.1, 7.6.5\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI have created a custom class that inherits from keras.utils.Sequence\r\nThis class is reading some images with opencv into memory when constructed\r\nIf i just instantiate this class and after this i call \r\n```\r\nimport gc\r\n\r\nmyObj = Myclass(imgpaths)\r\ndel myObj\r\ngc.colect()\r\n```\r\n\r\neverything is ok, but, if i am creating an instance + performing training:\r\n```\r\nimport gc\r\n\r\ntrain= Myclass(imgpaths)\r\nvalidation= Myclass(imgpaths)\r\n\r\nmodel = MyCustomModel()#i just used the pretrained Xception with 2 custom layers at the top for my case\r\nmodel.fit(train, validation,...)\r\ndel train\r\ndel validation\r\ngc.collect()\r\n#...\r\n#here i am doing other things\r\n#...\r\n\r\n```\r\nthen i can see that memory is not freed when calling gc.collect() and is freed only when the process is stopped\r\n**Describe the expected behavior**\r\n\r\nmodel should free the Sequence class passed to it when training is finished\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n#create a simple Sequence instance that is reading some images when constructed\r\n#create an Xception model and call fit() with that sequence\r\n#import gc, call del train_sequence, call gc.collect()\r\n#create a for loop so that you will be able to see that memory is not freed (for i in range(1,999999999999999999): a=5)\r\n#inspect RAM consumption and observe that RAM use is not decreasing after gc.collect(), meaning that keras model stores internally a reference of the Sequence object and it is not allowing to free the memory\r\n", "comments": ["Please can you share a very minimal buy runnable standalone example or Colab?", "ill share it tomorrow ", "@Moldoteck \r\nIs this still an issue.", "@Saduf2019 , yes, but i don't have time to provide an (minimal) example now. So, i'll close this issue and will re-open when i'll have time for an example in Colab", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43913\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43913\">No</a>\n"]}, {"number": 43912, "title": "Optimized cadence kernels need to use the new TfLiteEvalTensor API", "body": "The TFLM team recently ported all kernels except a handful of externally maintained optimized kernels. The new API enables very low memory overhead for TFLM. The primary change is the `TfLiteTensor` C struct is only available during `TfLiteRegistration::Prepare` for a kernel. Those structs are served under temporary memory and all data is only available during the lifetime of that method. All TFLM kernels should request the `TfLiteEvalTensor` C struct during `TfLiteRegistration::Eval` calls. \r\n\r\nA sample change looks like this:\r\nhttps://github.com/tensorflow/tensorflow/commit/e392050297b62772fb9e6aaf10cf1214cb5261e7\r\n\r\nThis issue tracks updating the Cadence hifi kernels to this new API.\r\n\r\n@pnikam-cad @nyadla-sys\r\n\r\nPTAL - happy to review any PRs as they come in.\r\n", "comments": ["@nkreeger \r\nThanks for the inputs above, we will do the necessary updates and submit a PR.", "I see the related PR is merged here https://github.com/tensorflow/tensorflow/pull/46411 and the justification of the linked PR is here https://github.com/tensorflow/tensorflow/pull/44581#issuecomment-759778491, please move this issue to closed if your issue is resolved. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43912\">No</a>\n"]}, {"number": 43911, "title": "\"device kernel image is invalid\" when using pre-built linux+gpu c library", "body": "Hi,\r\n\r\nI'm using FFmpeg which calls into tensorflow c library  with model file espcn.pb (see attached), the model is very simple with several layers.\r\n\r\n[espcn.zip](https://github.com/tensorflow/tensorflow/files/5354977/espcn.zip)\r\n\r\nit runs successfully based on cpu path (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.3.0.tar.gz), but there's issue when just change the tf c library to be gpu path (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.3.0.tar.gz)\r\n```\r\n...\r\n2020-10-09 20:47:23.123617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-09 20:50:38.630731: E tensorflow/core/common_runtime/session.cc:91] Failed to create session: Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n2020-10-09 20:50:38.630816: E tensorflow/c/c_api.cc:2184] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n```\r\n\r\nhttps://forums.developer.nvidia.com/t/cuda-10-1-243-tensorflow-gpu-2-3-0rc0-cuda-runtime-error-device-kernel-image-invalid/145187 said that this issue  happens with tf version 2.3, there's no such issue with tf version 2.2.  Could you confirm it? thanks.\r\n\r\nIs this issue still there with latest tf code? If no, i'll try to build from source code. \r\n\r\nHow about to roll back to tf version 1.15 which is the previous pre-built c library before current 2.3 version, thanks.\r\n\r\n", "comments": ["@guoyejun,\r\nCould you please provide the following information along with the complete code to reproduce the issue reported here.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:", "Also, please take a look at issue [#41990](https://github.com/tensorflow/tensorflow/issues/41990) with a similar error. Looks like the issue has been resolved with the latest TF-nightly. Thanks!", "thanks, i'll try with the latest TF.  But looks that there's no gpu version at the TF-nightly https://storage.googleapis.com/libtensorflow-nightly, they are all about cpu.   Where could I get the gpu version? thanks.", "> Looks like the issue has been resolved with the latest TF-nightly. Thanks!\r\n\r\ncould you provide how to get the tensorflow-gpu version from TF-nightly? and so i can verify it. thanks.\r\n\r\ni don't find tensorflow-gpu from https://storage.googleapis.com/libtensorflow-nightly.\r\n", "@guoyejun Are you using conda? If so we don't support conda.\r\n\r\n`!pip install tf-nightly-gpu` installs the nightly version of GPU.", "> @guoyejun Are you using conda? If so we don't support conda.\r\n> \r\n> `!pip install tf-nightly-gpu` installs the nightly version of GPU.\r\n\r\nthanks, what I want is the tensorflow c library of GPU version of the nightly build. 'pip install ...' does not contain the c library.\r\n\r\nsee more about c library at https://www.tensorflow.org/install/lang_c, thanks.", "Sorry for the long delay. I don't know why the error happens, assigning @sanjoy in case he knows.\r\nCan you check if your cuda version matches supported cuda version for 2.3? https://www.tensorflow.org/install/source.\r\n\r\nYou can also try building from source to get the latest version. Follow instructions in:\r\nhttps://www.tensorflow.org/install/source\r\nbut instead build `libtensorflow` target https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md", "Hi @guoyejun,\r\n\r\nYou can install libtensorflow from https://www.tensorflow.org/install/lang_c.  Can you try with the latest nightly images?  The set of supported GPUs has changed recently so it is possible that the latest version will just work for you.", "> Sorry for the long delay. I don't know why the error happens, assigning @sanjoy in case he knows.\r\n> Can you check if your cuda version matches supported cuda version for 2.3? https://www.tensorflow.org/install/source.\r\n> \r\n> You can also try building from source to get the latest version. Follow instructions in:\r\n> https://www.tensorflow.org/install/source\r\n> but instead build `libtensorflow` target https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md\r\n\r\nthanks, i've  tried so, but the build fails. I did not record the fail logs. And the configs (special for cuda option) might not correct even if the build succeeds. ", "> Hi @guoyejun,\r\n> \r\n> You can install libtensorflow from https://www.tensorflow.org/install/lang_c. Can you try with the latest nightly images? The set of supported GPUs has changed recently so it is possible that the latest version will just work for you.\r\n\r\nthanks, i'll try with new version 2.3.1 at https://www.tensorflow.org/install/lang_c when have time. It was version 2.3.0 when I reported this issue.\r\n\r\nfor the nightly image, there is no gpu version. I've reported the missing, but there is still no gpu version now.\r\n ", "@guoyejun \r\nI have found /libtensorflow-gpu-linux-x86_64-2.3.0.tar.gz [here](https://storage.googleapis.com/tensorflow)", "@guoyejun Could you please refer to this [link](https://www.tensorflow.org/install/lang_c) ? Could you please try on latest stable version of TF 2.5  and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43911\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43911\">No</a>\n"]}, {"number": 43910, "title": "Optimized arc kernels need to use the new TfLiteEvalTensor API", "body": "The TFLM team recently ported all kernels except a handful of externally maintained optimized kernels. The new API enables very low memory overhead for TFLM. The primary change is the `TfLiteTensor` C struct is only available during `TfLiteRegistration::Prepare` for a kernel. Those structs are served under temporary memory and all data is only available during the lifetime of that method. All TFLM kernels should request the `TfLiteEvalTensor` C struct during `TfLiteRegistration::Eval` calls. \r\n\r\nA sample change looks like this:\r\nhttps://github.com/tensorflow/tensorflow/commit/e392050297b62772fb9e6aaf10cf1214cb5261e7\r\n\r\nThis issue tracks updating the Arc kernels to this new API.\r\n\r\n@dzakhar \r\n@JaccovG\r\n\r\nPTAL - happy to review any PRs as they come in.\r\n", "comments": []}, {"number": 43909, "title": "tf.signal.stft has side effects", "body": "**Describe the current behavior**\r\n**Describe the expected behavior**\r\n**Standalone code to reproduce the issue**\r\n\r\nThe following script crashes if the designated line is **uncommented**, else it runs fine. Note that the output of tf.signal.stft is not used.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n# uncomment the next line for serious side effects on the GPU (tf 2.1.x, OK on 2.3.x, 2.2.x)\r\n# x = tf.signal.stft(tf.ones((16000,)), frame_length=512, frame_step=256)\r\nx = tf.ones((61, 257), dtype=tf.float32)\r\n\r\n# model\r\nl0 = keras.Input(shape=x.shape)\r\nl1 = layers.GRU(100)(l0)\r\nl2 = layers.Dense(100, activation=\"relu\")(l1)\r\nmodel = keras.Model(inputs=l0, outputs=l2)\r\nprint(model.summary())\r\n\r\n# call\r\nx = tf.expand_dims(x, 0)\r\ny = model(x)\r\n```\r\n\r\n**Other info / logs**\r\nThis is a partial crash log:\r\n```\r\n2020-10-09 12:45:02.659125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-09 12:45:03.818397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-10-09 12:45:03.974618: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-10-09 12:45:03.982373: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-10-09 12:45:03.982410: W tensorflow/stream_executor/stream.cc:2041] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 18, in <module>\r\n    y = model(x)\r\n  File \"/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 822, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 717, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 891, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 822, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py\", line 1142, in call\r\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\r\n  File \"/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 5616, in mat_mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/adev/.pyenv/versions/ndp120/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(1, 100), b.shape=(100, 100), m=1, n=100, k=100 [Op:MatMul]\r\n```\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5354967/tf_env.txt)\r\n", "comments": ["@bliep \r\n\r\nI am not seeing any issue in TF nightly version(`2.4.0-dev20201007`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/eb574159bb2868a8b643cb0f813f6920/untitled440.ipynb).Please, verify once and close the issue.Please, use the latest TF versions for better performance.Thanks!", "@ravikyram \r\n\r\nRetested on the provided colab gist, now with TF 2.1.x, unable to reproduce.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43909\">No</a>\n"]}, {"number": 43908, "title": "which cuda version is required for the pre-built c library", "body": "I'm trying to use linux gpu support pre-built c library at https://www.tensorflow.org/install/lang_c on a system which contains directory such as /usr/local/cuda-11.0/targets/x86_64-linux/lib/  (i want to use cuda instead of cpu)\r\n\r\n```\r\n# ls /usr/local/cuda-11.0/targets/x86_64-linux/lib/libcu\r\nlibcublas.so                 libcudart.so.11.0.221        libcuinj64.so.11.0           libcusolver.so\r\nlibcublas.so.11              libcudart_static.a           libcuinj64.so.11.0.221       libcusolver.so.10\r\nlibcublas.so.11.2.0.252      libcufft.so                  libculibos.a                 libcusolver.so.10.6.0.245\r\nlibcublasLt.so               libcufft.so.10               libcupti.so                  libcusolverMg.so\r\nlibcublasLt.so.11            libcufft.so.10.2.1.245       libcupti.so.11.0             libcusolverMg.so.10\r\nlibcublasLt.so.11.2.0.252    libcufftw.so                 libcupti.so.2020.1.1         libcusolverMg.so.10.6.0.245\r\nlibcudadevrt.a               libcufftw.so.10              libcurand.so                 libcusparse.so\r\nlibcudart.so                 libcufftw.so.10.2.1.245      libcurand.so.10              libcusparse.so.11\r\nlibcudart.so.11.0            libcuinj64.so                libcurand.so.10.2.1.245      libcusparse.so.11.1.1.245\r\n```\r\n\r\nI met the following issues when I tried to run my application based on the tf c library.\r\n```\r\n...\r\n2020-10-09 10:13:05.274477: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-09 10:13:05.276781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-09 10:13:05.557487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\r\ncoreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-10-09 10:13:05.558557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:82:00.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-10-09 10:13:05.559954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 2 with properties: \r\npciBusID: 0000:02:00.0 name: Tesla P40 computeCapability: 6.1\r\ncoreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s\r\n2020-10-09 10:13:05.560105: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/\r\n2020-10-09 10:13:05.560239: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/\r\n2020-10-09 10:13:05.561279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-09 10:13:05.561567: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-09 10:13:05.563732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-09 10:13:05.563897: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/\r\n2020-10-09 10:13:05.564023: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/workspace/tf/lib/\r\n2020-10-09 10:13:05.564037: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-09 10:13:05.564066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-09 10:13:05.564077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 2 \r\n2020-10-09 10:13:05.564085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N N \r\n2020-10-09 10:13:05.564091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N N \r\n2020-10-09 10:13:05.564098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 2:   N N N \r\n```\r\n\r\ni guess the pre-build c library depends on cuda 10? is it right? thanks.\r\n\r\nbtw, can I build the c library locally with cuda 11? thanks.", "comments": ["@guoyejun,\r\nEvery TensorFlow release is compatible with a certain CUDA/cuDNN version. For more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu) and [this](https://github.com/tensorflow/tensorflow/issues/42895#issuecomment-685874736) comment.\r\n\r\n> i guess the pre-build c library depends on cuda 10? is it right? thanks.\r\n\r\nYes, looking at the error log it seems like TensorFlow is searching for CUDA 10.1. Hence, could you please install CUDA 10.1 with cuDNN 7.6 and check if it works. Thanks!", "thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43908\">No</a>\n"]}, {"number": 43907, "title": "Cleanup Selectv2 broadcasting", "body": "This PR changes the broadcasting in the `Selectv2` op to use a 3-ary broadcast instead of multiple 2-ary broadcasts which should improve readability.", "comments": []}, {"number": 43906, "title": "Merge pull request #1 from tensorflow/master", "body": "update", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43906) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 43903, "title": "Link error occurs due to MetaGraphDef", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0(master branch)\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0/8.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am building tensorflow 2.4.0(master branch / lastest commit : 68a6fe0d984377b625b421d34f8c607d6ed73597) from source with additional link symbols\r\n\r\n\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\Anaconda3\\envs\\py37\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Anaconda3\\envs\\py37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Anaconda3\\envs\\py37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.0 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5, 5.0, 5.2, 6.1, 7.0, 7.5, 8.0\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\nbazel build -c opt --config=opt --config=cuda --config=v1 //tensorflow:libtensorflow_cc.so\r\n(or bazel build -c opt --config=opt --config=cuda --config=v1 //tensorflow:tensorflow_cc.dll)\r\n\r\nboth way occurs same error messages.\r\n\r\nI think this line causes error in c++\r\n\t\tMetaGraphDef graphDef;\r\n                std::istream istream\r\n\t\tbool stat = graphDef.ParseFromIstream(istream);\r\n and the error message is \r\nlibtensorflow_cc.so.2.4.0.if.exp : error LNK2001: \ud655\uc778\ud560 \uc218 \uc5c6\ub294 \uc678\ubd80 \uae30\ud638(maybe this phrase is same as 'unresolved external symbols') \"public: bool __cdecl google::protobuf::Message::ParseFromIstream(class std::basic_istream<char,struct std::char_traits<char> > *)\" (?ParseFromIstream@Message@protobuf@google@@QEAA_NPEAV?$basic_istream@DU?$char_traits@D@std@@@std@@@Z)\r\n\r\n\r\nwhen i'm building tf 1.13 from source(CUDA10.0/cudnn7) using tf_exported_symbols_msvc.lds following this(https://github.com/tensorflow/tensorflow/issues/23542) i could successfully import additional symbols\r\nHowever, when i'm building tf2.4.0, i could not import additional symbols via tf_exported_symbols_msvc.lds.\r\nAs described in (https://github.com/tensorflow/tensorflow/issues/23542), i think that tf do not support the way to add symbols by tf_exported_symbols_msvc.lds. So it tried to add symbols by explicitly add symbols on tensorflow/tools/def_file_filter/def_file_filter.py.tpl like following lines\r\n\r\n      if DATA_EXCLUDE_RE.search(line):\r\n        def_fp.write(\"\\t\" + decorated + \"\\n\")\r\n      else:\r\n        def_fp.write(\"\\t\" + decorated + \" DATA\\n\")\r\n      taken.add(decorated)\r\n\r\n    for sym in symbols_pybind:\r\n      def_fp.write(\"\\t{}\\n\".format(sym))\r\n      taken.add(sym)\r\n    # write additional symbols\r\n    def_fp.write(\"\\t??$CreateMaybeMessage@VTensorShapeProto@tensorflow@@$$V@Arena@protobuf@google@@CAPEAVTensorShapeProto@tensorflow@@PEAV012@@Z\\n\")\r\n    def_fp.write(\"\\t?AppendToString@MessageLite@protobuf@google@@QEBA_NPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z\\n\")\r\n    def_fp.write(\"\\t?SerializePartialAsString@MessageLite@protobuf@google@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ\\n\")\r\n\r\n\r\n\r\nBy this way, i think i could add additional symbols to tensorflow_cc.lib. and the log messsage says that lnk2001 unresolved symbols on these four symbols\r\n    def_fp.write(\"\\t??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z\\n\")\r\n    def_fp.write(\"\\t?ParseFromIstream@Message@protobuf@google@@QEAA_NPEAV?$basic_istream@DU?$char_traits@D@std@@@std@@@Z\\n\")\r\n    def_fp.write(\"\\t?find@string_view@absl@@QEBA_KD_K@Z\\n\")\r\n    def_fp.write(\"\\t?tensor_data@Tensor@tensorflow@@QEBA?AVstring_view@absl@@XZ\\n\")\r\nand when i erase these symbols from def_file_filter.py.tpl i could successfully add extra symbols on tensorflow_cc.lib(or libtensorflow_cc_.lib) by writing on def_file_filter.py.tpl\r\n\r\nPROBLEMS\r\n- I couldn't add above four extra symbols(Also, I couldn't find MetaGraphDef class.. that i couold found on tf1.13)\r\n\r\nThe question is, that i want to use those functions on c++, and i could use it when i build tensorflow on tf 1.13(cuda10.0, cudnn 7.0, bazel build -c opt --config=opt --config=cuda --config=noignite --config=nokafka //tensorflow:libtensorflow_cc.so)  by adding extra symbols from  tf_exported_symbols_msvc.lds. BUT when it comes to tf 2.4.0, i couldn't add some of the extra symbols that i could add in tf 1.13(above four symbols). It tooks almost 10 days to figure out that tf2.4.0 do not support extra symbols by tf_exported_mscvc.lds.... \r\nI think it's because while building tf1.13 they contains the codes from tensorflow/python/framework/meta_graph.py in c++ dll and lib. but tf2.4.0 does not..\r\n[build logs.txt](https://github.com/tensorflow/tensorflow/files/5352426/build.logs.txt)\r\n\r\nTherefore i think this problem does not related with exporting extra symbols(https://github.com/tensorflow/tensorflow/issues/23542), but it is related with how to build c++ with additional libraries...(I could find metagraphdef it tf.compat.v1.MetaGraphDef in tf2.3.0 python documentation. So i think they do support metagraphdef.. but i couldn't use it on c++ 2.4.0)\r\n\r\nIf anyone knows how to figure out this problems, i would appreciate your advice.\r\n\r\n\r\n[log and def_file_filter.zip](https://github.com/tensorflow/tensorflow/files/5352429/log.and.def_file_filter.zip)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["> Please input the desired Python library path to use. Default is [C:\\Anaconda3\\envs\\py37\\lib\\site-packages]\r\n\r\n@yangjonghyeon,\r\nCould you please try building TensorFlow using Python (outside of Anaconda environment) and check if it works?\r\n\r\nAlso, make sure you are following the steps mentioned as per the [build from source](https://www.tensorflow.org/install/source_windows) guide. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43903\">No</a>\n"]}, {"number": 43902, "title": "Supporting class_weight in train_step ?", "body": "Hi, I have read more about overriding train_step here: https://keras.io/guides/customizing_what_happens_in_fit/. It helps me a lot. But, I still have a problem about support for class_weight in train_step? How to do it? My part code is as follows:\r\n\r\n    def train_step(self, data):\r\n        if isinstance(data, tuple):\r\n            x,y,class_weight  = data\r\n        with tf.GradientTape() as tape:\r\n\r\n            Flair_encoder_out,T1_encoder_out,T1ce_encoder_out,T2_encoder_out,\\\r\n            Flair_decoder_out,T1_decoder_out,T1ce_decoder_out,T2_decoder_out,\\\r\n            grade_out = self.MMDR_model(x)\r\n\r\n            z_mean_Flair, z_log_var_Flair, z_Flair, common_Flair,spec_Flair = Flair_encoder_out\r\n            z_mean_T1, z_log_var_T1, z_T1, common_T1, spec_T1 =T1_encoder_out\r\n            z_mean_T1ce, z_log_var_T1ce, z_T1ce,common_T1ce, spec_T1ce = T1ce_encoder_out\r\n            z_mean_T2, z_log_var_T2, z_T2,common_T2, spec_T2 = T2_encoder_out\r\n\r\n            reconstruction_loss = tf.reduce_mean(\r\n                tf.keras.losses.mean_squared_error(x[0], Flair_decoder_out)\r\n            )+tf.reduce_mean(\r\n                tf.keras.losses.mean_squared_error(x[1], T1_decoder_out)\r\n            )+tf.reduce_mean(\r\n                tf.keras.losses.mean_squared_error(x[2], T1ce_decoder_out)\r\n            )+tf.reduce_mean(\r\n                tf.keras.losses.mean_squared_error(x[3], T2_decoder_out)\r\n            )\r\n\r\n            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var_Flair - tf.square(z_mean_Flair) - tf.exp(z_log_var_Flair))\\\r\n                      + (-0.5 * tf.reduce_mean(1 + z_log_var_T1 - tf.square(z_mean_T1) - tf.exp(z_log_var_T1)))\\\r\n                      + (-0.5 * tf.reduce_mean(1 + z_log_var_T1ce - tf.square(z_mean_T1ce) - tf.exp(z_log_var_T1ce)))\\\r\n                      + (-0.5 * tf.reduce_mean(1 + z_log_var_T2 - tf.square(z_mean_T2) - tf.exp(z_log_var_T2)))\r\n\r\n            com_loss = tf.reduce_mean(mean_squared_error(common_Flair, common_T1)) + tf.reduce_mean(mean_squared_error(common_Flair,common_T1ce)) + \\\r\n                        tf.reduce_mean(mean_squared_error(common_Flair, common_T2)) + tf.reduce_mean(mean_squared_error(common_T1, common_T1ce)) + \\\r\n                        tf.reduce_mean(mean_squared_error(common_T1,common_T2)) + tf.reduce_mean(mean_squared_error(common_T1ce,common_T2))\r\n\r\n            spec_loss = tf.reduce_mean(mean_squared_error(spec_Flair, spec_T1)) + tf.reduce_mean(mean_squared_error(spec_Flair, spec_T1ce)) + \\\r\n                        tf.reduce_mean(mean_squared_error(spec_Flair, spec_T2)) + tf.reduce_mean(mean_squared_error(spec_T1, spec_T1ce)) + \\\r\n                        tf.reduce_mean(mean_squared_error(spec_T1, spec_T2)) + tf.reduce_mean(mean_squared_error(spec_T1ce, spec_T2))\r\n            com_spec_loss = tf.sqrt(com_loss / spec_loss)\r\n\r\n            grade_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y,grade_out))\r\n\r\n            # grade_loss = self.compiled_loss(\r\n            #     y,\r\n            #     grade_out,\r\n            #     sample_weight=class_weight,\r\n            #     regularization_losses=self.losses,\r\n            # )\r\n            #total_loss = 0.1 * reconstruction_loss + 0.01 * kl_loss + 0.25 * com_spec_loss + grade_loss\r\n            total_loss =  0.1 * reconstruction_loss +  kl_loss + 0.1 * com_spec_loss + grade_loss\r\n            # The loss function is configured in `compile()`.\r\n\r\n        # The training happens here.\r\n        grads = tape.gradient(total_loss, self.trainable_variables)\r\n\r\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n        print(\">>>>>>>>>>>>>>metrics_names:\",self.metrics_names)\r\n\r\n        # Update metrics (includes the metric that tracks the loss)\r\n        self.compiled_metrics.update_state(y, grade_out)\r\n        return {\r\n            \"loss\": total_loss,\r\n            \"rec_loss\": reconstruction_loss,\r\n            \"kl_loss\": kl_loss,\r\n            \"cs_loss\": com_spec_loss,\r\n            \"grade_loss\": grade_loss,\r\n            self.metrics[0].name: self.metrics[0].result(),\r\n            self.metrics[1].name: self.metrics[1].result()\r\n        }", "comments": ["@chengjianhong \r\nCould you please explain the issue faced with \"support for class_weight in train_step\" as you have not filled the issue template also mention the tf version, if the issue is about how to use you may open a ticket at [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) as there is also a larger community that reads questions there.\r\n", "@Saduf2019 I want to use class_weight to solve the class imbalanced problem. But, I use TF2.2 and custom a train_step to achieve multi-task learning. And It does not work to directly add class_weight in model.fit()!", "@chengjianhong Did you try the example mentioned in https://keras.io/guides/customizing_what_happens_in_fit/ \r\n\r\nCan you please share a simple standalone code to reproduce any error you are facing? Thanks!", "@jvishnuvardhan The example mentioned in https://keras.io/guides/customizing_what_happens_in_fit/ is about the sample_weight instead of class_weight? My question is how to use class_weight when customizing the train_setp function. My custom train_step function is shown above.", "Hi @chengjianhong, apologies for the delay here. Please see [this example colab ](https://colab.research.google.com/gist/nikitamaia/e4c0779d0d8d885b76933240986ea697/class_weight_mnist.ipynb) that uses class_weight in train_step to train an MNIST model. Hope this helps!\r\n\r\nClosing this issue now. If you need further support help, please post on Stack Overflow."]}, {"number": 43900, "title": "[TF-TRT] Cast now supports more conversions if TRT >= 7.0.0.11", "body": "@bixia1 for review\r\n@tfeher FYI\r\n\r\nTF-TRT Converter is now able to perform the following:\r\n\r\n```\r\n#if IS_TRT_VERSION_GE(7, 0, 0, 11)\r\n   - DT_FLOAT => DT_FLOAT\r\n   - DT_FLOAT => DT_HALF\r\n   - DT_HALF => DT_HALF\r\n   - DT_HALF => DT_FLOAT\r\n#endif\r\n```", "comments": ["@bixia1 I doubled check with the TRT team. The extra casting are fine from TRT >= 7.0.0.11 (first GA release)\r\n\r\nAnd when running the unittest:\r\n```bash\r\n[ RUN      ] OpConverterTest.ConvertCast\r\n2020-10-08 21:54:29.956009: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:39] TF-TRT Warning: DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2020-10-08 21:54:29.957195: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:39] TF-TRT Warning: DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2020-10-08 21:54:29.958374: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:39] TF-TRT Warning: DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2020-10-08 21:54:29.959371: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:39] TF-TRT Warning: DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n[       OK ] OpConverterTest.ConvertCast (6 ms)\r\n```\r\n\r\nThe warning message is 100% normal. It's a desired behavior.", "@DEKHTIARJonathan  Can you please check @bixia1's comments and resolve conflicts?. Thanks!", "@bixia1 are we good ? I think I have everything ready\r\nI have squashed my commit", "> @bixia1 are we good ? I think I have everything ready\r\n> I have squashed my commit\r\n\r\nThere is one comment from me that you didn't see, and hasn't been resolved yet.  Can you please check? ", "@bixia1 last comment addressed and solved. This time we are good.", "Added tests are failing, here is an error message:\r\nthird_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:5722\r\nExpected equality of these values:\r\n  TfDataTypeToTrt(OUT_DTYPE)\r\n    Which is: 4-byte object <00-00 00-00>\r\n  output.tensor()->getType()\r\n    Which is: 4-byte object <01-00 00-00>\r\nExpected kFLOAT vs. actual kHALF\r\nStack trace:\r\n0x55d56e948a11: void tensorflow::tensorrt::convert::TestConvertCast<(tensorflow::DataType)19, (tensorflow::DataType)1>(tensorflow::tensorrt::convert::OpConverterTest*) @ ??:??\r\n0x55d56ef88561: testing::Test::Run() @ ??:??\r\n0x55d56ef89808: testing::TestInfo::Run() @ ??:??\r\n... Google Test internal frames ...", "@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@DEKHTIARJonathan  Any update on this PR? Please. Thanks!", "@DEKHTIARJonathan Can you please squash the two commits into one? thanks!", "@bixia1 still working on identifying the issue with TRT 5.\r\nI still can't get TF to compile with TRT 5 unfortunately.", "Would you please squash the commits into one?", "These two tests still failed:\r\nCastInt32ToFp32Test.testTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration\r\nCastInt32ToFp32Test.testTfTrt_OfflineConversion_StaticEngine_FP16_NoCalibration\r\n", "@bixia1 can you post the log ? The tests pass perfectly for me. Where is that test even defined ? I can't find `CastInt32ToFp32Test` in the codebase ?", "The following three \"tests\" failed in TRT5, I verified without your change they all pass\r\n(1) tensorflow/python/compiler/tensorrt:gpu_cast_test\r\nBuild Flags: --config=cuda --compilation_mode=opt --test_env=TF2_BEHAVIOR=1\r\n(2) tensorflow/python/compiler/tensorrt:gpu_cast_test\r\nBuild Flags: --config=cuda --compilation_mode=opt\r\n(3)tensorflow/python/compiler/tensorrt:gpu_rank_two_test\r\nBuild Flags: --config=cuda --compilation_mode=opt\r\n\r\nattached logs\r\n\r\n[rank_two_test.log](https://github.com/tensorflow/tensorflow/files/5594843/rank_two_test.log)\r\n[cast_test_tf1.log](https://github.com/tensorflow/tensorflow/files/5594845/cast_test_tf1.log)\r\n[cast_test_tf2.log](https://github.com/tensorflow/tensorflow/files/5594846/cast_test_tf2.log)\r\n", "Oh yeah ! Now I see the issue ;) \r\nThe unittests needs to be updated. They are not good anymore:\r\nExample of line that needs to be changed: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/compiler/tensorrt/test/cast_test.py#L50-L53", "Hi @bixia1,\r\n\r\nI implemented the mechanism \"NOT_AVAILABLE\" we talked about this morning. Now everything looks good and stable. All the unittests are passing & stable.\r\n\r\n- Could you run the Google CI / Unittests ?\r\n- Can you provide a feedback on this comment: https://github.com/tensorflow/tensorflow/blob/943771c98/tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc#L736-L739. If you prefer a different phrasing. Please give me exactly what would you like here and I will update ;) \r\n\r\n----------------\r\n\r\n```bash\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test                     PASSED in 14.7s\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                 PASSED in 14.5s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test                     PASSED in 85.0s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                 PASSED in 88.1s\r\n//tensorflow/compiler/tf2tensorrt:segment_test                           PASSED in 0.2s\r\n//tensorflow/compiler/tf2tensorrt:segment_test_gpu                       PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                       PASSED in 12.8s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                   PASSED in 12.7s\r\n//tensorflow/compiler/tf2tensorrt:trt_allocator_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test                     PASSED in 12.7s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                 PASSED in 12.6s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test           PASSED in 12.6s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu       PASSED in 12.2s\r\n//tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test   PASSED in 12.0s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu PASSED in 12.2s\r\n\r\n[...]\r\n\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test                   PASSED in 30.0s\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test_gpu               PASSED in 30.2s\r\n\r\n[...]\r\n\r\nRunning ./tensorflow/python/compiler/tensorrt/test/cast_test.py ... [SUCCESS]\r\n```", "I found a little issue. Please off to merge / run the CI ;) I'll fix it tomorrow.", "Here we go:\r\n\r\n```bash\r\nINFO: Build completed successfully, 58 total actions\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test                     PASSED in 14.7s\r\n//tensorflow/compiler/tf2tensorrt:convert_graph_test_gpu                 PASSED in 14.5s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test                     PASSED in 86.3s\r\n//tensorflow/compiler/tf2tensorrt:convert_nodes_test_gpu                 PASSED in 88.3s\r\n//tensorflow/compiler/tf2tensorrt:segment_test                           PASSED in 0.2s\r\n//tensorflow/compiler/tf2tensorrt:segment_test_gpu                       PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc                       PASSED in 12.9s\r\n//tensorflow/compiler/tf2tensorrt:tensorrt_test_cc_gpu                   PASSED in 12.9s\r\n//tensorflow/compiler/tf2tensorrt:trt_allocator_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test                     PASSED in 12.7s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_op_test_gpu                 PASSED in 12.5s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test           PASSED in 12.3s\r\n//tensorflow/compiler/tf2tensorrt:trt_engine_resource_ops_test_gpu       PASSED in 12.2s\r\n//tensorflow/compiler/tf2tensorrt:trt_lru_cache_test                     PASSED in 0.1s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test   PASSED in 12.3s\r\n//tensorflow/compiler/tf2tensorrt:trt_shape_optimization_profiles_test_gpu PASSED in 12.0s\r\n\r\n[...]\r\n\r\nINFO: Build completed successfully, 3 total actions\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test                   PASSED in 30.6s\r\n//tensorflow/python/compiler/tensorrt:trt_convert_test_gpu               PASSED in 31.6s\r\n\r\n[...]\r\n\r\nRunning ./tensorflow/python/compiler/tensorrt/test/annotate_max_batch_sizes_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/base_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/batch_matmul_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/biasadd_matmul_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/binary_tensor_weight_broadcast_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/cast_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/combined_nms_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/concatenation_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/const_broadcast_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/conv2d_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/dynamic_input_shapes_test.py ... [IGNORED]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/identity_output_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/int32_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/lru_cache_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/memory_alignment_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/multi_connection_neighbor_engine_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/neighboring_engine_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/quantization_mnist_test.py ... [IGNORED]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/quantization_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/rank_two_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/reshape_transpose_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/topk_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/trt_mode_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/unary_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/vgg_block_nchw_test.py ... [SUCCESS]\r\nRunning ./tensorflow/python/compiler/tensorrt/test/vgg_block_test.py ... [SUCCESS]\r\nAll 24 tests PASSED\r\n```\r\n\r\n@bixia1 can you run the Google CI ?", "tensorflow/python/compiler/tensorrt:gpu_cast_test still fails with TensorRT 7. see log\r\n[fail.log](https://github.com/tensorflow/tensorflow/files/5645837/fail.log)\r\n ", "@bixia1 what is the command you use to launch that job ? I can't replicate the problem\r\n", "> @bixia1 what is the command you use to launch that job ? I can't replicate the problem\r\n\r\nthere is nothing specific about the command I use to run the test. I think in github, you command may look like this:\r\nbazel test tensorflow/python/compiler/tensorrt:cast_test --config=cuda --compilation_mode=opt\r\n\r\nIt passed with cuda 10.1 + TRT 5, but failed with cuda 11  + TRT 7.1\r\n\r\n", "I does fail, in case this matter, on Tesla V100 with TRT 7.1.3.", "I don't understand something here. The command you gave me doesn't do anything:\r\n\r\n```bash\r\n$ bazel test tensorflow/python/compiler/tensorrt:cast_test --config=cuda --compilation_mode=opt\r\n\r\nWARNING: The following configs were expanded more than once: [v2, cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=205\r\nINFO: Reading rc options for 'test' from /opt/tensorflow/tensorflow-source/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'test' from /opt/tensorflow/tensorflow-source/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_t\r\noolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=stand\r\nalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'test' from /opt/tensorflow/tensorflow-source/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3.8 --config=xla --config=tensorrt --ac\r\ntion_env TF_CUDA_VERSION=11.1 --action_env TF_CUBLAS_VERSION=11 --action_env TF_CUDNN_VERSION=8 --action_env TF_TENSORRT_VERSION=7 --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_PATHS=/usr,/usr/local/\r\ncuda --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.0,6.1,7.0,7.5,8.0,8.6 --action_env LD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:/usr/local/cuda/e\r\nxtras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Reading rc options for 'test' from /root/.bazelrc:\r\n  Inherited 'build' options: --spawn_strategy=standalone --genrule_strategy=standalone\r\nINFO: Reading rc options for 'test' from /opt/tensorflow/tensorflow-source/.bazelrc:\r\n  'test' options: --define open_source_build=true --config=v2\r\nINFO: Reading rc options for 'test' from /opt/tensorflow/tensorflow-source/.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium --test_env=LD_LIBRARY_PATH\r\nINFO: Found applicable config definition build:short_logs in file /opt/tensorflow/tensorflow-source/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file /opt/tensorflow/tensorflow-source/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only --build_tag_filters=\r\n-benchmark-test,-no_oss,-no_gpu,-v1only\r\nINFO: Found applicable config definition build:xla in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:tensorrt in file /opt/tensorflow/tensorflow-source/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file /opt/tensorflow/tensorflow-source/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosst\r\nool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:v2 in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition test:v2 in file /opt/tensorflow/tensorflow-source/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only --build_tag_filters=\r\n-benchmark-test,-no_oss,-no_gpu,-v1only\r\nINFO: Found applicable config definition build:cuda in file /opt/tensorflow/tensorflow-source/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosst\r\nool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /opt/tensorflow/tensorflow-source/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PR\r\nEFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: All specified test targets were excluded by filters\r\nINFO: Analyzed 0 targets (1 packages loaded, 0 targets configured).\r\nINFO: Found 0 test targets...\r\nINFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/sandbox\r\nINFO: Elapsed time: 1.741s, Critical Path: 0.01s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Build completed successfully, 1 total action\r\n```", "you can change \"test\" to \"build\" and then run the binary it produces. What I meant is that however you build and run the tests in that directory using cuda 11 + tensorrt, use the same way to run case_test. ", "@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted ? Thanks!", "Still investigating", "FYI Managed to reproduce ;) \r\n\r\nSaving this here:\r\n```bash\r\nbazel build tensorflow/python/compiler/tensorrt:cast_test --config=cuda --compilation_mode=opt\r\n\r\nTF_CPP_VMODULE=trt_logger=2,trt_engine_utils=2,trt_engine_op=2,convert_nodes=2,convert_graph=2,segment=2,trt_shape_optimization_profiles=2,convert_nodes_test=2 bazel-bin/tensorflow/python/compiler/tensorrt/cast_test\r\n\r\n[...]\r\n\r\n======================================================================\r\nFAIL: testTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration (__main__.CastFp32Fp16Test)\r\ntestTfTrt_OfflineConversion_DynamicEngine_FP16_NoCalibration (__main__.CastFp32Fp16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1399, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 112\r\n0, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 104\r\n2, in RunTest\r\n    self._VerifyGraphDef(run_params, saved_model_dir, infer_saved_model_dir,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 922\r\n, in _VerifyGraphDef\r\n    self._VerifyGraphDefV1(run_params, original_gdef, gdef_to_verify,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 848\r\n, in _VerifyGraphDefV1\r\n    self._VerifyConnections(expected_engines, original_gdef, gdef_to_verify)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 664\r\n, in _VerifyConnections\r\n    self.assertEqual(\r\nAssertionError: {'TRTEngineOp_0': {'cast_fp16_to_fp32_1', 'input_0'}, 'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_0'}} != {'TRTEngineOp_0': {'input_0'}, 'input_0':\r\n set(), 'output_0': {'TRTEngineOp_0'}} (\r\nexpected:\r\n[('TRTEngineOp_0', {'cast_fp16_to_fp32_1', 'input_0'}), ('cast_fp16_to_fp32_1', {'TRTEngineOp_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'input_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})])\r\nrepr() of differing entries:\r\n'TRTEngineOp_0': {'cast_fp16_to_fp32_1', 'input_0'} != {'input_0'}\r\n\r\nMissing entries:\r\n'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}\r\n\r\n======================================================================\r\nFAIL: testTfTrt_OfflineConversion_DynamicEngine_FP32_NoCalibration (__main__.CastFp32Fp16Test)\r\ntestTfTrt_OfflineConversion_DynamicEngine_FP32_NoCalibration (__main__.CastFp32Fp16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1399, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 112\r\n0, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 104\r\n2, in RunTest\r\n    self._VerifyGraphDef(run_params, saved_model_dir, infer_saved_model_dir,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 922\r\n, in _VerifyGraphDef\r\n    self._VerifyGraphDefV1(run_params, original_gdef, gdef_to_verify,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 848\r\n, in _VerifyGraphDefV1\r\n    self._VerifyConnections(expected_engines, original_gdef, gdef_to_verify)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 664\r\n, in _VerifyConnections\r\n    self.assertEqual(\r\nAssertionError: {'TRTEngineOp_0': {'cast_fp16_to_fp32'}, 'TRTEngineOp_1': {'cast_fp32_to_fp16'}, 'TRTEngineOp_2': {'cast_fp16_to_fp32_1'}, 'cast_fp16_to_fp32': {'input_0'}, 'cast_fp16_to_fp32_1': {'TRTEngi\r\nneOp_1'}, 'cast_fp32_to_fp16': {'TRTEngineOp_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_2'}} != {'TRTEngineOp_0': {'cast_fp32_to_fp16'}, 'TRTEngineOp_1': {'cast_fp16_to_fp32_1'}, 'TRTEngineOp_2': {'c\r\nast_fp16_to_fp32'}, 'cast_fp16_to_fp32': {'input_0'}, 'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}, 'cast_fp32_to_fp16': {'TRTEngineOp_2'}, 'input_0': set(), 'output_0': {'TRTEngineOp_1'}} (\r\nexpected:\r\n[('TRTEngineOp_0', {'cast_fp16_to_fp32'}), ('TRTEngineOp_1', {'cast_fp32_to_fp16'}), ('TRTEngineOp_2', {'cast_fp16_to_fp32_1'}), ('cast_fp16_to_fp32', {'input_0'}), ('cast_fp16_to_fp32_1', {'TRTEngineOp_1'\r\n}), ('cast_fp32_to_fp16', {'TRTEngineOp_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_2'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'cast_fp32_to_fp16'}), ('TRTEngineOp_1', {'cast_fp16_to_fp32_1'}), ('TRTEngineOp_2', {'cast_fp16_to_fp32'}), ('cast_fp16_to_fp32', {'input_0'}), ('cast_fp16_to_fp32_1', {'TRTEngineOp_0'\r\n}), ('cast_fp32_to_fp16', {'TRTEngineOp_2'}), ('input_0', set()), ('output_0', {'TRTEngineOp_1'})])\r\nrepr() of differing entries:\r\n'TRTEngineOp_0': {'cast_fp16_to_fp32'} != {'cast_fp32_to_fp16'}\r\n'TRTEngineOp_1': {'cast_fp32_to_fp16'} != {'cast_fp16_to_fp32_1'}\r\n'TRTEngineOp_2': {'cast_fp16_to_fp32_1'} != {'cast_fp16_to_fp32'}\r\n'cast_fp16_to_fp32_1': {'TRTEngineOp_1'} != {'TRTEngineOp_0'}\r\n'cast_fp32_to_fp16': {'TRTEngineOp_0'} != {'TRTEngineOp_2'}\r\n'output_0': {'TRTEngineOp_2'} != {'TRTEngineOp_1'}\r\n\r\n======================================================================\r\nFAIL: testTfTrt_OfflineConversion_DynamicEngine_INT8_UseCalibration (__main__.CastFp32Fp16Test)\r\ntestTfTrt_OfflineConversion_DynamicEngine_INT8_UseCalibration (__main__.CastFp32Fp16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1399, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 112\r\n0, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 103\r\n6, in RunTest\r\n    infer_saved_model_dir = self._GetCalibratedInferGraph(\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 486\r\n, in _GetCalibratedInferGraph\r\n    self._VerifyGraphDef(run_params, saved_model_dir, int8_gdef,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 922\r\n, in _VerifyGraphDef\r\n    self._VerifyGraphDefV1(run_params, original_gdef, gdef_to_verify,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 848\r\n, in _VerifyGraphDefV1\r\n    self._VerifyConnections(expected_engines, original_gdef, gdef_to_verify)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 664\r\n, in _VerifyConnections\r\n    self.assertEqual(\r\nAssertionError: {'TRTEngineOp_0': {'cast_fp16_to_fp32_1', 'input_0'}, 'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_0'}} != {'TRTEngineOp_0': {'input_0'}, 'input_0':\r\n set(), 'output_0': {'TRTEngineOp_0'}} (\r\nexpected:\r\n[('TRTEngineOp_0', {'cast_fp16_to_fp32_1', 'input_0'}), ('cast_fp16_to_fp32_1', {'TRTEngineOp_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'input_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})])\r\nrepr() of differing entries:\r\n'TRTEngineOp_0': {'cast_fp16_to_fp32_1', 'input_0'} != {'input_0'}\r\n\r\nMissing entries:\r\n'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}\r\n\r\n======================================================================\r\nFAIL: testTfTrt_OfflineConversion_StaticEngine_FP16_NoCalibration (__main__.CastFp32Fp16Test)\r\ntestTfTrt_OfflineConversion_StaticEngine_FP16_NoCalibration (__main__.CastFp32Fp16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1399, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 112\r\n0, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 104\r\n2, in RunTest\r\n    self._VerifyGraphDef(run_params, saved_model_dir, infer_saved_model_dir,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 922\r\n, in _VerifyGraphDef\r\n    self._VerifyGraphDefV1(run_params, original_gdef, gdef_to_verify,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 848\r\n, in _VerifyGraphDefV1\r\n    self._VerifyConnections(expected_engines, original_gdef, gdef_to_verify)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 664\r\n, in _VerifyConnections\r\n    self.assertEqual(\r\nAssertionError: {'TRTEngineOp_0': {'cast_fp16_to_fp32_1', 'input_0'}, 'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_0'}} != {'TRTEngineOp_0': {'input_0'}, 'input_0':\r\n set(), 'output_0': {'TRTEngineOp_0'}} (\r\nexpected:\r\n[('TRTEngineOp_0', {'cast_fp16_to_fp32_1', 'input_0'}), ('cast_fp16_to_fp32_1', {'TRTEngineOp_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'input_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_0'})])\r\nrepr() of differing entries:\r\n'TRTEngineOp_0': {'cast_fp16_to_fp32_1', 'input_0'} != {'input_0'}\r\n\r\nMissing entries:\r\n'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}\r\n\r\n======================================================================\r\nFAIL: testTfTrt_OfflineConversion_StaticEngine_FP32_NoCalibration (__main__.CastFp32Fp16Test)\r\ntestTfTrt_OfflineConversion_StaticEngine_FP32_NoCalibration (__main__.CastFp32Fp16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1399, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 112\r\n0, in _Test\r\n    self.RunTest(run_params)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 104\r\n2, in RunTest\r\n    self._VerifyGraphDef(run_params, saved_model_dir, infer_saved_model_dir,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 922\r\n, in _VerifyGraphDef\r\n    self._VerifyGraphDefV1(run_params, original_gdef, gdef_to_verify,\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 848\r\n, in _VerifyGraphDefV1\r\n    self._VerifyConnections(expected_engines, original_gdef, gdef_to_verify)\r\n  File \"/opt/tensorflow/tensorflow-source/bazel-bin/tensorflow/python/compiler/tensorrt/cast_test.runfiles/org_tensorflow/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py\", line 664\r\n, in _VerifyConnections\r\n    self.assertEqual(\r\nAssertionError: {'TRTEngineOp_0': {'cast_fp16_to_fp32'}, 'TRTEngineOp_1': {'cast_fp32_to_fp16'}, 'TRTEngineOp_2': {'cast_fp16_to_fp32_1'}, 'cast_fp16_to_fp32': {'input_0'}, 'cast_fp16_to_fp32_1': {'TRTEngi\r\nneOp_1'}, 'cast_fp32_to_fp16': {'TRTEngineOp_0'}, 'input_0': set(), 'output_0': {'TRTEngineOp_2'}} != {'TRTEngineOp_0': {'cast_fp32_to_fp16'}, 'TRTEngineOp_1': {'cast_fp16_to_fp32_1'}, 'TRTEngineOp_2': {'c\r\nast_fp16_to_fp32'}, 'cast_fp16_to_fp32': {'input_0'}, 'cast_fp16_to_fp32_1': {'TRTEngineOp_0'}, 'cast_fp32_to_fp16': {'TRTEngineOp_2'}, 'input_0': set(), 'output_0': {'TRTEngineOp_1'}} (\r\nexpected:\r\n[('TRTEngineOp_0', {'cast_fp16_to_fp32'}), ('TRTEngineOp_1', {'cast_fp32_to_fp16'}), ('TRTEngineOp_2', {'cast_fp16_to_fp32_1'}), ('cast_fp16_to_fp32', {'input_0'}), ('cast_fp16_to_fp32_1', {'TRTEngineOp_1'\r\n}), ('cast_fp32_to_fp16', {'TRTEngineOp_0'}), ('input_0', set()), ('output_0', {'TRTEngineOp_2'})]\r\nvs actual:\r\n[('TRTEngineOp_0', {'cast_fp32_to_fp16'}), ('TRTEngineOp_1', {'cast_fp16_to_fp32_1'}), ('TRTEngineOp_2', {'cast_fp16_to_fp32'}), ('cast_fp16_to_fp32', {'input_0'}), ('cast_fp16_to_fp32_1', {'TRTEngineOp_0'\r\n}), ('cast_fp32_to_fp16', {'TRTEngineOp_2'}), ('input_0', set()), ('output_0', {'TRTEngineOp_1'})])\r\nrepr() of differing entries:\r\n'TRTEngineOp_0': {'cast_fp16_to_fp32'} != {'cast_fp32_to_fp16'}\r\n'TRTEngineOp_1': {'cast_fp32_to_fp16'} != {'cast_fp16_to_fp32_1'}\r\n'TRTEngineOp_2': {'cast_fp16_to_fp32_1'} != {'cast_fp16_to_fp32'}\r\n'cast_fp16_to_fp32_1': {'TRTEngineOp_1'} != {'TRTEngineOp_0'}\r\n'cast_fp32_to_fp16': {'TRTEngineOp_0'} != {'TRTEngineOp_2'}\r\n'output_0': {'TRTEngineOp_2'} != {'TRTEngineOp_1'}\r\n```", "@bixia1 I found the pattern, looks like all Jobs with these characteristics tend to fails:\r\n- `testTfTrt` (using `testTfTrtV2` is fine)\r\n- `StaticEngine` or `DynamicEngine` => Essentially it doesn't matter\r\n- All `OfflineConversion`\r\n\r\nWhat is different with `testTfTrt` vs `testTfTrtV2` ? I'm not sure where to start looking at this. Essentially all the tests are passing for `V2`. So if we find the problem with \"`V1`\" we close the case and can merge. Any idea ? I gotta say I stall a little ...", "The test with V2 suffix is run with --test_env=TF2_BEHAVIOR=1, you can also see this from the code that generates the V2 suffix here https://github.com/tensorflow/tensorflow/blob/4e6fbd12cb927f2b481eca74db767268951c206d/tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py#L1109.\r\n\r\nAnother way to compare is that CastFp32Fp16Test.testTfTrt_OfflineConversion_DynamicEngine_FP32_NoCalibration\r\n failed but the \"online\" version of the same \"test\", CastFp32Fp16Test.testTfTrt_OnlineConversion_DynamicEngine_FP16_NoCalibration,  passed.\r\n\r\nAlso, the reason for the failure is that the expected engine is NOT built. Tracking down the reason for this may help also.\r\n", "@bixia1 I pushed the working version of the test, it does not run INT8 Calibration as we agreed on.\r\nI left an env-var: `TFTRT_CAST_PYTEST_ALLOW_SEGFAULT=1` that you can define if you want to reproduce the bug to investigate.\r\n\r\nMy feeling is that the mistake is probably on our side, not on TRT/cuDNN side (the C++ unittests are passing in all precisions)\r\n\r\nLet me know if I can do anything else.", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@DEKHTIARJonathan  Any update on this PR? Please. Thanks!", "Still under work", "Still actively being worked on", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 43899, "title": "Fix typo in benchmark documentation iOS subtitle", "body": "", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac"]}]