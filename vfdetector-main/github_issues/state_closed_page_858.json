[{"number": 27774, "title": "reduce indent to display as regular text", "body": "reduce indent to display as regular text", "comments": ["Added another commit to change indentation in additional lines"]}, {"number": 27773, "title": "set indent to display as list", "body": "set indent to display as list", "comments": ["Could you pull rebase and push again?", "@drpngx done -- pull rebase and push again"]}, {"number": 27772, "title": "[Intel MKL] update mkl addn op for multi-input", "body": "", "comments": ["@penpornk I have update my code based on your comments, please help to review it again, thanks."]}, {"number": 27771, "title": "add back-ticks for parameter formatting", "body": "add back-ticks for parameter formatting", "comments": ["@rryan Hi, Could you PTAL and approve.", "@rryan Hi, Could you PTAL and approve."]}, {"number": 27770, "title": "AttributeError: module 'tensorflow._api.v1.nn' has no attribute 'softmax_cross_entryopy_with_logits_v2'", "body": "**System info**\r\n```\r\ngeoff@ubuntu:~$ uname -a\r\nLinux ubuntu 4.15.0-47-generic #50-Ubuntu SMP Wed Mar 13 10:44:52 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\ngeoff@ubuntu:~/work/interview/coursera_dl$ python --version\r\nPython 3.6.6 :: Anaconda, Inc.\r\ngeoff@ubuntu:~$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" \r\nb'v1.13.1-0-g6612da8951' 1.13.1\r\ngeoff@ubuntu:~/work/interview/coursera_dl$ python -c \"import keras; print(keras.__version__)\"\r\nUsing TensorFlow backend.\r\n2.2.4\r\n```\r\n\r\n**Describe Issue**\r\nNo luck getting this to work so far, and I can't find any similar issues.\r\n\r\n**Reproduce issue**\r\n\r\n[reproduce.txt](https://github.com/tensorflow/tensorflow/files/3071694/reproduce.txt)\r\n\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3071647/tf_env.txt)\r\n", "comments": ["same issue is there for log_softmax_v2", "Apologies for the delay in response.\r\nI believe there is typo error at line 96 as following:\r\n```python\r\n---> 96    softmax = tf.nn.softmax_cross_entryopy_with_logits_v2(logits=Z3, labels=Y)\r\n```\r\nYou can change it to following;\r\n```python\r\nsoftmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=Y)\r\n```\r\nOutput:\r\n```\r\ncost = 4.6648703\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27770\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27770\">No</a>\n"]}, {"number": 27769, "title": "[TF 2.0 keras] Unable save and load weights for double nested models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nload_weights throw exception on a doubly nested model\r\n\r\n**Describe the expected behavior**\r\nload_weights should work\r\n\r\nThis problem only happens on two+ layers of nested model with non-trainable weights.\r\nThe reason is save_weights and load_weights handles nested model differently\r\nsave_weights -> call layer.weights for each layer\r\nload_weights -> recursively call model.weights if layer is a nested Model\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization\r\n\r\nshape = (None, None, 3)\r\n\r\ndef BNModel():\r\n    x = inputs = Input(shape)\r\n    x = Conv2D(3, 1)(x)\r\n    x = BatchNormalization()(x)\r\n    return Model(inputs, x)\r\n\r\nx = inner_inputs = Input(shape)\r\nx = BNModel()(x)\r\nx = BNModel()(x)\r\ninner_model = Model(inner_inputs, x)\r\n\r\ninputs = Input(shape)\r\nmodel = Model(inputs, inner_model(inputs))\r\n\r\ninner_model.save_weights('test.h5')\r\ninner_model.load_weights('test.h5')  # works fine\r\n\r\nmodel.save_weights('test.h5')\r\nmodel.load_weights('test.h5')   # Exception: axes don't match array !!!\r\n```\r\n\r\n**Other info / logs**\r\nThis bug is also reported on upstream keras https://github.com/keras-team/keras/pull/11847\r\nHere is a detailed analysis on why this is happening https://github.com/keras-team/keras/pull/11847#issuecomment-482438283\r\n\r\nFull Exception\r\n```\r\n  File \"test.py\", line 27, in <module>\r\n    model.load_weights('test.h5')   # Exception: axes don't match array !!!\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1497, in load_weights\r\n    hdf5_format.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 751, in load_weights_from_hdf5_group\r\n    layer, weight_values, original_keras_version, original_backend)\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 377, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 365, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 377, in preprocess_weights_for_loading\r\n    weights = convert_nested_model(weights)\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 353, in convert_nested_model\r\n    original_backend=original_backend))\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 459, in preprocess_weights_for_loading\r\n    weights[0] = np.transpose(weights[0], (3, 2, 0, 1))\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 598, in transpose\r\n    return _wrapfunc(a, 'transpose', axes)\r\n  File \"/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\", line 51, in _wrapfunc\r\n    return getattr(obj, method)(*args, **kwds)\r\nValueError: axes don't match array\r\n```", "comments": ["This only affected .h5 format, tensorflow checkpoints format works fine.\r\nI guess alternatively we can tell users to not use h5 format instead of fixing it", "@zzh8829  What is the alternative way to save a model/weights? I am having this proble min .hdf5 fromat too.", "@abhigyank the alternative is save to `*.tf` which will create tensorflow checkpoint files instead of hdf5.", "Any news on this issue?\r\n\r\nI tried the *.tf and it works.", "It might seem like .tf saving works but in my experience the only difference is that it doesn't throw an error. \r\nSteps to reproduce:\r\nMake a model with nested models and set some layers to trainable=False\r\nTrain for some epochs\r\nSave weights\r\nEvaluate and save metrics\r\nClear everything\r\nMake model\r\nLoad weights\r\nEvaluate", "I am currently submitting a fix for H5. \r\n\r\n@veqtor What problem are you seeing with using the TF format?", "@k-w-w I have tested your fix and it works for me \ud83d\ude03 Thank you a lot!", "@k-w-w How can I use your fix? I have the same problem.", "@19giorgosts The fix should be in tensorflow-nightly, which you can install using `pip install tf-nightly`", "> It might seem like .tf saving works but in my experience the only difference is that it doesn't throw an error.\r\n> Steps to reproduce:\r\n> Make a model with nested models and set some layers to trainable=False\r\n> Train for some epochs\r\n> Save weights\r\n> Evaluate and save metrics\r\n> Clear everything\r\n> Make model\r\n> Load weights\r\n> Evaluate\r\n\r\nI am new coder to keras\u3002 Can you show me a demo about your description?\r\nThx", "@Lannist [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/0153524fb3f6e0b114ace9da25ac3f77/tf_27769_saveweights_tfformat.ipynb) is the colab gist to save/load the weights in *.tf format. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/48daec236b2c8a5c8cf7a482f258ee8a/tf_27769_saveweights_h5format.ipynb) to save/load the weights in *.h5 format. The only difference between those two gist is in changing the extension. Thanks!\r\n\r\nI am closing the issue as it was resolved in `tf-nightly`. Please feel free to open if the issue persists again. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27769\">No</a>\n", "is this change gonna be in tf 1 ? ", "> is this change gonna be in tf 1 ?\r\n\r\nhave you found the solution?\r\nI using the tensorflow 1.1.4 and meet the same error but can not find way to fix it ", "> @19giorgosts The fix should be in tensorflow-nightly, which you can install using `pip install tf-nightly`\r\n\r\nhow about tensorflow 1.1.4 or 1.1.5\r\ncan not install tensorflow-nightly by pip ", "> I am currently submitting a fix for H5.\r\n> \r\n> @veqtor What problem are you seeing with using the TF format?\r\n\r\nThat didn't work for me, using that fix in tf-nightly, for a siamese model such as: \r\n\r\n```\r\nimport os\r\nfrom typing import Optional\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.applications import EfficientNetB0, ResNet50\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\ndef l1_distance(vects) -> float:\r\n    \"\"\"\r\n    Finds the L1 distance between two vectors.\r\n\r\n    Args:\r\n        vects: List containing two tensors of same length.\r\n\r\n    Returns:\r\n        Element-wise L1 distance.\r\n    \"\"\"\r\n    x, y = vects\r\n\r\n    return K.abs(x - y)\r\n\r\ndef create_model(\r\n    target_shape = (224, 224, 3),\r\n    path = None,\r\n) -> Model:\r\n    \"\"\"\r\n    Creates the siamese model.\r\n\r\n    Args:\r\n        target_shape: image dimensions.\r\n        path: path to best weights.\r\n\r\n    Returns:\r\n        Siamese model.\r\n    \"\"\"\r\n    input_1 = layers.Input(shape=target_shape, name=\"inp_1\")\r\n    input_2 = layers.Input(shape=target_shape, name=\"inp_2\")\r\n    # input_1aug = img_augmentation(input_1)\r\n    # input_2aug = img_augmentation(input_2)\r\n\r\n    input = layers.Input(shape=target_shape, name=\"input\")\r\n    lambda_1 = layers.Lambda(\r\n        lambda image: tf.keras.applications.resnet.preprocess_input(image),\r\n        name=\"pre_process\",\r\n    )(input)\r\n    base_cnn = ResNet50(\r\n        weights=\"imagenet\",\r\n        input_tensor=lambda_1,\r\n        input_shape=target_shape,\r\n        include_top=False,\r\n    )\r\n    # CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\r\n    pool = layers.MaxPooling2D(pool_size=(2, 2))(base_cnn.output)\r\n    flatten = layers.Flatten(name=\"base_output_flatten\")(pool)\r\n    dense1 = layers.BatchNormalization(name=\"dense1_norm\")(flatten)\r\n    dense1 = layers.Dense(512, activation=\"relu\", name=\"dense1\")(dense1)\r\n    dense1 = layers.Dropout(0.3, name=\"dense1_dropout\")(dense1)\r\n    dense2 = layers.BatchNormalization(name=\"dense2_norm\")(dense1)\r\n    dense2 = layers.Dense(256, activation=\"relu\", name=\"dense2\")(dense2)\r\n    dense2 = layers.Dropout(0.2, name=\"dense2_dropout\")(dense2)\r\n    output = layers.Dense(256, name=\"dense_output\")(dense2)\r\n\r\n    embedding = Model(input, output, name=\"Embedding\")\r\n\r\n    trainable = False\r\n    for layer in base_cnn.layers:\r\n        if layer.name == \"conv5_block1_out\":\r\n            trainable = True\r\n        layer.trainable = trainable\r\n\r\n    tower_1 = embedding(input_1)\r\n    tower_2 = embedding(input_2)\r\n\r\n    merge_layer = layers.Lambda(l1_distance, name=\"l1\")([tower_1, tower_2])\r\n    normal_layer = tf.keras.layers.BatchNormalization(name=\"l1_norm\")(merge_layer)\r\n    comparison_layer = layers.Dense(\r\n        1,\r\n        activation=\"sigmoid\",\r\n        name=\"final_layer\",\r\n    )(normal_layer)\r\n    siamese = Model(inputs=[input_1, input_2], outputs=comparison_layer)\r\n\r\n    if path is not None:\r\n        siamese.load_weights(path)\r\n\r\n    return siamese\r\n\r\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(\r\n        monitor=\"loss\", patience=5\r\n    )\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n        log_dir=\"/logs\", histogram_freq=1\r\n    )\r\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n        filepath=\"/logs/weights{epoch:04d}.tf\", save_weights_only=True, save_freq=1\r\n    )\r\n\r\ntrain_generator = get_train_generator(\r\n        split_path, batch_size=batch_size, input_size=target_shape\r\n    )\r\nsteps_per_epoch = len(train_generator)\r\nclr = get_cyclical_lr(2 * steps_per_epoch)\r\noptimizer = Adam(clr)\r\n\r\nsiamese = create_model(target_shape)\r\n\r\nsiamese.compile(\r\n        loss=loss(margin=margin),\r\n        optimizer=optimizer,\r\n        metrics=[metrics.accuracy, metrics.precision, metrics.recall, metrics.f1],\r\n    )\r\nsiamese.summary()\r\n\r\nsiamese.fit(\r\n        train_generator,\r\n        validation_data=get_valid_generator(\r\n            split_path, batch_size=batch_size, input_size=target_shape\r\n        ),\r\n        epochs=epochs,\r\n        callbacks=[\r\n            early_stopping_callback,\r\n            tensorboard_callback,\r\n            model_checkpoint_callback,\r\n        ],\r\n        verbose=1,\r\n    )\r\n\r\nsiamese = create_model(path=\"/content/weights00000012.h5\")\r\n```\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-e1ec7f0dd441> in <module>()\r\n----> 1 siamese = create_model(path=\"/content/weights00000012.h5\")\r\n\r\n3 frames\r\n<__array_function__ internals> in transpose(*args, **kwargs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)\r\n     56 \r\n     57     try:\r\n---> 58         return bound(*args, **kwds)\r\n     59     except TypeError:\r\n     60         # A TypeError occurs if the object does have such a method in its\r\n\r\nValueError: axes don't match array\r\n```\r\n"]}, {"number": 27768, "title": "Added GELU function into Keras activations", "body": "", "comments": ["@fchollet Hi, Could you PTAL and approve.", "Thank you for the PR.\r\n\r\nOur guidelines for adding new symbols in the Keras API are the following:\r\n\r\n\r\n- It should be broadly useful to our users, rather than a niche feature that is only relevant to 1-2 teams at Google or to a specific vertical of researchers. Niche features should be maintained independently by those who need them (e.g. by extending the API via subclassing), as third-party add-on packages.\r\n- It should be widely recognized as a machine learning best practice. We will not add new layers/etc that were recently published to ArXiv.org, even in case of claims of increased accuracy/etc. We only add new objects that are already commonly used in the machine learning community. Presumably, a new technique that does result in meaningful gains would be broadly adopted after a few months anyway (like ResNet), and that\u2019s when we would be adding it to the core API. SIG-addons maintains a repository of significantly more volatile and independently maintained code to which the barriers to entry are lower.\r\n- It should have an owner committed to maintaining it in the long term. In particular, the code should be maintainable by multiple people on the team, not just by one technical guru.\r\n\r\n[source](https://docs.google.com/document/d/1uzWNPseXHywqiRyjvazY6eaxe23BEG2T82JasndCJjI/)\r\n\r\nThe symbol being considered here does not represent a current deep learning best practice, and as such we will not add it. It falls under \"We will not add new layers/etc that were recently published to ArXiv.org, even in case of claims of increased accuracy/etc. We only add new objects that are already commonly used in the machine learning community\".\r\n\r\nConsider adding it to TensorFlow Add-Ons instead."]}, {"number": 27767, "title": "Make link clickable", "body": "Make link clickable", "comments": []}, {"number": 27766, "title": "which version of bazel should I used to build the tensorflow-2.0.0-alpha0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow version: tensorflow-2.0.0-alpha0\r\n- Python version: python 3.5.2\r\n- Bazel version: 0.22.0\r\n- GCC/Compiler version: 5.4.0\r\n- CUDA/cuDNN version: no \r\n- GPU model and memory: no \r\n\r\n\r\nI want to build tensorflow-2.0.0-alpha0 by source and used the bazel with version 0.22.0, but bazel cannot recognize the BUILD file:\r\n\r\n```\r\nroot@XX-Latitude-E5470:~/share/project/tensorflow-2.0.0-alpha0# bazel build --config=opt //tensorflow/tools/pip-package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Repository rule 'build_bazel_rules_swift' returned: {\"remote\": \"https://github.com/bazelbuild/rules_swift.git\", \"commit\": \"001736d056d7eae20f1f4da41bc9e6f036857296\", \"shallow_since\": \"2019-01-18\", \"init_submodules\": False, \"verbose\": False, \"strip_prefix\": \"\", \"patches\": [], \"patch_tool\": \"patch\", \"patch_args\": [\"-p0\"], \"patch_cmds\": [], \"name\": \"build_bazel_rules_swift\"}\r\nDEBUG: /home/alcht0/.cache/bazel/_bazel_root/2c79635619ee2e9671f2cc31f8facdc4/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: \r\nWARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.\r\n\r\nERROR: Skipping '//tensorflow/tools/pip-package:build_pip_package': no such package 'tensorflow/tools/pip-package': BUILD file not found on package path\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package 'tensorflow/tools/pip-package': BUILD file not found on package path\r\nINFO: Elapsed time: 4.753s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\nit is the same with bazel version 0.19.2.\r\nso I don't know which bazel should I used to build the tensorflow-2.0.0-alpha0.\r\n\r\n\r\nI installed bazel by \r\n```\r\n./bazel-0.22.0-installer-linux-x86_64.sh\r\n```\r\nand switch the version also by the `.sh` cmd, I don't know if the issue related to it.\r\n\r\nBest regards.\r\n", "comments": ["It's OK in another PC, there must be something wrong with my bazel environment when I switched its version.\r\n"]}, {"number": 27765, "title": "Unable to convert fine-tuned model to tflite model", "body": "**System information**\r\n- Linux Ubuntu 18.04:\r\n- pip:\r\n- tensorflow==1.13.1:\r\n\r\n**CMD**\r\n\r\n```bash\r\nbazel-bin/tensorflow/lite/toco/toco \\\r\n--input_file=../output/ner_model.pb \\\r\n--output_file=../output/ner_model.tflite \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--input_shapes=1,512:1,512 \\\r\n--input_arrays=input_ids,input_mask \\\r\n--output_arrays=pred_ids \\\r\n--default_ranges_min=0 --default_ranges_max=512 \\\r\n--allow_custom_ops\r\n```\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nConstant array bert/embeddings/word_embeddings lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\nUnimplemented: this graph contains an operator of type OneHot for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\n```bash\r\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph \\\r\n--in_graph=../output/ner_model.pb\r\n```\r\n\r\n**output**\r\n\r\n```\r\nFound 2 possible inputs: (name=input_ids, type=int32(3), shape=[?,512]) (name=input_mask, type=int32(3), shape=[?,512]) \r\nNo variables spotted.\r\nFound 1 possible outputs: (name=pred_ids, op=Identity) \r\nFound 101686282 (101.69M) const parameters, 0 (0) variable parameters, and 18 control_edges\r\nOp types used: 683 Const, 214 Identity, 136 Mul, 106 Add, 74 MatMul, 73 BiasAdd, 57 Reshape, 57 Pack, 52 Transpose, 50 Mean, 38 Sub, 25 StopGradient, 25 SquaredDifference, 25 Rsqrt, 24 BatchMatMul, 23 Enter, 19 ExpandDims, 16 Shape, 14 StridedSlice, 12 Sqrt, 12 Softmax, 12 RealDiv, 12 Erf, 9 Range, 8 Switch, 8 NextIteration, 8 Merge, 7 ConcatV2, 4 TensorArrayV3, 4 Select, 4 Fill, 4 Less, 3 Assert, 3 Exit, 3 Slice, 3 Squeeze, 3 All, 3 Cast, 3 Max, 3 Maximum, 2 TensorArraySizeV3, 2 TensorArrayScatterV3, 2 TensorArrayReadV3, 2 ReverseSequence, 2 TensorArrayGatherV3, 2 TensorArrayWriteV3, 2 ArgMax, 2 Equal, 2 Placeholder, 2 Minimum, 2 LoopCond, 2 LogicalAnd, 2 GreaterEqual, 1 Sign, 1 OneHot, 1 Sum, 1 Abs, 1 Tanh, 1 LessEqual, 1 GatherV2, 1 GatherNd\r\n```\r\n", "comments": ["@Single430 , try to provide --mean_values & --std_dev_values along with QUANTIZED_UINT8 and let me know if you succeed.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78  Thank you very much for your reply, but how do I fill in the values of --mean_values and --std_values?\r\n", "@Single430 for starter you can fill below values \r\n--mean_values=128 \r\n--std_dev_values=127\r\n\r\nIf it works you can read the documentation for more details\r\n\r\nRegards\r\nAmit", "`ERROR:  2019-04-25 09:43:00.214538: F tensorflow/lite/toco/model_cmdline_flags.cc:263] Check failed: mean_values.size() == model_flags->input_arrays_size()`\r\n\r\nI don't know how to solve it. @amitsrivastava78 \u3000\u3000thanks you.", "@Single430 , sorry for the oversight, i just mixed up the issues, I saw your log and it clearly says following : - \r\n\r\n _Unimplemented: this graph contains an operator of type OneHot for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op)._\r\n\r\n\r\nProblem is quantize version of onehot vector is not supported by TFlite, \r\n\r\n@suharshs  whats your take on this ? Shall i go ahead and raise the PR for the same ?\r\n\r\n@Single430 : - Will take the further action once @suharshs gives a go.\r\n\r\nRegards\r\nAmit \r\n\r\n\r\n\r\n\r\n", "@Single430 , i think @suharshs is yet to respond in the mean time, can you upload the model file which has this quantized version of OneHot, I will try to verify my implementation against this model and send you the PR which can solve this problem.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78     I uploaded the pb file, please let me know if you need other files.  \r\n This is a model for Chinese entity recognition.\r\nhttps://drive.google.com/file/d/1vZNpt5tcqfivQFRfLrtApMwIoURxmYCy/view?usp=sharing\r\n\r\nThanks very much.\r\nsuper.single430", "thanks, yes sg to go ahead with a onehot impl. in addition to the kernel, you will likely need to add new hardcode rule in this file:\r\nhttps://github.com/rockchip-linux/tensorflow/blob/master/tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc\r\nSince the range of onehot should be hardcoded [0, 1].\r\n\r\nThanks!", "@suharshs , thanks for the response and for pointing me into the right direction, i have raised the PR for the same. Kindly review.\r\n\r\n@Single430 , it might take some time for the PR to merge, till then use this for your testing and post the results here.\r\n\r\nRegards\r\nAmit", "Please test with latest version of TF and reopen the issue if it still exists. Thank you.", "@Single430  hello, how did you solve the problem of unsupported op BatchMatMul?"]}, {"number": 27764, "title": "TFTRT: Allow FP16 inputs and outputs. ", "body": "This change allows models trained in FP16 precision to be imported using TF-TRT.\r\nAlso remove unnecessary conversions of weights from FP32 to FP16 because TRT does this automatically.", "comments": []}, {"number": 27763, "title": "after freezing quantization aware training model , batchnorm create size node, cast node, and it's not supported to convert to tflite by tflite_convert", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):nightly(1.14)\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.2, cudnn7\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nafter freezing quantization aware training model, batchnorm create some new nodes, called size and cast, and they are not supported to convert to tllite by tflite_convert\r\n\r\n2019-04-12 08:58:02.410692: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-04-12 08:58:02.430576: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2019-04-12 08:58:02.431034: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55678114f760 executing computations on platform Host. Devices:\r\n2019-04-12 08:58:02.431048: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-04-12 08:58:02.648299: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-04-12 08:58:02.648376: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-04-12 08:58:02.902493: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:684] Optimization results for grappler item: graph_to_optimize\r\n2019-04-12 08:58:02.902517: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 1684 nodes (-478), 1984 edges (-478), time = 14.17ms.\r\n2019-04-12 08:58:02.902521: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 1684 nodes (0), 1984 edges (0), time = 2.849ms.\r\n2019-04-12 08:58:02.902525: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1684 nodes (0), 1984 edges (0), time = 2.628ms.\r\n2019-04-12 08:58:02.902527: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 1538 nodes (-146), 1834 edges (-150), time = 73.3ms.\r\n2019-04-12 08:58:02.902530: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 1538 nodes (0), 1834 edges (0), time = 3.219ms.\r\n2019-04-12 08:58:02.902533: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 1085 nodes (-453), 1834 edges (0), time = 28.22ms.\r\n2019-04-12 08:58:02.902536: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   loop_optimizer: Graph size after: 1085 nodes (0), 1834 edges (0), time = 4.45ms.\r\n2019-04-12 08:58:02.902539: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 1081 nodes (-4), 1826 edges (-8), time = 9.244ms.\r\n2019-04-12 08:58:02.902542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   memory_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 30.734ms.\r\n2019-04-12 08:58:02.902545: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   model_pruner: Graph size after: 1081 nodes (0), 1826 edges (0), time = 4.841ms.\r\n2019-04-12 08:58:02.902549: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   implementation_selector: Graph size after: 1081 nodes (0), 1826 edges (0), time = 1.598ms.\r\n2019-04-12 08:58:02.902553: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   function_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 1.4ms.\r\n2019-04-12 08:58:02.902556: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   constant folding: Graph size after: 1081 nodes (0), 1826 edges (0), time = 16.989ms.\r\n2019-04-12 08:58:02.902559: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   shape_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 2.596ms.\r\n2019-04-12 08:58:02.902562: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   arithmetic_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 15.685ms.\r\n2019-04-12 08:58:02.902565: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:686]   dependency_optimizer: Graph size after: 1081 nodes (0), 1826 edges (0), time = 9.8ms.\r\nTraceback (most recent call last):\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 448, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 444, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 192, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 742, in convert\r\n    **converter_kwargs)\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 410, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 176, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-04-12 08:58:03.681724: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688089: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688112: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688225: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688236: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688242: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688274: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688282: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688287: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688338: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688345: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688351: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688521: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688530: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688535: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688587: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688595: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688600: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688631: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688638: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688643: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688708: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688713: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688745: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688752: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688758: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688795: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688802: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688808: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688838: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688846: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688851: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688900: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688907: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688913: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688942: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688950: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688955: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.688992: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.688999: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689005: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689034: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689042: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689047: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689083: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689091: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689096: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689126: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689135: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689141: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689197: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689209: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689222: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689265: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689272: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689281: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689334: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689342: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689347: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689378: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689386: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689391: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689429: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689436: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689441: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689472: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689479: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689485: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689521: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689529: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689534: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689564: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689572: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689577: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689611: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689618: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689624: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689666: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689674: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689680: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689710: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689717: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689723: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689759: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689766: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689772: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689802: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689809: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689814: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689850: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689858: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689863: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689893: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689901: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689906: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689942: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.689949: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.689955: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.690004: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Size\r\n2019-04-12 08:58:03.690012: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.690018: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\n2019-04-12 08:58:03.696569: F tensorflow/lite/toco/tooling_util.cc:1040] Check failed: array->has_shape() \r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f8353181740 (most recent call first):\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/syshang/anaconda3/envs/tfnightly0410/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["anyone can help?", "Did you get chance to look into some additional information in [FAQ](https://www.tensorflow.org/lite/guide/faq#why_are_some_operations_not_implemented_in_tensorflow_lite) or [experimental feature](https://www.tensorflow.org/lite/guide/ops_select). Let us know whether that helped or not.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27762, "title": "build pip package twice as a quick hack", "body": "", "comments": ["Discarding this"]}, {"number": 27761, "title": "Getting an \"Unimplemented\" error when using tensorflow-gpu built from source on TF Object Detection API model. ", "body": "My model is `faster_rcnn_inception_v2_coco` found within the TensorFlow object detection API located [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\r\n\r\nWhen I use `tensorflow-gpu` `1.13.1` and simply install it via Docker w/ the instructions found [here](https://www.tensorflow.org/install/docker#gpu_support) everything works perfectly upon inference. \r\n\r\n\r\n\r\nBut when I use the try to run `tensorflow-gpu` built from source (running TF `1.13.1`) built from source (via Docker w/ the instructions [here](https://www.tensorflow.org/install/source#gpu_support_2)) to take advantage of CPU optimizations I get hit with the following error upon attempting to do inference. \r\n\r\n\r\n```\r\n[evhttp_server.cc : 237] RAW: Entering the event loop ...\r\n2019-04-11 20:27:59.196073: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at conv_ops_fused_impl.h:648 : Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]\r\n2019-04-11 20:27:59.196141: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:637] Executor failed to create kernel. Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]\r\n\t [[{{node Conv/Relu6}}]]\r\n2019-04-11 20:28:01.013470: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at conv_ops_fused_impl.h:648 : Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]\r\n2019-04-11 20:28:01.013541: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:637] Executor failed to create kernel. Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]\r\n\t [[{{node Conv/Relu6}}]]\r\n2019-04-11 20:28:02.892400: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1408] OP_REQUIRES failed at conv_ops_fused_impl.h:648 : Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]\r\n2019-04-11 20:28:02.892476: E external/org_tensorflow/tensorflow/core/common_runtime/executor.cc:637] Executor failed to create kernel. Unimplemented: Fusion is not implemented: [BiasAdd,Relu6]\r\n\t [[{{node Conv/Relu6}}]]\r\n```\r\n\r\nThis is super odd, because things worked fine with tensorflow-gpu installed normally with Docker when doing inference. But, after building it from source, something breaks!\r\n\r\n", "comments": ["@farzaaCould you please provide the exact steps followed? In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. This is  the [link](https://www.tensorflow.org/install/source) for installation. Thanks!\r\n", "@farzaa Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27760, "title": "TFLite Toco unable to fully quantize (QUANTIZED_UINT8) graph with unpack/unstack", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Using Tensorflow in Anaconda\r\n- TensorFlow version (use command below): 1.12.0, 1.13.1, tf-nightly\r\n- Python version: 3.6.xxx\r\n\r\n**Describe the current behavior**\r\nI have a quantization-aware trained static rnn LSTM, with unstack. I used TFLiteLSTMCell defined in this file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/examples/lstm/rnn_cell.py\r\nI manually inserted fake quant nodes in the graph and generated a .pb file.\r\nWhen I want to convert my .pb file to .tflite model in toco with \"converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\"\r\nI got this error below:\r\nF tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:474] Unimplemented: this graph contains an operator of type Unpack for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\\nAborted (core dumped)\r\n\r\n**Describe the expected behavior**\r\nExpectation: The graph can be convert to .tflite file. \r\n\r\n**Code to reproduce the issue**\r\nI have my .pb file attached below:\r\n[my_frozen_graph.zip](https://github.com/tensorflow/tensorflow/files/3069858/my_frozen_graph.zip)\r\n\r\nMy toco code below to regenerate the error:\r\n`graph_def_file = \"my_frozen_graph.pb\"     # This is the .pb file.\r\ninput_arrays = [\"Reshape_1\"]        # This is the name of the input node\r\noutput_arrays = [\"labels_softmax\"]  # This is the name of the ouput node\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\nconverter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0] : (227., 0.92)}  # mean, std_dev\r\ntflite_model = converter.convert()\r\nopen(\"static_rnn_with_unstack.tflite\", \"wb\").write(tflite_model)   # The resulting .tflite file.`\r\n\r\n**Other info / logs**\r\nAs asked by @jdduke , I filled this issue. Thanks in advance.", "comments": ["@alanchiao any thoughts on adding this? Should be straightforward, and we already support pack.", "@jdduke , @alanchiao : I have uploaded PR for this issue, please check, Thanks!"]}, {"number": 27759, "title": "Issue in code of image captioning notebook", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: Any\r\n- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/image_captioning.ipynb\r\n\r\n\r\n**Describe the documentation issue**\r\nThe code in this [cell](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/image_captioning.ipynb#scrollTo=uEWM9xrYcg45) which sets `vocab_size = len(tokenizer.word_index) + 1` is wrong. `vocab_size` is set to `8235+1`, but actually needs to be set to `5000+1`, as `top_k = 5000`\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** Yes\r\n", "comments": ["The doc link you provided is incorrect. Can you please point to the correct one? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27758, "title": "Fix build error for tflite gpu delegate", "body": "Fixes #27757 ", "comments": ["Nagging Reviewer @impjdi: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Talked to @jaebaek in person.  For now, we will not pursue this further."]}, {"number": 27757, "title": "[Build error] TensorFlow Lite GPU delegate", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n    - Debian 4.19.20-1rodete1 (2019-02-12 > 2018) x86_64 GNU/Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n    - Build for Android by adding the following code:\r\n```\r\nandroid_sdk_repository (\r\n    name = \"androidsdk\",\r\n    api_level = 26,\r\n    build_tools_version = \"26.0.2\",\r\n    path = \"...\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name = \"androidndk\",\r\n    path = \".../android-ndk-r19c\",\r\n    api_level = 19,\r\n)\r\n```\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master, commit number 7bdb14a0b\r\n- Python version: 2.7.16rc1\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): Android (5058415 based on r339409) clang version 8.0.2\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nTried [TensorFlow Lite GPU delegate](https://www.tensorflow.org/lite/performance/gpu) and got a build error.\r\n\r\nAfter adding the following code into WORKSPACE under tensorflow source code's root directory,\r\n```\r\nandroid_sdk_repository (\r\n    name = \"androidsdk\",\r\n    api_level = 26,\r\n    build_tools_version = \"26.0.2\",\r\n    path = \"...\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name = \"androidndk\",\r\n    path = \".../android-ndk-r19c\",\r\n    api_level = 19,\r\n)\r\n```\r\n\r\nI ran the following command:\r\n```\r\nbazel build --cxxopt=--std=c++11 //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo\r\n```\r\n\r\nit reports:\r\n```\r\nbazel-out/android-armeabi-v7a-opt/bin/external/FP16/_virtual_includes/FP16/fp16.h:5:10: fatal error: 'fp16/fp16.h' file not found\r\n#include <fp16/fp16.h>\r\n         ^~~~~~~~~~~~~\r\n1 error generated.\r\nTarget //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hm. When I add android_sdk_repository or android_ndk_repository, I would get something like:\r\n\r\nERROR: /usr/local/google/home/impjdi/src/tensorflow/WORKSPACE:160:1: Cannot redefine repository after any load statement in the WORKSPACE file (for repository 'androidsdk')\r\nERROR: /usr/local/google/home/impjdi/src/tensorflow/WORKSPACE:165:1: Cannot redefine repository after any load statement in the WORKSPACE file (for repository 'androidndk')\r\nERROR: Error evaluating WORKSPACE file\r\n\r\nThen, when I remove, I get:\r\n\r\nERROR: /usr/local/google/home/impjdi/src/tensorflow/tensorflow/lite/java/demo/app/src/main/BUILD:7:1: no such package '@androidsdk//com.android.support': BUILD file not found on package path and referenced by '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo'\r\nERROR: /usr/local/google/home/impjdi/src/tensorflow/tensorflow/lite/java/demo/app/src/main/BUILD:7:1: no such package '@androidsdk//com.android.support': BUILD file not found on package path and referenced by '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo'\r\nERROR: Analysis of target '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo' failed; build aborted: no such package '@androidsdk//com.android.support': BUILD file not found on package path\r\n\r\nHowever, I can build gl_delegate.  Can you give me finer instructions?  I think you and I are not in sync.", "@jaebaek and @impjdi FYR. I can build apk with `bazel build --cxxopt=--std=c++11 //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo` on Ubuntu 18.04 and macOS without modification.", "@freedomtan \r\n\r\nInteresting.  Did you install Android Studio?", "@impjdi I do have Android Studio installed, but usually I use standalone SDK and NDK, something like the following lines are in my `.tf_configure.bazelrc`\r\n```\r\nbuild --action_env ANDROID_NDK_HOME=\"/Users/freedom/work/android-ndk-r17b\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"28\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"28.0.3\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"28\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/Users/freedom/work/android-sdk-mac_86\"\r\n```", "@jaebaek \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/bb530c6f82a06a8136042fccdd9456580f89f837\r\n\r\nmight have solved the issue.  Could you check?", "I just realized the commit I mentioned above does exactly what @jaebaek attempted to do in PR #27758.  Marking issue as closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27757\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27757\">No</a>\n"]}, {"number": 27756, "title": "Add CuDNN LSTM Projection (LSTMP)", "body": "We added LSTMP support to the tf.contrib.cudnn_rnn.CudnnLSTM. This feature will apply a new projected size (`num_proj`) and an extra weight matrix (projection matrix). Therefore, we need to distinguish the hidden_state_size and c_state_size, params_size of weights and biases throughout all the APIs. Previously, each pair of these sizes are treated same by default.\r\n\r\nThis projection feature is only supported in LSTM and CuDNN 7.1.1 or later.\r\n\r\nFYI. @nluehr ", "comments": ["I suggest this PR go to @protoget (He has worked with me on several related PR before.)", "That's awesome! Adding @protoget as I am not familiar with that part of the codebase.", "Thanks for adding @protoget to the review of this PR.", "@houtoms please resolve the conflicts.\r\n\r\n@chsigg, could you please help to take a look at the kernel change? Thanks.", "@aaroey, could you please find someone else for the op kernel review? I'm not familiar with LSTM projection.", "Hi @alextp, would you please help to review the newly added CudnnRNNParamsToCanonicalV2 and CudnnRNNCanonicalToParamsV2 op definition from API's perspective? Thanks.", "@aaroey I made the changes accordingly. Please have a look. Thanks.", "You can't delete; you need to have an apidef file in the python_api\nsubdirectory setting things to hidden, like\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/python_api/api_def_BatchMatMul.pbtxt\n\nOn Fri, May 31, 2019 at 9:48 AM Kaixi Hou <notifications@github.com> wrote:\n\n> *@houtoms* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/tools/api/golden/v1/tensorflow.pbtxt\n> <https://github.com/tensorflow/tensorflow/pull/27756#discussion_r289465812>\n> :\n>\n> > @@ -1032,6 +1032,14 @@ tf_module {\n>      name: \"cross\"\n>      argspec: \"args=[\\'a\\', \\'b\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\n>    }\n> +  member_method {\n> +    name: \"cudnn_rnn_canonical_to_params_v2\"\n>\n> Do you mean I should delete both cudnn_rnn_canonical_to_params_v2 and\n> cudnn_rnn_params_to_canonical_v2? Or is there any attribute that I can\n> set hidden?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27756?email_source=notifications&email_token=AAABHRKIYX7LBEDPAXMT4ETPYFJEXA5CNFSM4HFILARKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB2IVZSI#discussion_r289465812>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRP75IMYS7GPIFKZYG3PYFJEXANCNFSM4HFILARA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Thanks for clarification. I made those changes.", "The API changes look good, then. @aaroey please approve when the code is ready.", "Thanks @alextp for the review!", "@aaroey I fixed one issue of the shape inference and some other python format issues.", "@aaroey I just made the changes. Please check.", "@aaroey It seems `import/copybara \u2014 An error happened while migrating the change`. Did I do something wrong?", "@aaroey It seems we got some `SyntaxError` and it doesn't come from my changes. Can you advise what the failures are (and the `feedback/copybara`, which I don't have access to)?", "> @aaroey It seems `import/copybara \u2014 An error happened while migrating the change`. Did I do something wrong?\r\n\r\n@houtoms Sorry to interrupt but How did you fix this error?", "> > @aaroey It seems `import/copybara \u2014 An error happened while migrating the change`. Did I do something wrong?\r\n> \r\n> @houtoms Sorry to interrupt but How did you fix this error?\r\n\r\nI think it was some internal problem from Google back then."]}, {"number": 27755, "title": "Disable one more flaky test", "body": "", "comments": []}, {"number": 27754, "title": "Building Tensorflow on Windows (CPU only) issue", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 - c5.nlarge AWS Instance**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **NA**\r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version: **r1.13** (Tried with **r.12** as well)\r\n- Python version: **Python 3.6** (Conda)\r\n- Installed using virtualenv? pip? conda?: **NA**\r\n- Bazel version (if compiling from source): **0.19**\r\n- GCC/Compiler version (if compiling from source): **Using Visual Studio 2017**\r\n- CUDA/cuDNN version: **NA**\r\n- GPU model and memory: **NA**\r\n\r\n**Describe the problem**\r\nI started off with the [instructions provided on TensorFlow page](https://www.tensorflow.org/install/source_windows), \r\n\r\n1. I installed Python 3.6 in PATH.\r\n2. Installed Bazel 0.19 from Releases page. \r\n3. Installed Visual Studio 2017 since VS 2015 was not installing properly on Windows 10.\r\n4. Installed the c++ build tools for VS 2017.\r\n5. Installed MSYS2, and installed patch, unzip and git\r\n6. Created the environment variables as specified in [Bazel documentation](https://docs.bazel.build/versions/master/windows.html#build-c)\r\n```\r\nC:\\projects\\bazel> bazel build //examples/cpp:hello-world\r\n\r\nC:\\projects\\bazel> bazel-bin\\examples\\cpp\\hello-world.exe\r\n```\r\nThe above example worked fine. The paths I used were different.\r\n7. Used git to clone tensorflow and then checked out **r1.13**\r\n8. Used `python .\\configure.py` [Log file](https://github.com/tensorflow/tensorflow/files/3069060/configure_log.txt)\r\n9. Used `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` [Log file]\r\n(https://github.com/tensorflow/tensorflow/files/3069130/bazel_log.txt)\r\n\r\nI have tried the following things as well but they have resulted in errors everytime:\r\n\r\n```bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbazel build --config=opt --define=no_tensorflow_py_deps=true --local_resources 2048,.5,1.0 //tensorflow/tools/pip_package:build_pip_package```\r\n\r\n\r\nI would appreciate any help regarding this.", "comments": ["For the compilation of the CPU version, you can use CMAKE.In addition TF 1.13 needs to use MSVC 2015 update 3 compiler, MSVC2017 may have some problems.(see https://www.tensorflow.org/install/source_windows#tested_build_configurations)", "Thanks. I will try it out. "]}, {"number": 27753, "title": "ValueError: Provide an input shape for input array 'Placeholder'", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 16.04\r\n- TensorFlow installed from : conda install\r\n- TensorFlow version : gpu-1.13.1\r\n------------------------------------------------Error-------------------------------------------------------<\r\n`File \"convertpb2tflite.py\", line 5, in <module>\r\n    tflite_model=convert.convert()\r\nFile \"/home/yuchen/anaconda2/envs/Conv-TasNet/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 406, in ### ### convert\r\n    \"'{0}'.\".format(_tensor_name(tensor)))\r\n### **ValueError: Provide an input shape for input array 'Placeholder'.**\r\n`\r\n\r\n----------------------------------------------------My code----------------------------------------------------------<\r\nI try to convert the model from [https://github.com/chaodengusc/DeWave](url) to tflite format.\r\nAnd, the inference looks like this:\r\n`p_keep_ff = tf.placeholder(tf.float32, shape=None)\r\np_keep_rc = tf.placeholder(tf.float32, shape=None)\r\nin_data = tf.placeholder(tf.float32, shape=[batch_size, FRAMES_PER_SAMPLE, NEFF])\r\nBiModel = Model(n_hidden, batch_size, p_keep_ff, p_keep_rc)\r\nembedding = BiModel.inference(in_data)`\r\n\r\n\r\nThen, I use this code to convert frozen.pb:\r\n`convert= tf.lite.TFLiteConverter.from_frozen_graph(\"frozen.pb\",input_arrays=[\"Placeholder\",\"Placeholder_1\",\"Placeholder_2\"],output_arrays=[\"l2_normalize\"],input_shapes={\"Placeholder\":None,\"Placeholder_1\":None,\"Placeholder_2\":[128, 100, 129]})\r\nconvert.post_training_quantize=True\r\ntflite_model=convert.convert()\r\nopen(\"model.tflite\",\"wb\").write(tflite_model)`\r\n\r\n\r\n-----------------------------------------------------MyQuestion--------------------------------------------------<\r\nAs above, original shape placeholder is none, how should I set the shape of Placeholder?\r\nThank you!!\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "hello @ymodak  any answers? how you solve this question"]}, {"number": 27752, "title": "KeyError: 'ExperimentalFunctionBufferingResource' in Tf >= 1.13 (mkl)", "body": "- OS Platform and Distribution - Linux Ubuntu 16.04)\r\n- TensorFlow installed from Anaconda dist 3.6.\r\n- TensorFlow version 1.13-mkl\r\n- Python version: Python 3.6.6 :: Anaconda, Inc.\r\n\r\n**Describe the current behavior**\r\nModels trained using the `Iterator-Gen` for data and saved as `meta` & `ckpt` cannot be imported using the `tf.train.import_meta_graph` function. Gives a `KeyError`. \r\n**Describe the expected behavior**\r\nWorking fine with Tensorflow1.12(mkl). Error found in Tf version >= 1.13\r\n\r\n**Other info / logs**\r\nHere is the Traceback\r\n`read=tf.train.import_meta_graph(self.paths[0], clear_devices=True)\r\n  File \"/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_gra                                                    ph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_gr                                                    aph_with_return_elements\r\n    **kwargs))\r\n  File \"/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements                                         \r\n    return_elements=return_elements)\r\n  File \"/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func return func(*args, **kwargs)\r\n  File \"/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 399, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/pnayak/anaconda3/envs/coreml/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 159, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'ExperimentalFunctionBufferingResource'`\r\n", "comments": ["Hi,\r\nWhat kind of iterator are you using? We would have to reproduce in order to root cause", "The cifar-10 binary example in tensorlfow/models repo.", "same happens to me.\r\n\r\n* OS: CentOS7\r\n* python3.5.6\r\n* tf version: 1.13.1\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.tensorrt as trt\r\n\r\nmeta_graph_path = \"bert-eng/model.ckpt-7300.meta\"\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    with tf.Session() as sess:\r\n\r\n        saver = tf.train.import_meta_graph(meta_graph_path) // error occurs here\r\n```\r\n\r\nWhen I run the above code, I get error at line:\r\n```python\r\nsaver = tf.train.import_meta_graph(meta_graph_path) // error occurs here\r\n```\r\n\r\nstack trace:\r\n```\r\n2019-04-25 10:40:52.107550: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-04-25 10:40:52.119933: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099880000 Hz\r\n2019-04-25 10:40:52.122488: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5431820 executing computations on platform Host. Devices:\r\n2019-04-25 10:40:52.122547: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"tf_to_trt.py\", line 15, in <module>\r\n    saver = tf.train.import_meta_graph(meta_graph_path)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 399, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/msl/.virtualenvs/venv_waveglow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 159, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'ExperimentalFunctionBufferingResource'\r\n```", "Looks like this error is not related to mkl, but appears even with Google's public build", "encountered the same error while trying to load a model from the TF model zoo.\r\n\r\nTF version: 1.13.1 in NVIDIA NGC TF container 19.04 \r\nnvcr.io/nvidia/tensorflow:19.04-py3\r\n\r\n```\r\n# Inference with TF-TRT `MetaGraph` and checkpoint files workflow:\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    with tf.Session() as sess:\r\n        # First create a `Saver` object (for saving and rebuilding a\r\n        # model) and import your `MetaGraphDef` protocol buffer into it:\r\n        saver = tf.train.import_meta_graph(\"./data/resnet_v1_50/model.ckpt-225207.meta\")\r\n        # Then restore your training data from checkpoint files:\r\n        saver.restore(sess, \"./data/resnet_v1_50/model.ckpt-225207\")\r\n\r\n```\r\n\r\n```---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-6-8f550eab3689> in <module>\r\n     12         # First create a `Saver` object (for saving and rebuilding a\r\n     13         # model) and import your `MetaGraphDef` protocol buffer into it:\r\n---> 14         saver = tf.train.import_meta_graph(\"./data/resnet_v1_50/model.ckpt-225207.meta\")\r\n     15         # Then restore your training data from checkpoint files:\r\n     16         saver.restore(sess, \"./data/resnet_v1_50/model.ckpt-225207\")\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\r\n   1433   \"\"\"  # pylint: disable=g-doc-exception\r\n   1434   return _import_meta_graph_with_return_elements(\r\n-> 1435       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n   1436 \r\n   1437 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\r\n   1455           import_scope=import_scope,\r\n   1456           return_elements=return_elements,\r\n-> 1457           **kwargs))\r\n   1458 \r\n   1459   saver = _create_saver_from_imported_meta_graph(\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)\r\n    804         input_map=input_map,\r\n    805         producer_op_list=producer_op_list,\r\n--> 806         return_elements=return_elements)\r\n    807 \r\n    808     # Restores all the other collections.\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    397   if producer_op_list is not None:\r\n    398     # TODO(skyewm): make a copy of graph_def so we're not mutating the argument?\r\n--> 399     _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n    400 \r\n    401   graph = ops.get_default_graph()\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py in _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n    157     # Remove any default attr values that aren't in op_def.\r\n    158     if node.op in producer_op_dict:\r\n--> 159       op_def = op_dict[node.op]\r\n    160       producer_op_def = producer_op_dict[node.op]\r\n    161       # We make a copy of node.attr to iterate through since we may modify\r\n\r\nKeyError: 'ExperimentalFunctionBufferingResource'```", "I have the same issue when I want to load a meta-file", "you just need to add `*.so` compiled file generated by model implementation.\r\nas follows add this line before session and import meta graph.\r\n`tf.load_op_library('<compiled_filename>.so')`\r\nthis function add a new op in tensorflow runtime.\r\nfor example. i compiled a model that contains a `RoiPooling` layer and generated `roi_pooling.so` file. so that i added this line to my code\r\n`tf.load_op_library('op/roi_pooling.so')`\r\n\r\nand finally it works : )", "> you just need to add `*.so` compiled file generated by model implementation.\r\n> as follows add this line before session and import meta graph.\r\n> `tf.load_op_library('<compiled_filename>.so')`\r\n> this function add a new op in tensorflow runtime.\r\n> for example. i compiled a model that contains a `RoiPooling` layer and generated `roi_pooling.so` file. so that i added this line to my code\r\n> `tf.load_op_library('op/roi_pooling.so')`\r\n> \r\n> and finally it works : )\r\n\r\nCan you more specific ? Your use case is totally not reflected in your answer. Seems helpful though. Interested to know about this. \r\n\r\nSo the error is actually occurring when there is a model built using `tf.record`, which creates a `ExperimentalFunctionBufferingResource` tensor rather than a `tf.Placeholder` tensor. ", "> you just need to add `*.so` compiled file generated by model implementation.\r\n> as follows add this line before session and import meta graph.\r\n> `tf.load_op_library('<compiled_filename>.so')`\r\n> this function add a new op in tensorflow runtime.\r\n> for example. i compiled a model that contains a `RoiPooling` layer and generated `roi_pooling.so` file. so that i added this line to my code\r\n> `tf.load_op_library('op/roi_pooling.so')`\r\n> \r\n> and finally it works : )\r\n\r\n@amirmgh1375  Can you please elaborate ? \r\n", "@prateethvnayak \r\n\r\nIs this issue present?\r\n\r\nIf yes, could you refer to following issues?\r\n  https://github.com/tensorflow/tensorflow/issues/28131\r\n  https://github.com/tensorflow/tensorflow/issues/29751\r\nIf no, could you close it?\r\n", "@prateethvnayak \r\n\r\nIf your issue is fixed, could you close it?", "@NeoZhangJianyu One of the other issues is also posted by me. Seems like they are unsolved, and just died due to less activity as TF has moved to 2.0 heavily now. This is still a persistent problem with the mkl build of TF < 2.0. ", "@prateethvnayak \r\nOK,  we will check this issue with latest TF 1.15.x.\r\n\r\nThank you!", "@prateethvnayak \r\n\r\nI use tensorflow 1.15.0 mkl_py36h4920b83_0 installed by Conda, and test with standard bert model.\r\nThere is no such issue.\r\n\r\nCould you share the link of the model files?\r\n\r\nThank you!", "@prateethvnayak,\r\n\r\nWe see that this issue is about an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to latest stable version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27752\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27752\">No</a>\n"]}, {"number": 27751, "title": "Api doc updated", "body": "Tensor dimensions updated", "comments": []}, {"number": 27750, "title": "TF 2 - Method estimator.model_to_estimator( ) fails but model.fit works for tf.keras created model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 / Colab ( Linux )\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): **TF 2.0.0 Alpha**\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\nThe below code has been taken from the Tensorflow without PHD series - RNN time series prediction.\r\n\r\n```\r\ndef compile_keras_sequential_model(list_of_layers, msg):\r\n  \r\n    # a tf.keras.Sequential model is a sequence of layers\r\n    model = tf.keras.Sequential(list_of_layers)\r\n    \r\n    # keras does not have a pre-defined metric for Root Mean Square Error. Let's define one.\r\n    def rmse(y_true, y_pred): # Root Mean Squared Error\r\n      return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\r\n    \r\n    print('\\nModel ', msg)\r\n    \r\n    #Optimizer\r\n    sgd = tf.keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\r\n    \r\n    # to finalize the model, specify the loss, the optimizer and metrics\r\n    model.compile(\r\n       loss = 'mean_squared_error',\r\n       optimizer = sgd,\r\n#         optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\r\n       metrics = [rmse])\r\n    \r\n    # this prints a description of the model\r\n    model.summary()\r\n    \r\n    return model\r\n```\r\n```\r\n#Create Keras model\r\ndef model_fn_keras():\r\n    \r\n    # RNN model (RMSE: 0.164 after 10 epochs)\r\n    model_layers_RNN = [\r\n        l.Reshape([SEQLEN, 1], input_shape=[SEQLEN,]), # [BATCHSIZE, SEQLEN, 1] is necessary for RNN model\r\n        l.GRU(RNN_CELLSIZE, return_sequences=True),  # output shape [BATCHSIZE, SEQLEN, RNN_CELLSIZE]\r\n        l.GRU(RNN_CELLSIZE), # keep only last output in sequence: output shape [BATCHSIZE, RNN_CELLSIZE]\r\n        l.Dense(1) # output shape [BATCHSIZE, 1]\r\n    ]\r\n\r\n    model_RNN = compile_keras_sequential_model(model_layers_RNN, \"RNN\")\r\n    \r\n    return(model_RNN)\r\n```\r\n**While converting the Keras model to estimator version, below line gives error:**\r\n\r\n`estimator = tf.keras.estimator.model_to_estimator(keras_model=model_fn_keras())`\r\n\r\n```\r\nModel  RNN\r\nModel: \"sequential_27\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nreshape_27 (Reshape)         (None, 16, 1)             0         \r\n_________________________________________________________________\r\nunified_gru_57 (UnifiedGRU)  (None, 16, 32)            3360      \r\n_________________________________________________________________\r\nunified_gru_58 (UnifiedGRU)  (None, 32)                6336      \r\n_________________________________________________________________\r\ndense_27 (Dense)             (None, 1)                 33        \r\n=================================================================\r\nTotal params: 9,729\r\nTrainable params: 9,729\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1334     try:\r\n-> 1335       return fn(*args)\r\n   1336     except errors.OpError as e:\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1317       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1318       self._extend_graph()\r\n   1319       return self._call_tf_sessionrun(\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _extend_graph(self)\r\n   1352     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1353       tf_session.ExtendSession(self._session)\r\n   1354 \r\n\r\nInvalidArgumentError: Node 'training/SGD/gradients/unified_gru_58/StatefulPartitionedCall_grad/StatefulPartitionedCall': Connecting to invalid output 4 of source node unified_gru_58/StatefulPartitionedCall which has 4 outputs\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-52-05ea50da2f1c> in <module>()\r\n      5 #Convert Keras model to Estimator\r\n      6 # tf.disable_eager_execution()\r\n----> 7 estimator = tf.keras.estimator.model_to_estimator(keras_model=model_fn_keras())\r\n      8 # estimator = model_fn_keras()\r\n      9 \r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\estimator\\__init__.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)\r\n     71       custom_objects=custom_objects,\r\n     72       model_dir=model_dir,\r\n---> 73       config=config)\r\n     74 \r\n     75 # LINT.ThenChange(//tensorflow_estimator/python/estimator/keras.py)\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\keras.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)\r\n    488   if keras_model._is_graph_network:\r\n    489     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,\r\n--> 490                                              config)\r\n    491   elif keras_model.built:\r\n    492     logging.warning('You are creating an Estimator from a Keras model manually '\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\keras.py in _save_first_checkpoint(keras_model, custom_objects, config)\r\n    365           # pylint: disable=protected-access\r\n    366           model._make_train_function()\r\n--> 367           K._initialize_variables(sess)\r\n    368           # pylint: enable=protected-access\r\n    369         saver = saver_lib.Saver()\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in _initialize_variables(session)\r\n    760     # marked as initialized.\r\n    761     is_initialized = session.run(\r\n--> 762         [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n    763     uninitialized_vars = []\r\n    764     for flag, v in zip(is_initialized, candidate_vars):\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    928     try:\r\n    929       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 930                          run_metadata_ptr)\r\n    931       if run_metadata:\r\n    932         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1151     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1152       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1153                              feed_dict_tensor, options, run_metadata)\r\n   1154     else:\r\n   1155       results = []\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1327     if handle is None:\r\n   1328       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1329                            run_metadata)\r\n   1330     else:\r\n   1331       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1347           pass\r\n   1348       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1349       raise type(e)(node_def, op, message)\r\n   1350 \r\n   1351   def _extend_graph(self):\r\n\r\nInvalidArgumentError: Node 'training/SGD/gradients/unified_gru_58/StatefulPartitionedCall_grad/StatefulPartitionedCall': Connecting to invalid output 4 of source node unified_gru_58/StatefulPartitionedCall which has 4 outputs\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould not give any error\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@ymodak : I am not able to relate as to why should this not be a bug ? A simple operation done with a Keras exported TF model is resulting in an error which should not be happening. \r\n\r\nOn the same model above, if I call model.fit( ), it runs absolutely fine so it should also be converted easily using the function - tf.keras.estimator.model_to_estimator( ). \r\n\r\nAs per the **official TF documentation** here - https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/estimator/model_to_estimator this should work if the Keras model is valid.\r\n\r\nI believe this is a bug.\r\n\r\nAlso asked on: https://stackoverflow.com/questions/55772880/tf-2-0-method-estimator-model-to-estimator-fails-but-model-fit-works-for-tf", "Reopening this to debug further and handle appropriately.", "This is indeed a bug, which comes from a bad interaction between tf.function and model_to_estimator.\r\n\r\nhttps://colab.sandbox.google.com/gist/robieta/d20dee003c8a21aceb251ac3c64583c1/model_to_est_fn.ipynb\r\n\r\nThe RNN just happen to be the only layers (that I know of, at least) which use functions internally which is why this surfaced on the GRU layer.", "Thanks for reporting the issue. We are able to reproduce it. \r\n\r\nAs @robieta stated, this issue was caused by the combination of tf.function in keras layer and v1 tf.session which is used by model_to_estimator.\r\n\r\nWith LSTM/GRU v2, the call body has been changed to use tf.function. In model_to_estimator, model was first created with forward graph, and then model._make_train_function(), which will try to update the graph with gradient part. This will fail if they run under same session, with warning message:\r\n\r\nW0613 09:53:26.989550   66929 c_api.cc:338] Operation '{name:'gru/StatefulPartitionedCall' id:65 op device:{} def:{{{node gru/StatefulPartitionedCall}} = StatefulPartitionedCall[Tin=[DT_FLOAT, DT_FLOAT, DT_RESOURCE, DT_RESOURCE, DT_RESOURCE], Tout=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_FLOAT, DT_INT32], _gradient_op_type=\"PartitionedCall-1192\", config=\"\", config_proto=\"\\n\\007\\n\\003GPU\\020\\000\\n\\007\\n\\003CPU\\020\\0012\\002J\\0008\\001\", executor_type=\"\", f=__forward_standard_gru_2674[]](reshape/Reshape, gru/zeros, gru/kernel, gru/recurrent_kernel, gru/bias)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\r\n\r\nThe follow up K._initialize_variables(sess) will fail since the graph is in a weird state now.\r\n\r\nLet me see if there is any workaround for this issue.\r\n\r\n", "This should now be fixed by https://github.com/tensorflow/estimator/commit/c956dd32561bac645a1cd870d3c8cfe8e9fe969b, which should be available in tf estimator nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27750\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27750\">No</a>\n", "Thanks a lot guys :)"]}, {"number": 27749, "title": "negative pixel values for ssim input?", "body": "ssim, and ms_ssim take images as input and max_val which is defined as the dynamic range, implying that images can be eg in [-1,1] with maxval = 2. however they call convert_image_dtype() which assumes pixels are in [0,?]. so it is not clear whether ssim, ms_ssim can take images with negative pixel values.", "comments": ["@eyaler Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "the issue is not platform dependent, however: tf-gpu 1.12, windows 64. pip installed.\r\ngiven img1 and img2 in [0,1] you can compare:\r\n```\r\nms_ssim_op1 = tf.image.ssim_multiscale(img1, img2, 1) # reference images in [0,1]\r\nms_ssim_op2 = tf.image.ssim_multiscale(img1*2, img2*2, 2) # scaling to [0,2] will give same result\r\nms_ssim_op3 = tf.image.ssim_multiscale(img1*2-1, img2*2-1, 2) # scaling to [-1,1] will give different result\r\n```\r\n", "So this is a documentation problem? Want to send a PR clarifying that negative values are not allowed?", "i think we should give a warning", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27749\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27749\">No</a>\n"]}, {"number": 27747, "title": "Inception for unity", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  1.13.1\r\n\r\n\r\n\r\n\r\nI wanted to use Tensorflow Inception in Unity with reference to this link.\r\n https://github.com/chen0040/unity-tensorflow-samples\r\n\r\nBelow is the code I used, but I do not see any errors, but I can not discriminate the image. In other words, nothing is displayed in the concole window. Do you know the solution?\r\n  \r\nfirst code, \r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing UnityEngine;\r\nusing TensorFlow;\r\n\r\npublic class InceptionTest\r\n{\r\n\r\n    private TFGraph graph;\r\n    private TFSession session;\r\n\r\n    private static string[] labels = new string[] {\r\n            \"Linear type\",\r\n            \"point type\",\r\n            \"sidewalk block\",\r\n            \r\n    };\r\n\r\n    public void LoadModel(string your_name_graph)\r\n    {\r\n        if (graph == null)\r\n        {\r\n            Debug.Log(\"Loading tensor graph \" + your_name_graph);\r\n            TextAsset graphModel = Resources.Load<TextAsset>(your_name_graph);\r\n\r\n            if (graphModel == null)\r\n            {\r\n                Debug.LogError(\"Failed to load tensor graph \" + your_name_graph);\r\n            }\r\n\r\n            graph = new TFGraph();\r\n            graph.Import(graphModel.bytes);\r\n            session = new TFSession(graph);\r\n        }\r\n    }\r\n\r\n    public int PredictClass(Texture2D image)\r\n    {\r\n        Debug.Log(image);\r\n\r\n\r\n        float[] imageBytes = new float[image.width * image.height * 3];\r\n\r\n        int idx = 0;\r\n        for (int i = 0; i < image.width; i++)\r\n        {\r\n            for (int j = 0; j < image.height; j++)\r\n            {\r\n                Color pixel = image.GetPixel(i, j);\r\n\r\n                imageBytes[idx++] = pixel.r / 255.0f;\r\n                imageBytes[idx++] = pixel.g / 255.0f;\r\n                imageBytes[idx++] = pixel.b / 255.0f;\r\n            }\r\n        }\r\n\r\n\r\n        var runner = session.GetRunner();\r\n        runner\r\n            .AddInput(graph[\"conv2d_1_input\"][0], TFTensor.FromBuffer(new TFShape(new long[] { 1, 32, 32, 3 }), imageBytes, 0, 3072))\r\n            .AddInput(graph[\"dropout_1/keras_learning_phase\"][0], new TFTensor(TFDataType.Bool, new long[0], 1));\r\n\r\n        runner.Fetch(graph[\"output_node0\"][0]);\r\n        float[,] recurrent_tensor = runner.Run()[0].GetValue() as float[,];\r\n\r\n        float maxVal = float.MinValue;\r\n        int bestIdx = -1;\r\n        for (int i = 0; i < recurrent_tensor.GetUpperBound(1); ++i)\r\n        {\r\n            float val = recurrent_tensor[0, i];\r\n            if (val > maxVal)\r\n            {\r\n                maxVal = val;\r\n                bestIdx = i;\r\n            }\r\n        }\r\n\r\n        return bestIdx;\r\n    }\r\n\r\n    public string PredictLabel(Texture2D image)\r\n    {\r\n        int classId = PredictClass(image);\r\n        if (classId >= 0 && classId < labels.Length)\r\n        {\r\n            return labels[classId];\r\n        }\r\n        return \"unknown\";\r\n    }\r\n}\r\n\r\n\r\n\r\nsecond, \r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing UnityEngine;\r\n\r\npublic class cshInceptionTest : MonoBehaviour\r\n{\r\n    private InceptionTest classifier = new InceptionTest();\r\n    \r\n    // Start is called before the first frame update\r\n    void Start()\r\n    {\r\n        Debug.Log(TensorFlow.TFCore.Version);\r\n        classifier.LoadModel(\"tf_models/output_graph\");\r\n\r\n        string[] image_names = new string[9];\r\n        int index = 0;\r\n        for (int i = 1; i <= 3; ++i)\r\n        {\r\n            image_names[index++] = \"Linear type\" + i;\r\n            image_names[index++] = \"Point type\" + i;\r\n            image_names[index++] = \"sidewalk block\" + i;\r\n        }\r\n        foreach (string image_name in image_names)\r\n        {\r\n            Debug.Log(image_name);\r\n            Texture2D img = Resources.Load<Texture2D>(\"images/inception/\" + image_name);\r\n            Debug.Log(\"Predicted: \" + classifier.PredictLabel(img));\r\n        }\r\n\r\n    }\r\n\r\n    // Update is called once per frame\r\n    void Update()\r\n    {\r\n        \r\n    }\r\n}\r\n\r\n\r\n\r\nFirst, I try to determine the image of the folder by inserting the second code into any object.\r\nPlease Help me T_T\r\n\r\n\r\n\r\n", "comments": ["GitHub [unity-tensorflow-samples](https://github.com/chen0040/unity-tensorflow-samples/issues) can be good platform to raise this question. Please post it on [unity-tensorflow-samples](https://github.com/chen0040/unity-tensorflow-samples/issues). Thanks!"]}, {"number": 27746, "title": "How can I warm up the cache by calling the predict method with an all zero inputs in browsers?", "body": "You can warm up the cache by calling the predict method with an all zero inputs\r\n", "comments": ["Sorry, I do not know what you mean is?", "@XdyGoGo This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.  If you think it is a bug/feature request, please fill the respective template inorder to start triaging. Thanks !\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 27745, "title": "TF Lite lstm checks added for type", "body": "This is one of the TODO in the file. Changes are supported with TC", "comments": []}, {"number": 27744, "title": "Failed to build a debug version of tensorflow from scratch", "body": "**System information**\r\n- OS Platform and Distribution:  ubuntu 16.04 - NVIDIA Docker \r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: branch  r1.13 on TensorFlow repo \r\n- Python version:  3.6.7\r\n- Bazel version (if compiling from source):  0.22.0\r\n- GCC/Compiler version (if compiling from source):  gcc version 5.4.0 20160609\r\n- CUDA/cuDNN version:  CUDA 10.0 and cuDNN 7.3 \r\n\r\n**Describe the problem**\r\n\r\n**It's failed to build a debug version of TensorFlow from scratch**\r\n\r\nI wanted to use `gdb` and `cuda-gdb` tools to debug the c++ code on TensorFlow.  I followed the instruction on `tensorflow/BUILD` file,  which indicated that `dbg` was used to build with debug symbol for Tensorflow as the following:\r\n```\r\nconfig_setting(\r\n    name = \"debug\",\r\n    values = {\r\n        \"compilation_mode\": \"dbg\",\r\n    },\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\nI built the code on branch `r1.13` with the following command:\r\n```\r\nbazel build  -c dbg  --config cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nand I got an error \r\n```\r\n1 error detected in the compilation of \"/tmp/tmpxft_00007343_00000000-7_zero_initializer_op_gpu.cu.compute_70.cpp1.ii\".\r\nERROR: /root/tensorflow/tensorflow/contrib/framework/BUILD:109:1: output 'tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/zero_initializer_op_gpu.cu.pic.o' was not created\r\nERROR: /root/tensorflow/tensorflow/contrib/framework/BUILD:109:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps\r\n```\r\nI found the location on the `bazel-out/k8-dbg/bin/tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu` and couldn't find the file `zero_initializer_op_gpu.cu.pic.o` indeed.  \r\nBut the error was not always on the same location. If I ran the command again,  the error was almost the same but with the other files, such as:\r\n```\r\n1 error detected in the compilation of \"/tmp/tmpxft_00007f82_00000000-7_lstm_ops_gpu.cu.compute_70.cpp1.ii\".\r\nERROR: /root/tensorflow/tensorflow/contrib/rnn/BUILD:215:1: output 'tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/lstm_ops_gpu.cu.pic.o' was not created\r\nERROR: /root/tensorflow/tensorflow/contrib/rnn/BUILD:215:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n``` \r\n\r\n**Another way to build with debug symbol**\r\nI noticed that I could build with `cxxopt` command line option to get debug symbol with the following command\r\n```\r\nbazel build  --cxxopt='-g' --config cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nAnd I succeeded. I scan the sections on the built shared libraries and they showed the `debug` sections: \r\n```\r\nreadelf -S libtensorflow_framework.so\r\n\r\nSection Headers:\r\n  [Nr] Name              Type             Address           Offset\r\n       Size              EntSize          Flags  Link  Info  Align\r\n  [ 0]                   NULL             0000000000000000  00000000\r\n       0000000000000000  0000000000000000           0     0     0\r\n  [ 1] .note.gnu.build-i NOTE             0000000000000200  00000200\r\n       0000000000000020  0000000000000000   A       0     0     4\r\n  [ 2] .gnu.hash         GNU_HASH         0000000000000220  00000220\r\n       0000000000034ce4  0000000000000000   A       3     0     8\r\n  [ 3] .dynsym           DYNSYM           0000000000034f08  00034f08\r\n       00000000000afe30  0000000000000018   A       4     3     8\r\n  [ 4] .dynstr           STRTAB           00000000000e4d38  000e4d38\r\n       0000000000275ace  0000000000000000   A       0     0     1\r\n  [ 5] .gnu.version      VERSYM           000000000035a806  0035a806\r\n       000000000000ea84  0000000000000002   A       3     0     2\r\n  [ 6] .gnu.version_d    VERDEF           0000000000369290  00369290\r\n       0000000000000038  0000000000000000   A       4     2     8\r\n  [ 7] .gnu.version_r    VERNEED          00000000003692c8  003692c8\r\n       0000000000000280  0000000000000000   A       4     9     8\r\n  [ 8] .rela.dyn         RELA             0000000000369548  00369548\r\n       00000000000c2ca0  0000000000000018   A       3     0     8\r\n  [ 9] .init             PROGBITS         000000000042c1e8  0042c1e8\r\n       000000000000001a  0000000000000000  AX       0     0     4\r\n  [10] .plt              PROGBITS         000000000042c210  0042c210\r\n       0000000000000010  0000000000000010  AX       0     0     16\r\n  [11] .plt.got          PROGBITS         000000000042c220  0042c220\r\n       0000000000016078  0000000000000000  AX       0     0     8\r\n  [12] .text             PROGBITS         00000000004422a0  004422a0\r\n       00000000008e3cb7  0000000000000000  AX       0     0     32\r\n  [13] malloc_hook       PROGBITS         0000000000d25f60  00d25f60\r\n       0000000000000272  0000000000000000  AX       0     0     16\r\n  [14] .fini             PROGBITS         0000000000d261d4  00d261d4\r\n       0000000000000009  0000000000000000  AX       0     0     4\r\n  [15] .rodata           PROGBITS         0000000000d261e0  00d261e0\r\n       000000000007e79c  0000000000000000   A       0     0     32\r\n  [16] .eh_frame_hdr     PROGBITS         0000000000da497c  00da497c\r\n       000000000002da34  0000000000000000   A       0     0     4\r\n  [17] .eh_frame         PROGBITS         0000000000dd23b0  00dd23b0\r\n       00000000000f75fc  0000000000000000   A       0     0     8\r\n  [18] .gcc_except_table PROGBITS         0000000000ec99ac  00ec99ac\r\n       000000000002ed0e  0000000000000000   A       0     0     4\r\n  [19] .tdata            PROGBITS         00000000010f90f0  00ef90f0\r\n       0000000000000038  0000000000000000 WAT       0     0     8\r\n  [20] .tbss             NOBITS           00000000010f9128  00ef9128\r\n       0000000000000040  0000000000000000 WAT       0     0     8\r\n  [21] .init_array       INIT_ARRAY       00000000010f9128  00ef9128\r\n       0000000000000ce8  0000000000000000  WA       0     0     8\r\n  [22] .fini_array       FINI_ARRAY       00000000010f9e10  00ef9e10\r\n       0000000000000008  0000000000000000  WA       0     0     8\r\n  [23] .jcr              PROGBITS         00000000010f9e18  00ef9e18\r\n       0000000000000008  0000000000000000  WA       0     0     8\r\n  [24] .data.rel.ro      PROGBITS         00000000010f9e20  00ef9e20\r\n       0000000000025c50  0000000000000000  WA       0     0     32\r\n  [25] .dynamic          DYNAMIC          000000000111fa70  00f1fa70\r\n       0000000000000280  0000000000000010  WA       4     0     8\r\n  [26] .got              PROGBITS         000000000111fcf0  00f1fcf0\r\n       000000000001d310  0000000000000008  WA       0     0     8\r\n  [27] .data             PROGBITS         000000000113d000  00f3d000\r\n       0000000000002afc  0000000000000000  WA       0     0     32\r\n  [28] .bss              NOBITS           000000000113fb00  00f3fafc\r\n       0000000000010170  0000000000000000  WA       0     0     64\r\n  [29] .comment          PROGBITS         0000000000000000  00f3fafc\r\n       0000000000000035  0000000000000001  MS       0     0     1\r\n  [30] .debug_aranges    PROGBITS         0000000000000000  00f3fb31\r\n       0000000000069970  0000000000000000           0     0     1\r\n  [31] .debug_info       PROGBITS         0000000000000000  00fa94a1\r\n       0000000007272cf1  0000000000000000           0     0     1\r\n  [32] .debug_abbrev     PROGBITS         0000000000000000  0821c192\r\n       00000000002437ba  0000000000000000           0     0     1\r\n  [33] .debug_line       PROGBITS         0000000000000000  0845f94c\r\n       000000000057ff0f  0000000000000000           0     0     1\r\n  [34] .debug_str        PROGBITS         0000000000000000  089df85b\r\n       0000000003f8f295  0000000000000001  MS       0     0     1\r\n  [35] .debug_loc        PROGBITS         0000000000000000  0c96eaf0\r\n       00000000027734b9  0000000000000000           0     0     1\r\n  [36] .debug_ranges     PROGBITS         0000000000000000  0f0e1fa9\r\n       000000000095cfc0  0000000000000000           0     0     1\r\n  [37] .shstrtab         STRTAB           0000000000000000  0fe3e9e7\r\n       0000000000000188  0000000000000000           0     0     1\r\n  [38] .symtab           SYMTAB           0000000000000000  0fa3ef70\r\n       00000000000dd040  0000000000000018          39   7705     8\r\n  [39] .strtab           STRTAB           0000000000000000  0fb1bfb0\r\n       0000000000322a37  0000000000000000           0     0     1\r\n```   \r\n```\r\nreadelf -S _pywrap_tensorflow_internal.so\r\n\r\nSection Headers:\r\n  [Nr] Name              Type             Address           Offset\r\n       Size              EntSize          Flags  Link  Info  Align\r\n  [ 0]                   NULL             0000000000000000  00000000\r\n       0000000000000000  0000000000000000           0     0     0\r\n  [ 1] .note.gnu.build-i NOTE             0000000000000200  00000200\r\n       0000000000000020  0000000000000000   A       0     0     4\r\n  [ 2] .gnu.hash         GNU_HASH         0000000000000220  00000220\r\n       00000000000ab0d0  0000000000000000   A       3     0     8\r\n  [ 3] .dynsym           DYNSYM           00000000000ab2f0  000ab2f0\r\n       00000000002928f0  0000000000000018   A       4     3     8\r\n  [ 4] .dynstr           STRTAB           000000000033dbe0  0033dbe0\r\n       00000000011555b2  0000000000000000   A       0     0     1\r\n  [ 5] .gnu.version      VERSYM           0000000001493192  01493192\r\n       0000000000036e14  0000000000000002   A       3     0     2\r\n  [ 6] .gnu.version_d    VERDEF           00000000014c9fa8  014c9fa8\r\n       0000000000000038  0000000000000000   A       4     2     8\r\n  [ 7] .gnu.version_r    VERNEED          00000000014c9fe0  014c9fe0\r\n       0000000000000350  0000000000000000   A       4    11     8\r\n  [ 8] .rela.dyn         RELA             00000000014ca330  014ca330\r\n       00000000004469e8  0000000000000018   A       3     0     8\r\n  [ 9] .init             PROGBITS         0000000001910d18  01910d18\r\n       000000000000001a  0000000000000000  AX       0     0     4\r\n  [10] .plt              PROGBITS         0000000001910d40  01910d40\r\n       0000000000000010  0000000000000010  AX       0     0     16\r\n  [11] .plt.got          PROGBITS         0000000001910d50  01910d50\r\n       00000000000312c0  0000000000000000  AX       0     0     8\r\n  [12] .text             PROGBITS         0000000001942040  01942040\r\n       0000000004f9b080  0000000000000000  AX       0     0     64\r\n  [13] text_env          PROGBITS         00000000068dd0c0  068dd0c0\r\n       00000000000029b4  0000000000000000  AX       0     0     16\r\n  [14] .fini             PROGBITS         00000000068dfa74  068dfa74\r\n       0000000000000009  0000000000000000  AX       0     0     4\r\n  [15] .rodata           PROGBITS         00000000068e0000  068e0000\r\n       0000000000b22bf0  0000000000000000   A       0     0     4096\r\n  [16] .nv_fatbin        PROGBITS         0000000007402bf0  07402bf0\r\n       000000000dc10ba8  0000000000000000   A       0     0     8\r\n  [17] .eh_frame_hdr     PROGBITS         0000000015013798  15013798\r\n       0000000000128404  0000000000000000   A       0     0     4\r\n  [18] .eh_frame         PROGBITS         000000001513bba0  1513bba0\r\n       0000000000672ddc  0000000000000000   A       0     0     8\r\n  [19] .gcc_except_table PROGBITS         00000000157ae97c  157ae97c\r\n       0000000000069e81  0000000000000000   A       0     0     4\r\n  [20] .tdata            PROGBITS         0000000015a18a68  15818a68\r\n       0000000000000008  0000000000000000 WAT       0     0     8\r\n  [21] .tbss             NOBITS           0000000015a18a70  15818a70\r\n       00000000000000b0  0000000000000000 WAT       0     0     8\r\n  [22] .init_array       INIT_ARRAY       0000000015a18a70  15818a70\r\n       0000000000003680  0000000000000000  WA       0     0     8\r\n  [23] .fini_array       FINI_ARRAY       0000000015a1c0f0  1581c0f0\r\n       0000000000000008  0000000000000000  WA       0     0     8\r\n  [24] .jcr              PROGBITS         0000000015a1c0f8  1581c0f8\r\n       0000000000000008  0000000000000000  WA       0     0     8\r\n  [25] .data.rel.ro      PROGBITS         0000000015a1c100  1581c100\r\n       00000000001228f8  0000000000000000  WA       0     0     32\r\n  [26] .dynamic          DYNAMIC          0000000015b3e9f8  1593e9f8\r\n       00000000000002c0  0000000000000010  WA       4     0     8\r\n  [27] .got              PROGBITS         0000000015b3ecb8  1593ecb8\r\n       0000000000062330  0000000000000008  WA       0     0     8\r\n  [28] .data             PROGBITS         0000000015ba1000  159a1000\r\n       000000000000e1ac  0000000000000000  WA       0     0     32\r\n  [29] .nvFatBinSegment  PROGBITS         0000000015baf1b0  159af1b0\r\n       00000000000011b8  0000000000000000  WA       0     0     8\r\n  [30] .bss              NOBITS           0000000015bb0380  159b0368\r\n       0000000000013808  0000000000000000  WA       0     0     64\r\n  [31] .comment          PROGBITS         0000000000000000  159b0368\r\n       0000000000000035  0000000000000001  MS       0     0     1\r\n  [32] .debug_aranges    PROGBITS         0000000000000000  159b039d\r\n       00000000002afd30  0000000000000000           0     0     1\r\n  [33] .debug_info       PROGBITS         0000000000000000  15c600cd\r\n       000000003bd2d579  0000000000000000           0     0     1\r\n  [34] .debug_abbrev     PROGBITS         0000000000000000  5198d646\r\n       000000000080f1d5  0000000000000000           0     0     1\r\n  [35] .debug_line       PROGBITS         0000000000000000  5219c81b\r\n       000000000259eba1  0000000000000000           0     0     1\r\n  [36] .debug_str        PROGBITS         0000000000000000  5473b3bc\r\n       00000000767919c5  0000000000000001  MS       0     0     1\r\n  [37] .debug_loc        PROGBITS         0000000000000000  caeccd81\r\n       00000000120ceea9  0000000000000000           0     0     1\r\n  [38] .debug_ranges     PROGBITS         0000000000000000  dcf9bc2a\r\n       000000000551db30  0000000000000000           0     0     1\r\n  [39] .shstrtab         STRTAB           0000000000000000  e5913cef\r\n       00000000000001a1  0000000000000000           0     0     1\r\n  [40] .symtab           SYMTAB           0000000000000000  e24b9760\r\n       00000000005f9c28  0000000000000018          41   148688     8\r\n  [41] .strtab           STRTAB           0000000000000000  e2ab3388\r\n       0000000002e60967  0000000000000000           0     0     1\r\n```\r\nBut in this way, I couldn't debug cuda-related code with `cuda-gdb` after I dumped the sections information about the cubin files on the `_pywrap_tensorflow_internal.so` with `cuobjdump`\r\n\r\n**Question**\r\n* How to build a debug version of TensorFlow with bazel ?", "comments": ["Try this: https://github.com/tensorflow/tensorflow/issues/27495#issuecomment-481073927", "The method is to pass `-g` to compiler option, which is the same with ` --cxxopt='-g'` and used to debug the code for CPU rather than GPU.  Now I want to trace the runtime for CPU and GPU, where the compiler option `-G` may be passed for `nvcc`. ", "> The method is to pass `-g` to compiler option, which is the same with ` --cxxopt='-g'` and used to debug the code for CPU rather than GPU. Now I want to trace the runtime for CPU and GPU, where the compiler option `-G` may be passed for `nvcc`.\r\n\r\nI never tried that before, but my hunch is that passing `-G` for `*.cu.cc` would work.", "After I read the file `third_party/gpus/cuda_configure.bzl` and `third_party/gpus/cuda/build_defs.bzl.tpl` and tried other methods, I couldn't find a simple way to build tensorflow for cuda-gdb. The command line option `--per_file_copt`  is a choice.", "@qzhong0605  hi, I also want to debug tensorflow c/c++ source code with gdb, as in #27495 , I tried your method to add '-g' but the build process never finishes,  did you met the same issue? thanks.\r\n\r\nI tried with:\r\nbazel build --cxxopt='-g' //tensorflow/tools/lib_package:libtensorflow_test\r\n\r\nand it stops at the below line for hours:\r\n[7,070 / 7,073] Linking tensorflow/libtensorflow.so; 10322s local\r\n\r\nat this time, libtensorflow.so is there, but as data format.\r\n$ file /work/ml/tensorflow_work/tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow.so\r\n/work/ml/tensorflow_work/tensorflow/bazel-out/k8-opt/bin/tensorflow/libtensorflow.so: data\r\n\r\nIt is the exactly the same when i tried with:\r\nbazel test -c dbg //tensorflow/tools/lib_package:libtensorflow_test\r\n", "The environment for me: \r\n* gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\r\n* Cuda compilation tools, release 9.0, V9.0.176\r\n* clang version 8.0.0 (tags/RELEASE_800/final)\r\n* nvidia-docker os \r\n\r\nI had tried the command as you did,  and I had succeed. The result was the following: \r\n```\r\nTarget //tensorflow/tools/lib_package:libtensorflow_test up-to-date:\r\n  bazel-bin/tensorflow/tools/lib_package/libtensorflow_test\r\nINFO: Elapsed time: 1462.251s, Critical Path: 661.05s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 4967 processes: 4967 local.\r\nINFO: Build completed successfully, 6074 total actions\r\n```\r\nThe output for the command `file bazel-out/k8-opt/bin/tensorflow/libtensorflow.so` was the following: \r\n```\r\nbazel-out/k8-opt/bin/tensorflow/libtensorflow.so: ELF 64-bit LSB shared object, x86-64, version 1 (GNU/Linux), dynamically linked, BuildID[md5/uuid]=50d05d0a53f66565dede0bbb52abdec7, not stripped\r\n```\r\nAnd the file `bazel-out/k8-opt/bin/tensorflow/libtensorflow.so` has `debug` sections: \r\n```\r\nSection Headers:\r\n  [Nr] Name              Type             Address           Offset\r\n       Size              EntSize          Flags  Link  Info  Align\r\n  [ 0]                   NULL             0000000000000000  00000000\r\n       0000000000000000  0000000000000000           0     0     0\r\n  [ 1] .note.gnu.build-i NOTE             0000000000000200  00000200\r\n       0000000000000020  0000000000000000   A       0     0     4\r\n  [ 2] .gnu.hash         GNU_HASH         0000000000000220  00000220\r\n       0000000000000974  0000000000000000   A       3     0     8\r\n  [ 3] .dynsym           DYNSYM           0000000000000b98  00000b98\r\n       000000000000fae0  0000000000000018   A       4     3     8\r\n  [ 4] .dynstr           STRTAB           0000000000010678  00010678\r\n       0000000000024d7a  0000000000000000   A       0     0     1\r\n  [ 5] .gnu.version      VERSYM           00000000000353f2  000353f2\r\n       00000000000014e8  0000000000000002   A       3     0     2\r\n  [ 6] .gnu.version_d    VERDEF           00000000000368e0  000368e0\r\n       0000000000000038  0000000000000000   A       4     2     8\r\n  [ 7] .gnu.version_r    VERNEED          0000000000036918  00036918\r\n       0000000000000350  0000000000000000   A       4    11     8\r\n  [ 8] .rela.dyn         RELA             0000000000036c68  00036c68\r\n       00000000002e2798  0000000000000018   A       3     0     8\r\n  [ 9] .init             PROGBITS         0000000000319400  00319400\r\n       000000000000001a  0000000000000000  AX       0     0     4\r\n  [10] .plt              PROGBITS         0000000000319420  00319420\r\n       0000000000000010  0000000000000010  AX       0     0     16\r\n  [11] .plt.got          PROGBITS         0000000000319430  00319430\r\n       00000000000044a8  0000000000000000  AX       0     0     8\r\n  [12] .text             PROGBITS         000000000031d900  0031d900\r\n       0000000003a96370  0000000000000000  AX       0     0     64\r\n  [13] text_env          PROGBITS         0000000003db3c70  03db3c70\r\n       00000000000029b4  0000000000000000  AX       0     0     16\r\n  [14] .fini             PROGBITS         0000000003db6624  03db6624\r\n       0000000000000009  0000000000000000  AX       0     0     4\r\n  [15] .rodata           PROGBITS         0000000003db7000  03db7000\r\n       000000000083d810  0000000000000000   A       0     0     4096\r\n  [16] .nv_fatbin        PROGBITS         00000000045f4810  045f4810\r\n       000000000c89ec80  0000000000000000   A       0     0     8\r\n  [17] .eh_frame_hdr     PROGBITS         0000000010e93490  10e93490\r\n       00000000000f2ecc  0000000000000000   A       0     0     4\r\n  [18] .eh_frame         PROGBITS         0000000010f86360  10f86360\r\n       0000000000531624  0000000000000000   A       0     0     8\r\n  [19] .gcc_except_table PROGBITS         00000000114b7984  114b7984\r\n       0000000000054661  0000000000000000   A       0     0     4\r\n  [20] .tdata            PROGBITS         000000001170c2f8  1150c2f8\r\n       0000000000000008  0000000000000000 WAT       0     0     8\r\n  [21] .tbss             NOBITS           000000001170c300  1150c300\r\n       0000000000000120  0000000000000000 WAT       0     0     8\r\n  [22] .init_array       INIT_ARRAY       000000001170c300  1150c300\r\n       00000000000032b0  0000000000000000  WA       0     0     8\r\n  [23] .fini_array       FINI_ARRAY       000000001170f5b0  1150f5b0\r\n       0000000000000008  0000000000000000  WA       0     0     8\r\n  [24] .jcr              PROGBITS         000000001170f5b8  1150f5b8\r\n       0000000000000008  0000000000000000  WA       0     0     8\r\n  [25] .data.rel.ro      PROGBITS         000000001170f5c0  1150f5c0\r\n       0000000000106bf8  0000000000000000  WA       0     0     32\r\n  [26] .dynamic          DYNAMIC          00000000118161b8  116161b8\r\n       00000000000002b0  0000000000000010  WA       4     0     8\r\n  [27] .got              PROGBITS         0000000011816468  11616468\r\n       0000000000005b90  0000000000000008  WA       0     0     8\r\n  [28] .data             PROGBITS         000000001181c000  1161c000\r\n       0000000000005fec  0000000000000000  WA       0     0     32\r\n  [29] .nvFatBinSegment  PROGBITS         0000000011821ff0  11621ff0\r\n       00000000000010f8  0000000000000000  WA       0     0     8\r\n  [30] .bss              NOBITS           0000000011823100  116230e8\r\n       000000000000e128  0000000000000000  WA       0     0     64\r\n  [31] .comment          PROGBITS         0000000000000000  116230e8\r\n       0000000000000035  0000000000000001  MS       0     0     1\r\n  [32] .debug_aranges    PROGBITS         0000000000000000  1162311d\r\n       0000000000234bb0  0000000000000000           0     0     1\r\n  [33] .debug_info       PROGBITS         0000000000000000  11857ccd\r\n       00000000307d72b0  0000000000000000           0     0     1\r\n  [34] .debug_abbrev     PROGBITS         0000000000000000  4202ef7d\r\n       0000000000723bc3  0000000000000000           0     0     1\r\n  [35] .debug_line       PROGBITS         0000000000000000  42752b40\r\n       0000000001d4e671  0000000000000000           0     0     1\r\n  [36] .debug_str        PROGBITS         0000000000000000  444a11b1\r\n       000000006b9e0e21  0000000000000001  MS       0     0     1\r\n  [37] .debug_loc        PROGBITS         0000000000000000  afe81fd2\r\n       000000000d3d5e1a  0000000000000000           0     0     1\r\n  [38] .debug_ranges     PROGBITS         0000000000000000  bd257dec\r\n       00000000041aeda0  0000000000000000           0     0     1\r\n  [39] .shstrtab         STRTAB           0000000000000000  c3fa6619\r\n       00000000000001a1  0000000000000000           0     0     1\r\n  [40] .symtab           SYMTAB           0000000000000000  c1406b90\r\n       00000000004e7c38  0000000000000018          41   211645     8\r\n  [41] .strtab           STRTAB           0000000000000000  c18ee7c8\r\n       00000000026b7e51  0000000000000000           0     0     1\r\n```\r\n\r\nMaybe the file  `bazel-out/k8-opt/bin/tensorflow/libtensorflow.so` was too large for you disk:\r\n```\r\n3.1G Apr 17 07:39 bazel-out/k8-opt/bin/tensorflow/libtensorflow.so\r\n```\r\n* Switch compiler from `gcc` to `clang`\r\nNow I have change the compiler from `gcc` to `clang`, which can build for `cuda`.  The command for me to build is the following:\r\n```\r\nbazel build  --cxxopt=\"-g\" --config=cuda --define=using_cuda_nvcc=false --define=using_cuda_clang=true --incompatible_disallow_data_transition=false //tensorflow/tools/pip_package:build_pip_package\r\n```", "thanks, i tried again with '-g' option, it takes ~6 hours to finish ...\r\n\r\n`\r\nINFO: Elapsed time: 24108.686s, Critical Path: 21192.22s\r\n`", "You could choose a disk with higher bandwidth,  such as 200MB/s, for me.  Or you could mount a ram disk and flush them to disk later. ", "thanks,\r\n\r\ni added an SSD disk as /dev/sdb1, and mount it to ~/.cache, it takes less time to finish the Linking.\r\n`INFO: Elapsed time: 10584.843s, Critical Path: 5718.86s`\r\n\r\nLooks that 535MB/s is still not enough for me.\r\n`$ sudo hdparm -t /dev/sdb1`\r\n`/dev/sdb1:\r\n Timing buffered disk reads: 1608 MB in  3.00 seconds = 535.45 MB/sec\r\n`\r\n\r\n`$ sudo hdparm -t /dev/sda1`\r\n`/dev/sda1:\r\n Timing buffered disk reads: 512 MB in  2.52 seconds = 202.97 MB/sec\r\n`\r\n\r\nMy physical memory is 8G, looks too small to try the ram disk method.", "@qzhong0605 Could you please try on the latest TF v2.6.0 ? Please refer to the similar [issue](https://github.com/tensorflow/tensorflow/issues/45570) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27744\">No</a>\n"]}]