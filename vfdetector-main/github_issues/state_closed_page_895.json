[{"number": 26629, "title": "Keras Error: Variable does not exist, or was not created with tf.get_variable()", "body": "```\r\nimport tensorflow.keras as ks\r\ndef scope_error_test():\r\n    input_holder = tf.placeholder(dtype=tf.float32,\r\n                                  shape=(None, 368, 368, 3),\r\n                                  name='input')\r\n    with tf.variable_scope(\"scope_1\"):\r\n        with tf.variable_scope(\"scope_2\"):\r\n            conv1 = ks.layers.Conv2D(kernel_size=7,\r\n                                     filters=64,\r\n                                     strides=2,\r\n                                     padding='same',\r\n                                     activation=tf.nn.relu,\r\n                                     name='conv1')(input_holder)\r\n            pool1 = ks.layers.MaxPool2D(pool_size=3, padding='same',\r\n                                        strides=2,\r\n                                        name='pool1')(inputs=conv1)\r\n    print(pool1.get_shape().as_list())\r\nwith tf.Session() as sess:\r\n        scope_error_test()\r\n        sess.run(tf.global_variables_initializer())\r\n        print(tf.global_variables())\r\n        with tf.variable_scope('', reuse=True):\r\n            for variable in tf.global_variables():\r\n                var_name = variable.name.split(':')[0]\r\n                var_tf = tf.get_variable(var_name)\r\n```\r\n\r\n\r\nIn this example code, I want to understand why I get this error: ValueError: Variable scope_1/scope_2/conv1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\n\r\nNote: I used tensorflow.contrib.layers instead of tensorflow.keras.layers and It was working correctly! Could be a bug !?\r\n\r\nI have tried tensorflow 1.9.0 and tensorflow 1.12.0", "comments": ["@umbrellalalalala Thanks for your answer. However, it does not work. Did you try it on your machine before write it here ? I have already provide  a minimal working example to help other reproduce the error and try to correct it.", "@Gouiaa All right, the question in this page has no code indent, and I mistook it as a problem I solved the other day. I'm sorry that before I answered on your question page, I didn't read your indented question again.\n\nI'm curious about this problem after read it again. If I work it out on my pycharm, I will write it here.", "Why do you have \"with tf.variable_scope('', reuse=True):\"?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26629\">No</a>\n"]}, {"number": 26628, "title": "Pypi file for tensorflow-gpu 1.10.1 for Python 3.6 manylinux is zero bytes", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2012.3\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.10.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have a requirements.txt file that contains the following:\r\n\r\n```\r\ntensorflow-gpu==1.10.1 \\\r\n    --hash=sha512:cf7ee7c41166f936c06b4041efab5d901019cc0b6caba69e6645bcb5886243dfcf072f71119692eec31359585da3bede39f208e75e3b7dc84d9c99fe33cf18d6 \\\r\n    --hash=sha512:ae6d1c332e8128435f61cab9ae8a770aac897eafa94f4abc4937a9d288622bc717e73663f1941f0211b6effadf6ac08d20a70829a48e5663f2f3638e8e61b9ff\r\n```\r\nI use the following command in my Dockerfile to install tensorflow-gpu and other dependencies:\r\n```\r\nRUN pip install --no-deps --upgrade-strategy only-if-needed --no-cache-dir --require-hashes -r ./$REQUIREMENTS_TF_FILE\r\n```\r\nHowever, I get an error that the hash of the file downloaded from Pypi doesn't match the hash in my requirements file. I went to the Pypi website and tried to download the file [tensorflow_gpu-1.10.1-cp36-cp36m-manylinux1_x86_64.whl](https://files.pythonhosted.org/packages/3f/c3/000755084b5e7b5a11df1b9166a54936075ec280b7a615cecce42973fc8b/tensorflow_gpu-1.10.1-cp36-cp36m-manylinux1_x86_64.whl ). I found that the file is zero bytes!\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Apologies for the delay in response. Is this still an issue? I did download the tf wheel from the link you provided, the file size is around 253 mb. Can you please confirm? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26628\">No</a>\n", "@ymodak The file downloads fine now. I'm not sure what the issue was, but it looks like it got resolved. Thanks"]}, {"number": 26626, "title": "TOCO failed for tensorflow model using Universal Sentence Encoder module ", "body": "Hi, all. \r\n\r\nI'm trying to export a TensorFlow model to the TensorFlow Lite format for eventual use in Firebase ML Kit.\r\n\r\nThe model is a one-step Universal Sentence Encoder text embedding using the TensorFlow Hub module. I'm getting a converter error that seems to be due to unsupported ops. \r\n\r\nTried setting the `allow_custom_ops` and `target_ops` to no avail.\r\n\r\nWould anyone be kind enough to point out any silly mistakes I've made, or point me to other options for converting USE to a TensorFlow Lite model?\r\n\r\n<hr>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave Version 10.14.1\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 1.13.1\r\n- Python version: 3.7.0\r\n- Code running out of a Jupyter Notebook (hence the printed traceback at the top)\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\nI0312 15:25:26.630760 4626404800 saver.py:1483] Saver not created because there are no variables in the graph to restore\r\nINFO:tensorflow:Froze 95 variables.\r\nI0312 15:25:33.715335 4626404800 graph_util_impl.py:268] Froze 95 variables.\r\nINFO:tensorflow:Converted 95 variables to const ops.\r\nI0312 15:25:36.719951 4626404800 graph_util_impl.py:301] Converted 95 variables to const ops.\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-24-d7f2951cbdfb> in <module>\r\n     14 \r\n     15     converter = tf.lite.TFLiteConverter.from_session(session, [input_text], [embedding])\r\n---> 16     tflite_model = converter.convert()\r\n     17     open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n~/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    453           input_tensors=self._input_tensors,\r\n    454           output_tensors=self._output_tensors,\r\n--> 455           **converter_kwargs)\r\n    456     else:\r\n    457       result = _toco_convert_graph_def(\r\n\r\n~/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    440   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    441                              toco_flags.SerializeToString(),\r\n--> 442                              input_data.SerializeToString())\r\n    443   return data\r\n    444 \r\n\r\n~/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    203       stderr = _try_convert_to_unicode(stderr)\r\n    204       raise ConverterError(\r\n--> 205           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    206   finally:\r\n    207     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-03-12 15:25:49.976435: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: HashTableV2\r\n2019-03-12 15:25:49.993799: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-03-12 15:25:50.245977: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246040: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246082: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246103: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246138: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246169: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246204: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246234: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246265: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246289: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246317: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246350: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246380: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246413: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246441: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246478: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246508: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246542: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246570: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246603: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246631: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246663: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246690: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246709: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246741: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246769: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246800: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246829: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.246879: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringJoin\r\n2019-03-12 15:25:50.247652: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-12 15:25:50.247691: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringSplit\r\n2019-03-12 15:25:50.248189: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SparseFillEmptyRows\r\n2019-03-12 15:25:50.248347: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-03-12 15:25:50.248362: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-12 15:25:50.248417: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringToHashBucketFast\r\n2019-03-12 15:25:50.248447: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LookupTableFindV2\r\n2019-03-12 15:25:50.248461: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LookupTableSizeV2\r\n2019-03-12 15:25:50.248574: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-03-12 15:25:50.248717: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Size\r\n2019-03-12 15:25:50.249474: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Cos\r\n2019-03-12 15:25:50.249814: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-03-12 15:25:50.250088: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.250380: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.250680: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.251614: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.252034: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-12 15:25:50.252208: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.252508: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.252756: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-12 15:25:50.253019: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.253320: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.253626: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.254453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.254821: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-12 15:25:50.254965: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.255260: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.255486: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-12 15:25:50.255722: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.255989: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.256269: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.257084: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.257453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-12 15:25:50.257599: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.258020: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.258314: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-12 15:25:50.258708: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.259042: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.259322: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.260269: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.260736: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-12 15:25:50.260881: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.261149: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.261371: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-12 15:25:50.261605: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.261883: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.262145: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.262942: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.263375: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-12 15:25:50.263510: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.263780: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.263991: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-12 15:25:50.264199: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.264456: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.264696: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.265468: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.265813: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-12 15:25:50.265944: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.266190: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-12 15:25:50.266453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-12 15:25:50.661198: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1583 operators, 2816 arrays (0 quantized)\r\n2019-03-12 15:25:50.712913: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1583 operators, 2816 arrays (0 quantized)\r\n2019-03-12 15:25:50.824860: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1077 operators, 1955 arrays (0 quantized)\r\n2019-03-12 15:25:50.938753: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 1040 operators, 1918 arrays (0 quantized)\r\n2019-03-12 15:25:50.987522: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 1039 operators, 1916 arrays (0 quantized)\r\n2019-03-12 15:25:51.045394: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1039 operators, 1916 arrays (0 quantized)\r\n2019-03-12 15:25:51.086728: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 192 bytes, theoretical optimal value: 192 bytes.\r\n2019-03-12 15:25:51.104668: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EQUAL, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, LESS, LOGICAL_NOT, MAXIMUM, MEAN, MUL, NOT_EQUAL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RSQRT, SELECT, SHAPE, SIN, SLICE, SOFTMAX, SPARSE_TO_DENSE, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, Cos, DynamicPartition, DynamicStitch, GatherNd, HashTableV2, ListDiff, LookupTableFindV2, LookupTableSizeV2, RegexReplace, ScatterNd, Size, SparseFillEmptyRows, StringJoin, StringSplit, StringToHashBucketFast, Where.\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/ejonokuchi/.virtualenvs/py371/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/ejonokuchi/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/ejonokuchi/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Users/ejonokuchi/.virtualenvs/py371/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EQUAL, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, LESS, LOGICAL_NOT, MAXIMUM, MEAN, MUL, NOT_EQUAL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RSQRT, SELECT, SHAPE, SIN, SLICE, SOFTMAX, SPARSE_TO_DENSE, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul, Cos, DynamicPartition, DynamicStitch, GatherNd, HashTableV2, ListDiff, LookupTableFindV2, LookupTableSizeV2, RegexReplace, ScatterNd, Size, SparseFillEmptyRows, StringJoin, StringSplit, StringToHashBucketFast, Where.\r\n\r\n```\r\n\r\nModel code:\r\n```\r\nmodule_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/3'\r\n\r\nuniversal_sentence_encoder = hub.Module(\r\n    module_url,\r\n    trainable=False,\r\n    name='universal_sentence_encoder',\r\n    tags=None\r\n)\r\n\r\nwith tf.Session() as session:\r\n    session.run(tf.global_variables_initializer())\r\n    session.run(tf.tables_initializer())\r\n    \r\n    input_text = tf.placeholder(tf.string, shape=[None])\r\n    embedding = universal_sentence_encoder(input_text)\r\n\r\n    # these flags were set in response to the traceback message\r\n    tf.lite.TFLiteConverter.allow_custom_ops = True\r\n    tf.lite.TFLiteConverter.target_ops = set([\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS\r\n    ])\r\n\r\n    converter = tf.lite.TFLiteConverter.from_session(session, [input_text], [embedding])\r\n    tflite_model = converter.convert()\r\n    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```", "comments": ["Didn't think it was caused by a Python versioning issue, but just for good measure I replicated this on Python 3.6.7 as well", "Traceback (most recent call last):\r\n  File \"test1.py\", line 31, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/google/home/haoliang/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/google/home/haoliang/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/google/home/haoliang/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-03-13 23:14:28.455244: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: HashTableV2\r\n2019-03-13 23:14:28.463438: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-03-13 23:14:28.490493: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490549: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490568: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490583: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490598: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490613: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490627: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490641: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490668: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490682: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490697: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490713: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490728: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490751: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490768: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490785: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490800: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490816: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490831: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490846: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490861: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490877: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490891: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490909: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490925: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490941: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490957: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490972: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.490992: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringJoin\r\n2019-03-13 23:14:28.491025: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: RegexReplace\r\n2019-03-13 23:14:28.491041: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringSplit\r\n2019-03-13 23:14:28.491055: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: SparseFillEmptyRows\r\n2019-03-13 23:14:28.491111: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-03-13 23:14:28.491124: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-13 23:14:28.491136: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StringToHashBucketFast\r\n2019-03-13 23:14:28.491148: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LookupTableFindV2\r\n2019-03-13 23:14:28.491158: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LookupTableSizeV2\r\n2019-03-13 23:14:28.491201: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-03-13 23:14:28.491263: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Size\r\n2019-03-13 23:14:28.491554: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Cos\r\n2019-03-13 23:14:28.491701: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-03-13 23:14:28.491817: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.491953: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.492095: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.492458: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.492606: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-13 23:14:28.492668: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.492788: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.492889: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-13 23:14:28.492986: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.493108: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.493218: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.493532: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.493662: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-13 23:14:28.493716: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.493819: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.493912: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-13 23:14:28.493997: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.494094: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.494189: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.494496: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.494629: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-13 23:14:28.494684: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.494827: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.494920: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-13 23:14:28.495011: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.495116: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.495220: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.495536: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.495669: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-13 23:14:28.495726: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.495847: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.495947: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-13 23:14:28.496030: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.496124: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.496222: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.496528: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.496663: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-13 23:14:28.496718: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.496825: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.496935: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-13 23:14:28.497025: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.497125: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.497228: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.497530: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.497671: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: GatherNd\r\n2019-03-13 23:14:28.497723: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.497815: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ListDiff\r\n2019-03-13 23:14:28.497920: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: ScatterNd\r\n2019-03-13 23:14:28.576453: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1583 operators, 2816 arrays (0 quantized)\r\n2019-03-13 23:14:28.633880: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1583 operators, 2816 arrays (0 quantized)\r\n2019-03-13 23:14:28.741150: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1077 operators, 1955 arrays (0 quantized)\r\n2019-03-13 23:14:28.852369: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 1040 operators, 1918 arrays (0 quantized)\r\n2019-03-13 23:14:28.904719: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 1039 operators, 1916 arrays (0 quantized)\r\n2019-03-13 23:14:28.967839: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1039 operators, 1916 arrays (0 quantized)\r\n2019-03-13 23:14:29.022003: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 192 bytes, theoretical optimal value: 192 bytes.\r\n2019-03-13 23:14:29.034359: W tensorflow/lite/toco/tflite/operator.cc:1768] Op HashTableV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034380: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034387: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034392: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034398: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034404: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034409: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034413: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034418: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034422: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034427: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034431: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034436: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034440: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034444: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034449: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034453: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034458: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034463: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034467: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034472: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034476: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034481: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034487: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034491: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034496: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034500: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034504: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034509: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034519: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034526: W tensorflow/lite/toco/tflite/operator.cc:1768] Op StringSplit is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034542: W tensorflow/lite/toco/tflite/operator.cc:1768] Op StringToHashBucketFast is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034548: W tensorflow/lite/toco/tflite/operator.cc:1768] Op LookupTableFindV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034553: W tensorflow/lite/toco/tflite/operator.cc:1768] Op LookupTableSizeV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034643: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034692: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034738: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034798: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034848: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034893: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034953: W tensorflow/lite/toco/tflite/operator.cc:1768] Op HashTableV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034979: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034987: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.034994: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035001: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035006: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035011: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035015: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035019: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035024: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035028: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035033: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035038: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035044: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035049: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035055: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035059: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035065: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035069: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035074: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035078: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035083: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035087: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035092: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035096: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035100: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035108: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035112: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035118: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035127: W tensorflow/lite/toco/tflite/operator.cc:1768] Op RegexReplace is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035133: W tensorflow/lite/toco/tflite/operator.cc:1768] Op StringSplit is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035147: W tensorflow/lite/toco/tflite/operator.cc:1768] Op StringToHashBucketFast is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035154: W tensorflow/lite/toco/tflite/operator.cc:1768] Op LookupTableFindV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035162: W tensorflow/lite/toco/tflite/operator.cc:1768] Op LookupTableSizeV2 is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035255: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035311: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035365: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035419: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035472: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035526: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-03-13 23:14:29.035717: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EQUAL, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, LESS, LOGICAL_NOT, MAXIMUM, MEAN, MUL, NOT_EQUAL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RSQRT, SELECT, SHAPE, SIN, SLICE, SOFTMAX, SPARSE_TO_DENSE, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: DynamicPartition, DynamicStitch, HashTableV2, LookupTableFindV2, LookupTableSizeV2, RegexReplace, ScatterNd, StringSplit, StringToHashBucketFast.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/google/home/haoliang/tensorflow_venv/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/google/home/haoliang/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/google/home/haoliang/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/google/home/haoliang/tensorflow_venv/local/lib/python2.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, DIV, EQUAL, EXPAND_DIMS, FLOOR_DIV, FLOOR_MOD, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, LESS, LOGICAL_NOT, MAXIMUM, MEAN, MUL, NOT_EQUAL, PACK, PAD, RANGE, REDUCE_MAX, RESHAPE, RSQRT, SELECT, SHAPE, SIN, SLICE, SOFTMAX, SPARSE_TO_DENSE, SQUARE, SQUEEZE, STRIDED_SLICE, SUB, SUM, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: DynamicPartition, DynamicStitch, HashTableV2, LookupTableFindV2, LookupTableSizeV2, RegexReplace, ScatterNd, StringSplit, StringToHashBucketFast.\r\n\r\n\r\nI could reproduce your error, and I think the reason is that some TF ops in your model are not available as a flex op in TF Lite, which means that we can't use the TF version of those ops. For example:\r\n2019-03-13 23:14:29.035419: W tensorflow/lite/toco/tflite/operator.cc:1768] Op ScatterNd is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.", "Thanks for the update @haozha111. Do you know when these TF ops will be available in TF Lite?", "We are actively working on adding more built-in ops and support more flex ops. Currently I'm looking into supporting Hashtable ops in flex. But it should take a while (at least 1-2 months). You can always implement those custom ops if not too hard. Thank you."]}, {"number": 26625, "title": "tf.sparse_tensor_to_dense does not have a gradient", "body": "This issue has never been solved, why so eager to close it?\r\nsee #6391, #22543", "comments": ["On a first read those two issues look different to me.\r\n\r\nCan you put here an example of the code you think should work?\r\n\r\n(and sorry for the unintentional closing of non-fixed issues)", "import tensorflow as tf\r\nindices = tf.placeholder(tf.int64, (None, 2))\r\nvalues = tf.placeholder(tf.float32, (None,))\r\nsparse_tensor = tf.SparseTensor(indices, values, (5, 7))\r\ndense_tensor1 = tf.sparse_tensor_to_dense(sparse_tensor)\r\ndense_tensor2 = tf.sparse_to_dense(indices, (5, 7), values)\r\ndense_tensor3 = tf.sparse_add(tf.zeros((5, 7)), sparse_tensor)\r\ndense_tensor4 = tf.sparse_tensor_dense_matmul(sparse_tensor, tf.zeros((7, 5)))\r\nsum1 = tf.reduce_sum(dense_tensor1)\r\nsum2 = tf.reduce_sum(dense_tensor2)\r\nsum3 = tf.reduce_sum(dense_tensor3)\r\nsum4 = tf.reduce_sum(dense_tensor4)\r\nprint(tf.gradients(sum1, values))\r\nprint(tf.gradients(sum2, values))\r\nprint(tf.gradients(sum3, values))\r\nprint(tf.gradients(sum4, values))", "Thanks! Indeed in all of these cases the correct gradient is not None,\nwhile now I get\n```\n[None] [None] [<tf.Tensor\n'gradients_6/SparseTensorDenseAdd_1_grad/GatherNd:0' shape=(?,)\ndtype=float32>] [<tf.Tensor\n'gradients_7/SparseTensorDenseMatMul_1/SparseTensorDenseMatMul_grad/Sum:0'\nshape=(?,) dtype=float32>]\n```\n\nOn Fri, Mar 15, 2019 at 8:33 AM chwang <notifications@github.com> wrote:\n\n> import tensorflow as tf\n> indices = tf.placeholder(tf.int64, (None, 2))\n> values = tf.placeholder(tf.float32, (None,))\n> sparse_tensor = tf.SparseTensor(indices, values, (5, 7))\n> dense_tensor1 = tf.sparse_tensor_to_dense(sparse_tensor)\n> dense_tensor2 = tf.sparse_to_dense(indices, (5, 7), values)\n> dense_tensor3 = tf.sparse_add(tf.zeros((5, 7)), sparse_tensor)\n> dense_tensor4 = tf.sparse_tensor_dense_matmul(sparse_tensor, tf.zeros((7,\n> 5)))\n> sum1 = tf.reduce_sum(dense_tensor1)\n> sum2 = tf.reduce_sum(dense_tensor2)\n> sum3 = tf.reduce_sum(dense_tensor3)\n> sum4 = tf.reduce_sum(dense_tensor4)\n> print(tf.gradients(sum1, values))\n> print(tf.gradients(sum2, values))\n> print(tf.gradients(sum3, values))\n> print(tf.gradients(sum4, values))\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26625#issuecomment-473330342>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbHnalqlZvHhOWZxpq5QznPJbozrks5vW70ugaJpZM4brnr6>\n> .\n>\n\n\n-- \n - Alex\n", "I confirm that this has been fixed in tf nightly.", "Thanks. "]}, {"number": 26624, "title": "delete me pls", "body": "delete pls", "comments": []}, {"number": 26623, "title": "DetectNet alternative", "body": "Are there any common alternatives (in tensorflow ecosystem) to Nvidia DetectNet network (object detection), \r\nwhich use bbox layer?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 26622, "title": "Export tf.lookup.TextFileIndex in 2.0", "body": "**System information**\r\n- TensorFlow version: `2.0.0.dev20190311`\r\n- Are you willing to contribute it: No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe exported class `tf.lookup.TextFileInitializer` takes as argument `key_index` and `value_index` but the enum `TextFileIndex` is not available via the API.\r\n\r\nThe `tf.lookup` module should export this enum.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThis will add a new symbol covered by backward compatibility guarantees.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers of `tf.lookup.TextFileInitializer`.", "comments": ["Added PR #26644 for the fix."]}, {"number": 26621, "title": "Compilation error of TensorFlow Lite on Windows", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): I don't use python since building TensorFlow Lite\r\n- Python version: See above\r\n- Bazel version (if compiling from source): See above\r\n- GCC/Compiler version (if compiling from source): gcc (Rev2, Built by MSYS2 project) 8.3.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\n\r\nCompilation error\r\n\r\n```\r\ntensorflow/lite/experimental/c/c_api.cc:80:29: error: function 'void TFL_InterpreterOptionsSetErrorReporter(TFL_InterpreterOptions*, void (*)(void*, const char*, va_list), void*)' definition is marked dllimport\r\n TFL_CAPI_EXPORT extern void TFL_InterpreterOptionsSetErrorReporter(\r\n                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccess to compile\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nSee tensorflow/lite/experimental/c/c_api.cc This is broken C syntax when TFL_CAPI_EXPORT is `__declspec(dllimport)`\r\n\r\nThis is small step to way to reproduce on Windows.\r\n\r\nfoo.cc\r\n```\r\n__declspec(dllimport) extern void foo() {\r\n}\r\n```\r\n```\r\n$ g++ foo.cc\r\nfoo.cc:1:35: error: function 'void foo()' definition is marked dllimport\r\n __declspec(dllimport) extern void foo() {\r\n```\r\n\r\n**Other info / logs**\r\n\r\nSee above\r\n\r\nI created pull-request https://github.com/tensorflow/tensorflow/pull/26615\r\n", "comments": ["Closed by #26615\r\n"]}, {"number": 26620, "title": "No improvement in performance of mobilenet_v1_1.0_224 on TFLite for GPU", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8.0.0\r\n- Mobile device: Samsung S9+, Sony XPeria XZ2\r\n- TensorFlow installed from (source or binary): binary `'org.tensorflow:tensorflow-lite:0.0.0-gpu-experimental'`\r\n- TensorFlow version (use command below): - (used pretrained *.tflite file)\r\n- Python version: - (used pretrained *.tflite file)\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: Exynos 9810 Octa (Samsung), Sanpdragon 845 (Sony)\r\n\r\n**Describe the current behavior**\r\n\r\nNo differences in performance for Samsung S9+ between CPU and GPU versions.\r\n\r\n**Describe the expected behavior**\r\n\r\nAccording to [this TensorFlow's article on Medium](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7) expected at least 3x improvement.\r\n\r\n**Code to reproduce the issue**\r\nTFLite archive for MobileNet v1 (224x224) has been downloaded from link from [article, mentioned above](https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7)\r\n\r\nCode:\r\n```java\r\n    private final int ImageHeight = 224;\r\n    private final int ImageWidth = 224;\r\n    private final int ImageChannels = 3;\r\n    private final int ImageSize = ImageHeight * ImageWidth * ImageChannels;\r\n\r\n    private final String tfliteFileName = \"mobilenet_v1_1.0_224.tflite\";\r\n\r\n    public String run(Context context, AssetManager assetManager) throws IOException {\r\n        AssetFileDescriptor fd = assetManager.openFd(tfliteFileName);\r\n        FileChannel fileChannel = new FileInputStream(fd.getFileDescriptor()).getChannel();\r\n        MappedByteBuffer model =\r\n            fileChannel.map(FileChannel.MapMode.READ_ONLY, fd.getStartOffset(), fd.getDeclaredLength());\r\n\r\n        int runCount = 100;\r\n        long elapsedTime = 0;\r\n\r\n        GpuDelegate delegate = new GpuDelegate();\r\n        Interpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);\r\n\r\n        try(Interpreter tflite = new Interpreter(model, options)) {\r\n            Map<Integer, Object> output = new HashMap<Integer, Object>();\r\n            output.put(Integer.valueOf(0), new float[1][1001]);\r\n\r\n            Object inputData = 0;\r\n            float[][][][] in = new float[1][ImageHeight][ImageWidth][ImageChannels];\r\n            // For example: filling blob uniformly from -5. to 5. in Channel-first format.\r\n            int pos = 0;\r\n            for(int c = 0; c < ImageChannels; ++c) {\r\n                for(int j = 0; j < ImageHeight; ++j) {\r\n                    for(int i = 0; i < ImageWidth; ++i) {\r\n                        in[0][j][i][c] = 10 * (float)(pos++) / (ImageSize - 1) - 5;\r\n                    }\r\n                }\r\n            }\r\n            inputData = in;\r\n            Object[] input = new Object[] { inputData };\r\n            tflite.runForMultipleInputsOutputs(input, output);\r\n\r\n            for (int i = 0; i < runCount; i++) {\r\n                // run the second time to make sure all the buffers (and other lazy stuff) have been initialized\r\n                long timeStart = System.currentTimeMillis();\r\n                tflite.runForMultipleInputsOutputs(input, output);\r\n                float[][] res = (float[][]) output.get(0);\r\n                elapsedTime += System.currentTimeMillis() - timeStart;\r\n            }\r\n        }\r\n\r\n        delegate.close();\r\n\r\n        return \"RUN completed: \" + (elapsedTime / runCount) + \" ms\";\r\n    }\r\n```\r\n**Other info / logs**\r\n\r\n***Samsung (Exynos GPU)***\r\n\r\nVersion with GPUDelegate is a little bit slower (170 ms per run vs 140 ms)\r\n\r\n***Sony XPeria (Sanpdragon GPU)***\r\n\r\nVersion with GPUDelegate is a little bit faster (102 ms per run vs 125 ms).\r\n\r\nThe warning `WARNING: op code #42 cannot be handled by this delegate.` **does not** appear in LogCat during the execution, so it doesn't look like GpuDelegate couldn't handle some operation.\r\n", "comments": ["Either ~140 ms or ~170 ms looks too slow. See [my numbers on Pixel 2](https://github.com/freedomtan/glDelegateBench/blob/master/README.md) and [S9 numbers from @Burgomehl](https://github.com/freedomtan/glDelegateBench/issues/2)", "@freedomtan I've checked my config and found out that my Samsung has Exynos onboard. That's why I checked this app on Sony Xperia XZ2 with Snapdragon 845 and got some improvement, but not even close to mentioned in the article (125 ms per run vs 102 ms). Updated info in PR message. Is it OK, or performance should be better?", "@freedomtan May be there are some other conditions (for ex. targetSdkVersion or anything else) in project properties which could have a huge effect on performance?", "@FedyuninV I don't have either Snapdragon 845 or Samsung S9 phones. I did find an newly released Xiaomi Mi 9, a 855 phone, to run [my little program](https://github.com/freedomtan/glDelegateBench/blob/master/README.md). \r\n\r\n|model name|CPU 1 thread (ms)|CPU 4 threads (ms) |GPU (ms)|\r\n|----------|------------:|-------------:|---:|\r\n|Mobilenet | 39 | 35 | 15 |\r\n|PoseNet   | 48 | 47 | 19 |\r\n|DeepLab V3| 61 | 64 | 65 |\r\n|Mobilenet SSD V2 COCO| 69 | 75 | 36 |\r\n\r\nPerformance numbers of 845 are supposed to between 835's and 855's.\r\n\r\nNote that when doing benchmark / measurement, the first runs usually are slower because some initialization overheads.", "@FedyuninV \r\n\r\nThanks for trying out the GPU delegate.  I got an S8 (Exynos) and get CPU 255,126us vs GPU FP16 17,792us (buffers) & 33,979 (textures), averaged over 20 runs.  IIRC the developer preview was set to textures, (we now have logic to switch to buffers if it's faster on a particular device), but you should see some improvement.\r\n\r\nI'll try to dig up whether we also have an S9 (Exynos).\r\n\r\nWe don't have the Sony XPeria, so I couldn't check that.\r\n\r\nAnyway, you should see some improvement when using the GPU delegate, and it puzzles me why you run into this issue.  I don't think it's a device capability issue.\r\n\r\n> long timeStart = System.currentTimeMillis();\r\n> for (int i = 0; i < runCount; i++) tflite.runForMultipleInputsOutputs(input, output);\r\n> long timeEnd = System.currentTimeMillis();\r\n> return \"RUN completed: \" + ((timeEnd - timeStart) / runCount) + \" ms\";\r\n\r\n\r\nshould give you the timing without interrupts for getting the system time.", "@FedyuninV \r\n\r\nBtw, are you able to get the `GpuDelegate` with just `new`?  Shouldn't you use the `GpuDelegateHelper.isGpuDelegateAvailable()` & `GpuDelegateHelper.createGpuDelegate()`?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L174", "@impjdi \r\n\r\nI'll be able to continue research on Monday.\r\n\r\nAbout `new GpuDelegate();`: I used [this tutorial](https://www.tensorflow.org/lite/performance/gpu#trying_the_gpu_delegate_on_your_own_model).", "@impjdi FYR. `new GpuDelegate()` works for me.", "@freedomtan Wow, didn't know.  I live in the C++ world.\r\n@FedyuninV Sorry for the confusion.  Apparently, that also works.", "@freedomtan Your program shows improvements (101 cpu, 16 gpu). I have to go deeper. As soon as I find the cause of fails of my program I'll close the issue.", "Not officially announced yet, but FYI: GPU code is now visible at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu\r\n\r\nif you need the code for better insight what is happening.", "@FedyuninV Did you found the source of the problem? I am facing the same problem.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26620\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26620\">No</a>\n"]}, {"number": 26619, "title": "Faster bilinear resize for x86", "body": "- Resubmission of PR 21681\r\n- Added F16C check around _mm_cvtph_ps function\r\n- Integrated changes with half-pixel centers", "comments": ["@penpornk I think you're a better reviewer for this", "@MattConley Apologies for my delay! I was on vacation last week and I'm still catching up with other stuff. The PR looks awesome and I hope to get to it in a few days. Thank you for contributing!", "@penpornk No problem, thanks for the response!", "@penpornk any update on this?", "@penpornk   Are you back and can take a look?", "@MattConley @nluehr @tfboyd Apologies again! I'm also on vacation this week. I'll review this by the coming Monday (for real this time).", "Thanks for the review, I'm making progress with the changes and will push the fixes soon.", "@penpornk Thanks again for your review, I just pushed up some changes addressing your comments.", "This code is pretty unreadable. @martinwicke  do you have any guidance? I don't think we can accept this PR as is. @MattConley  Could you reuse parts of Eigen instead of adding your own library of low-level operations using intrinsics directly?", "Hmm... I don't see where intrinsics are used directly here? If you cannot read the code that's a bad sign though.", "@martinwicke Intrinsics are in `crop_resize_bilinear_core.h`. It is pretty big so github collapses the file by default. ", "@martinwicke I exaggerated a bit, but it basically looks like a lot of plain C code.", "NVIDIA will do an A/B test and determine if it is worth reworking this in Eigen or just closing.", "Here are the requested performance numbers run on my local machine with an Intel i7-5820K\r\n\r\nWithout the changes:\r\n```\r\nBenchmark                                           Time(ns) Iterations\r\n-----------------------------------------------------------------------\r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_3_512_512    6270880        100\t 196.0M items/s\r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_1_512_512    3954793        164\t 103.6M items/s\r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_3_512_512     9292230        100\t 132.2M items/s\r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_1_512_512     5378040        100\t 76.2M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_3_512_512    5738020        100\t 214.2M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_1_512_512    5039507        150\t 81.3M items/s\r\nBM_CropAndResize_cpu_DT_FLOAT_1_80_80_512_7_7         140624       4096\t 932.1M items/s\r\n```\r\n\r\nWith the changes:\r\n```\r\nBenchmark                                           Time(ns) Iterations\r\n-----------------------------------------------------------------------\r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_3_512_512    3932621        198  312.5M items/s \r\nBM_CropAndResize_cpu_DT_UINT8_1_640_640_1_512_512    1568988        427  261.1M items/s \r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_3_512_512     6684210        100  183.8M items/s \r\nBM_CropAndResize_cpu_DT_HALF_1_640_640_1_512_512     2892926        204  141.6M items/s \r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_3_512_512    2587333        267  474.9M items/s \r\nBM_CropAndResize_cpu_DT_FLOAT_1_640_640_1_512_512    1094434        585  374.3M items/s \r\nBM_CropAndResize_cpu_DT_FLOAT_1_80_80_512_7_7         132819       4054  741.5M items/s \r\n```", "@MattConley Thank you very much! :)", "NVIDIA is closing based on not having internal experience with Eigen.  Would need to be picked up by TesnorFlow and maybe team up if needed."]}, {"number": 26618, "title": "TFTRT: Don't try to rebuild failed engines in dynamic mode", "body": "If conversion fails in dynamic mode for an engine with a given input shape, we attempted to keep rebuilding it every time that input shape is seen. Since the conversion process should be deterministic wrt the input shapes, we can't expect building the same engine to succeed after failing once.\r\n\r\n* An empty engine will be stored in the cache for the input shape which failed to build.\r\n* The warning message which tells the user that the engine native fallback will be used will only appear the first time.\r\n* This will prevent us from attempting to build the same engine over and over, as well as spamming the user with warnings.", "comments": []}, {"number": 26617, "title": "Keras RNN Layer does not work with dynamic_decode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):From pip\r\n- TensorFlow version (use command below): tf-nightly-gpu-1.14.1.dev20190310\r\n- Python version: 3.7.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10, 7.5 for CUDA Version 10.0\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nUsing tf.keras.layers.RNN does not work in place of tf.nn.dynamic_rnn, or at least not in a way that I can find. Everytime I attempted to use it, I would get \r\n`Shape must be rank 2 but is rank 1 for 'decode/Training_Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/lstm_cell_3/MatMul_4' (op: 'MatMul') with input shapes: [1024], [1024,1024]`\r\nbut as soon as I swap to dynamic_rnn, it executes perfectly. However I see that dynamic_rnn will be removed. \r\n**Describe the expected behavior**\r\ntf.keras.RNN should be equivalent to dynamic_rnn\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nThere was too much code so I used pastebin.\r\nhttps://pastebin.com/iJ5diuie\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "My project was just a slightly modified version of this https://github.com/Currie32/Spell-Checker. \r\n`def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\r\n    '''Create the encoding layer'''\r\n    \r\n    if direction == 1:\r\n        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\r\n            for layer in range(num_layers):\r\n                with tf.variable_scope('encoder_{}'.format(layer)):\r\n                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\r\n\r\n                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \r\n                                                         input_keep_prob = keep_prob)\r\n\r\n                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \r\n                                                              rnn_inputs,\r\n                                                              sequence_length,\r\n                                                              dtype=tf.float32)\r\n\r\nreturn enc_output, enc_state`\r\nI changed the dropout wrapper to both not being there, and also a keras dropout layer. I changed that cell to a keras LSTM cell. And tf.nn.dynamic_rnn was changed to tf.keras.RNN.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 26616, "title": "Dependency on absl_py needs to be updated for compatibility with Bazel version >= 0.25", "body": "In Bazel 0.23,\r\n\r\n```\r\nbazel build --nobuild //tensorflow/tools/pip_package:build_pip_package --incompatible_remove_old_python_version_api\r\n```\r\n\r\nhas a breakage in `@absl_py`. I submitted an upstream fix that should appear on github today, with a release occurring tomorrow. When that lands, [workspace.bzl](https://github.com/tensorflow/tensorflow/blob/a4c589d5c754493c86f10cc71022fe38d184001a/tensorflow/workspace.bzl#L323-L336) will need to be updated to reference the new abseil-py release.\r\n\r\nThe fix relies on a feature introduced in Bazel 0.22. The incompatible change flag should be flipped in Bazel 0.25, at which point versions of TF without this fix will break by default.\r\n\r\nFor more info see bazelbuild/bazel#7308.", "comments": ["I do not have cycles to get to this quickly, but I am happy to review a PR.\r\n\r\n@meteorcloudy recently bumped our max bazel version requirement to 0.23.2, I thought we had TF tested with that bazel version. Looks like without this change TF wont build with bazel 0.23.2. Yun, should we revert the max bazel version in TF?", "@gunan, TensorFlow does build with Bazel 0.23.2, this is tested on Bazel CI.\r\nhttps://buildkite.com/bazel/tensorflow/builds/2384\r\n\r\n@brandjon is reporting a breaking change in future Bazel version. This breaking change can be enabled in 0.23.2 with `--incompatible_remove_old_python_version_api` so that you can test and migrate.\r\n\r\nAs the incompatibility comes from a dependency, TF can just upgrade `@absl_py` when a new release comes out. We don't have to revert the max bazel version in TF. \r\n", "Right, this change is to ensure TensorFlow works with 0.25 onward. But because the fix to the dependency uses a feature introduced in 0.22, this probably means that the minimum version needs to be bumped to 0.22.\r\n\r\nThe release of the dependency just went through, so I'll make a PR updating TF's use of it and the min bazel version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26616\">No</a>\n"]}, {"number": 26615, "title": "Fix build of TensorFlow Lite", "body": "Fixes compilation error on MinGW64 compiler.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/489f2cac03ee2940d971be0042e7c025686759ac/tensorflow/lite/experimental/c/c_api.cc#L80-L86\r\n\r\nThis is broken C syntax.", "comments": ["What's the compiler error you see? Could you paste the steps to reproduce (if you believe this is a bug, could you open a github issue with the exact environment information and steps we can reproduce)?", "```\r\ntensorflow/lite/experimental/c/c_api.cc:80:29: error: function 'void TFL_InterpreterOptionsSetErrorReporter(TFL_InterpreterOptions*, void (*)(void*, const char*, va_list), void*)' definition is marked dllimport\r\n TFL_CAPI_EXPORT extern void TFL_InterpreterOptionsSetErrorReporter(\r\n                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\nOnly this.", "Repro step, I'm trying to port tensorflow lite to Windows.\r\n\r\nSee https://github.com/tensorflow/tensorflow/pull/26040\r\n\r\nThis is easy way to reproduce on Windows.\r\n\r\nfoo.cc\r\n```\r\n__declspec(dllimport) extern void foo() {\r\n}\r\n```\r\n\r\n```\r\n$ g++ foo.cc\r\nfoo.cc:1:35: error: function 'void foo()' definition is marked dllimport\r\n __declspec(dllimport) extern void foo() {\r\n```\r\n", "Filed issue https://github.com/tensorflow/tensorflow/issues/26621"]}, {"number": 26614, "title": "Updated TensorFlow logo", "body": "I think this is a necessary change @rthadur \r\n\r\nThe repo still shows the old logo.", "comments": ["That doesn't seem right. I think the [image](https://www.tensorflow.org/images/tf_logo_transp.png) has been updated.  You might have caught this before we had a chance to update. Closing.", "But it doesn't show the changes on the repo, and my PR fixes that. Am I wrong? @drpngx @rthadur "]}, {"number": 26613, "title": "Fix for #26612", "body": "Fixed #26612", "comments": ["Is this good @rthadur ?????", "I'm closing this, casting is unnecessary"]}, {"number": 26612, "title": "Casting not necessary", "body": "Tell me if I'm wrong, but in `tensorflow/lite/kernels/internal/reference/integer_ops/softmax.h`, is casting to <int 32> necessary? (Line 89 & 90)", "comments": ["It should be necessary. C++ std::min and std::max expect to see two arguments with the same type. So we need to do an explicit casting to ensure this, otherwise there might be a compiler error (based on the compiler you use). For more context, please see:\r\nhttps://blogs.msmvps.com/gdicanio/2017/05/22/subtle-bug-with-stdminmax-function-templates/", "Yes, that makes sense. Thanks"]}, {"number": 26611, "title": "pix2pix_eager.ipynb error: NotImplementedError: exceptions are not yet supported", "body": "I run the [pix2pix_eager.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb) on colab. \r\n\r\nAt the step [training](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb#scrollTo=a1zZmKmvOH85):\r\n`train(train_dataset, EPOCHS)`\r\n\r\nThe error log: \r\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-22-d152560ca122> in <module>()\r\n----> 1 train(train_dataset, EPOCHS)\r\n\r\n<ipython-input-18-24e63cd58368> in train(dataset, epochs)\r\n      6 \r\n      7       with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n----> 8         gen_output = generator(input_image, training=True)\r\n      9 \r\n     10         disc_real_output = discriminator(input_image, target, training=True)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    590       else:\r\n    591         # Eager execution on data tensors.\r\n--> 592         outputs = self.call(inputs, *args, **kwargs)\r\n    593         self._handle_activity_regularization(inputs, outputs)\r\n    594         return outputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n    862   def __call__(self, *args, **kwargs):\r\n    863     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n--> 864     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n    865     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n    866 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   1174                 self._input_signature,\r\n   1175                 autograph=self._autograph,\r\n-> 1176                 arg_names=arg_names),\r\n   1177             self._function_attributes)\r\n   1178         if self._input_signature:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, add_control_dependencies, arg_names, op_return_value)\r\n    446         tf_decorator.rewrap(python_func, original_func, converted_func)\r\n    447 \r\n--> 448       func_outputs = python_func(*func_args, **func_kwargs)\r\n    449 \r\n    450       # invariant: `func_outputs` contains only Tensors, IndexedSlices,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in bound_method_wrapper(*args, **kwargs)\r\n   1654     # If __wrapped__ was replaced, then it is always an unbound function\r\n   1655     # that takes self as first argument.\r\n-> 1656     return wrapped_fn(weak_instance(), *args, **kwargs)\r\n   1657 \r\n   1658   # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    439                   strip_decorators=(def_function.function,),\r\n    440                   optional_features=(),\r\n--> 441               ), *args, **kwargs)\r\n    442 \r\n    443         # Wrapping around a decorator allows checks like tf_inspect.getargspec\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, *args, **kwargs)\r\n    287       experimental_partial_types=partial_types)\r\n    288 \r\n--> 289   result = converted_f(*effective_args, **kwargs)\r\n    290 \r\n    291   # The converted function's closure is simply inserted into the function's\r\n\r\n/tmp/tmply9ws8vn.py in tf__call(self, x, training)\r\n      1 from __future__ import print_function\r\n      2 def tf__call(self, x, training):\r\n----> 3   x1 = ag__.converted_call('down1', self, ag__.converter.ConversionOptions(recursive=True, verbose=0, strip_decorators=(function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), x, training=training)\r\n      4   x2 = ag__.converted_call('down2', self, ag__.converter.ConversionOptions(recursive=True, verbose=0, strip_decorators=(function_1, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), x1, training=training)\r\n      5   x3 = ag__.converted_call('down3', self, ag__.converter.ConversionOptions(recursive=True, verbose=0, strip_decorators=(function_2, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), x2, training=training)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, *args, **kwargs)\r\n    285       experimental_strip_decorators=options.strip_decorators,\r\n    286       experimental_verbose=options.verbose,\r\n--> 287       experimental_partial_types=partial_types)\r\n    288 \r\n    289   result = converted_f(*effective_args, **kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in to_graph(entity, recursive, arg_values, arg_types, experimental_optional_features, experimental_strip_decorators, experimental_verbose, experimental_partial_types)\r\n    407       uncompiled_modules=config.DEFAULT_UNCOMPILED_MODULES)\r\n    408   _, name, namespace = conversion.entity_to_graph(entity, program_ctx,\r\n--> 409                                                   arg_values, arg_types)\r\n    410 \r\n    411   nodes = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/conversion.py in entity_to_graph(o, program_ctx, arg_values, arg_types)\r\n    137     node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)\r\n    138   elif tf_inspect.ismethod(o):\r\n--> 139     node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)\r\n    140   # TODO(mdan,yashkatariya): Remove when object conversion is implemented.\r\n    141   elif hasattr(o, '__class__'):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/conversion.py in function_to_graph(f, program_ctx, arg_values, arg_types, owner_type)\r\n    335       owner_type=owner_type)\r\n    336   context = converter.EntityContext(namer, entity_info, program_ctx)\r\n--> 337   node = node_to_graph(node, context)\r\n    338 \r\n    339   if isinstance(node, gast.Lambda):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/conversion.py in node_to_graph(node, context)\r\n    373   # TODO(mdan): Insert list_comprehensions somewhere.\r\n    374 \r\n--> 375   node = converter.standard_analysis(node, context, is_initial=True)\r\n    376   # Past this point, line numbers are no longer accurate so we ignore the\r\n    377   # source.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/core/converter.py in standard_analysis(node, context, is_initial)\r\n    469   # TODO(mdan): Consider not running all analyses every time.\r\n    470   # TODO(mdan): Don't return a node because it's modified by reference.\r\n--> 471   graphs = cfg.build(node)\r\n    472   node = qual_names.resolve(node)\r\n    473   node = activity.resolve(node, context.info, None)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in build(node)\r\n    819 def build(node):\r\n    820   visitor = AstToCfg()\r\n--> 821   visitor.visit(node)\r\n    822   return visitor.cfgs\r\n\r\n/usr/lib/python3.6/ast.py in visit(self, node)\r\n    251         method = 'visit_' + node.__class__.__name__\r\n    252         visitor = getattr(self, method, self.generic_visit)\r\n--> 253         return visitor(node)\r\n    254 \r\n    255     def generic_visit(self, node):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_FunctionDef(self, node)\r\n    672     self._process_basic_statement(node.args)\r\n    673     for stmt in node.body:\r\n--> 674       self.visit(stmt)\r\n    675 \r\n    676     self.builder.exit_section(node)\r\n\r\n/usr/lib/python3.6/ast.py in visit(self, node)\r\n    251         method = 'visit_' + node.__class__.__name__\r\n    252         visitor = getattr(self, method, self.generic_visit)\r\n--> 253         return visitor(node)\r\n    254 \r\n    255     def generic_visit(self, node):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_With(self, node)\r\n    814       self._process_basic_statement(item)\r\n    815     for stmt in node.body:\r\n--> 816       self.visit(stmt)\r\n    817 \r\n    818 \r\n\r\n/usr/lib/python3.6/ast.py in visit(self, node)\r\n    251         method = 'visit_' + node.__class__.__name__\r\n    252         visitor = getattr(self, method, self.generic_visit)\r\n--> 253         return visitor(node)\r\n    254 \r\n    255     def generic_visit(self, node):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_If(self, node)\r\n    727     self.builder.new_cond_branch(node)\r\n    728     for stmt in node.body:\r\n--> 729       self.visit(stmt)\r\n    730 \r\n    731     self.builder.new_cond_branch(node)\r\n\r\n/usr/lib/python3.6/ast.py in visit(self, node)\r\n    251         method = 'visit_' + node.__class__.__name__\r\n    252         visitor = getattr(self, method, self.generic_visit)\r\n--> 253         return visitor(node)\r\n    254 \r\n    255     def generic_visit(self, node):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_With(self, node)\r\n    814       self._process_basic_statement(item)\r\n    815     for stmt in node.body:\r\n--> 816       self.visit(stmt)\r\n    817 \r\n    818 \r\n\r\n/usr/lib/python3.6/ast.py in visit(self, node)\r\n    251         method = 'visit_' + node.__class__.__name__\r\n    252         visitor = getattr(self, method, self.generic_visit)\r\n--> 253         return visitor(node)\r\n    254 \r\n    255     def generic_visit(self, node):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_If(self, node)\r\n    731     self.builder.new_cond_branch(node)\r\n    732     for stmt in node.orelse:\r\n--> 733       self.visit(stmt)\r\n    734 \r\n    735     self.builder.exit_cond_section(node)\r\n\r\n/usr/lib/python3.6/ast.py in visit(self, node)\r\n    251         method = 'visit_' + node.__class__.__name__\r\n    252         visitor = getattr(self, method, self.generic_visit)\r\n--> 253         return visitor(node)\r\n    254 \r\n    255     def generic_visit(self, node):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/pyct/cfg.py in visit_Try(self, node)\r\n    800     if node.handlers:\r\n    801       # TODO(mdan): Should we still support bare try/except? Might be confusing.\r\n--> 802       raise NotImplementedError('exceptions are not yet supported')\r\n    803 \r\n    804     self._exit_lexical_scope(node)\r\n\r\nNotImplementedError: exceptions are not yet supported\r\n```\r\n\r\n\r\n", "comments": ["@shaolinkhoa The colab runs successfully against TF 1.10. However for TF versions later than 1.10 it has to be updated.\r\nPlease try to run against TF 1.10 as of now. Thanks!\r\ncc @yashk2810 ", "This version is not maintained anymore. Can you try running this one: https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/pix2pix.ipynb", "thank you @ymodak and @yashk2810.\r\nMy problem now is choosing between tensorflow version 1.10 ,1.13.1 or 2.0-alpha.\r\n+ Version 2.0-alpha is still unstable.\r\n+ Version 1.13.1 is better than 1.10 . But as ymodak said: the tutorial on tensorflow website doesn't support version 1.13.1 and you guys don't have any plan to maintain all tutorials or guides.\r\n\r\nI don't know why but my Jupyter Notebook (run local)(version 5.7.4) always restart the kernal while I running the code Tensorflow-gpu version 2.0-alpha.\r\nSo should I just use tensorflow 2.0-alpha and keep updating the tensorflow version for now and then? \r\n\r\np/s: please support us to write a note of tensorflow version fit with each tutorial/guide. By default, we always think you guys updated the tutorials/guides when a new tensorflow version is released.", "I understand your dilemma here but I would like to encourage you for using TF 2.0 alpha since its only going to get better moving forward and can provide more support. I will close this issue now that we have several workarounds available. Thanks!"]}, {"number": 26610, "title": "TensorFlow 1.13 build fails with MPI support", "body": "**System information:**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.4\r\n- TensorFlow installed from source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6.4\r\n- Bazel version: 0.19 (recompiled from https://copr-be.cloud.fedoraproject.org/results/vbatts/bazel/fedora-27-x86_64/00823004-bazel/bazel-0.19.1-1.fc27.src.rpm since Bazel version provided by COPR for RHEL 7.4 is 0.23, that is not compatible with TensorFlow 1.13)\r\n- GCC/Compiler version: GCC 4.8.5 \r\n- with no CUDA support\r\n- Open MPI version: 2.0.4\r\n\r\n\r\n**Problem description:**\r\nBuild fails with MPI support with the following message:\r\n\r\n```\r\nERROR: /tmp/tensorflow/sources/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)\r\n[<machine>:30644] mca_base_component_repository_open: unable to open mca_patcher_overwrite: <path-to-openmpi-2.0.4>/lib/openmpi/mca_patcher_overwrite.so: undefined symbol: mca_patcher_base_patch_t_class (ignored)\r\n[<machine>:30644] mca_base_component_repository_open: unable to open mca_shmem_posix: <path-to-openmpi-2.0.4>/lib/openmpi/mca_shmem_posix.so: undefined symbol: opal_show_help (ignored)\r\n[<machine>:30644] mca_base_component_repository_open: unable to open mca_shmem_sysv: <path-to-openmpi-2.0.4>/lib/openmpi/mca_shmem_sysv.so: undefined symbol: opal_show_help (ignored)\r\n[<machine>:30644] mca_base_component_repository_open: unable to open mca_shmem_mmap: <path-to-openmpi-2.0.4>/lib/openmpi/mca_shmem_mmap.so: undefined symbol: opal_show_help (ignored)\r\n--------------------------------------------------------------------------\r\nIt looks like opal_init failed for some reason; your parallel process is\r\nlikely to abort.  There are many reasons that a parallel process can\r\nfail during opal_init; some of which are due to configuration or\r\nenvironment problems.  This failure appears to be an internal failure;\r\nhere's some additional information (which may only be relevant to an\r\nOpen MPI developer):\r\n\r\n  opal_shmem_base_select failed\r\n  --> Returned value -1 instead of OPAL_SUCCESS\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nIt looks like orte_init failed for some reason; your parallel process is\r\nlikely to abort.  There are many reasons that a parallel process can\r\nfail during orte_init; some of which are due to configuration or\r\nenvironment problems.  This failure appears to be an internal failure;\r\nhere's some additional information (which may only be relevant to an\r\nOpen MPI developer):\r\n\r\n  opal_init failed\r\n  --> Returned value Error (-1) instead of ORTE_SUCCESS\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nIt looks like MPI_INIT failed for some reason; your parallel process is\r\nlikely to abort.  There are many reasons that a parallel process can\r\nfail during MPI_INIT; some of which are due to configuration or environment\r\nproblems.  This failure appears to be an internal failure; here's some\r\nadditional information (which may only be relevant to an Open MPI\r\ndeveloper):\r\n\r\n  ompi_mpi_init: ompi_rte_init failed\r\n  --> Returned \"Error\" (-1) instead of \"Success\" (0)\r\n--------------------------------------------------------------------------\r\n*** An error occurred in MPI_Init_thread\r\n*** on a NULL communicator\r\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\r\n***    and potentially your MPI job)\r\n[<machine>:30644] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nI must say that I am clueless since I cannot understand which script/program is launched that triggers this error. If I understand well, it crashes while applying `tensorflow:tf_python_api_gen_v1` rule that comes from `tensorflow/BUILD`, but it is unclear to me where this rule is actually applied.\r\n\r\n**Sequence of commands:**\r\n```\r\n./configure\r\n/usr/local/bin/bazel --output_user_root=/tmp/bazel/build build --config=opt  //tensorflow/tools/pip_package:build_pip_package --distdir=builddeps\r\n```\r\n\r\n**Additional infos:**\r\n- My Open MPI installation is working flawlessly appart from this issue.\r\n- Note that I am building TensorFlow behind a proxy and was therefore forced to download all the required dependencies in a `distdir` folder.\r\n- I first tried to build TensorFlow from master, but had first another issue (`InitPartial` signature change was not reported in `tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc` and I had to patch this file) but ended up with a similar build fail.\r\n- For the sake of completeness, I also gave a try with GNU 8.2.0 -- even though I doubted it had anything to do with the problem at stake.", "comments": ["From our side, MPI is a community maintained feature.\r\n@tatianashp does Intel help with maintaining MPI support?", "Maybe @jhseu has knowledge of the party that supports MPI.", "I don't think Intel supports MPI in TensorFlow. Adding @agramesh1  just in case.", "@jbedorf mind to take a look here?", "That is a weird error, especially given that it happens during the build process. Could it be that there is a Python package installed that loads MPI libraries, e.g. `mpi4py`?", "@tatianashp  Thanks, will see if we can help.  Pinging @TensorFlow-MKL .", "Hi @marcjoos-phd ,  \r\nAt Intel we test TensorFlow with multinode CPU and MPI frequently. Currently the recommended way of trying distributed training on CPU is via horovod. ResNet50 are most frequently tested, but the good thing is that multi-node training of CPU with horovod does *not* require you to build TensorFlow with MPI. Just remember to use --config=mkl to turn on Intel optimizations with MKL :)\r\nFYI: some blogs published by Intel might be of help: https://ai.intel.com/using-intel-xeon-for-multi-node-scaling-of-tensorflow-with-horovod/\r\nRecently I tested that using TF v1.13.1 and horovod 0.13.1, and cnn_tf_v1.13_compatible branch of this repo should just work for CPU. Commands are similar to the commands in the above blog. Both Intel MPI and OpenMPI work, you can start with OpenMPI (e.g. OpenMPI 3.0). \r\n\r\nthere is also an Intel white paper on distributed training on CPU architecture: https://builders.intel.com/docs/aibuilders/best-practices-for-scaling-deep-learning-training-and-inference-with-tensorflow-on-intel-xeon-processor-based-hpc-infrastructures.pdf\r\n\r\nFeel free to let us know of your questions. And thank you from Intel for trying distributed training with CPU!\r\n", "To clarify the better bazel build instruction (for CPU) is \r\n/usr/local/bin/bazel --output_user_root=/tmp/bazel/build build --config=mkl --config=opt  //tensorflow/tools/pip_package:build_pip_package --distdir=builddeps\r\n\r\nBefore you run the above bazel command, while you configure TF, reply \"N\" to \"build TF with MPI?\"[y/N] ", "MPI question on 1.13 is addressed, closing this issue. 1.14 is tracked under a different ticket. Please reopen if you continue to experience the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26610\">No</a>\n"]}, {"number": 26609, "title": "Added return descriptions", "body": "Added return descriptions to functions in initializers.py", "comments": ["Review @rthadur ???", "Thanks a lot, I should have seen that, I fixed it though. Is it good or should i change anything else? @penpornk ", "It's been a while, is anything lacking here @penpornk @rthadur ??", "@kyscg I don't think you need to do anything more. The PR just needs to be pulled in.\r\n@rthadur Gentle ping for pulling. Thank you!"]}, {"number": 26608, "title": "Subsequent gather_nd Operation leads to error: Tensor.graph is meaningless when eager execution is enabled.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: python 3.5\r\n- CUDA/cuDNN version: 10.\r\n- GPU model and memory: TITAN\r\n\r\n**Describe the current behavior**\r\nGradient computation fails with the message: \"Tensor.graph is meaningless when eager execution is enabled.\"\r\n \r\nThe gradient becomes an IndexedSlices through the second gather_nd operation. On the IndexedSlices the check in https://github.com/tensorflow/tensorflow/blob/8cb1aa18882be9142a5e4d3977de0a076ce2c791/tensorflow/python/framework/ops.py#L5956-L5957 evaluates to True and in line 5965 the .graph attribute is called which in EagerExecution causes the error mentioned above.\r\nhttps://github.com/tensorflow/tensorflow/blob/8cb1aa18882be9142a5e4d3977de0a076ce2c791/tensorflow/python/framework/ops.py#L5965\r\n\r\nA workaround/fix for the problem can be achieved by adding another check to the if clause making sure that op_input is not an IndexedSlices: `and not isinstance(op_input, IndexedSlices)`\r\n\r\n```python\r\n    if (isinstance(op_input, (Operation, _TensorLike)) and\r\n       ((not isinstance(op_input, Tensor)) or type(op_input) == Tensor) and\r\n       (not isinstance(op_input, IndexedSlices))):  # pylint: disable=unidiomatic-typecheck\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nGradients to be computed correctly\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass DebugModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super(DebugModel, self).__init__()\r\n    self.dense = tf.keras.layers.Dense(20, activation='relu', input_shape=(5,))\r\n\r\n  def __call__(self, input, *args, **kwargs):\r\n    x = self.dense(input)\r\n    ind = [[0], [1], [2]]\r\n    x = tf.gather_nd(x, ind)\r\n    ind = [[0], [1]]\r\n    x = tf.gather_nd(x, ind)\r\n    return x\r\n\r\nmy_model = DebugModel()\r\n\r\nmy_optim = tf.keras.optimizers.Adam(1e-3)\r\nwith tf.GradientTape() as tape:\r\n  out = my_model(tf.random.uniform((16,5)))\r\n  my_loss = -tf.reduce_mean(out)\r\n  grads = tape.gradient(my_loss, my_model.trainable_weights)\r\n  my_optim.apply_gradients(zip(grads, my_model.trainable_weights))\r\n```\r\n\r\n", "comments": ["@josh11b You are mentioned in a TODO above the problematic if statement. Can you check whether this is a valid fix?", "@goldiegadde This is still present in 2.0.0-dev20190319", "It seems plausible but I'm not certain I understand the implications of this change.", "We should probably drop this validating that everything is in the same graph bit. It'll likely be false with functions anyway.", "@meyerjo do you feel like sending a pull request which adds a unit test based on your code and deletes this validation block in ops.py?", "Do you feel like this whole validation block is not necessary? I will try to create a pull request beginning of April. Till then I am rather occupied.", "Yes, I feel like it's wrong more often than not now.\n\nOn Wed, Mar 27, 2019 at 12:25 AM Johannes Meyer <notifications@github.com>\nwrote:\n\n> Do you feel like this whole validation block is not necessary? I will try\n> to create a pull request beginning of April. Till then I am rather occupied.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26608#issuecomment-477012346>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYRqeL06Lw4JLnIFuEJaY1dRhkaqks5vaxzygaJpZM4bq6HM>\n> .\n>\n\n\n-- \n - Alex\n", "@meyerjo did you ever send a PR to fix this?", "@meyerjo did you had a chance to send a PR to fix this? Thanks!", "For now, the only fix I found is to multiply the tensor by 1. In between both the operations\r\nBut not sure if that is the best way to get rid of this error\r\n\r\n\r\n```\r\n    x = self.dense(input)\r\n    ind = [[0], [1], [2]]\r\n    x = tf.gather_nd(x, ind)\r\n    x = x*tf.constant(1.0)\r\n    ind = [[0], [1]]\r\n    x = tf.gather_nd(x, ind)\r\n    return x\r\n```", "Unfortunately, I did not find time yet to work on the pull request. However, the quick-fix mentioned in the initial issue is still working in TF2.0 . Hopefully, by mid of this week I will have some more time to take a deeper look at it.", "@meyerjo,\r\nI was able to reprocude the error with TF v2.0. However, the issue seems to be fixed with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/55ed1dfb50678c98c263f9a0c5d2e098/2-1-template.ipynb). Please find the attached gist. Thanks!", "@meyerjo,\r\nAny updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26608\">No</a>\n"]}, {"number": 26607, "title": "How to run tflite with float input", "body": "Hi Sir,\r\n  I am trying to run tflite on mobile to classify sound event.\r\n  The input of the model is mfcc features from the sound stream, which's type are float32.\r\n  I have converted the pb model to tflite model, but I cannot feed the input data whith float32.\r\n\r\nIt will be throw exception as follow whether I setting the post_training_quantize to True or not\r\n **cannot convert between a tensorflowlite tensor with type float32 and a java object of type**\r\n", "comments": ["@LeighSu Could you fill issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md) and if possible, please provide a code to reproduce the bug? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26607\">No</a>\n"]}, {"number": 26606, "title": "AttributeError: module 'tensorflow.python.ops.image_ops' has no attribute 'is_jpeg'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10 Pro 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.6\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0\r\n- GPU model and memory: Nvidia Quadro P4000 - 8GB\r\n\r\n\r\n\r\nWhile running the train.py - it fails with the attribute error\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nPS D:\\Nanda\\DeepLabs\\models-master\\research\\deeplab> python train.py --logtostderr --train_split=\"train\" --model_variant=\"xcep\r\ntion_65\" --atrous_rates=6 --atrous_rates=12 --atrous_rates=18 --output_stride=16 --decoder_output_stride=4 --train_crop_size=5\r\n13 --train_crop_size=513 --train_batch_size=4 --training_number_of_steps=100 --fine_tune_batch_norm=true --tf_initial_checkpoi\r\nnt=\"D:/Nanda/DeepLabs/models-master/research/deeplab/datasets/Datathon/exp/train_on_trainval_set/init_models/deeplabv3_pascal_\r\ntrain_aug/model.ckpt\" --train_logdir=\"D:/Nanda/DeepLabs/models-master/research/deeplab/datasets/Datathon/exp/train_on_trainval\r\n_set/train/\" --dataset_dir=\"D:/Nanda/DeepLabs/models-master/research/deeplab/datasets/Datathon/tfrecord\"\r\nINFO:tensorflow:Training on train set\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 499, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\nvenugop\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 451, in main\r\n    dataset.get_one_shot_iterator(), dataset.num_of_classes,\r\n  File \"D:\\Nanda\\DeepLabs\\models-master\\research\\deeplab\\datasets\\data_generator.py\", line 335, in get_one_shot_iterator\r\n    .map(self._parse_function)\r\n  File \"C:\\Users\\nvenugop\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 790, in map\r\n    return MapDataset(self, map_func)\r\n  File \"C:\\Users\\nvenugop\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1597, in __init__\r\n    self._map_func.add_to_graph(ops.get_default_graph())\r\n  File \"C:\\Users\\nvenugop\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\function.py\", line 486, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"C:\\Users\\nvenugop\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\function.py\", line 321, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"C:\\Users\\nvenugop\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\function.py\", line 338, in _create_definition_if_needed_impl\r\n    outputs = self._func(*inputs)\r\n  File \"C:\\Users\\nvenugop\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1562, in tf_map_func\r\n    ret = map_func(nested_args)\r\n  File \"D:\\Nanda\\DeepLabs\\models-master\\research\\deeplab\\datasets\\data_generator.py\", line 246, in _parse_function\r\n    image = _decode_image(parsed_features['image/encoded'], channels=3)\r\n  File \"D:\\Nanda\\DeepLabs\\models-master\\research\\deeplab\\datasets\\data_generator.py\", line 223, in _decode_image\r\n    tf.image.is_jpeg(content),\r\nAttributeError: module 'tensorflow.python.ops.image_ops' has no attribute 'is_jpeg'", "comments": ["@Aliennandy Could you check this with latest version of TF and let us know whether the bug persists with latest TF. Could you share a code to reproduce the bug? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "AttributeError: module 'tensorflow.python.ops.image_ops' has no attribute 'sobel_edges' using tensorflow=1.7"]}, {"number": 26605, "title": "how to build armeabi-v7a tensorflow-lite.a with ndk?", "body": "", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26604, "title": "TF Lite covert_test updated", "body": "", "comments": []}, {"number": 26603, "title": "added explanation for certain activation functions", "body": "Added explanation for the following activation functions:\r\n\r\n1. tanh\r\n2. exponential\r\n3. sigmoid\r\n4. Linear\r\n\r\nPlease review. @rthadur ", "comments": []}, {"number": 26602, "title": "Partial function specified through keyword on first position in tf.function", "body": "Wrapping in tf.function a partial with first argument specified:\r\n```\r\ndef f(x, y):\r\n  return x + y\r\n\r\npartial_func = functools.partial(f, x=5)\r\ntf_func = tf.function(partial_func)\r\n\r\nprint(tf_func(5))\r\n```\r\n\r\nThis does not work in Python2.x, because [tf_inspect.getfullargspec cannot represent such construct](https://github.com/tensorflow/tensorflow/blob/c46d383150564c8b72b05acc65182c16f7221694/tensorflow/python/util/tf_inspect.py#L175) using Argspec.\r\n\r\nUnfortunately this also does not work in Python3, where Argspecs are already capable of representing this:\r\n```\r\nTypeError: tf__f() got multiple values for argument 'x'\r\n```", "comments": ["Can we isolate this issue to misbehavior of getfullargspec instead of tf.function?\r\n\r\nI ask because as far as tf.function is concerned nothing breaks if we just wrap the partial call into a lambda *args, **kwds: partial_call(*args, **kwds) so we should probably just do that at intake time if the user passes a functools.partial function.", "I am not actually sure that this is a misbehavior of getfullargspec.\r\n\r\nCurrently we are using ArgSpec to be able to bind arguments to (partial or normal) function later. We want to represent partial function fully with ArgSpec, but this is not possible with Python2.x: it is not possible to have arguments with defaults before arguments without defaults.\r\n\r\nI would consider this more an integration issue. For example, do we really need to represent partial function using getfullargspec on the inner function (the one with more arguments)? Could we somehow forget about the inner partial and work with the outer only? I haven't tried it myself.", "I think we should do the latter and forget about the inner partial.\n\nOr we should not rely on argspec for this at all and instead look at the\narguments as they're presented at calling time (which I much prefer).\n\nOn Thu, Mar 14, 2019 at 7:30 AM Vojtech Bardiovsky <notifications@github.com>\nwrote:\n\n> I am not actually sure that this is a misbehavior of getfullargspec.\n>\n> Currently we are using ArgSpec to be able to bind arguments to (partial or\n> normal) function later. We want to represent partial function fully with\n> ArgSpec, but this is not possible with Python2.x: it is not possible to\n> have arguments with defaults before arguments without defaults.\n>\n> I would consider this more an integration issue. For example, do we really\n> need to represent partial function using getfullargspec on the inner\n> function (the one with more arguments)? Could we somehow forget about the\n> inner partial and work with the outer only? I haven't tried it myself.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26602#issuecomment-472882315>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWfus7xJCyx0oit5DEAcSXFRFacdks5vWl0OgaJpZM4bqjiP>\n> .\n>\n\n\n-- \n - Alex\n", "@vbardiovskyg I think you're working on this, right?", "Since we are now depending on partial to do the argument binding, this becomes infeasible (i.e. we don't want to provide more functionality than partial already does).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26602\">No</a>\n", "@vbardiovskyg  Please see comments in the following code (TF 2.2.0rc2).  Why does the first case succeed and second fail?  Is this a bug?  Thanks.\r\n```python\r\nimport tensorflow as tf\r\nimport functools\r\n\r\ndef f(x, y):\r\n    return x + y\r\n\r\nbind_x = functools.partial(f, x=1)\r\nbind_y = functools.partial(f, y=1)\r\n\r\ndataset = tf.data.Dataset.range(1)\r\n\r\nprint(next(iter(dataset.map(bind_y))))  # This works\r\nprint(next(iter(dataset.map(bind_x))))  # TypeError: tf__f() got multiple values for argument 'x'\r\n```", "Hi @mmilosav,\r\n\r\nthis is due to how partial works. See the following snippet using partial outside of TensorFlow context:\r\n\r\n```\r\ndef f(x,y):\r\n  print(x+y)\r\n\r\nbind_x = functools.partial(f, x=1)\r\n\r\nbind_x(3)  # TypeError: f() got multiple values for argument 'x'\r\n```"]}, {"number": 26601, "title": "Can I attach a c++ object to a tensor so that the object can be forwarded through the network and share the same lifetime with the tensor?", "body": "I'm trying to develop a complex network in which, some custom c++ ops output c++ objects that are to be received by other custom ops.  One solution is by serializing the c++ object into a byte tensor, but this way adds (de)serialization overhead. Another solution is only outputting the address of the object and storing the actual object as a data member of the c++ op instance, but this way only works in graph mode. Is there a better solution for this problem? Thank you.", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 26600, "title": "InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' used by {{node cu_dnnlstm/CudnnRNN}}with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]", "body": "\r\nI am user macintosh  \r\n\r\n\r\ncode \r\n\r\n\r\n```\r\n`import tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM #, CuDNNLSTM\r\n\r\n\r\nmnist = tf.keras.datasets.mnist  # mnist is a dataset of 28x28 images of handwritten digits and their labels\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()  # unpacks images to x_train/x_test and labels to y_train/y_test\r\n\r\nx_train = x_train/255.0\r\nx_test = x_test/255.0\r\n\r\nprint(x_train.shape)\r\nprint(x_train[0].shape)\r\n\r\nmodel = Sequential()\r\n\r\n\r\nmodel.add(LSTM(128, input_shape=(x_train.shape[1:]), activation='relu', return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(LSTM(128, activation='relu'))\r\nmodel.add(Dropout(0.1))\r\n\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(Dense(10, activation='softmax'))\r\n\r\nopt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\r\n\r\n# Compile model\r\nmodel.compile(\r\n    loss='sparse_categorical_crossentropy',\r\n    optimizer=opt,\r\n    metrics=['accuracy'],\r\n)\r\n\r\nmodel.fit(x_train,\r\n          y_train,\r\n          epochs=3,\r\n          validation_data=(x_test, y_test))`\r\n```\r\n\r\noutput \r\n\r\n```\r\n`(60000, 28, 28)\r\n(28, 28)\r\nTrain on 60000 samples, validate on 10000 samples\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1316       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1317       self._extend_graph()\r\n   1318       return self._call_tf_sessionrun(\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1351     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1352       tf_session.ExtendSession(self._session)\r\n   1353 \r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' used by {{node cu_dnnlstm/CudnnRNN}}with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node cu_dnnlstm/CudnnRNN}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-7-d1c9ab123f67> in <module>\r\n     41           y_train,\r\n     42           epochs=3,\r\n---> 43           validation_data=(x_test, y_test))\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    878           initial_epoch=initial_epoch,\r\n    879           steps_per_epoch=steps_per_epoch,\r\n--> 880           validation_steps=validation_steps)\r\n    881 \r\n    882   def evaluate(self,\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\r\n    249     # Setup work for each epoch\r\n    250     epoch_logs = {}\r\n--> 251     model.reset_metrics()\r\n    252     callbacks.on_epoch_begin(epoch, epoch_logs, mode=mode)\r\n    253     progbar.on_epoch_begin(epoch, epoch_logs)\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in reset_metrics(self)\r\n   1117     if hasattr(self, 'metrics'):\r\n   1118       for m in self.metrics:\r\n-> 1119         m.reset_states()\r\n   1120       if self._distribution_strategy:\r\n   1121         training_distributed._reset_metrics(self)  # pylint: disable=protected-access\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py in reset_states(self)\r\n    458     \"\"\"\r\n    459     for v in self.variables:\r\n--> 460       K.set_value(v, 0)\r\n    461 \r\n    462   @abc.abstractmethod\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in set_value(x, value)\r\n   2845         x._assign_placeholder = assign_placeholder\r\n   2846         x._assign_op = assign_op\r\n-> 2847       get_session().run(assign_op, feed_dict={assign_placeholder: value})\r\n   2848 \r\n   2849 \r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in get_session()\r\n    480   if not _MANUAL_VAR_INIT:\r\n    481     with session.graph.as_default():\r\n--> 482       _initialize_variables(session)\r\n    483   return session\r\n    484 \r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in _initialize_variables(session)\r\n    756     # marked as initialized.\r\n    757     is_initialized = session.run(\r\n--> 758         [variables_module.is_variable_initialized(v) for v in candidate_vars])\r\n    759     uninitialized_vars = []\r\n    760     for flag, v in zip(is_initialized, candidate_vars):\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' used by node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) ]]\r\n\r\nCaused by op 'cu_dnnlstm/CudnnRNN', defined at:\r\n  File \"/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/anaconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/anaconda3/lib/python3.7/asyncio/base_events.py\", line 528, in run_forever\r\n    self._run_once()\r\n  File \"/anaconda3/lib/python3.7/asyncio/base_events.py\", line 1764, in _run_once\r\n    handle._run()\r\n  File \"/anaconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\r\n    ret = callback()\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1233, in inner\r\n    self.run()\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1147, in run\r\n    yielded = self.gen.send(value)\r\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\r\n    yielded = next(result)\r\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/anaconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\r\n    return runner(coro)\r\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\r\n    if (yield from self.run_code(code, result)):\r\n  File \"/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-6-3c5692df850b>\", line 19, in <module>\r\n    model.add(CuDNNLSTM(128, input_shape=(x_train.shape[1:]), return_sequences=True))\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\", line 442, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 164, in add\r\n    layer(x)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 701, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/cudnn_recurrent.py\", line 111, in call\r\n    output, states = self._process_batch(inputs, initial_state)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/cudnn_recurrent.py\", line 501, in _process_batch\r\n    is_training=True)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 142, in cudnn_rnn\r\n    seed2=seed2, is_training=is_training, name=name)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNN' used by node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\r\nRegistered devices: [CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[node cu_dnnlstm/CudnnRNN (defined at <ipython-input-6-3c5692df850b>:19) ]]`\r\n```", "comments": ["What version of Tensorflow are u currently using? Also do u have gpu on your local machine?", "I am using MacBook pro early Early 2015. Tensorflow version is 1.13.1 @qlzh727 ", "Hi,\r\nI'm also facing same issue.\r\nI'm using,\r\nubuntu 16.04\r\ntensorflow==1.12.0\r\ncuda-9.0\r\ncudnn=7.0.5\r\nGPU Tesla C2075\r\n \r\n\r\n```\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU,XLA_GPU], Registered kernels:\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n\t\r\n [[node bidirectional_1/CudnnRNN (defined at /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py:922)  = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"lstm\", seed=87654321, seed2=0](bidirectional_1/transpose, bidirectional_1/ExpandDims_1, bidirectional_1/ExpandDims_2, bidirectional_1/concat)]]\r\n```\r\n\r\n", "This seems to be quite weird, since the code snippet does not specify cudnn kernel at all. Btw, do u have tensorflow-alpha release installed? Can u run \"pip list\" and show all installed tf package/version here? ", "Hi,\r\nI faced the same error after GPU swap between servers. The problem was that Tensorflow was compiled with higher compute capability then the GPUs that were put in.", "I am still having trouble to reproduce this issue with the code above. The cudnnLSTM import was commented out above and the model shouldn't use any cudnn kernel.\r\n\r\nFor anyone who experience the same issue, can u post the code snippet to reproduce the issue? Please also include the TF version you installed.", "I'm getting the same problem as @ritdubal3011 \r\n\r\n* Ubuntu 18.04, nvidia drivers installed using instructions [here](https://www.tensorflow.org/install/gpu)\r\n* NVIDIA GTX 1080\r\n\r\nPython modules:\r\n\r\n```\r\n# Installed in anaconda\r\n\r\ntensorboard               1.12.2           py36he6710b0_0  \r\ntensorflow                1.12.0          gpu_py36he74679b_0  \r\ntensorflow-base           1.12.0          gpu_py36had579c0_0  \r\ntensorflow-gpu            1.12.0               h0d30ee6_0  \r\n\r\n# These also seem relevant modules\r\ncudatoolkit               9.2                           0  \r\ncudnn                     7.3.1                 cuda9.2_0  \r\n```\r\n\r\nI've got a pretty self-contained reproduction here, though I certainly imagine it's either user error or a drivers issue, as I'm currently learning TF and have it set up for the first time.\r\n\r\n```python\r\n# %%\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = list(map(lambda n: n / 10000, range(0, 10000+1)))\r\n\r\n# %%\r\n\r\nsequences = list(zip(data, data[1:], data[2:], data[3:], data[4:], data[5:]))\r\ntrain_x = np.array(list(map(\r\n    lambda seq: seq[:-1], sequences))).reshape(len(sequences), len(sequences[0])-1, 1)\r\ntrain_y = np.array(\r\n    list(map(lambda seq: seq[-1], sequences)))\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.CuDNNLSTM(\r\n        3,\r\n        return_sequences=False),\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(0.1, decay=0.003), \r\n    loss=tf.keras.losses.mse)\r\n\r\n# %%\r\n\r\nmodel.fit(train_x, train_y, epochs=100, validation_split=0, shuffle=False)\r\n```\r\n\r\n```\r\nInvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU,XLA_CPU,XLA_GPU], Registered kernels:\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n```", "Running the following seems to confirm what I'm suspecting.\r\n\r\nMy GPU doesn't seem to be getting picked up despite having gone through all the instructions perfectly before, and verifying it was working using the test code [here](https://www.tensorflow.org/guide/using_gpu). The test code is also not working properly now, so it's definitely the GPU config.\r\n\r\n\\* sigh \\*... Linux...\r\n\r\n```\r\ntf.test.is_gpu_available(\r\n    cuda_only=False,\r\n    min_cuda_compute_capability=None\r\n)\r\n```", "@Nick-Lucas, does the tf.test.is_gpu_available return False? If that's the case, then it definitely means the GPU is not configured correctly. \r\n\r\nAlso, the error message somehow indicating that the GPU device is not there:\r\n\r\nRegistered devices: [CPU,XLA_CPU,XLA_GPU]\r\n\r\nXLA_GPU is not same as GPU.", "Adding @gunan who is the expert of build/packaging. When user installed both tf and tf-gpu, which kernel will the runtime pickup?", "> XLA_GPU is not same as GPU.\r\n\r\nThis is useful to know for next time.\r\n\r\nSo in my case I ended up rebooting the machine and it suddenly started working fine. I imagine the drivers got into a bad state but it wasn't being reported. Sorry I meant to update the thread but forgot.\r\n\r\nSomething which was interesting and might help others above though. When running in a jupyter instance the error would hang around until I restarted the kernel manually. ie. I run with CudNNLSTM and get the error, then change it out to a CPU LSTM layer, and continue to get exactly the same error. Basically the code above where the CuDNN variant is commented out may have still been erroring if @connect2robiul was running in a jupyter instance.", "Thanks for the information, closing this issue now.", "I had the same issue. On shifting to \"GPU\" option, the error disappeared and the code started working. In the \"Runtime\" menu, change \"Change Runtime Types\" to \"GPU\".", "> I had the same issue. On shifting to \"GPU\" option, the error disappeared and the code started working. In the \"Runtime\" menu, change \"Change Runtime Types\" to \"GPU\".\r\n\r\nHow do I change to GPU? Where do I find a runtime menu?", "I have same error  when  I train and save a model in  local machine, but when I load my model in  server,it get this error", "Had same problem, \r\nInstalling tensorflow-gpu compatible with CUDA solved the problem..\r\nTry to search \"cuda tensorflow gpu compatibility\"", "I solved this issue by choosing GPU on colab through change runtime and it will through same error if you will not have gpu in your local system.", "> I solved this issue by choosing GPU on colab through change runtime and it will through same error if you will not have gpu in your local system.\r\n\r\nsame, i forget to switch to gpu in colab", "I had the same problem even with a TPU execution on colab. It's solved only by switching to a GPU execution", "I am getting a very similar error, but it is in a very specific set of circumstances.\r\n\r\nI have 2 models, let's call them model_A and model_B. model_A was trained on a CPU machine and has no GPU layers, only LSTM/Dense layers. model_B was trained on a GPU machine and has CuDNNGRU layers. I can almost always load model_A on a machine with no GPU, and I cannot load model_B on a machine with no GPU (as expected). The weird behavior is this: I spin up a new terminal on a machine with no GPUs. I load model_A on my machine, with no issues (as expected). I try to load model_B on my machine, and I get this error (as expected). I try to re-load model_A on my machine, and I get this error (not expected, model_A has no GPU dependent layers). I have to restart my terminal window to be able to load model_A. I can repeat this behavior when substituting any non-GPU model for model_A, and when substituting any GPU model for model_B. \r\n\r\nThe  error is slightly different, it is CudnnRNNV2 instead of CudnnRNN: `InvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNNV2' used by node cu_dnngru_2/CudnnRNNV2 (defined at /my/conda/env/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [input_mode=\"linear_input\", T=DT_FLOAT, direction=\"unidirectional\", rnn_mode=\"gru\", is_training=true, seed2=0, seed=0, dropout=0]`. ", "I had the same issue, installing correct version of cuda- cudnn solved my problem.", "Registered devices: [CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n         [[cu_dnnlstm_1/CudnnRNN]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node cu_dnnlstm_1/CudnnRNN:\r\n cu_dnnlstm_1/transpose (defined at C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:484)\r\n cu_dnnlstm_1/ExpandDims_2 (defined at C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:488)\r\n cu_dnnlstm_1/ExpandDims_1 (defined at C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:487)\r\n cu_dnnlstm_1/concat_1 (defined at C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\layers\\cudnn_recurrent.py:60)\r\n\r\nCan anyone help me?", "I uninstall tensorflow-cpu==1.15.2 and tensorflow-gpu==1.15.2 completely, and then reinstall tensorflow-gpu==1.15.2 only. It works!"]}, {"number": 26599, "title": "from tensorflow.contrib import layers -> Cannot import name autograph", "body": "'from tensorflow.contrib import layers'\r\n\r\nThis simple line ends up with the following error : ImportError: cannot import name autograph\r\n\r\nWhy ?\r\n", "comments": ["https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md", "@jcrevoisier What version of TF are you using? Can you switch to TF 1.12 or TF 1.13.1?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "hey @ymodak, im using TF 1.13 on a nvidia xavier trying top get better performance from my image segmentation model implemented in keras. when importing tensorflow.contrib it gives me the same ERROR \"cannot import name autograph\"", "Did you solve it? I'm having the same issue", "@tenshis @miladnoori1996  Please post a new issue and provide all information asked by the template. Thanks!"]}]