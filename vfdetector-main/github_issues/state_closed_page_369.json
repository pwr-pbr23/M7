[{"number": 42978, "title": "Multiple version of tensorflow that exist in system site packages and venv site packages", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: nightly (2.4.0.dev20200904)\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: virtualenv+pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\nError when importing tensorflow nightly installed in venv while having another version of tensorflow installed system-wide.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFollowing steps in https://www.tensorflow.org/install/pip.\r\n```\r\npip3 install --upgrade tensorflow-cpu\r\npython3 -m venv --system-site-packages venv\r\nsource venv/bin/activate\r\n(venv) pip install --upgrade pip\r\n(venv) pip install --upgrade tf-nightly-cpu\r\n(venv) python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n```\r\n\r\n**Any other info / logs**\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"somewhere/venv/lib/python3.8/site-packages/tensorflow/__init__.py\", line 433, in <module>\r\n    _ll.load_library(_main_dir)\r\n  File \"somewhere/venv/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 154, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/user/.local/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n```\r\n\r\nI have tensorflow 2.3.0 installed in system site packages and that causes https://github.com/tensorflow/tensorflow/blob/47aab0b49ef7432261f60430f5e29bb331cd5233/tensorflow/api_template.__init__.py#L138-L142 to load tensorflow kernel from system site packages.\r\n\r\nIs there a way to have multiple version of tensorflow that exist in system site packages and venv site packages?", "comments": ["Can you try to create the venv without `--system-site-packages`?", "> Can you try to create the venv without `--system-site-packages`?\r\n\r\nSame result. Unless I also comment out https://github.com/tensorflow/tensorflow/blob/47aab0b49ef7432261f60430f5e29bb331cd5233/tensorflow/api_template.__init__.py#L119\r\n\r\nThanks for pointing out that flag. Now I have a workaround.", "@kafji \r\nIf the issue is resolved please feel free to move this to closed status.", " @kafji Probably it was a side effect of https://github.com/tensorflow/tensorflow/pull/38663 /cc @angerson @yongtang ", "@bhack I think it was from an older commit https://github.com/tensorflow/tensorflow/commit/5195204b47d1cf9516ff5eea8232fbff8d320521#diff-62ec18ff8bdd93adaff55160f27a7e09R69", "I'm closing this.\r\n\r\nThe workaround is to comment out https://github.com/tensorflow/tensorflow/blob/47aab0b49ef7432261f60430f5e29bb331cd5233/tensorflow/api_template.__init__.py#L119 which located in tensorflow's `__init__.py`\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42978\">No</a>\n", "If you don't mind, I'd rather keep this open. It seems quite reasonable to me for TF to load shared libraries from the virtualenv installation rather than the user directory. The current behavior is quite unintuitive.", "I am also hit by the problem -- with TF 2.4.0rc3, `pip install` in a virtual environment followed by `import tensorflow as tf` fails with\r\n```\r\n~/.local/lib/python-3.6/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n```\r\n\r\n[EDIT: TF 2.3 is already installed in the user site.]\r\n\r\n[EDIT: The workaround of editing `__init__.py` works, but the package seem to be broken to me if it requires editing its sources to work in virtual environment.]\r\n\r\nThis makes virtual environments unusable with TF :-(", "BTW, shouldn't the `site.USER_SITE` be used only when `site.ENABLE_USER_SITE` is True?", "I created a PR #45399 to address the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42978\">No</a>\n", "Which __init__.py should you edit? \r\nI know to comment  `_site_packages_dirs += [] if _site.USER_SITE is None else [_site.USER_SITE]` this out but in which version of tensorflow, the one in the venv?\r\n\r\nEdit all I see in __init__.py is:\r\n```\r\n# Get sitepackages directories for the python installation.\r\n_site_packages_dirs = []\r\n_site_packages_dirs += [_site.USER_SITE]\r\n_site_packages_dirs += [_p for _p in _sys.path if 'site-packages' in _p]\r\nif 'getsitepackages' in dir(_site):\r\n  _site_packages_dirs += _site.getsitepackages()\r\n```", "The issue is fixed in TF 2.4.1 and tf-nightly, so it should be enough to use a new TF.\r\n\r\nBut if you want to fix it manually, you need to fix the one you are actually importing (which is the one in venv, I suppose)."]}, {"number": 42977, "title": "ERROR: No matching distribution found for tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): py -m pip install tensorflow\r\n- Python version: Python 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\nError Message:\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n\r\nIm trying to install Tensorflow through pip but it doesn't seem to work. I have upgraded both pip and python to the newest versions\r\n\r\n\\- Regards\r\n", "comments": ["I think I worked it out by using this:\r\n\r\n`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl`\r\n\r\nI am closing this now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42977\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42977\">No</a>\n", "Wrong command there. I used the Anaconda Prompt with the normal pip command.\r\n\r\n`py -m pip install tensorflow`"]}, {"number": 42976, "title": "UnimplementedError: Fusion is not implemented: [BiasAdd,Add]", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):some source, some binary\r\n- TensorFlow version (use command below):unknown 2.3.0\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:11/8\r\n- GPU model and memory: GTX1070\r\n\r\n**Describe the current behavior**\r\nRunning the following code with TF 2.3:\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_eager_execution()\r\n\r\ndef model():\r\n    a = tf.get_variable('x1', [1, 256, 200, 304])\r\n    b = tf.get_variable('x2', [1, 256, 100, 152])\r\n\r\n    def upsample2x(x):\r\n        resize = tf.image.resize_images\r\n        shp2d = tf.shape(x)[2:]\r\n        x = tf.transpose(x, [0, 2, 3, 1])\r\n        x = resize(x, shp2d * 2, 'nearest')\r\n        x = tf.transpose(x, [0, 3, 1, 2])\r\n        return x\r\n\r\n    def conv(x, kernel):\r\n        return tf.layers.Conv2D(256, kernel, padding='SAME',\r\n                data_format='channels_first')(x)\r\n\r\n    a, b = [conv(c, 1) for c in [a, b]]\r\n    a = a + upsample2x(b)\r\n    out = [conv(x, 3) for x in [a, b]]\r\n\r\n    loss = tf.add_n([tf.reduce_mean(x) for x in out])\r\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss,\r\n            var_list=tf.trainable_variables())\r\n    return train_op\r\n\r\nwith tf.device('/gpu:0'):\r\n    train_op = model()\r\nconfig = tf.ConfigProto()\r\nsess = tf.Session(config=config)\r\nwith sess.as_default():\r\n    sess.run(tf.global_variables_initializer())\r\n    train_op.run()\r\n```\r\n\r\nthrows the following error in __certain__ environments:\r\n```\r\n2020-09-04 18:42:52.932125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-09-04 18:42:53.726786: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-04 18:42:53.731417: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3600000000 Hz\r\n2020-09-04 18:42:53.731651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562fabf1f410 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-04 18:42:53.731662: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-04 18:42:53.733086: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-04 18:42:53.796642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-04 18:42:53.797070: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562fabfc86d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-04 18:42:53.797085: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2020-09-04 18:42:53.797204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-04 18:42:53.797483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-09-04 18:42:53.797507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-09-04 18:42:53.799077: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-09-04 18:42:53.799724: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-04 18:42:53.799867: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-04 18:42:53.801546: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-04 18:42:53.801930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-09-04 18:42:53.802055: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-09-04 18:42:53.802122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-04 18:42:53.802476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-04 18:42:53.802778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-04 18:42:53.802801: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-09-04 18:42:54.070300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-04 18:42:54.070324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-04 18:42:54.070330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-04 18:42:54.070460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-04 18:42:54.070791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-04 18:42:54.071130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5626 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-09-04 18:42:54.071508: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2020-09-04 18:42:54.294130: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at conv_ops_fused_impl.h:700 : Unimplemented: Fusion is not implemented: [BiasAdd,Add]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1349, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1441, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,Add]\r\n         [[{{node add}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 37, in <module>\r\n    train_op.run()\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 2551, in run\r\n    _run_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 5547, in _run_using_default_session\r\n    session.run(operation, feed_dict)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 957, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1180, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1358, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,Add]\r\n         [[node add (defined at bug.py:23) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node add:\r\n transpose_1 (defined at bug.py:15)\r\n conv2d/BiasAdd (defined at bug.py:19)\r\n\r\nOriginal stack trace for 'add':\r\n  File \"bug.py\", line 32, in <module>\r\n    train_op = model()\r\n  File \"bug.py\", line 23, in model\r\n    a = a + upsample2x(b)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\", line 1125, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\", line 1447, in _add_dispatch\r\n    return gen_math_ops.add_v2(x, y, name=name)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 495, in add_v2\r\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\r\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3477, in _create_op_internal\r\n    ret = Operation(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n\r\nEnvironment that I have tried:\r\n1. GTX1070, cuda11, cudnn8: failed\r\n2. GTX1080Ti, cuda10.2, cudnn8: failed\r\n3. GTX1080Ti, cuda10.2, cudnn7.6: failed\r\n    * under the same environment, using TF1.15 instead of TF2.3: succeed\r\n4. P100, cuda10.1, cudnn7.6: succeed\r\n5. V100, cuda10.1, cudnn7.6: succeed\r\n6. Colab with Tesla T4, cuda10.x, cudnn7: succeed\r\n\r\nMaybe this is unique to GPUs that have compute capability 6.1", "comments": ["@ppwwyyxx,\r\nLooks like the issue is with the cuDNN and CUDA packages. TensorFlow 2.3 is compatible with cuDNN 7.6 and CUDA 10.1.\r\n\r\nFor more information, please check the [tested build configurations](https://www.tensorflow.org/install/source#gpu). Thanks!", "> TensorFlow 2.3 is compatible with cuDNN 7.6 and CUDA 10.1.\r\n\r\nMore precisely, __the prebuilt binaries of TensorFlow 2.3__ is compatible with cuDNN 7.6 and CUDA 10.1. But the code of TensorFlow 2.3 is compatible with at least cuDNN 7\\~8 and CUDA 10\\~11.\r\nFor all the versions I tested that are not using cuDNN 7.6 and CUDA 10.1, I was using tensorflow built by myself or by the archlinux distro. \r\n\r\nHowever, providing a reproducible build so others can reproduce the issue is too much work. I guess I'll just close this for now and wait to see if the bug can be reproduced when TF officially builds with CUDA 10.2+"]}, {"number": 42975, "title": " [ROCm] Cannot find rocm library hip_hcc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: N/A\r\n- ROCm version: 3.7.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen building I am getting the following error:\r\n```\r\nINFO: Repository local_config_rocm instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule rocm_configure defined at:\r\n  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl:866:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_rocm':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 845\r\n\t\t_create_local_rocm_repository(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 652, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 453, in _find_libs\r\n\t\t_select_rocm_lib_paths(<3 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 422, in _select_rocm_lib_paths\r\n\t\tauto_configure_fail(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 162, in auto_configure_fail\r\n\t\tfail(<1 more arguments>)\r\n\r\nROCm Configuration Error: Cannot find rocm library hip_hcc\r\nERROR: Skipping '//tensorflow:libtensorflow.so': no such package '@local_config_rocm//rocm': Traceback (most recent call last):\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 845\r\n\t\t_create_local_rocm_repository(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 652, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 453, in _find_libs\r\n\t\t_select_rocm_lib_paths(<3 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 422, in _select_rocm_lib_paths\r\n\t\tauto_configure_fail(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 162, in auto_configure_fail\r\n\t\tfail(<1 more arguments>)\r\n\r\nROCm Configuration Error: Cannot find rocm library hip_hcc\r\nERROR: no such package '@local_config_rocm//rocm': Traceback (most recent call last):\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 845\r\n\t\t_create_local_rocm_repository(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 652, in _create_local_rocm_repository\r\n\t\t_find_libs(repository_ctx, <2 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 453, in _find_libs\r\n\t\t_select_rocm_lib_paths(<3 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 422, in _select_rocm_lib_paths\r\n\t\tauto_configure_fail(<1 more arguments>)\r\n\tFile \"/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl\", line 162, in auto_configure_fail\r\n\t\tfail(<1 more arguments>)\r\n\r\nROCm Configuration Error: Cannot find rocm library hip_hcc\r\nINFO: Elapsed time: 18.566s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow ... (2 packages)\r\n    Fetching @com_google_protobuf; fetching\r\n    Fetching ...otobuf; Extracting /home/acxz/.cache/bazel/_bazel_acxz/032a3a4c537da51b5f6f596867eba382/external/com_google_prot\\\r\nobuf/v3.9.2.zip\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. git clone\r\n2. `export TF_NEED_ROCM=1`\r\n3. `./configure`\r\n4. \r\n```\r\n  bazel \\\r\n        build --config=mkl --config=avx2_linux -c opt \\\r\n          //tensorflow:libtensorflow.so \\\r\n          //tensorflow:libtensorflow_cc.so \\\r\n          //tensorflow:install_headers \\\r\n          //tensorflow/tools/pip_package:build_pip_package\r\n      bazel-bin/tensorflow/tools/pip_package/build_pip_package --gpu \"${srcdir}\"/tmpoptrocm\r\n```\r\n\r\nTo be exactly precise I am using the following build script (PKGBUILD):\r\nhttps://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=tensorflow-rocm\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDownstream issue: https://github.com/rocm-arch/tensorflow-rocm/issues/5\r\n", "comments": ["@acxz \r\nPlease confirm if this is a duplicate of #42081", "Not a duplicate, #42081 has already been addressed with #42095 \r\nEssentially the problem boils down to hip_hcc is not included in ROCm anymore. (it was in 3.5 but is not in 3.7)\r\n\r\nI believe this issue can be closed if #42689 gets merged.", "Closing this issue as #42689 has been merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42975\">No</a>\n"]}, {"number": 42974, "title": "[TF-TRT] Adding mixed precision support for non converted OPs & native segments", "body": "This PR adds support for TF-TRT mixed precision on the Tensorflow OPs that are not converted by TF-TRT and the fallback native segments.\r\n\r\n**Changes in the API:**\r\n- `TrtConversionParams` obtains a new parameter: `allow_mixed_precision_on_unconverted_ops`, True by default. Can be turned off manually.\r\n- This behavior is **only active** if `TrtConversionParams.precision_mode < FP32` (in other words if user request FP16 or INT8)\r\n", "comments": ["@DEKHTIARJonathan  Can you please check @bixia1's comments and keep us posted ? Thanks!", "@bixia1 I fixed all the comments. Any further comment or we can merge this PR ?\r\nThanks a lot", "@bixia1 changes requested applied.\r\n\r\n```\r\nINFO: Found applicable config definition test:v2 in file /opt/tensorflow/tensorflow-source/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only         \r\nINFO: Found applicable config definition build:cuda in file /opt/tensorflow/tensorflow-source/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1     \r\nINFO: Found applicable config definition build:linux in file /opt/tensorflow/tensorflow-source/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --c xxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels          \r\nINFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/tensorflow-source/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed 6 targets (371 packages loaded, 32875 targets configured).\r\nINFO: Found 4 targets and 2 test targets...      \r\nINFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/sandbox      \r\nINFO: Elapsed time: 40.294s, Critical Path: 27.30s\r\nINFO: 2 processes: 2 local.  \r\nINFO: Build completed successfully, 3 total actions                               \r\n//tensorflow/python/compiler/tensorrt:trt_convert_test                   PASSED in 19.3s     \r\n//tensorflow/python/compiler/tensorrt:trt_convert_test_gpu               PASSED in 19.2s\r\n\r\nExecuted 2 out of 2 tests: 2 tests pass.\r\nINFO: Build completed successfully, 3 total actions    \r\n```", "@DEKHTIARJonathan  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned : I seriously don't see how can I address this issue. I have no idea what is failing, no detail is being given:\r\n\r\n![image](https://user-images.githubusercontent.com/10923599/94190516-107afd80-fe61-11ea-8f6c-5be4ffeb8ed7.png)\r\nhttps://source.cloud.google.com/results/invocations/a10b2c39-0518-4278-af89-4aedecec0be8/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/tests\r\n\r\n@bixia1 any idea ?", "@gbaned could you re-trigger the job. Looks like the job failed. Not my changes\r\n", "@bixia1 I think I fixed the unittest (I forgot to update the v2 golden API pbtxt).\r\nCan you re-approve and rerun kokoro tests ?", "@bixia1 can you look at this test:\r\n![image](https://user-images.githubusercontent.com/10923599/94316100-99b13380-ff38-11ea-9e22-92bd0b5ceadc.png)\r\nhttps://source.cloud.google.com/results/invocations/38f8c701-c5e7-4a4a-844b-b10875aae322/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/tests\r\n\r\nI have no idea how to debug this, there's no error message\r\n", "working on merging it manually", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@gbaned working on it. We will need a few days", "Commits Squashed\r\n@bixia1 for final review & approval", "@bixia1 : we have the same problem as before, can you give it a look: https://source.cloud.google.com/results/invocations/5281c8a0-b309-4f20-b397-5791019e02e8/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/tests\r\n", "Seems auto-merge is not happening but the changes are now committed, so we can close this. Thank you for the PR."]}, {"number": 42971, "title": "[Intel MKL] Fix UT failures due to explicit padding for MKL-DNN", "body": "Latest explicit_padding changes in Maxpool/pooling related ops caused failures in MKL-DNN unit tests and internal benchmarks.\r\nThis patch, adds a check to by-pass MKL Maxpooling ops if explicit_padding is seen as an input.", "comments": []}, {"number": 42970, "title": "Enable FusedBatchNormV3 ops to support 5D tensors", "body": "This PR enables the FusedBatchNormV3 and FusedBatchNormGradV3 to support 5D tensors.\r\n\r\nSome potential benefits from this PR: \r\n(1) The 3D convolution networks using batch norm can use the FusedBatchNormV3 and benefit from cuDNN on GPU.\r\n(2) This PR doesn't take the reshape-based method (where we convert to 4D tensors using tf.reshape and then call nn.batch_normalization), because the inserted Reshape ops might prevent the grappler layout optimization pass from cancelling off the adjacent NHWC<->NCHW ops around batch normalization.\r\n\r\nFYI. @benbarsdell @nluehr ", "comments": ["@kaixih can you please resolve conflicts ?", "@reedwm @andyly Are your comments addressed to your satisfaction?", "@kaixih Can you please resolve conflicts? Thanks!", "The conflicts are resolved. PTAL."]}, {"number": 42969, "title": "Allow dict input_spec for keras.Model", "body": "This PR tries to address the issue raised in #42691 where\r\nusage of dict will cause ValueError in keras.Model.\r\n\r\nThis PR fixes #42691.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @qlzh727 for the review. The PR has been updated. Please take a look.", "> Thanks @qlzh727 for the review. The PR has been updated. Please take a look.\r\n\r\nSomehow the test file is removed from the PR. Can u please add it back? Thanks.", "@yongtang Can you please check @qlzh727's comments and keep us posted ? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@yongtang Any update on this PR? and please resolve conflicts Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 42968, "title": "Large number is multiplied with the embedding output", "body": "## URL(s) with the issue:\r\n\r\nThe url with the issue is https://www.tensorflow.org/tutorials/text/transformer\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nWhile, I was checking the transformers code from this [tensorflow documentation](https://www.tensorflow.org/tutorials/text/transformer), in the **Encoder** subsection under **Encoder and decoder** section, I found that before adding positional encoding, square root of `d_model` is multiplied with the embedding output. I doubt what is the specific reason behind multiplying the embedding output with a large number. Attaching the screenshot of the specific code snippet from the link.\r\n\r\n<img width=\"551\" alt=\"Screenshot 2020-09-05 at 1 01 55 AM\" src=\"https://user-images.githubusercontent.com/44110256/92278708-69ffa480-ef13-11ea-8e02-7f20f939bb6a.png\">\r\n", "comments": ["This Stack Overflow [thread](https://stackoverflow.com/questions/56930821/why-does-embedding-vector-multiplied-by-a-constant-in-transformer-model) tries to address the very same question.\r\nPerhaps you can engage with the community there to know more. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42966, "title": "Misleading error message on keras.Model initialization", "body": "### System information\r\n---\r\n Ubuntu linux 20.04\r\nFrom pip, tf version 2.3.0\r\n\r\n### Describe the current behavior\r\n---\r\nConsider this simple code from [this tutorial](https://www.tensorflow.org/guide/keras/sequential_model#feature_extraction_with_a_sequential_model)\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nassert tf.__version__ == '2.3.0'\r\n\r\ninitial_model = keras.Sequential(\r\n    [\r\n        keras.Input(shape=(250, 250, 3)),\r\n        keras.layers.Conv2D(32, 5, strides=2, activation=\"relu\"),\r\n        keras.layers.Conv2D(32, 3, activation=\"relu\"),\r\n        keras.layers.Conv2D(32, 3, activation=\"relu\"),\r\n    ]\r\n)\r\n\r\n\r\nfeature_extractor = keras.Model(\r\n    inputs=initial_model.inputs,\r\n    #Note the error!\r\n    blah=[layer.output for layer in initial_model.layers], \r\n)\r\n\r\n```\r\nAnd the error is:\r\n> TypeError: ('Keyword argument not understood:', 'inputs')  \r\n\r\n### Describe the expected behavior\r\n---\r\nI expect the error to contain a meaningful message, and at least to mention the incorrectly specified keyword `blah`. Just mentioning the one argument that is correctly written does not help. Maybe the error could be like \r\n> TypeError: ('Keyword argument not understood:', 'blah')  \r\n\r\nor   \r\n\r\n> TypeError: Invalid keyword arguments\r\n\r\n\r\n", "comments": ["I tried running the code you have mentioned  with a little change and got this error message: TypeError: ('Keyword argument not understood:', 'blah'). I suggest **replace** from tensorflow.keras import layers **with** from keras import layers", "@AnimeshMaheshwari22 Could you explain why? I have other keras backends and importing just keras would risk importing the wrong keras type. ", "@tornikeo,\r\nI was able to reproduce the error with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/914cd3beedc2d86332c8b4c66ceff3c7/42966.ipynb). However, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3bd91481518d8b4c4e21161396be59f1/42966-tf-nightly.ipynb). Please find the attached gist. Thanks!", "That resolves this issue. Thanks @amahendrakar!"]}, {"number": 42965, "title": "[TFLite] Fix uint8 MUL operator with broadcast when the scaling factor is >= 1", "body": "Hi,\r\n\r\nThe uint8 `MUL` operator with broadcast uses `MulSimpleBroadcast` to do the multiplication which in turn uses `MultiplyByQuantizedMultiplierSmallerThanOneExp` for the scaling. The problem is that in the `Prepare` method of `MUL` there is no guarantee that the scaling factor is smaller than one (it's simply quantized with `QuantizeMultiplier` without any check) .\r\n\r\nI changed the `MulSimpleBroadcast` to use `MultiplyByQuantizedMultiplier` instead and adapted the multiplication with broadcast test to also test the case where the scaling factor is greater or equal to one.\r\n\r\nThe following script can be used to reproduce the bug.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput1_shape = (2,)\r\n# Doesn't work (broadcasting)\r\ninput2_shape = (1,)\r\n# Works (no broadcasting)\r\n# input2_shape = (2,)\r\n\r\ndef get_dequantized_value(quantized_val, params):\r\n    quant_params = params[\"quantization_parameters\"]\r\n    scale = quant_params[\"scales\"]\r\n    zero_point = quant_params[\"zero_points\"]\r\n\r\n    return (quantized_val - zero_point) * scale\r\n\r\n# Model\r\ninput1 = tf.keras.Input(shape=input1_shape)\r\ninput2 = tf.keras.Input(shape=input2_shape)\r\noutput = tf.keras.layers.Multiply()([input1, input2])\r\nmodel = tf.keras.Model(inputs=[input1, input2], outputs=output)\r\nmodel.save(\"model.h5\")\r\n\r\n# Convert to TFLite\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(\"model.h5\")\r\nconverter.inference_type = tf.compat.v1.lite.constants.QUANTIZED_UINT8\r\ninput_arrays = converter.get_input_arrays()\r\nconverter.quantized_input_stats = {input_arrays[0]: (0.0, 3.0),\r\n                                   input_arrays[1]: (0.0, 3.0)}\r\nconverter.default_ranges_stats = (0, 9.0)\r\ntflite_model = converter.convert()\r\n\r\n# Test TFLite\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninput1 = np.full(shape=(1, *input1_shape), fill_value=1, dtype=np.uint8)\r\ninterpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], input1)\r\nprint(\"input1: \", get_dequantized_value(input1, interpreter.get_input_details()[0]))\r\n\r\ninput2 = np.full(shape=(1, *input2_shape), fill_value=1, dtype=np.uint8)\r\ninterpreter.set_tensor(interpreter.get_input_details()[1][\"index\"], input2)\r\nprint(\"input2: \", get_dequantized_value(input2, interpreter.get_input_details()[1]))\r\n\r\ninterpreter.invoke()\r\n\r\noutput = interpreter.get_tensor(interpreter.get_output_details()[0][\"index\"])\r\nprint(\"output: \", get_dequantized_value(output, interpreter.get_output_details()[0]))\r\n```", "comments": []}, {"number": 42964, "title": "MicroInterpreter::tensors_size() always returns zero", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 32\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 238981a91a9b780ab4449829469a71c5d668a273\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): x86\r\n\r\n**Describe the problem**\r\n\r\nThe tensors_size() method of the MicroInterpreter class always returns zero.  \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nTo demonstrate, insert the following line:\r\n\r\n`TF_LITE_MICRO_EXPECT_GT(static_cast<size_t>(0), interpreter.tensors_size());`\r\n\r\nhere: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/micro_interpreter_test.cc#L87\r\n\r\nAnd run:\r\n\r\n`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=\"x86\" test_micro_interpreter_test`\r\n\r\n", "comments": ["@advaitjain Could you take a look at this issue?", "@keithm-xmos I think you are using a bad TARGET=\"x86\". Can you try and remove that?\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile test_micro_interpreter_test`\r\n\r\nThat passes for me on Darwin x86_64.\r\n\r\n", "@nkreeger Did you make the modification to micro_interpreter_test.cc before rebuilding and running your suggestion?  It fails on x86 and xcore for me.  I think you will find that it will fail on any platform.  ", "@keithm-xmos ah sorry i missed that - our macro is a little confusing and the test code isn't the cleanest at the moment.\r\n\r\nTry this:\r\nTF_LITE_MICRO_EXPECT_GT(interpreter.tensors_size(), static_cast<size_t>(0));\r\n\r\nThat works for me - the macro is checking that |0 > tensors_size| which is always false.\r\n", "@nkreeger I applied your suggestion and I get the following: \r\n\r\n```\r\nTesting TestInterpreter\r\ninterpreter.tensors_size() > static_cast<size_t>(0) failed at tensorflow/lite/micro/micro_interpreter_test.cc:87\r\nOutput tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!\r\nTesting TestMultiTenantInterpreter\r\nTesting TestKernelMemoryPlanning\r\nOutput tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!\r\nOutput tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!\r\nOutput tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!\r\nOutput tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!\r\nOutput tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!\r\nOutput tensors not at index 0 are allocated from the persistent memory arena. Repeat calls will cause excess allocation!\r\nTesting TestVariableTensorReset\r\nTesting TestIncompleteInitialization\r\nTesting InterpreterWithProfilerShouldProfileOps\r\nTesting TestIncompleteInitializationAllocationsWithSmallArena\r\nFailed to allocate tail memory. Requested: 240, available 208, missing: 32\r\nFailed to allocate memory for context->eval_tensors, 240 bytes required\r\nFailed starting model allocation.\r\n\r\nTesting TestInterpreterDoesNotAllocateUntilInvoke\r\n[RecordingMicroAllocator] Arena allocation total 768 bytes\r\n[RecordingMicroAllocator] Arena allocation head 32 bytes\r\n[RecordingMicroAllocator] Arena allocation tail 736 bytes\r\n[RecordingMicroAllocator] 'TfLiteEvalTensor data' used 240 bytes with alignment overhead (requested 240 bytes for 10 allocations)\r\n[RecordingMicroAllocator] 'Persistent TfLiteTensor data' used 0 bytes with alignment overhead (requested 0 bytes for 0 tensors)\r\n[RecordingMicroAllocator] 'Persistent TfLiteTensor quantization data' used 0 bytes with alignment overhead (requested 0 bytes for 0 allocations)\r\n[RecordingMicroAllocator] 'TfLiteTensor variable buffer data' used 40 bytes with alignment overhead (requested 12 bytes for 3 allocations)\r\n[RecordingMicroAllocator] 'NodeAndRegistration struct' used 168 bytes with alignment overhead (requested 168 bytes for 3 NodeAndRegistration structs)\r\n[RecordingMicroAllocator] 'Operator runtime data' used 0 bytes with alignment overhead (requested 0 bytes for 0 OpData structs)\r\n7/8 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n\r\nInterpreter has 0 tensors and 2 nodes\r\nInputs: 0\r\nOutputs: 2 3\r\n\r\n\r\nNode   0 Operator Custom Name mock_custom\r\n  Inputs: 0 1\r\n  Outputs: 2\r\nNode   1 Operator Custom Name mock_custom\r\n  Inputs: 0 1\r\n  Outputs: 3\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:368: test_micro_interpreter_test] Error 1\r\n```", "I ran with\r\n\r\n> make -f tensorflow/lite/micro/tools/make/Makefile test_micro_interpreter_test\r\n\r\nHere's my diff\r\n\r\n```\r\ndiff --git a/tensorflow/lite/micro/micro_interpreter_test.cc b/tensorflow/lite/micro/micro_interpreter_test.cc\r\nindex a4a4143a2a..c5a13af8da 100644\r\n--- a/tensorflow/lite/micro/micro_interpreter_test.cc\r\n+++ b/tensorflow/lite/micro/micro_interpreter_test.cc\r\n@@ -84,6 +84,7 @@ TF_LITE_MICRO_TEST(TestInterpreter) {\r\n     TF_LITE_MICRO_EXPECT_LE(interpreter.arena_used_bytes(), 928 + 100);\r\n     TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(1), interpreter.inputs_size());\r\n     TF_LITE_MICRO_EXPECT_EQ(static_cast<size_t>(2), interpreter.outputs_size());\r\n+    TF_LITE_MICRO_EXPECT_GT(interpreter.tensors_size(), static_cast<size_t>(0));\r\n \r\n     TfLiteTensor* input = interpreter.input(0);\r\n     TF_LITE_MICRO_EXPECT_NE(nullptr, input);\r\n\r\n```", "@keithm-xmos sorry I had a bad vim buffer - yes this was accidentally regressed. I just opened https://github.com/tensorflow/tensorflow/pull/43109 to fix this -\r\n\r\nHopefully this isn't breaking you on anything right now - this API is mostly around for unit tests and inspecting of a model. I'd advise against looping through the graph on that API. Those values are now allocated from the \"persistent\" memory area of the arena. I have some documentation coming on those changes.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42964\">No</a>\n", "Should be fixed with this change: https://github.com/tensorflow/tensorflow/commit/ba071d416fc3d00b79021658c8d666b40b3fcb77"]}, {"number": 42963, "title": "Muliple conventions ", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/quickstart/beginner\r\nhttps://www.tensorflow.org/tutorials/keras/classification?hl=en, https://keras.io/api/models/sequential/\r\nhttps://keras.io/guides/sequential_model/\r\n\r\n## Description of issue (what needs changing):\r\ntf.keras.layers.Dense\r\nkeras.layers.Dense\r\nlayers.Dense\r\n\r\nMuliple conventions are followed. Which is the preferred way?", "comments": ["basically tensorflow uses keras as backend ,so its  basically depends upon which one your using either keras or tennsorflow.\r\nif your using keras ,then you can use\r\n `import keras`\r\n`from keras.layers import Dense`\r\n`model= Sequential()`\r\n`model.add(Dense(120, activation='relu'))`\r\n\r\nif you want to use tensorflow  you  can  use tensorflow\r\n\r\n`import tensorflow as tf`\r\n`model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))`\r\n \r\nbasically both codes working same.\r\nKeras is a neural network library while TensorFlow is the open-source library for a number of various tasks in machine learning. TensorFlow provides both high-level and low-level APIs while Keras provides only high-level APIs. **Keras is built in Python which makes it way more user-friendly than TensorFlow**\r\n\r\nfor your reference i have uploded all different architecture and there codes. \r\n[https://github.com/akashAD98/Deep_learing/tree/master/Diffrent%20CNN](url)", "The answer is: `layers.Dense` is preferred (over `tf.keras.layers.Dense`, which is too long, or over `Dense`, which tends to involve importing many individual symbols)."]}, {"number": 42962, "title": "Specify the optimization level in a variable.", "body": "Makes it possible to manually specify the optimization used by the compiling by appending OPTIMIZATION_LEVEL=-Ofast (for example) when compiling with make.\r\n\r\nFixes #42924", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "One thought I had immediately after submitting this pull request is if I should document this feature somewhere in a README?", "> One thought I had immediately after submitting this pull request is if I should document this feature somewhere in a README?\r\n\r\nYeah, that would be nice. Unfortunately we don't have good organization for documenting the Makefiles at this time. We will be taking a pass at documentation once we make some changes to the framework."]}, {"number": 42961, "title": "Where is the source code for keras v2.4.3?", "body": "Not found in both tensorflow and keras github repo?\r\n\r\nIt is released before tensorflow 2.3.0. Why it is not integrated into this?", "comments": ["Keras was redirected to `tf.keras` in https://github.com/keras-team/keras/pull/14121/.\r\nJust a few commits later the version was bump to v2.4.3 https://github.com/keras-team/keras/commit/1a3ee8441933fc007be6b2beb47af67998d50737", "@SmartManoj,\r\nAdding to @bhack's comment, you can read more about this from the [release notes](https://github.com/keras-team/keras/releases) of Keras 2.4.0.\r\n\r\nLink to source code of [Keras v2.4.3](https://github.com/keras-team/keras/blob/master/keras/__init__.py#L32) for reference. Thanks!", "\r\n\r\nUsed [this](https://github.com/keras-team/keras/search?q=%272.4.3%27&unscoped_q=%272.4.3%27&type=Commits) search but found '.' is not supported [yet](https://github.com/isaacs/github/issues/402) in search \r\n"]}, {"number": 42960, "title": "Tensorflow build with AVX2 without warnings of ignoring it. But after the installation it is not using it anymore.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-Windows 10 Pro\r\n-GTX 1060 3gb\r\n-compute capabilities 6.1\r\n-Python 3.6\r\n-Tensorflow 2.3\r\n-bazel 3.1.0\r\n-TensorRT 6.0\r\n-using AVX2\r\n\r\n\r\n\r\n**Describe the problem**\r\nI build tensorflow 2.3 with all the infos at the top. In the python ./configure.py part in the installation i choosed \"Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\". While building there was no Error or Warning that it would be ignored. \r\nAfter the installation of the package i ran a deepspeech command to test it and it printed out this : \"I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\". So my CPU supports AVX2 but my build wasnt compile to use that.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nC:\\TenserFlow\\tensorflow>python ./configure.py\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\Python38-32\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Python38-32\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Python38-32\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /arch:AVX2\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n\r\nC:\\TenserFlow\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nC:\\TenserFlow\\tensorflow>bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\r\n\r\nC:/tmp/tensorflow_pkg>pip3 install C:/tmp/tensorflow_pkg/tensorflow-version-cp36-cp36m-win_amd64.whl\r\n\r\nThen i ran DeepSpeech that used my installed Tensorflow Version.\r\ndeepspeech --model deepspeech-0.8.1-models.pbmm --scorer deepspeech-0.8.1-models.scorer --audio audio/2830-3980-0043.wav\r\n\r\nThe output works fine but it also gives the Info that AVX2 isnt used.", "comments": ["Can you past the content of your `.tf_configure.bazelrc`?", "build --action_env PYTHON_BIN_PATH=\"C:/Python38-32/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Python38-32/lib/site-packages\"\r\nbuild --python_path=\"C:/Python38-32/python.exe\"\r\nbuild --config=xla\r\nbuild:opt --copt=/arch:AVX2\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --define=override_eigen_strong_inline=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n", "Do you had any `cl : Command line warning` on build?", "no only found this:\r\nINFO: Found applicable config definition build:opt in file c:\\tenserflow\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true", "What is your MSVC version?", "Newest Version so Visual Studio 2019\r\n", "It seems all ok. I don't only understand why we have this `--copt` difference with https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L365\r\nSorry but I don't have a windows machine to debug it more. Waiting for @Saduf2019 ", "@Ch33s3Burger What looks a bit suspcious is the C:/Python38-**32**/python.exe part, initial \"System information\" section mentions \"Python 3.6\". Would you mind running `C:\\Python38-32\\python.exe -VV` and make sure it's a 64bit python build, not \r\n32bit? TF requires 64bit. Worth double checking if the freshly built wheel is really the one that is being installed and gives the AVX2 warning.\r\n\r\nI do have a Windows machine and could try to reproduce.", "@ahtik Nice catch. Yes python 64 Is required.", "@Ch33s3Burger \r\nYou could be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python or 32-bit OS\r\n[Your CPU does not support AVX instructions, please provide the make and model of your CPU in this case.]\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check these similar duplicate issues: #42058 #41596 #40459 #39007 #38916 #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 .\r\n\r\nThanks!", "Thx for the catch with python 32-bit i fixed it. I also looked over the system requirements and i have all. And as above metioned my CPU can use AVX2. I am trying it now with python 3.6 64-bit.", "This is my .tf_configure.bazelrc\r\n\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Python36/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Python36/lib/site-packages\"\r\nbuild --python_path=\"C:/Python36/python.exe\"\r\nbuild --config=xla\r\nbuild:opt --copt=/arch:AVX2\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --define=override_eigen_strong_inline=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\nBut still after building and installing its still not compiled to AVX2", "@Ch33s3Burger Could you post the output from `C:/Python36/python.exe -VV`?\r\n\r\nOne other \"moving part\" is the MS VS Built Tools path, hopefully the build is picking up a good version. It's cl.exe compiler is what is eventually getting the AVX2 flag. Mine is at `C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.26.28801\\bin\\Hostx64\\x64\\cl.exe`. To set/override it for the TF build, use smth like `SET BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC`. ", "C:\\tmp\\tensorflow_pkg>python -VV\r\nPython 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)]", "Where should i put this \"SET BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\" command?", "`SET BAZ....=....` goes to the same command prompt used for building, before running the TF configure and build commands. To set the env variables. Run `set` to see all current ones. But in your case it's likely not going to help/TF build should pick up the latest installed compiler anyway and BAZEL_VC is about more precise control about the build env.\r\n\r\nNot sure if it helps right now but I posted some notes on building with Windows at https://github.com/tensorflow/tensorflow/issues/38174#issuecomment-613722488.\r\n\r\nTo pinpoint the issue, let's reduce the surface area, try running `python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([10000, 10000])));print(tf.random.__file__)\"` instead of deepspeech.", "@Ch33s3Burger \r\nYou may also refer to #38712", ">>> import tensorflow as tf\r\n>>> print(tf.reduce_sum(tf.random.normal([10000, 10000])))\r\n2020-09-11 11:57:43.047859: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2463f500370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-11 11:57:43.048081: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\ntf.Tensor(-6465.927, shape=(), dtype=float32)\r\n>>> print(tf.random.__file__)\r\nC:\\Python36\\lib\\site-packages\\tensorflow\\_api\\v2\\random\\__init__.py", "@Ch33s3Burger Looks like the AVX2 warning does not appear here and your wheel is fine, right? If the AVX2 were missing, it would have been reported already.\r\n\r\n**Maybe** it's a deepspeech build, bundling or runtime env topic after all?", "OK thx for your help. I am gonna take a look on deepspeech then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42960\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42960\">No</a>\n"]}, {"number": 42959, "title": "TFLu: Simplify CMSIS quantized AvgPooling", "body": "This PR slightly simplifies the CMSIS quantized AvgPooling kernel by splitting `AverageEvalQuantized` into separate functions for `int8` and  `unit8` similar to how maxpooling is handled.\r\nThis also removes the need  to construct `PoolParams` for the optimized `int8` kernel.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "I'm ok with this change.\r\n\r\nTagging @freddan80 from the CMSIS-NN team for their thoughts as well (before I formally approve this PR).", "Hi @lgeiger . Thanks for your contribution!\r\nIn general, we want to align the code flow of the CMSIS-NN kernel wrappers to the reference kernels:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/pooling.cc\r\nTherefore, we changed to using `AverageEvalQuantized` some months ago with: `c69f7964570b5d731451b39a02ebfc5e814bd360`. \r\nMaxPool is handled in a similar way in the reference kernel:\r\nhttps://github.com/tensorflow/tensorflow/blob/62a92db4ec9cc1738718f0cde2c29062eec96db9/tensorflow/lite/micro/kernels/pooling.cc#L202\r\nTherefore we'd actually like to change the MaxPool in to using the same style. Hope this makes sense.\r\n\r\nCheers!\r\nFredrik\r\n", "Thanks for the response, closing the PR then"]}, {"number": 42958, "title": "Tensorflow and RTX 3000 (TF 1.X or 2.X required)", "body": "1. Will tensorflow support new cards?\r\n2. Which version of TF is required?\r\n3. Will I run the program on version tf.1.x with RTX3000?", "comments": ["I think yes. If you can see there are also some specific feature in nightly for the Ampere arch in  https://github.com/tensorflow/community/pull/287.\r\nAlso if currently the CI is not running on this card GEN:\r\nhttps://github.com/tensorflow/community/pull/287/files#diff-0f47247f8a899bec07e8ab8b8fa219d4R34\r\n\r\nI don't think it will officially support new CUDA releases in TF 1.x as you can see in https://github.com/tensorflow/tensorflow/issues/42895#issuecomment-685874736\r\n\r\n/cc @reedwm ", "@bhack is correct, TensorFlow will support RTX 3000 cards. I don't think TF 2.3 will work on the upcoming cards, but TF 2.4 will. Unfortunately TF 2.4 will be released after the cards are released, but tf-nightly will work as well.\r\n\r\nAs @bhack mentioned, TF 1.x does not support RTX 3000, as it only supports older versions of CUDA. But [you can TF1 programs](https://www.tensorflow.org/guide/migrate) that don't depend on `tf.contrib` in TF 2 (including TF 2.4) with\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\n\r\n", "RTX 3080 TI does not work with TF 1.12 CUDA 9.2\r\n\r\nSeems like no backward compatibility provided by TF.\r\n\r\nSo what reason to make applications using TF which is not future-proof ?", "What is an RTX 3080Ti, there is only a 3080 and 3090 right?"]}, {"number": 42957, "title": "GPU devices not detected in tf-nightly in latest releases (Google Colaboratory)", "body": "Hello, \r\n\r\nI installed `tf-nightly` in a Google Colaboratory notebook and noticed that the model training time is significantly longer than when I was training it at the beginning of August. Then I ran \r\n\r\n```\r\nprint(tf.test.gpu_device_name())\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\n\r\nand no GPU devices were detected, even though the accelerator was set to 'GPU'. \r\n\r\nBut after uninstalling the latest release and installing `tf-nightly-2.4.0-dev20200815` the GPU was successfully detected. \r\nTrying out different releases, seems like it doesn't work as of `2.4.0-dev20200819` release.\r\n", "comments": ["I confirm this. The c++ wrapper return only the CPU device:\r\n```\r\nfrom tensorflow.python import pywrap_tfe\r\nprint(pywrap_tfe.TF_ListPhysicalDevices())\r\n```", "/cc @martinwicke this could create some false negative to your first stage triagers when the protocol require them to test on Colab gists with nightly about reproducibility. This is limited to GPU only ticket that require GPU runtimes.\r\n\r\nE.g. See https://github.com/tensorflow/tensorflow/issues/42885", "P.s. I can also confirm that it starts with:\r\n`pip install tf-nightly==2.4.0.dev20200819`\r\n", "Was able to reproduce the issue with latest TF-nightly i.e. v2.4.0-dev20200904, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/25b8598fea215239f22d406a387c1735/42957.ipynb). Thanks!", "@drajsel I agree this is an issue with colab. Is this happening in the local with GPU? Thanks!", "`2.4.0-dev20200819` is the first release with CUDA 11, so it is very likely that that's the root issue.  Colab needs to be adjusted to use CUDA 11 instead of CUDA 10.1.", "@drajsel As mentioned by @sanjoy this is related to colab. Can you please open an issue in colab repository [here](https://github.com/googlecolab/colabtools). Thanks!", "CC @fischman", "@jvishnuvardhan I opened the issue [there](https://github.com/googlecolab/colabtools/issues/1574). Thank you all for your time!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42957\">No</a>\n"]}, {"number": 42956, "title": "benchmark  error  cannot run in phone", "body": "use  benchmark\r\n\r\nHWBKK-Q:/data/local/tmp $ ls\r\nbenchmark_model         libhexagon_nn_skel_v65.so     \r\nlibhexagon_interface.so libhexagon_nn_skel_v66.so     \r\nlibhexagon_nn_skel.so   mobilenet_quant_v1_224.tflite \r\n/benchmark_model --graph=./mobilenet_quant_v1_224.tflite --num_threads=4      <\r\n/system/bin/sh: ./benchmark_model: not executable: 64-bit ELF file\r\n1|HWBKK-Q:/data/local/tmp $ uname -a\r\nLinux localhost 4.9.82+ #1 SMP PREEMPT Mon Jul 20 02:03:26 CST 2020 aarch64\r\nHWBKK-Q:/data/local/tmp $ \r\n", "comments": ["bazel build -c opt --fat_apk_cpu=arm64-v8a tensorflow/lite/tools/benchmark:benchmark_model", "(base) chenxin@Nitro-AN515:~/Downloads/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark$ ./benchmark_model --graph=/home/chenxin/Downloads/tensorflow/mobilenet_quant_v1_224.tflite --num_threads=4\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [/home/chenxin/Downloads/tensorflow/mobilenet_quant_v1_224.tflite]\r\n#threads used for CPU inference: [4]\r\nLoaded model /home/chenxin/Downloads/tensorflow/mobilenet_quant_v1_224.tflite\r\nThe input model file size (MB): 4.2761\r\nInitialized session in 0.384ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=8 first=65650 curr=71055 min=64394 max=73197 avg=68889.1 std=3197\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=71198 curr=72666 min=69611 max=78963 avg=70973 std=1487\r\n\r\nInference timings in us: Init: 384, First inference: 65650, Warmup (avg): 68889.1, Inference (avg): 70973\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=3.90234 overall=10.8125\r\n", "Are you using the Android build section to compile `benchmark_model` to run in the phone? \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark#to-buildinstallrun", "You can also find pre-compiled android binary at https://www.tensorflow.org/lite/performance/measurement?hl=en", "@mathpopo   In short, what you have is not an arm64-v8a binary. The  `--fat_apk_cpu` is for building transient dependencies only, See https://docs.bazel.build/versions/master/user-manual.html#flag--fat_apk_cpu.\r\n\r\nEasy way to build `benchmark_model` for android as stated in benchmark_model's [README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md) is something like\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\nTo see what `android_arm64`  does, check TF's [.bazelrc](https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L105-L115)", "@bhack \r\n(base) chenxin@Nitro-AN515:~/Downloads/tensorflow$ bazel build -c opt \\\r\n>   --crosstool_top=//external:android/crosstool \\\r\n>   --cpu=armeabi-v7a \\\r\n>   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n>   --config monolithic \\\r\n>   tensorflow/tools/benchmark:benchmark_model\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/chenxin/Downloads/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/chenxin/Downloads/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/chenxin/Downloads/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/chenxin/Downloads/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file /home/chenxin/Downloads/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /home/chenxin/Downloads/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/chenxin/Downloads/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/tools/benchmark:benchmark_model (101 packages loaded, 4440 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /home/chenxin/.cache/bazel/_bazel_chenxin/f2be0bb8460d7bf102965465c62801f2/sandbox\r\nERROR: /home/chenxin/.cache/bazel/_bazel_chenxin/f2be0bb8460d7bf102965465c62801f2/external/com_google_absl/absl/base/BUILD.bazel:83:1: C++ compilation of rule '@com_google_absl//absl/base:spinlock_wait' failed (Exit 1)\r\nTarget //tensorflow/tools/benchmark:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 97.375s, Critical Path: 0.32s\r\nINFO: 5 processes: 5 local.\r\nFAILED: Build did NOT complete successfully\r\n", "@freedomtan \r\n(base) chenxin@Nitro-AN515:~/Downloads/tensorflow$ bazel build -c opt \\\r\n>   --config=android_arm64 \\\r\n>   tensorflow/lite/tools/benchmark:benchmark_model\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'build' from /home/chenxin/Downloads/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/chenxin/Downloads/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /home/chenxin/Downloads/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/chenxin/Downloads/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:android_arm64 in file /home/chenxin/Downloads/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /home/chenxin/Downloads/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nINFO: Build options --copt, --cpu, --define, and 1 more have changed, discarding analysis cache.\r\nERROR: /home/chenxin/.cache/bazel/_bazel_chenxin/f2be0bb8460d7bf102965465c62801f2/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\nERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 0.491s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded, 67 targets configured)\r\n\r\n\r\n(base) chenxin@Nitro-AN515:~/Downloads/tensorflow$ echo $NDK\r\n/home/chenxin/armnn-devenv/toolchains/android-ndk-r20b\r\n(base) chenxin@Nitro-AN515:~/Downloads/tensorflow$ \r\n", "it seems you didn't configure Android SDK and NDK, in the [README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md)\r\n```\r\n(0) Refer to https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android to edit the WORKSPACE to configure the android NDK/SDK.\r\n.....\r\n```", "@freedomtan   no where can see how set my ndk path?\r\n\r\nworkspace(name = \"org_tensorflow\")\r\n\r\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\r\n\r\nhttp_archive(\r\n    name = \"io_bazel_rules_closure\",\r\n    sha256 = \"5b00383d08dd71f28503736db0500b6fb4dda47489ff5fc6bed42557c07c6ba9\",\r\n    strip_prefix = \"rules_closure-308b05b2419edb5c8ee0471b67a40403df940149\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",\r\n        \"https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",  # 2019-06-13\r\n    ],\r\n)\r\n\r\n# Load tf_repositories() before loading dependencies for other repository so\r\n# that dependencies like com_google_protobuf won't be overridden.\r\nload(\"//tensorflow:workspace.bzl\", \"tf_repositories\")\r\n# Please add all new TensorFlow dependencies in workspace.bzl.\r\ntf_repositories()\r\n\r\nregister_toolchains(\"@local_config_python//:py_toolchain\")\r\n\r\nload(\"@io_bazel_rules_closure//closure:defs.bzl\", \"closure_repositories\")\r\n\r\nclosure_repositories()\r\n\r\nload(\"//third_party/toolchains/preconfig/generate:archives.bzl\",\r\n     \"bazel_toolchains_archive\")\r\n\r\nbazel_toolchains_archive()\r\n\r\nload(\r\n    \"@bazel_toolchains//repositories:repositories.bzl\",\r\n    bazel_toolchains_repositories = \"repositories\",\r\n)\r\n\r\nbazel_toolchains_repositories()\r\n\r\n# Use `swift_rules_dependencies` to fetch the toolchains. With the\r\n# `git_repository` rules above, the following call will skip redefining them.\r\nload(\"@build_bazel_rules_swift//swift:repositories.bzl\", \"swift_rules_dependencies\")\r\nswift_rules_dependencies()\r\n\r\n# We must check the bazel version before trying to parse any other BUILD\r\n# files, in case the parsing of those build files depends on the bazel\r\n# version we require here.\r\nload(\"//tensorflow:version_check.bzl\", \"check_bazel_version_at_least\")\r\ncheck_bazel_version_at_least(\"1.0.0\")\r\n\r\nload(\"//third_party/android:android_configure.bzl\", \"android_configure\")\r\nandroid_configure(name=\"local_config_android\")\r\nload(\"@local_config_android//:android.bzl\", \"android_workspace\")\r\nandroid_workspace()\r\n\r\n# If a target is bound twice, the later one wins, so we have to do tf bindings\r\n# at the end of the WORKSPACE file.\r\nload(\"//tensorflow:workspace.bzl\", \"tf_bind\")\r\ntf_bind()\r\n\r\nhttp_archive(\r\n    name = \"inception_v1\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"7efe12a8363f09bc24d7b7a450304a15655a57a7751929b2c1593a71183bb105\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v1.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"mobile_ssd\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"bddd81ea5c80a97adfac1c9f770e6f55cbafd7cce4d3bbe15fbeb041e6b8f3e8\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"mobile_multibox\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"859edcddf84dddb974c36c36cfc1f74555148e9c9213dedacf1d6b613ad52b96\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/download.tensorflow.org/models/mobile_multibox_v1a.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"stylize\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"3d374a730aef330424a356a8d4f04d8a54277c425e274ecb7d9c83aa912c6bfa\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/download.tensorflow.org/models/stylize_v1.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"speech_commands\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"c3ec4fea3158eb111f1d932336351edfe8bd515bb6e87aad4f25dbad0a600d0c\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/download.tensorflow.org/models/speech_commands_v0.01.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"person_detect_data\",\r\n    sha256 = \"170542270da256994ce24d1e357f6e84a54fdaf7d28ff2b74725a40b70b082cf\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_24.zip\",\r\n    ],\r\n)\r\n\r\n# Required for dependency @com_github_grpc_grpc\r\n\r\nload(\"@com_github_grpc_grpc//bazel:grpc_deps.bzl\", \"grpc_deps\")\r\n\r\ngrpc_deps()\r\n\r\nload(\r\n    \"@build_bazel_rules_apple//apple:repositories.bzl\",\r\n    \"apple_rules_dependencies\",\r\n)\r\n\r\napple_rules_dependencies()\r\n\r\nload(\r\n    \"@build_bazel_apple_support//lib:repositories.bzl\",\r\n    \"apple_support_dependencies\",\r\n)\r\n\r\napple_support_dependencies()\r\n\r\nload(\"@upb//bazel:repository_defs.bzl\", \"bazel_version_repository\")\r\n\r\nbazel_version_repository(name = \"bazel_version\")\r\n\r\nload(\"//third_party/googleapis:repository_rules.bzl\", \"config_googleapis\")\r\n\r\nconfig_googleapis()\r\n", "@mathpopo \r\nPlease update if this is still an issue.", "@mathpopo in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android, you can find the following message, \r\n\r\n```\r\nNOTE: As long as you have the SDK and NDK installed, the ./configure script will create these rules for you. \r\nAnswer \"Yes\" when the script asks to automatically configure the ./WORKSPACE.\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42955, "title": "tf.io.gfile.glob hangs if gcs directory contains a directory named '/'.", "body": "**Describe the current behavior**\r\n```\r\nmkdir -p .//\r\ngsutil -m rsync -r .//  gs://remote-folder//\r\n```\r\n```\r\nimport tensorflow as tf\r\ntf.io.gfile.glob('gs://remote-folder//*')\r\n```\r\nprogram hangs forever. The remote folder in google cloud storage has to contain a folder named '/'.  \r\nThe directory was created using `gsutil rsync`.\r\n\r\nThis was on Tensorflow 1.15.3 enterprise, and had the same behavior on 1.15.2 from pip.\r\n\r\nedit: just happened again. I am not sure what generates that directory. Tensorboard's 1.15 TPU profiler maybe?\r\n\r\n![image](https://user-images.githubusercontent.com/112599/92290815-6267dd80-eeca-11ea-875c-ebf6eac7b804.png)\r\n\r\n~~seems this line does the trick of creating that spurious directory:~~\r\n~~tensorboard --logdir=gs://$BUCKET/models/$MODEL/$EXPERIMENT/ --reload_multifile=true --master_tpu_unsecure_channel=$TPU_NAME~~\r\n~notice the trailing `/`~\r\n\r\nThis is the culprit:\r\n\r\n```python\r\nfrom tensorflow.contrib import summary\r\nwith summary.create_file_writer(os.path.join(self._log_dir, 'scalars')).as_default():\r\n    offset = 0\r\n     with summary.record_summaries_every_n_global_steps(\r\n```\r\nif self.log_dir ends with '/' summary writer will create that '/' directory. \r\nThe problem though is most likely in the logic of `gfile.glob`", "comments": ["Can you fill in issue template, please? Curious which version of TF are you using. Can you try with 2.3 and nightly please? I recall fixing a similar issue a few months ago.", "@mihaimaruseac updated with the tensorflow info. I use only version `1.15.x`. I deleted that abnormal directory on GCS so I can't reproduce. ", "Yeah, we did not backport that fix since we only do patch releases for security issues.", "@Mistobaan,\r\nSince [it is fixed](https://github.com/tensorflow/tensorflow/issues/42955#issuecomment-687287334) in **`Tensorflow Version 2.3`**, as it is recommended to switch to **`Tensorflow Version 2.x`**, from **`1.x`**,  can you please let us know if we can close this issue? Thanks!", "I'd close this issue as it was fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42955\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42955\">No</a>\n"]}, {"number": 42954, "title": "savermodel quantization arrive TFlite model,on the windows+python Test times are slow", "body": "quantization: \r\nimport tensorflow as tf\r\nsaved_model_dir=\"D:/tf_ocr/tf_resnet10/\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quantized_model=converter.convert()\r\nopen(\"D:/sfz/tf_sfz/model.tflite\",\"wb\").write(tflite_quantized_model)\r\n\r\ntest:\r\n inputImg=(img/255).astype(np.float32)\r\n        interpreter = tf.lite.Interpreter(model_path=model)\r\n        interpreter.allocate_tensors()\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n        interpreter.set_tensor(input_details[0]['index'], [inputImg])\r\n        a = time.time()\r\n        # print(a)\r\n        interpreter.invoke()\r\n        b = time.time()\r\n        # print(b)\r\n        print((b - a) * 1000)\r\n\r\ntensorflow 1.15.0 + python3.5 + windows10\r\n\r\nThe size of the model is reduced to its original size 1/4,but the savermodel test time about is 17.16ms,After the quantitative lite test time about is 55.62ms,\r\nThe speed was slowed down three times\r\n\r\n", "comments": ["@huafeihuayu,\r\nIn order to reproduce the issue reported here, could you please share the contents of the `tf_resnet10` folder with us?\r\n\r\nAlso TensorFlow 1.x is not actively supported, please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "> @amahendrakar ,\r\n\r\n\r\nsorry\uff0cDue to the company's confidentiality, I cannot provide it for the time being.This is still the case after I tried it with TensorFlow2.3.\r\nThe file model in our TF_resnet10 consists of CKPT converted into Savermodel. pb and variables after tensorFlow training. We directly converted into TF_lite and the quantified TF_LITE.During the training, the data is float32, the quantized TF_lite, the incoming data must also be float32, otherwise an error will be reported, and the quantized data is UINT8 according to the official website, whether the time is slow is related to this.\r\n", "> sorry\uff0cDue to the company's confidentiality, I cannot provide it for the time being.This is still the case after I tried it with TensorFlow2.3.\r\n\r\n@huafeihuayu,\r\nWithout a reproducible code, it'd be difficult for us to debug the issue. In this case, is it possible for you to provide a dummy model to replicate the issue reported here? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42953, "title": "Can't loading model from checkpoint if use adam optimizer", "body": "Tensorflow version:v2.3.0-rc2-23-gb36436b087 2.3.0\r\nUbuntu 18.04\r\n\r\n===================\r\ncode:\r\n```\r\nmodel.compile(optimizer=\"adam\",\r\n                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n                 metrics=[\"accuracy\"])\r\n\r\ncheckpoint_dir = 'training/'\r\ncheckpoint_path = os.path.join(checkpoint_dir,\"cp-{epoch:04d}.ckpt\")\r\ncheckpoint_path = os.path.join(checkpoint_path)\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_path,\r\n    verbose=1,\r\n    save_weights_only=True,\r\n    save_freq=1000)\r\n\r\nmodel.load_weights(tf.train.latest_checkpoint(os.path.dirname(checkpoint_dir))).expect_partial()\r\n```\r\nIf the optimizer = \"adam\" , load_weights get a error:\r\n```\r\nMake sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-14.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-14.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-16.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-16.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-18.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-18.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-20.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-20.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-22.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-22.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-24.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-24.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-26.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-26.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-28.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-28.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-30.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-30.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.moving_variance\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-32.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-32.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-13.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-13.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-14.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-14.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-15.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-15.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-16.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-16.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-17.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-17.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-18.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-18.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-19.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-19.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-20.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-20.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-21.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-21.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-22.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-22.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-23.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-23.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-24.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-24.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-25.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-25.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-26.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-26.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-27.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-27.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-28.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-28.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-29.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-29.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-30.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-30.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-31.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-31.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-32.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-32.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-13.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-13.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-14.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-14.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-15.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-15.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-16.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-16.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-17.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-17.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-18.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-18.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-19.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-19.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-20.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-20.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-21.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-21.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-22.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-22.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-23.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-23.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-24.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-24.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-25.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-25.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-26.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-26.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-27.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-27.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-28.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-28.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-29.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-29.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-30.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-30.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-31.gamma\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-31.beta\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-32.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-32.bias\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n```\r\n\r\nBut if optimizer ='sgm' ,everything is OK.  ", "comments": ["@smallworld-network-wupeng \r\nPlease provide with complete stand alone code for us to replicate, as i am facing a different error with the code shared.\r\nPlease find the [gist here](https://colab.research.google.com/gist/Saduf2019/50ecb53892c507ea8227682c16816ad7/untitled402.ipynb), or if possible share colab gist with error reported.\r\n", "full code:\r\n```\r\n\r\nfrom alfred.dl.tf.common import mute_tf\r\nmute_tf()\r\nimport pathlib\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom datetime import datetime\r\nimport numpy as np\r\nimport os\r\nfrom pynvml import *\r\nimport multiprocessing\r\nimport argparse\r\nimport time \r\n\r\nos.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\r\n\r\nparser = argparse.ArgumentParser(description=\"Image classification\")\r\nparser.add_argument('--input_dir',type=str,default=\"data_dir\",help='input image directory,default is data_dir')\r\nparser.add_argument('--epochs',type=int,default=500,help='epochs times,default is 500')\r\nparser.add_argument('--batch_size',type=int,default=64,help='batch size,default is 64')\r\nparser.add_argument('--from_cp',type=bool,default=False,help='training from last checkpoint')\r\nargs = parser.parse_args()\r\n\r\n\r\nl_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nnvmlInit()\r\ndevice_count = nvmlDeviceGetCount()\r\nfor i in range(device_count):\r\n    handle = nvmlDeviceGetHandleByIndex(i)\r\n    mem = nvmlDeviceGetMemoryInfo(handle).total // 1024 //1024\r\n    tf.config.experimental.set_virtual_device_configuration(gpus[i], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=mem*0.9)])\r\n    print('GPU Total Memory:{} limit: {}'.format(mem,mem*0.9))\r\n\r\ncpu_core = multiprocessing.cpu_count()\r\nprint('CPU total core: {}'.format(cpu_core))\r\n\r\nIMG_HEIGHT = 224\r\nIMG_WIDTH = 224\r\nNUM_WORKERS = cpu_core\r\nbatch_size = args.batch_size\r\nepochs = args.epochs\r\ndata_dir = args.input_dir\r\nfrom_cp = args.from_cp\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\n\r\n\r\n\r\n#gpus = tf.config.experimental.list_physical_devices('GPU')\r\n#tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=mem_0*0.9)])\r\n#tf.config.experimental.set_virtual_device_configuration(gpus[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=mem_0*0.9)])\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\n\r\ntrain_dir = pathlib.Path(os.path.join(data_dir,'train'))\r\nvalid_dir = pathlib.Path(os.path.join(data_dir,'valid'))\r\nimage_count = len(list(train_dir.glob('*/*.jpg'))) + len(list(valid_dir.glob('*/*.jpg')))\r\nclass_names = np.array(sorted([item.name for item in train_dir.glob('*')]))\r\nTRAIN_STEPS_PER_EPOCH = np.ceil((image_count*0.8/batch_size)-1)\r\nVAL_STEPS_PER_EPOCH = np.ceil((image_count*0.2/batch_size)-1)\r\n\r\ndef pre_process_image(image):\r\n\r\n    image = tf.image.random_flip_left_right(image)\r\n    #image = tf.image.random_hue(image, max_delta=0.05)\r\n    #image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\r\n    image = tf.image.random_brightness(image, max_delta=0.2)\r\n    #image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\r\n    #image = tf.image.rot90(image, np.random.randint(1,4))\r\n    return image\r\n\r\n\r\ndef get_label(file_path):\r\n    # convert the path to a list of path components\r\n    parts = tf.strings.split(file_path, os.path.sep)\r\n    # The second to last is the class-directory\r\n    one_hot = parts[-2] == class_names\r\n    print(class_names)\r\n    # Integer encode the label\r\n    return tf.argmax(one_hot)\r\n\r\ndef decode_img(img):\r\n    # convert the compressed string to a 3D uint8 tensor\r\n    img = tf.image.decode_jpeg(img, channels=3)\r\n    #img = pre_process_image(img)\r\n    # resize the image to the desired size\r\n    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\r\n\r\ndef process_path(file_path):\r\n    label = get_label(file_path)\r\n    # load the raw data from the file as a string\r\n    img = tf.io.read_file(file_path)\r\n    img = decode_img(img)\r\n    return img, label\r\n\r\ndef get_dataset(subset):\r\n    if subset == 'train':\r\n        list_ds = tf.data.Dataset.list_files(str(train_dir/'*/*'),shuffle=False)\r\n    if subset == 'valid':\r\n        list_ds = tf.data.Dataset.list_files(str(valid_dir/'*/*'),shuffle=False)\r\n    list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\r\n\r\n    print(subset)\r\n    '''\r\n    log_txt = open(subset+'.txt', 'w')\r\n    for d in ds:\r\n        log_txt.write(str(d) + '\\n')\r\n    log_txt.close()\r\n    '''\r\n    ds = list_ds.map(process_path,num_parallel_calls=cpu_core)\r\n    #ds = ds.cache()\r\n    ds = ds.shuffle(buffer_size=1000)\r\n    ds = ds.batch(batch_size)\r\n    ds = ds.prefetch(buffer_size=cpu_core).repeat()\r\n    return ds\r\n\r\ntrain_ds = get_dataset('train')\r\nvalid_ds = get_dataset('valid')\r\n\r\n#print(AUTOTUNE)\r\n#train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\r\n#valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)\r\n\r\n\r\nwith mirrored_strategy.scope():\r\n    tinydarknet = keras.Sequential([\r\n        keras.layers.Conv2D(16, (3, 3), strides=[1, 1], padding=\"same\", input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\r\n        keras.layers.Conv2D(32, (3, 3), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\r\n        keras.layers.Conv2D(16, (1, 1), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(128, (3, 3), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(16, (1, 1), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(128, (3, 3), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\r\n        keras.layers.Conv2D(32, (1, 1), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(256, (3, 3), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(32, (1, 1), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(256, (3, 3), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\r\n        keras.layers.Conv2D(64, (1, 1), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(512, (3, 3), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(64, (1, 1), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(512, (3, 3), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(128, (1, 1), strides=[1, 1], padding=\"same\"),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.Conv2D(1000, (1, 1)),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.AveragePooling2D(),\r\n        keras.layers.Flatten(),\r\n        keras.layers.Dense(1,activation=\"sigmoid\")\r\n    ])\r\n\r\n\r\n    tinydarknet.compile(optimizer=\"adam\",\r\n                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n                 metrics=[\"accuracy\"])\r\n\r\nlogs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\r\n                                                 histogram_freq = 1,\r\n                                                 profile_batch = '500,520')\r\n\r\n#save checkpoint\r\n\r\ncheckpoint_dir = 'training/'\r\ncheckpoint_path = os.path.join(checkpoint_dir,\"cp-{epoch:04d}.ckpt\")\r\ncheckpoint_path = os.path.join(checkpoint_path)\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_path,\r\n    verbose=1,\r\n    save_weights_only=True,\r\n    save_freq=1000)\r\n\r\nif from_cp:\r\n    print(\"checkpoint_path::\" + os.path.dirname(checkpoint_dir))\r\n    tinydarknet.load_weights(tf.train.latest_checkpoint(os.path.dirname(checkpoint_dir))).expect_partial()\r\n    #checkpoint = tf.train.Checkpoint(myModel=tinydarknet)\r\n    #checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n\r\nhistory = tinydarknet.fit(\r\n    train_ds,\r\n    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\r\n    epochs=epochs,\r\n    validation_data=valid_ds,\r\n    validation_steps=VAL_STEPS_PER_EPOCH,\r\n    workers=NUM_WORKERS,\r\n    callbacks = [tboard_callback,cp_callback]\r\n)\r\n\r\nacc = history.history['accuracy']\r\nval_acc = history.history['val_accuracy']\r\n\r\nloss=history.history['loss']\r\nval_loss=history.history['val_loss']\r\n\r\nepochs_range = range(epochs)\r\n\r\n\r\n\r\n#save model\r\ntinydarknet.save(l_time+\"-keras_model\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tinydarknet)\r\ntflite_model = converter.convert()\r\n\r\nwith open(l_time+\"-tinydarknet.tflite\", \"w+b\") as fp:\r\n    fp.write(tflite_model)\r\n    fp.flush()\r\n```", "@smallworld-network-wupeng\r\nI ran the code shared and face different error,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b3ee0778a1eed73ce85771b6d5280faf/untitled407.ipynb).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42953\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42953\">No</a>\n"]}, {"number": 42952, "title": "Remove redundant lines to make dockerfiles", "body": "There were identical lines in spec.yml to make dockerfiles.\r\n158 ~ 249 th lines == 473 ~ 564 th lines\r\nThus, remove such redundant lines.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42952) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42952) for more info**.\n\n<!-- ok -->", "Thanks for check this pr @angerson !\r\nHowever, some ci build failed for some other reason I guess. Could you check or re-test such CI case?"]}, {"number": 42951, "title": "CMSIS-NN: Incorrect flags for non-DSP/MVE processors (convolution op only)", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): 5a16264ba6f12883726d12d484d4cd61405ddab7\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm internal testing using models for Cortex-M3\r\n\r\n**Describe the problem**\r\nCalculateOpData() in cmsis-nn/conv.cc is inside a '#if defined(__ARM_FEATURE_DSP) || defined(__ARM_FEATURE_MVE)' flag which is not set for a Cortex-M3 or similar that do not have DSP or MVE extensions. This results in uninitialized quantization parameters being passed on to the convolution API in the Eval() procedure. This could result in an assert in debug builds or atleast an incorrect output from convolution.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem** \r\nJust a visual code check.\r\n\r\n", "comments": ["We'll have a look at this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42951\">No</a>\n"]}, {"number": 42950, "title": "Regarding the operation of arbitrary scale access to the network", "body": "\r\nWhen using vgg for migration learning, the specific layer of the network will be used here. I hope that the network can accept pictures of any size as input. I found some information on the Internet. One way is to use lambda as the input layer to replace the original vgg The input layer, but I don\u2019t know how to operate this, is there a better way.", "comments": ["@wangwenchao-job,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42949, "title": "[Intel MKL] unit test for bfloat16 on matmul.", "body": "[Intel MKL] unit test for bfloat16 on matmul.\r\nUnit test for bfloat16 for matmul is created. This is the first PR for bfloat16 unit test creation. After getting reviews, rest of the mkl backed op's unit tests will be created following similar methods. \r\n\r\nIn this unit tests, to evaluate the benchmark value we convert the twice. First float32 random values are generated. Later this values are converted to bfloat16 (which is used in the graph) and these bfloat16 values are converted back to float32 again for using the numpy library. (numpy and default gradient function doesn't have bfloat16 operations). \r\nIf we don't convert twice and use the original random float32 values to get the benchmark results, we need to have very loose tolerance. \r\n", "comments": ["@noim210  Can you please check @penpornk's comments and keep us posted ? Thanks!", "@noim210 Can you please resolve conflicts? Thanks!", "@noim210 Can you please resolve conflicts? Thanks!"]}, {"number": 42948, "title": "Using custom model with fewer that 1000 classes", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nAndoid studio 4.01 emulator will crash with error below if model has fewer classes than 1000\r\n\r\n\r\n**Describe the problem**\r\nI am trying to use a custom trained model that has fewer than 1000 classes - is there a way to change the number of classes? I see all the pretrained ones have 1000 classes\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n java.lang.IllegalArgumentException: Label number 1005 mismatch the shape on axis 1 is the error I get because the output tensor of my model only includes 4 classes. Is this configurable? If not it would be a great feature to add.\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dsanerph \r\nPlease provide with simple stand alone code such that we can replicate the issue faced, or if possible share a colab with error reported.", "You may use transfer learning approach to retrain the model as per your given classes.\r\nTypically you will be using some pre-trained model architecture and set `include_top` as false.\r\nThis will not load the classification layer of the pre trained model.\r\nFurther allows you to add an output layer and specify units (as per your class requirements).\r\nFor more information see https://www.tensorflow.org/tutorials/images/transfer_learning", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42947, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Surface Pro\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2\r\n- Python version:3.7.9\r\n- Installed using virtualenv? pip? conda?:PIP\r\n\r\nCan't import\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rache\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\rache\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\rache\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\rache\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\rache\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\rache\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\rache\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@HiImAri \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #42751 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Running on 64 bits and im running Intel(R) Atom(TM) x7-Z8700 CPU@160GHz and i already downloaded the Microsoft visual c++ while following the instructions through Tensorflow website", "@HiImAri \r\n\r\nLooking at specification of your cpu seems like avx is not supported.Please, refer the [link here](https://ark.intel.com/content/www/us/en/ark/products/85475/intel-atom-x7-z8700-processor-2m-cache-up-to-2-40-ghz.html).\r\nIn this case you can build Tensorflow from source or use Google colab as an alternative. Thanks!", "I faced the same issue with tensorflow what i did was :\r\n\r\n`Install anaconda  \r\n\r\nopen anaconda prompt\r\n\r\nconda create -n tf20 python=3.6\r\n\r\nactivate tf20\r\n\r\nconda install tensorflow or pip install tensorflow==2.0.0\r\n\r\nNow check by importing tensorflow\r\nthe version i used was 3.6, but you can also try with 3.7 Hope it helps!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42947\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42947\">No</a>\n"]}, {"number": 42945, "title": "Is this framework suitable for IPv6 network environment?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux centOS7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nIs this framework suitable for IPv6 network environment?\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is not a bug in the code. Usage questions should be asked on StackOverflow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42945\">No</a>\n"]}]