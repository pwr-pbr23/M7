[{"number": 17728, "title": "pb-\u300btflite is ok\uff1f", "body": "bazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n --input_file=/home/ubuntu/Desktop/code/tensorflow/graph_opt.pb \\\r\n --output_file=/home/ubuntu/Desktop/code/tensorflow/graph_opt.pb.tflite \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n --output_format=TFLITE \\\r\n--inference_type=FLOAT \\\r\n--input_shape=1,128,128,3 \\\r\n --input_array=image \\\r\n--output_arrays=Openpose/concat_stage7\r\nWARNING: Config values are not defined in any .rc file: opt\r\nWARNING: /home/ubuntu/.cache/bazel/_bazel_ubuntu/bb7db75967eaf264207472309623127f/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/ubuntu/.cache/bazel/_bazel_ubuntu/bb7db75967eaf264207472309623127f/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/toco/toco\r\nINFO: Elapsed time: 0.643s, Critical Path: 0.01s\r\nINFO: Build completed successfully, 1 total action\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/ubuntu/Desktop/code/tensorflow/graph_opt.pb' '--output_file=/home/ubuntu/Desktop/code/tensorflow/graph_opt.pb.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=FLOAT' '--input_shape=1,128,128,3' '--input_array=image' '--output_arrays=Openpose/concat_stage7'\r\n2018-03-14 17:06:23.573744: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 426 operators, 649 arrays (0 quantized)\r\n2018-03-14 17:06:23.590727: I INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/ubuntu/Desktop/code/tensorflow/graph_opt.pb' '--output_file=/home/ubuntu/Desktop/code/tensorflow/graph_opt.pb.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=FLOAT' '--input_shape=1,128,128,3' '--input_array=image' '--output_arrays=Openpose/concat_stage7'\r\n2018-03-14 17:06:23.573744: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 426 operators, 649 arrays (0 quantized)\r\n2018-03-14 17:06:23.590727: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 426 operators, 649 arrays (0 quantized)\r\n2018-03-14 17:06:23.608262: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 151 operators, 438 arrays (0 quantized)\r\n2018-03-14 17:06:23.614578: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 151 operators, 438 arrays (0 quantized)\r\n2018-03-14 17:06:23.620831: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 3400704 bytes, theoretical optimal value: 1886208 bytes.\r\n2018-03-14 17:06:23.621951: I tensorflow/contrib/lite/toco/toco_tooling.cc:309] Estimated count of arithmetic ops: 1.03065 billion (note that a multiply-add is counted as 2 ops).\r\n\r\n\r\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\r\n\r\n\r\nbut i run the  tflite get the error\r\n\r\n\r\n03-15 10:26:22.109 14024-14074/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n                                                                                      Process: android.example.com.tflitecamerademo, PID: 14024\r\n                                                                                      java.lang.IllegalArgumentException: Failed to get input dimensions. 0-th input should have 196608 bytes, but found 406272 bytes.\r\n                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)\r\n                                                                                          at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:82)\r\n                                                                                          at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)\r\n                                                                                          at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)\r\n                                                                                          at com.example.android.tflitecamerademo.ImageClassifierQuantizedMobileNet.runInference(ImageClassifierQuantizedMobileNet.java:95)\r\n                                                                                          at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:110)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Camera2BasicFragment.java)\r\n                                                                                          at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:559)\r\n                                                                                          at android.os.Handler.handleCallback(Handler.java:815)\r\n                                                                                          at android.os.Handler.dispatchMessage(Handler.java:104)\r\n                                                                                          at android.os.Looper.loop(Looper.java:207)\r\n                                                                                          at android.os.HandlerThread.run(HandlerThread.java:61)\r\n03-15 10:26:22.119 1125-1823/? E/ActivityManager: Invalid thumbnail dimensions: 0x0\r\n\r\n", "comments": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n  http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n\r\npackage com.example.android.tflitecamerademo;\r\n\r\nimport android.app.Activity;\r\nimport android.util.Log;\r\n\r\nimport java.io.IOException;\r\n\r\n/**\r\n * This classifier works with the Inception-v3 slim model.\r\n * It applies floating point inference rather than using a quantized model.\r\n */\r\npublic class ImageClassifierFloatInception extends ImageClassifier {\r\n\r\n  /**\r\n   * The inception net requires additional normalization of the used input.\r\n   */\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128.0f;\r\n\r\n  /**\r\n   * An array to hold inference results, to be feed into Tensorflow Lite as outputs.\r\n   * This isn't part of the super class, because we need a primitive array here.\r\n   */\r\n  private float[][] labelProbArray = null;\r\n  private float[] outputs = null;\r\n  private float[][][][] outputs_zyx = null;\r\n  /**\r\n   * Initializes an {@code ImageClassifier}.\r\n   *\r\n   * @param activity\r\n   */\r\n  ImageClassifierFloatInception(Activity activity) throws IOException {\r\n    super(activity);\r\n    labelProbArray = new float[1][getNumLabels()];\r\n    outputs = new float[28*28*57];\r\n    outputs_zyx = new float[1][28][28][57];\r\n  }\r\n\r\n  @Override\r\n  protected String getModelPath() {\r\n    // you can download this file from\r\n    // https://storage.googleapis.com/download.tensorflow.org/models/tflite/inception_v3_slim_2016_android_2017_11_10.zip\r\n    return \"graph_opt.pb.224.tflite\";\r\n  }\r\n\r\n  @Override\r\n  protected String getLabelPath() {\r\n    return \"labels_imagenet_slim.txt\";\r\n  }\r\n\r\n  @Override\r\n  protected int getImageSizeX() {\r\n    return 224;\r\n  }\r\n\r\n  @Override\r\n  protected int getImageSizeY() {\r\n    return 224;\r\n  }\r\n\r\n  @Override\r\n  protected int getNumBytesPerChannel() {\r\n    // a 32bit float value requires 4 bytes\r\n    return 4;\r\n  }\r\n\r\n  @Override\r\n  protected void addPixelValue(int pixelValue) {\r\n    imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n    imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n    imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n  }\r\n\r\n  @Override\r\n  protected float getProbability(int labelIndex) {\r\n    return labelProbArray[0][labelIndex];\r\n  }\r\n\r\n  @Override\r\n  protected void setProbability(int labelIndex, Number value) {\r\n    labelProbArray[0][labelIndex] = value.floatValue();\r\n  }\r\n\r\n  @Override\r\n  protected float getNormalizedProbability(int labelIndex) {\r\n    // TODO the following value isn't in [0,1] yet, but may be greater. Why?\r\n    return getProbability(labelIndex);\r\n  }\r\n\r\n  @Override\r\n  protected void runInference() {\r\n\r\n\r\n    long oldTime = System.currentTimeMillis();\r\n    tflite.run(imgData, outputs_zyx);\r\n\r\n    Log.e(\"aaa\", \"aaa\"+outputs_zyx[0].length+\"   time:\"+(System.currentTimeMillis() - oldTime));\r\n    Log.e(\"aaa\", \"aaa\"+outputs_zyx[0].length+\"   time:\"+(System.currentTimeMillis() - oldTime));\r\n//    for (int i = 0; i < outputs_zyx.length; ++i) {\r\n//      Log.e(\"\u6210\u529f1\", \"\u6210\u529f1\");\r\n//        for (int j = 0; j < outputs_zyx[i];++j){\r\n//          Log.e(\"\u6210\u529f2\", \"\u6210\u529f2\");\r\n//          for (int a = 0;a < outputs_zyx[i][j];++a){\r\n//\r\n//            Log.e(\"\u6210\u529f3\", \"\u6210\u529f3\");\r\n//\r\n//            for (int e = 0;e < outputs_zyx[i][j][e];++e)\r\n//          }\r\n//        }\r\n//\r\n//    }\r\n\r\n  }\r\n}\r\n"]}, {"number": 17727, "title": "Updated Session.run() documentation", "body": "For `session.run(ops)`, order in which ops are evaluated is undefined, but that information isn't specified anywhere in documentation. It's quite natural to assume that ops are evaluated in order provided, especially since that's what cpu version of Tensorflow does (at least on every implementation I tried). GPU versions don't always follow FIFO evaluation order though. If needed I can provide a test that shows different behavior with `tensorflow` and `tensorflow-gpu` purely due to different order of evaluation inside `session.run(ops)`.\r\n\r\nIn this pull request I added a single sentence to documentation to make it clear that order of ops evaluation inside `session.run(...)` is undefined.\r\n\r\nDiscussion of the issue can be found here: \r\nhttps://github.com/tensorflow/tensorflow/issues/13133#issuecomment-373198596", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Since googlebot asked I confirm I'm ok with my commits being contributed to the project - yes, I am. Is there some sort of checkbox or link I need to click to trigger the ok response?", "Hi @PuchatekwSzortach, can you recreate the PR with the commits squashed? The CLA block means I can't merge it in.", "@frankchn I submitted a new PR https://github.com/tensorflow/tensorflow/pull/17773"]}, {"number": 17726, "title": "contrib/gan: minor spelling tweaks", "body": "", "comments": []}, {"number": 17725, "title": "Patch release notes", "body": "", "comments": []}, {"number": 17724, "title": "Update ios.md", "body": "brew install commonly fails on modern versions of OSX, where the symlink needs to be manually added. I added instructions to chown /usr/local/*, which will allow for this.", "comments": []}, {"number": 17723, "title": "Pin the version of cuDNN used in Dockerfile.gpu", "body": "Related: #17566\r\nFixes: #17431\r\n\r\nSigned-off-by: Felix Abecassis <fabecassis@nvidia.com>\r\n", "comments": ["@gunan "]}, {"number": 17722, "title": "Update README.md", "body": "Fixed location of download_models.sh in the iOS Demo App section", "comments": []}, {"number": 17721, "title": "Branch 189071037", "body": "", "comments": []}, {"number": 17720, "title": "map_and_batch tensor shape does not match value of tensor in the same way that calling map and batch individually does", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nHappens with stock code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu Server 17.10.1\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nb'v1.6.0-0-gd2e24b6039' 1.6.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010\r\n- **CUDA/cuDNN version**:\r\n9.1/7.0.5\r\n- **GPU model and memory**:\r\nnot applicable\r\n- **Exact command to reproduce**:\r\nnot applicable\r\n\r\nWhen I create a tf.data.Dataset from tfrecord files that utilizes a call to `map` to parse the tfrecord file and a call to `batch` to batch the dataset I am able to filter out the last small batch utilizing a straight forward call to `filter`. This same function does not work correctly when utilizing the combined `map_and_batch` function. The filter function in question is:\r\n\r\n```\r\ndataset = dataset.filter(\r\n    lambda *args: tf.equal(tf.shape(args[0])[0], batch_size)\r\n)\r\n``` \r\n\r\nThe reason this does not work is because every tensor passed through `tf.shape` when utilizing `map_and_batch` has the same shape even though the contents of the tensor does not. This is not the case when executing `map` and `batch` separately, the last batch has a shape returned from `tf.shape` that correctly matches the shape of the value.\r\n\r\nMy real world example has 7153 batches in 1 epoch, using `map` and `batch` separately I am returned 7152 batches that have a shape returned from `tf.shape` of 128 and the final 1 batch returned as 70, in this case the filter function works correctly. When I swap out this configuration with `map_and_batch` I am returned 7153 batches with shape returned from `tf.shape` of 128, in this case my filter function does not work and my estimator throws an exception because it receives a batch of 70 (when it was expecting a batch of 128).\r\n\r\nI would very much like to use `map_and_batch` because it takes 1/3 the time of `map` and `batch` separately.\r\n\r\nHere is an example script:\r\n\r\n```python\r\n# example.py\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\n\r\nflags.DEFINE_boolean('use_broken_map_and_batch', False,\r\n                     'Directory to write the model and ')\r\nflags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')\r\n\r\nFLAGS = flags.FLAGS\r\n\r\ndef parse_fn(example):\r\n    example_fmt = {\r\n        'src': tf.FixedLenFeature([], tf.int64),\r\n        'dst': tf.FixedLenFeature([], tf.int64),\r\n    }\r\n\r\n    features = tf.parse_single_example(\r\n        example,\r\n        features=example_fmt\r\n    )\r\n\r\n    return tuple(features[k] for k in example_fmt)\r\n\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    files = tf.data.Dataset.list_files(FLAGS.train_data)\r\n    dataset = files.interleave(tf.data.TFRecordDataset, cycle_length=4)\r\n\r\n    if FLAGS.use_broken_map_and_batch:\r\n        dataset = dataset.apply(\r\n            tf.contrib.data.map_and_batch(\r\n                map_func=parse_fn,\r\n                batch_size=128,\r\n                num_parallel_batches=28\r\n            )\r\n        )\r\n    else:\r\n        dataset = dataset.map(parse_fn, num_parallel_calls=28)\r\n        dataset = dataset.batch(128)\r\n\r\n    dataset = dataset.filter(\r\n        lambda *args: tf.equal(tf.shape(args[0])[0], 128)\r\n    )\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    src, dst = iterator.get_next()\r\n\r\n    train_op = src * dst\r\n\r\n    init_op = tf.group(tf.global_variables_initializer(),\r\n                       tf.local_variables_initializer())\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    sess.run(init_op)\r\n\r\n    shapes = []\r\n\r\n    try:\r\n        while True:\r\n            val = sess.run(train_op)\r\n            shapes.append(val.shape[0])\r\n    except tf.errors.OutOfRangeError:\r\n        print(shapes[-10:])\r\n```\r\n\r\nWhen executed with the following parameters you should see the output:\r\n\r\n```\r\n$ python example.py --train_data data.tfrecord\r\n\r\n[128, 128, 128, 128, 128, 128, 128]\r\n\r\n$ python example.py --train_data data.tfrecord \u200a--use_broken_map_and_batch\r\n\r\n[128, 128, 128, 128, 128, 128, 128, 104]\r\n```\r\n\r\nTo be clear the execution with the \u200a`\u200a--use_broken_map_and_batch` shows a 104 at the end, this is because that batch was not filtered out, you can recreate this by using the following code to generate a tfrecord file:\r\n\r\n```\r\n# data_generation.py\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\n\r\nflags.DEFINE_string('train_data', 'data.tfrecord', 'Training tfrecord file. ')\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nvals = np.random.randint(1, 1000, (1000, 2))\r\n\r\nwith tf.python_io.TFRecordWriter(FLAGS.train_data) as writer:\r\n    for src, dst in vals:\r\n        example = tf.train.Example(\r\n            features=tf.train.Features(\r\n                feature={\r\n                    'src': tf.train.Feature(\r\n                        int64_list=tf.train.Int64List(value=[src])),\r\n                    'dst': tf.train.Feature(\r\n                        int64_list=tf.train.Int64List(value=[dst])),\r\n                }\r\n            )\r\n        )\r\n\r\n        serialized = example.SerializeToString()\r\n        writer.write(serialized)\r\n```\r\n\r\nAnd you can execute in this way:\r\n\r\n```\r\n$ python data_generation.py --train_data data.tfrecord\r\n```\r\n\r\nAny help on this issue is greatly appreciated.", "comments": ["It looks like this has been broken since 8693bf519399495cedd91293ec82b492ea401f6f, which did not update the Python shape inference logic when adding the ability for `map_and_batch()` to return a smaller final batch. Sorry for the inconvenience, and we'll have a fix for this soon. In the meantime, you can fix by using `tf.placeholder_with_default()` to inhibit the shape inference:\r\n\r\n         dataset = dataset.apply(\r\n            tf.contrib.data.map_and_batch(\r\n                map_func=parse_fn,\r\n                batch_size=tf.placeholder_with_default(tf.constant(128, dtype=tf.int64)),\r\n                num_parallel_batches=28\r\n            )\r\n         )\r\n\r\n\r\n/cc @jsimsa ", "To clarify, it should be:\r\n\r\n```python\r\ndataset = dataset.apply(\r\n    tf.contrib.data.map_and_batch(\r\n        map_func=parse_fn,\r\n        batch_size=tf.placeholder_with_default(tf.constant(128, dtype=tf.int64), ()),\r\n        num_parallel_batches=28\r\n     )\r\n)\r\n```\r\n\r\n`tf.placeholder_with_default` needs shape passed to it, is it correct to pass that in as `()` in this situation? I don't get a problem, and it fixes the issue, thank you!", "Right, that should work. "]}, {"number": 17719, "title": "Elaborate on documentation for using tf.layers.batch_normalization", "body": "This PR adds details about a gotcha not previously documented related to `batch_normalization`. See https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/Y0BSYDRQ_BU", "comments": []}, {"number": 17718, "title": "Patching r1.2", "body": "", "comments": ["\"The function 'set' has been removed in favor of 'depset', please use the latter.\". bazel deprecated set in 0.7.0. We will need to downgrade bazel on our build machine for this."]}, {"number": 17717, "title": "Add working example of distributed tensorflow using Estimator API for K-Means", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Redhat\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N.A.\r\n- **GCC/Compiler version (if compiling from source)**: N.A.\r\n- **CUDA/cuDNN version**: N.A.\r\n- **GPU model and memory**: N.A.\r\n- **Exact command to reproduce**: N.A.\r\n\r\n### Describe the problem\r\nI am using standard library-provided k-means with Estimator API for distributed tensorflow. I have a cluster of three machine and I have updated the TF_CONFIG env. variable on all three machine. I am using HDFS to store the model directory so that all machine can access that it, but when I execute the python file from the master machine, the gRPC server gets created and then \"PS server and Worker server waiting for response from master\"  message is repeated after every 10 seconds or so.  \r\n\r\nIf sample code for using this Estimator-API based K-Means would had been present, it would have helped better\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pandas as pd\r\nk = 5\r\nn = 100\r\nvariables = 2\r\npoints = np.random.uniform(0, 1000, [n, variables])\r\ninput_fn=lambda: tf.train.limit_epochs(tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)\r\nkmeans=tf.contrib.factorization.KMeansClustering(num_clusters=k, use_mini_batch=False,model_dir=\"my_hdfs_path\")\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn, max_steps=100)\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\r\n\r\ntf.estimator.train_and_evaluate(kmeans, train_spec, eval_spec)\r\nlist(kmeans.predict(input_fn=input_fn))\r\n\r\n### TF_CONFIG:\r\n{ \"cluster\":{ \"chief\":[ \"master:22220\" ], \"ps\":[ \"slave02:22220\" ], \"worker\":[ \"slave01:22220\" ] }, \"task\":{  \"type\":\"chief\", \"index\":0, } }\r\n\r\nError message:\r\nCreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\nCreateSession still waiting for response from worker: /job:worker/replica:0/task:0", "comments": ["Interestingly enough I just created a [question](https://stackoverflow.com/questions/49296917/matrix-input-for-prediction-in-tensorflow) in SO about the serving input function requesting some example on how to actually do that.\r\n\r\nI would be happy to contribute with an example once this question is resolved, although your question pretty much is about input when using the standard Estimator API :)\r\n", "@martinwicke  what do you think?\r\n", "@cy89 @ccolby please have a look at this issue", "Nagging Assignee @bignamehyp: It has been 157 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17716, "title": "Patch 1", "body": "", "comments": ["Could you resolve the conflicts?", "@slevental could you make a better PR description (and resolve conflicts?). \r\n\r\nThanks!", "Sorry for confusion, those changes were already merged."]}, {"number": 17715, "title": "GPU issue", "body": "Hi. I am new to TensorFlow, and I installed everything as instructed. However, I get the error below after successfully importing tf. Does anyone know how to fix this? I am using a Surface Book 2 with a 1060 NVIDIA GPU.\r\n\r\nThanks\r\n\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n2018-03-14 12:19:36.435333: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-03-14 12:19:37.605256: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1212] Found device 0 with properties:\r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.569\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.97GiB\r\n2018-03-14 12:19:37.611418: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-03-14 12:19:39.295420: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4744 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:02:00.0, compute capability: 6.1)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "these look like standard informational messages to me. None of them are error messages.", "oh okay. will these kind of messages always show up? is there anyway i can hide them?", "They will always show up when you use GPU the first time.\r\nThere are ways to hide them, but those will also hide any real error messages that show up.\r\n\r\nWhat you need to look out for is the first character after the timestamp:\r\n2018-03-14 12:19:39.295420: **I** C:\\tf_jenkins\\workspace\\...\r\nThis \"I\" means this is an INFO log. for warnings, you will see \"W\", For non fatal errors you will see \"E\", and for unrecoverable issues you will see \"F\"."]}, {"number": 17714, "title": "Branch 189041421", "body": "", "comments": []}, {"number": 17713, "title": "Manual Placement of Graph Nodes of High-level Optimizers and Loss functions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.1/7.0\r\n- **GPU model and memory**: Tesla k80 (11441MiB)\r\n- **Exact command to reproduce**: python cifar10_train.py\r\n\r\n### Describe the problem\r\nI want to use my own device placement algorithm to be used to place nodes across multiple devices my machine has. I looked into this : https://www.tensorflow.org/programmers_guide/using_gpu#manual_device_placement which suggests that if I figure out exact placement statically before executing the graph, I can use \"with device\" annotations to partition my graph across devices. But, if I am using some high-level optimizers and loss functions. Each of these operations are going to have a multi-node graph as well. Suppose I am running this example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10 and want to specify where each node of the complete tensor flow graph should go given a list of compute resources (cpu cores, gpu, tpu etc.) my machine has.  \r\n\r\nIs there any api or framework that allows you to do fine grained placement of graph nodes across multiple compute resources ? What's the best way to achieve this ?  \r\n\r\nThanks a lot for any help!\r\n\r\n\r\n### Source code / logs\r\ncifar10 example : https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10", "comments": ["@benoitsteiner is there documentation on whether Grappler could be used to plug in a third-party placement policy?", "Hi, I think stackoverflow could be a better place to ask the question. \r\n\r\nAs far as I known, you can use device_function with `tf.device` to manage all ops placement by yourself, see the example below:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6f0dd0425c51360fe2be5a938a8f3fb39e420fa3/tensorflow/python/framework/graph_util_test.py#L41-L45\r\n\r\nHowever, in most case, `allow_soft_placement` is good enough, see [Using a single GPU on a multi-GPU system](https://www.tensorflow.org/programmers_guide/using_gpu#using_a_single_gpu_on_a_multi-gpu_system)", "Hi @facaiy ,\r\nI want to run some heuristic after creating a complete graph and want to do fine grained placement of nodes of the graph while using high-level api's like loss functions, optimizers, functions from NN library etc. But using \"with device annotations\", I think I can only do placement on a very high-level. Please correct me if I am wrong. Thanks! ", "You can pass a function to tf.device, and use the function to guide where each op is placed. see that `test_divice_function_ping_variable_to_cpu` in above example always assign all variable ops to cpu. \r\n\r\nwith tf.device(your_placement_policy_function): \r\n bulid your whole network\r\n\r\n", "Thanks, I will look into that. ", "Hi @facaiy , what you suggested works fine in the case when you already know nodes -> device mapping while creating the graph. But, for my case, I first want to construct the whole graph and then run some heuristic based algorithm on the complete graph and place them intelligently based on the nature of the complete graph (on runtime, dynamically instead of placing the whole graph across gpus statically before execution). Do you know if this feature/flexibility already exists in Tensorflow ? ", "@xilenteyex Sorry, I have no idea. \r\ncc @martinwicke Excuse me, do you know anyone who can answer the issue?", "@xilenteyex: I think this is a basic example of what you're trying to do:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import meta_graph\r\nfrom tensorflow.python.framework import ops as tf_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import nn_ops\r\nfrom tensorflow.python.ops import random_ops\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\ndef _buildMnist(batch_size=128,\r\n\t\tinput_size=256,\r\n\t\tnum_classes=1024,\r\n\t\tnum_layers=10,\r\n\t\thidden_size=256,\r\n\t\tname='mnist'):\r\n\tg = tf_ops.get_default_graph()\r\n\twith g.as_default():\r\n\t  ops = {}\r\n\t  x = random_ops.random_uniform(\r\n\t\t  [batch_size, input_size], -0.1, 0.1, dtype=dtypes.float32)\r\n\t  for layer_id in range(num_layers):\r\n\t\twith variable_scope.variable_scope('layer_{}'.format(layer_id)):\r\n\t\t  a = input_size if layer_id == 0 else hidden_size\r\n\t\t  b = hidden_size if layer_id < num_layers - 1 else num_classes\r\n\t\t  w = variable_scope.get_variable('w', [a, b])\r\n\t\t  x = math_ops.matmul(x, w)\r\n\t\t  x = nn_ops.relu(x)\r\n\t  ops['y_preds'] = math_ops.argmax(x, axis=1)\r\n\r\n\ttrain_op = g.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP)\r\n\ttrain_op.append(ops['y_preds'])\r\n\treturn g\r\n\r\n# build your graph\r\ngraph = _buildMnist()\r\n# extract metagraph\r\nmg = meta_graph.create_meta_graph_def(graph=graph)\r\n\r\n# Assign your devices to nodes in the loop below\r\n# As a simple example, assigning all ops to CPU.\r\n# If you were to remove this, you would see some nodes assigned to GPUs by default\r\nfor node in metagraph.graph_def.node:\r\n\tnode.device = '/device:CPU:0'\r\n\t\r\n# define a new graph\r\nnew_graph = tf.Graph()\r\n# log_device_placement so we can see the final placement\r\nconfig = tf.ConfigProto(log_device_placement)\r\nwith tf.Session(graph=new_graph,\r\n                config=config) as session:\r\n\t# load your modified metagraph\r\n\tsaver = tf.train.import_meta_graph(mg)\r\n\t# initialize and run your train_op\r\n\tsession.run(tf.global_variables_initializer())\r\n\tsession.run(new_graph.get_collection_ref(tf_ops.GraphKeys.TRAIN_OP))\r\n```", "@Benyuel thanks for your input. Your approach looks similar to what @facaiy suggested. I am more interested in an API/feature available in TensorFlow which allows you to do dynamic placement of nodes while executing the graph instead of statically assigning nodes to devices before graph execution.  For example, If I have 10 nodes in the graph, after executing first three nodes based on the static placement, is it possible to change the placement of next few nodes or all nodes that are going to be executed ?", "You might be able to adapt something from how eager execution works: https://www.tensorflow.org/programmers_guide/eager", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17712, "title": "update doc for tf.train.Saver() max_to_keep argument to resolve #17554", "body": "Currently, if max_to_keep is set to 0, all checkpoints are kept but only the last one is kept in the 'checkpoint' file. This was not previously clear in the docs. See #17554 ", "comments": []}, {"number": 17711, "title": "Fix broken link of TensorFlow Debugger (tfdbg) in layers tutorials", "body": "Described as PR tile.\r\nJust follow the similar format of linking to debugger.md to fix current broken link issue in layer.md tutorial: https://github.com/tensorflow/tensorflow/blob/173b4067eb8a724b5608f709c83b5a80df87dcea/tensorflow/docs_src/programmers_guide/index.md \r\n> @{$programmers_guide/debugger}, which explains how to use the TensorFlow debugger (tfdbg).", "comments": []}, {"number": 17710, "title": "gofmt with tensorflow/go/genop/internal/genop.go", "body": "By default golang use `gofmt -s` to format the code though\r\nit looks like tensorflow/go/genop/internal/genop.go is not\r\nformattted with gofmt.\r\n\r\nhttps://goreportcard.com/report/github.com/tensorflow/tensorflow\r\n\r\nThis fix updates with gofmt.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17709, "title": "Error: C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed", "body": "I am trying to build tensorflow inference library using the instructions [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android).\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:  Master (10be1f2377bf7aad1a4cfa306277c53e44493a57)\r\n- **Python version**: Python 3.6.3 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Tesla K80 12GB\r\n- **Exact command to reproduce**: ```bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a --verbose_failures```\r\n\r\n### Describe the problem\r\nBazel build is failing with the error `C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed`.\r\nThe full error is pasted below. I have tried `bazel clean`.\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/protobuf_archive/BUILD:66:1: C++ compilation of rule '@protobuf_archive//:protobuf_lite' failed (Exit 1): false failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n  /bin/false -MD -MF bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/_objs/protobuf_lite/external/protobuf_archive/src/google/protobuf/arena.pic.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/_objs/protobuf_lite/external/protobuf_archive/src/google/protobuf/arena.pic.o' -fPIC -iquote external/protobuf_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-opt/genfiles/external/protobuf_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-writable-strings -c external/protobuf_archive/src/google/protobuf/arena.cc -o bazel-out/armeabi-v7a-opt/bin/external/protobuf_archive/_objs/protobuf_lite/external/protobuf_archive/src/google/protobuf/arena.pic.o)\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 3.864s, Critical Path: 0.01s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution", "It turns out that the issue is being caused by ndk path not being set in the WORKSPACE file (see [here](https://github.com/tensorflow/tensorflow/blob/master/WORKSPACE)). Strangely Bazel doesn't give any hint of what's going wrong!", "I'm getting the same error, how can I fix this? I am on master hash `b4ae85234b4f626e4aaee1d2c531a6b534712dbb`", "@farhanhubble Where exactly should we add android sdk and ndk repository information in the workspace file ?", "Same Error!\r\n\r\nPlatform: CentOS\r\nGCC: 7.3.1\r\nbazel: 3.3.1\r\ntensorflow: master branch\r\n\r\n\r\nWARNING: /home/projects/2-framework/tensorflow/tensorflow/core/BUILD:1750:11: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /home/projects/2-framework/tensorflow/tensorflow/core/BUILD:2162:16: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation\r\nINFO: Analyzed 5 targets (0 packages loaded, 19455 targets configured).\r\nINFO: Found 5 targets...\r\nERROR: /root/.cache/bazel/_bazel_root/e46112f22ffa5fec40c0ac30c24ecbbd/external/com_google_protobuf/BUILD:301:11: C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-std=c++14'\r\nERROR: /home/projects/2-framework/tensorflow/tensorflow/core/protobuf/BUILD:103:20 C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1)\r\nINFO: Elapsed time: 3.168s, Critical Path: 0.49s\r\nINFO: 20 processes: 20 local.\r\nFAILED: Build did NOT complete successfully\r\n"]}, {"number": 17708, "title": "Suggestion:Tensor add the finalize method to close tensor object after GC", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): Master\r\nPython version: 3.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce: Run the snippet below.\r\n\r\nDescribe the problem\r\nTensor object should initiative call close method to release native memory, but the document don't have any reference to mention this important function.\r\nTo avoid some guy to forget call this method, suggest invoke close method in the finalize method that let GC try compensate it at last.\r\n`Public void finalize(){\r\nclose()\r\n}`\r\n\r\n  ", "comments": ["I apologize but I am having a hard time understanding what the problem is. Could you please clarify what feature you are suggesting?", "Tensor object should be initiative invoked the closed method if you don't want use the tensor. You can n't expect the GC to collect this object otherwise will occur the memory leak.\r\nBut I  don't see any reference to mention this important behave in the document, it is very dangerous. In my product environment, the developer forget trigger the close function, the product meet OOM and crash last.\r\nTo compensate this, I suggest in the finalize method (tensor.java) to close the tensor object,  to avoid the developer forget closing the tensor object. At last, GC will trigger the finalize the method to close the tensor object.\r\n\r\n\r\n\r\n\r\n\r\n    ", "@asimshankar Do you agree with this proposal?", "I'm tempted to avoid a finalizer. See [\"Effective Java\", Item 7](http://www.informit.com/articles/article.aspx?p=1216151&seqNum=7) for various reasons, but basically the performance and memory management costs may not be worth it, as the finalizer may make native memory usage unpredictable.\r\n\r\nThe `Tensor` class [implements the `AutoCloseable` interface](https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L45), which is one indication (often picked up by IDEs) that `close()` should be invoked. We [mention this in the javadoc as well](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L35).\r\n\r\nSo, I'd not like to introduce a finalizer. However, I'm very open to suggestions on how to make this easier to discover/harder to mess up.\r\n\r\nThanks!"]}, {"number": 17706, "title": "Changed sparse_column_with_vocabulary_file to estimate vocab_size", "body": "Prior to this change, the function required vocab_size to be explicitly specified by the user which made the API inconsistent with categorical_column_with_vocabulary_file.", "comments": ["Hi @superbobry, we'll deprecate contrib.layers.feature_columns. Is there any reason you prefer sparse_column over core equivalent of categorical_column?", "Hey @ispirmustafa, the only reason is that `one_hot_column` requires a sparse column as input.", "Hi @superbobry,\r\nyou can use tf.feature_column.indicator_column instead of one_hot one. they are same.", "Awesome, thank you very much. Closing the PR."]}, {"number": 17705, "title": "Couldn't open CUDA library cublas64_80.dll", "body": "tensorflow just run hello world\r\nit is work, but alarms appear.\r\nis this ok ? it is run by gpu or cpu\r\nos: win10\r\nGPU Toolkit version : v9.0\r\ncudnn verson: v7.1\r\nanaconda3 version: v5.1\r\n\r\nimage easy to view\r\nhttps://drive.google.com/open?id=1ko9Le7e_2FTJRaAGohRh4I8w43c565yf\r\n\r\ntext for more detail\r\n\r\n> (tensorflow) C:\\Users\\yi\\Anaconda3\\Scripts>python\r\nPython 3.5.5 |Anaconda, Inc.| (default, Mar  9 2018, 12:39:44) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3517] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:126] Couldn't open CUDA library curand64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_rng.cc:338] Unable to load cuRAND DSO.\r\n>>> import tensorflow as tf\r\n>>> hello=tf.constant('hello, TensorFlow!')\r\n>>> sess=tf.Session()\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 1050\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.62GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\r\n>>> print(sess.run(hello))\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'hello, TensorFlow!'", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "You say you have SDK 9.0 installed, but your Tensorflow code is looking for SDK 8.0 dlls, which indicates Tensorflow ~v1.3. Either install SDK 8.0 or upgrade to a newer version of TF.", "Thank you @robosmith.  @lolmuta please re-open if this suggestion does not work for you. ", "@robosmith thank for you respond, the problem is tensorflow version.\r\nCause i using anaconda,the key is you need to know which python using now , and which python need update tensorflow version."]}, {"number": 17704, "title": "atrous_conv2d doc fix newline", "body": "fix newline in docs for `tf.nn.atrous_conv2d`", "comments": []}, {"number": 17703, "title": "Fix masking order", "body": "This fixes the same issue as #14831 but for the BatchNormalization layer and Bidirectional wrapper.", "comments": ["Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @fchollet: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Thanks for the PR, and sorry for the delayed review. Could you please rebase, since the layers have changed recently? I believe the fix is no longer necessary in BN but may be necessary in `Bidirectional`.", "Rebased, and should be ready to go.", "Nagging Assignee @fchollet: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think we can merge this when the tests pass.", "The master this was based on previously was causing the test error, rebased to current master to (hopefully) fix the test...", "Nagging Assignee @fchollet: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delay in getting this tested. Could you please rebase to fix the merge conflicts?", "@droidicus any chance to pull rebase and push again?", "It looks like the branch is in conflict due to the bug being fixed upstream already. This PR can be closed."]}, {"number": 17702, "title": "UnrecognizedFlagError: Unknown command line flag 'f'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: tensorflow 1.5.0\r\n- **Python version**: pyhton 3.6.3\r\n- **Bazel version (if compiling from source)**: None\r\n- **GCC/Compiler version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**:  None\r\n- **GPU model and memory**: GTX960M\r\n- **Exact command to reproduce**:\r\n\r\nProblem Description\r\n\r\nWhen trying to run this code\r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\r\nflags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\r\nflags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\r\nflags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\r\nflags.DEFINE_integer('batch_size', 100, 'Batch size. '\r\n'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\r\nflags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\r\n'for unit testing.')\r\n\r\nFLAGS = flags.FLAGS\r\nprint(FLAGS.learning_rate)\r\n```\r\nI got this error message\r\n```\r\n---------------------------------------------------------------------------\r\nUnrecognizedFlagError                     Traceback (most recent call last)\r\n<ipython-input-3-1ce89dff3e81> in <module>()\r\n      1 FLAGS = flags.FLAGS\r\n----> 2 print(FLAGS.learning_rate)\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py in __getattr__(self, name)\r\n     82     # a flag.\r\n     83     if not wrapped.is_parsed():\r\n---> 84       wrapped(_sys.argv)\r\n     85     return wrapped.__getattr__(name)\r\n     86 \r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_flagvalues.py in __call__(self, argv, known_only)\r\n    628       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n    629       raise _exceptions.UnrecognizedFlagError(\r\n--> 630           name, value, suggestions=suggestions)\r\n    631 \r\n    632     self.mark_as_parsed()\r\n\r\nUnrecognizedFlagError: Unknown command line flag 'f'\r\n```\r\nThis is another\r\n```\r\nimport tensorflow as tf\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\nFLAGS.learning_rate = 0.02\r\nFLAGS.name = 'test'\r\n\r\nprint(FLAGS.learning_rate)\r\nprint(FLAGS.name)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nUnrecognizedFlagError                     Traceback (most recent call last)\r\n<ipython-input-1-fe17797ba132> in <module>()\r\n      2 \r\n      3 FLAGS = tf.app.flags.FLAGS\r\n----> 4 FLAGS.learning_rate = 0.02\r\n      5 FLAGS.name = 'test'\r\n      6 \r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py in __setattr__(self, name, value)\r\n     86 \r\n     87   def __setattr__(self, name, value):\r\n---> 88     return self.__dict__['__wrapped'].__setattr__(name, value)\r\n     89 \r\n     90   def __delattr__(self, name):\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_flagvalues.py in __setattr__(self, name, value)\r\n    494       raise AttributeError(name)\r\n    495     if name not in fl:\r\n--> 496       return self._set_unknown_flag(name, value)\r\n    497     fl[name].value = value\r\n    498     self._assert_validators(fl[name].validators)\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_flagvalues.py in _set_unknown_flag(self, name, value)\r\n    372       except NameError:  # Flag name is not valid.\r\n    373         pass\r\n--> 374     raise _exceptions.UnrecognizedFlagError(name, value)\r\n    375 \r\n    376   def append_flag_values(self, flag_values):\r\n\r\nUnrecognizedFlagError: Unknown command line flag 'learning_rate'\r\n\r\n```\r\nDid I miss anything??\r\nI cannot figure out what the problem is.", "comments": ["I have the same problem. I build from source against CUDA 8.0 and cuDNN 6.0.\r\nI used `git checkout r1.6` before compiling.\r\n\r\n```\r\nflags = tf.app.flags\r\nflags.FLAGS.train_dir = '../data/ckpt'\r\n```\r\n\r\n`print(flags.FLAGS)` gives a valid a valid output:\r\n```\r\nobject_detection.dataset_tools.create_pet_tf_record:\r\n  --data_dir: Root directory to raw pet dataset.\r\n    (default: '')\r\n  --[no]faces_only: If True, generates bounding boxes for pet faces.  Otherwise\r\n    generates bounding boxes (as well as segmentations for full pet bodies).\r\n    Note that in the latter case, the resulting files are much larger.\r\n    (default: 'true')\r\n  --label_map_path: Path to label map proto\r\n    (default: 'data/pet_label_map.pbtxt')\r\n  --mask_type: How to represent instance segmentation masks. Options are \"png\"\r\n    or \"numerical\".\r\n    (default: 'png')\r\n  --output_dir: Path to directory to output TFRecords.\r\n    (default: '')\r\n\r\nobject_detection.train:\r\n  --[no]clone_on_cpu: Force clones to be deployed on CPU.  Note that even if set\r\n    to False (allowing ops to run on gpu), some ops may still be run on the CPU\r\n    if they have no GPU kernel.\r\n    (default: 'false')\r\n  --input_config_path: Path to an input_reader_pb2.InputReader config file.\r\n    (default: '')\r\n  --master: Name of the TensorFlow master to use.\r\n    (default: '')\r\n  --model_config_path: Path to a model_pb2.DetectionModel config file.\r\n    (default: '')\r\n  --num_clones: Number of clones to deploy per worker.\r\n    (default: '1')\r\n    (an integer)\r\n  --pipeline_config_path: Path to a pipeline_pb2.TrainEvalPipelineConfig config\r\n    file. If provided, other configs are ignored\r\n    (default: '')\r\n  --ps_tasks: Number of parameter server tasks. If None, does not use a\r\n    parameter server.\r\n    (default: '0')\r\n    (an integer)\r\n  --task: task id\r\n    (default: '0')\r\n    (an integer)\r\n  --train_config_path: Path to a train_pb2.TrainConfig config file.\r\n    (default: '')\r\n  --train_dir: Directory to save the checkpoints and training summaries.\r\n    (default: '')\r\n  --worker_replicas: Number of worker+trainer replicas.\r\n    (default: '1')\r\n    (an integer)\r\n\r\nabsl.flags:\r\n  --flagfile: Insert flag definitions from the given file into the command line.\r\n    (default: '')\r\n  --undefok: comma-separated list of flag names that it is okay to specify on\r\n    the command line even if the program does not define a flag with that name.\r\n    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\r\n    format.\r\n    (default: '')\r\n```\r\n\r\n `print(flags.FLAGS.train_dir)` gives an error:\r\n```\r\n---------------------------------------------------------------------------\r\nUnrecognizedFlagError                     Traceback (most recent call last)\r\n<ipython-input-23-d8c0abc56bc4> in <module>()\r\n----> 1 print(flags.FLAGS.train_dir)\r\n\r\n~/workspace/marvin-workspace/.env/lib/python3.6/site-packages/tensorflow/python/platform/flags.py in __getattr__(self, name)\r\n     82     # a flag.\r\n     83     if not wrapped.is_parsed():\r\n---> 84       wrapped(_sys.argv)\r\n     85     return wrapped.__getattr__(name)\r\n     86 \r\n\r\n~/workspace/marvin-workspace/.env/lib/python3.6/site-packages/absl/flags/_flagvalues.py in __call__(self, argv, known_only)\r\n    628       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n    629       raise _exceptions.UnrecognizedFlagError(\r\n--> 630           name, value, suggestions=suggestions)\r\n    631 \r\n    632     self.mark_as_parsed()\r\n\r\nUnrecognizedFlagError: Unknown command line flag 'f'\r\n```\r\n\r\nThis example works directly on the command line python interpreter but it does not work in my jupyter notebook\r\n```\r\nimport tensorflow as tf\r\nflags = tf.app.flags\r\nflags.DEFINE_integer(\"age\", 17, \"age of user(default:20)\")\r\nprint(flags.FLAGS.age)\r\n```\r\n\r\n", "@magicmutal \r\n**For the first code snippet:** \r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\r\nflags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\r\nflags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\r\nflags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\r\nflags.DEFINE_integer('batch_size', 100, 'Batch size. '\r\n'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\r\nflags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\r\n'for unit testing.')\r\n\r\nFLAGS = flags.FLAGS\r\nprint(FLAGS.learning_rate)\r\n```\r\n\r\nThis works on Tensorflow version 1.6.0 and on the source version.\r\n\r\n**For the second code snippet:**\r\n```\r\nimport tensorflow as tf\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\nFLAGS.learning_rate = 0.02\r\nFLAGS.name = 'test'\r\n\r\nprint(FLAGS.learning_rate)\r\nprint(FLAGS.name)\r\n```\r\nYou must define the learning rate and name within tf.app.flags before you can change it within tf.app.flags.FLAGS (notice the different flags). Therefore you can make this code work by doing this:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags \r\nFLAGS = tf.app.flags.FLAGS\r\nflags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\r\nflags.DEFINE_string('name', 'test', 'The name of our flag')\r\n# FLAGS.learning_rate = 0.03\r\n# FLAGS.name = 'Testing'\r\n\r\nprint(FLAGS.learning_rate)\r\nprint(FLAGS.name)\r\n```\r\n\r\nYou can un-comment FLAGS.learning_rate = 0.03 and FLAGS.name = 'Testing' if you want to change the values of learning rate or name, but you MUST declare both these variables within tf.app.flags as shown by the lines above.  @dketterer the explanation for the second code snippet is similar to your problem. I have not looked at the jupyter notebook issue however, maybe make a different issue for that.\r\n\r\nHope that helps!\r\n\r\nAs a PR idea/suggestion, maybe documentation could be made on this? Not too sure.", "@PiyushDatta \r\nThanks a lot. There was a progress!!\r\nI upgraded Tensorflow to 1.6.0 and ran the codes but I got another error message.\r\n\r\n**the first code**\r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\r\nflags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\r\nflags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\r\nflags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\r\nflags.DEFINE_integer('batch_size', 100, 'Batch size. '\r\n'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\r\nflags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\r\n'for unit testing.')\r\n\r\nFLAGS = flags.FLAGS\r\nprint(FLAGS.learning_rate)\r\n```\r\n```\r\n--------------------------------------------------------------------------------\r\nDuplicateFlagError                        Traceback (most recent call last)\r\n<ipython-input-5-95226cf0354f> in <module>()\r\n      3 flags = tf.app.flags\r\n      4 FLAGS = flags.FLAGS\r\n----> 5 flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\r\n      6 flags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\r\n      7 flags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py in wrapper(*args, **kwargs)\r\n     56           'Use of the keyword argument names (flag_name, default_value, '\r\n     57           'docstring) is deprecated, please use (name, default, help) instead.')\r\n---> 58     return original_function(*args, **kwargs)\r\n     59 \r\n     60   return tf_decorator.make_decorator(original_function, wrapper)\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_defines.py in DEFINE_float(name, default, help, lower_bound, upper_bound, flag_values, **args)\r\n    289   parser = _argument_parser.FloatParser(lower_bound, upper_bound)\r\n    290   serializer = _argument_parser.ArgumentSerializer()\r\n--> 291   DEFINE(parser, name, default, help, flag_values, serializer, **args)\r\n    292   _register_bounds_validator_if_needed(parser, name, flag_values=flag_values)\r\n    293 \r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_defines.py in DEFINE(parser, name, default, help, flag_values, serializer, module_name, **args)\r\n     80   \"\"\"\r\n     81   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\r\n---> 82               flag_values, module_name)\r\n     83 \r\n     84 \r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_defines.py in DEFINE_flag(flag, flag_values, module_name)\r\n    102   # Copying the reference to flag_values prevents pychecker warnings.\r\n    103   fv = flag_values\r\n--> 104   fv[flag.name] = flag\r\n    105   # Tell flag_values who's defining the flag.\r\n    106   if module_name:\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_flagvalues.py in __setitem__(self, name, flag)\r\n    425         # module is simply being imported a subsequent time.\r\n    426         return\r\n--> 427       raise _exceptions.DuplicateFlagError.from_flag(name, self)\r\n    428     short_name = flag.short_name\r\n    429     # If a new flag overrides an old one, we need to cleanup the old flag's\r\n\r\nDuplicateFlagError: The flag 'learning_rate' is defined twice. First from C:\\Users\\Jinsu\\python code\\MEMN2N\\ipykernel_launcher.py, Second from C:\\Users\\Jinsu\\python code\\MEMN2N\\ipykernel_launcher.py.  Description from first occurrence: Initial learning rate.\r\n```\r\n\r\nthe second code\r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags \r\nFLAGS = tf.app.flags.FLAGS\r\nflags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\r\nflags.DEFINE_string('name', 'test', 'The name of our flag')\r\n# FLAGS.learning_rate = 0.03\r\n# FLAGS.name = 'Testing'\r\n\r\nprint(FLAGS.learning_rate)\r\nprint(FLAGS.name)\r\n```\r\n```\r\n--------------------------------------------------------------------------------\r\nDuplicateFlagError                        Traceback (most recent call last)\r\n<ipython-input-6-19d9c11b9a80> in <module>()\r\n      3 flags = tf.app.flags\r\n      4 FLAGS = tf.app.flags.FLAGS\r\n----> 5 flags.DEFINE_float('learning_rate', 0.02, 'Initial learning rate.')\r\n      6 flags.DEFINE_string('name', 'test', 'The name of our flag')\r\n      7 # FLAGS.learning_rate = 0.03\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py in wrapper(*args, **kwargs)\r\n     56           'Use of the keyword argument names (flag_name, default_value, '\r\n     57           'docstring) is deprecated, please use (name, default, help) instead.')\r\n---> 58     return original_function(*args, **kwargs)\r\n     59 \r\n     60   return tf_decorator.make_decorator(original_function, wrapper)\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_defines.py in DEFINE_float(name, default, help, lower_bound, upper_bound, flag_values, **args)\r\n    289   parser = _argument_parser.FloatParser(lower_bound, upper_bound)\r\n    290   serializer = _argument_parser.ArgumentSerializer()\r\n--> 291   DEFINE(parser, name, default, help, flag_values, serializer, **args)\r\n    292   _register_bounds_validator_if_needed(parser, name, flag_values=flag_values)\r\n    293 \r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_defines.py in DEFINE(parser, name, default, help, flag_values, serializer, module_name, **args)\r\n     80   \"\"\"\r\n     81   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\r\n---> 82               flag_values, module_name)\r\n     83 \r\n     84 \r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_defines.py in DEFINE_flag(flag, flag_values, module_name)\r\n    102   # Copying the reference to flag_values prevents pychecker warnings.\r\n    103   fv = flag_values\r\n--> 104   fv[flag.name] = flag\r\n    105   # Tell flag_values who's defining the flag.\r\n    106   if module_name:\r\n\r\nc:\\users\\jinsu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\absl\\flags\\_flagvalues.py in __setitem__(self, name, flag)\r\n    425         # module is simply being imported a subsequent time.\r\n    426         return\r\n--> 427       raise _exceptions.DuplicateFlagError.from_flag(name, self)\r\n    428     short_name = flag.short_name\r\n    429     # If a new flag overrides an old one, we need to cleanup the old flag's\r\n\r\nDuplicateFlagError: The flag 'learning_rate' is defined twice. First from C:\\Users\\Jinsu\\python code\\MEMN2N\\ipykernel_launcher.py, Second from C:\\Users\\Jinsu\\python code\\MEMN2N\\ipykernel_launcher.py.  Description from first occurrence: Initial learning rate.\r\n\r\n```\r\n\r\nActually, I got same error message. \r\nShould I edit the ipykernel_launcher.py file??", "@magicmutal \r\nI think you are trying to run both code snippets in the same file. If so, that is the reason you are getting this error:\r\n`DuplicateFlagError: The flag 'learning_rate' is defined twice.`\r\n\r\nIf you want to run two flag objects in the same code snippet you must change the variable names for learning rate, try the code below:\r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags \r\nFLAGS = tf.app.flags.FLAGS\r\nflags.DEFINE_float('learning_rate_1', 0.02, 'Initial learning rate.')\r\nflags.DEFINE_string('name', 'test', 'The name of our flag')\r\n# FLAGS.learning_rate = 0.03\r\n# FLAGS.name = 'Testing'\r\n\r\nprint(FLAGS.learning_rate_1)\r\nprint(FLAGS.name)\r\n\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_float('learning_rate_2', 0.01, 'Initial learning rate.')\r\nflags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')\r\nflags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')\r\nflags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')\r\nflags.DEFINE_integer('batch_size', 100, 'Batch size. '\r\n'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')\r\nflags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\r\n'for unit testing.')\r\n\r\nFLAGS = flags.FLAGS\r\nprint(FLAGS.learning_rate_2)\r\n```\r\n\r\nAs you can see learning rate was changed to learning_rate_1 and learning_rate_2.\r\nLet me know if that fixes it.", "I run that code but still get the same error:\r\n---------------------------------------------------------------------------\r\nUnrecognizedFlagError                     Traceback (most recent call last)\r\n<ipython-input-1-1b7dec891ee0> in <module>()\r\n      8 # FLAGS.name = 'Testing'\r\n      9 \r\n---> 10 print(FLAGS.learning_rate_1)\r\n     11 print(FLAGS.name)\r\n     12 \r\n\r\n~/projects/thanos/recently_action/env3/lib/python3.6/site-packages/tensorflow/python/platform/flags.py in __getattr__(self, name)\r\n     82     # a flag.\r\n     83     if not wrapped.is_parsed():\r\n---> 84       wrapped(_sys.argv)\r\n     85     return wrapped.__getattr__(name)\r\n     86 \r\n\r\n~/projects/thanos/recently_action/env3/lib/python3.6/site-packages/absl/flags/_flagvalues.py in __call__(self, argv, known_only)\r\n    628       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n    629       raise _exceptions.UnrecognizedFlagError(\r\n--> 630           name, value, suggestions=suggestions)\r\n    631 \r\n    632     self.mark_as_parsed()\r\n\r\nUnrecognizedFlagError: Unknown command line flag 'f'", "Is there any progress on this issue, I am having the same problem?", "@quangvu0702 @moguzozcan are you both running tensorflow 1.6.0? And which code snippet gives you the error?", "I was getting the error first at 1.5.0 then I upgraded to the latest version 1.7.0. When I call cifar10.maybe_download_and_extract() method it gives the error. I am using anaconda.\r\n\r\n\r\n`UnrecognizedFlagError                     Traceback (most recent call last)\r\n<ipython-input-13-02a754d7036a> in <module>()\r\n----> 1 cifar10.maybe_download_and_extract()\r\n\r\nD:\\Google Drive\\Akademik\\Ozyegin\\CS566\\Project 2\\cifar10.py in maybe_download_and_extract()\r\n    381 def maybe_download_and_extract():\r\n    382   \"\"\"Download and extract the tarball from Alex's website.\"\"\"\r\n--> 383   dest_directory = FLAGS.data_dir\r\n    384   if not os.path.exists(dest_directory):\r\n    385     os.makedirs(dest_directory)\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py in __getattr__(self, name)\r\n     82     # a flag.\r\n     83     if not wrapped.is_parsed():\r\n---> 84       wrapped(_sys.argv)\r\n     85     return wrapped.__getattr__(name)\r\n     86 \r\n\r\n~\\anaconda3\\lib\\site-packages\\absl\\flags\\_flagvalues.py in __call__(self, argv, known_only)\r\n    628       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n    629       raise _exceptions.UnrecognizedFlagError(\r\n--> 630           name, value, suggestions=suggestions)\r\n    631 \r\n    632     self.mark_as_parsed()\r\n\r\nUnrecognizedFlagError: Unknown command line flag 'f'`", "I had the same issue and I figured out now it works with version `1.7.0`. It might be useful for the others:\r\n`import tensorflow as tf`\r\nThen define your flags with using `tf.app.flags`. When you finished defining flags do your assignment as `FLAGS = tf.app.flags.FLAGS`.\r\n\r\nAnd if you need to run `tf.app.run` run on main function.", "But how to run using jupter ? ", "I face this problem when running code in jupyter notebook. And I don't set any flag named `f`.\r\nWhen using command line to run the same `.py` file, there's no more `Unknown command line flag 'f'`.\r\nMay this info could help.", "Same here. Any suggestions? Updated to the latest 1.8.0 version of TF", "Same here in Jupyter Notebook\r\n(Working well in terminal)\r\n\r\nTF version is latest (1.8.0) \r\nAnaconda2(Python2.7) Jupyter notebook\r\n\r\nprint(FLAGS.learning_rate)  \r\nthrows\r\n-----------------------------------------------------------------------------------------------------------\r\nUnrecognizedFlagErrorTraceback (most recent call last)\r\n<ipython-input-15-aa999f68f930> in <module>()\r\n----> 1 print(FLAGS.learning_rate)\r\n\r\n/home/budweiser/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/flags.pyc in __getattr__(self, name)\r\n     82     # a flag.\r\n     83     if not wrapped.is_parsed():\r\n---> 84       wrapped(_sys.argv)\r\n     85     return wrapped.__getattr__(name)\r\n     86 \r\n\r\n/home/budweiser/anaconda2/lib/python2.7/site-packages/absl/flags/_flagvalues.pyc in __call__(self, argv, known_only)\r\n    628       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n    629       raise _exceptions.UnrecognizedFlagError(\r\n--> 630           name, value, suggestions=suggestions)\r\n    631 \r\n    632     self.mark_as_parsed()\r\n\r\nUnrecognizedFlagError: Unknown command line flag 'f'\r\n-----------------------------------------------------------------------------------------------------------EO.Error\r\n\r\nThis is working ...\r\nprint(FLAGS['learning_rate'].value)\r\n0.001", "Same here:\r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\n\r\nflags.DEFINE_string(\"positive_data_file\", \"./data/raw_comments.pos\", \"Data source for the positive data.\")\r\nflags.DEFINE_string(\"neutral_data_file\", \"./data/raw_comments.neu\", \"Data source for the neutral data.\")\r\nflags.DEFINE_string(\"negative_data_file\", \"./data/raw_comments.neg\", \"Data source for the negative data.\")\r\n\r\nprint(FLAGS.negative_data_file)\r\n```\r\n\r\ngives\r\n```python\r\n---------------------------------------------------------------------------\r\nUnrecognizedFlagError                     Traceback (most recent call last)\r\n<ipython-input-1-27f35e1d55fc> in <module>()\r\n      8 flags.DEFINE_string(\"negative_data_file\", \"./data/raw_comments.neg\", \"Data source for the negative data.\")\r\n      9 \r\n---> 10 print(FLAGS.negative_data_file)\r\n\r\n~/venvs/scipylearn/lib/python3.5/site-packages/tensorflow/python/platform/flags.py in __getattr__(self, name)\r\n     82     # a flag.\r\n     83     if not wrapped.is_parsed():\r\n---> 84       wrapped(_sys.argv)\r\n     85     return wrapped.__getattr__(name)\r\n     86 \r\n\r\n~/venvs/scipylearn/lib/python3.5/site-packages/absl/flags/_flagvalues.py in __call__(self, argv, known_only)\r\n    628       suggestions = _helpers.get_flag_suggestions(name, list(self))\r\n    629       raise _exceptions.UnrecognizedFlagError(\r\n--> 630           name, value, suggestions=suggestions)\r\n    631 \r\n    632     self.mark_as_parsed()\r\n\r\nUnrecognizedFlagError: Unknown command line flag 'f'\r\n```", "I get this error on google colab. What is the way forward/workaround?", "I solved this issue by adding the line:\r\n`tf.app.flags.DEFINE_string('f', '', 'kernel')`\r\nwhat this essentially does is that it creates a flag f. This is effective as jupyter notebook supplies a flag \"-f\" to pass a name for a JSON file, likely for the kernel.", "@taixhi simple but cool solution there. thanks a lot.", "Thanks @taixhi, that solved it for me too. ", "same issue, 1.8.0 and python 3.6.5.\r\nAnd solved with @taixhi 's method. thanks", "Thanks @taixhi . It is only valid for jupyter env. guys!", "Thanks a lot @taixhi, it works. ", "This is still a bug, I'm using the native pip environment.\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): tensorflow 1.9.0\r\nPython version: pyhton 3.6.6\r\nBazel version (if compiling from source): None\r\nGCC/Compiler version (if compiling from source): None\r\nCUDA/cuDNN version:  9.2.148/7.1.4\r\nGPU model and memory: GTX770m", "Your solutions works like a charm. Thanks, @taixhi !\r\nAnd yes, it's still a bug", "@taixhi very neat solution! Tnx.", "this is still a bug, and there isn't a known cause. please reopen", "@magicmutal  Is this still an issue? Did you get a chance to try taixhi's suggestion?\r\n\r\n> I solved this issue by adding the line:\r\n> `tf.app.flags.DEFINE_string('f', '', 'kernel')`\r\n> what this essentially does is that it creates a flag f. This is effective as jupyter notebook supplies a flag \"-f\" to pass a name for a JSON file, likely for the kernel.\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to staleness. Please use the latest version for TensorFLow and build again. Feel free to open a new issue if it still persists. Thanks!", "For those who are having issues using the flags module in jupyter notebooks, it is because sys.argv is populated from jupyter, not your notebook code.\r\nPut the following in your first cell:\r\n```python\r\nimport sys\r\nsys.argv = \" --your flags --placed here\".split(\" \")\r\n# Note the space at the beginning\r\n```", "As @spotiris mentioned you could just remove all the cli args by\r\n```python\r\nimport sys\r\nsys.argv = sys.argv[:1]\r\n```", "\r\n\r\nI want to train my model but I am receiving this error kindly help me + [ ! -f DeepSpeech.py ] + python -u DeepSpeech.py --train_files /home/sehar/urdu/train/urdu-train.csv --dev_files /home/sehar/urdu/urdu-train.csv --test_files /home/sehar/urdu/train/urdu-train.csv --train_batch_size 80 --dev_batch_size 80 --test_batch_size 40 --n_hidden 375 --epochs 33 --validation_step 1 --early_stop True --earlystop_nsteps 6 --estop_mean_thresh 0.1 --dropout_rate 0.22 --learning_rate 0.00095 --report_count 100 --use_seq_length False --export_dir /home/sehar/urdu-models --checkpoint_dir /home/nvidia/DeepSpeech/checkout/ --alphabet_config_path /home/sehar/urdu-models/native_client/alphabet.txt/ /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)]) FATAL Flags parsing error: Unknown command line flag 'validation_step' Pass --helpshort or --helpfull to see help on flags.\r\n", "> I want to train my model but I am receiving this error kindly help me + [ ! -f DeepSpeech.py ] + python -u DeepSpeech.py --train_files /home/sehar/urdu/train/urdu-train.csv --dev_files /home/sehar/urdu/urdu-train.csv --test_files /home/sehar/urdu/train/urdu-train.csv --train_batch_size 80 --dev_batch_size 80 --test_batch_size 40 --n_hidden 375 --epochs 33 --validation_step 1 --early_stop True --earlystop_nsteps 6 --estop_mean_thresh 0.1 --dropout_rate 0.22 --learning_rate 0.00095 --report_count 100 --use_seq_length False --export_dir /home/sehar/urdu-models --checkpoint_dir /home/nvidia/DeepSpeech/checkout/ --alphabet_config_path /home/sehar/urdu-models/native_client/alphabet.txt/ /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)]) /home/sehar/DeepSpeech/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)]) /home/sehar/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\"resource\", np.ubyte, 1)]) FATAL Flags parsing error: Unknown command line flag 'validation_step' Pass --helpshort or --helpfull to see help on flags.\r\n\r\nsame to me ", "@taixhi Could you please help me in solving for different DEFINED Parameters. As your solution just solves it for **DEFINE_string** by using\r\ntf.app.flags.DEFINE_string('f', '', 'kernel')\r\n\r\nHow can I declare the flags for the below code which has integer, string and float ?\r\n\r\nflags = tf.app.flags\r\nFLAGS = tf.app.flags.FLAGS\r\nflags.**DEFINE_integer**('hidden1', 128, 'Number of units in hidden layer 1.')\r\nflags.**DEFINE_string**('train_dir', 'tf_logs', 'Directory to put the training data.')\r\nflags.**DEFINE_float**('reg_constant', 0.1, 'Regularization constant.')", "I can confirm this bug is very much still there. What makes it worse is a lot of the tf-agents example scripts out there have the bug baked in... It really needs fixing....", "bump", "> As @spotiris mentioned you could just remove all the cli args by\r\n> \r\n> ```python\r\n> import sys\r\n> sys.argv = sys.argv[:1]\r\n> ```\r\n\r\nIt can help me."]}, {"number": 17701, "title": "contrib/tpu: minor spelling tweaks", "body": "", "comments": []}, {"number": 17700, "title": "make TFLite kernel tests work", "body": "make\r\n```\r\nbazel build --config opt //tensorflow/contrib/lite/kernels:all\r\n```\r\nwork. And then\r\n```\r\nbazel test --config opt //tensorflow/contrib/lite/kernels:all\r\n```\r\nworks.", "comments": ["Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @aselle: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @aselle: It has been 46 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 17699, "title": "How to convert .pb file to .ckpt file", "body": "How to convert .pb file to .ckpt file??\r\n   as following code raise error:  ValueError: No variables to save\r\n\r\n#!/usr/bin/env python\r\n#coding: utf-8\r\n\r\nimport tensorflow as tf\r\nimport argparse \r\n\r\n# Pass the filename as an argument\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--frozen_model_filename\", default=\"frozen_inference_graph.pb\", type=str, help=\"Pb model file to import\")\r\nargs = parser.parse_args()\r\n\r\n# We load the protobuf file from the disk and parse it to retrieve the \r\n# unserialized graph_def\r\nwith tf.gfile.GFile(args.frozen_model_filename, \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\n    #saver=tf.train.Saver()\r\n    with tf.Graph().as_default() as graph:\r\n\r\n        tf.import_graph_def(\r\n            graph_def,\r\n            input_map=None,\r\n            return_elements=None,\r\n            name=\"prefix\",\r\n            op_dict=None,\r\n            producer_op_list=None\r\n        )\r\n        sess = tf.Session(graph=graph)\r\n        saver = tf.train.Saver()\r\n        save_path = saver.save(sess, \"model.ckpt\")\r\n        print(\"Model saved to chkp format\")", "comments": ["I have save the model to protobuf  (pb and pbtxt,  as Protobuf.pb and Text_Protobuf.pbtxt)", "As far as I known, pb file contains only graph information, while ckpt file needs both graph and variable. So I don't think it sounds feasible.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@angersson I posted it to StackOverflow, could you help out posting there the code to convert pb to ckpt ?", "@dinglong1020 have you got it done?I also want to convert pb into ckpt file,but failed finnally. If you kown how to fix it now,please tell me,thanks!", "Has anyone known how to do it? I also want to convert .pb to a ckpt file.", "Does anyone know how to solve this problem?", "Has anyone known how to do it? I also want to convert .pb to a ckpt file.thx", "is it resolved, i also want to know how to convert pb to ckpt", "Has this been solved yet?"]}, {"number": 17698, "title": "Session JNI interface has memory leak", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): Master\r\nPython version: 3.6\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\nExact command to reproduce: Run the snippet below.\r\n\r\nDescribe the problem\r\nRead the tensorflow_jni.cc code in the code section as blow.\r\n\r\n```\r\n  TF_Graph* graph = reinterpret_cast<TF_Graph*>(graph_handle);\r\n  TF_Status* status = TF_NewStatus();\r\n  TF_SessionOptions* opts = TF_NewSessionOptions();\r\n  const char* ctarget = nullptr;\r\n  jbyte* cconfig = nullptr;\r\n  if (target != nullptr) {\r\n    ctarget = env->GetStringUTFChars(target, nullptr);\r\n  }\r\n  if (config != nullptr) {\r\n    cconfig = env->GetByteArrayElements(config, nullptr);\r\n    TF_SetConfig(opts, cconfig,\r\n                 static_cast<size_t>(env->GetArrayLength(config)), status);\r\n    if (!throwExceptionIfNotOK(env, status)) {\r\n      env->ReleaseByteArrayElements(config, cconfig, JNI_ABORT);\r\n      //Not release the status, opts\r\n      return 0;\r\n    }\r\n  }\r\n```\r\nAfter throw exception, it look like not release the status, opts memory.\r\n\r\n\r\n", "comments": ["Update the code in the github try to fix it. Please check it.\r\nhttps://github.com/raintung/TensorflowPatch/blob/master/v1.6/java/src/main/native/session_jni.cc", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thank you for bringing this to our attention. That does appear to be a small memory leak. We'd welcome a pull request fixing it.", "@jart Can you please assign me this. I'm new might need help, but will submit a PR fixing this"]}]