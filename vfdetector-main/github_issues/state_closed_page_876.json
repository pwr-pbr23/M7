[{"number": 27212, "title": "Update eigen for pcmp_eq method for altivec [ppc64le]", "body": "Commit dcbc812 made use of the pcmp_eq method for Packet\r\nfor the first time, this works fine on x86 but fails to compile\r\non ppc64le because the pcmp_eq method was not implemented in\r\nEigen's arch/AltiVec/Complex.h for float.\r\n\r\nEigen has now been updated to implement pcmp_eq in AltiVec,\r\nupdating TensorFlow to use that version of Eigen.", "comments": ["FYI @rmlarsen , I appreciate you taking the time to review my changes in Eigen. ", "@rthadur I think @rmlarsen is the more appropriate person to review this.", "Was resolved by: https://github.com/tensorflow/tensorflow/commit/731c1c71f2017a57ce06f929ababb39e69785b7d\r\n\r\nThanks!"]}, {"number": 27211, "title": "Building the whl fails with Extension not found", "body": "If using Extension, you best import it\r\nFix break by commit: e5778e4", "comments": ["FYI @av8ramit "]}, {"number": 27210, "title": "[Intel MKL] Temporarily disable mkl dequantize op", "body": "Dsiabling mkl dequantize temporarily.", "comments": []}, {"number": 27209, "title": "TensorFlowLiteSwift performance problem with bitcode enabled on iOS", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, I converted the [SpeechCommands example](https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/ios/SpeechCommands) to use the TensorFlowLiteSwift API instead of the C wrapper in the original example.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPad Mini 2 real device and iPhone 6s simulator\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): latest commit is 95e72f827f8aa0c352d6e1f47ffeec58fe4c9ec9 from March 22.\r\n- Python version: N/A for this problem\r\n- Bazel version (if compiling from source): 0.23.2\r\n- GCC/Compiler version (if compiling from source):\r\n$ gcc --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1\r\nApple LLVM version 10.0.0 (clang-1000.11.45.5)\r\nTarget: x86_64-apple-darwin18.5.0\r\n\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nCompiling the TensorFlowLiteC_framework with bitcode support enabled slows down model inference on the sample model from ~72 ms (without bitcode support) to ~1200 ms (with bitcode support).\r\n\r\n**Describe the expected behavior**\r\nWould be nice if performance was approximately equivalent.\r\n\r\n**Code to reproduce the issue**\r\nCompile the TensorFlowLiteC_framework without bitcode support:\r\nbazel build tensorflow/lite/experimental/c:TensorFlowLiteC_framework -c opt --ios_multi_cpus=x86_64,armv7,arm64\r\nThen for comparison compile it with bitcode support:\r\nbazel build tensorflow/lite/experimental/c:TensorFlowLiteC_framework -c fastbuild --ios_multi_cpus=x86_64,armv7,arm64 --apple_bitcode=embedded --copt=-fembed-bitcode\r\n\r\nUse these with the experimental TensorFlowLiteSwift CocoaPod (not yet publicly available, see #25800 for reference) to perform a model inference on a real device or a simulator.  Regardless of debug/release build, if you use the framework with bitcode support in it then performance suffers greatly.\r\n\r\n**Other info / logs**\r\n\r\nIf it would help to provide my Xcode project, I'm happy to provide it.  The Podfile would need to be modified to locate your own build CocoaPods because the TensorFlowLiteC pod uses the framework in question and it's not yet publicly available.", "comments": ["Is `-c fastbuild` required for bitcode support? In general, we don't really make any promises about performance in debug builds, particularly debug builds running on emulators.", "On the referenced issue #25800 @temrich said we needed that switch for bitcode support at least until bazel 0.24 is available.  You think that switch (vs `-c opt`) is what's making all the difference?  Nothing to do with bitcode?\r\n\r\nRegarding emulators, I think the numbers above were my numbers on a real device.  Simulator is much faster, but with somewhat similar ratios of performance to a real device.", "@temrich just released a publicly available `TensorFlowLiteSwift` pod and it doesn't have this problem.  Performance is the same with/without bitcode enabled so I'm closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27209\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27209\">No</a>\n"]}, {"number": 27208, "title": "TFTRT: Fix compilation errors and crash in NMS test", "body": "* Fix type mismatch crash in CombinedNMS test.\r\n* Fix compilation error due to absl::InlinedVector\r\n* Fix compilation error for input argument to network()->addSlice()", "comments": ["@trevor-m your last commit conflict with the submission of this PR, and now the PR is actually merged but that was not shown here. Sorry about that."]}, {"number": 27207, "title": "TF2.0 distribution strategy hanging on keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source \r\n from [tf2.0 branch](https://github.com/tensorflow/tensorflow/tree/v2.0.0-alpha0)\r\n- TensorFlow version (use command below): 2.0.0a0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 19.2\r\n- GCC/Compiler version (if compiling from source): 6.3\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the current behavior**\r\nTrying to run simple example on cpu in distributed mode according to the documentation.  [tf2.0 documentation](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/guide/distribute_strategy.ipynb) \r\n\r\nIf I run the code using only python py_file.py then it is running fine. \r\nIf I run it by setting TF_CONFIG environment variable with only 2 different node or even a single node then it is hanging. \r\n\r\n\r\nIt hangs after generating this information: \r\n2019-03-27 11:44:19.963538: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-03-27 11:44:19.976632: I tensorflow/core/common_runtime/process_util.cc:92] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0327 11:44:19.979773 47985184916416 deprecation.py:506] From /panfs/users/mdkamruz/anaconda3/envs/py352_otf2.0_source/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0327 11:44:20.015144 47985184916416 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0327 11:44:20.015259 47985184916416 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2019-03-27 11:44:20.021537: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3002}\r\n2019-03-27 11:44:20.027697: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:359] Started server with target: grpc://localhost:3002\r\n2019-03-27 11:44:20.027718: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:363] Server already started (target: grpc://localhost:3002)\r\nW0327 11:44:20.095507 47985184916416 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0327 11:44:20.095634 47985184916416 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nW0327 11:44:20.097250 47985184916416 distributed_training_utils.py:933] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n\r\n**Describe the expected behavior**\r\nIt should run smoothly as described in the documentation. \r\n\r\n**Code to reproduce the issue**\r\nPython code: \r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).shuffle(buffer_size=10000).batch(10)\r\n# single node\r\nds_strategy=tf.distribute.MirroredStrategy()\r\n# cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()\r\n# multinode\r\n# ds_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\nwith ds_strategy.scope():\r\n    model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\r\n    model.compile(loss='mse', optimizer='sgd')\r\n    model.fit(dataset,epochs=200)\r\n\r\n```\r\nTF_CONFIG environment variable: \r\n```\r\nexport TF_CONFIG='{\r\n    \"cluster\": {\r\n          \"worker\": [\"IP_ADDR:3001\"]\r\n          },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n}'\r\n# for single node multi worker\r\nexport TF_CONFIG='{\r\n    \"cluster\": {\r\n          \"worker\": [\"IP_ADDR:3001\",\"IP_ADDR:3002\"]\r\n          },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n}'\r\n# for multinode \r\n# node 1 and similar for node 2\r\nexport TF_CONFIG='{\r\n    \"cluster\": {\r\n          \"worker\": [\"IP_ADDR_1:3001\",\"IP_ADDR_2:3002\"]\r\n          },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n}'\r\n```\r\n\r\n**Other info / logs**\r\nRunning on cpu, machine don't have any gpu\r\n", "comments": ["According to the provided document.\r\n`tf.distribute.MirroredStrategy support synchronous distributed training on multiple GPUs on one machine. `\r\n\r\nSo running dist strat on cpu does not make sense...", "> According to the provided document.\r\n> `tf.distribute.MirroredStrategy support synchronous distributed training on multiple GPUs on one machine. `\r\n> \r\n> So running dist strat on cpu does not make sense...\r\n\r\nThe nccl leads to tensorflow hang all the time no matter local training or multiworker training.", "The tutorial you have referenced works fine using google colab. Please give it a try using google colab. Thanks!", "It seems that google colab do not provide GPU, so TensorFlow will not use `nccl` library to reduce, it can not  repeat the problem.", "You can select gpu accelerator under edit > notebook settings > hardware accelerator ", "But there is only one GPU in google colab, in which case tensorflow will not choose `nccl` to do reduction.", "Not sure if the same issue, but I also experienced frequent hanging when using MultiWorkerMirroredStrategy with nccl. TF 2.0.0-alpha0 uses nccl 2.3.5-5. I modified the tensorflow source to use 2.3.7-1 instead and that seems to have fixed the issue. The [release notes](https://github.com/NVIDIA/nccl/releases/tag/v2.3.7-1) for 2.3.7-1 mentions\r\n> Fixed a hang during bootstrap due to socket reuse.", "I used master branch source code to build tensorflow 2.0 and seems it is working. Hence closing the issue. "]}, {"number": 27206, "title": "Use current_java_runtime instead of local_jdk.", "body": "Relying on @local_jdk breaks remote execution from a host without JDK installed.\r\n\r\nhttps://github.com/bazelbuild/bazel/issues/5594", "comments": ["This doesn't seem to work. I'll look into it and ping back to this thread when it's ready for review.", "@katre and @cushon is this the current recommended way to access the java set by the --host_javabase?", "You should use `@bazel_tools//tools/jdk:current_host_java_runtime` in this situation, but it looks good otherwise."]}, {"number": 27205, "title": "[XLA] Replace computation in kAllReduce correctly", "body": "kAllReduce is not currently handled ", "comments": []}, {"number": 27204, "title": "[TF2.0] How to build with Api v.2", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Buster\r\n- TensorFlow version: v2.0.0-alpha0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.23.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: 10 / 7\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI have tried compiling TF 2.0 from source, because my platform has a libc which is too old for the normal distribution.\r\nI have followed the instructions from here: https://www.tensorflow.org/install/source.\r\n\r\nIt compiles successfully, however it seems that the created package still uses the old v1 api.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\ngit checkout v2.0.0-alpha0\r\n\r\nbazel build --local_resources 4096,25,1 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag /tmp/tensorflow_pkg\r\n\r\npip3 install tf_nightly-2.0.0a0-cp37-cp37m-linux_x86_64.whl --user\r\n```\r\n\r\nI then get the error\r\n\r\n```\r\nAttributeError: module 'tensorflow._api.v1.summary' has no attribute 'create_file_writer'\r\n```\r\n\r\n", "comments": ["Steps\r\n```instructions\r\n     Bazel -Ok\r\n     tensorflow Ok\r\n   \r\n      python ok\r\n      python-packages ok\r\n      tensorflow xtla  Yes\r\n      tensorflow cuda no\r\n      tensorflow roc support Yes\r\n      tensorflow mip support no\r\n      fresh clang yes\r\n      bazel build -c opt ok\r\nandroid support no \r\n```\r\n`sudo cp ${GOPATH}/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow/libtensorflow.so /usr/local/lib `      fail\r\n`export LIBRARY_PATH=${GOPATH}/src/github.com/tensorflow/tensorflow/tensorflow`    pass\r\n```\r\nKoohinoor-ceo% go test github.com/tensorflow/tensorflow/tensorflow/go \r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\n/usr/bin/ld: cannot find -ltensorflow\r\ncollect2: error: ld returned 1 exit status\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go [build failed]\r\n```\r\n```\r\nKindly share me how i build tensorflow go\r\n   \r\n\r\n", "I found out the solution was adding a `--define=tf_api_version=2` flag to the bazel invocation:\r\n\r\n```bash\r\nbazel build --local_resources 2048,25,2 --define=tf_api_version=2 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```", "Add '--config=v2' to the bazel command line should be enough,", "Configuration had finished successfully, but Libtensorflow.so can't find.\nThat's why build failed. How to do that? Please help me\nThis Message send through secure channel called BBM\n\n\n\nOn Thu, Mar 28, 2019 at 12:10 AM William D. Irons <notifications@github.com>\nwrote:\n\n> Add '--config=v2' to the bazel command line should be enough,\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27204#issuecomment-477306667>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ATnYD0E1q76dG3Xohh9JXRlp_yFBTcNlks5va8IRgaJpZM4cOURF>\n> .\n>\n", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Please open new issue for other questions. Thanks!"]}, {"number": 27203, "title": "install page wrong for version 2.0 alpha", "body": "on page https://www.tensorflow.org/install/pip\r\n\r\nchange \"tensorflow==2.0.0-alpha0\" to \"tensorflow==2.0.0-a0\"    (two instances)\r\nchange \"tensorflow-gpu==2.0.0-alpha0\" to \"tensorflow-gpu==2.0.0-a0\"\r\nchange \"tensorflow==2.0.0-alpha0-gpu\" to \"tensorflow-gpu==2.0.0-a0\"\r\n\r\non page https://www.tensorflow.org/install/gpu\r\n\r\nchange \"tensorflow==2.0.0-alpha0\" to \"tensorflow==2.0.0-a0\" \r\n\r\ni had to make these changes on my ubuntu-18-04 system to get it to install.", "comments": ["```pip install tensorflow-gpu==2.0.0-alpha0``` and\r\n```pip install tensorflow==2.0.0-alpha0``` work as expected.\r\nI was able to download the wheel and install it successfully.", "you are right, they are both there.  I had tried \"tensorflow==2.0.0-alpha0-gpu\" and when that did not work just went looking around and came across the a0 aliases. \r\n\r\nI do believe that that \"tensorflow==2.0.0-alpha0-gpu\" reference is actually wrong, but the others are correct.   ", "Yes you are right about the ```tensorflow==2.0.0-alpha0-gpu``` we have to correct it. Thanks!", "@zadeck It has been fixed now. Thanks!"]}, {"number": 27202, "title": "Tflite JNI wraps seems failing to release int array.", "body": "Hi, it seems that current impl of tflite jni overlooked ref release for array, And the current tflite really could make JNI reference table overflow some phones with Android 4.4.4 (API 19).\r\n\r\nHow: \r\nJust invoke `resizeInput` every time you run interpreter, even put the same int array to it. You can see the reference table is booming.\r\n\r\nversion:\r\nI've tried 1.13.1 and 0.0.0-nightly, it's the same.\r\n\r\nthe relevant code is [here](https://github.com/tensorflow/tensorflow/blob/c18034bc927f733e5e5a43d0c421775f969e0d04/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L104)\r\n\r\nThe strange thing is the code above, althrough didn't deal with release in some situation, should work fine with same int array. ", "comments": ["You're absolutely right, will push a fix shortly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27202\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27202\">No</a>\n"]}, {"number": 27201, "title": "Error when restoring model using an LSTM layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1803\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nReincarnation of bug #25327 for different layer type (LSTM rather than Embedding). After applying fix [cebce4a](https://github.com/tensorflow/tensorflow/commit/cebce4a2b5e33a06a1c92772008082895568f10a) for bug #25327, the code provided below results in `ValueError: Input 0 of node lstm/while/ReadVariableOp/Enter_1 was passed float from lstm/kernel_1:0 incompatible with expected resource.`.\r\n\r\n**Describe the expected behavior**\r\nShould restore model rather than raise error.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n# from tensorflow.python.framework import graph_util\r\nimport graph_util_fixed as graph_util  # using file with fix cebce4a\r\nfrom tensorflow.python.framework import graph_io\r\nfrom tensorflow.python.platform import gfile\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Embedding(0x1000, output_dim=128))\r\nmodel.add(keras.layers.LSTM(512))\r\nmodel.add(keras.layers.Dense(1, activation=\"sigmoid\"))\r\n\r\nsess = keras.backend.get_session()\r\noutput_node_names = [node.op.name for node in model.outputs]\r\nconstant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), output_node_names)\r\ngraph_io.write_graph(constant_graph, \".\", \"test.pb\", as_text=False)\r\n\r\nwith tf.Session() as sess:\r\n    with gfile.FastGFile(\"test.pb\", \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        sess.graph.as_default()\r\n        tf.import_graph_def(graph_def, name=\"\")\r\n```\r\n", "comments": ["@jgehw For saving and restoring, we would recommend you use tf.keras.Model.save() or tf.keras.experimental.export_saved_model(model). These will export to the Keras serialization format and the TF Saved Model format, respectively, both of which can be restored. ", "@karmel Thanks for your hint. The code I provided was just a minimalistic example. Restoration of the model (i.e. the last 6 lines) is to be done using the tensorflow C bindings from a different program written in C++. I'd love to just call tf.keras.models.load_model() if it was available in the C bindings; the example code I provided was just my desperate try to get a tf.keras.Model saved in a way loadable using the tensorflow C bindings. Please correct me if I'm wrong or if there's a better way to achieve this.", "Assigning this to Kathy who is working on the save/load model. From my understanding of save/load model, we don't have strong support on c bindings. I will leave this to Kathy and Karmel for final answer.", "I don't entirely follow the use case and what you want, but the saved model loader provided from the TF C++ API will work with the models produced from tf.keras.experimental.export_saved_model, at least for inference: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.h#L50", "@karmel Thanks for pointing me to tf.keras.experimental.export_saved_model. I verified that this works in my setting, too. Provided that this experimental API will become regular API one day and not get removed, this obsoletes my code reproducing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27201\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27201\">No</a>\n"]}, {"number": 27200, "title": "[tflite] multithreaded DepthwiseConv returns wrong results on aarch64 platform", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): nope\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android Pie (Pixel 2) and Debian (Coral EdgeTPU Dev Board)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): recent master 8d1c139712a65a25419e951eca737c38f917262b\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): 0.24.0\r\n- GCC/Compiler version (if compiling from source): gcc 6 on Coral Dev Board, clang in Android NDK r17b\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\n> adb shell\r\nwalleye:/ $ cd /data/local/tmp\r\nwalleye:/data/local/tmp $ ./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 1                                                              \r\nLoaded model mobilenet_v1_1.0_224_quant.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 66.784 ms \r\n0.419608: 907 Windsor tie\r\n0.356863: 653 military uniform\r\n0.0352941: 458 bow tie\r\n0.027451: 668 mortarboard\r\n0.0235294: 543 drumstick\r\nwalleye:/data/local/tmp $./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 2                                                              \r\nLoaded model mobilenet_v1_1.0_224_quant.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 36.938 ms \r\n0.215686: 543 drumstick\r\n0.129412: 594 harmonica\r\n0.0470588: 700 panpipe\r\n0.0470588: 559 flute\r\n0.0235294: 773 safety pin\r\nwalleye:/data/local/tmp $./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 3                                                              \r\nLoaded model mobilenet_v1_1.0_224_quant.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 32.779 ms \r\n0.592157: 797 ski mask\r\n0.258824: 434 bathing cap\r\n0.0156863: 728 planetarium\r\n0.0117647: 736 poncho\r\n0.00784314: 773 safety pin\r\nwalleye:/data/local/tmp $ ./label_image_new_aarch64  -m mobilenet_v1_1.0_224_quant.tflite  -t 4                                                              <\r\nLoaded model mobilenet_v1_1.0_224_quant.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 25.737 ms \r\n0.823529: 728 planetarium\r\n0.0352941: 669 mosque\r\n0.0156863: 434 bathing cap\r\n0.0156863: 418 balloon\r\n0.0117647: 611 jersey\r\nwalleye:/data/local/tmp $ \r\n```\r\n**Describe the expected behavior**\r\n2, 3, 4 threads (`-t 2, -t 3, -t 4`) are expected to return the same results as single-thread case.\r\n**Code to reproduce the issue**\r\nTFLite `label_image_new_aarch64`: \r\n\r\n```\r\nbazel build --config android_arm --cxxopt=-std=c++11  tensorflow/lite/examples/label_image:label_image  --config monolithic\r\n```\r\n\r\nlabel_image needs input image and labels file. I use\r\ninput_image:  [grace_hoper.bmp](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp)\r\nlabels.txt: from [mobilenet_v1_1.0_224_frozen.tgz](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz \r\n)\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Did you use `implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'` , I  rebuild my project and got incorrect result recently but it works fine a few weeks ago,  I change that to version 1.13.1 and I got correct result but longer inference time, I think that's a bug in new version since it updates every day.", "FYI, I got wrong results only when I use multiThreads", "  @buaadf I built from source code. I think what you got from `org.tensorflow:tensorflow-lite:0.0.0-nightly` is the same problem. That is, source code changes intended to improve performance broke multithreaded results.", "I have the same problem! ", "Reassigning to TFLite.", "Any update? I also encounter the same issue on arm-64 linux. Threads 1, 4, 6 return different results. Single thread returns correct result.\r\n\r\n> ./label_image --labels=labels.txt --image=dog.bmp --tflite_model=mobilenet_quant_v1_224.tflite --threads=1\r\nLoaded model mobilenet_quant_v1_224.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 242.961 ms \r\n0.988235: 209 Labrador retriever\r\n0.00784314: 208 golden retriever\r\n0.00392157: 435 bath towel\r\n0.00392157: 163 beagle\r\n\r\n> ./label_image --labels=labels.txt --image=dog.bmp --tflite_model=mobilenet_quant_v1_224.tflite --threads=4\r\nLoaded model mobilenet_quant_v1_224.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 70.697 ms \r\n0.172549: 751 quilt\r\n0.145098: 895 wardrobe\r\n0.0627451: 521 crib\r\n0.0392157: 877 tub\r\n0.0313726: 670 mosquito net\r\n\r\n> ./label_image --labels=labels.txt --image=dog.bmp --tflite_model=mobilenet_quant_v1_224.tflite --threads=6\r\nLoaded model mobilenet_quant_v1_224.tflite\r\nresolved reporter\r\nINFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\n_average time: 52.457 ms_ \r\n0.392157: 521 crib\r\n0.203922: 670 mosquito net\r\n0.145098: 751 quilt\r\n0.0901961: 832 studio couch\r\n0.0901961: 565 four-poster", "I tried to convert the .pb model as attached in the issue into tflite, but failed due to the ops \"Dequantize\". May I know how was this mobilenet_v1_1.0_224_quant.tflite generated?", "@lu-wang-g the `mobilenet_v1_1.0_224_quant.tflite` I tried is from this [hosted model](http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz). There is no labels in this hosted model. So I have to get it elsewhere. Sorry for confusing you.", "Thank you for reporting the issue and providing information to reproduce it! There is a fix to the bug, and the change has been submitted. Please check it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27200\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27200\">No</a>\n"]}, {"number": 27199, "title": "Building Tensorflow Examples via Bazel on macOS doesn't work. Building for Object Tracking Support.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: macOS 10.13.6\r\n- Mobile device: Not applicable\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: The latest one as of writing\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.23.2\r\n- GCC/Compiler version (if compiling from source): 10.0.0\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build the Tensorflow Examples via Bazel so that I can have the Object Tracking Support for Object Detection in TF-Lite.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n[I used this guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android/app#building-from-source-with-bazel), supplemented with [this guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android#bazel)\r\n\r\nAnd here are the steps I took.\r\n\r\n> cd ~/my/android/directory/\r\n>git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git\r\n\r\nI checked for my bazel version:\r\n> bazel version\r\n> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> Build label: 0.23.2\r\n> Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n> Build time: Mon Mar 11 16:50:03 2019 (1552323003)\r\n> Build timestamp: 1552323003\r\n> Build timestamp as int: 1552323003\r\n\r\n> cd tensorflow\r\n> ./configure\r\n\r\nHere were the output, I just hit the defaults for all of these:\r\n\r\n> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> You have bazel 0.23.2 installed.\r\n> Please specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: \r\n\r\n> Found possible Python library paths:\r\n>   /usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\n> Please input the desired Python library path to use.  Default is [/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n\r\n> Do you wish to build TensorFlow with XLA JIT support? [y/N]: \r\n> No XLA JIT support will be enabled for TensorFlow.\r\n\r\n> Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\n> No OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: \r\n> No ROCm support will be enabled for TensorFlow.\r\n\r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: \r\n> No CUDA support will be enabled for TensorFlow.\r\n\r\n> Do you wish to download a fresh release of clang? (Experimental) [y/N]: \r\n> Clang will not be downloaded.\r\n\r\n> Do you wish to build TensorFlow with MPI support? [y/N]: \r\n> No MPI support will be enabled for TensorFlow.\r\n\r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\n> Not configuring the WORKSPACE for Android builds.\r\n\r\n> Would you like to configure TensorFlow for iOS builds? [y/N]: \r\n> Not configuring TensorFlow for iOS builds.\r\n\r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n> \t--config=mkl         \t# Build with MKL support.\r\n> \t--config=monolithic  \t# Config for mostly static monolithic build.\r\n> \t--config=gdr         \t# Build with GDR support.\r\n> \t--config=verbs       \t# Build with libverbs support.\r\n> \t--config=ngraph      \t# Build with Intel nGraph support.\r\n> \t--config=numa        \t# Build with NUMA support.\r\n> \t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n> \t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n> \t--config=nogcp       \t# Disable GCP support.\r\n> \t--config=nohdfs      \t# Disable HDFS support.\r\n> \t--config=noignite    \t# Disable Apache Ignite support.\r\n> \t--config=nokafka     \t# Disable Apache Kafka support.\r\n> \t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\n> Configuration finished\r\n\r\nThe guide in the 2nd link says:\r\n\r\n> \"The Android entries in <workspace_root>/WORKSPACE must be uncommented with the paths filled in appropriately depending on where you installed the NDK and SDK. Otherwise an error such as: \"The external label '//external:android/sdk' is not bound to anything\" will be reported.\"\r\n\r\nBut I don't see anything on that directory aside from the tensorflow folder that was cloned?\r\n\r\nAnd in the next paragraph, it says:\r\n> \"Also edit the API levels for the SDK in WORKSPACE to the highest level you have installed in your SDK. This must be >= 23 (this is completely independent of the API level of the demo, which is defined in AndroidManifest.xml). The NDK API level may remain at 14.\"\r\n\r\nBut when I checked my WORKSPACE file through nano and cat, I only see this:\r\n\r\n\r\n> workspace(name = \"org_tensorflow\")\r\n> \r\n> load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\", \"http_file\")\r\n> \r\n> http_archive(\r\n>     name = \"io_bazel_rules_closure\",\r\n>     sha256 = \"ddce3b3a3909f99b28b25071c40b7fec7e2e1d1d1a4b2e933f3082aa99517105\",\r\n>     strip_prefix = \"rules_closure-316e6133888bfc39fb860a4f1a31cfcbae485aef\",\r\n>     urls = [\r\n>         \"https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz\",\r\n>         \"https://github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz\",  # 2019-03-21\r\n>     ],\r\n> )\r\n> \r\n> load(\"@io_bazel_rules_closure//closure:defs.bzl\", \"closure_repositories\")\r\n> \r\n> closure_repositories()\r\n> \r\n> load(\"//third_party/toolchains/preconfig/generate:archives.bzl\",\r\n>      \"bazel_toolchains_archive\")\r\n> \r\n> bazel_toolchains_archive()\r\n> \r\n> load(\r\n>     \"@bazel_toolchains//repositories:repositories.bzl\",\r\n>     bazel_toolchains_repositories = \"repositories\",\r\n> )\r\n> \r\n> bazel_toolchains_repositories()\r\n> \r\n> load(\r\n>     \"@io_bazel_rules_docker//repositories:repositories.bzl\",\r\n>     container_repositories = \"repositories\",\r\n> )\r\n> \r\n> container_repositories()\r\n> \r\n> load(\"//third_party/toolchains/preconfig/generate:workspace.bzl\",\r\n>      \"remote_config_workspace\")\r\n> \r\n> remote_config_workspace()\r\n> \r\n> # Apple and Swift rules.\r\n> http_archive(\r\n>     name = \"build_bazel_rules_apple\",\r\n>     sha256 = \"4b90786009fa8df25230442244bad2832ba8d6bc4987f68150a7de59c8827e90\",\r\n>     strip_prefix = \"rules_apple-0.14.0\",\r\n>     urls = [\"https://github.com/bazelbuild/rules_apple/archive/0.14.0.tar.gz\"],\r\n> )\r\n> http_file(\r\n>    name = \"xctestrunner\",\r\n>     executable = 1,\r\n>     urls = [\"https://github.com/google/xctestrunner/releases/download/0.2.6/ios_test_runner.par\"],\r\n> )\r\n> \r\n> http_archive(\r\n>     name = \"bazel_skylib\",\r\n>     sha256 = \"2c62d8cd4ab1e65c08647eb4afe38f51591f43f7f0885e7769832fa137633dcb\",\r\n>     strip_prefix = \"bazel-skylib-0.7.0\",\r\n>     urls = [\"https://github.com/bazelbuild/bazel-skylib/archive/0.7.0.tar.gz\"],\r\n> )\r\n> \r\n> http_archive(\r\n>     name = \"build_bazel_apple_support\",\r\n>     sha256 = \"835663c4bb02f4bf01dce8a2a176df7fa682dbb867d3698ae12258c1628bb8f0\",\r\n>     strip_prefix = \"apple_support-0.5.0\",\r\n>     urls = [\"https://github.com/bazelbuild/apple_support/archive/0.5.0.tar.gz\"],\r\n> )\r\n> \r\n> http_archive(\r\n>     name = \"build_bazel_rules_swift\",\r\n>     sha256 = \"32d124878cd49775d84f59ba90440c8b23b7c775aec8fec1978f751c76ddee8a\",\r\n>     strip_prefix = \"rules_swift-0.7.0\",\r\n>     urls = [\"https://github.com/bazelbuild/rules_swift/archive/0.7.0.tar.gz\"],\r\n> )\r\n> \r\n> http_archive(\r\n>     name = \"com_github_apple_swift_swift_protobuf\",\r\n>     type = \"zip\",\r\n>     strip_prefix = \"swift-protobuf-1.2.0/\",\r\n>     urls = [\"https://github.com/apple/swift-protobuf/archive/1.2.0.zip\"],\r\n> )\r\n> \r\n> # Use swift_rules_dependencies to fetch the tolchains.\r\n> # Since we defined all the \"git_repository\" rules above, the following call will\r\n> # skip redefining them.\r\n> load(\"@build_bazel_rules_swift//swift:repositories.bzl\", \"swift_rules_dependencies\")\r\n> swift_rules_dependencies()\r\n> \r\n> # We must check the bazel version before trying to parse any other BUILD\r\n> # files, in case the parsing of those build files depends on the bazel\r\n> # version we require here.\r\n> load(\"//tensorflow:version_check.bzl\", \"check_bazel_version_at_least\")\r\n> check_bazel_version_at_least(\"0.19.0\")\r\n> \r\n> load(\"//tensorflow:workspace.bzl\", \"tf_workspace\")\r\n> \r\n> load(\"//third_party/android:android_configure.bzl\", \"android_configure\")\r\n> android_configure(name=\"local_config_android\")\r\n> load(\"@local_config_android//:android.bzl\", \"android_workspace\")\r\n> android_workspace()\r\n> \r\n> # Please add all new TensorFlow dependencies in workspace.bzl.\r\n> tf_workspace()\r\n> \r\n> http_archive(\r\n>     name = \"inception_v1\",\r\n>     build_file = \"//:models.BUILD\",\r\n>     sha256 = \"7efe12a8363f09bc24d7b7a450304a15655a57a7751929b2c1593a71183bb105\",\r\n>     urls = [\r\n>         \"http://storage.googleapis.com/download.tensorflow.org/models/inception_v1.zip\",\r\n>         \"http://download.tensorflow.org/models/inception_v1.zip\",\r\n>     ],\r\n> )\r\n> \r\n> http_archive(\r\n>     name = \"mobile_ssd\",\r\n>     build_file = \"//:models.BUILD\",\r\n>     sha256 = \"bddd81ea5c80a97adfac1c9f770e6f55cbafd7cce4d3bbe15fbeb041e6b8f3e8\",\r\n>     urls = [\r\n>         \"http://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip\",\r\n>         \"http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip\",\r\n>     ],\r\n> )\r\n> \r\n> http_archive(\r\n>    name = \"mobile_multibox\",\r\n>     build_file = \"//:models.BUILD\",\r\n>     sha256 = \"859edcddf84dddb974c36c36cfc1f74555148e9c9213dedacf1d6b613ad52b96\",\r\n>     urls = [\r\n>         \"http://storage.googleapis.com/download.tensorflow.org/models/mobile_multibox_v1a.zip\",\r\n>         \"http://download.tensorflow.org/models/mobile_multibox_v1a.zip\",\r\n>     ],\r\n> )\r\n> \r\n> http_archive(\r\n>     name = \"stylize\",\r\n>     build_file = \"//:models.BUILD\",\r\n>     sha256 = \"3d374a730aef330424a356a8d4f04d8a54277c425e274ecb7d9c83aa912c6bfa\",\r\n>     urls = [\r\n>         \"http://storage.googleapis.com/download.tensorflow.org/models/stylize_v1.zip\",\r\n>         \"http://download.tensorflow.org/models/stylize_v1.zip\",\r\n>     ],\r\n> )\r\n> \r\n> http_archive(\r\n>     name = \"speech_commands\",\r\n>     build_file = \"//:models.BUILD\",\r\n>     sha256 = \"c3ec4fea3158eb111f1d932336351edfe8bd515bb6e87aad4f25dbad0a600d0c\",\r\n>     urls = [\r\n>         \"http://storage.googleapis.com/download.tensorflow.org/models/speech_commands_v0.01.zip\",\r\n>         \"http://download.tensorflow.org/models/speech_commands_v0.01.zip\",\r\n>     ],\r\n> )\r\n\r\nThere wasn't anything that resembled `api_levels` or anything similar. \r\n\r\nAnd when I finally hit:\r\n> bazel build -c opt --cxxopt='--std=c++11' --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n>  //tensorflow/lite/examples/android:tflite_demo\r\n\r\nI got the following errors:\r\n\r\n> ERROR: /private/var/tmp/_bazel_angelopvillasanta/70db096b7d05a90f0f37f9f041e057b0/external/local_config_cc/BUILD:45:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\n> ERROR: Analysis of target '//tensorflow/lite/examples/android:tflite_demo' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\n> INFO: Elapsed time: 14.026s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (24 packages loaded, 215 targets configured)\r\n>     Fetching @local_jdk; fetching\r\n\r\nNow I'm not sure what I should do next. I'm not sure how to install C++11 on the macOS, as that seems to be the only dependency I'm not sure I have covered. \r\n\r\nBasically my objective is to build the Tensorflow example in a macOS, because I need to be able to have support of the Object Tracking feature in Object Detection for Tensorflow-Lite.\r\n\r\nIs this possible? Did I miss any steps?\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nThe logs are all above. \r\n", "comments": ["I made this with cmake.\r\n\r\n1. Add this to build gradle\r\n`defaultConfig {\r\n        ndk {\r\n            abiFilters \"${cpuType}\"\r\n        }\r\n        externalNativeBuild {\r\n            cmake {\r\n                cppFlags \"-std=c++11\", \"-Wall\"\r\n                arguments '-DANDROID_TOOLCHAIN=clang', '-DANDROID_STL=c++_static'\r\n            }\r\n        }\r\n    }\r\n    externalNativeBuild {\r\n        cmake {\r\n            path './jni/CMakeLists.txt'\r\n        }\r\n    }`\r\n\r\n2. Copy the jni folder from tensorflow/tensorflow/example/android that contains code for object tracking\r\n\r\n3. Make sure that the paths inside CMakeLists are consistent with your environment.\r\n\r\nNow you can have object tracking right out of Android Studio without having to deal with complicated Bazel build.", "> I made this with cmake.\r\n> \r\n> 1. Add this to build gradle\r\n>    `defaultConfig { ndk { abiFilters \"${cpuType}\" } externalNativeBuild { cmake { cppFlags \"-std=c++11\", \"-Wall\" arguments '-DANDROID_TOOLCHAIN=clang', '-DANDROID_STL=c++_static' } } } externalNativeBuild { cmake { path './jni/CMakeLists.txt' } }`\r\n> 2. Copy the jni folder from tensorflow/tensorflow/example/android that contains code for object tracking\r\n> 3. Make sure that the paths inside CMakeLists are consistent with your environment.\r\n> \r\n> Now you can have object tracking right out of Android Studio without having to deal with complicated Bazel build.\r\n\r\nDid you have to download the CMake module via Android Studio's SDK Manager Tool?\r\n\r\n![Screen Shot 2019-03-30 at 6 17 03 PM](https://user-images.githubusercontent.com/1715673/55274839-0b92c780-5318-11e9-833e-2c2544690245.png)\r\n ", "> > I made this with cmake.\r\n> > \r\n> > 1. Add this to build gradle\r\n> >    `defaultConfig { ndk { abiFilters \"${cpuType}\" } externalNativeBuild { cmake { cppFlags \"-std=c++11\", \"-Wall\" arguments '-DANDROID_TOOLCHAIN=clang', '-DANDROID_STL=c++_static' } } } externalNativeBuild { cmake { path './jni/CMakeLists.txt' } }`\r\n> > 2. Copy the jni folder from tensorflow/tensorflow/example/android that contains code for object tracking\r\n> > 3. Make sure that the paths inside CMakeLists are consistent with your environment.\r\n> > \r\n> > Now you can have object tracking right out of Android Studio without having to deal with complicated Bazel build.\r\n> \r\n> Did you have to download the CMake module via Android Studio's SDK Manager Tool?\r\n> \r\n> ![Screen Shot 2019-03-30 at 6 17 03 PM](https://user-images.githubusercontent.com/1715673/55274839-0b92c780-5318-11e9-833e-2c2544690245.png)\r\n\r\nYes I did.", "> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n> Not configuring the WORKSPACE for Android builds.\r\n\r\nWhen you run ./configure you need to enable it for Android builds.\r\n", "@Zeit42 Please close the issue if the issue was resolved. Thanks!", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken."]}, {"number": 27198, "title": "Error importing tensorflow, tensorflow library was compiled to use AVX instructions, but these aren;t in your machine", "body": "\r\n\r\n**System information**\r\n-  Linux Ubuntu 16.04\r\n- \r\n- TensorFlow installed from binary (pip install)\r\n- TensorFlow version:\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: pip and virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Problem described** \r\n\r\ni was following the tutorial for using intel neural stick 2 for object detection https://towardsdatascience.com/speed-up-predictions-on-low-power-devices-using-neural-compute-stick-and-openvino-98f3ae9dcf41\r\nin the  example i install the prerequisites using the command \r\n`sudo ./opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh`\r\n\r\ntensorflow was installed with the prerequisites , i also installed tensorflow using pip install , but when i run the next command \r\n\r\n`mo_tf.py \\\r\n    --input_model ~/Downloads/ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb \\\r\n    --tensorflow_use_custom_operations_config     /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/extensions/front/tf/ssd_support.json \\\r\n    --tensorflow_object_detection_api_pipeline_config ~/Downloads/ssd_mobilenet_v1_coco_2018_01_28/pipeline.config \\\r\n    --data_type FP16`\r\n\r\n\r\ni get the following error \r\n`F tensorflow/core/platform/cpu_feature_guard.cc:37] \r\nThe tensorflow library was compiled to use AVX instructions, but these aren't available in your machine \r\nAborted (core dumped) ` \r\n\r\ni am getting the same error when try and **import tensorflow** \r\n\r\nwhat should i do to solve this error ? \r\n\r\n\r\n\r\n", "comments": ["Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue like CUDA and cuDNN versions. It would be great if you can provide a small code to reproduce the error. Thanks!", "Hi. Found somewhere that I needed to install tensorflow 1.5 and the issue\nwould be resolved. And that's what happened. I had tensorflow 1.13 which\nisn't compatible\n\nOn Fri, Mar 29, 2019, 6:26 PM muddham <notifications@github.com> wrote:\n\n> Please fill the issue template\n> <https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md>.\n> Could you update them if they are relevant in your case, or leave them as\n> N/A? Along with the template, please provide as many details as possible to\n> find the root cause of the issue like CUDA and cuDNN versions. It would be\n> great if you can provide a small code to reproduce the error. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27198#issuecomment-477995464>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AuRma_-pn6b2xnFlgmNSGRsiZUBTL0k9ks5vbhSagaJpZM4cN3_u>\n> .\n>\n", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27198\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27198\">No</a>\n", "Try Anaconda. It has new TensorFlow distribution for old CPUs.", "@foobar167 : I didn't see such a distribution. Perhaps you could link to it?", "@r-barnes : There is [Anaconda Python](https://www.anaconda.com/distribution/) distribution which is more flexible sometimes, than Python distribution. For example, I installed TensorFlow on the old CPU using **Anaconda Python**. Meanwhile Python refused to work (old CPU, CUDA not installed, wrong version of cuDNN, etc.).\r\n\r\nAnaconda Python has better automatic software package installer and versions tracker. So in my opinion, it is a little bit easier for beginner to deploy a **virtual environment**, install some packages like TensorFlow.\r\n\r\nIf you're beginner try the following software:\r\n   * Anaconda Python instead of Python\r\n   * PyCharm Community edition as an integrated development environment (IDE)\r\n   * try Jupyter Notebook for your software templates\r\n   * [Google Colab](https://colab.research.google.com/) or [Kaggle Kernels](https://www.kaggle.com/kernels) or some other [Cloud Jupyter Notebooks](https://www.dataschool.io/cloud-services-for-jupyter-notebook/), especially if you have an old CPU and old GPU\r\n   * Review my links on Machine Learning [here](https://github.com/foobar167/articles/blob/master/Machine_Learning/courses_on_machine_learning.md) and [here](https://github.com/foobar167/articles/blob/master/Ubuntu/13_Keras_and_TensorFlow_how-tos.md).\r\n"]}, {"number": 27197, "title": "pr_curve gives error regardless of label type", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I'm Trying to use pr_curve with tensorboard, and in one case I get an error telling me by labels need to be of type bool.  If I change it to a bool, I get an error from deep within the tf code that the type need to be float.  Everything else is working great.\r\n- Linux Ubuntu 16.04\r\n- Source compiled for CUDA 10 and CDNN 7.4.2\r\n- TensorFlow version: 1.13.0-dev20190218\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10/7.4.2\r\n- GPU model and memory: 1070ti with 4GB\r\n\r\n\r\nI'm trying to use pr_curve with tensorboard, and when I issue the pr_curve command I get an error telling me the labels (parameter 3) are supposed to by of type bool.  If I set it to a bool, the command runs, but later from deep within tf code errors with parameter is to be of type float, which is was originally.\r\n\r\nIf I remove the pr_curve command, I no longer get any errors.\r\n\r\n**Code to reproduce the issue**  Problem is line 109 or 110.\r\n\r\n    from __future__ import absolute_import\r\n    from __future__ import division\r\n    from __future__ import print_function\r\n    import pdb \r\n    import numpy as np\r\n    import math, time\r\n    import tensorflow as tf\r\n    from tensorboard import summary as summ_lib\r\n    import flags as f\r\n    import input, lenet5, evaluate\r\n    \r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    similar results every run)\r\n    tf.set_random_seed(1)\r\n    np.random.seed(10)\r\n    \r\n    summaries = {'train': [], 'validate': [], 'test': []}\r\n    \r\n    def main(unused_argv):\r\n         train_dataset, validate_dataset, test_dataset = \r\n    input.input(shuffle_files=False)\r\n         info = tf.constant(\r\n                [\"Batch size = %s\" % f.FLAGS.batch_size,\r\n                 \"Epochs = %s\" % f.FLAGS.num_epochs,\r\n                 \"Learning rate = %s\" % f.FLAGS.learning_rate,\r\n                 \"Batch normalization = No\",\r\n                 \"Window size = %s\" % f.FLAGS.window_size,\r\n                 \"Shuffle Files = No\",\r\n                 \"CNN model = %s\" % f.FLAGS.cnn_model,\r\n                 \"Shuffle Samples = YES\"]\r\n          )\r\n          with tf.name_scope('input'):\r\n                x = tf.placeholder(tf.float32, [None, input.SAMPLE_DEPTH, \r\n    input.SAMPLE_HEIGHT, input.SAMPLE_WIDTH])\r\n                y_ = tf.placeholder(tf.float32, [None, 2])\r\n                dropout_rate = tf.placeholder(tf.float32)\r\n                is_training = tf.placeholder(tf.bool)\r\n    \r\n          with tf.name_scope('logits'):\r\n                if f.FLAGS.cnn_model == \"lenet5\":\r\n                      logits = lenet5.model_fn(sample_input = x, \r\n    is_training=is_training, summaries=summaries)\r\n    \r\n          with tf.name_scope('loss'):\r\n                cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logits)\r\n                mean_cross_entropy_loss = tf.reduce_mean(cross_entropy)\r\n    \r\n                loss_summ = tf.summary.scalar('Mean_cross_entropy_loss', mean_cross_entropy_loss)\r\n                summaries['train'].append(loss_summ)\r\n    \r\n          with tf.name_scope('adam_optimizer'):\r\n                optimizer = tf.train.AdamOptimizer(f.FLAGS.learning_rate).minimize(mean_cross_entropy_loss)\r\n    \r\n          with tf.name_scope('accuracy'):\r\n                preds = tf.argmax(logits, 1)\r\n                correct_preds = tf.argmax(y_, 1)\r\n                equal = tf.equal(preds, correct_preds)\r\n                training_accuracy_op = tf.reduce_mean(tf.cast(equal, tf.float32))\r\n    \r\n    summaries['train'].append(tf.summary.scalar('Training_Accuracy', training_accuracy_op))\r\n    \r\n    \r\n          with tf.name_scope('Evaluation_Metrics'):\r\n                tp_op = evaluate.tp(logits=logits, labels=y_)\r\n                fp_op = evaluate.fp(logits=logits, labels=y_)\r\n                tn_op = evaluate.tn(logits=logits, labels=y_)\r\n                fn_op = evaluate.fn(logits=logits, labels=y_)\r\n    \r\n                tp_sum = tf.placeholder(tf.float32)\r\n                tn_sum = tf.placeholder(tf.float32)\r\n                fp_sum = tf.placeholder(tf.float32)\r\n                fn_sum = tf.placeholder(tf.float32)\r\n    \r\n                precision_op = evaluate.precision(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)\r\n                accuracy_op = evaluate.accuracy(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)\r\n                recall_op = evaluate.recall(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)\r\n                fscore_op = evaluate.fscore(tp=tp_sum, fp=fp_sum, tn=tn_sum, fn=fn_sum)\r\n    \r\n                precision_summ = tf.summary.scalar('Precision', precision_op)\r\n                accuracy_summ = tf.summary.scalar('Accuracy', accuracy_op)\r\n                recall_summ = tf.summary.scalar('Recall', recall_op)\r\n                fscore_summ = tf.summary.scalar('Fscore', fscore_op)\r\n    \r\n                summaries['validate'].append(accuracy_summ)\r\n                summaries['validate'].append(precision_summ)\r\n                summaries['validate'].append(recall_summ)\r\n                summaries['validate'].append(fscore_summ)\r\n    \r\n                summaries['test'].append(accuracy_summ)\r\n                summaries['test'].append(precision_summ)\r\n                summaries['test'].append(recall_summ)\r\n                summaries['test'].append(fscore_summ)\r\n    \r\n          print (\"Saving graph to %s\" % f.FLAGS.log_dir)\r\n          mAP_summ = summ_lib.pr_curve(name='mAP',predictions=logits,labels=y_,num_thresholds=11)\r\n          # OR THIS ONE mAP_summ = summ_lib.pr_curve(name='mAP',predictions=logits,labels=tf.cast(y_,tf.bool),num_thresholds=11)\r\n    \r\n          train_writer = tf.summary.FileWriter(f.FLAGS.log_dir + \"/train\")\r\n          validate_writer = tf.summary.FileWriter(f.FLAGS.log_dir + \"/validate\")\r\n          test_writer = tf.summary.FileWriter(f.FLAGS.log_dir + \"/test\")\r\n          train_writer.add_graph(tf.get_default_graph())\r\n    \r\n          train_summaries = tf.summary.merge(summaries['train'])\r\n          validate_summaries = tf.summary.merge(summaries['validate'])\r\n          test_summaries = tf.summary.merge(summaries['test'])\r\n    \r\n          with tf.Session() as sess:\r\n                train_writer.add_summary(sess.run(tf.summary.text(\"Information\", info)))\r\n                train_iter = train_dataset.make_initializable_iterator()\r\n                train_next_elem = train_iter.get_next()\r\n                sess.run(tf.global_variables_initializer())\r\n                global_step = 0\r\n                display_freq = 10\r\n                validate_freq = 50\r\n                test_freq = 50\r\n                for epoch in range(1, f.FLAGS.num_epochs+1):\r\n                      sess.run(train_iter.initializer)\r\n                      step_time = 0.0\r\n                      fetch_time = 0.0\r\n                      while True:\r\n                            try:\r\n                                  a = time.time()\r\n                                  global_step += 1\r\n                                  #print(make_bread())\r\n                                  sample, label = sess.run(train_next_elem)\r\n                                  fetch_time += time.time() - a\r\n                                  a = time.time()\r\n                                  _, summ = sess.run([optimizer, train_summaries], feed_dict={x: sample, y_: label, dropout_rate: 0.5, is_training: True})\r\n                                  train_writer.add_summary(summ, global_step)\r\n                                  step_time += time.time() - a\r\n                            except tf.errors.OutOfRangeError:\r\n                                  break\r\n    \r\n                            if global_step % display_freq == 0:\r\n                                  batch_loss, batch_accuracy = sess.run([mean_cross_entropy_loss, training_accuracy_op],\r\n                                                                        feed_dict={x: sample, y_: label, dropout_rate: 1.0, is_training: False})\r\n                                  print (\"Epoch {:3}\\t Step {:5}:\\t Loss={:.3f}, \\tTraining Accuracy={:.5f} \\tStep Time {:4.2f}m, Fetch Time {:4.2f}m\".\r\n                                         format(epoch, global_step, batch_loss, batch_accuracy, step_time/60, fetch_time/60))\r\n                                  step_time = 0.0\r\n                                  fetch_time = 0.0\r\n    \r\n    \r\n                      #Validate and test after each epoch\r\n                      val_it = validate_dataset.make_one_shot_iterator()\r\n                      val_next_elem = val_it.get_next()\r\n                      tot_tp, tot_tn, tot_fp, tot_fn = 0, 0, 0, 0\r\n                      while True:\r\n                            try:\r\n                                  sample, label = sess.run(val_next_elem)\r\n                                  tp, fp, tn, fn = sess.run([tp_op, fp_op, tn_op, fn_op],\r\n                                                            feed_dict={x: sample, y_: label, dropout_rate: 1.0, is_training: False})\r\n                            except tf.errors.OutOfRangeError:\r\n                                  break\r\n                            tot_tp += tp\r\n                            tot_fp += fp\r\n                            tot_fn += fn\r\n                            tot_tn += tn\r\n                      precision, recall, accuracy, fscore, summ = sess.run([precision_op, recall_op, accuracy_op, fscore_op, validate_summaries],\r\n                                                                           feed_dict={tp_sum: tot_tp, tn_sum: tot_tn, fp_sum: tot_fp, fn_sum: tot_fn})\r\n                      validate_writer.add_summary(summ, global_step)\r\n                      print (\"Epoch %d, Step %d\" % (epoch, global_step))\r\n                      print (\"=\"*10, \"Validating Results\", \"=\"*10)\r\n                      print (\"TP: %g\\nTN: %g\\nFP: %g\\nFN: %g\" % (tot_tp, tot_tn, tot_fp, tot_fn))\r\n                      print (\"\\tPrecision: %g\\n\\tRecall: %g\\n\\tF1_score: %g\\n\\tAccuracy: %g\" % (precision, recall, fscore, accuracy))\r\n    \r\n    \r\n                      test_it = test_dataset.make_one_shot_iterator()\r\n                      test_next_elem = test_it.get_next()\r\n                      tot_tp, tot_tn, tot_fp, tot_tn = 0, 0, 0, 0\r\n                      while True:\r\n                            try:\r\n                                  sample, label = sess.run(test_next_elem)\r\n                                  tp, fp, tn, fn, pred, lab = sess.run([tp_op, fp_op, tn_op, fn_op, logits, y_],\r\n                                                            feed_dict={x: sample, y_: label, dropout_rate: 1.0, is_training: False})\r\n                            except tf.errors.OutOfRangeError:\r\n                                  break\r\n                            tot_tp += tp\r\n                            tot_fp += fp\r\n                            tot_fn += fn\r\n                            tot_tn += tn\r\n                      precision, recall, accuracy, fscore, summ = sess.run([precision_op, recall_op, accuracy_op, fscore_op, test_summaries],\r\n                                                                           feed_dict={tp_sum: tot_tp, tn_sum: tot_tn, fp_sum: tot_fp, fn_sum: tot_fn})\r\n    \r\n                      test_writer.add_summary(summ, global_step)\r\n    \r\n                      print (\"=\"*10, \"Testing Results\", \"=\"*10)\r\n                      print (\"TP: %g\\nTN: %g\\nFP: %g\\nFN: %g\" % (tot_tp, tot_tn, tot_fp, tot_fn))\r\n                      print (\"\\tPrecision: %g\\n\\tRecall: %g\\n\\tF1_score: %g\\n\\tAccuracy: %g\" % (precision, recall, fscore, accuracy))\r\n                      print (\"=\"*10, \"===============\", \"=\"*10)\r\n                map_writer = tf.summary.FileWriter(f.FLAGS.log_dir + \"/mAP\")\r\n                map_writer.add_summary(sess.run(mAP_summ),global_step=0)\r\n                map_writer.close()\r\n    \r\n    if __name__ == \"__main__\":\r\n          tf.app.run()\r\n\r\n\r\n**Other info / logs**  Here are the errors depending on the pr_curve command used around line 110.\r\n\r\n\r\n    10:06:00:26.03.2019:/media/randy/Data1/Python/Machine_Learning/3d_cnn ./run2.sh\r\n    Number of training files:  68\r\n    Number of validation files:  23\r\n    Number of testing files:  23\r\n    W0326 22:06:28.115116 139978789848896 deprecation.py:506] From /home/randy/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n    Instructions for updating:\r\n    Call initializer instance with the dtype argument instead of passing it to the constructor\r\n    Saving graph to /media/randy/Data1/log/proc_order/ABS-Correlated\r\n    Traceback (most recent call last):\r\n      File \"cnn_model.py\", line 208, in <module>\r\n        tf.app.run()\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n        _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n        _run_main(main, args)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n        sys.exit(main(argv))\r\n      File \"cnn_model.py\", line 108, in main\r\n        mAP_summ = summ_lib.pr_curve(name='mAP',predictions=logits,labels=y_,num_thresholds=11)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorboard/plugins/pr_curve/summary.py\", line 94, in op\r\n        tf.compat.v1.assert_type(labels, tf.bool)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py\", line 1502, in assert_type\r\n        tf_type))\r\n    TypeError:   input/Placeholder_1:0 must be of type <dtype: 'bool'>\r\n    \r\n    \r\n    \r\n    \r\n    \r\n    10:06:28:26.03.2019:/media/randy/Data1/Python/Machine_Learning/3d_cnn ./run2.sh\r\n    Number of training files:  68\r\n    Number of validation files:  23\r\n    Number of testing files:  23\r\n    W0326 22:07:04.819849 140201659422528 deprecation.py:506] From /home/randy/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py:187: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\n    Instructions for updating:\r\n    Call initializer instance with the dtype argument instead of passing it to the constructor\r\n    Saving graph to /media/randy/Data1/log/proc_order/ABS-Correlated\r\n    2019-03-26 22:07:04.963555: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n    2019-03-26 22:07:04.966910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcuda.so.1\r\n    2019-03-26 22:07:05.054544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1010] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-03-26 22:07:05.054971: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x4dd1b80 executing computations on platform CUDA. Devices:\r\n    2019-03-26 22:07:05.054985: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX 1070 Ti, Compute Capability 6.1\r\n    2019-03-26 22:07:05.072833: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\r\n    2019-03-26 22:07:05.073556: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x4d5bc70 executing computations on platform Host. Devices:\r\n    2019-03-26 22:07:05.073585: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n    2019-03-26 22:07:05.073788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Found device 0 with properties:\r\n    name: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\n    pciBusID: 0000:01:00.0\r\n    totalMemory: 7.93GiB freeMemory: 7.53GiB\r\n    2019-03-26 22:07:05.073797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1543] Adding visible gpu devices: 0\r\n    2019-03-26 22:07:05.073837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudart.so.10.0\r\n    2019-03-26 22:07:05.074339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n    2019-03-26 22:07:05.074346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0\r\n    2019-03-26 22:07:05.074349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N\r\n    2019-03-26 22:07:05.074521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1146] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7324 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n    W0326 22:07:05.090326 140201659422528 deprecation.py:323] From cnn_model.py:122: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n    Instructions for updating:\r\n    Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\r\n    2019-03-26 22:07:06.920871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcublas.so.10.0\r\n    2019-03-26 22:07:07.021886: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudnn.so.7\r\n    Epoch   1        Step    10:     Loss=0.723,    Training Accuracy=0.75977       Step Time 0.57m, Fetch Time 0.01m\r\n    Epoch   1        Step    20:     Loss=0.528,    Training Accuracy=0.80859       Step Time 0.55m, Fetch Time 0.00m\r\n    Epoch   1        Step    30:     Loss=0.399,    Training Accuracy=0.86523       Step Time 0.55m, Fetch Time 0.00m\r\n    W0326 22:09:01.058223 140201659422528 deprecation.py:323] From cnn_model.py:157: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\n    Instructions for updating:\r\n    Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\r\n    Epoch 1, Step 35\r\n    ========== Validating Results ==========\r\n    TP: 980\r\n    TN: 3093\r\n    FP: 302\r\n    FN: 1257\r\n            Precision: 0.764431\r\n            Recall: 0.438087\r\n            F1_score: 0.556976\r\n            Accuracy: 0.723189\r\n    ========== Testing Results ==========\r\n    TP: 1124\r\n    TN: 3500\r\n    FP: 272\r\n    FN: 2505\r\n            Precision: 0.805158\r\n            Recall: 0.309727\r\n            F1_score: 0.447363\r\n            Accuracy: 0.62478\r\n    ========== =============== ==========\r\n    Traceback (most recent call last):\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n        return fn(*args)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1320, in _run_fn\r\n        options, feed_dict, fetch_list, target_list, run_metadata)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1408, in _call_tf_sessionrun\r\n        run_metadata)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input/Placeholder_1' with dtype float and shape [?,2]\r\n             [[{{node input/Placeholder_1}}]]\r\n             [[mAP/stack/_199]]\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"cnn_model.py\", line 208, in <module>\r\n        tf.app.run()\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n        _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n        _run_main(main, args)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n        sys.exit(main(argv))\r\n      File \"cnn_model.py\", line 204, in main\r\n        map_writer.add_summary(sess.run(mAP_summ),global_step=0)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 930, in run\r\n        run_metadata_ptr)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1153, in _run\r\n        feed_dict_tensor, options, run_metadata)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _do_run\r\n        run_metadata)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1349, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input/Placeholder_1' with dtype float and shape [?,2]\r\n             [[node input/Placeholder_1 (defined at cnn_model.py:50) ]]\r\n             [[mAP/stack/_199]]\r\n    \r\n    Original stack trace for 'input/Placeholder_1':\r\n      File \"cnn_model.py\", line 208, in <module>\r\n        tf.app.run()\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n        _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n        _run_main(main, args)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n        sys.exit(main(argv))\r\n      File \"cnn_model.py\", line 50, in main\r\n        y_ = tf.placeholder(tf.float32, [None, 2])\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2084, in placeholder\r\n        return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6098, in placeholder\r\n        \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 800, in _apply_op_helper\r\n        op_def=op_def)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n        return func(*args, **kwargs)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3473, in create_op\r\n        op_def=op_def)\r\n      File \"/home/randy/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1961, in __init__\r\n        self._traceback = tf_stack.extract_stack()\r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n", "comments": ["Hi @rklepetko! Thanks for the report. I can\u2019t reproduce the problem with\r\nour unit tests or end-to-end demos, with any versions of TF and Python.\r\n\r\nI can\u2019t run your example, because it relies on external libraries that\r\nwe don\u2019t have access to (`input`, `lenet5`, `evaluate`, \u2026). But, reading\r\nthe code and the error message, it appears that the error is unrelated.\r\nThe problem is that your `sess.run(mAP_summ)` command near the very\r\nbottom of the program does not feed a value for the `y_` placeholder,\r\neven though `mAP_summ` depends on `y_`.\r\n\r\nIf you\u2019re still experiencing this problem, please provide a minimal,\r\nself-contained example to reproduce the problem.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27197\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27197\">No</a>\n"]}, {"number": 27196, "title": "Failed to load the native TensorFlow runtime and ImportError: No module named '_pywrap_tensorflow_internal'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: python -m pip install --upgrade https://storage.googleapis2.0-py3-none-any.whl\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nRecently Installed TensorFlow using  >>>python -m pip install --upgrade https://storage.goo2.0-py3-none-any.whl\r\n\r\nError:\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n>>> import numpy\r\n>>> import tensorflow as tf\r\n\r\nErrors:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nInstallation Logs:\r\n\r\nCollecting tensorflow==1.12.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none\r\n  Downloading https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl (62.0MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.0MB 168kB/s\r\nCollecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b (3.0MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 1.6MB/s\r\nCollecting protobuf>=3.6.1 (from tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/8c/bf/e7f80b9d8e45d90ba411d5851c1154ff25b35ebb4c5728eb340 (865kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 870kB 1.0MB/s\r\nRequirement already satisfied, skipping upgrade: six>=1.10.0 in c:\\users\\ashwani\\appdata\\local\\programs\\pythonow==1.12.0) (1.12.0)\r\nCollecting keras-applications>=1.0.6 (from tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710ne-any.whl (51kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 1.9MB/s\r\nCollecting absl-py>=0.1.6 (from tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc86947\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 2.5MB/s\r\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\ashwani\\appdata\\local\\programs\\pythflow==1.12.0) (1.16.2)\r\nCollecting termcolor>=1.1.0 (from tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab4617\r\nCollecting wheel>=0.26 (from tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa30771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl\r\nCollecting grpcio>=1.8.6 (from tensorflow==1.12.0)                                              88e8f7b4b033953\r\n  Downloading https://files.pythonhosted.org/packages/4f/51/5e14559ceae1826a33158c48cec067b43dfe88e8f7b4b033953a004230b9/grpcio-1.19.0-cp37-cp37m-win32.whl (1.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB 1.3MB/s\r\nCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.12.0)                                 73f2c31ac719ab2  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 5.1MB/s\r\nCollecting astor>=0.6.0 (from tensorflow==1.12.0)                                               242d0b9f59534ef\r\n  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl                                       c62b9de38ac4a4a\r\nCollecting gast>=0.2.0 (from tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a21/gast-0.2.2.tar.gz\r\nCollecting werkzeug>=0.11.10 (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/24/4d/2fc4e872fbaaf44cc3fd5a9cd42fda7e57c031f08e28c9f356898/Werkzeug-0.15.1-py2.py3-none-any.whl (328kB)    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 337kB 2.2MB/s\r\nCollecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79aaf/Markdown-3.1-py2.py3-none-any.whl (87kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 5.4MB/s\r\nRequirement already satisfied, skipping upgrade: setuptools in c:\\users\\ashwani\\appdata\\local\\programs\\python\\7-32\\lib\\site-packages (from protobuf>=3.6.1->tensorflow==1.12.0) (40.6.2)Collecting h5py (from keras-applications>=1.0.6->tensorflow==1.12.0)\r\n  Downloading https://files.pythonhosted.org/packages/e2/3e/76c3d0b25fee11ab73049475ca1833229fbf6e6a6bb56a8bab8f2/h5py-2.9.0-cp37-cp37m-win32.whl (2.1MB)    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.1MB 2.3MB/s\r\nInstalling collected packages: protobuf, werkzeug, grpcio, markdown, wheel, tensorboard, h5py, keras-applicatioications, absl-py, termcolor, keras-preprocessing, astor, gast, tensorflow  \r\n\r\nThe script markdown_py.exe is installed in 'C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\Scrip\\Scripts' which is not on PATH.                                                                          -locat  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.  \r\n\r\nThe script wheel.exe is installed in 'C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts' whichh is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.  \r\n\r\nThe script tensorboard.exe is installed in 'C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts'' which is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.  \r\n\r\nRunning setup.py install for absl-py ... done  \r\nRunning setup.py install for termcolor ... done\r\nRunning setup.py install for gast ... done\r\n\r\n  The scripts freeze_graph.exe, saved_model_cli.exe, tensorboard.exe, tflite_convert.exe, toco.exe and toco_from_prprotos.exe are installed in 'C:\\Users\\Ashwani\\AppData\\Local\\Programs\\Python\\Python37-32\\Scripts' which is not onAT PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.n.\r\n\r\nSuccessfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 kerasas-preprocessing-1.0.9 markdown-3.1 protobuf-3.7.1 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0 werkzeug-150.15.1 wheel-0.33.1\r\n\r\n\r\n\r\n", "comments": ["TF 1.12 does not support python 3.7 you have switch to python 3.6 or lower.\r\nHowever TF 1.13 and above support python 3.7 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27196\">No</a>\n"]}, {"number": 27195, "title": "[TF2.0alpha] Eagerness/Distribute Strategies performance issues", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04/18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190327\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: TITAN X 12 GB\r\n\r\n**Describe the current behavior**\r\n\r\nI have some code, where to the best of my knowledge not everything can be wrapped with the tf.function. Thus, in the function calling strategy.experimental_run I removed the `@tf.function` wrapper. However, this tremendously decreases the overall performance and falls way behind the 'normal' eager execution. \r\n\r\nFor the toy example in this issue, these are the performance differences:\r\n\r\n```\r\nComputing for 1000-steps, no strategy: Time: 7.228003740310669 sec\r\nComputing for 1000-steps, strategy with tf-function: Time: 2.6209354400634766\r\nComputing for 1000-steps, strategy with no tf-function: Time: 63.60745906829834\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect that without tf.function at least the usual performance of eagerness is achieved.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport time\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nclass DebugModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super(DebugModel, self).__init__()\r\n    self.dense = tf.keras.layers.Dense(20, activation='relu', input_shape=(5,))\r\n\r\n  def __call__(self, input, *args, **kwargs):\r\n    x = self.dense(input)\r\n    return x\r\n\r\n\r\ndef train_step(input):\r\n  with tf.GradientTape() as tape:\r\n    out = my_model(input)\r\n    my_loss = -tf.reduce_mean(out)\r\n  gradients = tape.gradient(my_loss, my_model.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\r\n  return my_loss\r\n\r\n\r\ndef distributed_train():\r\n  return strategy.experimental_run(train_step, train_iterator)\r\n\r\n@tf.function\r\ndef distributed_train_tf_function():\r\n  return strategy.experimental_run(train_step, train_iterator)\r\n\r\n\r\nif __name__ == '__main__':\r\n  dataset = tf.data.Dataset.from_tensor_slices(\r\n    (tf.random.uniform([3000, 16, 5])))\r\n  N_steps = 1000\r\n\r\n  # Plain computation\r\n  my_model = DebugModel()\r\n  optimizer = tf.keras.optimizers.Adam(1e-2)\r\n  _s = time.time()\r\n  i = 0\r\n  for data in dataset:\r\n    if i >= N_steps:\r\n      break\r\n    train_step(data)\r\n    i += 1\r\n  print('Computing for {}-steps, no strategy: Time: {} sec'.format(\r\n      N_steps, time.time() - _s))\r\n\r\n\r\n  # Mirrored strategy with @tf.function\r\n  dataset = tf.data.Dataset.from_tensor_slices(\r\n    (tf.random.uniform([3000, 16, 5])))\r\n  strategy = tf.distribute.MirroredStrategy()\r\n\r\n  with strategy.scope():\r\n    train_iterator = strategy.make_dataset_iterator(dataset)\r\n    train_iterator.initialize()\r\n    optimizer = tf.keras.optimizers.Adam(1e-2)\r\n    my_model = DebugModel()\r\n\r\n    _s = time.time()\r\n    for s in range(N_steps):\r\n      distributed_train_tf_function()\r\n    print('Computing for {}-steps, strategy with tf-function: Time: {}'.format(\r\n      N_steps, time.time() - _s))\r\n\r\n\r\n  # Mirrored strategy without @tf.function decorator\r\n  dataset = tf.data.Dataset.from_tensor_slices(\r\n    (tf.random.uniform([3000, 16, 5])))\r\n  strategy = tf.distribute.MirroredStrategy()\r\n  with strategy.scope():\r\n    train_iterator = strategy.make_dataset_iterator(dataset)\r\n    train_iterator.initialize()\r\n    optimizer = tf.keras.optimizers.Adam(1e-2)\r\n    my_model = DebugModel()\r\n\r\n    _s = time.time()\r\n    for s in range(N_steps):\r\n      distributed_train()\r\n    print('Computing for {}-steps, strategy with no tf-function: Time: {}'.format(\r\n      N_steps,\r\n      time.time() - _s))\r\n```\r\n", "comments": ["@goldiegadde @dynamicwebpaige Would be nice to see some progress on this issue", "Assigning to @alextp for triage.\r\n\r\nI might be wrong, but at a glance it might be a caching issue. You might want to wrap `train_step` into a `tf.function`.", "@alextp can you please guide me, I want to work on this issue.", "@shashvatshahi1998 the first step is to try to reproduce the issue, and then try to follow @mdanatg 's advice to wrap train_step in tf.function and see whether the issue goes away.\r\n\r\nIf it doesn't, running a profiler like python's cprofiler on the code to find out what is happening and triaging that to a separate issue is the next step.", "Wrapping train_step in tf.function does not work. It raises the following error: ```RuntimeError: `merge_call` called while defining a new graph. [...]```. \r\n", "@guptapriya is this a known issue internally?", "There are couple of things mentioned here, let me clarify:\r\n1. Performance without tf.function when using distribution strategy is terrible - this is known issue and we do not recommend doing this right now, and it is not something we are prioritizing yet either. The recommended approach is to wrap experimental_run inside a tf.function. Fixing this requires a significant amount of work inside MirroredStrategy. \r\n\r\n2. Wrapping just the train_step into a tf.function: The error seen there is also known, and I am working on fixing that. However, that will not address the performance issue from #1. For anyone who can wrap their train_step in a tf.function, they should be able to wrap strategy.experimental_run inside the tf.function as well - AND get the performance benefit. So there is not a great reason yet to wrap just the train_step into a tf.function but not strategy.run. So while I am working on fixing that issue, it will still not help with performance.\r\n\r\nHope this helps. Please let me know if there are more questions. \r\n", "@guptapriya Regarding 1.), this is rather confusing to me. My main problem causing this issue was my dependance on eager execution during the forward/backward pass. \r\nHowever, wrapping my experimental_run with tf.function straps me from the eager execution. The new TF policy should not be eager-execution is the default except you want to use multiple GPUs for training, should it?", "@meyerjo Ideally eager execution is available and performant for all use cases on day 1, but we have limited resources and have to prioritize what gets done first and then what gets fast first. I agree with @guptapriya's priorities outlined above as being the best to move the existing ecosystem forward, and I'm sure that once performance is good for the tf.function use cases we'll start to improve it for pure-eager multi-gpu.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27195\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27195\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 27194, "title": "Replaced get_shape() to shape.", "body": "This is the recommended method.", "comments": ["@omalleyt12 , thanks for the review, i have updated the code as per your suggestion. Kindly check and approve.\r\n\r\nRegards\r\nAmit"]}, {"number": 27193, "title": "Non-uniform FFT layer", "body": "**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes (but I only know how to code in Python)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHave a layer similar to the fft layer that will implement the non-uniform FFT.\r\n\r\n**Will this change the current api? How?** This will allow to have one more option when using the signal module, `nfft`.\r\n\r\n**Who will benefit with this feature?** People working with neural networks involving Non-uniform FFT. The main application I know of is MRI, but according to the [Wikipedia page](https://en.wikipedia.org/wiki/Non-uniform_discrete_Fourier_transform) there may be more applications. In particular, I think computed tomography can make use of it to accelerate acquisition times.\r\n\r\n**Any Other info.** Since the NuFFT is not based on the FFT it would require a custom kernel and is therefore not trivial to implement. I haven't looked much into it but it could potentially make use of the [Flatiron Institute library](https://finufft.readthedocs.io/en/latest/).\r\n", "comments": ["@zaccharieramzi  Thanks for your contribution. we will keep this noted.", "@zaccharieramzi. While this is an interesting possible feature, we have no plans to work on it currently. Let us know if you plan to start this work. It could easily go external to to TF in a custom op library to start and prove out the usability of the feature. Thanks!", "Sure. So for the record I have been working on it in this PR: https://github.com/odlgroup/odl/pull/1488. \r\n\r\nIt still needs some polishing to fit the requirements of `odl` but I am using it as is currently in a project and it seems to work as expected.", "So I understood better how non-uniform fft works (which is definitely based on the fft, apologies), and I translated this [code](https://github.com/mmuckley/torchkbnufft) from pytorch to tf.\r\nThe result is [here](https://github.com/zaccharieramzi/tfkbnufft).\r\n\r\nEDIT\r\n-----\r\n\r\nYou can now install `tfkbnufft` from pip: `pip install tfkbnufft`.", "@zaccharieramzi,\r\nCan you please let us know if we can close this issue as it has been implemented? Thanks! ", "Well it has been implemented but not in TensorFlow itself.\r\nClosing anyway as I think it won't be the case in the near future, and the current solution is satisfying."]}, {"number": 27192, "title": "myWork", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 27191, "title": "AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'", "body": "i ran on Google Colab and saw error at the start of training model. \r\n----> 8     for (batch, (img_tensor, target)) in enumerate(dataset):\r\n--->545   def __next__(self):  # For Python 3 compatibility\r\n--> 546     return self.next()\r\n--> 575       return self._next_internal()\r\n--> 567             output_shapes=self._flat_output_shapes)\r\n-> 1848       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\nUnknownError: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 205, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 107, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"<ipython-input-45-2085415d85d1>\", line 2, in map_func\r\n    img_tensor = np.load(img_name.decode('utf-8')+'.npy')", "comments": ["@phambao Request you to please fill the [template](https://github.com/tensorflow/tensorflow/issues/new/choose) with infrastructure details so that we can suggest you the solution.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 27190, "title": "Losses with Reduction.NONE do not keep the input shape (the result is averaged along the last axis)", "body": "**System information**\r\n- OS Platform and Distribution: **Linux Manjaro**\r\n- TensorFlow installed from: **binary**\r\n- TensorFlow version (use command below): **2.0.0-dev20190326** (nightly build)\r\n- Python version: **3.7.2**\r\n\r\n**Describe the current behavior**\r\nCurrently, when a loss object is created with reduction=tf.losses.Reduction.NONE, the result is averaged along the last axis. This happens with all the losses in the tf.losses module.\r\n\r\n**Describe the expected behavior**\r\nWhen a loss object is created with reduction=tf.losses.Reduction.NONE, the output shape should match the input shape (y_true and y_pred shape).\r\nThis is also reported in the doc: [tf.losses.Reduction](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/Reduction)\r\n\r\nMoreover, when a  _sample_weight_ parameter ([BinaryCrossentropy](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/BinaryCrossentropy)) is provided with the same shape of y_pred, it throws an exception. \r\n\r\n**Code to reproduce the issue**\r\n\r\nLoss function with Reduction.NONE \r\n```\r\nbce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)\r\ny_true = [1., 1., 1., 1.]\r\ny_pred = [1., 1., 0.5, 0.5]\r\nloss = bce(y_true, y_pred)\r\nprint('Loss: ', loss.numpy())\r\n```\r\nCURRENT RESULT: 0.34657347\r\nEXPECTED RESULT: [0., 0., 0.69314694, 0.69314694]\r\n\r\nLoss function with Reduction.NONE and sample_weight\r\n```\r\nbce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)\r\ny_true = [1., 1., 1., 1.]\r\ny_pred = [1., 1., 0.5, 0.5]\r\nsample_weight = [1, 1, 2, 1]\r\nloss = bce(y_true, y_pred, sample_weight=sample_weight)\r\nprint('Loss: ', loss.numpy())\r\n```\r\nCURRENT RESULT: Exception\r\nEXPECTED RESULT: [-0., -0., 1.3862939, 0.69314694]\r\n\r\n\r\n**Other info / logs**\r\nCurrently, to achieve the expected result, an additional \"fake\" dimension must be added to y_true and y_pred, as in the following example.\r\n```\r\nbce = tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)\r\ny_true = [1., 1., 1., 1.]\r\ny_pred = [1., 1., 0.5, 0.5]\r\nsample_weight = [1, 1, 2, 1]\r\nloss = bce(tf.expand_dims(y_true, -1), tf.expand_dims(y_pred, -1), sample_weight=sample_weight)\r\nprint('Loss: ', loss.numpy())\r\n```", "comments": ["@amitsrivastava78 Can you please take a look at this issue? Thanks!", "@matgad , actually BinaryCrossentropy uses the function nn.sigmoid_cross_entropy_with_logits which takes into account the reduction and  applies applying the below formula :-\r\n\r\n_\r\n\r\n> max(x, 0) - x * z + log(1 + exp(-abs(x)))\r\n> where in your case y_pred = x and y_true = z.\r\n\r\n_\r\n\r\nSo based on this the computed size is same as that of input, but there is a twist in this before the actual result is passed it is done a reduce_mean with axis as -1 which means it will add the axis with input dimension and do the mean on tat axes, which s the result you are getting : -\r\n0.34657347\r\n\r\nHope this clears things.\r\n\r\nRegards\r\nAmit\r\n", "Hi @amitsrivastava78, thanks for the answer. The behavior that you described is the default one. But when you define a loss with the optional parameter `reduction=tf.losses.Reduction.NONE`, the last reduce_mean (or any other reduce) should be skipped. This issue only applies with None reduction.\r\n`BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)` is just an example, the same behavior should be extended for any loss function.\r\n", "@matgad ,i see your point, i think this is a BUG, irrespective of the reduction, the tensorflow is applying the reduce_mean to everything, in the coming days i will try to raise one PR to fix this issue.\r\n\r\nRegards\r\nAmit ", "Great, many thanks. Please also take a look on the exception that is thrown when `sample_weight` has the same shape of the input (see the example in my first post for details), I think the two issues are related.", "@matgad , i have raised the PR and included your cases as unit test cases as well, Lets wait for this PR to merge, in the mean time you can  use this PR in case you are blocked.\r\n\r\n#27784 \r\n\r\nRegards\r\nAmit", "Wonderful, thank you.", "@matgad @amitsrivastava78  The loss functions actually do not perform any reduction by default. The expectation is that the input passed to the functions will be atleast 2D. These functions return one loss value per sample as output.\r\n\r\nEg if y_true and y_pred have the shape (3, 3), return value will be of shape (3,). => we get one loss value per sample\r\nif y_true and y_pred have the shape (3, 2, 4), return value will be of shape (3, 2) => we get one loss value per sample, per timestep etc.\r\n\r\nSorry that the functions do not have any documentation about this currently. We are working on adding that, please feel free to contribute to the documentation if you are interested.\r\n\r\nI just replied on the PR with this as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27190\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27190\">No</a>\n", "Why would issue be closed before updating the documentation? I just ran into the problem because documentation specifically said reduction=Reduction.NONE would give me same shape as I send in.", "@pat-coady sure we can leave this issue open until we have documentation updated. ", "@pavithrasv If it is helpful, I can update docstrings in appropriate places and submit the PR. I haven't contributed to the project before, but am reviewing Contributing Guidelines now.", "@pat-coady Oh yes absolutely. You are welcome to make contributions anytime :) As mentioned in my comment above we will need to add documentation for all standalone functions in losses.py (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L752) indicating that the input needs to be at least 2D.", "Submitted PR:\r\nhttps://github.com/tensorflow/tensorflow/pull/30463", "Great thank you!", "Automatically closing this out since I understand it to be resolved by the PR #30463 (merged already), but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27190\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27190\">No</a>\n", "@jvishnuvardhan This still is NOT resolved even with TF2.2. \r\n\r\n```\r\na = np.array([[.3, .4], [.5, .6]])\r\nb = np.array([[.312, .433], [.555, .66]])\r\n\r\nprint('incorrect output shape:', tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)(a, b))\r\nprint('+'*10)\r\nprint('correct output shape:', tf.nn.sigmoid_cross_entropy_with_logits(a, b))\r\n\r\nincorrect output shape: tf.Tensor([0.64322317 0.69003338], shape=(2,), dtype=float64)\r\n++++++++++\r\ncorrect output shape: tf.Tensor(\r\n[[0.76766615 0.75970248]\r\n [0.73116606 0.6806367 ]], shape=(2, 2), dtype=float64)\r\n\r\n```\r\n\r\nreduction='none' needs to mean reduction equals none...not sure why this would still not be the case. :( ", "@pGit1 This is expected and this is what @pavithrasv described in detail [here](https://github.com/tensorflow/tensorflow/issues/27190#issuecomment-487233022).\r\n\r\nPlease check the usage examples and description [here]( https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy?version=nightly).\r\n\r\nPlease let me know if there is anything I am missing. Thanks!", "@jvishnuvardhan I see that but it still doesn't make sense to me. I discovered the tf.nn.sigmoid_cross_entropy by accident and it behaves _exactly as one would expect_ when there is no reduction (i.e. element-wise loss scalars). @pavithrasv explanation doesn't makes sense to me and the documentation doesn't either. It's is weird to me that no reduction actually leads to a loss in tensor dimension...I just cannot wrap my head around on what \"sample\" means in @pavithrasv explanation or the official documentation. \r\n\r\nIf I am comparing two 3x3 tensors with no reduction I have **no idea** why I am not getting a 3x3 loss tensor out. I really may be just misunderstanding what \"sample\" means here. It almost seems like by sample you mean **row** in y_pred and y_true tensors. Its as if you actually do compute an element-wise loss and once this is done you take the mean along the -1 dimension. But the fact that a mean (or sum) is taken means there is a **reduction** by definition. :(\r\n\r\nI really dislike this inflexibility. It would be nice if reduction='none' actually meant no reduction in tensor dimension. This behavior means of all the useful losses that tf.losses already provides would need to be re-implemented or do the adding an extra dimension workaround that @matgad pointed out. Again reduction equals none should not mean the software takes an average for me. I've updated and confirmed my example above:\r\n\r\n```\r\na = np.array([[.3, .4], [.5, .6]])\r\nb = np.array([[.312, .433], [.555, .66]])\r\n\r\nprint('incorrect output shape:', tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE,\r\n                                                                    from_logits=True)(a, b))\r\nprint('+'*10)\r\nprint('correct output shape:', tf.nn.sigmoid_cross_entropy_with_logits(a, b))\r\nv = tf.nn.sigmoid_cross_entropy_with_logits(a, b).numpy()\r\nprint(f\"confirming naughty behavior of reduction='none':{v.mean(axis=-1)} \")\r\n\r\nincorrect output shape: tf.Tensor([0.76368433 0.70590138], shape=(2,), dtype=float64)\r\n++++++++++\r\ncorrect output shape: tf.Tensor(\r\n[[0.76766615 0.75970248]\r\n [0.73116606 0.6806367 ]], shape=(2, 2), dtype=float64)\r\n++++++++++\r\nconfirming naughty behavior of reduction='none':[0.76368431 0.70590138] \r\n```\r\n\r\nNo reduction should mean no reduction. Even if the documentation points out the behavior it is still **incredibly inflexible and frustrating**. ", "@pGit1 Sorry about that. I understand this can be confusing. This is by design and has been since the beginning of Keras, so any changes to this will be a big breaking change. If your data is 1D, we request that this be converted to 2D as losses expect 2D data. You will get 1D data back ie. one loss value per example when reduction is None. ", "@pavithrasv I guess adding an extra dimension solution isnt so bad but I still think its weird. Thanks for clarifying. ", "@pavithrasv @jvishnuvardhan \r\n\r\nQuick question by the way while on this topic. I wonder what tf.keras is actually doing with  tensors in general because if I use a tf.keras.Reshape layer I can literally reshape the layer to anything I want and it will still work. Also I can have an output layer for a model of some fixed dimension and pass it a tensor of different dimension during training and it still works. Why is that?? What is going on now? Im using TF 2.2 by the way. Also tf.reshape behaves as it should, i.e. it breaks when I try to arbitrarily reshape a tensor with more or less elements than input tensor...", "> \r\n> \r\n> Hi @amitsrivastava78, thanks for the answer. The behavior that you described is the default one. But when you define a loss with the optional parameter `reduction=tf.losses.Reduction.NONE`, the last reduce_mean (or any other reduce) should be skipped. This issue only applies with None reduction.\r\n> `BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)` is just an example, the same behavior should be extended for any loss function.\r\n\r\nThat was exactly the case in my custom loss function created in class inherited from **tf.keras.losses.Loss** . After deletion of all **reduce_mean**/**reduce_sum** the function works like a charm. Thank you so much for your short explanation!!!", "If we set the reduction in our binary cross entropy loss to tf.losses.Reduction.NONE when we compile our model is there a way to retrieve the unreduced losses when we evaluate on a sample? I'm running into an issue where the loss in my model (with zeroed-out weights and bias - code below on how i'm doing this) is not what I'm expecting. \r\n\r\n```\r\n    for ix, layer in enumerate(model.layers):\r\n      weights = layer.get_weights()\r\n      for arr in weights:\r\n        arr[(arr > 0) | (arr < 0)] = 0\r\n      layer.set_weights(weights)\r\n```\r\n\r\n\r\nSo I'd like to diagnose how my model is calculating the loss by looking at output from the 3 stages of reduction: NONE, SUM, and SUM_OVER_BATCH_SIZE", "Issue still exists on CoLab"]}, {"number": 27189, "title": "bazel error: name 'http_archive' is not defined", "body": "\r\n\r\nI want to build libtensorflowLite.so for Android\r\n```\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow.git -b r1.12\r\ncd tensorflow\r\nvim WORKSPACE \r\n```\r\n# 1. modify WORKSPACE:\r\n```\r\nandroid_sdk_repository(\r\n   name = \"androidsdk\",\r\n   api_level = 29,\r\n   build_tools_version = \"29.0.0\",\r\n   path = \"/Users/gongjia/Library/Android/sdk\",\r\n)\r\n\r\n# Android NDK r12b is recommended (higher may cause issues with Bazel)\r\nandroid_ndk_repository(\r\n   name=\"androidndk\",\r\n   # path=\"/Users/gongjia/Library/Android/sdk/ndk-bundle\",\r\n   path=\"/Users/gongjia/Library/Android/sdk/android-ndk-r14b\",\r\n   api_level=21\r\n)\r\n```\r\n# 2. add in tensorflow/contrib/lite/BUILD\r\n```\r\ncc_binary(\r\n    name = \"libtensorflowLite.so\",\r\n    linkopts = [\"-shared\", \"-Wl,-soname=libtensorflowLite.so\"],\r\n    visibility = [\"//visibility:public\"],\r\n    linkshared = 1,\r\n    copts = tflite_copts(),\r\n    deps = [\r\n        \":framework\",\r\n        \"//tensorflow/contrib/lite/kernels:builtin_ops\",\r\n    ],\r\n)\r\n```\r\n# 3. exec: \r\n```\r\nbazel build -c opt //tensorflow/contrib/lite:libtensorflowLite.so   --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" --verbose_failures\r\n```\r\n\r\n# 4. result:\r\n```\r\nERROR: /Users/gongjia/Documents/github/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\nERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package\r\nINFO: Elapsed time: 0.082s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\n# 5. Info:\r\ntensorflow: r1.12\r\nconfigure: default\r\nmacOS Mojave 10.14.3\r\npython:2.7\r\nbazel: 0.24.0", "comments": ["@307509256 Request you to please fill the [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md) with infrastructure details so that we can suggest you the solution", "What is the bazel version?", "Meet the same error with bazel 0.24.1", "Including all the workspace modifications? Or from a pristine fork of the repo?", "> What is the bazel version?\r\n\r\n0.24.0", "> Including all the workspace modifications? Or from a pristine fork of the repo?\r\n\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow.git -b r1.12\r\nis ok? ", "Can you try building first with no modifications to the code after cloning?", "> Can you try building first with no modifications to the code after cloning?\r\n\r\nyes, I have try it, fail also ", "Can you try with r1.13 branch instead? Or, if you want r1.12, can you try downgrading bazel to 0.19.2?", "@307509256 Please check the tested build config [here](https://www.tensorflow.org/install/source#cpu). For TF1.12, you need to downgrade Bazel to 0.15.0. For TF1.13.1, you need to use Bazel 0.19.2. Thanks!", "> @307509256 Please check the tested build config [here](https://www.tensorflow.org/install/source#cpu). For TF1.12, you need to downgrade Bazel to 0.15.0. For TF1.13.1, you need to use Bazel 0.19.2. Thanks!\r\n\r\nThe problem is solved, thank you!\r\nanother question , I build success\uff0c the size of file \r\n\u201cbazel-bin/tensorflow/contrib/lite/libtensorflowLite.so\u201d is  49M\uff0c\r\n but I download from maven is 2.3M\r\nurl\uff1a http://jcenter.bintray.com/org/tensorflow/tensorflow-lite/1.12.0/", "Size difference can be caused by the compile options (if you have debug info, etc.). You can also try `strip` on the `.so` file to get rid of some of the symbols.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Please open a new issue for other questions as it will help the community. Thanks!", "Just had the same issue, thank you guys for making our life difficult with your magic build system..\r\nI have to swap build system versions to build different library versions.. This is great..! \r\nDowngrading to bazel 0.19.2 works", "The issue is that bazel 0.* is unstable and changes wildly. Soon we will have a bazel 1.0 release which is going to be much stable.", "What bazel version should I downgrade to for TF 1.10.0 ?", "Pleae see table at https://www.tensorflow.org/install/source#tested_build_configurations"]}, {"number": 27188, "title": "Minor doc fix", "body": "In Deprecation warning,  ```keras.layers.RNN(cell)``` -> ```tf.keras.layers.RNN(cell)```\r\n\r\nIssue #27186", "comments": ["@gshashank84 can you please check failed builds ", "> @gshashank84 can you please check failed builds\r\n\r\nI checked for the build logs, it states ```error loading packages```. I have only one commit which is also a comment, therefore there must not be any error. Any suggestions? @rthadur ", "@yifeif Can you please help merge this PR. Thanks!", "@yifeif Can you please help merge this PR. Thanks!", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 27187, "title": "tflite InterpreterBuilder error and crash", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution : MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nOPPO A57, \u9b45\u65cf M6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.20.0-homebrew\r\n- GCC/Compiler version (if compiling from source): 8.2.0 (Homebrew GCC 8.2.0)\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n- ndk version: 16\r\n\r\n**Describe the current behavior**\r\nI build jni so file to use on android, but there will be a unscheduled SIGSEGV bug when I loaded the tflite model to build the interpreter. And the app will crash.\r\n\r\n**Other info / logs**\r\n\r\n    #00 pc 00103e5c libtensorflowlite.so tflite::GetRegistrationFromOpCode(tflite::OperatorCode const*, tflite::OpResolver const&, tflite::ErrorReporter*, _TfLiteRegistration const**) [armeabi-v7a]\r\n    #01 pc 000f9251 libtensorflowlite.so tflite::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::Interpreter, std::__ndk1::default_delete<tflite::Interpreter> >*, int) [armeabi-v7a]\r\n    #02 pc 0001f44d /data/app/com.ziipin.softkeyboard-2/lib/arm/libsmallime.so (tensorflow::tflite_inference::Inference::LoadLSTMModel(char const*)+92) [armeabi-v7a]\r\n\r\n\r\n", "comments": ["\u5982\u3000\u4f60\u7684\u4ee3\u7801\u662f\u5728\u3000ndk\u3000\u4e2d\u5199\u5f97\uff0c\u4e5f\u5c31\u662fc/c++\u65b9\u5f0f\u7136\u540e\u7f16\u8bd1\u6210so\u5e93\uff0c\u6211\u4e4b\u524d\u4e5f\u662f\u8fd9\u4e48\u64cd\u4f5c\u7684...\r\n\u6709\u4e00\u4e2a\u539f\u56e0\u662f\u3000tflite::InterpreterBuilder\u3000\u7684\u6784\u9020\u65b9\u5f0f\u65f6\uff0c\u4f20\u5165\u7684\u53c2\u6570\u3000\u8fdb\u884c\u521d\u59cb\u4e86\uff0c\u7533\u8bf7\u4e86\u5185\u5b58\uff0c\u5728stack\u4e2d\uff0c\u5185\u5b58\u53ef\u80fd\u4e0d\u591f...\r\n\u6216\u8bb8\u6709\u53e6\u5916\u4e00\u4e2a\u65b9\u6cd5\u80fd\u591f\u5e2e\u5230\u4f60\uff0c\u5c31\u662f\u7528new\u3000\u4e00\u4e2a\u6307\u9488\u7684\u65b9\u5f0f\uff0c\u6765\u64cd\u4f5c...\r\n\u4f8b\u5982\uff0c\u6211\u5c31\u5c1d\u8bd5\u8fd9\u4e48\u53bb\u505a\u4e86\uff0c\u800c\u4e14\u6210\u529f\u4e86\uff0c\u76ee\u524d\u5728 \u521d\u59cb\u5316\u8fd9\u4e2a\u7c7b\u3000\u65f6\uff0c\u6ca1\u53d1\u73b0\u4ec0\u4e48\u5f02\u5e38...\r\nauto resolver = std::unique_ptr<tflite::ops::builtin::BuiltinOpResolver>(\r\n            new tflite::ops::builtin::BuiltinOpResolver());\r\n   \r\n    auto builder = std::unique_ptr<tflite::InterpreterBuilder>(\r\n            new tflite::InterpreterBuilder(*model, `*resolver));`\r\n", "\u4f60\u597d\uff0c\u8bf7\u95ee\u4f60\u7684tensorflowlite\u5728andoroid\u4e0a\u6709\u8dd1\u8d77\u6765\u5417\uff1f\u6709\u6ca1\u6709\u5176\u4ed6\u95ee\u9898\uff0c\u4f8b\u5982\r\nstd::unique_ptr<tflite::Interpreter> interpreter\u3000\u91cc\u9762\u6709\u4e9b\u53d8\u91cf\u867d\u7136\u53ef\u4ee5run\uff0c\u4f46\u662f\u6253\u5370\u6570\u503c\r\n\u5374\u662f\u4e0d\u884c\u7684\uff0c\u5f88\u60f3\u8ddf\u4f60\u591a\u4ea4\u6d41\u4e00\u4e0b\uff0c\u80fd\u5426\u7559\u4e0b\u8054\u7cfb\u65b9\u5f0f\uff0cQQ : 459103831\u3000\u8fd9\u662f\u3000\u6211\u7684\u53f7\u7801\r\n\u5e0c\u671b\u80fd\u591f\u4e0e\u4f60\u4e00\u8d77\u591a\u63a2\u8ba8\u3000\u63a2\u8ba8\uff0c\u8c22\u8c22", "Closing the issue. Please try what @teleger suggested."]}, {"number": 27186, "title": "Wrong function referenced in doc", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.13\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\r\n\r\n\r\n**Describe the documentation issue**\r\nOn the warning about this function being deprecated, it is suggested to use instead use keras.layers.RNN. However, the function is called tf.keras.layers.RNN (https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN) \r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Looks to have been fixed. Thanks! "]}, {"number": 27184, "title": "Added Variable shape and lookup errors TCs.", "body": "This is one of the TODO in the file.", "comments": ["@alanchiao can you take a look?", "@alanchiao Could you PTAL and approve.", "@alanchiao Could you PTAL and approve.", "@alanchiao , thanks for taking time for the review of my PR, i have updated the code as per your feedback, kindly approve.\r\n\r\nRegards\r\nAmit", "In the future, it'd be helpful also to keep the older commits with the previous comments. Right now, I'm not able to access the older commits, which may be because you're force pushing. I know I made this comment in another PR also.", "> In the future, it'd be helpful also to keep the older commits with the previous comments. Right now, I'm not able to access the older commits, which may be because you're force pushing. I know I made this comment in another PR also.\r\n\r\n@alanchiao , thanks for the review i have updated the code based on your review comments, kindly check and approve.\r\n\r\nRegards\r\nAmit", "@alanchiao , thanks for approving the PR.\r\n\r\n@gbaned can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 27183, "title": "Add some log info within GradientTape", "body": "@alextp, reference to #26143.\r\n\r\nI saw @shashvatshahi1998 made some changes already, So I did some supplement here.\r\n\r\nPlease take a look at it, Thanks.\r\n", "comments": ["@superbobry please review my pull request also I don't know why my cla is still showing no while Ihave signed that on 8th March", "Please fix the lint errors:\r\n\r\n```\r\ntensorflow/python/eager/backprop.py:950: [C0301(line-too-long), ] Line too long (85/80)\r\n\r\ntensorflow/python/eager/backprop.py:964: [C0301(line-too-long), ] Line too long (85/80)\r\n```", "@alextp, Done now. Thanks!", "@alextp  @superbobry, Thanks for all you help!", "@alextp, Appreciate you help again, and why the check is pending now?", "@alextp, why `Ubuntu Makefile` failed? I couldn't see any details about it.", "No idea, I can't see it either. Rerunning.", "@alextp, It it have already be merged, I think this thread could be closed now."]}, {"number": 27182, "title": "[TF 2.0 alpha] Tutorial \"Loading Data\" wrong link path ", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: Tensorflow 2.0 alpha\r\n- Doc Link: \r\n[Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images)\r\n[Using TFRecords and tf.Example](https://www.tensorflow.org/alpha/tutorials/load_data/tf_records)\r\n\r\n**Describe the documentation issue**\r\n[Load images with tf.data](https://www.tensorflow.org/alpha/tutorials/load_data/images) and [Using TFRecords and tf.Example](https://www.tensorflow.org/alpha/tutorials/load_data/tf_records) have wrong links to Colab and github.\r\n\r\nThey should link to \r\nhttps://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/load_data/images.ipynb\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/load_data/tf_records.ipynb\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@shaolinkhoa The issue has been updated and commits are approved. You can close this issue. Thank you for letting us know :)", "Closing this issue since its resolved. Thanks all."]}]