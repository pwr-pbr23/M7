[{"number": 1210, "title": "Fix a wrong link to random_crop()", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Merged, thanks!\n"]}, {"number": 1209, "title": "Fix typos in comments.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thank you for adding me in `RELEASE.md`. It motivates me. :)\n", "@martinwicke . I changed that, too. :)\n", "Merged, thanks!\n", "Thank you again!\n"]}, {"number": 1208, "title": "Rename `testDoubleRandomTranposeBoth` to `testDoubleRandomTransposeBoth`", "body": "This PR renames a test case in `matmul_op_test.py` to fix a typo.\n", "comments": ["Jenkins, test this please.\n", "Jenkins, test this please (again). @dongjoon-hyun I'm just using this PR to test some Jenkins stuff. Hope you don't mind.\n", "No problem, @martinwicke . :)\n", "Merged. Thanks. I'm pretty sure this PR is not responsible for the coordinator test failure.\n", ":-)\nThanks again.\n"]}, {"number": 1207, "title": "Avoid copying invalid large constant data.", "body": "Tensorflow cannot create an operation with a NodeDef larger than 2GB.\n(Python Unit Test Case: ConstantTest.testTooLargeConstant)\nHowever, before throwing a meaningful exception, it consumes lots of memory\nto copy the invalid input data. This causes `std::bad_alloc` errors sometimes.\n\nThis PR prevents those invalid copying. The allocated memory for the following\ntestcase decreases from over 4GB into less than 1GB.\n\nbazel-bin/tensorflow/python/constant_op_test ConstantTest.testTooLargeConstant\n", "comments": ["Can one of the admins verify this patch?\n", "I am not sure expanding the API of `make_tensor_proto` for this purpose is a good idea, but that's my opinion. Adding @josh11b and @mrry for opinions.\n", "I'm fine with this, but the code needs to be wrapped at 80 columns.\n", "I agree with @keveman, it feels weird to inject the error message and the limit. I would add it as a special check in a try catch on the same operation ...\n\n```\ntry:\n    tensor_proto.tensor_content = nparray.tostring()\ncatch:\n    // check error and check size and if is greater than 2GB raise error.\n```\n", "Agreed, I prefer try/catching rather than adding args to the API.  Otherwise looks great.  Probably worth adding a test of the core functionality (without necessarily creating a 2GiB array)\n", "@vrv . Oh, it's not. We should avoid run the `try` part. \nWhen you call the following statements, it really consume all memory.\n\n```\ntensor_proto.tensor_content = nparray.tostring()\n```\n", "HI, @Mistobaan and @keveman .\nFor the injection, the exception message depends on the situation.\nFor this PR, it's for `constant` op generation. \nFor the other code, the error message will be different.\n", "I updated the PR and rebased to the master again.\n- Remove the added parameter: `limit` and `limit_msg`.\n- Use `try` statement in `constant_op.py` instead.\n- Keep the original behavior (including other `ValueError` exceptions).\n\nThank you, @vrv , @keveman , @josh11b , and @Mistobaan .\n", "Hi, @keveman . I updated the code according to your advice. Also, it rebased to the master and passed the test. Thanks.\nBy the way, during rebasing, the messages on the code are gone. You can see that here.\nhttps://github.com/dongjoon-hyun/tensorflow/commit/9682a424943ed3e2bbe3186398210d88f1e64a52#commitcomment-16256209\n"]}, {"number": 1206, "title": "ci_build: support tensorflow as submodule", "body": "this enable tensorflow/serving reuse our ci_build\n", "comments": ["@jendap, is this ready to go? You aborted the tests because they were after the critical point?\n", "The tests were aborted for unrelated restart of a gpu node. It should be ready. But just for sure...\n\n@tensorflow-jenkins test this please\n", "@martinwicke if the test is successful (which it should) it sill be ready.\n\nNote: There is one important thing here - it is upgrading bazel to 0.1.5 for all our ci builds. The 0.1.5 is needed for submodules (serving has tensorflow as a submodule).\n", "test seems to not have been successful.\n", "It did because at that time the mac-python3 was failing on master. It was fixed since. But now master is broken again - the current build will fail again :( I'll start another one once master is fixed.\n\nBTW: Since we have to upgrade bazel to 0.1.5+ I'm going to upgrade it here to the latest - 0.2.0\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please\n\n(I just pushed a flaky test fix to public that should help)\n", "It did help. Last build was successful on linux. And mac is not touched by this PR.\n\nI have just rebased to master.\n", "@vrv @martinwicke can I merge this? The upgrade of bazel can make some noise. But serving team would be happy.\n", "Actually as long as nobody use feature in bazel 0.2.0 we are fine. And mac is still on 0.1.4.\n"]}, {"number": 1205, "title": "Inconsistent memory usage between CPU and GPU", "body": "Running a large network on CPU uses ~2GB memory, whereas running the same network on GPU takes ~6GB of memory (the full size of the card). Is it just allocating all the available GPU memory even though it might not all be utilized? This is using the recent 0.7 pip version.\n", "comments": ["The memory optimizations between the CPU and GPU approaches are very different. In particular the GPU version will often allocate more memory to take advantage of batching and other approaches that speed up operations on CUDA devices. Is this causing an issue for you, or are you just interested in the inconsistency?\n", "I wouldn't necessarily call it an issue blocking me from completing my work, but I was interested in using nvidia-smi to gauge how resource intensive my network is so that I could evaluate scaling potential. Not a big deal though since I can increase batch size until the network fails to load onto the GPU.\n", "We'll have better tools to understand memory utilization soon, I hope.  Closing for now, thanks!\n"]}, {"number": 1204, "title": "Possible bug in    master/tensorflow/python/ops/rnn.py", "body": "in rnn.py and the function def _rnn_step(...):\n\nline 261: copy_cond = (time >= sequence_length)\n\nsequence_length is a vector of ints and time is an int,  should time be compared with one element of sequence_length, instead of the whole thing? \n\n(Is this function still work in progress?)\n", "comments": ["The inequality broadcasts, and the resulting variable is a vector. The vector decides for each batch entry whether to copy through or not.\n\nIt's a feature, not a bug!\n", "Hey, what's the status of this \"feature\"?"]}, {"number": 1203, "title": "Unable to safely terminate tensor flow running on gpu", "body": "I have implement a network using tensorflow on a Nvidia GPU. Even after I call sess.close(), the program won't stop. When I hit ctrl+c for termination, the program crashes the nvidia driver and creates a zombie process named \"python\". I am not able to kill the zombie process, neither can I reboot ubuntu system by sudo reboot.\n\nIs this a known issue? Please help!\n", "comments": ["Same problem here with tensorflow 0.7.1 on Ubuntu 15.10 \nPython 2.7.10 and 2 Titan X cards \nMxnet 0.5 works fine so it's probably not a problem with cuda(7.5) or cudnn(v4)\n\nthe symptom can be reproduced easily with the simple example \n\n$ python\n\n> > > import tensorflow as tf\n> > > hello = tf.constant('Hello, TensorFlow!')\n> > > sess = tf.Session()\n> > > sess.run(hello)\n> > > Hello, TensorFlow!\n> > > quit()\n\nat this point the console freeze.\n", "here's the kernel trace if it can help\n\nFeb 23 17:36:53 bob kernel: [  784.040306] nvidia_uvm: Loaded the UVM driver, major device number 247\nFeb 23 17:38:38 bob kernel: [  889.142620] BUG: unable to handle kernel NULL pointer dereference at 0000000000000020\nFeb 23 17:38:38 bob kernel: [  889.142657] IP: [<ffffffffc14e2b4d>] _nv002815rm+0x51d/0x610 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142658] PGD 0\nFeb 23 17:38:38 bob kernel: [  889.142659] Oops: 0000 [#1] SMP\nFeb 23 17:38:38 bob kernel: [  889.142671] Modules linked in: nvidia_uvm(POE) bnep arc4 nls_iso8859_1 nvidia(POE) intel_rapl x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel ath10k_pci ath10k_core kvm btusb crct10dif_pclmul snd_hda_codec_hdmi crc32_pclmul btrtl btbcm ath btintel mac80211 bluetooth aesni_intel snd_hda_codec_realtek snd_hda_codec_generic aes_x86_64 cfg80211 snd_hda_intel snd_hda_codec drm lrw gf128mul sb_edac mxm_wmi edac_core snd_hda_core glue_helper ablk_helper cryptd serio_raw snd_hwdep snd_pcm mei_me snd_timer snd mei soundcore shpchp lpc_ich mac_hid wmi autofs4 psmouse alx ahci mdio libahci\nFeb 23 17:38:38 bob kernel: [  889.142673] CPU: 3 PID: 1216 Comm: python Tainted: P           OE   4.2.0-30-generic #35-Ubuntu\nFeb 23 17:38:38 bob kernel: [  889.142674] Hardware name: MSI MS-7883/X99A GODLIKE GAMING (MS-7883), BIOS 1.20 10/30/2015\nFeb 23 17:38:38 bob kernel: [  889.142674] task: ffff881041fb5280 ti: ffff881041fcc000 task.ti: ffff881041fcc000\nFeb 23 17:38:38 bob kernel: [  889.142699] RIP: 0010:[<ffffffffc14e2b4d>]  [<ffffffffc14e2b4d>] _nv002815rm+0x51d/0x610 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142700] RSP: 0018:ffff881041fcfa08  EFLAGS: 00010246\nFeb 23 17:38:38 bob kernel: [  889.142700] RAX: 0000000000000000 RBX: ffff88107037c008 RCX: 0000000000000000\nFeb 23 17:38:38 bob kernel: [  889.142700] RDX: 0000000000000000 RSI: 0000000000000011 RDI: 0000000000000000\nFeb 23 17:38:38 bob kernel: [  889.142701] RBP: ffff881071f82f58 R08: 00000002f6a4c439 R09: ffff8810705d4700\nFeb 23 17:38:38 bob kernel: [  889.142701] R10: 0000000056cc8b0e R11: ffffffffc19037f0 R12: 0000000000000000\nFeb 23 17:38:38 bob kernel: [  889.142701] R13: 0000000000000001 R14: 0000000000000001 R15: ffff881071f46808\nFeb 23 17:38:38 bob kernel: [  889.142702] FS:  00007f2e30ff9700(0000) GS:ffff88107f2c0000(0000) knlGS:0000000000000000\nFeb 23 17:38:38 bob kernel: [  889.142703] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nFeb 23 17:38:38 bob kernel: [  889.142703] CR2: 0000000000000020 CR3: 0000000002c0c000 CR4: 00000000001406e0\nFeb 23 17:38:38 bob kernel: [  889.142703] Stack:\nFeb 23 17:38:38 bob kernel: [  889.142704]  ffff88107037c008 ffff881074831008 ffff881071f49008 0000000000000000\nFeb 23 17:38:38 bob kernel: [  889.142705]  ffff881072108008 ffffffffc14dcb50 ffff881074831008 ffff88107037c008\nFeb 23 17:38:38 bob kernel: [  889.142706]  0000000000001100 0000000000000000 0000000000000026 ffffffffc14d3881\nFeb 23 17:38:38 bob kernel: [  889.142706] Call Trace:\nFeb 23 17:38:38 bob kernel: [  889.142731]  [<ffffffffc14dcb50>] ? _nv003017rm+0xf0/0x1c0 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142755]  [<ffffffffc14d3881>] ? _nv003008rm+0x11/0x50 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142809]  [<ffffffffc16fba40>] ? _nv002021rm+0x2640/0x3c30 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142833]  [<ffffffffc194ea69>] ? _nv000654rm+0x2b9/0x340 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142857]  [<ffffffffc1944f9a>] ? rm_disable_adapter+0x6a/0x130 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142882]  [<ffffffffc1960da6>] ? nv_uvm_notify_stop_device+0x46/0x60 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142906]  [<ffffffffc195523e>] ? nvidia_close+0x19e/0x3d0 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142930]  [<ffffffffc1952e5d>] ? nvidia_frontend_close+0x4d/0x90 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142932]  [<ffffffff811ff954>] ? __fput+0xe4/0x220\nFeb 23 17:38:38 bob kernel: [  889.142933]  [<ffffffff811ffade>] ? ____fput+0xe/0x10\nFeb 23 17:38:38 bob kernel: [  889.142935]  [<ffffffff810991c3>] ? task_work_run+0x73/0x90\nFeb 23 17:38:38 bob kernel: [  889.142936]  [<ffffffff8107e180>] ? do_exit+0x3a0/0xb10\nFeb 23 17:38:38 bob kernel: [  889.142936]  [<ffffffff8107e983>] ? do_group_exit+0x43/0xb0\nFeb 23 17:38:38 bob kernel: [  889.142937]  [<ffffffff8108aa94>] ? get_signal+0x274/0x600\nFeb 23 17:38:38 bob kernel: [  889.142940]  [<ffffffff81014467>] ? do_signal+0x37/0xa40\nFeb 23 17:38:38 bob kernel: [  889.142943]  [<ffffffff810fc011>] ? SyS_futex+0x81/0x180\nFeb 23 17:38:38 bob kernel: [  889.142944]  [<ffffffff810ed88d>] ? getrawmonotonic64+0x2d/0xc0\nFeb 23 17:38:38 bob kernel: [  889.142945]  [<ffffffff81014ecb>] ? do_notify_resume+0x5b/0x70\nFeb 23 17:38:38 bob kernel: [  889.142947]  [<ffffffff817f2f44>] ? int_signal+0x12/0x17\nFeb 23 17:38:38 bob kernel: [  889.142954] Code: f8 73 00 31 c9 44 89 f2 be 2c 00 00 00 48 89 c7 ff 50 20 48 85 c0 49 89 c4 0f 84 d6 00 00 00 31 c9 31 d2 be 11 00 00 00 4c 89 e7 <41> ff 54 24 20 be 30 00 00 00 48 8b b8 a8 05 00 00 48 89 c3 ff\nFeb 23 17:38:38 bob kernel: [  889.142978] RIP  [<ffffffffc14e2b4d>] _nv002815rm+0x51d/0x610 [nvidia]\nFeb 23 17:38:38 bob kernel: [  889.142979]  RSP <ffff881041fcfa08>\nFeb 23 17:38:38 bob kernel: [  889.142979] CR2: 0000000000000020\nFeb 23 17:38:38 bob kernel: [  889.143700] ---[ end trace a558f3a5039f98c9 ]---\nFeb 23 17:38:38 bob kernel: [  889.143700] Fixing recursive fault but reboot is needed!\n", "here's a few more information\n\nI use \ncudnn : 4.0.7 (can't find the 4.0.4 that seems to be the one that's officialy supported)\ncuda : 7.5.18\ndrivers : 352.79 \n\nmaybe that will help find the bug\njust to be clear because the title doesn't reflect this :\n**the nvidia driver crash. you can't use the cards anymore until you physicaly reboot the server.\nif you try a soft reboot it it will crash the system completly.**\n", "I've switched to ubuntu 15.04 and it fixed my problem\n", "I had the same problem. Went from 14.04 -> 15.10, everything started working. Cudnn 4.0.7, CUDA 7.5, driver 352.79. This is ubuntu server for reference.\n", "have the same problem with exactly the same driver and cuda versions, do I really have to switch my OS for it to work? #1947 \n", "I've heard it's a problem with cuda 7.5 and ubuntu 14 -- downgrading cuda to 7.0 also apparently works.  Hopefully cuda 8.0 doesn't have this problem, but it's still in RC, so our pip installations still require 7.5.\n", "unfortunately downgrading using 7.0 with 4.0.7 did not work and I tried it with the drivers 352.39 and 346.46\n", "@vrv: Should we close, or is a fix possible on our end? \n", "I have a way to reproduce or debug, so we either need a fix from the community, or a repro\n", "*I don't \n", "I did write down all steps while setting up a g2 aws instance where I encountered that problem. Maybe this could help with reproducing - I may have missed some apt-get or cd but the majority should be here.\n\nThe os was ubuntu server 14.04 - ami-fce3c696.\n\n```\nsudo apt-get update -y\nsudo apt-get upgrade -y\nsudo apt-get install -y build-essential\n\nsudo apt-get install -y make pkg-config xorg-dev zip zlib1g-dev\n\n# Blacklist Noveau which has some kind of conflict with the nvidia driver\necho -e \"blacklist nouveau\\nblacklist lbm-nouveau\\noptions nouveau modeset=0\\nalias nouveau off\\nalias lbm-nouveau off\\n\" | sudo tee /etc/modprobe.d/blacklist-nouveau.conf\necho options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf\nsudo update-initramfs -u\nsudo reboot # Reboot\n\n# install nvidia driver, cuda, cud\nwget http://developer.download.nvidia.com/compute/cuda/7.5/Prod/local_installers/cuda_7.5.18_linux.run\n# wget http://developer.download.nvidia.com/compute/cuda/7_0/Prod/local_installers/cuda_7.0.28_linux.run\n\nsudo apt-get install -y linux-image-extra-`uname -r` linux-headers-`uname -r` linux-image-`uname -r`\n\nchmod +x cuda_7.5.18_linux.run\n./cuda_7.5.18_linux.run -extract=`pwd`/nvidia_installers\ncd nvidia_installers\n#install driver\nsudo ./NVIDIA-Linux-x86_64-352.39.run \ncd ..\n\n#install cuda\nsudo ./nvidia_installers/cuda-linux64-rel-7.5.18-19867135.run \n\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-7.5/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\necho 'export CUDA_HOME=/usr/local/cuda-7.5' >> ~/.bashrc\necho 'export PATH=$PATH:/usr/local/cuda-7.5/bin' >> ~/.bashrc\n\n#install cudnn --> first download cudnn-7.5-linux-x64-v5.0-rc.tar or 7.0-v4\ntar -xf cudnn-7.5-linux-x64-v5.0-rc.tar\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include/\n\n# install all package dependencies\nsudo apt-get install -y \\\nzip \\\nvim \\\nswig \\\nfortran \\\ngit \\\nlibboost-all-dev \\\nlibatlas-base-dev \\\nlibblas-dev \\\nliblapack-dev \\\npython-dev \\\npython-pip \\\nvim \\\nsoftware-properties-common \\\nlibjpeg8-dev \\\nbyobu \\\nlibfreetype6-dev \\\nlibpng-dev\n\nsudo pip install -U pip\n\nsudo pip install -U \\\nvirtualenv \\\nbumpy \\\nscipy \\\nmatplotlib \\\ngensim \\\nsacred \\\nscikit-learn \\\nlangdetect \\\npymongo \\\njupiter \\\nscikit-image \\\npyyaml \\\npandas \\\nbs4\n\n# install jdk 1.8 for bazel\nsudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\n\n# install bazel\ngit clone https://github.com/bazelbuild/bazel.git\ncd bazel\ngit checkout tags/0.2.0\n./compile.sh\nsudo cp output/bazel /usr/bin\ncd ..\n\n# install tensorflow\ngit clone -b r0.8 --recurse-submodules https://github.com/tensorflow/tensorflow\ncd tensorflow\n./configure\n\n# bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow\nsudo pip install /tmp/tensorflow/tensorflow-0.8.0rc0-py2-none-any.whl\n```\n", "@flow-ryan: Thank you for the instructions, but unfortunately this sounds like a bug below the level of TensorFlow, either in Linux or the GPU driver.  Since it's already fixed in more recent Ubuntu, I'm going to close.  We'll happily accept a PR if you find a reasonable workaround for the problem, though.\n", "@girving I'm suffering from this issues too. I found that it can be reproduced when there is no other process using GPUs, such as lightdm\\X server\\other gpu programs.(in the other word, Tensorflow has fully occupied all the GPU of one host).\n\nLooking forward to heard from you.\n", "@jmhessel @suniliitm @Lazare , we are facing with the same problem you mentioned above. Our setting is ubuntu14.04 lts, cuda 7.5.18, cudnn 4.0. (hardware, 2_titanX per machine, or 8_tesla k40 per machine).  As chivee said, tensorflow can work correctly in the case when some process else related to GPU started(e.g. x server, or another tf/torch/theano process keep running). And what's more we have further checked, if there is just one GPU in the server, this issue doesn't exist as well. I would like to make sure you said updating the Ubuntu version would get through, did you check whether another Linux version has triggered x server inside which starting to use GPU? @girving , do you have any suggestion for us, if we want to check the detail code?\n", "@chivee @taifeng1205 Unfortunately, as noted above this seems like a bug in Linux or the GPU driver, so we may not be able to fix it on our end.  We're happen to accept PRs if you find a fix, but upgrading may be the easiest route.\n", "On my Debian machine enabling [persistence mode](http://docs.nvidia.com/deploy/driver-persistence/) resolved the problem.\n", "@Qwlouse it works! thank you so much.\n", "@Qwlouse thanks!", "I am having the same problem with redhat el7, cuda9.0.\r\nHow can I fix it? reboot is not an option. please help."]}, {"number": 1202, "title": "Bidirectional rnn state", "body": "Added return state from forward and backward rnn in bidirectional_rnn. Updated tests to pass in kernel_tests/rnn_test.py\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Can you squash the commits and make sure the email you have used for your CLA is the author of the remaining commit?\n", "@martinwicke Yes I can do that. I will probably close this and commit from another user name\n"]}, {"number": 1201, "title": "run within PHP ", "body": "Is it possible to run \n\n`bazel-bin/tensorflow/examples/label_image/label_image`\n\ninside PHP script and echo the output of main.cc to PHP script?\n\nIs there any way to remotely call and get the results?\n", "comments": ["There is separate project named TensorFlow Serving.\nYou could call API from PHP.\n", "This is a question about how to use PHP to invoke processes, not a TensorFlow issues.\n"]}, {"number": 1200, "title": "Cherry-pick: Fix tensorboard import path", "body": "For 0.7.1\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n"]}, {"number": 1199, "title": "No module named tensorboard in the last pull and build 0.7", "body": "I just pulled the repo and build it from source and when calling tensorboard in the terminal I get following error:\n\n``` shell\nTraceback (most recent call last):\n  File \"/Users/tensorflow/bin/tensorboard\", line 7, in <module>\n    from tensorflow.tensorboard.backend.tensorboard import main\nImportError: No module named tensorboard\n```\n\nEnv is Mac OS X El Capitan. \n", "comments": ["Do you have more than one installations of TensorFlow on your system? It could that the new tensorboard binary is trying to import from an older version of TensorFlow installation. \n", "No, I'm using vitualenv and before instilling 0.7, I removed 0.6 manually \n", "Can you try creating a new virtualenv and installing from scratch?  I just checked in a fresh install and that module exists.\n", "I checked it in new virtualenv and results as follow:\n- Installing from the built source by me >> got the same error (in the other virtualenv I'm also using my own build)\n- Installing from wheel works fine\n\nI build from the source touching nothing, just git pull and following the instruction on the website.\n\nbtw, the reason I built it from source was that the `freeze_graph()` python code doesn't run because of lacking `graph_util.convert_variables_to_constants()` method was not in the wheel but in the git. I think you should update the wheel with new build.  \n", "Hmm, our new wheels should get that function -- not sure how it didn't make it into the 0.7.0 ones.\n\nAs for the tensorboard bug, now I'm seeing it when building from HEAD.  Will look into it and make sure it's fixed in 0.7.1\n", "Thanks, looking forward to it.\n\nSomeone [mentioned](http://stackoverflow.com/a/35295754/1973820) that there is mismatches in the BUILD but I couldn't get it to work by the proposed solution.\n", "https://github.com/tensorflow/tensorflow/commit/716b7713e4c8b2d8f093f639ca41816cf4e1c696\n\nI think that should fix it -- I'm cherry-picking that into 0.7.1 right now.  Feel free to let me know if it's still not working.\n", "Yes, Thank you very much, it solved the issue. \n"]}, {"number": 1198, "title": "reverse_sequence's inability to accept int32 can break bidirectional_rnn", "body": "In the latest releases `bidirectional_rnn` has been changed to accept int32 tensors for the `sequence_length` argument, but `tf.reverse_sequence` only accepts int64 tensors, and this is currently causing an error when an int32 tensor is passed to `bidirectional_rnn`.\n", "comments": ["@ludimagister: Is this problematic to fix? \n", "Sorry for the delay, this is fixed at head now.\n\nMike\n", "@ludimagister: For future use: if you include \"Fixes #<issue>\" in the commit description the bug will be automatically closed on push.\n"]}, {"number": 1197, "title": "Problem with OpenCV - Initializing TensorFlow variables takes very long", "body": "For some reason, the variable initialization of TensorFlow takes a very long time when OpenCV is imported (`import cv2`). Running the operation `tf.init_all_variables()` takes less than 1 second without OpenCV, but as soon as I import `cv2` this takes 22 seconds (!). See attachment for the simple - Hello World - code that I run. [hello_world.txt](https://github.com/tensorflow/tensorflow/files/138119/hello_world.txt) \n\nThis problem occurs in TensorFlow 0.6 and 0.7. During the initialization the CPU load is 100%, GPU is idle but all available memory is initialized by TensorFlow.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.0\n```\n\nTensorFlow is installed in an virtualenv using `pip` after installing CUDA Toolkit 7.5 and CuDNN v3. My OpenCV installation is from sources (GitHub, master), so this is version 3.1+. Some possibly relevant installation parameters are `WITH_CUDA=ON`, `WITH_CUBLAS=OFF`.\n\nI am using a powerful machine running Linux Mint, 32 CPU cores and an Titan X.  \n", "comments": ["Mysterious. I don't know what cv2 does, but since you are building it with Cuda, it could be preallocating GPU memory (similar to what tensorflow does). The symptom is decidedly odd though. @zheng-xq, do you have an idea?\n", "Oke, I just recompiled OpenCV 3.1 without CUDA support (i.e. `WITH_CUDA=OFF`) and this solved the problem. TensorFlow initialization with and without `import cv2` is now fast. So it seems like there is an incompatibility with TensorFlow and OpenCV compiled with CUDA. Like you said, probably both TF and OpenCV are initializing GPU memory at the same time which causes trouble?\n", "TF takes most of the GPU memory upon its initialization. You can set the following session config to use smaller amount of memory:\n\nconfig.gpu_options.per_process_gpu_memory_fraction\n\nIf that doesn't work, then it is useful to look at some profiling results and see where the cycles are spent. \n", "Closing for now.  For better or worse, TensorFlow assumes it owns the whole GPU, so other libraries trying to use the GPU at the same time will cause issues.\n"]}, {"number": 1196, "title": "tensorflow using cpu+2 gpus turns out 3 times slower than theano using 1 gpu", "body": "1. Does tensorflow automatically use all cpus and gpus to run the computational task?\n\n2.I found that tensorflow use 1 5960X cpu and 2 Titan X gpus cost 300% time compared to theano use only 1 Titan X gpu, is there anything wrong with my configuration? I use the same model generated by keras, using different backend(theano and tensorflow)\n\nI installed on Ubuntu 15.10, cuda 7.5, cudnn 4, gcc 4.9, installed from source code, built with GPU support.\n", "comments": ["@Dringite I didn't do data augmentation, and the data preparing time is not counted in the total training time\n", "1) yes, Tensorflow uses all available GPUs and CPUs, although not necessarily in an optimal fashion.\n2) I cannot comment on what keras exports, so it's entirely possible that it restricts to run only on some devices. I would take this up with people who know about keras first, or dig into the code that keras generated and ask about that.\n\nFrom your description, it is very hard to tell what, if anything, is wrong with TensorFlow, so this is probably a better question for the mailing list, or, if you have a more specific question about using multiple devices, for stackoverflow. \n\nI will close this bug for now. Please reopen this if you find a more specific problem.\n", "Even I experienced similar slow down when I first used tensorflow, but tensorflow gives huge speed boost with large batch sizes as it is able to utilize memory of multiple GPUs much effectively."]}, {"number": 1195, "title": "Dynamic Partition Gradient", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System: Linux (CentOS) cluster, custom build. tensorflow built from source by IT folks managing the cluster. \n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. The functions of interest contain many dynamic_partitions\n2. I then compute the gradient with respect to the variables for optimization\n3. I obtain the following error\n   \n   train_op = optimizer.minimize(loss_instance, global_step=global_step)\n   File \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 186, in minimize\n     aggregation_method=aggregation_method)\n   File \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n     aggregation_method=aggregation_method)\n   File \"/share/sw/free/tensorflow/0.6.0/tensorflow/python/ops/gradients.py\", line 426, in gradients\n     (op.name, op.type))\n   LookupError: No gradient defined for operation 'DynamicPartition_1' (op type: DynamicPartition)\n### What have you tried?\n1. I am currently trying to see if I can remove the dynamic partition from my code, but it would be nice if we could keep the dynamic partition inside; however, I understand why the dynamic partition might not be differentiable if variable indices are used. In my case, I just have a constant placeholder that performs that indexing. Could there be a way to compute gradients in this case? Thank you so much!\n", "comments": ["Gradients for `tf.dynamic_partition()` should be supported as of https://github.com/tensorflow/tensorflow/commit/2b672c4a2f6aeaea8457fd4941f48f5a9e80d283 (and certainly in the 0.7 release). Does you custom build pre-date that change?\n", "Ahh ok. Thanks, it does. I won't be able to update it right now unfortunately. Do you know if in 0.6.0 there is slice gradient support? I think I can work with that if it does.\n", "I bring good news! You can just paste the implementation from that commit into your program somewhere (with the appropriate changes as below):\n\n``` python\n@tf.RegisterGradient(\"DynamicPartition\")\ndef _DynamicPartitionGrads(op, *grads):\n  \"\"\"Gradients for DynamicPartition.\"\"\"\n  data = op.inputs[0]\n  indices = op.inputs[1]\n  num_partitions = op.get_attr(\"num_partitions\")\n\n  prefix_shape = tf.shape(indices)\n  original_indices = tf.reshape(tf.range(tf.reduce_prod(prefix_shape)), prefix_shape)\n  partitioned_indices = tf.dynamic_partition(original_indices, indices, num_partitions)\n  reconstructed = tf.dynamic_stitch(partitioned_indices, grads)\n  reconstructed = tf.reshape(reconstructed, tf.shape(data))\n  return [reconstructed, None]\n```\n\n...then the gradient will be available from your code.\n"]}, {"number": 1194, "title": "Fixed spelling", "body": "Minor spelling mistake\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA needs to be signed for us to look at it :(\n", "A little late, but I signed it. :)\n"]}, {"number": 1193, "title": "'utf-8' codec can't decode byte (in tutorial)", "body": "I'm getting an error \"'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\" error when I try to run the MNIST tutorial (specifically, the dataset import).\n### Environment info\n\nOperating System: Ubuntu 15.04\nPython: 3.4.3\nTensorflow from source, commit: 03bff43\n### Steps to reproduce\n\n1) Install fresh python3 venv (pyvenv venv)\n2) Activate venv\n3) uninstall and re-install protobuf 3.0.0a3 to fix [487](https://github.com/tensorflow/tensorflow/issues/487)\n3) pip install \"built whl file\"\n4) In a python terminal, run:\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n### Full Error Output\n\nUnicodeDecodeError                        Traceback (most recent call last)\n<ipython-input-1-4fcb59292e9b> in <module>()\n      1 import tensorflow\n      2 from tensorflow.examples.tutorials.mnist import input_data\n----> 3 mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/examples/tutorials/mnist/input_data.py in read_data_sets(train_dir, fake_data, one_hot, dtype)\n    197 \n    198   local_file = maybe_download(TRAIN_IMAGES, train_dir)\n--> 199   train_images = extract_images(local_file)\n    200 \n    201   local_file = maybe_download(TRAIN_LABELS, train_dir)\n\n/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/examples/tutorials/mnist/input_data.py in extract_images(filename)\n     56   print('Extracting', filename)\n     57   with tf.gfile.Open(filename) as f, gzip.GzipFile(fileobj=f) as bytestream:\n---> 58     magic = _read32(bytestream)\n     59     if magic != 2051:\n     60       raise ValueError(\n\n/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/examples/tutorials/mnist/input_data.py in _read32(bytestream)\n     49 def _read32(bytestream):\n     50   dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n---> 51   return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n     52 \n     53 \n\n/usr/lib/python3.4/gzip.py in read(self, size)\n    363         else:               # just get some more of it\n    364             while size > self.extrasize:\n--> 365                 if not self._read(readsize):\n    366                     if size > self.extrasize:\n    367                         size = self.extrasize\n\n/usr/lib/python3.4/gzip.py in _read(self, size)\n    431             # jump to the next member, if there is one.\n    432             self._init_read()\n--> 433             if not self._read_gzip_header():\n    434                 return False\n    435             self.decompress = zlib.decompressobj(-zlib.MAX_WBITS)\n\n/usr/lib/python3.4/gzip.py in _read_gzip_header(self)\n    290 \n    291     def _read_gzip_header(self):\n--> 292         magic = self.fileobj.read(2)\n    293         if magic == b'':\n    294             return False\n\n/usr/lib/python3.4/gzip.py in read(self, size)\n     88             self._read = None\n     89             return self._buffer[read:] + \\\n---> 90                    self.file.read(size-self._length+read)\n     91 \n     92     def prepend(self, prepend=b'', readprevious=False):\n\n/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/python/platform/default/_gfile.py in sync(self, _args, *_kwargs)\n     43       if hasattr(self, '_locker'): self._locker.lock()\n     44       try:\n---> 45         return fn(self, _args, *_kwargs)\n     46       finally:\n     47         if hasattr(self, '_locker'): self._locker.unlock()\n\n/home/ubuntu/venv/standard/lib/python3.4/site-packages/tensorflow/python/platform/default/_gfile.py in read(self, n)\n    197       A string of the bytes read, up to the end of file.\n    198     \"\"\"\n--> 199     return self._fp.read(n)\n    200 \n    201   @_synchronized\n\n/usr/lib/python3.4/codecs.py in decode(self, input, final)\n    317         # decode input (taking the buffer into account)\n    318         data = self.buffer + input\n--> 319         (result, consumed) = self._buffer_decode(data, self.errors, final)\n    320         # keep undecoded input until the next call\n    321         self.buffer = data[consumed:]\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n", "comments": ["very same problem. I had to build from source to have cuda 7.5 since the current binaries are for 7.0 even for the new release 0.7\nThe build was fine and even some basic ops seem to work but opening the internal MNIST examples breaks exactly like above\n", "Thanks for the report, this was fixed yesterday, so either build from source at current HEAD or wait for our next patch release (soon)\n"]}, {"number": 1192, "title": "gcc: error: unrecognized command line option '-fcolor-diagnostics'", "body": "I had counted such problem when compiling tensorflow from source in mac system:\ngcc: error: unrecognized command line option '-fcolor-diagnostics'\ngcc: error: unrecognized command line option '-Wthread-safety'\ngcc: error: unrecognized command line option '-Wself-assign'\nERROR: /Users/clhuang/Downloads/tensorflow/google/protobuf/BUILD:29:1: C++ compilation of rule '//google/protobuf:protobuf_lite' failed: osx_gcc_wrapper.sh failed: error executing command external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException\n\nAnd my gcc version is 4.9,when I just run the command /my/path/gcc-4.9 -Wthread-safety example.cc,\nit also shows gcc: error: unrecognized command line option '-Wthread-safety'.could anyone give me some advise ,thanks in advance.\n", "comments": ["We generally use the default of clang for OS X compilation, which explains your error. When I run `gcc --version` I get `Apple LLVM version 7.0.2 (clang-700.1.81)`. You should try switching to clang instead of gcc, since that's the supported approach.\n", "yeh,after I switch to clang there is no such problems above.When I run gcc -v I get Apple LLVM version 6.0 (clang-600.0.57) .While some others occour:\n\ntensorflow/core/kernels/fifo_queue.cc:164:46: error: expected body of lambda expression\n          [callback, this](Attempt* attempt) EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n                                             ^\n./tensorflow/core/platform/default/thread_annotations.h:89:3: note: expanded from macro 'EXCLUSIVE_LOCKS_REQUIRED'\n  THREAD_ANNOTATION_ATTRIBUTE__(exclusive_locks_required(**VA_ARGS**))\n  ^\n./tensorflow/core/platform/default/thread_annotations.h:42:42: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'\n#define THREAD_ANNOTATION_ATTRIBUTE__(x) **attribute**((x))\n                                         ^\ntensorflow/core/kernels/fifo_queue.cc:248:46: error: expected body of lambda expression\n          [callback, this](Attempt* attempt) EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n                                             ^\n./tensorflow/core/platform/default/thread_annotations.h:89:3: note: expanded from macro 'EXCLUSIVE_LOCKS_REQUIRED'\n  THREAD_ANNOTATION_ATTRIBUTE__(exclusive_locks_required(**VA_ARGS**))\n  ^\n./tensorflow/core/platform/default/thread_annotations.h:42:42: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'\n#define THREAD_ANNOTATION_ATTRIBUTE__(x) **attribute**((x))\n                                         ^\n8 warnings and 4 errors generated.\nERROR: /Users/clhuang/Downloads/tensorflow/tensorflow/core/BUILD:359:1: C++ compilation of rule '//tensorflow/core:kernel_lib' failed: osx_gcc_wrapper.sh failed: error executing command external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 77 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n\nI should update xcode and get the same gcc version  as you?I have searched from internet and I find someone with linux occoured the same problem when using gcc 4.7,but after update to 4.9 everything is fine.Is the matter suportting c++11?And I think my version can also support c++ compiling.Thx very much!\n", "@ScottHCL I've got the same errors with you. I fixed it by upgrading `gcc` via upgrading Xcode.\n\nSee http://izeye.blogspot.com/2016/03/error-expected-body-of-lambda-expression.html\n", "Assuming issue has been addressed. Please reopen if not.\n"]}, {"number": 1191, "title": "Build of pip package with current HEAD of bazel fails", "body": "Bazel built from it's current head (cc8fd606f2f1878cbb5ed0ff8b533e514818c9dd for me) fails to build the pip package of tensorflow due to exclamation marks occurring in the path of some files. Error message:\n\n```\nERROR: /home/panmari/tensorflow/tensorflow/tensorboard/BUILD:31:1: //tensorflow/tensorboard:all_files: invalid label 'components/prism/tests/languages/css!+css-extras/entity_feature.test' in element 1233 of attribute 'srcs' in 'filegroup' rule: invalid target name 'components/prism/tests/languages/css!+css-extras/entity_feature.test': target names may not contain '!'.\nERROR: /home/panmari/tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Target '//tensorflow/tensorboard:tensorboard' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 1.032s\n```\n\nSimply removing the exclamation mark in the offending paths by renaming them does make the build proceed again. But I still have to check if the tensorboard still works.\n", "comments": ["Version of bazel? Our jenkins instances are totally fine...\n", "```\nBuild label: head (@cbbbf2e)\nBuild target: bazel-out/local_linux-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Feb 17 20:57:21 2016 (1455742641)\nBuild timestamp: 1455742641\nBuild timestamp as int: 1455742641\n```\n\nI'll try again after deleting these files, maybe these were just old, cached files.\n", "Seems like this was an issue with some cached files, I wonder why these showed up in bazel... closing.\n"]}, {"number": 1190, "title": "update release version to 0.7.1", "body": "That should be all that but RELEASE.md\n", "comments": ["LGTM. As we do this more often, we should perhaps write a script to do this automatically.\n"]}, {"number": 1189, "title": "add samples to docker and start jupyter by default", "body": "", "comments": ["Should this not be the default?\n\nNote, people can still \"docker run -it --rm tensorflow/tensorflow /bin/bash\".\n\nBTW: Perhaps we could for 0.8 bundle by default [tensorflow/examples/udacity](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity).\n", "Merged\n"]}, {"number": 1188, "title": "remove ld_library_path from docker image", "body": "as suggested by @3XX0 in https://github.com/tensorflow/tensorflow/issues/970#issuecomment-185954223\n", "comments": ["Merged\n"]}, {"number": 1187, "title": "Build fail: hd_warning_disable.h broken link", "body": "Complete noob - please forgive my ignorance...\n\nUbuntu 15.10\npython 2.7\ncuda 7.5\ncudnn 4.0\ngcc  (Ubuntu 5.2.1-22ubuntu2) 5.2.1 20151010\n\ngit fetch\n\n```\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nINFO: Found 1 target...\nERROR: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/hd_warning_disable.h'.\nERROR: /home/keith/tensorflow/tensorflow/core/BUILD:1048:1: //tensorflow/core:gpu_runtime: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/hd_warning_disable.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nERROR: /home/keith/tensorflow/tensorflow/core/BUILD:1048:1 1 input file(s) do not exist.\nINFO: Elapsed time: 0.284s, Critical Path: 0.02s\nroot@NL015:/home/keith/tensorflow# \n```\n\n... and the problem is fixed - repeated git fetch and bazel build a few hours later.\n", "comments": []}, {"number": 1186, "title": "R0.7", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I suspect this was an accident.\n"]}, {"number": 1185, "title": "Logistic Regression(LR) Results looks strange when using bias but good without bias", "body": "I tried the LR for sparse dataset using the tensorflow package(0.7.0)\nThe following is part of my procedure:\n\nweight_values=generateWeight([trainset.feature_num,1],name='weight')\nbias=init_bias([1,1],name='bias')\nsp_shape=tf.placeholder(tf.int64)\nsp_indices=tf.placeholder(tf.int64)\nsp_ids_value=tf.placeholder(tf.int64)\nsp_features_value=tf.placeholder(tf.float32)\nY=tf.placeholder('float',name='Y')\n\nsp_ids=tf.SparseTensor(sp_indices,sp_ids_value,sp_shape)\nsp_values=tf.SparseTensor(sp_indices,sp_features_value,sp_shape)\n**#Z=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values);\nZ_b=tf.nn.embedding_lookup_sparse(weight_values,sp_ids,sp_values)+bias**\npredict_op=tf.sigmoid(Z_b,name='result')\n# cost=tf.nn.sigmoid_cross_entropy_with_logits(Z,Y)\n\ncost=tf.nn.sigmoid_cross_entropy_with_logits(Z_b,Y)\ntrain_op=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n...\n**but I find the results is  wired when using the bias,using the bias I find the abs(cost) became larger and larger and even the abs(bias) also became larger and larger.** \nIN the early iterations the results like this :\n      1. labels[-1], j-sample 1662, i-bias 0.100000 cost 0.761811, z_b 0.045602\n      2 labels[1], j-sample 823, i-bias 0.084886 cost 0.653277, z_b 0.081396\n      3 labels[-1], j-sample 20802, i-bias 0.089683 cost 0.826316, z_b 0.088132\n      4 labels[-1], j-sample 25965, i-bias 0.074462 cost 0.806052, z_b 0.074804\n      5 labels[1], j-sample 10322, i-bias 0.059276 cost 0.664358, z_b 0.058433\n      6 labels[-1], j-sample 23946, i-bias 0.064129 cost 0.795182, z_b 0.067642\n      ......\nbut with more iteration the results like this :\n 270504 labels[-1], j-sample 446, i-bias -248.318787 cost -250.818787, z_b -250.818787\n 270505 labels[1], j-sample 10314, i-bias -248.328781 cost 248.306259, z_b -248.306259\n 270506 labels[1], j-sample 3820, i-bias -248.318787 cost 247.367340, z_b -247.367340\n 270507 labels[1], j-sample 2922, i-bias -248.308792 cost 248.276184, z_b -248.276184\n 270508 labels[-1], j-sample 20797, i-bias -248.298798 cost -255.061432, z_b -255.061432\n 270509 labels[-1], j-sample 19755, i-bias -248.308792 cost -251.686646, z _b -251.686646\n 270510 labels[1], j-sample 9528, i-bias -248.318787 cost 248.405624, z_ b 248.405624\n\nHowever if I donot use the bias the results looks good, what maybe the problems and I wonder the reasons very much.\n\nCan anynone help me, Thank you!\n", "comments": ["Hi, this is a question better suited to StackOverflow -- github issues are for bugs and feature requests.   Thanks!\n"]}, {"number": 1184, "title": "Cherry-pick python3 and libcuda.so fixes from master into r0.7 branch", "body": "I think it would be nice to get these changes into the patch release too, at least the python3 bug fix one.  Thoughts?\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@danijar: I assume this is okay since you contributed this already to master.\n\n(This is a weird edge case @willnorris)\n", "yeah I've seen another case like this in kubernetes, where commits that were already reviewed and merge into one branch are simply being cherry-picked over to a different branch.  In these cases, you are much more likely to get into a \"need author consent\" state.  At the time, someone had suggested checking to see if the commit was already in the repo in a different branch, which might be possible.  I've cc'ed you on the internal bug.\n", "Would it be safe to ignore the label, or should I still wait for consent?\n", "see [go/cla#github-consent](http://go/cla#github-consent).  This label will never change to \"cla: yes\", so yes it's safe to ignore in this case.\n", "Ok, great, thanks :)\n", "@tensorflow-jenkins: test this please\n\nadded a whl package change as well.\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 1183, "title": "Clean up unused variables in a example", "body": "This PR removes unused variables in `mnist_with_summaries.py` and makes its document align with it.\n", "comments": ["Can one of the admins verify this patch?\n", "I think for illustration purposes, the current code might be useful to show that these functions do return objects that _could_ be used, even though at the moment the side effects are that they add to the graph. What do you think?\n", "@vrv If they have been added intentionally, this PR could be closed. Thanks for the feedback!\n", "@danmane: what do you think?  Originally they were given names (which led to unused_var lint errors).   Do you still think it's worth capturing the return values?\n", "The summary ops are magic in that the canonical summary use depends on the collection. They are ops just like all the others, but their return values are very rarely used (I guess you could depend on them, but typically you're really happy that they don't run, and what are you going to do with the serialized proto anyway except for write it to disk?). I would omit the return types in the example code, I don't think it's all that enlightening.\n", "Ok, merged\n"]}, {"number": 1182, "title": "Small bug in TensorBoard: Visualizing Learning tutorial [FIXED, pending website pushes]", "body": "There is a small bug in the code on the site: https://www.tensorflow.org/versions/r0.7/how_tos/summaries_and_tensorboard/index.html\n\nb = tf.Variable(tf.zeros([10], name='bias'))\nshould be:\n\nb = tf.Variable(tf.zeros([10]), name='bias')\n\nThe consequence is that when you view the graph, the 'bias' variable has no useful label and is simply Variable.\n", "comments": ["This was fixed earlier today in: https://github.com/tensorflow/tensorflow/commit/5df9ff89a58a17e6efca7f186b57f22430130768\n\nWe'll probably amend our docs to fix this eventually.  Going to leave this open until then.\n", "Are the docs fixed? I can take this one to begin with.\n", "This is fixed for docs versions later than 0.7. Do we have a policy on backporting documentation fixes to older versions?\n@martinwicke want to chime in?\n", "FWIW I would be fine just closing this because I don't think many people will find the 0.7 docs.\n", "@robwell (or anyone else), if you are motivated enough to send a fixing PR against 0.7 we'll happily accept it. But I will close this issue, it's not enough of a problem since it's fixed in 0.8 and master.\n"]}, {"number": 1181, "title": "tensorflow.gfile.Open() error with MNIST example in python 3.4", "body": "The `tf.gfile.Open(filename)` function throws an error when a bytestream is read. The issue arises in tensorflow/examples/tutorials/mnist using python 3.4\n### Environment info\n\nOperating System:\nUbuntu LTS 14.04 64bit\n1. pip package installed: \n   `sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.0-cp34-none-linux_x86_64.whl`\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\". \n   `python3 -c \"import tensorflow; print(tensorflow.__version__)\"\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n   0.7.0\n   `\n   ### Steps to reproduce\n   Run the mnist example in tensorflow with python3\n   `from tensorflow.examples.tutorials.mnist import input_data\n   mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)`\n### Steps to overcome\n\nGot rid of the tf.gfile.Open function call. Replaced the line:\n`with tf.gfile.Open(filename) as f, gzip.GzipFile(fileobj=f) as bytestream:`\nwith\n`with gzip.GzipFile(filename=filename) as bytestream:`\n### Logs\n\n`Extracting MNIST_data/train-labels-idx1-ubyte.gz`\n`Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.`\n`Extracting MNIST_data/train-images-idx3-ubyte.gz`\n`Traceback (most recent call last):`\n`File \"<stdin>\", line 1, in <module>`\n`File \"/usr/local/lib/python3.4/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 202, in read_data_sets`\n`train_images = extract_images(local_file)`\n`File \"/usr/local/lib/python3.4/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 58, in extract_images`\n`magic = _read32(bytestream)`\n`File \"/usr/local/lib/python3.4/dist-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 51, in _read32`\n`return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]`\n`File \"/usr/lib/python3.4/gzip.py\", line 365, in read`\n`if not self._read(readsize):`\n`File \"/usr/lib/python3.4/gzip.py\", line 433, in _read`\n`if not self._read_gzip_header():`\n`File \"/usr/lib/python3.4/gzip.py\", line 292, in _read_gzip_header`\n`magic = self.fileobj.read(2)`\n`File \"/usr/lib/python3.4/gzip.py\", line 90, in read`\n`self.file.read(size-self._length+read)`\n`File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_gfile.py\", line 45, in sync`\n`return fn(self, *args, **kwargs)`\n`File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_gfile.py\", line 199, in read`\n`return self._fp.read(n)`\n`File \"/usr/lib/python3.4/codecs.py\", line 319, in decode`\n`(result, consumed) = self._buffer_decode(data, self.errors, final)`\n`UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte`\n", "comments": ["This was fixed in 555e73da8f171992085c68614f74b23b8180292c, which we will hopefully cherry-pick to 0.7.1 soon.\n", "Thank you @vrv \n", "We have some new candidate wheels that have the fix here: https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/g3doc/get_started/os_setup.md -- let us know if those are good for you.\n", "Thank you @vrv \n"]}]