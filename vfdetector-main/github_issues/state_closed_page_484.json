[{"number": 39270, "title": "train_on_batch fails with MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX1080Ti\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running `train_on_batch` on a model created under a distribution strategy such as MirroredStrategy the error `ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.` is thrown\r\n\r\nPutting the `train_ob_batch` into the strategy scope does not change that error. However when doing so with a model containing a batch normalization layer (e.g. Resnet50) it throws a different error: `RuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.` (the other error location is not reached in that case as it bails out with the above)\r\n\r\n**Describe the expected behavior**\r\n\r\n`train_on_batch` should work\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10),\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n\r\nfor x, y in train_dataset:\r\n    model.train_on_batch(x, y)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nFull traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_issue_train_on_batch.py\", line 24, in <module>\r\n    model.train_on_batch(x, y)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1078, in train_on_batch\r\n    standalone=True)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:305 train_on_batch  *\r\n        outs, total_loss, output_losses, masks = (\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:273 _process_single_batch\r\n        model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:444 apply_gradients\r\n        kwargs={\"name\": name})\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1949 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1956 _merge_call\r\n        return merge_fn(self._strategy, *args, **kwargs)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:485 _distributed_apply\r\n        scope_name), distribution.extended.colocate_vars_with(var):\r\n    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__\r\n        return next(self.gen)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4112 _colocate_with_for_gradient\r\n        with self.colocate_with(op, ignore_existing):\r\n    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__\r\n        return next(self.gen)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4161 colocate_with\r\n        op = _op_to_colocate_with(op, self)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:6548 _op_to_colocate_with\r\n        if hasattr(v, \"handle\") and isinstance(v.handlTraceback (most recent call last):\r\n  File \"tf_issue_train_on_batch.py\", line 24, in <module>\r\n    model.train_on_batch(x, y)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1078, in train_on_batch\r\n    standalone=True)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:305 train_on_batch  *\r\n        outs, total_loss, output_losses, masks = (\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:273 _process_single_batch\r\n        model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:444 apply_gradients\r\n        kwargs={\"name\": name})\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1949 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1956 _merge_call\r\n        return merge_fn(self._strategy, *args, **kwargs)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:485 _distributed_apply\r\n        scope_name), distribution.extended.colocate_vars_with(var):\r\n    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__\r\n        return next(self.gen)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4112 _colocate_with_for_gradient\r\n        with self.colocate_with(op, ignore_existing):\r\n    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__\r\n        return next(self.gen)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4161 colocate_with\r\n        op = _op_to_colocate_with(op, self)\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:6548 _op_to_colocate_with\r\n        if hasattr(v, \"handle\") and isinstance(v.handle, Tensor):\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py:720 handle\r\n        raise ValueError(\"`handle` is not available outside the replica context\"\r\n\r\n    ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.\r\n\r\n```e, Tensor):\r\n    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py:720 handle\r\n        raise ValueError(\"`handle` is not available outside the replica context\"\r\n\r\n    ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.\r\n\r\n", "comments": ["@Flamefire,\r\nI was able to reproduce the error with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/dd941f06afa3d22db01cfe186c87a6d6/39270-2-1.ipynb). However, the issue seems to be fixed with [TF v2.2.0-rc4](https://colab.research.google.com/gist/amahendrakar/dccbfd95695100d154b7bf34e0f6ca0d/39270.ipynb). Please find the attached gist. Thanks!", "@amahendrakar I can confirm this. However I noticed a change in TF 2.2 to 2.1: Using `train_on_batch` in a tf.function is no longer possible as it errors out with `AttributeError: 'Tensor' object has no attribute 'numpy'`. Is `train_on_batch` supposed to be called outside a tf.function? My understanding was to put as much as possible into tf.functions.\r\n\r\nThis used to work in TF 2.1\r\n\r\nSee gist: https://colab.research.google.com/drive/1e3UwhKhQUFyYh5GsIO3TWOmvMynAPJUg?usp=sharing", "Was able to run the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/4aa161d93375269ac26a929feca8f0b1/39270-2-1.ipynb). \r\n\r\nHowever, running the code with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/5fbc906723702078352d3ea97464af9f/39270-2-2.ipynb#scrollTo=ZvOt6Ry6X7b-) throws an error stating `    AttributeError: 'Tensor' object has no attribute 'numpy'`. Please find the attached gist. Thanks!", "As a note on that issue: It seems that it converts the Tensor (which is likely on GPU, isn't it?) to a numpy type (on CPU, AFAIK numpy types can't be on GPU), which involves a Device->Host copy and a synchronization. The value converted is the metric and loss values.\r\n\r\nSo doing that on every step seems wasteful to me as it may not be used at all. So it might be a good idea not to convert that eagerly into a numpy type which would resolve the AttributeError and maybe improve performance.", "@Flamefire the original issue of train_on_batch failed with MirroredStrategy is fixed in 2.2.0.\r\n\r\nThe second issue of Attribute Error when using train_on_batch with tf.function \r\nmodel.train_on_batch is disallowed inside a tf.function , since it is a high-level end point that manages its own tf.function since it creates an Iterator, converts results to NumPy, etc. The best thing to use is Model.train_step, which is just the logic of the training step and is safe to tf.function. \r\n\r\nPlease let us know if that answers your questions and it is ok to close this issue. ", "Just for my understanding: In another issue I was told that `tf.function` is supposed to be the replacement for the TF 1 \"graph mode\" to recover (some of) the lost performance due to eager execution.   \r\nHowever it now seems to be impossible to create a whole training loop or at least a single epoch iteration in \"graph mode\" (`tf.function`) as e.g. calling `train_on_batch` inside a `tf.function` is (now?) disallowed. So this still looses quite some performance especially because of the (potentially unneccessary) conversion to numpy types after each batch. To my understanding feeding the GPU new batches as fast as possible without first waiting on the result of previous computations is vital in leveraging the full performance of the GPU.\r\n\r\nMy point in using `train_on_batch` over `model.train_step` (besides the latter didn't exist in 2.1) was to remove as much overhead as possible as I figure `model.fit` does quite some work so it works with any model and any kind of inputs and outputs.\r\n\r\nSo bottom line: Shouldn't also \"high level functions\" like `rain_on_batch` be able to be put in `tf.function` so at least parts of the overhead are removed? I suppose quite some checks can be effectively removed when that functions is compiled for a given model and dataset type, and not going through all those Python abstractions but executing (compiled) binary code instead would be a win.", "@Flamefire I executed the code in TF 2.5 and didnt face any errors. Could you please confirm and let us know if we can close the issue.", "Thanks for the update. Well there is at least an open question:\r\n\r\n>  Shouldn't also \"high level functions\" like train_on_batch be able to be put in tf.function so at least parts of the overhead are removed? I suppose quite some checks can be effectively removed when that functions is compiled for a given model and dataset type, and not going through all those Python abstractions but executing (compiled) binary code instead would be a win.\r\n\r\nSee my previous message for details.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39270\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39270\">No</a>\n"]}, {"number": 39267, "title": "CUDA_ERROR_LAUNCH_FAILED when training RNNs", "body": "I'm getting the following error when training LSTMs or GRUs\r\n\r\nI tensorflow/stream_executor/stream.cc:1868 [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not wait for [stream=0000019AB6A73AF0,impl=0000019ACE30D4B0]\r\nI tensorflow/stream_executor/stream.cc:4816 [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not memcpy host-to-device; source: 0000019F7DB15080\r\nI tensorflow/stream_executor/stream.cc:1868] [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not wait for [stream=0000019AB6A73AF0,impl=0000019ACE30D4B0]\r\nI tensorflow/stream_executor/stream.cc:4816] [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not memcpy host-to-device; source: 0000019A943627C0\r\nE tensorflow/stream_executor/stream.cc:332] Error recording event in stream: error recording CUDA event on stream 0x19ad7cd0600: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\nE tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n[I 13:52:21.664 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\n\r\nThe problem occurs only on the newly bought machine, not on the previously used one, although both have the same setup with same tensorflow and CUDA versions:\r\n\r\nIntel Core i9 9960X\r\nNvidia Quadro RTX 6000\r\nWindows 10\r\nTensorflow 2.0\r\nCUDA 10.1\r\ncuDNN 7.6.5\r\n\r\nAlso, all users properly initialize the GPU in the jupyter notebooks to allow for multiple notebooks being trained with\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(enable=True,device=gpus[0])\r\n\r\nI would appreciate any help, since I haven't found any solution for this on the web.\r\n\r\nThanks!\r\n\r\n\r\n\r\n", "comments": ["@AndreasPangerl,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "\r\n    def create_model(stateful, num_lstm, size_lstm, num_dense, size_dense, batch_size, optimizer):\r\n    \r\n    model = Sequential()\r\n    model.add(Masking(mask_value=0.0))\r\n\r\n    model.add(Bidirectional(merge_mode='ave',layer=LSTM(200,\r\n                  input_shape=X_train.shape[1:3],\r\n                  batch_size=batch_size,\r\n                  stateful=stateful,\r\n                  return_sequences=True),\r\n                  batch_input_shape=(batch_size, timeSteps, numFeats)))\r\n\r\n    model.add(Bidirectional(merge_mode='ave',layer=LSTM(100,\r\n                  input_shape=X_train.shape[1:3],\r\n                  batch_size=batch_size,\r\n                  stateful=stateful,\r\n                  return_sequences=True),\r\n                  batch_input_shape=(batch_size, timeSteps, numFeats)))\r\n\r\n    model.add(Dense(100,activation='selu',kernel_initializer=lecun_normal()))\r\n    model.add(Dense(50,activation='selu',kernel_initializer=lecun_normal()))\r\n    model.add(Dense(25,activation='selu',kernel_initializer=lecun_normal()))\r\n    model.add(Dense(1,activation=\"linear\"))\r\n  \r\n    model.compile(loss=losses.mean_squared_error, optimizer=optimizer)\r\n    return model\r\n\r\n    model_stateful =create_model(stateful=False,optimizer=Adam(lr=0.001),batch_size=200\r\n\r\n    ModelCpt = ModelCheckpoint(\"model_1.h5\", monitor=\"loss\", save_best_only=True)\r\n    ReduceLRCpt = ReduceLROnPlateau(monitor='loss', factor=0.85, patience=5, min_lr=1e-7,verbose=1)\r\n\r\n    history_stateful = model_stateful.fit(X_train,\r\n                                      Y_train,\r\n                                      batch_size=batch_size_aux,epochs=2500,verbose=1,\r\n                                      validation_data=(X_test, Y_test),\r\n                                      callbacks=[ModelCpt,ReduceLRCpt],\r\n                                      shuffle=False))\r\n\r\n\r\n\r\nDo you need more than the above code? Unfortunately I cannot share the data used for this.\r\n", "@AndreasPangerl,\r\nWhile running the above code, I'm facing a SyntaxError stating `ModelCpt = ModelCheckpoint(\"model_1.h5\", monitor=\"loss\", save_best_only=True)`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/be3d879f23f2a89d9cc1970fe6b917c2/39267.ipynb).\r\n\r\nCould you please provide the completed code and a sample data to reproduce this issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39267\">No</a>\n"]}, {"number": 39266, "title": "Java buffer: Allow returned output length in a larger allocated fixed destination buffer.", "body": "Be sure to allocate enough bytes before (e.g. using the experimental ctc_beam_search_decoder in TFLite).\r\n\r\nThe check against the exact source buffer length with the exact destination buffer length is not useful in Java, even if you use it in the original way because then you know the exact expected returned length (you can still check in your code).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39266) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39266) for more info**.\n\n<!-- ok -->", "Hi Jared, can you take a look?", "@cefaci Can you please check @jdduke's comments and keep us posted. Thanks!", "I'm still thinking about how best to structure the code to handle this. ", "How about passing an attribute (e.g. boolean) to Tensor.java over the NativeInterpreterWrapper.java to allow a dynamic output (> instead of !=)? ", "Thanks! Looks good! ", "Let us know if you run into any issues, and thanks for your patience!", "@jdduke I will definitely, no the other way round, thank you all for your commitment and time! :)\r\n\r\n"]}, {"number": 39265, "title": "Tensorflow release 2.2.0 not found via `pip install`", "body": "**tldr;** pip does not find the new release 2.2.0 published yesterday\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not Applicable\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Server 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): Not Applicable\r\n- Python version: 3.8.2\r\n- PIP version: 20.1\r\n- CUDA/cuDNN version: Not Applicable (10.2)\r\n- GPU model and memory: Not Applicable (RTX 2080 Ti)\r\n\r\n**Describe the current behavior**\r\nFrom the change log and the Github releases I noticed that Tensorflow 2.2.0 was released and that I could upgrade the Tensorflow version currently used in my project (2.2.0rc4). Sadly, `pip install` failed to install the update as it was unable to find release 2.2.0. (Clearing the cache of pip didn't help.) The PyPi website also says latest version is the rc4, not the [2.2.0 release](https://pypi.org/project/tensorflow/#history).\r\n\r\n**Describe the expected behavior**\r\nWell, I would like to be able to install Tensorflow 2.2.0 over `pip`.\r\n\r\n**Standalone code to reproduce the issue**\r\n`pip install tensorflow==2.2.0`\r\n\r\n**Other info / logs**\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0  (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4)\r\nERROR: No matching distribution found for tensorflow==2.2.0 (from -r projects/hohl.thesis/requirements.txt (line 3))\r\n```", "comments": ["It seems the pips didn't upload yet. Looking into it", "@hohl thanks for reporting the issue, the pips are being uploaded now. Sorry for the confusion, we are fixing the automation on our end. ", "@hohl After the wheels are available at https://pypi.org/project/tensorflow/2.2.0/#files, you might face another problem with the CUDA version compatibility. TF 2.2 is built to work with CUDA 10.1. CUDA 10.2 support requires a manual build from source.\r\n\r\nBut you can add CUDA 10.1 in addition to the existing 10.2 using offline install and keeping the latest driver and adjust the paths for the tensorflow python env, so it's quite easily doable. https://twitter.com/ahtik/status/1238079762758807554 has a few hints, maybe helps.", "No need to build from source, can also be done by symlinking 10.2 to 10.1.", "@mihaimaruseac Thanks! Is this the proper [symlinking procedure to use for CUDA 10.2 with the public release wheels](https://github.com/tensorflow/profiler/blob/master/docs/profile_multi_gpu.md#linux-setup)? As I understand, this instruction assumes that the Linux machine has only CUDA 10.2 installed (and not 10.1 in parallel, which is a rare but possible use case).\r\n\r\nAny ideas if something similar is possible on Windows? I couldn't figure it out by myself without a build from source.\r\nEDIT: It's probably possible by symlinking cudart64_102.dll to cudart64_101.dll, but as I've already built the wheel for CUDA 10.2 and like to keep both CUDA versions for various testing reasons, will not go that route.", "Thanks for fixing that so quick. Release 2.2.0 was now found through `pip install`.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39265\">No</a>\n", "(venv) C:\\>pip install tensorflow==2.2.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.2.0\r\n\r\nstill getting error. \r\n\r\n(venv) C:\\>python --version\r\nPython 3.8.0\r\n\r\n(venv) C:\\>pip --version\r\npip 20.1.1 from c:\\venv\\lib\\site-packages\\pip (python 3.8)", "Today, I've re-installed the my whole Linux environment for tensorflow2.x because of the bug in \r\nhttps://github.com/tensorflow/tensorflow/issues/40278\r\nBut after I installed the GPU driver, I found I still couldn't install tensorflow 2.20. I've tried many many commands: \r\n```\r\n python3 -m pip install tensorflow==2.2.0\r\nCollecting tensorflow==2.2.0\r\n  Cache entry deserialization failed, entry ignored\r\n  Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow==2.2.0\r\n\r\npip3 install tensorflow==2.2.0\r\nCollecting tensorflow==2.2.0\r\n  Cache entry deserialization failed, entry ignored\r\n  Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow==2.2.0\r\n\r\n\r\n```\r\n\r\nWhy tensorflow2.2.0 disappeared again in pip3 repos? \r\nAnd another question: why tensorflow2.2.0 still use pip to install its package? As I know, for all python3 package we should use pip3 instead of pip, my understanding is right? But according to here:\r\nhttps://www.tensorflow.org/install/pip\r\nI feel the pip&pip3 usage is very messy. And until now, I couldn't find how to install Tensorflow2.2.0 with GPU support.", "@clockzhong please update `pip` on your container (`python3 -m pip install --upgrade pip`)", "@mihaimaruseac Thanks! Yes, it works now. I was totally confused on the pip&pip3&python&python3.", "The issue still persists.", "@rj1024 please update `pip` and make sure your Python is on 64 bits. If after these two steps the issue still persists, please open a new issue, filling in issue template. This one has been solved many times already (by upgrading `pip`)"]}, {"number": 39264, "title": "E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): msvc\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10, cudnn 7.4\r\n- GPU model and memory: gtx 1650\r\n\r\n\r\n\r\n**Describe the problem**\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": [" Resource exhausted: OOM when allocating tensor with shape[10,64,800,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc", "@noronhahaha \r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39264\">No</a>\n"]}, {"number": 39262, "title": "Tensorflow v2.2.0 build failure at the final step on macOS 10.13.6 with CUDA enabled", "body": "I've been building [wheel packages of Tensorflow with CUDA support for macOS](https://github.com/TomHeaven/tensorflow-osx-build/releases) for a while. Usually, TF will be built successfully with some patches on sources and bazel config files. \r\n\r\nHowever, I met a troublesome issue building Tensorflow v2.2.0 at the final step: The compiler complained about **Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE** when loading _pywrap_tensorflow_internal.so. The details are as follows:\r\n\r\n### System information\r\n- **Mac OS 10.13.6**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 2.2.0\r\n- **Python version**: 3.7.0\r\n- **Bazel version (if compiling from source)**: 2.0.0\r\n- **GCC/Compiler version (if compiling from source)**: AppleClang++ 9.0\r\n- **CUDA/cuDNN version**: 10.0/7.4\r\n- **GPU model and memory**: Nvidia Titan V\r\n- **Exact command to reproduce**:\r\n```\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --config=nonccl --config=monolithic --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n- **Build Failure Info**:\r\n```\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /Volumes/Data/libraries/tensorflow/tensorflow/python/keras/api/BUILD:117:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE\r\n  Referenced from: /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE\r\n  Referenced from: /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /Volumes/Data/libraries/tensorflow/tensorflow/python/tools/BUILD:82:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nINFO: Elapsed time: 0.967s, Critical Path: 0.31s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@TomHeaven \r\nCould you please have a look at below issues with similar error.\r\n\r\n#35584 #34429 #36945 #34117", "> @TomHeaven\r\n> Could you please have a look at below issues with similar error.\r\n> \r\n> #35584 #34429 #36945 #34117\r\n\r\nThanks for the reply. The issue seems the most similar to #35584, but the cause is different.\r\n\r\nI've figured it out. The problem is in `tensorflow/core/kernels/data/experimental/snapshot_util.h:73-77`:\r\n```cpp\r\nclass SnapshotReader {\r\n public:\r\n   //The reader input buffer size is deliberately large because the input reader\r\n   //will throw an error if the compressed block length cannot fit in the input\r\n   //buffer.\r\n  static constexpr const int64 kSnappyReaderInputBufferSizeBytes =\r\n      1 << 30;  // 1 GiB\r\n  // TODO(b/148804377): Set this in a smarter fashion.\r\n  static constexpr const int64 kSnappyReaderOutputBufferSizeBytes =\r\n  /   32 << 20;  // 32 MiB\r\n...\r\n``` \r\nThere are two variables `kSnappyReaderInputBufferSizeBytes` and `kSnappyReaderOutputBufferSizeBytes` declared within  class SnapshotReader and referenced in `tensorflow/core/kernels/data/experimental/snapshot_util.cc`. Somehow, the constant variables in the class are compiled as indirect symbols that cannot be found in flat namespace on macOS using AppleClang++.\r\n\r\nThe solution is quite simple. I just move those two variable declarations from `snapshot_util.h` to `snapshot_util.cc:32-40` such as\r\n```cpp\r\nnamespace tensorflow {\r\nnamespace data {\r\nnamespace experimental {\r\n// Tom Added to solve symbol not found error on macOS\r\nstatic constexpr const int64 kSnappyReaderInputBufferSizeBytes =\r\n    1 << 30;  // 1 GiB\r\n    // TODO(b/148804377): Set this in a smarter fashion.\r\nstatic constexpr const int64 kSnappyReaderOutputBufferSizeBytes =\r\n    32 << 20;  // 32 MiB\r\n```\r\nand re-compile Tensorflow.\r\n\r\nI'll try to make a pull request to close this issue.", "I created a pull request https://github.com/tensorflow/tensorflow/pull/39297 to solve this issue and multiple other compilation errors on macOS with CUDA enabled.", "Thank you @TomHeaven I will be eagerly awaiting the result of this PR.", "@TomHeaven \r\n\r\nPlease confirm if we may move this issue to closed status as there is a pr to monitor it", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39262\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39262\">No</a>\n"]}, {"number": 39261, "title": "Im triying to use rasa inside Anaconda and using \"rasa init\" command to create a rasa project I get errors!", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Im triying to use rasa inside Anaconda and using \"rasa init\" command to create a rasa project I get errors\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 home with Anaconda\r\n \r\n- TensorFlow version (use command below): tensorflow-estimator==2.1.0 (Anaconda virtualenv)\r\n- Python version: python-3.6.10\r\n- GPU model and memory: dell x-64, 4GB, i3\r\n\r\n**(rasa) C:\\Users\\carlo>rasa init**\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Uma rotina de inicializa\u00e7\u00e3o da biblioteca de v\u00ednculo din\u00e2mico (DLL) falhou.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\carlo\\anaconda3\\envs\\rasa\\Scripts\\rasa.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\rasa\\__main__.py\", line 82, in main\r\n    set_log_level(log_level)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\rasa\\utils\\common.py\", line 71, in set_log_level\r\n    update_tensorflow_log_level()\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\rasa\\utils\\common.py\", line 112, in update_tensorflow_log_level\r\n    import tensorflow as tf\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\carlo\\anaconda3\\envs\\rasa\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Uma rotina de inicializa\u00e7\u00e3o da biblioteca de v\u00ednculo din\u00e2mico (DLL) falhou.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@carlomcz \r\n\r\nFor Anconda related issues, please open an issue in [Anaconda repo.](https://github.com/ContinuumIO/anaconda-issues/issues) for faster resolution.We can close the issue here and we can track the issue in Anaconda repo. Please, confirm.Thanks!", "@carlomcz \r\n\r\nIt may occur because you may not have Visual C++ Redistributable for Windows. Please download it from [here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).Please, check Your CPU/Python is on 32 bits?Please, refer #36167 .Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39261\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39261\">No</a>\n"]}, {"number": 39260, "title": "Cross Compile tensorflow for arm using a different python version ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Host linux ubuntu 18.04 amd64 , Target : linux ubuntu 18.04 arm 32 bit \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0\r\n- Python version:  3.6.9\r\n- Installed using virtualenv? pip? conda?:  python venv\r\n- Bazel version (if compiling from source):  4.0.0\r\n- GCC/Compiler version (if compiling from source):  arm-linux-gnueabihf-gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no \r\n\r\nHi,\r\ni am cross compiling tensorflow ( version 2.1.0 ) since i need the new delegate features for my hardware accelerator. \r\nI tried to install directly tensorflow lite from the official binaries for linux arm 32 however when i run a model i get the \"illegal instruction\" which is probably due to different floating point support of the processor and/or avx instructions (those ideas come from other github issue)\r\nMy board is the Pynq Z2 from tul with arm processor on 32 bit architecture running a linux based OS. I am following the procedure for building  tensorflow from source for the rasberry ( i changed some of the compiler flags for correctly match my architecture) however i cannot change the python version of the wheel ( i need a cp36 instead of a cp35 ) from the build process. Is there any workaround?\r\n\r\nIf you are wondering why i did not install python3.5, it is becouse there may be some incompatibility with the pynq package. \r\n\r\nPS i tried to change brutally the name of the wheel, from cp35 to cp36, but when i import tensorflow into the python enviroment i get the incompatibility error of the two version.\r\n", "comments": ["HI franout,\r\n\r\nCan you give more details about the steps you have tried? Also want to ask if you want to build whl for TensorFlow, or TensorFlow Lite. These are the instructions I'm aware of for each packages, but not sure where you encountered wheel version issue.\r\n\r\nTensorFlow: https://www.tensorflow.org/install/source_rpi#python-3\r\nTensorFlow Lite: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package\r\n\r\nFor following instruction for building TFLite package, you might need to change include path of `pybind11`. See https://github.com/tensorflow/tensorflow/issues/38903 for details.", "my first try was to cross compile tensorflow lite for the zynq board, but i got the error about the pybind11 includes ( i didn't find the #38903 ). \r\nTherefore i cross compiled the entire tensorflow, without any problems following the guide you already cited (cross compile tensorflow for raspberry). Following that i obtained a wheel for python3.5 ,My question is: Is possible to produce a wheel for python3.6 instead?\r\n\r\nAnyway, I tested the wheel for python3.5 on the board ( i needed to build  python3.5 from scratch on the board) and it is up and running now.\r\n\r\n [Here](https://github.com/franout/tensorflow_for_pynqz2)   you can findthe wheel for the Pynq Z2 board ( zynq 7000 xilinx SoC) and the modified build script of the raspberry.\r\n\r\n", "Update CROSSTOOL_PYTHON_INCLUDE_PATH to point Python3.6 if you want to produce a wheel for Python 3.6\r\n https://www.tensorflow.org/install/source_rpi#python-3", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39260\">No</a>\n"]}, {"number": 39259, "title": "TensorFlowLite app's camera doesn't focus on objects - produces blurry images", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No.**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10+ and Samsung Galaxy A70 and Samsung Galaxy S7 Edge (I tried on many diff. devides)\r\n- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/examples.git\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm trying to achieve this image quality:\r\n[https://i.stack.imgur.com/tPfdg.jpg](https://i.stack.imgur.com/tPfdg.jpg)\r\n\r\n**Describe the expected behavior**\r\nI'm achieving this image quality (currently):\r\n[https://i.stack.imgur.com/qD8uh.jpg](https://i.stack.imgur.com/qD8uh.jpg)\r\n\r\n**Standalone code to reproduce the issue**\r\nAll the info. can be found in the following link:\r\n[https://stackoverflow.com/questions/61611050/tensorflows-app-camera-images-are-blurry](https://stackoverflow.com/questions/61611050/tensorflows-app-camera-images-are-blurry)\r\n\r\nAlso, I've added most of it here to. (see below)\r\n\r\nSet up the working directory\r\n\r\ngit clone https://github.com/tensorflow/examples.git\r\nOpen the project with Android Studio\r\n\r\nOpen a project with Android Studio by taking the following steps:\r\n\r\nOpen Android Studio. After it loads select \" Open an existing Android Studio project\".\r\n\r\nIn the file selector, choose examples/lite/examples/image_classification/android from your working directory to load the project.\r\n\r\nIn `LegacyCameraConnectionFragment.java`, in the function `onSurfaceTextureAvailable`\r\n\r\nAdd in line 90~ this code is added to turn on the flash all the time (it's off by default):\r\n\r\n```\r\nList<String> flashModes = parameters.getSupportedFlashModes();\r\nif (flashModes.contains(android.hardware.Camera.Parameters.FLASH_MODE_TORCH))\r\n{\r\n  parameters.setFlashMode(parameters.FLASH_MODE_TORCH);\r\n}\r\n```\r\n\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nIt's simple, please take a cup or any other object, which you can put your phone over it, put a small object into it, for example like a gum, or some sort of a pill w/ text in it, and see if the image is blurry or not w/ TensorFlow's app.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@AnaRhisT94 \r\nCould you please share a simple stand alone code for us to replicate this issue, code provided is not sufficient to replicate the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Could you confirm that the issue is still happening without your modification of parameters.setFlashMode() ?", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39258, "title": "Adding Instruction to install tf using Conda", "body": "It's a well know fact that the Installation of tf-gpu might become a difficult process . \r\n\r\nTo solve this issue we can use the method of installation through conda .\r\n\r\nConda provide a single line solution to install all the files required like cuda , cdunn etc.\r\n\r\nI hope adding this step as a Installation process will help many user to \r\n quickly start with tensorflow-gpu.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39258) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39258) for more info**.\n\n<!-- ok -->", "Hi \r\nI am waiting for a reply \r\nPlease review it . \r\nIf you want to reject you can do that also \r\n\r\nThanks", "We don't own/test/support TensorFlow with conda, so it's hard for us to include this in the official docs.\r\n\r\nRight @lamberta?", "Hey Mark \n\nI know you don't support the conda .\n\nBut still conda is one of the way  to use tensorflow .\n\nThere are lot of problem which the user face in installing the tensorflow-gpu .As you also have to install Cuda , Cudnn  etc .\n\nIts better to use a single line command using conda . This will automatically install the most appropriate version of cuda , Cudnn etc .\n\nIt would be great if you include this installing process also in your official docs.\n\nThanks", "We mention/link to Conda [here in the pip install guide](https://www.tensorflow.org/install/pip?hl=en&lang=python3#2.-create-a-virtual-environment-recommended), but Conda is not officially supported (and often lags behind). I think the TensorFlow Docker images would be a better recommendation.\r\n\r\nI'm open to a brief comment and link to the Conda section in the TF install guide, but including commands here seems out-of-scope.\r\n\r\ncc: @gunan ", "Okay should I move on to the  \"  TensorFlow Docker images \" installation . If you are okay including that . ", "I agree, if we document conda here, then we open ourselves to questions about why conda packages are available much later than pip packages, bugs in conda due to conda using different build options.\r\nTF builds with manylinux 2010 standard, and builds a whole ecosystem around using the same toolchains. Conda releases use a different toolchain, that can cause incompatibilities with pip packages, or custom ops, or other packages under tensorflow umbrella, like addons.\r\n\r\nSo, I am definitely against having full instructions in TF website.\r\nIf Anaconda has a page with TF instructions, we can point to there.\r\nSo, we definitely cannot accept this change.", "> Okay should I move on to the \" TensorFlow Docker images \" installation .\r\n\r\n@shubham15gupta09 Yes, we should link to this section: https://www.tensorflow.org/install/docker#gpu_support", "Hi @lamberta  @MarkDaoust @MarkDaoust \r\nWe can include these links to the tf installation instructions as Anaconda has instructions to install tensorflow .\r\nI think it would be better to give these as a link .\r\n\r\nhttps://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\nhttps://www.anaconda.com/blog/tensorflow-in-anaconda\r\nhttps://anaconda.org/conda-forge/tensorflow"]}, {"number": 39257, "title": "\"OverflowError: cannot serialize a bytes object larger than 4 GiB\" when using model.fit([...], use_multiprocessing=True) on custom generator", "body": "Hi,\r\n\r\nI'm trying to train a model consisting of some GRU layers on data stored in a large numpy array (~18gb) on 2 GPUs using the MirroredStrategy.\r\n\r\nMy system:\r\n* AMD Ryzen Threadripper 3960X 24-Core Processor \r\n* 64 GB RAM\r\n* two NVIDIA GeForce RTX 2070 SUPER with 8192MiB each\r\n* Win 10 (unfortunately, the ASrock Creator TRX40 motherboard we bought is currently incompatible with Linux, wtf...)\r\n* TF 2.1.0 installed from binary (anaconda)\r\n* Python 3.7.7\r\n* CUDA Version 10.2.89\r\n\r\nThis is my code:\r\n\r\n```python\r\nimport tensorflow as tf \r\nimport numpy as np \r\n\r\ntrain_tokens_X = np.zeros((1066673, 61, 69), dtype=np.float32)\r\ntrain_tokens_X[:] = np.eye(69)[:61,:]\r\n\r\ntrain_target = np.zeros((1066673, 3943), dtype=np.float32)\r\ntrain_target[:,2] = 1\r\n\r\nvalid_tokens_X= np.zeros((133366, 61, 69), dtype=np.float32)\r\nvalid_tokens_X[:] = np.eye(69)[:61,:]\r\n\r\nvalid_target= np.zeros((133366, 3943), dtype=np.float32)\r\nvalid_target[:,2] = 1\r\n\r\noutput_size = 3943\r\nmax_id = 68\r\nbatch_size = 256\r\n\r\nclass Sequencer(tf.keras.utils.Sequence):\r\n\r\n    def __init__(self, x_set, y_set, batch_size):\r\n        self.x, self.y = x_set, y_set\r\n        self.batch_size = batch_size\r\n\r\n    def __len__(self):\r\n        return math.ceil(len(self.x) / self.batch_size)\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x = self.x[idx * self.batch_size:(idx + 1) *\r\n        self.batch_size]\r\n        batch_y = self.y[idx * self.batch_size:(idx + 1) *\r\n        self.batch_size]\r\n\r\n        return np.array(batch_x), np.array(batch_y)\r\n\r\n\r\ntrain_generator = Sequencer(train_tokens_X, train_target, batch_size)\r\nvalid_generator = Sequencer(valid_tokens_X, valid_target, batch_size)\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n\r\nwith mirrored_strategy.scope():\r\n    model = keras.models.Sequential([\r\n        keras.layers.GRU(128, return_sequences=True, input_shape=[ None, max_id+1], use_bias=False),\r\n        keras.layers.GRU(128, return_sequences=True, use_bias=False),\r\n        keras.layers.GRU(128, use_bias=False),\r\n        keras.layers.Flatten(),\r\n        keras.layers.Dense(output_size, activation=\"softmax\")\r\n    ])\r\n    model.compile(loss=[focal_loss_umbertogriffo.categorical_focal_loss(alpha=.25, gamma=2)], optimizer=\"adam\", metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_generator, validation_data=valid_generator, epochs=25, callbacks = callbacks, max_queue_size=10, workers=2, use_multiprocessing=True)\r\n```\r\n\r\nand there it crashes with:\r\n\r\n```\r\nException in thread Thread-12:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\data_utils.py\", line 844, in _run\r\n    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\data_utils.py\", line 823, in pool_fn\r\n    initargs=(seqs, None, get_worker_id_queue()))\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\multiprocessing\\context.py\", line 119, in Pool\r\n    context=self.get_context())\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\multiprocessing\\pool.py\", line 176, in __init__\r\n    self._repopulate_pool()\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\multiprocessing\\pool.py\", line 241, in _repopulate_pool\r\n    w.start()\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\multiprocessing\\process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\multiprocessing\\context.py\", line 322, in _Popen\r\n    return Popen(process_obj)\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\multiprocessing\\popen_spawn_win32.py\", line 89, in __init__\r\n    reduction.dump(process_obj, to_child)\r\n  File \"C:\\Users\\KI\\anaconda3\\envs\\tensorflow_test\\lib\\multiprocessing\\reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nOverflowError: cannot serialize a bytes object larger than 4 GiB\r\n```\r\n\r\nIf trained without the `use_multiprocessing ` flag, training works seamlessly, but is very slow due to the generator overhead (I guess) with only ~10% load on each GPU and also ~10% on the CPU.\r\n\r\nIf trained without the using the generator at all (putting the data directly into model.fit), model.fit crashes due to memory issues:\r\n\r\n`2020-05-07 12:46:43.785479: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16823566556 exceeds 10% of system memory.`\r\n\r\n...so I have to use a generator for memory efficiency.\r\n\r\nI also tried to use a tf.data pipeline but the same `exceeds 10% of system memory` arises, unfortunately. \r\n\r\nWhat can I do here?", "comments": ["@philipp-ghtest \r\n\r\nWill it be possible to share supporting data files to reproduce the issue in our environment.Thanks!", "> @philipp-ghtest\r\n> \r\n> Will it be possible to share supporting data files to reproduce the issue in our environment.Thanks!\r\n\r\nMy data is proprietary but I'll try to provide some fake data since training doesn't start anyways", "> @philipp-ghtest\r\n> \r\n> Will it be possible to share supporting data files to reproduce the issue in our environment.Thanks!\r\n\r\nThis code creates fake data with the same size and shape as the real data. This should do it for this purpose:\r\n\r\n```\r\ntrain_tokens_X = np.zeros((1066673, 61, 69), dtype=np.float32)\r\ntrain_tokens_X[:] = np.eye(69)[:61,:]\r\n\r\ntrain_target = np.zeros((1066673, 3943), dtype=np.float32)\r\ntrain_target[:,2] = 1\r\n\r\nvalid_tokens_X= np.zeros((133366, 61, 69), dtype=np.float32)\r\nvalid_tokens_X[:] = np.eye(69)[:61,:]\r\n\r\nvalid_target= np.zeros((133366, 3943), dtype=np.float32)\r\nvalid_target[:,2] = 1\r\n\r\noutput_size = 3943\r\nmax_id = 68\r\nbatch_size = 256\r\n```", "Hey, any updates on this?", "@ymodak @ravikyram any updates on this?", "Apologies for the delay in response. I was able to reproduce crashing behavior using google colab. I have updated the repro code in the issue thread above.\r\nIt fails without distribution strategy as well.  I will tag owners for this. \r\nThanks!", "> Apologies for the delay in response. I was able to reproduce crashing behavior using google colab. I have updated the repro code in the issue thread above.\r\n> It fails without distribution strategy as well. I will tag owners for this.\r\n> Thanks!\r\n\r\nThank you!", "Hi @philipp-ghtest , sorry for the delay in getting to this!\r\n\r\nSo, if I'm correctly understanding this, the crux of your issue is you have a very large numpy array that you're trying to use performantly as your training/evaluation data. Using a large pre-built numpy array is generally unlikely to be the most performant way to process your data.\r\n\r\nIf you're looking to get peak performance, we suggest reading your data from disk (or generating it) directly as a tf.data dataset. Depending on how your data is stored, maybe you'll find useful utils in tensorflow.io:\r\ngithub.com/tensorflow/io\r\n\r\nYou can also load csvs directly into a tf.data.dataset, and there's support for various other loading too.\r\n\r\n---------------------\r\n\r\n**Regarding the generator multiprocessing error:**\r\nWe recommend datasets for performance over trying to use a generator w/ multiprocessing.\r\n\r\nI believe various objects have to be pickled/unpickled for python multiprocessing to work. because your generator holds a reference to the full numpy array, serializing/deserializing the closures means the whole numpy object has to be pickled/unpickled, so you're running into fundamental python limitations here.\r\n\r\n-----------------\r\n\r\n**Memory error when putting the data directly into model.fit**\r\n\r\nThe path in model.fit that handles numpy arrays directly is optimized for performantly handling *small* numpy arrays as quickly as possible, rather than minimizing memory usage. It ends up forcing a copy of your data converted to a tensor into memory, which is what's causing your crash.\r\n\r\nAlthough we do recommend using tf.data instead of large numpy arrays, you're not the first person to have run into issues with this. We do have a separate path that can handle bigger arrays w/o forcing the data into memory, and I'll look into triggering that alternate path if we detect a numpy object is large.", "Hi Tomer, \r\n\r\nthank you for your elaborate reply and all your helpful hints.\r\nSince my code crashes when using tf.data on large numpy arrays (as I said, at first I get this 'exceeds 10% of system memory' and then it terminates without further notice...), I guess I have to redesign my preprocessing in order to avoid the large numpy arrays at all, but at least I have a way out now.", "@philipp-ghtest  Closing the issue as of now. Please feel free  to re-open the issue if you have any further quetions.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39257\">No</a>\n"]}, {"number": 39256, "title": "save tensorflow estimator based custom model for inference ", "body": "thanks for the example codes. Experimented with Text Classification for character_rnn (https://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/examples/learn/text_classification_character_rnn.py).\r\n\r\nHow can i write a serving_input_fn for it ? I want to save and restore this model\r\n\r\nextended the code to save but getting error, please help\r\n`from tensorflow.contrib.learn.python.learn.utils import input_fn_utils` \r\n`feature_spec = {\"feature\":tf.FixedLenFeature([100],tf.int64)}`\r\n`serving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)`\r\n and than\r\n`classifier.export_savedmodel(export_dir_base='model', serving_input_receiver_fn=serving_input_fn)`\r\n\r\nand getting this error\r\n\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'feature': <tf.Tensor 'ParseExample/ParseExample:0' shape=(?, 100) dtype=int64>}. Consider casting elements to a supported type.", "comments": ["@neuralminds,\r\nCould you please provide a minimal code snippet to reproduce the issue reported here and also the TensorFlow version you are using.\r\n\r\nI was able to run the code given in the link without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/fe4375a75821fcd036489ebff763321f/39256.ipynb). Thanks!", "@amahendrakar Thanks for a response. I am using Tensorflow 1.15\r\npls find the code snippet here (https://gist.github.com/neuralminds/ea46e94faebc2beb7af20a0b609f1485).\r\n\r\nI have exported the trained model as .pb files but when trying to restore for inference it throws error.\r\n\r\nAlso attaching rendered notebook here for reference\r\n[estimator save and restore example.zip](https://github.com/tensorflow/tensorflow/files/4599110/estimator.save.and.restore.example.zip)\r\n\r\n", "@neuralminds,\r\nOn running the code, I am facing an error stating `InvalidArgumentError: Name: <unknown>, Key: chars, Index: 0.  Data types don't match. Data type: float but expected type: int64\r\n\t [[{{node ParseExample/ParseExample}}]]`\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f4a7a6246a1004091b0e74845bf01305/39256.ipynb). Could you please confirm if this is the intended error. Thanks!", "yes, same error @amahendrakar ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39256\">No</a>\n"]}, {"number": 39255, "title": "Pull request", "body": "Pull request", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/39255\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com'>ReviewNB</a></i>", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39255) for more info**.\n\n<!-- need_sender_cla -->", "Hi @sushovan14, \r\n\r\nIt looks like you made some mistake with branches or merges and a bunch of the history is getting duplicated.\r\n\r\nPlease make a new PR with only your commits in it."]}, {"number": 39254, "title": "[XLA] Adapting HLO-to-LHLO-legalization to use Buffer Assignment", "body": "Using MLIR::BufferAssignmentPlacer in legalization from HLO to LHLO.", "comments": []}, {"number": 39253, "title": "[TF Lite C API] TfLiteInterpreterGetOutputTensor returns nullpointer in second iteration", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GNU/Linux aarch64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): gcc version 7.4.0 (Ubuntu/Linaro 7.4.0-1ubuntu1~18.04.1)\r\n\r\n**Describe the current behavior**\r\nI am trying to use the C API for TF Lite on a custom ARM board. I built `libtensorflowlite_c.so` with `select_tf_ops` enabled via bazel:\r\n\r\n```\r\nbazel build --config=monolithic \\\r\n--define=with_select_tf_ops=true \\\r\n-c opt //tensorflow/lite/c:tensorflowlite_c \\\r\n--config noaws\r\n```\r\n\r\nI tried to use the API as described as in `tensorflow/lite/c/c_api.h`, but instead of just doing a single inference, I wanted to have cyclic evaluation of my inputs with the model. To do this, I simply put the code there in a for-loop:\r\n\r\n```\r\n#include \"tensorflow/lite/c/c_api.h\"\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n\r\n\r\nint main (int argc, char* argv[]) {\r\n\r\n   for(int i = 0; i < 3; i++)\r\n    {\r\n        printf(\"Iteration: %d\\n\", i);\r\n\r\n        float input[49] = { 0.0 };\r\n        TfLiteModel* model = TfLiteModelCreateFromFile(\"model.tflite\");\r\n        TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n        TfLiteInterpreterOptionsSetNumThreads(options, 2);\r\n        TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n        TfLiteInterpreterAllocateTensors(interpreter);\r\n\r\n        TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n        TfLiteTensorCopyFromBuffer(input_tensor, input, 49 * sizeof(float));\r\n\r\n        TfLiteInterpreterInvoke(interpreter);\r\n\r\n        const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 14);\r\n\r\n        float output[49];\r\n        TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));\r\n\r\n        printf(\"Output: \\n\\n\");\r\n        for (int j = 0; j < 49; j++) {\r\n            printf(\"%d: %f\\n\", j, output[j]);\r\n        }\r\n\r\n        TfLiteInterpreterDelete(interpreter);\r\n        TfLiteInterpreterOptionsDelete(options);\r\n        TfLiteModelDelete(model);\r\n    }\r\n    return 0;\r\n}\r\n```\r\nThe first iteration runs fine and returns something. But on the second iteration, I get a SegFault when calling `TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));`. Reason for this is that the previous function `TfLiteInterpreterGetOutputTensor` returns a nullpointer.\r\n\r\n**Describe the expected behavior**\r\nI expected to run this multiple times without any problems, as I destroy all old instances of variables at the end of the for-loop and thus start a fresh interpreter everytime. Obviously, this is not the case.\r\n\r\nCan somebody tell me what is going wrong here? I am currently building the debug version of the shared object, but this will take some time. Any help is appreciated!", "comments": ["Faced with similar problem while using TFLite multiple times and still unsolved. When I create multiple TFlite models in one time, the previous models behaves badly. TfLiteInterpreterGetInputTensor gives wrong result.\r\nWhile debugging, I find that as if the memory inside TFlite interpreter was occupied by others.", "@Huang-Jin \r\nThat is interesting, can you provide example code for this? It might be helpful to compare to the example provided by TF.", "@DocDriven you don't need to create interpreter every time.\r\nJust reuse it.\r\n\r\n```\r\n#include \"tensorflow/lite/c/c_api.h\"\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n\r\n\r\nint main (int argc, char* argv[]) {\r\n\r\n  TfLiteModel* model = TfLiteModelCreateFromFile(\"model.tflite\");\r\n  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n  TfLiteInterpreterOptionsSetNumThreads(options, 2);\r\n  TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n  TfLiteInterpreterAllocateTensors(interpreter);\r\n  for(int i = 0; i < 3; i++)\r\n  {\r\n    printf(\"Iteration: %d\\n\", i);\r\n\r\n    float input[49] = { 0.0 };\r\n\r\n    TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n    TfLiteTensorCopyFromBuffer(input_tensor, input, 49 * sizeof(float));\r\n\r\n    TfLiteInterpreterInvoke(interpreter);\r\n\r\n    const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 14);\r\n\r\n    float output[49];\r\n    TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));\r\n\r\n    printf(\"Output: \\n\\n\");\r\n    for (int j = 0; j < 49; j++) {\r\n      printf(\"%d: %f\\n\", j, output[j]);\r\n    }\r\n\r\n  }\r\n  TfLiteInterpreterDelete(interpreter);\r\n  TfLiteInterpreterOptionsDelete(options);\r\n  TfLiteModelDelete(model);\r\n  return 0;\r\n}\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39253\">No</a>\n", "> @DocDriven you don't need to create interpreter every time.\r\n> Just reuse it.\r\n> \r\n> ```\r\n> #include \"tensorflow/lite/c/c_api.h\"\r\n> #include <stdio.h>\r\n> #include <stdlib.h>\r\n> \r\n> \r\n> int main (int argc, char* argv[]) {\r\n> \r\n>   TfLiteModel* model = TfLiteModelCreateFromFile(\"model.tflite\");\r\n>   TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n>   TfLiteInterpreterOptionsSetNumThreads(options, 2);\r\n>   TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n>   TfLiteInterpreterAllocateTensors(interpreter);\r\n>   for(int i = 0; i < 3; i++)\r\n>   {\r\n>     printf(\"Iteration: %d\\n\", i);\r\n> \r\n>     float input[49] = { 0.0 };\r\n> \r\n>     TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n>     TfLiteTensorCopyFromBuffer(input_tensor, input, 49 * sizeof(float));\r\n> \r\n>     TfLiteInterpreterInvoke(interpreter);\r\n> \r\n>     const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 14);\r\n> \r\n>     float output[49];\r\n>     TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));\r\n> \r\n>     printf(\"Output: \\n\\n\");\r\n>     for (int j = 0; j < 49; j++) {\r\n>       printf(\"%d: %f\\n\", j, output[j]);\r\n>     }\r\n> \r\n>   }\r\n>   TfLiteInterpreterDelete(interpreter);\r\n>   TfLiteInterpreterOptionsDelete(options);\r\n>   TfLiteModelDelete(model);\r\n>   return 0;\r\n> }\r\n> ```\r\n\r\nYes\uff0cI used similar codes.\r\nBut when I ask for some large memory (like 300KB) after \u201cTfLiteInterpreterAllocateTensors\u201d\uff0cthe result from interpreter went wrong. It is a very strange thing :(.", "It seems that the memory inside interpreter was broken. \r\nDue to security, I can't give the sample codes. sorry!\r\nI'm just faced such problem and when I use valgrind to detect memory leak I found 100+ invalid read in the function \"tflite::Interpreter::Invoke\".", "By the way, I used TfLiteModelCreate rather than TfLiteModelCreateFromFile because I need to read tflite from buffer. Maybe this is the problem? The \"file\" version doesn't faced such problem.", "@Huang-Jin \r\nI exclusively use the creation from files for my current project, so I cannot tell if this causes the errors you are describing. But I have several models as well, and will test if they behave strangely. If I have the resultls, I can report them to you.", "@DocDriven I think I found the problem. The function \"TfLiteModelCreate\" needs a \"char *\" string buffer as input, but it doesn't copy the buffer inside the process. Due to this issue, when I use a local std::string::c_str() as input, the buffer will be deleted when local variable destructed.", "Just wanted to give a big bump on @Huang-Jin's previous comment. This is totally true and solved issues for me as well.\r\n\r\nThe fact that the input buffer to `TFLiteModelCreate` is not copied and therefore will be deleted if it goes out of scope might be a valuable addition to the docs.", "hello please a use the same program in c program on linux but the program give me the error \r\nundefined reference to `TfLiteModelCreateFromFile' this is my program \r\n/*\r\n *  Copyright (C) 2020-2021 FotaHub Inc. All rights reserved.\r\n *\r\n *  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\r\n *  not use this file except in compliance with the License.\r\n *  You may obtain a copy of the License at\r\n *\r\n *  http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n *  Unless required by applicable law or agreed to in writing, software\r\n *  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\r\n *  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n *  See the License for the specific language governing permissions and\r\n *  limitations under the License.\r\n *\r\n *  This file is part of the FotaHub(R) Device SDK program (https://fotahub.com)\r\n */\r\n#include \"Main.h\"\r\n#include \"tensorflow/c/c_api.h\"\r\n#include \"Configuration.h\"\r\n#include \"DemoFOTAUpdateWorkflow.h\"\r\n#include \"SinglePartitionDemoFirmwareUpdateInfoFileReader.h\"\r\n#include \"DemoProductInfo.h\"\r\n#include \"FotaHub.h\"\r\n#include <string.h>\r\n#include <libgen.h>\r\n#include <unistd.h>\r\n\r\n\r\n\r\n#include <tensorflow/lite/c/c_api.h>\r\n\r\n\r\nint32_t main(int32_t argc, char *argv[])\r\n{\r\n  /* \r\n   * Change directory to where this program's executable is located\r\n   */\r\n  char *programDir = dirname(strdup(argv[0]));\r\n  chdir(programDir);\r\n  ////////////////////////////////////////\r\n  TfLiteModel* model = TfLiteModelCreateFromFile(\"iris.tflite\");\r\n  \r\n\r\n\r\n  printf(\"\\n--------------------------------------------------------------------------\\n\");\r\n  printf(\"Running %s %s firmware the version with yaml hers\\n\", DEMO_PRODUCT_NAME, DEMO_PRODUCT_FIRMWARE_VERSION);\r\n  printf(\"--------------------------------------------------------------------------\\n\\n\");\r\n  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n  firmwareUpdateInfoReader_init();\r\n  fotaUpdateWorkflow_init();\r\n  fotahub_init(argv[0], DEMO_PRODUCT_ID, DEMO_PRODUCT_NAME, &FOTA_UPDATE_CLIENT_CONFIG);\r\n  /* \r\n   * Assume machine this program is running on to be connected to some Wi-Fi or Ethernet network\r\n   */\r\n  fotahub_notifyNetworkStatusChange(NETWORK_ADAPTER_STATUS_CONNECTED);\r\n  \r\n  firmwareUpdateInfoReader_explain(programDir);\r\n  \r\n  while (true)\r\n  {\r\n    firmwareUpdateInfoReader_run();\r\n    fotahub_run();\r\n  }\r\n}\r\n"]}, {"number": 39252, "title": "Fixed loading saved keras model (h5 files) when using advanced activation functions", "body": "@k-w-w and @jvishnuvardhan  , reference to #38994 \r\n\r\nPlease review changes. Tests: activations_test.py and advanced_activations.py passed.\r\n\r\nWe need to bring in the classes from advanced_activations if there are no custom objects specified. When no custom objects are specified, our module_objects/globals() in activations.deserialize() won't contain any advanced_activations.", "comments": ["Thanks for adding this!", "@k-w-w  Sorry, can you review the new changes 5224e939e0a21f88f1aeee11d9477e67d8b9b07f, had to fix pylint issues.", "This is failing the internal builds with the error `ImportError: cannot import name 'advanced_activations'`\r\nPlease update the [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/BUILD#L113-L123) so that the activations build rule includes `\"//tensorflow/python/keras/layers:advanced_activations\"` as a dependency.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39252) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39252) for more info**.\n\n<!-- ok -->", "@k-w-w Done\r\n@gbaned Sorry, can you remove all reviewers except @k-w-w , it was asked automatically"]}, {"number": 39251, "title": "Code gets stuck when using mirrored strategy", "body": "**System information:**\r\n\r\nHave I written custom code : Yes\r\nOS Platform and Distribution : Linux Ubuntu 18.04\r\nTensorFlow installed from : binary (install from pip)\r\nTensorFlow version (use command below): 2.1.0\r\nPython version: 3.6.8\r\nCUDA/cuDNN version: Cuda Toolkit 10.1 / cuDNN 7.6.4\r\nGPU model and memory: 2 Tesla V 100 16 GB each\r\n\r\n**Question:**\r\n\r\n\r\nI would like to store different variables and run different tensorflow ops on different devices (GPU/CPU). I want to do this because my model cannot fit on a single GPU. I am trying to train embeddings. I want to place recurring words on all GPUs and non common words on different devices (as a part of preprocessing I do all this) and transfer sentences to respective GPUs. \r\n\r\n\r\nFor the this question I am not pasting my actual code. I am writing a snippet which is close to what I want to do. When I run the code, I do not get any error and neither does it execute successfully. I think the problem is that this code gets stuck (may be deadlock on GPU). \r\n\r\nBelow is the snippet. For the sake of simplicity say I want to do  `y = x * w1 + x * w2 + 2 * w3`, where `w1`, `w2` and `w3` are trainable variables. I place `w1` on `GPU0` and `w2` on `GPU1`. Instead of doing the `2 * w3` on a single device say I want to split it and perform on 2 devices (to see how mirrored strategy works). I perform respective computations on respective GPUs. First of all is that a correct way to do it (using mirrored scope to assign devices to variables). Secondly if it is wrong why does tensorflow not throw any error, why is it getting stuck and why is it showing 100% GPU usage?\r\n\r\nIf i change all the device/scopes to a single GPU it executes (in seconds) successfully but I don't want that. I want to place some variables on single devices and some on both and perform computations as I illustrated below to speed up processing.\r\n```\r\nimport os\r\n\r\n#os.environ['TF2_BEHAVIOR'] = '1'\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\n\r\n#tf.compat.v1.disable_eager_execution()\r\n\r\ntf.config.set_soft_device_placement(False)\r\ntf.debugging.set_log_device_placement(True)\r\nimport numpy as np\r\n\r\nclass ProdLayer(tf.keras.layers.Layer):\r\n    def __init__(self, name):\r\n        super(ProdLayer, self).__init__()\r\n        self.w = tf.keras.backend.variable(0.01, name='var_'+ name)\r\n\r\n\r\n    def call(self, x):\r\n        return x * self.w \r\n\r\nclass SumLayer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(SumLayer, self).__init__()\r\n        \r\n    def call(self, x1, x2):\r\n        return x1 + x2\r\n    \r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\r\n\r\nclass EmbeddingModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(EmbeddingModel, self).__init__()\r\n        \r\n        # place w1 on GPU 0 and create the layer\r\n        with tf.device('/gpu:0'):\r\n            self.L1 = ProdLayer('w1')\r\n        \r\n        # place w2 on GPU 0 and create the layer        \r\n        with tf.device('/gpu:1'):\r\n            self.L2 = ProdLayer('w2')\r\n        \r\n        # place w3 on both GPU0 and GPU1 using mirrored scope. Can I do this?\r\n        with mirrored_strategy.scope():\r\n            self.w3 = tf.keras.backend.variable(0.01, name='var_w3')\r\n            \r\n        # may be do this on CPU? But for now let it perform this on GPU0\r\n        with tf.device('/gpu:0'):\r\n            self.L3 = SumLayer()\r\n        \r\n    \r\n    def call(self, input_layer):\r\n        # w1 is on GPU0, w2 is on GPU1 and w3 is placed using mirrored scope on both GPUs\r\n        # y1 = w1 * x + w3\r\n        with tf.device('/gpu:0'):\r\n            y1 = self.L1(input_layer) + self.w3\r\n\r\n        # y2 = w2 * x + w3\r\n        with tf.device('/gpu:1'):\r\n            y2 = self.L2(input_layer) + self.w3\r\n\r\n        # y = y1 + y2 (i.e. w1 * x + w2 * x + 2 * w3) \r\n        with tf.device('/gpu:0'):\r\n            y_hat = self.L3(y1, y2)\r\n            \r\n        return y_hat\r\n    \r\n    \r\ndef myLoss(y, y_hat):\r\n    return tf.reduce_sum((y_hat-y) * (y_hat-y))\r\n\r\nwith mirrored_strategy.scope():\r\n    model = EmbeddingModel()\r\n    model.compile(tf.optimizers.Adam(lr=0.0001), \r\n                      loss=myLoss)\r\n\r\ntrain_dataset = np.random.choice(100, size=(1000,))\r\nmodel.fit(x=train_dataset.astype(np.float32), y=train_dataset.astype(np.float32), \r\n       batch_size=1000, \r\n       epochs=10, shuffle=False, verbose=True)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["@StarDust1980 \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Thanks!", "System information\r\n\r\nHave I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 18.04\r\nTensorFlow installed from (source or\r\nbinary): binary (install from pip)\r\nTensorFlow version (use command below): 2.1.0\r\nPython version: 3.6.8\r\nCUDA/cuDNN version: Cuda Toolkit 10.1 / cuDNN 7.6.4\r\nGPU model and memory: 2 Tesla V 100 16 GB each\r\n\r\nI have updated in the question now. \r\n", "were you able to replicate this issue on your side?", "In MirroredStrategy, your model is replicated across each GPU and the variables are kept in sync by applying identical updates. Because copies of all variables in the model are stored on each GPU, it does not sound like MirroredStrategy is suited to your use case where you want to put w1 and w2 on different GPUs.\r\n\r\nIf your main concern is that the model does not fit on your GPU, you could consider trying the [CentralStorageStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CentralStorageStrategy), which only distributes the compute operations. This strategy will not allow you to split the variables across devices as you have suggested, but it could solve your problem of the embedding being too large to fit on GPU. \r\n\r\nAlternatively you could take a look at the [mesh-tensorflow library](https://github.com/tensorflow/mesh) which is designed for model parallelism, where you are splitting the model itself between different devices. If you are interested in learning more about this library, there is a [fifteen minute introduction](https://www.youtube.com/watch?v=HgGyWS40g-g) from the TF Dev Summit in 2019.", "@StarDust1980 Have you tried using a [custom training loop(CTL)](https://www.tensorflow.org/tutorials/distribute/custom_training) instead of the native tf.keras `compile` and `fit`? Since you are explicitly placing variables and calling it on specific devices I would suggest using a CTL to give you more extensibility.\r\nIf you place any variable creation under MirroredStrategy scope then it is replicated on the devices that you used to instantiate the strategy with. I don't know how `compile` will behave since we haven't tested this setup which is why you may be seeing your training hang in this setup. \r\n\r\nAs @nikitamaia mentioned above `tf.distribute` does not support ModelParallelism yet. Using CentralServerStorage is one way to address large embeddings that don't fit on the GPU. ", "I was mainly trying to figure out if I can have some variables (like w1 and w2) on independent GPUs and some replicated (like w3 - using mirrored strategy). I remember trying CTL but even that froze. Can this be done at all? I mean using mirrored strategy for some trainable variables. In the documentation, you copy all the variables on all the GPUs. I do not want to do that. I just need parts to be replicated.\r\n\r\nIf it can be done using mirrored strategy, can you provide a dummy snippet which does that (w1 on GPU1, w2 on GPU2 and w3 on both GPU1 and GPU2 using mirroring). The loss/use case doesn't have to make sense. Just want to know if that can be done and how? ", "@StarDust1980 Here is a snippet that works for me. Notice the commented out code:\r\n```\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [\r\n  tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\r\n  tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\r\n])\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\r\n\r\nclass EmbeddingModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(EmbeddingModel, self).__init__()\r\n        \r\n        # place w1 on GPU 0 and create the layer\r\n        with tf.device('/gpu:0'):\r\n            self.L1 = ProdLayer('w1')\r\n        \r\n        # place w2 on GPU 0 and create the layer        \r\n        with tf.device('/gpu:1'):\r\n            self.L2 = ProdLayer('w2')\r\n        \r\n        # place w3 on both GPU0 and GPU1 using mirrored scope. Can I do this?\r\n        with mirrored_strategy.scope():\r\n            self.w3 = tf.keras.backend.variable(0.01, name='var_w3')\r\n            \r\n        # may be do this on CPU? But for now let it perform this on GPU0\r\n        with tf.device('/gpu:0'):\r\n            self.L3 = SumLayer()\r\n        \r\n    \r\n    def call(self, input_layer):\r\n        # w1 is on GPU0, w2 is on GPU1 and w3 is placed using mirrored scope on both GPUs\r\n        # y1 = w1 * x + w3\r\n        with tf.device('/gpu:0'):\r\n            y1 = self.L1(input_layer) + self.w3\r\n\r\n        # y2 = w2 * x + w3\r\n        with tf.device('/gpu:1'):\r\n            y2 = self.L2(input_layer) + self.w3\r\n\r\n        # y = y1 + y2 (i.e. w1 * x + w2 * x + 2 * w3) \r\n        with tf.device('/gpu:0'):\r\n            y_hat = self.L3(y1, y2)\r\n            \r\n        return y_hat\r\n    \r\n    \r\ndef myLoss(y, y_hat):\r\n    return tf.reduce_sum((y_hat-y) * (y_hat-y))\r\n\r\n// with mirrored_strategy.scope(): # This cannot work\r\n    // model = EmbeddingModel()\r\n    // model.compile(tf.optimizers.Adam(lr=0.0001), \r\n    //                  loss=myLoss)\r\n\r\ntrain_dataset = np.random.random((10))\r\nmodel = EmbeddingModel()\r\nmodel(train_dataset)\r\n// Output\r\n// <tf.Tensor: shape=(10,), dtype=float64, numpy=\r\n// array([0.02833105, 0.03860711, 0.02790513, 0.02937286, 0.03635627,\r\n//       0.02511177, 0.03697144, 0.03635903, 0.03183693, 0.03885038])>\r\n```\r\n\r\nYou can't place EmbeddingModel in strategy scope after specifying the variable scope explicitly. This might have been why the training was not making any progress. Hope this helps unblock your use case. Note that we don't support ModelParallelism so you can't use tf.keras and tf.distribute strategy APIs out out of the box.", "Hope this was helpful, @StarDust1980. Closing this issue now since an example has been provided."]}, {"number": 39250, "title": "When I run Interpreter.run() method I get: 'DataType error: cannot resolve DataType of [[Ljava.lang.Float;' ", "body": "This is the code :\r\n    fun estimateTheAction(detectedSkeletons: ArrayList<Float>): String {\r\n             var output = Array(1) {FloatArray(4)}\r\n             val arrayInput: Array<Float> = detectedSkeletons.toTypedArray()\r\n             //var output = mapOf(1 to \"person_jumping\", 2 to \"person_kicking\", 3 to \"person_running\", 4 to \"person_walking\")\r\n             var input = arrayOf(arrayInput)\r\n             try {\r\n                      interpreter?.run(input, output)\r\n             }catch (e: Exception ){\r\n                      e.printStackTrace()\r\n        }\r\n        //var index = output[0].toString()\r\n        return \"actionName[output[0][0]]\r\n    }\r\n\r\n\r\nComplete Error:\r\nW/System.err: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of [[Ljava.lang.Float;\r\nW/System.err:     at org.tensorflow.lite.Tensor.dataTypeOf(Tensor.java:199)\r\n        at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:257)\r\n        at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:162)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:132)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)\r\n       .....", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 39249, "title": "Maybe a bug", "body": "Hi,\r\n\r\nI am not sure if it is bug. However, the code uses iterator as the advice using iterator from the previous same error. But it does not work. When I tried saving model, `save_path = saver.save(sess, ckpt_saver_file_final, global_step)`, I get the error.\r\n\r\n```\r\nError:\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/message_lite.cc:68] CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\r\nFatal Python error: Aborted\r\n```\r\n\r\nTensorflow: 1.8-cuda\r\nPython: 3.6\r\nGPU :  Tesla V100-SXM2-16GB\r\nTrain and Model Code:\r\n\r\n```\r\nError Trace ;\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/message_lite.cc:68] CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.\r\nFatal Python error: Aborted\r\n\r\nThread 0x00002aabae642700 (most recent call first):\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/threading.py\", line 295 in wait\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/queue.py\", line 164 in get\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 159 in run\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/threading.py\", line 916 in _bootstrap_inner\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/threading.py\", line 884 in _bootstrap\r\n\r\nCurrent thread 0x00002aaaaaaea900 (most recent call first):\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3169 in _as_graph_def\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3228 in as_graph_def\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1766 in export_meta_graph\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1728 in save\r\n  File \"/mnt/lscratch/users/wma/ncc/inst2vec/inst2vec_embedding.py\", line 417 in train_skip_gram\r\n  File \"/mnt/lscratch/users/wma/ncc/inst2vec/inst2vec_embedding.py\", line 546 in train_embeddings\r\n  File \"train_inst2vec.py\", line 60 in main\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"train_inst2vec.py\", line 74 in <module>\r\n/var/lib/slurmd/job1799931/slurm_script: line 21: 217538 Aborted                 python train_inst2vec.py\r\n```\r\n\r\nCode:\r\n```\r\n########################################################################################################################\r\n# Training embeddings\r\n########################################################################################################################\r\ndef train_skip_gram(V, data_folder, data_folders, dataset_size, reverse_dictionary,\r\n                    param, valid_examples, log_dir, vocab_metada_file, embeddings_pickle,\r\n                    ckpt_saver_file, ckpt_saver_file_init, ckpt_saver_file_final,\r\n                    restore_variables):\r\n    \"\"\"\r\n    Train embeddings (Skip-Gram model)\r\n    :param V: vocabulary size\r\n    :param data_folder: string containing the path to the parent directory of raw data sub-folders\r\n    :param data_folders: list of sub-folders containing pre-processed LLVM IR code\r\n    :param dataset_size: number of data pairs in total in the training data set\r\n    :param reverse_dictionary: [keys=statement index, values=statement]\r\n    :param param: parameters of the inst2vec training\r\n    :param valid_examples: statements to be used as validation examples (list of indices)\r\n    :param log_dir: logging directory for Tensorboard output\r\n    :param vocab_metada_file: vocabulary metadata file for Tensorboard\r\n    :param embeddings_pickle: file in which to pickle embeddings\r\n    :param ckpt_saver_file: checkpoint saver file (intermediate states of training)\r\n    :param ckpt_saver_file_init: checkpoint saver file (initial state of training)\r\n    :param ckpt_saver_file_final: checkpoint saver file (final state of training)\r\n    :param restore_variables: boolean: whether to restore variables from a previous training\r\n    :return: embeddings matrix\r\n    \"\"\"\r\n    ####################################################################################################################\r\n    # Extract parameters from dictionary \"param\"\r\n    N = param['embedding_size']\r\n    mini_batch_size = param['mini_batch_size']\r\n    num_sampled = param['num_sampled']\r\n    num_epochs = param['num_epochs']\r\n    learning_rate = param['learning_rate']\r\n    l2_reg_scale = param['beta']\r\n    freq_print_loss = param['freq_print_loss']\r\n    step_print_neighbors = param['step_print_neighbors']\r\n    context_width = param['context_width']\r\n\r\n    ####################################################################################################################\r\n    # Set up for analogies\r\n    analogies, analogy_types, n_questions_total, n_questions_relevant = i2v_eval.load_analogies(data_folder)\r\n    folder_evaluation = embeddings_pickle.replace('.p', '') + 'eval'\r\n    if not os.path.exists(folder_evaluation):\r\n        os.makedirs(folder_evaluation)\r\n    analogy_evaluation_file = os.path.join(folder_evaluation, \"analogy_results\")\r\n\r\n    config = None\r\n    options = None\r\n    metadata = None\r\n    if FLAGS.profile:\r\n        options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        metadata = tf.RunMetadata()\r\n    if FLAGS.xla:\r\n        config = tf.ConfigProto()\r\n        config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\n    ####################################################################################################################\r\n    # Read data using Tensorflow's data API\r\n    data_files = get_data_pair_files(data_folders, context_width)\r\n    print('\\ttraining with data from files:', data_files)\r\n    with tf.name_scope(\"Reader\") as scope:\r\n\r\n        random.shuffle(data_files)\r\n        dataset_raw = tf.data.FixedLengthRecordDataset(filenames=data_files,\r\n                                                       record_bytes=8)  # <TFRecordDataset shapes: (), types: tf.string>\r\n        dataset = dataset_raw.map(record_parser)\r\n        dataset = dataset.shuffle(int(1e5))\r\n        dataset_batched = dataset.apply(tf.contrib.data.batch_and_drop_remainder(mini_batch_size))\r\n        dataset_batched = dataset_batched.prefetch(int(100000000))\r\n        iterator = dataset_batched.make_initializable_iterator()\r\n        saveable_iterator = tf.contrib.data.make_saveable_from_iterator(iterator)\r\n        next_batch = iterator.get_next()  # Tensor(\"Shape:0\", shape=(2,), dtype=int32)\r\n\r\n    ####################################################################################################################\r\n    # Tensorflow computational graph\r\n    # Placeholders for inputs\r\n    with tf.name_scope(\"Input_Data\") as scope:\r\n        train_inputs = next_batch[:, 0]\r\n        train_labels = tf.reshape(next_batch[:, 1], shape=[mini_batch_size, 1], name=\"training_labels\")\r\n\r\n    # (input) Embedding matrix\r\n    with tf.name_scope(\"Input_Layer\") as scope:\r\n        W_in = tf.Variable(tf.random_uniform([V, N], -1.0, 1.0), name=\"input-embeddings\")\r\n\r\n        # Look up the vector representing each source word in the batch (fetches rows of the embedding matrix)\r\n        h = tf.nn.embedding_lookup(W_in, train_inputs, name=\"input_embedding_vectors\")\r\n\r\n    # Normalized embedding matrix\r\n    with tf.name_scope(\"Embeddings_Normalized\") as scope:\r\n        normalized_embeddings = tf.nn.l2_normalize(W_in, name=\"embeddings_normalized\")\r\n\r\n    # (output) Embedding matrix (\"output weights\")\r\n    with tf.name_scope(\"Output_Layer\") as scope:\r\n        if FLAGS.softmax:\r\n            W_out = tf.Variable(tf.truncated_normal([N, V], stddev=1.0 / math.sqrt(N)), name=\"output_embeddings\")\r\n        else:\r\n            W_out = tf.Variable(tf.truncated_normal([V, N], stddev=1.0 / math.sqrt(N)), name=\"output_embeddings\")\r\n\r\n        # Biases between hidden layer and output layer\r\n        b_out = tf.Variable(tf.zeros([V]), name=\"nce_bias\")\r\n\r\n    # Optimization\r\n    with tf.name_scope(\"Optimization_Block\") as scope:\r\n        # Loss function\r\n        if FLAGS.softmax:\r\n            logits = tf.layers.dense(inputs=h, units=V)\r\n            onehot = tf.one_hot(train_labels, V)\r\n            loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=onehot, logits=logits)\r\n        else:\r\n            loss_tensor = tf.nn.nce_loss(weights=W_out,\r\n                                         biases=b_out,\r\n                                         labels=train_labels,\r\n                                         inputs=h,\r\n                                         num_sampled=num_sampled,\r\n                                         num_classes=V)\r\n        train_loss = tf.reduce_mean(loss_tensor, name=\"nce_loss\")\r\n\r\n        # Regularization (optional)\r\n        if l2_reg_scale > 0:\r\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, W_in)\r\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, W_out)\r\n            regularizer = tf.contrib.layers.l2_regularizer(l2_reg_scale)\r\n            reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n            reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\r\n            loss = train_loss + reg_term\r\n        else:\r\n            loss = train_loss\r\n\r\n        # Optimizer\r\n        if FLAGS.optimizer == 'adam':\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n        elif FLAGS.optimizer == 'nadam':\r\n            optimizer = tf.contrib.opt.NadamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n        elif FLAGS.optimizer == 'momentum':\r\n            global_train_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=\"global_step\")\r\n            # Passing global_step to minimize() will increment it at each step.\r\n            optimizer = (\r\n                tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_train_step)\r\n            )\r\n        else:\r\n            raise ValueError('Unrecognized optimizer ' + FLAGS.optimizer)\r\n\r\n    if FLAGS.optimizer != 'momentum':\r\n        global_train_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=\"global_step\")\r\n\r\n    ####################################################################################################################\r\n    # Validation block\r\n    with tf.name_scope(\"Validation_Block\") as scope:\r\n        valid_dataset = tf.constant(valid_examples, dtype=tf.int32, name=\"validation_data_size\")\r\n        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\r\n        cosine_similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\r\n\r\n    ####################################################################################################################\r\n    # Summaries\r\n    with tf.name_scope(\"Summaries\") as scope:\r\n        tf.summary.histogram(\"input_embeddings\", W_in)\r\n        tf.summary.histogram(\"input_embeddings_normalized\", normalized_embeddings)\r\n        tf.summary.histogram(\"output_embeddings\", W_out)\r\n        tf.summary.scalar(\"nce_loss\", loss)\r\n\r\n        analogy_score_tensor = tf.Variable(0, trainable=False, dtype=tf.int32, name=\"analogy_score\")\r\n        tf.summary.scalar(\"analogy_score\", analogy_score_tensor)\r\n\r\n    ####################################################################################################################\r\n    # Misc.\r\n    restore_completed = False\r\n    init = tf.global_variables_initializer()        # variables initializer\r\n    summary_op = tf.summary.merge_all()             # merge summaries into one operation\r\n\r\n    ####################################################################################################################\r\n    # Training\r\n    with tf.Session(config=config) as sess:\r\n\r\n        # Add TensorBoard components\r\n        writer = tf.summary.FileWriter(log_dir)  # create summary writer\r\n        writer.add_graph(sess.graph)\r\n        gvars = [gvar for gvar in tf.global_variables() if 'analogy_score' not in gvar.name]\r\n        saver = tf.train.Saver(gvars, max_to_keep=5)  # create checkpoint saver\r\n        config = projector.ProjectorConfig()  # create projector config\r\n        embedding = config.embeddings.add()  # add embeddings visualizer\r\n        embedding.tensor_name = W_in.name\r\n        embedding.metadata_path = vocab_metada_file  # link metadata\r\n        projector.visualize_embeddings(writer, config)  # add writer and config to projector\r\n\r\n        # Set up variables\r\n        if restore_variables:   # restore variables from disk\r\n            restore_file = tf.train.latest_checkpoint(log_dir)\r\n            assert restore_file is not None, \"No restore file found in folder \" + log_dir\r\n            assert os.path.exists(restore_file + \".index\"), \\\r\n                \"Trying to restore Tensorflow session from non-existing file: \" + restore_file + \".index\"\r\n            init.run()\r\n            saver.restore(sess, restore_file)\r\n            print(\"\\tVariables restored from file\", ckpt_saver_file, \"in TensorFlow \")\r\n\r\n        else:  # save the computational graph to file and initialize variables\r\n\r\n            graph_saver = tf.train.Saver(allow_empty=True)\r\n            init.run()\r\n            graph_saver.save(sess, ckpt_saver_file_init, global_step=0, write_meta_graph=True)\r\n            tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable_iterator)\r\n            print(\"\\tVariables initialized in TensorFlow\")\r\n\r\n        # Compute the necessary number of steps for this epoch as well as how often to print the avg loss\r\n        num_steps = int(math.ceil(dataset_size / mini_batch_size))\r\n        step_print_loss = int(math.ceil(num_steps / freq_print_loss))\r\n        print('\\tPrinting loss every ', step_print_loss, 'steps, i.e.', freq_print_loss, 'times per epoch')\r\n\r\n        ################################################################################################################\r\n        # Epoch loop\r\n        epoch = 0\r\n        global_step = 0\r\n        while epoch < int(num_epochs):\r\n            print('\\n\\tStarting epoch ', epoch)\r\n            sess.run(iterator.initializer)      # initialize iterator\r\n\r\n            # If restoring a previous training session, set the right training epoch\r\n            if restore_variables and not restore_completed:\r\n                epoch = int(math.floor(global_train_step.eval() / (dataset_size / mini_batch_size)))\r\n                global_step = global_train_step.eval()\r\n                print('Starting from epoch', epoch)\r\n\r\n            ############################################################################################################\r\n            # Loop over steps (mini batches) inside of epoch\r\n            step = 0\r\n            avg_loss = 0\r\n            while True:\r\n\r\n                try:\r\n\r\n                    # Print average loss every x steps\r\n                    if step_print_loss > 0 and step % int(step_print_loss) == 0:    # update step with logging\r\n\r\n                        # If restoring a previous training session, set the right training epoch\r\n                        if restore_variables and not restore_completed:\r\n                            restore_completed = True\r\n\r\n                        # Write global step\r\n                        if FLAGS.optimizer != 'momentum':\r\n                            global_train_step.assign(global_step).eval()\r\n\r\n                        # Perform an update\r\n                        # print('\\tStarting local step {:>6}'.format(step))  # un-comment for debugging\r\n                        [_, loss_val, train_loss_val, global_step] = sess.run(\r\n                            [optimizer, loss, train_loss, global_train_step], options=options,\r\n                            run_metadata=metadata)\r\n                        assert not np.isnan(loss_val), \"Loss at step \" + str(step) + \" is nan\"\r\n                        assert not np.isinf(loss_val), \"Loss at step \" + str(step) + \" is inf\"\r\n                        avg_loss += loss_val\r\n\r\n                        if step > 0:\r\n                            avg_loss /= step_print_loss\r\n\r\n                        analogy_score = i2v_eval.evaluate_analogies(W_in.eval(), reverse_dictionary, analogies,\r\n                                                                    analogy_types, analogy_evaluation_file,\r\n                                                                    session=sess, print=i2v_eval.nop)\r\n                        total_analogy_score = sum([a[0] for a in analogy_score])\r\n                        analogy_score_tensor.assign(total_analogy_score).eval()  # for tf.summary\r\n\r\n                        [summary, W_in_val] = sess.run([summary_op, W_in])\r\n\r\n                        if FLAGS.savebest is not None:\r\n                            filelist = [f for f in os.listdir(FLAGS.savebest)]\r\n                            scorelist = [int(s.split('-')[1]) for s in filelist]\r\n                            if len(scorelist) == 0 or total_analogy_score > sorted(scorelist)[-1]:\r\n                                i2v_utils.safe_pickle(W_in_val, FLAGS.savebest + '/' + 'score-' +\r\n                                                      str(total_analogy_score) + '-w.p')\r\n\r\n                        # Display average loss\r\n                        print('{} Avg. loss at epoch {:>6,d}, step {:>12,d} of {:>12,d}, global step {:>15} : {:>12.3f}, analogies: {})'.format(\r\n                            str(datetime.now()), epoch, step, num_steps, global_step, avg_loss, str(analogy_score)))\r\n                        avg_loss = 0\r\n\r\n                        # Pickle intermediate embeddings\r\n                        i2v_utils.safe_pickle(W_in_val, embeddings_pickle)\r\n\r\n                        # Write to TensorBoard\r\n                        saver.save(sess, ckpt_saver_file, global_step=global_step, write_meta_graph=False)\r\n                        writer.add_summary(summary, global_step=global_step)\r\n\r\n                        if FLAGS.profile:\r\n                            fetched_timeline = timeline.Timeline(metadata.step_stats)\r\n                            chrome_trace = fetched_timeline.generate_chrome_trace_format()\r\n                            with open('timeline_step_%d.json' % step, 'w') as f:\r\n                                f.write(chrome_trace)\r\n\r\n                        if step > 0 and FLAGS.extreme:\r\n                            sys.exit(22)\r\n\r\n                    else:   # ordinary update step\r\n                        [_, loss_val] = sess.run([optimizer, loss])\r\n                        avg_loss += loss_val\r\n\r\n                    # Compute and print nearest neighbors every x steps\r\n                    if step_print_neighbors > 0 and step % int(step_print_neighbors) == 0:\r\n                        print_neighbors(op=cosine_similarity, examples=valid_examples, top_k=6,\r\n                                        reverse_dictionary=reverse_dictionary)\r\n\r\n                    # Update loop index (steps in epoch)\r\n                    step += 1\r\n                    global_step += 1\r\n\r\n                except tf.errors.OutOfRangeError:\r\n\r\n                    # We reached the end of the epoch\r\n                    print('\\n\\t Writing embeddings to file ', embeddings_pickle)\r\n                    i2v_utils.safe_pickle([W_in.eval()], embeddings_pickle)                   # WEIRD!\r\n                    epoch += 1      # update loop index (epochs)\r\n                    break           # from this inner loop\r\n\r\n        ################################################################################################################\r\n        # End of training:\r\n        # Print the nearest neighbors at the end of the run\r\n        if step_print_neighbors == -1:\r\n            print_neighbors(op=cosine_similarity, examples=valid_examples, top_k=6,\r\n                            reverse_dictionary=reverse_dictionary)\r\n\r\n        # Save state of training and close the TensorBoard summary writer\r\n        save_path = saver.save(sess, ckpt_saver_file_final, global_step)\r\n        writer.add_summary(summary, global_step)\r\n        writer.close()\r\n\r\n        return W_in.eval()\r\n```\r\n\r\n\r\n", "comments": ["@Marvinmw,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here. \r\n\r\nAlso, is there any specific reason you are using TensorFlow 1.8? Could you please upgrade your TensorFlow version to 2.x or 1.15 and let us know if you're facing the same issue. Thanks!", "The provided code example is too long. It would be great if it could be minimized to the lines that produce the bug and nothing more, so that we can run it quickly and debug.\r\n\r\nThis is indeed a bug and we need to solve it if it happens in 1.15 or 2.x. If it happens on versions before 1.15 then, since those are no longer supported, we can only close the issues and recommend and update.", "hi, thanks. i will try Tensorflow 2.0. But to use Tensorflow 2.0,  it is compatible with  Tensorflow 1.8", "@Marvinmw,\r\nTo run your code with TF 2.x, you'll have to migrate the code from TF 1.x to TF 2.x. Please take a look at [this guide](https://www.tensorflow.org/guide/migrate) which might help you in this case. Thanks!", "TF 1.8 is not supported. We cannot dedicate time to fix that and we won't issue a fix anyway.", "I tried tensorflow 1.13 and still have the issue.", "@Marvinmw,\r\nPlease take a look at [this issue](https://github.com/tensorflow/tensorflow/issues/19657) with a similar error and let us know if it helps. Thanks!", "@Marvinmw 1.13 is still out of support. 1.15 is the earliest supported", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39248, "title": ".bazelversion and configure.py conflicts", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  source master\r\n- TensorFlow version: master 2.1.0\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.2/7.6.5\r\n- GPU model and memory: GTX1080Ti GDDR5X 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n.bazelversion and configure.py. bazel version requirements are different from each other,\r\ncausing error.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n.configure\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\\.configure\r\n```\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1557, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1369, in main\r\n    current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\r\n  File \"./configure.py\", line 482, in check_bazel_version\r\n    curr_version = run_shell(\r\n  File \"./configure.py\", line 159, in run_shell\r\n    output = subprocess.check_output(cmd, stderr=stderr)\r\n  File \"/home/wmind/anaconda3/lib/python3.8/subprocess.py\", line 411, in check_output\r\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n  File \"/home/wmind/anaconda3/lib/python3.8/subprocess.py\", line 512, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1.\r\n```\r\n\r\nbazel --version\r\n```\r\nERROR: The project you're trying to build requires Bazel 3.0.0 (specified in /home/wmind/repo/tensorflow/.bazelversion), but it wasn't found in /usr/bin.\r\n\r\nYou can install the required Bazel version via apt:\r\n  sudo apt update && sudo apt install bazel-3.0.0\r\n\r\nIf this doesn't work, check Bazel's installation instructions for help:\r\n  https://docs.bazel.build/versions/master/install-ubuntu.html\r\n```", "comments": ["@alanpurple \r\nCould you please downgrade your python version and try the same.\r\nAlso please refer to this [link](https://stackoverflow.com/questions/56563041/calledprocesserror-while-installing-tensorflow-using-bazel) , [link1](https://github.com/tensorflow/tensorflow/issues/37702#issuecomment-601370963) , [link2](https://github.com/FloopCZ/tensorflow_cc/issues/216#issuecomment-616306768)\r\n#38736 #37702 #38325 #22743", "Master branch seems ok as the version in `.bazelversion` is included in the range from `configure.py`: https://github.com/tensorflow/tensorflow/blob/62d50aa8e28209c701e8919d315ed0cb3ca804de/configure.py#L52-L53 and https://github.com/tensorflow/tensorflow/blob/62d50aa8e28209c701e8919d315ed0cb3ca804de/.bazelversion#L1\r\n\r\nThe same happens on `r2.2` and `r2.1` branches. `.bazelversion` does not exist before that.", "@Saduf2019 \r\ndone, same error with python 3.8.1\r\n\r\n@mihaimaruseac \r\nyes, that's what i'm saying, master branch should be okay, but error occurs", "What is the output of `bazel version` (not `bazel --version`)?", "@mihaimaruseac @alanpurple From the https://github.com/bazelbuild/bazel/issues/10465#issuecomment-567842873, my understanding is that:\r\n- bazelversion is meant to be used by a \"bazel wrapper\" (installed form `apt-get install bazel`, or binary download of `bazelisk`)\r\n- the \"bazel wrapper\" will selectively choose the exact version of the bazel (3.0.0 in this case) to run.\r\n\r\nHowever, since traditionally tensorflow developers outside of google normally download and install bazel directly (not through the wrapper), a downloaded version might be different and the error happens.\r\n\r\nMy suggestion is to either update the documentation for installation and ask people to use bazel wrapper instead (and remove version range in configure.py), or remove bazelversion restriction, ", "@yongtang \r\nno, I've already known that apt install bazel and I use that version of bazel, this is not the install type problem after all", "this is not happening in windows 10 btw", "@alanpurple \r\nIs this still an issue", "@Saduf2019 \r\n\r\nyes same, nothing's been resolved", "Asking again, what is the output of `bazel version`?", "@mihaimaruseac \r\n\r\nthis time, 3.2.0, and .bazelversion has 3.0.0", "Sorry, please post the exact output.", "@mihaimaruseac \r\n\r\n```\r\nERROR: The project you're trying to build requires Bazel 3.0.0 (specified in /home/wmind/repo/tensorflow/.bazelversion), but it wasn't found in /usr/bin.\r\n\r\nYou can install the required Bazel version via apt:\r\n  sudo apt update && sudo apt install bazel-3.0.0\r\n\r\nIf this doesn't work, check Bazel's installation instructions for help:\r\n  https://docs.bazel.build/versions/master/install-ubuntu.html\r\n```", "This seems to be caused by a logic error in `configure.py`. Will try to send a fix soon.", "This should be fixed on master by 56a99d6. Can you please try again from `master` branch, syncing to a fresh hash? If it fails, can you please post the commit hash you are building from?\r\n\r\nPreferably, can you post the full output of running the following commands?\r\n\r\n```\r\nbazel clean --expunge\r\ngit checkout master\r\ngit pull --rebase\r\ngit show\r\npython configure.py\r\n```\r\n\r\nApologies for the delay, spent a while tried to replicate before I thought of looking into the code and saw the code change", "@mihaimaruseac \r\n\r\n```\r\nwmind@wmind-deep:~/repo/tensorflow$ bazel clean --expunge\r\nERROR: The project you're trying to build requires Bazel 3.1.0 (specified in /home/wmind/repo/tensorflow/.bazelversion), but it wasn't found in /usr/bin.\r\n\r\nYou can install the required Bazel version via apt:\r\n  sudo apt update && sudo apt install bazel-3.1.0\r\n\r\nIf this doesn't work, check Bazel's installation instructions for help:\r\n  https://docs.bazel.build/versions/master/install-ubuntu.html\r\nwmind@wmind-deep:~/repo/tensorflow$ git checkout master\r\nAlready on 'master'\r\nYour branch is up to date with 'origin/master'.\r\nwmind@wmind-deep:~/repo/tensorflow$ git pull --rebase\r\nAlready up to date.\r\nwmind@wmind-deep:~/repo/tensorflow$ git show\r\ncommit d8b90acf95bdff09e29b1065739e28e8bd7500d9 (HEAD -> master, origin/master, origin/HEAD)\r\nAuthor: Dan Moldovan <mdan@google.com>\r\nDate:   Mon Jun 22 18:46:12 2020 -0700\r\n\r\n    Enable type annotations for python/ops.\r\n    \r\n    PiperOrigin-RevId: 317779660\r\n    Change-Id: Ife6b7319ef394b611798f5d01b64ebdb3c0a01cc\r\n\r\ndiff --git a/tensorflow/python/ops/logging_ops.py b/tensorflow/python/ops/logging_ops.py\r\nindex 8ca63f5598..02fce27769 100644\r\n--- a/tensorflow/python/ops/logging_ops.py\r\n+++ b/tensorflow/python/ops/logging_ops.py\r\n@@ -54,11 +54,9 @@ except NameError:\r\n # call relies on certain conditionals for its dependencies.  Use\r\n # control_flow_ops.Assert.\r\n \r\n-# Assert and Print are special symbols in python, so we must\r\n-# have an upper-case version of them.\r\n-#\r\n-# For users with Python 3 or Python 2.7\r\nwmind@wmind-deep:~/repo/tensorflow$ python configure.py\r\nWARNING: current bazel installation is not a release version.\r\nMake sure you are running at least bazel 3.1.0\r\nPlease specify the location of python. [Default is /home/wmind/anaconda3/bin/python]: \r\n```", "This seems consistent with what we have\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d8b90acf95bdff09e29b1065739e28e8bd7500d9/configure.py#L52-L53\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d8b90acf95bdff09e29b1065739e28e8bd7500d9/.bazelversion#L1\r\n\r\nSeems you have a broken bazelisk? Or a bazelisk which cannot automatically install bazel? What is the output of `type bazel`?", "@mihaimaruseac \r\noutside tensorflow repo, bazel is fine, my bazel version is 3.3.0\r\n\r\nwith modification to .bazeversion(to 3.3.0) works fine", "```\r\nwmind@wmind-deep:~/repo/tensorflow$ type bazel\r\nbazel is hashed (/usr/bin/bazel)\r\nwmind@wmind-deep:~/repo/tensorflow$ bazel version\r\nERROR: The project you're trying to build requires Bazel 3.1.0 (specified in /home/wmind/repo/tensorflow/.bazelversion), but it wasn't found in /usr/bin.\r\n\r\nYou can install the required Bazel version via apt:\r\n  sudo apt update && sudo apt install bazel-3.1.0\r\n\r\nIf this doesn't work, check Bazel's installation instructions for help:\r\n  https://docs.bazel.build/versions/master/install-ubuntu.html\r\nwmind@wmind-deep:~/repo/tensorflow$ cd ..\r\nwmind@wmind-deep:~/repo$ bazel version\r\nBuild label: 3.3.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 17 12:55:32 2020 (1592398532)\r\nBuild timestamp: 1592398532\r\nBuild timestamp as int: 1592398532\r\nwmind@wmind-deep:~/repo$ \r\n```", "```\r\nwmind@wmind-deep:~/repo/tensorflow$ vim .bazelversion \r\nwmind@wmind-deep:~/repo/tensorflow$ bazel version\r\nBuild label: 3.3.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 17 12:55:32 2020 (1592398532)\r\nBuild timestamp: 1592398532\r\nBuild timestamp as int: 1592398532\r\nwmind@wmind-deep:~/repo/tensorflow$ ./configure \r\nYou have bazel 3.3.0 installed.\r\nPlease specify the location of python. [Default is /home/wmind/anaconda3/bin/python3]: \r\n```", "so it's not my bazel's fault after all", "This is what I see on my end, on a Docker container with bazelisk installed.\r\n\r\n```\r\nroot@faa022ec076e:/src# bazel version\r\n2020/06/23 15:09:08 Downloading https://releases.bazel.build/3.3.0/release/bazel-3.3.0-linux-x86_64...\r\nBazelisk version: v1.1.0\r\nExtracting Bazel installation... \r\nBuild label: 3.3.0                                                                                                                 \r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 17 12:55:32 2020 (1592398532)\r\nBuild timestamp: 1592398532                                      \r\nBuild timestamp as int: 1592398532\r\nroot@faa022ec076e:/src# bazel version\r\nBazelisk version: v1.1.0                                         \r\nBuild label: 3.3.0      \r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 17 12:55:32 2020 (1592398532)\r\nBuild timestamp: 1592398532                                                                                                        \r\nBuild timestamp as int: 1592398532               \r\nroot@faa022ec076e:/src# cd -\r\n/src/tensorflow                                                  \r\nroot@faa022ec076e:/src/tensorflow# bazel version\r\nBazelisk version: v1.1.0\r\nStarting local Bazel server and connecting to it...\r\nBuild label: 3.1.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Apr 22 10:32:27 2020 (1587551547)\r\nBuild timestamp: 1587551547\r\nBuild timestamp as int: 1587551547\r\nroot@faa022ec076e:/src/tensorflow# type bazel\r\nbazel is hashed (/usr/bin/bazel) \r\nroot@faa022ec076e:/src/tensorflow# file /usr/bin/bazel\r\n/usr/bin/bazel: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, not stripped\r\n```\r\n\r\nNote how bazelisk is able to download the missing bazel on my system.\r\n\r\nIf I change `.bazelversion` to something which does not exist then I am able to reproduce your output:\r\n\r\n```\r\nroot@faa022ec076e:/src/tensorflow# git diff  \r\ndiff --git a/.bazelversion b/.bazelversion                 \r\nindex fd2a0186..18091983 100644\r\n--- a/.bazelversion\r\n+++ b/.bazelversion                                              \r\n@@ -1 +1 @@\r\n-3.1.0\r\n+3.4.0\r\nroot@faa022ec076e:/src/tensorflow# bazel version\r\n2020/06/23 15:14:22 Downloading https://releases.bazel.build/3.4.0/release/bazel-3.4.0-linux-x86_64...\r\n2020/06/23 15:14:23 could not download Bazel: HTTP GET https://releases.bazel.build/3.4.0/release/bazel-3.4.0-linux-x86_64 failed w\r\nith error 404\r\nroot@faa022ec076e:/src/tensorflow# python configure.py\r\nWARNING: current bazel installation is not a release version.\r\nMake sure you are running at least bazel 3.1.0\r\nPlease specify the location of python. [Default is /usr/local/bin/python]:\r\n```\r\n\r\nTo go further, can you run the following two commands in the `tensorflow` directory please? (Install `strace` if missing)\r\n\r\n```\r\nstrace -f -o bazel_version.log bazel version\r\nstrace -f -o configure.log python configure.py\r\n```\r\n\r\nAnd then attach the `bazel_version.log` and `configure.log` files.", "[configure.log](https://github.com/tensorflow/tensorflow/files/4822727/configure.log)\r\n[bazel_version.log](https://github.com/tensorflow/tensorflow/files/4822728/bazel_version.log)\r\n\r\n@mihaimaruseac ", "You have `bazel`, not `bazelisk` (`/usr/bin/bazel-real` is the binary that runs). Bazelisk is the process which should read `.bazelversion` and install the corresponding Bazel. It seems Bazel now checks the `.bazelversion` file too.", "That is, if you install Bazelisk (and uninstall Bazel), this should not happen anymore", "@mihaimaruseac \r\nso what u mean is, this is bazel's s problem?\r\n\r\nand bazelisk is okay but original bazel is not.\r\n\r\nis this right?\r\n\r\nI'd rather modify .bazelversion, is it okay?\r\n\r\n\r\nthank you", "`.bazelversion` is used by Bazelisk. In all my tests Bazel would just ignore the file. Don't know why the binary that executes when you call `bazel` on your system does not ignore the file.\r\n\r\n`.bazelversion` lists the version of Bazel that we built with. As long as you change it to a version that is in the range specified by `configure.py` that should be ok", "@mihaimaruseac \r\n\r\nthank you for your efforts, but it seems that this issue is not resolved", "What do you mean by \"is not resolved\"? What changed since https://github.com/tensorflow/tensorflow/issues/39248#issuecomment-649235945 ?", "@mihaimaruseac \r\nI mean, since official bazel has problem with tensorflow build, even though bazelisk has no problem", "I am not sure what is the problem You are able to run `./configure` and `bazel build`, right? There is no more a conflict between `.bazelversion` and `configure.py`\r\n\r\nOn a docker container, with no bazelisk, no bazel, I am able to build even if using bazel 3.3.0.\r\n\r\n```\r\nroot@dde9f7f2f021:/src/tensorflow# wget -O /usr/bin/bazel https://github.com/bazelbuild/bazel/releases/download/3.3.0/bazel-3.3.0-linux-x86_64 \r\noot@dde9f7f2f021:/src/tensorflow# chmod +x /usr/bin/bazel        \r\nroot@dde9f7f2f021:/src/tensorflow# bazel version                                                                                   \r\nExtracting Bazel installation...                 \r\nStarting local Bazel server and connecting to it...\r\nBuild label: 3.3.0                                                                                                                 \r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jun 17 12:55:32 2020 (1592398532)        \r\nBuild timestamp: 1592398532                                                                                                        \r\nBuild timestamp as int: 1592398532                                                                                                 \r\nroot@dde9f7f2f021:/src/tensorflow# bazel --version        \r\nbazel 3.3.0                                                                                                                        \r\nroot@dde9f7f2f021:/src/tensorflow# python configure.py                                                                             \r\nYou have bazel 3.3.0 installed.                                                                                                    \r\nPlease specify the location of python. [Default is /usr/local/bin/python]: \r\n...\r\nroot@dde9f7f2f021:/src/tensorflow# bazel build tensorflow/tools/pip_package:build_pip_package\r\n...\r\nroot@dde9f7f2f021:/src/tensorflow# cat .bazelversion \r\n3.1.0\r\n```\r\n\r\nSo, I am unsure what is left to be solved on this issue.", "@alanpurple \r\nCould you please check on tf 2.4.1/upgrade tf version to latest and let us know if you still face the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39248\">No</a>\n"]}, {"number": 39247, "title": "Support CUDA 10.2", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0-rc4\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntensorflow has hardcoded the dll name of CUDA and tries to detect version 10.1, but version 10.2 is the current.\r\n\r\n```W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll ```\r\n\r\nThe current name is cudart64_102.dll\r\n\r\n**Will this change the current api? How?**\r\nno\r\n\r\n**Who will benefit with this feature?**\r\nall new users\r\n\r\n**Any Other info.**\r\n-\r\n", "comments": ["Unfortunately it is too late to change this as the effort to remove the hardcoded string will be too long and we already released the final version.\r\n\r\nHowever, this can be solved by symlinking the 10.2 libs to 10.1 ones. At least on Linux, there were several people reporting success doing that.", "Do you mean there won't be any more tensorflow version ?\r\nI was able to run with 10.1 correctly.", "The 2.2 release is out. Next release is 2.3 which will probably target a new version of CUDA.\r\n\r\nPatch releases are not allowed to change binary dependencies, except for security reasons", "Is there a timeline for when 10.2 will be supported in tf-nightly? \r\n\r\n[The documentation for the profiler](https://www.tensorflow.org/guide/profiler#install_the_profiler_and_gpu_prerequisites) says to install CUDA 10.1 or newer, which is misleading if only 10.1 is currently supported. It also explicitly says 10.2 needs to be installed in order to profile multi-gpu setups.\r\n", "2.3 will likely target CUDA 11, as was discussed in a recent SIG Build meeting", "Closing this issue now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">No</a>\n", "> 2.3 will likely target CUDA 11, as was discussed in a recent SIG Build meeting\r\n\r\n@mihaimaruseac The notes for TF 2.3.0 RC0 make no mention of CUDA 11 or 10.2, so is it still only supporting 10.1 officially?", "Reopening the issue as the support for newer CUDA has been downprioritized for TF 2.4", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Removing the staleness label, as this is an issue we need to keep open until at least TF 2.4", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">No</a>\n", "(accidental close, reopening)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Looks like tf_nightly_gpu_2.4.0 doesn't support CUDA 10.2 either. Can anyone confirm symlinking from 10.2 to 10.1 works for GPU training too?\r\n\r\nEdit: I took the plunge despite someone warning against binaries being incompatible. Seems fine so far...", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">No</a>\n", "Please have it reopened until TF gets 10.2 cuda support.", "Is there any tf version that fits CUDA 10.2?", "No. We are jumping from CUDA 10.1 to CUDA 11.", "TF 2.4 is released and supports CUDA 11.\r\nSee https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0 \r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39247\">No</a>\n"]}, {"number": 39246, "title": "GPU support for tf.image.resize with antialias=True", "body": "**System information**\r\n- TensorFlow version 2.1.0\r\n- Python 3.7\r\n- CUDA 10.1\r\n- Ubuntu 18.04\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nRight now `tf.image.resize` returns a CPU Tensor after being called with `antialias=True`.\r\n\r\n```python\r\nimport tensorflow as tf\r\nwith tf.device('/device:GPU:0'):\r\n    for antialias in (True, False):\r\n        x = tf.random.uniform((64, 32, 32, 1), maxval=1)\r\n        x = tf.image.resize(images, (28, 28), antialias=antialias)\r\n        print(antialias, x.device)\r\n```\r\n\r\nPrints:\r\n\r\n```\r\nTrue /job:localhost/replica:0/task:0/device:CPU:0\r\nFalse /job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\nWhen antialiasing is enabled, a CPU Tensor is returned. When it is disabled, a GPU tensor is returned.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who wants to rescale images without severe resampling artifacts.\r\n", "comments": ["Hi @kylemcdonald ,\r\n\r\ntf.image.resize would call different kernels according to the value of **antialias**\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/ops/image_ops_impl.py#L1228-L1357\r\n\r\n```\r\n      if antialias:\r\n        return resize_with_scale_and_translate('triangle')\r\n      else:\r\n        return gen_image_ops.resize_bilinear(\r\n            images_t, new_size, half_pixel_centers=True)\r\n```\r\n\r\nIf antialias is True, it calls a function resize_with_scale_and_translate, and this func calls a kernel named scale_and_translate which has no CUDA implementation yet.\r\nElse antialias is False, it calls kernel resize_bilinear which has a CUDA implementation, and you can find it here (https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/core/kernels/resize_bilinear_op.cc#L391-L413).\r\n\r\nThats the reason why the same function (Op) gets executed on different device.\r\n\r\nThanks.", "@LeicongLi Thanks for the clear explanation. @kylemcdonald Please check the above response and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39245, "title": "Add my name Kartikey Rawat", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39245) for more info**.\n\n<!-- need_sender_cla -->", "@kari-554 Thank you for your contribution. Can you please sign CLA? Thanks!"]}, {"number": 39244, "title": "Fixed auxiliary verbs in two error messages.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39244) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39244) for more info**.\n\n<!-- ok -->", "Unfortunately we do not accept these types of isolated PRs. Please see the following contribution guideline:\r\n\r\n> As every PR requires several CPU/GPU hours of CI testing, we discourage submitting PRs to fix one typo, one warning,etc. We recommend fixing the same issue at the file level at least (e.g.: fix all typos in a file, fix all compiler warning in a file, etc.)\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#general-guidelines-and-philosophy-for-contribution"]}, {"number": 39243, "title": "GPU-deterministic tf.image.resize (bilinear)", "body": "This pull request extends the deterministic functionality that is enabled by `TF_DETERMINISTIC_OPS`.\r\n\r\nPrior to the changes delivered by this pull-request, the CUDA back-prop kernels for `tf.image.resize` with `method=ResizeMethod.BILINEAR` introduced uncontrollable noise.\r\n\r\nAfter application of this pull request, setting the environment variable `TF_DETERMINISTIC_OPS` to \"true\" or \"1\" selects a new, deterministic CUDA back-prop kernel for this op.\r\n\r\nThe exact performance of the deterministic kernels relative to the pre-existing, non-deterministic kernels has not yet been tested. However, the performance is expected to be similar for pass-through (one-to-one re-sampling) and also for down-sampling (output has less pixels than input). The performance should degrade linearly in each dimension (horizontal and vertical) as that dimension is up-sampled (output has more pixels than input). For example, the back-prop for a 1:2 deterministic up-sample in one dimension could take up to twice as long. The slowdown is also proportional to the product of the up-sampling in the two dimensions. For example, the back-prop for a 1:2 deterministic up-sample in both dimensions could take up to four times (2*2) as long.\r\n\r\nThis pull request also significantly improves the test coverage of the existing, non-deterministic functionality of `method=ResizeMethod.BILINEAR`, and fixes one of the tests for `method=ResizeMethod.BICUBIC`.", "comments": ["> Thanks Duncan! I only have some minor comments.\r\n\r\nThanks Christian! I'm working on some changes to address your comments.", "@chsigg Could you take a look?", "@chsigg Can you please review this PR ? Thanks!", "@chsigg Can you please review this PR ? Thanks!", "Hi @gbaned, I addressed @chsigg's original review comments the same day he made them (2020-05-11) and he seems to have not been able to re-review for three months. Is there someone else who could be assigned to review?", "@reedwm ", "@duncanriach Sorry for the delay, can you please resolve conflicts? Thanks!", "Thanks for pushing, @gbaned. I'll be on vacation for a week. When I get back, I'll resolve the conflicts.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Yes, this is being worked on. Please leave open.", "Hi @gbaned,\r\n\r\nThe conflicts have been resolved and tested locally. I attempted to capture goodness from this [conflicting commit](https://github.com/tensorflow/tensorflow/commit/67d15573a776119d5a544ed266dc2514ae13c3b5#diff-bb1fdb8f5be7a85b4505589762579bf7) that my changes obliterate.\r\n\r\nPlease will you now move this towards merge?", "@chsigg can you finish reviewing?", "Thank you, @chsigg, @gbaned, @cheshire, and @reedwm."]}, {"number": 39242, "title": "Static link GPU OpenGL tests", "body": "Apply \" linkstatic = True\" for all OpenGL delegate op tests.\r\nThis change fixes issue #39025", "comments": ["Is the `linkstatic = True` really necessary?  I was able to build it with:\r\n\r\n`bazel build --config android_arm64 --linkopt -lm :resize_test`\r\n\r\nwhich means, `linkopts` is the thing missing.  However, read on.  From:\r\n\r\nlibexternal_Scom_Ugoogle_Uabsl_Sabsl_Sbase_Slibexponential_Ubiased.so\r\nlibexternal_Scom_Ugoogle_Uabsl_Sabsl_Sstrings_Slibstr_Uformat_Uinternal.so\r\nlibexternal_Scom_Ugoogle_Uabsl_Sabsl_Stime_Slibtime.so\r\nlibexternal_Scom_Ugoogle_Uabsl_Sabsl_Sstrings_Slibstrings.so\r\nlibexternal_Scom_Ugoogle_Uabsl_Sabsl_Snumeric_Slibint128.so\r\n\r\nlibexternal_Sflatbuffers_Ssrc_Slibflatbuffers.so\r\n\r\nlooks like the real place where you have to add those `linkopts` `-lm` is on abseil-cpp and flatbuffers side, and not here?", "Without this change, user need to upload those shared libraries to run OpenGL test on devices.\r\nAlso like cl/262954953, the static link is already enabled for OpenCL test.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/cl/kernels/BUILD#L623"]}, {"number": 39241, "title": "Return ValueError in case of empty list input for tf.map_fn", "body": "This PR tries to address the issue raised in #39229 where\r\nempty lists input was not checked and throw out a non-obvious error:\r\n```python\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> fn = lambda x: x\r\n>>> tf.map_fn(fn, [])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/3.7/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Library/Python/3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 425, in map_fn_v2\r\n    name=name)\r\n  File \"/Library/Python/3.7/site-packages/tensorflow/python/ops/map_fn.py\", line 213, in map_fn\r\n    static_shape = elems_flat[0].shape\r\nIndexError: list index out of range\r\n>>>\r\n```\r\n\r\n\r\nThis PR update to perform a check and thrown out\r\n`ValueError(\"elems must not be empty\")` to help clarify.\r\n\r\nThis PR fixes #39229.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @edloper for the review. The PR has been updated.\r\n\r\nOn a side note, @AdrienCorenflos  suggests to return empty sequence directly (see https://github.com/tensorflow/tensorflow/issues/39229#issuecomment-625120761). \r\n\r\nDo you think this makes sense? I can go either way.", "@yongtang If `elems` consists only of structure and not values (i.e., if `tf.nest.flatten(elems)==[]`), then returning `elems` as-is might be technically reasonable.  Though if `fn_output_signature` is specified, then we'd need to return `nest.pack_structure_as(fn_output_signature, [])` instead.\r\n\r\nBut I think that anyone who's calling `map_fn` with a structure that contains no tensors is likely to be making a mistake (thinking that `elems` treats a sequence as a thing to iterate over, as opposed to as a nested container of things to iterate over).  So I think raising an exception is probably better here.\r\n\r\nCould you also update the docstring for `elems` to say something like: \"Must contain at least one tensor.\"?\r\n", "> But I think that anyone who's calling `map_fn` with a structure that contains no tensors is likely to be making a mistake (thinking that `elems` treats a sequence as a thing to iterate over, as opposed to as a nested container of things to iterate over). So I think raising an exception is probably better here.\r\n\r\nHi,\r\n\r\nEven when one is not making a mistake, this should definitely work:\r\n`tf.map_fn(lambda x: x, [[]])`\r\n If any thing it should work because this works:\r\n`tf.map_fn(lambda x: x, [np.array([5.]), []])`\r\n\r\nEDIT:\r\nThis works too `tf.map_fn(lambda x: x, [[], np.array([5.])])`", "@edloper the PR has been updated with docstring updated now.\r\n\r\n@AdrienCorenflos The case of `tf.map_fn(lambda x: x, [[]])` is actually an ambiguous one. TF is really tensor-based. So depends on how `[[]]` is interpreted, it could be a single tensor of shape `[1, 1]`, or a list of 1-element tensor with shape `[1]`, or a structured recursive empty list.\r\n\r\nI remember TF treat `dict` and `tuple` as safe structured, while `list` might be converted to tensor first. So it is quite ambiguous for `tf.map_fn`.", "> @edloper the PR has been updated with docstring updated now.\r\n> \r\n> @AdrienCorenflos The case of `tf.map_fn(lambda x: x, [[]])` is actually an ambiguous one. TF is really tensor-based. So depends on how `[[]]` is interpreted, it could be a single tensor of shape `[1, 1]`, or a list of 1-element tensor with shape `[1]`, or a structured recursive empty list.\r\n> \r\n> I remember TF treat `dict` and `tuple` as safe structured, while `list` might be converted to tensor first. So it is quite ambiguous for `tf.map_fn`.\r\n\r\nIt doesn't work with a tuple either:\r\n`tf.map_fn(lambda x: x, ()` raises the same error\r\n`tf.map_fn(lambda x: x, np.array([]))` does not\r\nEDIT: `tf.map_fn(lambda x: x, ((),))` raises too", "@AdrienCorenflos The `tf.map_fn(lambda x: x, np.array([]))` case is special, as numpy array is always converted to a tensor. So it is pretty much equal to `tf.map_fn(lambda x: x, tf.constant([]))`.\r\n\r\nMy point is not about empty `tuple`/`dict`/`list` is correctly handled. As was the case in this PR, when the number of items is 0, `tf.map_fn` does not catch this error.\r\n\r\nHowever, I am trying to clarify that for `tuple` and `dict` the meaning is \"safe\", while the meaning of `list` is not \"safe\" in TF.\r\n\r\nSo, `tf.map_fn(lambda x: x, ())` is always interpreted as an empty tuple. It will never be converted to a `tf.constant([])` scalar.\r\n\r\nOn the other hand, list `[]` could be interpreted as either an empty list, or a `tf.constant([])` scalar.", "@AdrienCorenflos:\r\n\r\n`tf.map_fn(f, x)` only means \"apply `f` to each element element of `x`\" if `x` is a Tensor (or a numpy array, which TensorFlow automatically converts to a tensor.  If `x` is a `list`, or `tuple`, or `dictionary`, or anything else for which `tf.nest.is_nested(x)` returns true, then `tf.map_fn(f, x)` means \"for the nested structure `x`, whose leaf values must all be tensors with equal length N (or numpy arrays, which get converted to tensors), call `f` `N` times, each time passing in a nested value that has the same structure as `x`, but where each leaf is replaced by a single-element slice of the corresponding tensor in `x`; and then combine the resulting values by stacking corresponding slices.\"  \r\n\r\nExample: In the expression:\r\n\r\n```tf.map_fn(lambda f, [np.array([1, 2, 3, 4]), np.array([5, 6, 7, 8])])```\r\n\r\n `f` will get called **four times**, because each numpy array contains 4 elements.  It will *not be called 2 times* (which is what you'd expect if it were iterating over the list, or if the list got converted to a tensor).  Each time it's called, it will be called with a Python `list` of the form `[a, b]`, where `a` is a Tensor sliced from the first array, and `b` is a Tensor sliced from the second array.  I.e., map_fn will call the function `f` with the following values:\r\n\r\n```\r\nf([np.array([1, 2, 3, 4])[0], np.array([5, 6, 7, 8])[0])\r\nf([np.array([1, 2, 3, 4])[1], np.array([5, 6, 7, 8])[1])\r\nf([np.array([1, 2, 3, 4])[2], np.array([5, 6, 7, 8])[2])\r\nf([np.array([1, 2, 3, 4])[3], np.array([5, 6, 7, 8])[3])\r\n```\r\n\r\nI think the confusion comes from the fact that most TensorFlow ops will automatically convert an argument whose value is a list (or tuple) to a `Tensor`.  So for example `tf.reduce_sum([1, 2, 3])` will convert `[1, 2, 3]` to a Tensor, and then sum its values.  This general rule applies for all TensorFlow ops where an argument is expected to be a single tensor.  It does *not* apply to ops such as `tf.map_fn` (or `tf.cond`, or other \"control-flow-like\" ops) where an argument accepts a nested structure of tensors.  For ops where an argument accepts a nested structure (as defined by `tf.nest`), lists, tuples, etc., are never converted to tensors.\r\n\r\n@yongtang: You said *\"On the other hand, list [] could be interpreted as either an empty list, or a tf.constant([]) scalar.\"*  The only place I know of where lists get treated as tensors (rather than structure) for arguments that accept nested structures is inside `tf.data`.  (And I think that's an unfortunate inconsistency.)  Outside of `tf.data`, I believe that `lists` should always be treated as structure by any TensorFlow API that accepts nested structures.\r\n\r\n\r\n\r\n\r\n", "@edloper Thanks for the great explanation \ud83d\udc4d \r\n\r\n> The only place I know of where lists get treated as tensors (rather than structure) for arguments that accept nested structures is inside tf.data. (And I think that's an unfortunate inconsistency.) \r\n\r\nIndeed this is an unfortunate inconsistency. I used tf.data related APIs before so I was always under the impression that list is treated as a non-structure. It really would be great if all APIs are treated consistently across TF to avoid user confusion.", "@yongtang, @edloper Any update on this PR? Please. Thanks!", "Thanks @edloper. The PR has been updated with typo fixed."]}, {"number": 39240, "title": "ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.", "body": "I train faster RCNN for medical image classification in Google Colab. I get the following error and dont know how to fix it. This is the github page I use: https://github.com/you359/Keras-FasterRCNN/blob/master/README.md\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_frcnn.py\", line 145, in <module>\r\n    shared_layers = nn.nn_base(img_input, trainable=True)\r\n  File \"/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/resnet.py\", line 189, in nn_base\r\n    x = FixedBatchNormalization(axis=bn_axis, name='bn_conv1')(x)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 75, in symbolic_fn_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 463, in __call__\r\n    self.build(unpack_singleton(input_shapes))\r\n  File \"/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/FixedBatchNormalization.py\", line 30, in build\r\n    trainable=False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 279, in add_weight\r\n    weight = K.variable(initializer(shape, dtype=dtype),\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/initializers.py\", line 46, in __call__\r\n    return K.constant(1, shape=shape, dtype=dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 649, in constant\r\n    value, dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 970, in constant\r\n    return constant_op.constant(value, dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 262, in constant\r\n    allow_broadcast=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 291, in _constant_impl\r\n    return _eager_fill(shape.as_list(), t, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 52, in _eager_fill\r\n    dims = convert_to_eager_tensor(dims, ctx, dtypes.int32)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\n**ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.**\r\n```\r\n\r\nI could not find any threat that can help to fix this issue in the Google Colab. Thanks for any input. ", "comments": ["@Qcatbot \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here, also include your TensorFlow version.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n ", "@Qcatbot\r\nPlease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39240\">No</a>\n", "I have the same problem. I modified scripts because I am using TF2 so I updated from tf.placeholder to tf.Variable and then I found this error.\r\n\r\n#TF1\r\n#tf.placeholder(tf.int32, shape=(None, None), name='inputs'),\r\n#tf.placeholder(tf.int32, shape=(None, ), name='input_lengths'),\r\n#tf.placeholder(tf.float32, shape=(None, None, hparams.num_mels), name='mel_targets'),\r\n#tf.placeholder(tf.float32, shape=(None, None), name='token_targets'),\r\n#tf.placeholder(tf.float32, shape=(None, None, hparams.num_freq), name='linear_targets'),\r\n#tf.placeholder(tf.int32, shape=(None, ), name='targets_lengths'),\r\n#tf.placeholder(tf.int32, shape=(hparams.tacotron_num_gpus, None), name='split_infos'),\r\n#TF2\r\ntf.Variable(tf.zeros(shape=(None, None)), name='inputs', dtype=tf.int32),\r\ntf.Variable(tf.zeros(shape=(None, )), name='input_lengths', dtype=tf.int32),\r\ntf.Variable(tf.zeros(shape=(None, None, hparams.num_mels)), name='mel_targets', dtype=tf.float32),\r\ntf.Variable(tf.zeros(shape=(None, None)), name='token_targets', dtype=tf.float32),\r\ntf.Variable(tf.zeros(shape=(None, None, hparams.num_freq)), name='linear_targets', dtype=tf.float32),\r\ntf.Variable(tf.zeros(shape=(None, )), name='targets_lengths', dtype=tf.int32),\r\ntf.Variable(tf.zeros(shape=(hparams.tacotron_num_gpus, None)), name='split_infos', dtype=tf.int32),", "> I train faster RCNN for medical image classification in Google Colab. I get the following error and dont know how to fix it. This is the github page I use: https://github.com/you359/Keras-FasterRCNN/blob/master/README.md\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"train_frcnn.py\", line 145, in <module>\r\n>     shared_layers = nn.nn_base(img_input, trainable=True)\r\n>   File \"/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/resnet.py\", line 189, in nn_base\r\n>     x = FixedBatchNormalization(axis=bn_axis, name='bn_conv1')(x)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 75, in symbolic_fn_wrapper\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 463, in __call__\r\n>     self.build(unpack_singleton(input_shapes))\r\n>   File \"/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/FixedBatchNormalization.py\", line 30, in build\r\n>     trainable=False)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 279, in add_weight\r\n>     weight = K.variable(initializer(shape, dtype=dtype),\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/initializers.py\", line 46, in __call__\r\n>     return K.constant(1, shape=shape, dtype=dtype)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 649, in constant\r\n>     value, dtype=dtype, shape=shape, name=name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 970, in constant\r\n>     return constant_op.constant(value, dtype=dtype, shape=shape, name=name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 262, in constant\r\n>     allow_broadcast=True)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 291, in _constant_impl\r\n>     return _eager_fill(shape.as_list(), t, ctx)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 52, in _eager_fill\r\n>     dims = convert_to_eager_tensor(dims, ctx, dtypes.int32)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n>     return ops.EagerTensor(value, ctx.device_name, dtype)\r\n> **ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.**\r\n> ```\r\n> \r\n> I could not find any threat that can help to fix this issue in the Google Colab. Thanks for any input.\r\n\r\nDid you fixed this error? I'm having the same error", "> > I train faster RCNN for medical image classification in Google Colab. I get the following error and dont know how to fix it. This is the github page I use: https://github.com/you359/Keras-FasterRCNN/blob/master/README.md\r\n> > ```\r\n> > Traceback (most recent call last):\r\n> >   File \"train_frcnn.py\", line 145, in <module>\r\n> >     shared_layers = nn.nn_base(img_input, trainable=True)\r\n> >   File \"/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/resnet.py\", line 189, in nn_base\r\n> >     x = FixedBatchNormalization(axis=bn_axis, name='bn_conv1')(x)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 75, in symbolic_fn_wrapper\r\n> >     return func(*args, **kwargs)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 463, in __call__\r\n> >     self.build(unpack_singleton(input_shapes))\r\n> >   File \"/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/FixedBatchNormalization.py\", line 30, in build\r\n> >     trainable=False)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 279, in add_weight\r\n> >     weight = K.variable(initializer(shape, dtype=dtype),\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/keras/initializers.py\", line 46, in __call__\r\n> >     return K.constant(1, shape=shape, dtype=dtype)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 649, in constant\r\n> >     value, dtype=dtype, shape=shape, name=name)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 970, in constant\r\n> >     return constant_op.constant(value, dtype=dtype, shape=shape, name=name)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 262, in constant\r\n> >     allow_broadcast=True)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 291, in _constant_impl\r\n> >     return _eager_fill(shape.as_list(), t, ctx)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 52, in _eager_fill\r\n> >     dims = convert_to_eager_tensor(dims, ctx, dtypes.int32)\r\n> >   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n> >     return ops.EagerTensor(value, ctx.device_name, dtype)\r\n> > **ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.**\r\n> > ```\r\n> > \r\n> > \r\n> > I could not find any threat that can help to fix this issue in the Google Colab. Thanks for any input.\r\n> \r\n> Did you fixed this error? I'm having the same error\r\n\r\nHi, I have the same problem. I tried to run de keras frcnn in my machine with TF2.4.1", "> \r\n> \r\n> I have the same problem. I modified scripts because I am using TF2 so I updated from tf.placeholder to tf.Variable and then I found this error.\r\n> \r\n> #TF1\r\n> #tf.placeholder(tf.int32, shape=(None, None), name='inputs'),\r\n> #tf.placeholder(tf.int32, shape=(None, ), name='input_lengths'),\r\n> #tf.placeholder(tf.float32, shape=(None, None, hparams.num_mels), name='mel_targets'),\r\n> #tf.placeholder(tf.float32, shape=(None, None), name='token_targets'),\r\n> #tf.placeholder(tf.float32, shape=(None, None, hparams.num_freq), name='linear_targets'),\r\n> #tf.placeholder(tf.int32, shape=(None, ), name='targets_lengths'),\r\n> #tf.placeholder(tf.int32, shape=(hparams.tacotron_num_gpus, None), name='split_infos'),\r\n> #TF2\r\n> tf.Variable(tf.zeros(shape=(None, None)), name='inputs', dtype=tf.int32),\r\n> tf.Variable(tf.zeros(shape=(None, )), name='input_lengths', dtype=tf.int32),\r\n> tf.Variable(tf.zeros(shape=(None, None, hparams.num_mels)), name='mel_targets', dtype=tf.float32),\r\n> tf.Variable(tf.zeros(shape=(None, None)), name='token_targets', dtype=tf.float32),\r\n> tf.Variable(tf.zeros(shape=(None, None, hparams.num_freq)), name='linear_targets', dtype=tf.float32),\r\n> tf.Variable(tf.zeros(shape=(None, )), name='targets_lengths', dtype=tf.int32),\r\n> tf.Variable(tf.zeros(shape=(hparams.tacotron_num_gpus, None)), name='split_infos', dtype=tf.int32),\r\n\r\nHi, have you solve the error? I am facing the same problem here", "same problem ", "problem persist", "tensorflow 1.x\r\nX       = tf.placeholder(tf.float32, shape=(None, image_height, image_width, image_depth)) \r\nY_obj   = tf.placeholder(tf.float32, shape=(None, num_of_anchors,2))\r\nY_coor  = tf.placeholder(tf.float32, shape=(None, num_of_anchors,4))\r\nanch_bool = tf.placeholder(tf.float32, shape=(None, num_of_anchors))\r\n\r\ntensorflow 2.x\r\nX       = tf.Variable(tf.ones(shape=[None, image_height, image_width, image_depth]), dtype=tf.float32)\r\nY_obj   = tf.Variable(tf.ones(shape=[None, num_of_anchors, 2]), dtype=tf.float32)\r\nY_coor  = tf.Variable(tf.ones(shape=[None, num_of_anchors, 4]), dtype=tf.float32)\r\nanch_bool = tf.Variable(tf.ones(shape=[None, num_of_anchors]), dtype=tf.float32)\r\n\r\nsame problem", "same problem", "same problem.", "same problem.", "Same problem\r\n", "Looking at the [Keras-FasterRCNN repo it uses \"keras\" libraries and if one mixes that up with tf.keras equivalent libraries, this error would occur. \r\nGetting all modules from tensorflow.keras stack may be the fix"]}, {"number": 39239, "title": "add -DDEBUG_BUILD to dbg profile", "body": "this prevents issue https://github.com/tensorflow/tensorflow/issues/37498.\r\nThe optimized AWS SDK can only be built in release mode. Fall back in debug mode\r\nSee: https://github.com/TileDB-Inc/TileDB/issues/1351", "comments": ["This solution isn't sufficient in at least some cases. I'm currently doing the tensorflow vcpkg port microsoft/vcpkg/pull/13028/, where linux CI tests still fail. I fixed it by applying DEBUG_BUILD directly to third_party/aws-checksums, see PR #42743."]}, {"number": 39238, "title": "Tensorflow with gpflow ", "body": "Since tensorflow probability does not have a way of creating a custom kernel I decided to create a custom kernel on a tensorflow compatible library called gpflow and use that. \r\n\r\nI have found it necessary to manually add attributes to ensure that the kernel works but unfortunately i am getting the following error\r\n\r\n'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'feature_ndims'\r\n\r\nHow would i go about adding this feature_ndims attribute to my kernel such that this goes away? ", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.?\r\n\r\nRequest you to provide colb link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I am using tensorflow 2.1 in a google colab enviroment. ", "@lordfiftyfive \r\n\r\nRequest you to provide colb link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "x = x.astype(np.float64)\r\n#feature_ndims = 0\r\nclass Brownian(gpflow.kernels.Kernel):\r\n    def __init__(self,**kwargs):\r\n        super().__init__()\r\n        self._dtype = np.float64\r\n        dtype = kwargs.get('dtype', None)\r\n        \r\n        self._feature_ndims = [0]\r\n        #feature_ndims = kwargs.get('feature_ndims', None)\r\n        #self.variance = gpflow.Parameter(1.0, transform=positive(),dtype=dtype)\r\n        self.H = gpflow.Parameter(0.85, transform=positive(),dtype=dtype)#tf.transpose(X2)\r\n        \r\n    def K(self, x, X2=None):\r\n      if X2 is None:\r\n        X2 = X\r\n        v = (np.absolute(x))**(2*self.H)+(np.absolute(tf.transpose(X2)))**(2*self.H) - (np.absolute(x))-(np.absolute(tf.transpose(X2)))**(2*self.H)\r\n        #1/2*(1+1**(2*self.H)-1-X**(2*self.H))\r\n      return 1/2*v#.astype(x.dtype) # this returns a 2D tensor\r\n\r\n    def K_diag(self, x):\r\n      return(1/2 * tf.reshape(v, (-1,))).astype(x.dtype)            # this returns a 1D tensor\r\n    \r\n    @property\r\n    def feature_ndims(self):\r\n      return tf.keras.backend.ndim(x)\r\n\r\n\r\n    @property\r\n    def dtype(self):\r\n      #DType over which the kernel operates.\r\n      return self._dtype\r\n      \r\n    @property\r\n    def kernel(self):\r\n      return (1/2 * tf.reshape(x, (-1,)))#.astype(x.dtype)  \r\n\r\n    \r\n\r\n#k_brownian = Brownian(dtype=x.dtype)\r\nimport sys\r\nsys.setrecursionlimit(10000)\r\n\r\n   \r\n    tfp.layers.VariationalGaussianProcess(\r\n    num_inducing_points=num_inducing_points, kernel_provider=Brownian(dtype=x.dtype) , event_shape=(1,),\r\n    inducing_index_points_initializer=tf.compat.v1.constant_initializer(\r\n            np.linspace(0,x_range, num=1125,\r\n                        dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(np.float64))), mean_fn=None,\r\n    jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)", "@lordfiftyfive \r\n\r\nRequest you to provide colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\n", "```\r\nclass Brownian(gpflow.kernels.Kernel):\r\n    def __init__(self,**kwargs):\r\n        super().__init__()\r\n        self._dtype = np.float64\r\n        dtype = kwargs.get('dtype', None)\r\n        \r\n        self._feature_ndims = [0]\r\n        #feature_ndims = kwargs.get('feature_ndims', None)\r\n        #self.variance = gpflow.Parameter(1.0, transform=positive(),dtype=dtype)\r\n        self.H = gpflow.Parameter(0.85, transform=positive(),dtype=dtype)#tf.transpose(X2)\r\n        \r\n    def K(self, x, X2=None):\r\n      if X2 is None:\r\n        X2 = X\r\n        v = (np.absolute(x))**(2*self.H)+(np.absolute(tf.transpose(X2)))**(2*self.H) - (np.absolute(x))-(np.absolute(tf.transpose(X2)))**(2*self.H)\r\n        #1/2*(1+1**(2*self.H)-1-X**(2*self.H))\r\n      return 1/2*v#.astype(x.dtype) # this returns a 2D tensor\r\n\r\n    def K_diag(self, x):\r\n      return(1/2 * tf.reshape(v, (-1,))).astype(x.dtype)            # this returns a 1D tensor\r\n    \r\n    @property\r\n    def feature_ndims(self):\r\n      return tf.keras.backend.ndim(x)\r\n\r\n\r\n    @property\r\n    def dtype(self):\r\n      #DType over which the kernel operates.\r\n      return self._dtype\r\n      \r\n    @property\r\n    def kernel(self):\r\n      return (1/2 * tf.reshape(x, (-1,)))#.astype(x.dtype)  \r\n\r\n    \r\n    \"\"\"\r\n    @property\r\n    def kernels(self):\r\n    The list of kernels this _SumKernel sums over.\r\n      return self._kernels\r\n    \"\"\"\r\n#k_brownian = Brownian(dtype=x.dtype)\r\nimport sys\r\nsys.setrecursionlimit(10000)\r\n```\r\n\r\n```\r\nx_tst = x[189::]\r\nx_range = 237\r\nnum_distributions_over_Functions = 1\r\ntf.keras.backend.set_floatx('float64')\r\n#kernel = Brownian #tfp.positive_semidefinite_kernels.ExponentiatedQuadratic#MaternOneHalf()\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.Input(shape=(1,14), dtype=np.float64),\r\n    tf.keras.layers.LSTM(25,kernel_initializer='ones',activation='tanh', dtype = x.dtype, use_bias=True),\r\n    #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\r\n    tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(100,kernel_initializer='ones', use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(175,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(250,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(225,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(200,kernel_initializer='ones',use_bias=False),\r\n    #goal is to eventually replace the first dense layer with an LSTM layer\r\n    #tf.keras.layers.LSTM\r\n    #tf.keras.layers.TimeDistributed(Dense(vocabulary)))\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(150,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(125,kernel_initializer='ones', use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(100,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(75,kernel_initializer='ones', use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(50,kernel_initializer='ones',use_bias=False),\r\n    tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.Dense(25, kernel_initializer='ones',use_bias=False,),\r\n    tfp.layers.VariationalGaussianProcess(\r\n    num_inducing_points=num_inducing_points, kernel_provider=RBFKernelFn(dtype=x.dtype) , event_shape=(1,),\r\n    inducing_index_points_initializer=tf.compat.v1.constant_initializer(\r\n            np.linspace(0,x_range, num=1125,\r\n                        dtype=x.dtype)[..., np.newaxis]), unconstrained_observation_noise_variance_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(x.dtype))),variational_inducing_observations_scale_initializer=(tf.compat.v1.constant_initializer(np.log(np.expm1(1.)).astype(np.float64))), mean_fn=None,\r\n    jitter=1e-06, convert_to_tensor_fn=tfp.distributions.Distribution.sample)\r\n\r\n  \r\n    #in unconstrained thing replace astype with tf.dtype thing.  RBFKernelFn(dtype=x.dtype)  Brownian(dtype=x.dtype)#tf.initializers.constant(-10.0)\r\n])\r\n```\r\n```\r\n\r\nloss = lambda y, rv_y: rv_y.variational_loss(\r\n    y, kl_weight=np.array(batch_size, x.dtype) / x.shape[0])\r\n#tf.keras.optimizers.Adam(1e-4) tf.optimizers.Adam(learning_rate=0.011)\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.011), loss=loss)#tf.optimizers.Adam(learning_rate=0.01)\r\nmodel.fit(x, y,epochs=370, verbose=True,validation_split=0.2)\r\n```\r\n", "@lordfiftyfive \r\n\r\nLooks like code is incomplete. I am seeing the message `NameError: name 'gpflow' is not defined`.equest you to provide colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This is the code needed to resolve that issue\r\n\r\n```\r\n!pip install gpflow\r\nimport gpflow\r\n```\r\n\r\nI am also using the latest tensorflow version and tensorflow probability 0.10", "@lordfiftyfive \r\n\r\nRequest you to share colab lonk or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.I am seeing the below error message.`NameError: name 'x' is not defined`.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39238\">No</a>\n"]}]