[{"number": 29509, "title": "How to convert a tensorlfow SpaceToBatchND-Conv2D-BatchToSpaceND to a single Conv2D in tflite", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm trying to train my own deeplab model using this [code](https://github.com/tensorflow/models/tree/master/research/deeplab) and convert it to tflite.\r\nMy target is to get a model similar to [this](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite)\r\n\r\nHowever, the model is obtained contains operations like:\r\n![image](https://user-images.githubusercontent.com/43549654/59057361-135a7500-884f-11e9-9546-e2bd20e69c95.png)\r\nSpaceToBatchND and BatchToSpaceND operations are not supported by tflite + opengles backend, they reduced the model's performance on my device.\r\n\r\nIn your hosted deeplab model, those three ops are replaced by DEPTHWISE_CONV_2D v2, which has options to set dilation factor. This would be the best solution for me but I'm not sure how to convert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2).\r\n\r\nFYI, I have tried the graph_transforms tool under tensorflow/tools/graph_transforms to flatten the atrous conv. It upsampled the kernels instead of Space_To_Batch + Batch_To_Space. But this transform leads to much more computations that I cannot afford.\r\n\r\n**Describe the expected behavior**\r\n\r\nconvert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nYou can try any model under deeplab model zoo for example [http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz](url)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hey, I am currently facing the same problem. One thing that I noticed is when working with quantized model this conversion is being done. Problem is then I end up with a uint8 model.", "TOCO has a pass to do this kind of transformation:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc\r\n\r\nIsn't it working in your case?", "I'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.", "> I'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.\r\n\r\nAre you using the old converter or the new MLIR-based converter?", "Seems to happen regardless of which converter is chosen. I can ask my client to provide an untrained SavedModel as a repro, if that helps.", "I'm pretty sure it would repro with segmentation specific variant of MobileNet V3, i.e. this one: https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py, using \"large_segmentation\" config: https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_configs.py#L69.", "This is where it bails in my case (in `identify_dilated_conv.cc`):\r\n```\r\n120   Operator* bias_add_op = !has_bias_before_bts ? final_op : next_op;\r\n121   if (bias_add_op->type != OperatorType::kAdd) {\r\n122     // Bias op is required before or after BatchToSpace\r\n123     return false;\r\n124   }\r\n```", "Upon export, a _dilated_ MobileNet V3 block turns into this:\r\n![dilated_bug](https://user-images.githubusercontent.com/46361887/72696297-7eac3b00-3af0-11ea-8cf6-d45bfc7e40be.png)\r\nIn its original form, the block is defined as follows: https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py#L155\r\n", "It expects that there will be bias add either before or after BatchToSpaceNd, but it looks like the bias is folded into DepthwiseConv2D. What that Mul is doing there, I don't know.", "Minimal repro:\r\n```python3\r\n#!/usr/bin/env python3\r\n\r\nimport pathlib\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninput = keras.Input([128, 128, 3])\r\nx = keras.layers.Conv2D(8, 5, dilation_rate=2, padding=\"same\", use_bias=False)(input)\r\nx = keras.layers.BatchNormalization()(x)\r\noutput = keras.layers.ReLU()(x)\r\n\r\nm = keras.Model(inputs=input, outputs=output)\r\nout_dir = pathlib.Path(\"/tmp/minimal_bug_repro\")\r\nm.save(str(out_dir), save_format=\"tf\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))\r\ntflite_model = converter.convert()\r\noutput_file = out_dir / \"model.tflite\"\r\noutput_file.write_bytes(tflite_model)\r\n\r\nprint(f\"Converted model was written to {output_file}\")\r\n```\r\nYou get the following on the other end:\r\n![minimal_repro_bug](https://user-images.githubusercontent.com/46361887/72698007-1b71d700-3af7-11ea-9176-bf05f210c17d.png)\r\nWhich is not runnable on the GPU.", "Thanks @depthwise for the repro example.\r\n\r\nI'm working on adding support of dilated conv into the MLIR-based converter. I will update this thread when I'm finished.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509\">No</a>\n", "FYI, @haozha111, this PR improved the issue, but did not fully fix it. Here's an updated repro which demonstrates the remaining issue:\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport pathlib\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninput = keras.Input([128, 128, 3])\r\nx1 = keras.layers.Conv2D(8, 5, dilation_rate=6, padding=\"same\", use_bias=False)(input)\r\nx1 = keras.layers.BatchNormalization()(x1)\r\noutput1 = keras.layers.ReLU()(x1)\r\n\r\nx2 = keras.layers.Conv2D(8, 5, dilation_rate=12, padding=\"same\", use_bias=False)(input)\r\nx2 = keras.layers.BatchNormalization()(x2)\r\noutput1 = keras.layers.ReLU()(x2)\r\n\r\noutput = tf.concat([x1, x2], axis=3)\r\n\r\nm = keras.Model(inputs=input, outputs=output)\r\nout_dir = pathlib.Path(\"/tmp/minimal_bug_repro\")\r\nm.save(str(out_dir), save_format=\"tf\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))\r\ntflite_model = converter.convert()\r\noutput_file = out_dir / \"model.tflite\"\r\noutput_file.write_bytes(tflite_model)\r\n\r\nprint(f\"Converted model was written to {output_file}\")\r\n```\r\n\r\nThis produces the following:\r\n\r\n![minimal_repro2](https://user-images.githubusercontent.com/46361887/73354717-3bb63a00-424b-11ea-9767-5762e6863f05.png)\r\n\r\nEach of the convs in isolation is fixed, but if I concatenate them we're back to the status quo ante.", "Such dilations are commonly used in the ASPP module of segmentation models.", "That's a bit interesting. I haven't tested for this case.\r\n\r\nSo do you mean if you have only one conv2d in your graph, then it can be correctly folded?", "Looks like conv2d's by themselves work fine. The simpler repro I posted before looks \"correct\", although I have not tested this in a full blown, trained model yet.", "I tested with your new code, and convert it. Then I'm getting the tflite graph looks like the following:\r\n![mum9U9Ewi6y](https://user-images.githubusercontent.com/6316921/73408979-bbc3ba80-42b2-11ea-91ab-46f8a05219f5.png)\r\n\r\nI also have the corresponding tflite file, but not sure how to attach it here.\r\n\r\nThis suggests that the graph is converted as expected. I'm wondering if my previous change has been pushed into the nightly already. Maybe you can download tomorrow's nightly and give a try.", "OK, I'll try again. My example was converted using a build directly from master as of last night. Maybe something else got submitted in the interim.", "did you set converter.experimental_new_converter = True between those two lines?\r\n\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))\r\ntflite_model = converter.convert()`", "I'm actually converting as follows, using the master branch as of this afternoon:\r\n```bash\r\nbazel run --copt=\"-Wno-unused-result\" :tflite_convert -- \\\r\n    --saved_model_dir=/tmp/minimal_bug_repro \\\r\n    --experimental_new_converter=True --output_file=/tmp/minimal_bug_repro/model.tflite\r\n```\r\nThe original savedmodel is created with TF 2.1.0 release version, the conversion is done with `master` `tflite_convert`.\r\n\r\nHere's the original model and its conversion, in case it's useful in debugging: https://storage.googleapis.com/depthwise-temp/minimal_bug_repro.zip", "For me the graph looks like the image I posted previously, incorrect.", "I could reproduce the issue with your saved model, but the graph I posted before is from the keras testing code. I'm not sure why there is a difference here.\r\n\r\nDigging it further, I found that there is a difference in the MLIR representation of the two models. For saved model, the IR looks like:\r\n...\r\n  %10 = \"tf.Identity\"(%arg0) {T = f32, device = \"\"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>\r\n  %11 = \"tf.Identity\"(%10) {T = f32, device = \"\"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>\r\n  %12 = \"tf.SpaceToBatchND\"(%11, %0, %1) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<36x26x26x3xf32>\r\n  %13 = \"tf.SpaceToBatchND\"(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>\r\n  %14 = \"tf.Conv2D\"(%12, %8) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>\r\n  %15 = \"tf.BatchToSpaceND\"(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\r\n  %y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = \"tf.FusedBatchNormV3\"(%15, %6, %7, %7, %6) {T = f32, U = f32, data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<*xf32>)\r\n  %16 = \"tf.Conv2D\"(%13, %9) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<144x15x15x3xf32>, tensor<5x5x3x8xf32>) -> tensor<144x11x11x8xf32>\r\n  %17 = \"tf.BatchToSpaceND\"(%16, %3, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\r\n  %18 = \"tf.Mul\"(%17, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\r\n  %19 = \"tf.Add\"(%18, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\r\n  %20 = \"tf.ConcatV2\"(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = \"\"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor<i32>) -> tensor<1x128x128x16xf32>\r\n  return %20 : tensor<1x128x128x16xf32>\r\n}<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\r\n  %18 = \"tf.Mul\"(%17, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\r\n  %19 = \"tf.Add\"(%18, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\r\n  %20 = \"tf.ConcatV2\"(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = \"\"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor<i32>) -> tensor<1x128x128x16xf32>\r\n  return %20 : tensor<1x128x128x16xf32>\r\n}\r\n\r\nFor the keras test model, the IR is:\r\n  %7 = \"tf.Const\"() {value = dense<12> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %8 = \"tf.Const\"() {value = dense<[[24, 28], [24, 28]]> : tensor<2x2xi32>} : () -> tensor<2x2xi32>\r\n  %9 = \"tf.Const\"() {value = dense<3> : tensor<i32>} : () -> tensor<i32>\r\n  %10 = \"tf.Identity\"(%arg0) {T = f32, device = \"\"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>\r\n  %11 = \"tf.SpaceToBatchND\"(%10, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<36x26x26x3xf32>\r\n  %12 = \"tf.Conv2D\"(%11, %2) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>\r\n  %13 = \"tf.BatchToSpaceND\"(%12, %3, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\r\n  %y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = \"tf.FusedBatchNormV3\"(%13, %0, %1, %1, %0) {T = f32, U = f32, data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<*xf32>)\r\n  %14 = \"tf.SpaceToBatchND\"(%10, %7, %8) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>\r\n  %15 = \"tf.Conv2D\"(%14, %6) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<144x15x15x3xf32>, tensor<5x5x3x8xf32>) -> tensor<144x11x11x8xf32>\r\n  %16 = \"tf.BatchToSpaceND\"(%15, %7, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\r\n  %17 = \"tf.Mul\"(%16, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\r\n  %18 = \"tf.Add\"(%17, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>\r\n  %19 = \"tf.ConcatV2\"(%y, %18, %9) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = \"\"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor<i32>) -> tensor<1x128x128x16xf32>\r\n  return %19 : tensor<1x128x128x16xf32>\r\n\r\nI think the issue is that the IR for saved model is a bit strange, notice those lines:\r\n%13 = \"tf.SpaceToBatchND\"(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = \"\"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>\r\n  %14 = \"tf.Conv2D\"(%12, %8) {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>\r\n  %15 = \"tf.BatchToSpaceND\"(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = \"\"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>\r\n\r\nThe `block_shape` parameter before/after the Conv2D op is different, which causes the pattern to not match successfully. Will investigate why the IR is different.", "Hi, Nupur, do you know what might be the cause for the difference between calling the python API and the command line tool?", "I also tried to convert the model via command line 'tflite_convert' directly (no bazel) in tf-nightly, and the model could be converted correctly. I think the issue is probably with bazel, it might be calling the TF 1 saved model loader which doesn't produce the expected result.\r\n\r\nCan you please try using `tf-nightly` and then run 'tflite_conveter' tool directly (please avoid using bazel)?\r\n\r\nThanks.", "Just to be clear, once again, the SavedModel I posted was saved using TF 2.1.0, not the new \"master\" build. It's only the converter that was built from master. Let me try to replicate this with a wheel of TF build off master to see if the model is being saved incorrectly in the first place. Build times being what they are, this will take a bit of time.", "I think the saved model is generated correctly, it doesn't matter if you generate it from TF 2.1.0 or the master build.\r\n\r\nThe issue is with the converter. When you build from master, and then run `bazel`, even if it's pulling the latest code, I think there is some mis-configuration in bazel that causes the model loading to incorrectly use the old TF 1.x codepath. So what I'm suggesting maybe the easiest approach is to download the tf-nightly, and then call 'tflite_convert' tool directly (in a virtualenv).", "Nightly as of last night LGTM, in both my full deeplab model and the minimal example. Thanks for the quick fix, much appreciated. This was blocking things in unpleasant ways.", "Great to hear that!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509\">No</a>\n", "Is this fix going to be released in TF 2.6?", "> Is this fix going to be released in TF 2.6?\r\n\r\nTF 2.6 is published in August. I think this fix should already been included.", "@haozha111 thanks for your response, I am currently using the stable TF 2.6 release and I am having exactly same issue, where the tf lite converter is breaking the dilated convolution down to the spacetodepth, conv2d and depth to space and then does not apply quantization to conv2d. I checked the QAT model and looks fine and there are no spaceToDepth and depthToSpace layers in the model summary. I did not encounter this problem in TF 1.15. The issue that this creates is that the model that is quantized and converted in TF 1.15 runs faster than one quantized and converted in TF 2.6, due to the float conv layers.\r\n\r\nConverter params being used for your reference - \r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(inferenceModel)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.change_concat_input_ranges = True\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n```", "> @haozha111 thanks for your response, I am currently using the stable TF 2.6 release and I am having exactly same issue, where the tf lite converter is breaking the dilated convolution down to the spacetodepth, conv2d and depth to space and then does not apply quantization to conv2d. I checked the QAT model and looks fine and there are no spaceToDepth and depthToSpace layers in the model summary. I did not encounter this problem in TF 1.15. The issue that this creates is that the model that is quantized and converted in TF 1.15 runs faster than one quantized and converted in TF 2.6, due to the float conv layers.\r\n> \r\n> Converter params being used for your reference -\r\n> \r\n> ```\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(inferenceModel)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> converter.inference_input_type = tf.int8\r\n> converter.inference_output_type = tf.int8\r\n> converter.change_concat_input_ranges = True\r\n> converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n> ```\r\n\r\nThanks for the info. Have you validated if this code works in tf-nightly? If not, I'm guessing this is a new problem we need to fix.", "@haozha111 I have not tested on TF nightly, however, tested on latest stable TF 2.6 release. Also, TFMOT 0.7.0 and tf-nightly-gpu = '2.7.0-dev20210806' seem to be giving build issues. Is there a 1:1 match for both that works well?", "> @haozha111 I have not tested on TF nightly, however, tested on latest stable TF 2.6 release. Also, TFMOT 0.7.0 and tf-nightly-gpu = '2.7.0-dev20210806' seem to be giving build issues. Is there a 1:1 match for both that works well?\r\n\r\nI would suggest that we test it on tf-nightly, since this is the most updated code.", "> > @haozha111 I have not tested on TF nightly, however, tested on latest stable TF 2.6 release. Also, TFMOT 0.7.0 and tf-nightly-gpu = '2.7.0-dev20210806' seem to be giving build issues. Is there a 1:1 match for both that works well?\r\n> \r\n> I would suggest that we test it on tf-nightly, since this is the most updated code.\r\n\r\nI did try to run on tf nightly but I got errors related to tfmot, looking up further I saw that it was tfmot compatibility issue with tf-nightly gpu version mentioned above. Anyway, I can provide you a standalone code to reproduce the issue tomorrow. Thank you for your prompt responses, appreciate it.", "> > @haozha111 I have not tested on TF nightly, however, tested on latest stable TF 2.6 release. Also, TFMOT 0.7.0 and tf-nightly-gpu = '2.7.0-dev20210806' seem to be giving build issues. Is there a 1:1 match for both that works well?\r\n> \r\n> I would suggest that we test it on tf-nightly, since this is the most updated code.\r\n\r\n@haozha111  Please find below the code that will help you reproduce the issue.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize\r\nfrom tensorflow.python import keras\r\n\r\nl = tf.keras.layers\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\ndef functional_model():\r\n    \"\"\"Builds an MNIST functional model.\"\"\"\r\n    inp = tf.keras.Input(shape=image_input_shape())\r\n    x = l.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)\r\n    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n    # TODO(pulkitb): Add BatchNorm when transformations are ready.\r\n    # x = l.BatchNormalization()(x)\r\n    x = l.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n    x = l.Conv2D(filters=64, kernel_size=3, dilation_rate=(3, 3), padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n    x = l.Flatten()(x)\r\n    x = l.Dense(1024, activation='relu')(x)\r\n    x = l.Dropout(0.4)(x)\r\n    out = l.Dense(10, activation='softmax')(x)\r\n\r\n    return tf.keras.Model(inp, [out])\r\n\r\n\r\ndef image_input_shape(img_rows=28, img_cols=28):\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        return 1, img_rows, img_cols\r\n    else:\r\n        return img_rows, img_cols, 1\r\n\r\n\r\ndef preprocessed_data(img_rows=28,\r\n                      img_cols=28,\r\n                      num_classes=10):\r\n    \"\"\"Get data for mnist training and evaluation.\"\"\"\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    else:\r\n        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n    x_train /= 255\r\n    x_test /= 255\r\n\r\n    # convert class vectors to binary class matrices\r\n    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\n    return x_train, y_train, x_test, y_test\r\n\r\n\r\nmodel = functional_model()\r\nmodel.summary()\r\nx_train, y_train, x_test, y_test = preprocessed_data()\r\n\r\nmodel.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=500)\r\n_, model_accuracy = model.evaluate(x_test, y_test, verbose=0)\r\n\r\nprint(\"Quantizing model\")\r\n\r\nquantized_model = quantize.quantize_model(model)\r\nquantized_model.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n\r\nquantized_model.fit(x_train, y_train, batch_size=500)\r\n_, quantized_model_accuracy = quantized_model.evaluate(\r\n    x_test, y_test, verbose=0)\r\nmodel.save(\"/home/anurag/git/train_data/testOrig.h5\")\r\nquantized_model.save(\"/home/anurag/git/train_data/test.h5\")\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.change_concat_input_ranges = True\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntfliteModel = converter.convert()\r\nwith open(\"/home/anurag/git/train_data/test.tflite\", 'wb') as outfile:\r\n    outfile.write(tfliteModel)\r\n```", "> > > @haozha111 I have not tested on TF nightly, however, tested on latest stable TF 2.6 release. Also, TFMOT 0.7.0 and tf-nightly-gpu = '2.7.0-dev20210806' seem to be giving build issues. Is there a 1:1 match for both that works well?\r\n> > \r\n> > \r\n> > I would suggest that we test it on tf-nightly, since this is the most updated code.\r\n> \r\n> @haozha111 Please find below the code that will help you reproduce the issue.\r\n> \r\n> ```\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> from tensorflow_model_optimization.python.core.quantization.keras import quantize\r\n> from tensorflow.python import keras\r\n> \r\n> l = tf.keras.layers\r\n> \r\n> tf.config.run_functions_eagerly(True)\r\n> \r\n> def functional_model():\r\n>     \"\"\"Builds an MNIST functional model.\"\"\"\r\n>     inp = tf.keras.Input(shape=image_input_shape())\r\n>     x = l.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu',\r\n>                  kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)\r\n>     x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n>     # TODO(pulkitb): Add BatchNorm when transformations are ready.\r\n>     # x = l.BatchNormalization()(x)\r\n>     x = l.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu',\r\n>                  kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n>     x = l.Conv2D(filters=64, kernel_size=3, dilation_rate=(3, 3), padding='same', activation='relu',\r\n>                  kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n>     x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n>     x = l.Flatten()(x)\r\n>     x = l.Dense(1024, activation='relu')(x)\r\n>     x = l.Dropout(0.4)(x)\r\n>     out = l.Dense(10, activation='softmax')(x)\r\n> \r\n>     return tf.keras.Model(inp, [out])\r\n> \r\n> \r\n> def image_input_shape(img_rows=28, img_cols=28):\r\n>     if tf.keras.backend.image_data_format() == 'channels_first':\r\n>         return 1, img_rows, img_cols\r\n>     else:\r\n>         return img_rows, img_cols, 1\r\n> \r\n> \r\n> def preprocessed_data(img_rows=28,\r\n>                       img_cols=28,\r\n>                       num_classes=10):\r\n>     \"\"\"Get data for mnist training and evaluation.\"\"\"\r\n>     (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n> \r\n>     if tf.keras.backend.image_data_format() == 'channels_first':\r\n>         x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n>         x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n>     else:\r\n>         x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n>         x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n> \r\n>     x_train = x_train.astype('float32')\r\n>     x_test = x_test.astype('float32')\r\n>     x_train /= 255\r\n>     x_test /= 255\r\n> \r\n>     # convert class vectors to binary class matrices\r\n>     y_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n>     y_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n> \r\n>     return x_train, y_train, x_test, y_test\r\n> \r\n> \r\n> model = functional_model()\r\n> model.summary()\r\n> x_train, y_train, x_test, y_test = preprocessed_data()\r\n> \r\n> model.compile(\r\n>     loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n> model.fit(x_train, y_train, batch_size=500)\r\n> _, model_accuracy = model.evaluate(x_test, y_test, verbose=0)\r\n> \r\n> print(\"Quantizing model\")\r\n> \r\n> quantized_model = quantize.quantize_model(model)\r\n> quantized_model.compile(\r\n>     loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n> \r\n> quantized_model.fit(x_train, y_train, batch_size=500)\r\n> _, quantized_model_accuracy = quantized_model.evaluate(\r\n>     x_test, y_test, verbose=0)\r\n> model.save(\"/home/anurag/git/train_data/testOrig.h5\")\r\n> quantized_model.save(\"/home/anurag/git/train_data/test.h5\")\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> converter.inference_input_type = tf.int8\r\n> converter.inference_output_type = tf.int8\r\n> converter.change_concat_input_ranges = True\r\n> converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n> tfliteModel = converter.convert()\r\n> with open(\"/home/anurag/git/train_data/test.tflite\", 'wb') as outfile:\r\n>     outfile.write(tfliteModel)\r\n> ```\r\n\r\n@haozha111 any updates on this?\r\n\r\n", "sorry for the delay. will take a deeper look today.", "Hello @haozha111 thank you for looking into this, @jvishnuvardhan recommended me to create a new issue for the same and I did that yesterday. The issue number is #53025 ."]}, {"number": 29508, "title": "[TF 2.0 API Docs] tf.image.crop_and_resize", "body": "Added a usage example in image.crop_and_resize and under image_ops_impl.py. The link to the issue is https://github.com/tensorflow/tensorflow/issues/29507", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29508) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29508) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 29507, "title": "[TF 2.0 API Docs] tf.image.crop_and_resize", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/crop_and_resize\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Raises listed and defined\r\n\r\nRaises are not defined\r\n\r\n### Usage example\r\n\r\nNo usage example is given\r\n\r\n### Submit a pull request?\r\n\r\nYes\r\nhttps://github.com/tensorflow/tensorflow/pull/29508", "comments": ["Closing this issue since the associated PR has been merged. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29507\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29507\">No</a>\n"]}, {"number": 29506, "title": "[TF2.0-nightly] GRU/LSTM layers don't use cuDNN properly ", "body": "**System information**\r\n- Have I written custom code : Yes\r\n- OS Platform and Distribution :\r\n     - Ubuntu 16.04 + Docker 18.09.6-ce\r\n     - Arch Linux 5.1.5\r\n- TensorFlow installed from : pip install tf-nightly-gpu-2.0-preview\r\n- TensorFlow version : 2.0.0-dev20190606, but every nightly since 2.0.0-dev20190319 presents the same behaviour.\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CUDA V10.0.130 / cuDNN 7.5.0.56\r\n- GPU model and memory:\r\n     - Nvidia GTX 980Ti (6GB)\r\n     - Nivida GTX 1070 (8GB)\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nGRU/LSTM layers don't use the cuDNN implementation properly, resulting in much worse performance.  Let's take for example this toy network :\r\n\r\n```\r\n# Imports\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.executing_eagerly()\r\nprint('TensorFlow version: ' + str(tf.__version__))\r\n\r\n# Print checks\r\nfrom tensorflow.python.eager import context\r\nprint('Executing eagerly? : ' + str(context.executing_eagerly()))\r\nprint('Number of GPUs: ' + str(context.num_gpus()))\r\n\r\n# Generate random data\r\nX = np.random.rand(6720,700,3)\r\ny = X[:,1,1]\r\nprint('Shapes: ', X.shape, y.shape)\r\n\r\n# Define toy network\r\ninput_shape = X.shape[2]\r\nrnn_state_size = 1\r\ntimesteps = X.shape[1]\r\n\r\ninputs = tf.keras.layers.Input(shape=[timesteps, input_shape], dtype=np.float32)\r\noutput = tf.keras.layers.LSTM(rnn_state_size)(inputs)\r\nmodel = tf.keras.Model(inputs, output)\r\nmodel.compile('rmsprop', 'mse')\r\nprint(model.summary())\r\n\r\n# Fit\r\nmodel.fit(X,y)\r\n```\r\n\r\nWith the last nightly this is what we obtain:\r\n\r\n```\r\nTensorFlow version: 2.0.0-dev20190606\r\nExecuting eagerly? : True\r\n2019-06-06 12:52:23.635654: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-06-06 12:52:23.660930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1658] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845\r\npciBusID: 0000:42:00.0\r\n2019-06-06 12:52:23.661142: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-06 12:52:23.661983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-06 12:52:23.662749: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-06 12:52:23.662937: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-06 12:52:23.663896: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-06 12:52:23.664621: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-06 12:52:23.667023: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-06 12:52:23.667936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1781] Adding visible gpu devices: 0\r\n2019-06-06 12:52:23.668222: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-06 12:52:23.756255: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b18abf73d0 executing computations on platform CUDA. Devices:\r\n2019-06-06 12:52:23.756289: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2019-06-06 12:52:23.758641: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3494060000 Hz\r\n2019-06-06 12:52:23.759820: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b18aff2990 executing computations on platform Host. Devices:\r\n2019-06-06 12:52:23.759845: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-06 12:52:23.760484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1658] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845\r\npciBusID: 0000:42:00.0\r\n2019-06-06 12:52:23.760515: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-06 12:52:23.760527: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-06 12:52:23.760537: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-06 12:52:23.760547: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-06 12:52:23.760557: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-06 12:52:23.760567: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-06 12:52:23.760577: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-06 12:52:23.761521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1781] Adding visible gpu devices: 0\r\n2019-06-06 12:52:23.761549: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-06 12:52:23.762256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-06 12:52:23.762272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1205]      0 \r\n2019-06-06 12:52:23.762280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1218] 0:   N \r\n2019-06-06 12:52:23.763253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6407 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:42:00.0, compute capability: 6.1)\r\nNumber of GPUs: 1\r\nShapes:  (6720, 700, 3) (6720,)\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 700, 3)]          0         \r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, 1)                 20        \r\n=================================================================\r\nTotal params: 20\r\nTrainable params: 20\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nTrain on 6720 samples\r\n2019-06-06 12:52:26.219667: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n6720/6720 [==============================] - 114s 17ms/sample - loss: 0.1441\r\n```\r\n\r\n\r\nWhich is much slower than what we obtained with version 2.0.0-dev20190319 and previous (including version 2.0-alpha) :\r\n\r\n```\r\nTensorFlow version: 2.0.0-dev20190319\r\nExecuting eagerly? : True\r\n2019-06-06 13:23:14.360714: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-06 13:23:14.379231: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-06-06 13:23:14.500580: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a01550ac0 executing computations on platform CUDA. Devices:\r\n2019-06-06 13:23:14.500637: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2019-06-06 13:23:14.525050: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3494060000 Hz\r\n2019-06-06 13:23:14.526497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a01662bb0 executing computations on platform Host. Devices:\r\n2019-06-06 13:23:14.526541: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-06 13:23:14.526816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1551] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845\r\npciBusID: 0000:42:00.0\r\ntotalMemory: 7.92GiB freeMemory: 6.59GiB\r\n2019-06-06 13:23:14.526860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0\r\n2019-06-06 13:23:14.526931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-06 13:23:14.527880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1082] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-06 13:23:14.527903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1088]      0 \r\n2019-06-06 13:23:14.527925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1101] 0:   N \r\n2019-06-06 13:23:14.528098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1222] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6407 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:42:00.0, compute capability: 6.1)\r\nNumber of GPUs: 1\r\nShapes:  (6720, 700, 3) (6720,)\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 700, 3)]          0         \r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, 1)                 20        \r\n=================================================================\r\nTotal params: 20\r\nTrainable params: 20\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n2019-06-06 13:23:16.864613: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n6720/6720 [==============================] - 6s 884us/sample - loss: 0.1065\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI have tried in different computers and I am able to reproduce the issue.\r\nWith the modifications from [this pull request](https://github.com/tensorflow/tensorflow/pull/29424), I obtain the same performance in the last nightly as in 2.0.0-dev20190319, but with the advantage of being able to use cuDNN with masking, which was added by @qlzh727 in [this commit](https://github.com/tensorflow/tensorflow/commit/ac04087f7fb9d535d33b800d6e2bfb82c7df7077#diff-a9f256601f2626075300a37eeb4cea5f).\r\n\r\nI am willing to contribute to solve this issue in a better way if you would like me to.\r\nThanks!", "comments": ["Thanks for reporting the issue. Let me check the details and and see if the kernel is actaully landed on GPU or not. Will reply when I have more findings.", "I think it does land on GPU, but there is some performance regression introduced by https://github.com/tensorflow/tensorflow/commit/ac04087f7fb9d535d33b800d6e2bfb82c7df7077#diff-a9f256601f2626075300a37eeb4cea5f. Let me dig deep into the root cause.", "I think I found the issue, submitting the fix now.", "Nice! I'll test the fix once you submit it.", "Should be fixed by e691be7814bcd7065950ec940456a4c9d8991645. Will be available in the next nightly build.", "> Should be fixed by [e691be7](https://github.com/tensorflow/tensorflow/commit/e691be7814bcd7065950ec940456a4c9d8991645). Will be available in the next nightly build.\r\n\r\nThanks for the fix!\r\nIt is not yet implemented in today's nightly, so I modified `recurrent_v2.py` manually to test it. \r\nThe network is learning correctly and it just takes 4 seconds per epoch now. However, I'm getting this error:\r\n\r\n`W tensorflow/core/grappler/optimizers/implementation_selector.cc:196] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_357_535' and '__inference___backward_cudnn_lstm_357_535_specialized_for_RMSprop_gradients_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_1515' both implement 'lstm_54689970-be31-4336-9a58-a64ddb74d552' but their signatures do not match.`", "ah, that warning was expected during model rewrite. Previously the function inlining was revert the model rewrite to properly kickin, and now it should properly swap the cpu kernel with gpu kernel. \r\n\r\nHaving said that all, the change I make yesterday was rollback, since it somehow breaks the correctness on GPU when the LSTM is stateful. I need to dig deep and see why that's case. If you are not building a stateful LSTM network, you can leave ur temp fix as is, which will correctly trigger do the GPU kernel, otherwise, the LSTM might predict some incorrect number after layer.reset_states().", "With the Beta I am having a very similar issue, no GPU use but the warning is slightly different.\r\n\r\n> [W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_1437_1939_specialized_for_training_Adam_gradients_gradients_lstm_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_2941' and '__inference___backward_cudnn_lstm_425_601' both implement 'lstm_3cf5bdec-2f80-424a-9b99-a4158f3beb59' but their signatures do not match.\r\n", "@mr-ubik , the beta release didn't contain my latest fix yet, sorry for the breakage. We are cherrypicking my fix into beta1, which will probably released within this week. If you need to access the latest fix now, you can use tf-nightly-gpu-2.0-preview", "Btw, there are two fixes for this bug.\r\n\r\nb2dfb9bff5b8f603c19e0a8a3f280b7724fb89c1 and d8379699d3cf5e951e03e70fcc5335726955f260. You will need both of them if you would like to patch your forked tensorflow repo.", "@qlzh727 I am running a freshly installed version `v1.12.1-3892-g127aae0 2.0.0-dev20190612` but the problem is still there when trying the following [example](https://github.com/tensorflow/tfjs-examples/blob/master/translation/python/translation.py).\r\n\r\n> 2019-06-13 10:11:22.562132: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_1317_1817_specialized_for_training_RMSprop_gradients_gradients_lstm_1_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_3180' and '__inference___backward_cudnn_lstm_843_1019' both implement 'lstm_08afbb59-9254-44a3-9bdb-4d8ae621adc2' but their signatures do not match.", "I made some tests here too.\r\n\r\nWith ``tf-nightly-gpu-2.0-preview`` and I got 90s per epoch (better 25min), but in alpha the epochs take 80s, even with the CuDNNLSTM warnings. \r\n\r\nIs this normal?", "@mr-ubik, That warning message is just a red herring. I will try to suppress it and only show it when needed. \r\n\r\nThe nightly version should have performance on-par with alpha release.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29506\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29506\">No</a>\n", "Hello @qlzh727 , any timeline for finishing the implementation of `with_mask_support` for [GRU](https://github.com/tensorflow/tensorflow/blob/ba730a4f6de09ab8635091517933462dc70e4443/tensorflow/python/keras/layers/recurrent_v2.py#L406) and [LSTM](https://github.com/tensorflow/tensorflow/blob/ba730a4f6de09ab8635091517933462dc70e4443/tensorflow/python/keras/layers/recurrent_v2.py#L927) ? Let me know if I can be of help. Thanks!", "Hi @dbuades, currently the change was blocked by another issue of tf.cond for device placement. It is been actively working on, and we should address it before the formal release.", "Masking support for LSTM and GRU with CuDNN has now been implemented [here](https://github.com/tensorflow/tensorflow/commit/c33f1d1a6186ab0f4a9ca3b9af3a7affc85f251d) by @qlzh727 and it is working properly for me. Thanks!", "I got same problem today, what should i do with keras"]}, {"number": 29505, "title": "Add missing contributor to RELEASE.md", "body": "https://github.com/tensorflow/tensorflow/pull/27271 was part of 1.14\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/compiler/xla/service/slice_sinker.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/compiler/xla/service/slice_sinker.h\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/compiler/xla/service/slice_sinker_test.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/compiler/xla/service/BUILD\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/compiler/xla/service/gpu/BUILD", "comments": ["cc: @xinan-jiang "]}, {"number": 29504, "title": "tf.function fails to parse for-loop in some circumstances", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n```python\r\n@tf.function\r\ndef tf_function_with_loop(num_iter):\r\n  digit_list = []\r\n  for i in tf.range(num_iter):\r\n    digit_list.append(i)\r\n  return tf.add_n(digit_list)\r\n\r\ntf_function_with_loop(5)\r\nfails with log posted in log section.\r\n\r\ndef function_with_loop(num_iter):\r\n  digit_list = []\r\n  for i in tf.range(num_iter):\r\n    digit_list.append(i)\r\n  return tf.add_n(digit_list)\r\n\r\nfunction_with_loop(5)  # samething without @tf.function\r\nreturns tf.Tensor(10, shape=(), dtype=int32)\r\n```\r\n**Describe the expected behavior**\r\ntf_function_with_loop(5) should return tf.Tensor(10, shape=(), dtype=int32)\r\n\r\n\r\n**Code to reproduce the issue**\r\nwritten above.\r\n\r\n**Other info / logs**\r\n\r\nValueError: Trying to capture a tensor from an inner function. This can be caused by accessing a tensor defined inside a loop or conditional body, or a subfunction, from a calling function, without going through the proper return value mechanism. Consider using TensorFlow mechanisms such as TensorArrays to return tensors from inner functions or loop / conditional bodies. Tensor: Tensor(\"TensorArrayV2Read/TensorListGetItem:0\", shape=(), dtype=int32); tensor graph: FuncGraph(name=while_body_40, id=139946430861008); this graph: FuncGraph(name=tf_function_with_loop, id=139946437179024)\r\n", "comments": ["I am able to reproduce the issue on colab with Tf 2.0.0.beta0. Thanks!", "Found this in a autograph example. It seems that you need TensorArray for this. Feel free to close it. I'm not sure if supporting python list was intended or not (since Autograph otherwise support parsing python list).\r\n\r\n```\r\n@tf.function\r\ndef square_if_positive_naive(x):\r\n  result = tf.TensorArray(tf.int32, size=x.shape[0])\r\n  for i in tf.range(x.shape[0]):\r\n    if x[i] > 0:\r\n      result = result.write(i, x[i] ** 2)\r\n    else:\r\n      result = result.write(i, x[i])\r\n  return result.stack()\r\n```\r\n\r\n\r\nsquare_if_positive_naive(tf.range(-5, 5))", "Closing this issue since the error message helps resolving it.  Feel free to reopen if have further questions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29504\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29504\">No</a>\n", "I think this deserves another look. Especially since the solution provided by the poster stops gradients from flowing back in graph mode. See: \r\n\r\n```\r\n@tf.function\r\ndef tf_function(x):\r\n  arr = tf.TensorArray(tf.float32, size=5)\r\n  with tf.GradientTape() as tape:\r\n    for i in range(5):\r\n      arr.write(i, i)\r\n    result = tf.reduce_sum(x * arr.stack())\r\n  grad = tape.gradient(result, x)\r\n  return grad\r\n\r\ndef py_function(x):\r\n  arr = tf.TensorArray(tf.float32, size=5)\r\n  with tf.GradientTape() as tape:\r\n    for i in range(5):\r\n      arr.write(i, i)\r\n    result = tf.reduce_sum(x * arr.stack())\r\n  grad = tape.gradient(result, x)\r\n  return grad\r\n\r\nx = tf.Variable(tf.constant(1.))\r\ng = tf_function(x)\r\ntf.print('return from tf_function: ', g)\r\ng = py_function(x)\r\ntf.print('return from py_function: ', g)\r\n```\r\n\r\nReturns:\r\n\r\n```\r\nreturn from tf_function:  0\r\nreturn from py_function:  10\r\n```\r\n\r\nSee this colab notebook: https://colab.research.google.com/drive/1d5E9Xs58t6lJuKFfnXwcnaxsE_fGHj7Z\r\n\r\n", "@rhezab Can you please post another issue to report your problem? Thanks!", "@ymodak  So I realized this was a stupid mistake where I should've been doing \r\n\r\n`arr = arr.write(i,i)`\r\n\r\nApparently this is required in graph mode - perhaps a side effect problem. Is this the same problem with using list appends? Why does it sometimes work and sometimes not?\r\n\r\nI guess some explanation of the above would be nice... then again maybe I need to read the AutoGraph documentation more carefully. \r\n\r\nThanks!"]}, {"number": 29503, "title": "Remove cl/225589244 relnotes", "body": "", "comments": []}, {"number": 29502, "title": "A test case for ValueError in serialization.serialize_fea", "body": "Add a test case for ValueError raised by serialization.serialize_feature_column", "comments": []}, {"number": 29501, "title": "tensorflow debugger `run -t` fails on keras", "body": "See the description at https://stackoverflow.com/questions/56452641/tensorflow-debugger-run-t-failed-running-keras-model\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nException thrown\r\n\r\n**Describe the expected behavior**\r\n\r\nRun the number of iteration as specified in the `run -t` command\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee  https://stackoverflow.com/questions/56452641/tensorflow-debugger-run-t-failed-running-keras-model\r\n\r\n**Other info / logs**\r\n", "comments": ["I am able to reproduce the reported issue with tensorflow 1.13.1 version. Thanks!", "@winston-zillow  Thanks for the bug report. We'll push a commit to fix this at HEAD soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29501\">No</a>\n"]}, {"number": 29500, "title": "[ROCm] Adding ROCm support for the matrix_diag op", "body": "This PR adds ROCm support for the matrix_diag op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n---------------------------------------\r\n\r\n@tatianashp @whchung", "comments": []}, {"number": 29499, "title": "[XLA] Seed each convolution with the same rng state, so that the conv\u2026", "body": "\u2026 autotuning input is consistent even when run individually.\r\n\r\nPiperOrigin-RevId: 251332367", "comments": []}, {"number": 29498, "title": "Implement swapping the oldest tensor first", "body": "Basically, this PR implements: https://github.com/tensorflow/tensorflow/blob/a4c3fa196506d26a1604d3518c58761221e99217/tensorflow/core/kernels/stack.cc#L251\r\n\r\nThe way this is implemented is by keeping track which tensors in `Stack` are unswapped, in order from the oldest. When it's ok to swap out (the heuristic is unchanged), instead of swapping current tensor, swap the oldest unswapped tensor in `Stack` and mark it as swapped.\r\n\r\nSimilarly, for the swapping in, swapped tensors are tracked in order from the most recent. When it's ok to swap in (the heuristic is the reversal of swapping out's heuristic), asynchronously swap in the most recent swapped tensors.\r\n\r\nSince older tensors are the latest to be used again in the backpropagation phase, swapping them instead of current tensors leaves more time to asynchronously swap them back. Thus, this may reduce the duration of blockings due to waiting for tensors to be swapped back in.\r\n\r\nPotential race conditions are guarded by condition variables `cond_swapping_ins`, aside from locking the mutex `mu_`.\r\n\r\nBased on my experiments using `tf.nn.dynamic_rnn` with `swap_memory=True`, this version can reduce training times by up to ~3%, particularly on cases with large enough unrollings (timesteps).\r\n\r\nI'll be happy to explain more about this PR, thank you. :slightly_smiling_face:", "comments": ["@rthadur I'm not the best person to review this.  Maybe @ezhulenev ?", "> @rthadur I'm not the best person to review this. Maybe @ezhulenev ?\r\n\r\nthank you , reassigned.", "@rthadur I no longer work on TensorFlow.", "> @rthadur I no longer work on TensorFlow.\r\n\r\nthanks for letting me know", "@jhseu hi, here are the results of `rnn_long_sequence_benchmark` as shown by the tables below.\r\n\r\nFirstly, I ran these with a Tesla P4 (8GB memory) on a cloud instance. With the default batch size of 512, I got memory errors (OOM), so I lowered it to 232. That value was found by binary searching from 0 to 512, and 232 was the highest that didn't cause OOM for my machine. With a high enough batch size, the expectation is that TensorFlow will swap more tensors, showing more differences between the original and modified memory swapping.\r\n\r\nLooking at both tables, it seems that the change reduces training times, although by a small margin (less than 1%), but t-test shows they're statistically different. On the PR description I mentioned that in my own experiments, the change reduces around 3% of training times. In those experiments, the durations are hours, so I suspect that the change reduces training time more significantly on longer trainings.\r\n\r\n`swap-oldest-tensor-first` branch:\r\n\r\n```\r\nCalculation: Long LSTM Sequence\r\n  batch    len     units   dynamic         elapsed_t       elapsed_t/len\r\n  232      1000    512     True    1.963648        0.001964\r\n  232      1000    512     True    1.971784        0.001972\r\n  232      100     512     True    0.194638        0.001946\r\n  232      100     512     True    0.194937        0.001949\r\n  232      100     512     True    0.194494        0.001945\r\n  232      200     512     True    0.385660        0.001928\r\n  232      200     512     True    0.385978        0.001930\r\n  232      200     512     True    0.385486        0.001927\r\n  232      300     512     True    0.576875        0.001923\r\n  232      300     512     True    0.573860        0.001913\r\n  232      300     512     True    0.575114        0.001917\r\n  232      400     512     True    0.765838        0.001915\r\n  232      400     512     True    0.766009        0.001915\r\n  232      400     512     True    0.766313        0.001916\r\n  232      500     512     True    0.962388        0.001925\r\n  232      500     512     True    0.961968        0.001924\r\n  232      500     512     True    0.961917        0.001924\r\n  232      600     512     True    1.151776        0.001920\r\n  232      600     512     True    1.147099        0.001912\r\n  232      600     512     True    1.146932        0.001912\r\n  232      700     512     True    1.338601        0.001912\r\n  232      700     512     True    1.338159        0.001912\r\n  232      700     512     True    1.343874        0.001920\r\n  232      800     512     True    1.526116        0.001908\r\n  232      800     512     True    1.526567        0.001908\r\n  232      800     512     True    1.529896        0.001912\r\n  232      900     512     True    1.723834        0.001915\r\n  232      900     512     True    1.726770        0.001919\r\n  232      900     512     True    1.727901        0.001920\r\n  232      1000    512     True    1.964122        0.001964\r\n  232      1000    512     True    1.965714        0.001966\r\n  232      1000    512     True    1.962936        0.001963\r\n```\r\n\r\n`master` branch:\r\n\r\n```\r\nCalculation: Long LSTM Sequence\r\n  batch    len     units   dynamic         elapsed_t       elapsed_t/len\r\n  232      1000    512     True    2.003835        0.002004\r\n  232      1000    512     True    1.999439        0.001999\r\n  232      100     512     True    0.195903        0.001959\r\n  232      100     512     True    0.195091        0.001951\r\n  232      100     512     True    0.195629        0.001956\r\n  232      200     512     True    0.389084        0.001945\r\n  232      200     512     True    0.387731        0.001939\r\n  232      200     512     True    0.388356        0.001942\r\n  232      300     512     True    0.579127        0.001930\r\n  232      300     512     True    0.578301        0.001928\r\n  232      300     512     True    0.577634        0.001925\r\n  232      400     512     True    0.769592        0.001924\r\n  232      400     512     True    0.768946        0.001922\r\n  232      400     512     True    0.770764        0.001927\r\n  232      500     512     True    0.964491        0.001929\r\n  232      500     512     True    0.968767        0.001938\r\n  232      500     512     True    0.965089        0.001930\r\n  232      600     512     True    1.156382        0.001927\r\n  232      600     512     True    1.155993        0.001927\r\n  232      600     512     True    1.153975        0.001923\r\n  232      700     512     True    1.348376        0.001926\r\n  232      700     512     True    1.348622        0.001927\r\n  232      700     512     True    1.349369        0.001928\r\n  232      800     512     True    1.534212        0.001918\r\n  232      800     512     True    1.535128        0.001919\r\n  232      800     512     True    1.538840        0.001924\r\n  232      900     512     True    1.730844        0.001923\r\n  232      900     512     True    1.731744        0.001924\r\n  232      900     512     True    1.730779        0.001923\r\n  232      1000    512     True    2.010532        0.002011\r\n  232      1000    512     True    2.009148        0.002009\r\n  232      1000    512     True    2.001267        0.002001\r\n```\r\n", "@jhseu thank you for approving the PR! :smile:\r\n\r\nHowever, I just realized that I forgot to unimport `<queue>`, reformat the code, and changed some more indices to `size_t`, which were done in the last 2 commits I just pushed.\r\n\r\nProbably one of these was what caused import/copybara to fail?", "The CI have 2 failing checks. However, the details leads to an unopenable link, so I can't get any explanation. Can I do something to help this get merged?", "Trying to trigger it again. The failure looks innocuous.", "@jhseu I've pushed an empty commit, may this be moved to the approved pr queue again? Thank you.", "(There's no need to submit an empty change to trigger the build, we can trigger it on our side.)", "It's got less failing checks now. Unfortunately, the failing one is the one I can't see the details on. Is there something I can do?", "Looks like it's getting a segfault in StackPopOp::ComputeAsync() in some internal tests, but there's no more detail than that. I can take another look later this week if you're unable to figure out why... it's strange that the open-source tests pass.", "> Looks like it's getting a segfault in StackPopOp::ComputeAsync() in some internal tests, but there's no more detail than that. I can take another look later this week if you're unable to figure out why... it's strange that the open-source tests pass.\r\n\r\nThanks for pointing out the segfault.\r\n\r\nAfter analyzing the code, I suspect that the segfault comes from `to_swap_out` and `to_swap_in` pointers, which point to `stack_`. Since `stack_` is a vector, there's a possibility that it gets moved in memory, therefore invalidating said pointers.\r\n\r\nMy proposed solution is changing those variables from pointers to values, as in the last commit I pushed. The downside is that the values get copied instead in `GetTensorToSwapOut/In` (from `*value = &(stack_[*index].value)` to `*value = stack_[*index].value`). But rnn_long_sequence_benchmark shows it doesn't affect the performance.\r\n\r\nAnother possibility I can think of is that the methods I added doesn't make use of macros like `TF_RETURN_IF_ERROR` or `OP_REQUIRES_OK_ASYNC`, but tbh I don't really understand their uses, even after reading the comments.", "Can one of the admins verify this patch?", "We are seeing some test failures internally. I kicked off another set of Kokoro tests to verify.", "@rmlarsen gentle ping if you have update on this ?", "@devinalvaro Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 29497, "title": "[ROCm] Adding ROCm support for the mirror_pad op", "body": "This PR adds ROCm support for the mirror_pad op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n--------------------------------------------\r\n\r\n@tatianashp @whchung ", "comments": []}, {"number": 29496, "title": "[ROCm] Adding ROCm support for the bucketize op", "body": "This PR adds ROCm support for the bucketize op\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n------------------------------\r\n\r\n@tatianashp @whchung\r\n\r\n\r\n", "comments": []}, {"number": 29495, "title": "Estimator API - Unexpected behaviour for Validation Loss during early training steps", "body": "**System information**\r\n- Have I written custom code : Yes\r\n- OS Platform and Distribution : Linux Ubuntu 18.04.2 LTS\r\n- TensorFlow installed from : binary\r\n- TensorFlow version : 1.12.0\r\n- Python version: 2.7.15\r\n- CUDA/cuDNN version: 10.1/7\r\n- GPU model and memory: GeForce GTX 1080Ti 11Gb\r\n\r\nI am trying the TF Estimator API for monitoring validation metrics during training. To debug my code I have set up some runs with the following:\r\n\r\n- train and val sets are the same, both consisting of 1 sample\r\n- batch size is set to 1 both for training and validation\r\n- learning rate is set to 0.0 so that no changes happen to the network\r\n\r\n**Describe the expected behavior**\r\nI would expect training and validation loss to be constant throughout training and equal to each other\r\n\r\n**Describe the current behavior**\r\nHowever, what is observed in Tensorboard is:\r\n- training loss behaves as expected\r\n- validation loss is different in the beginning, slowly converging towards the constant value of the training loss\r\n- increasing the learn rate seems to reduce this effect significantly. In the plots below all is the same apart from the learn rates which are 0.001 and 0.1\r\n\r\nMy question is if this is a bug or this behaviour is expected and if so what is the underlying mechanism driving this? \r\n\r\nlear_rate = 0.0\r\n![image](https://user-images.githubusercontent.com/18511714/59041124-ea96a780-886f-11e9-94d5-1e3db786902e.png)\r\n\r\nlearn_rate = 0.001\r\n![image](https://user-images.githubusercontent.com/18511714/59041183-0b5efd00-8870-11e9-9061-bff53435292a.png)\r\n\r\nlearn_rate = 0.1\r\n![image](https://user-images.githubusercontent.com/18511714/59041217-174abf00-8870-11e9-8ad2-eed13161450e.png)\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndata_paths = \"/home/user/Desktop/test_csv_files/1sample.csv\"\r\nbatch_size = 1\r\n\r\ndef get_batch(paths, options):\r\n    ....\r\n    return {\"images\": image_features_tensor}, labels_tensor\r\n    \r\ndef fully_connected_network(features, sizes, is_training):\r\n    inputs = features['images']\r\n    if sizes == [0]:\r\n        return inputs\r\n    for i, size in enumerate(sizes):\r\n        inputs = tf.layers.dense(inputs=inputs, units=size)\r\n        inputs = tf.nn.relu(inputs)\r\n        inputs = tf.layers.batch_normalization(\r\n            inputs, momentum=0.99, axis=-1, epsilon=0.001, training=is_training, \r\n            reuse=None, fused=USE_FUSED_BN)\r\n    return inputs\r\n    \r\ndef model_fn(features, labels, mode, params):\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        is_training = True\r\n    else:\r\n        is_training = False\r\n    print(\"IS TRAINING: \", is_training)\r\n\r\n    features = fully_connected_network(features, 1, is_training)\r\n    logits = tf.layers.dense(inputs=features, units=3)\r\n    softmax = tf.nn.softmax(logits, name=\"softmax_out\")\r\n    predicted_classes = tf.reshape(tf.math.argmax(softmax, axis=1, output_type=tf.int32), (-1, 1))\r\n\r\n    predictions = {\"classes\": predicted_classes, \"probabilities\": softmax}\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n    one_hot_labels = tf.one_hot(labels, depth=3)\r\n    loss = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits))\r\n\r\n    batch_accuracy = batch_multiclass_metrics(labels, predicted_classes)\r\n\r\n    tf.summary.scalar('accuracy', batch_accuracy)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        initial_learn_rate = tf.constant(learn_rate, tf.float32)\r\n        learn_rate = tf.train.exponential_decay(learning_rate=0.001,\r\n            global_step=tf.train.get_global_step(), decay_steps=1000,\r\n            decay_rate=0.95,staircase=True)\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate)\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        train_op = optimizer.minimize(\r\n            loss=loss, global_step=tf.train.get_global_step())\r\n        train_op = tf.group([train_op, update_ops])\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode, loss=loss, train_op=train_op)  # ,\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        eval_metric_ops = {\r\n            \"accuracy\": tf.metrics.accuracy(labels=labels, \r\n                                            predictions=predicted_classes)}\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          predictions=predictions,  # test\r\n                                          eval_metric_ops=eval_metric_ops)\r\n                                          \r\ncheckpoint_config = tf.estimator.RunConfig(\r\n        save_checkpoints_steps=1000,\r\n        save_summary_steps=200,\r\n        keep_checkpoint_max=10)\r\n\r\nestimator = tf.estimator.Estimator(\r\n    model_fn=model_fn,\r\n    params=config_model_options,\r\n    model_dir=savedir,\r\n    config=checkpoint_config,\r\n    warm_start_from=None)\r\n\r\ntrain_input_fn = lambda paths: get_batch(data_paths, options=config_train_options)\r\neval_input_fn = lambda paths: get_batch(data_paths, options=config_val_options)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=lambda: train_input_fn(data__paths),\r\n                                    max_steps=None)\r\n\r\neval_spec = tf.estimator.EvalSpec(input_fn=lambda: eval_input_fn(data_paths),\r\n                                  steps=1,\r\n                                  start_delay_secs=20,\r\n                                  throttle_secs=20)\r\n\r\ntf.estimator.train_and_evaluate(estimator=estimator,\r\n                                train_spec=train_spec,\r\n                                eval_spec=eval_spec)  \r\n```\r\n\r\n**Other info / logs**\r\nThe question was originally asked in stack overflow https://stackoverflow.com/questions/56188656/tensorflow-estimator-api-validation-loss-during-early-training-steps\r\n\r\nMany thanks,\r\nMichael\r\n", "comments": ["@michaeltrs Looks input sample.csv is missing to reproduce the reported issue here. Please provide the input data. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29494, "title": "Slow model training in Tensorflow 1.11, 1.12, 1.13", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Docker\r\n- TensorFlow version (use command below): 1.11, 1.12, 1.13.1,\r\n- Python version: Version supplied in docker container\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI have a fairly large, complicated estimator-based model that I want to train using a Tesla P100. Previously, performance has been okay:\r\n\r\n```\r\nINFO:tensorflow:loss = 5.1591277, step = 400 (5.961 sec)\r\nINFO:tensorflow:global_step/sec: 16.6121\r\nINFO:tensorflow:loss = 2.3208628, step = 500 (6.020 sec)\r\nINFO:tensorflow:global_step/sec: 16.8526\r\n```\r\n\r\nBut starting from TF 1.11 and up (as of writing newest is 1.13.1), performance has been much worse:\r\n\r\n```\r\nINFO:tensorflow:global_step/sec: 6.49846\r\nINFO:tensorflow:loss = 0.115654044, step = 2300 (15.389 sec)\r\nINFO:tensorflow:global_step/sec: 6.56155\r\nINFO:tensorflow:loss = 0.002102174, step = 2400 (15.240 sec)\r\nINFO:tensorflow:global_step/sec: 6.45419\r\nINFO:tensorflow:loss = 0.6258155, step = 2500 (15.494 sec)\r\n```\r\n\r\nUsing `tf.train.ProfilerHook`, I have generated the following performance profile for Tensorflow 1.13.1: ![profile](https://user-images.githubusercontent.com/448023/59034033-71e11c80-886a-11e9-9fd6-d590a61b82eb.PNG). The last part of the profile is several applications of `ApplyAdam` and the operations they depend on (reshapes, sums, mul, etc). It seems that these Adam applications and intermediate operations are completely serial with no parallelism. Is this expected? I would imagine it to be as parallel as the model itself is (i.e. a model with many sequential dependencies would have many sequential dependencies when computing the gradients).\r\n\r\nUnfortunately, as `tf.train.ProfilerHook` is a fairly new addition I have no execution profiles for tensorflow versions older than 1.13.1.\r\n\r\nThere are a few differences in optimizer error/warnings that might explain why this difference in training speed happens:\r\n\r\n- [ArithmeticOptimizer fails in 1.11 and up](https://github.com/tensorflow/tensorflow/issues/29052). The warning disappears if I use the nightly tensorflow docker images, but model training is still slow. A similar warning is logged in 1.10, but with a slightly different wording.\r\n- Dependency optimizer fails in 1.11 and up, but not in 1.10:\r\n    ```\r\n    2019-06-06 11:46:25.390684: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:704] Iteration = 0, topological sort failed with message: The graph     couldn't be sorted in topological order.\r\n    2019-06-06 11:46:25.563595: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:704] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.\r\n    ```\r\n\r\n[This issue](https://github.com/tensorflow/tensorflow/issues/20843) mentions a slowdown caused by additional graph optimization in 1.9. I do not think that this is the case here, as it is the steps of the model itself that is slow.\r\n\r\n**Describe the expected behavior**\r\nI would expect TF 1.11 to be as fast or faster as TF 1.10.", "comments": ["I would like to bump it: we are experiencing 1.5 times slower execution for even a fairly simple Estimator-based model.", "@VOvchinnikov Are you also using the Adam optimizer, and are you also seeing the issue from version 1.11 and up?", "@noctune In order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@noctune Are you using slim?", "> @noctune Are you using slim?\r\n\r\nNope. Not using slim."]}, {"number": 29493, "title": "TFLite_Convert error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10(10.0.17763.379)\r\n- TensorFlow installed from (source or binary): https://anaconda.org/anaconda/tensorflow-gpu\r\n- TensorFlow version (or github SHA if from source): 1.13.1\r\n\r\nConverting with next command\r\ntoco --graph_def_file=tflite_graph.pb --output_file=ssd_ocr.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_decv_values=128  --alow_custom_ops=True\r\n\r\n```\r\n2019-06-06 16:57:31.348601: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\n2019-06-06 16:57:31.708229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.61GiB\r\n2019-06-06 16:57:31.718843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-06-06 16:57:32.934642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-06 16:57:32.939220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-06-06 16:57:32.941782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n2019-06-06 16:57:32.946998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6360 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\Scripts\\toco-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_convert.py\", line 191, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 461, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 411, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-06-06 16:57:37.302686: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2019-06-06 16:57:37.303745: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\r\n2019-06-06 16:57:37.304168: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\r\n2019-06-06 16:57:37.304742: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\r\n2019-06-06 16:57:37.305160: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\r\n2019-06-06 16:57:37.305829: I tensorflow/lite/toco/import_tensorflow.cc:1373] Unable to determine output type for op: TFLite_Detection_PostProcess\r\n2019-06-06 16:57:37.388054: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 900 operators, 1355 arrays (0 quantized)\r\n2019-06-06 16:57:37.484883: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 900 operators, 1355 arrays (0 quantized)\r\n2019-06-06 16:57:39.390818: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 112 operators, 224 arrays (1 quantized)\r\n2019-06-06 16:57:39.397064: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 112 operators, 224 arrays (1 quantized)\r\n2019-06-06 16:57:39.399525: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 65 operators, 177 arrays (1 quantized)\r\n2019-06-06 16:57:39.402192: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 65 operators, 177 arrays (1 quantized)\r\n2019-06-06 16:57:39.766641: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 71 operators, 183 arrays (151 quantized)\r\n2019-06-06 16:57:39.791435: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 71 operators, 183 arrays (155 quantized)\r\n2019-06-06 16:57:39.826873: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 3: 66 operators, 178 arrays (157 quantized)\r\n2019-06-06 16:57:39.840230: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 4: 66 operators, 178 arrays (158 quantized)\r\n2019-06-06 16:57:39.849804: W tensorflow/lite/toco/graph_transformations/quantize.cc:127] Constant array anchors lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2019-06-06 16:57:39.852829: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 5: 64 operators, 176 arrays (159 quantized)\r\n2019-06-06 16:57:39.861480: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before shuffling of FC weights: 64 operators, 176 arrays (159 quantized)\r\n2019-06-06 16:57:39.866445: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 2880000 bytes, theoretical optimal value: 2160000 bytes.\r\n2019-06-06 16:57:39.868004: I tensorflow/lite/toco/toco_tooling.cc:399] Estimated count of arithmetic ops: 2.49483 billion (note that a multiply-add is counted as 2 ops).\r\n2019-06-06 16:57:39.869067: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\Scripts\\toco_from_protos-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Users\\Freyr\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\n```\r\n\r\nhttp://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\r\n\r\ndownloaded model is coco_ssd_mobilenet_v1_1.0_quant_2018_06_29\r\ntrying to convert tfile_graph from downloaded zip archive", "comments": ["Managed to do it with tflite_convert utility"]}, {"number": 29492, "title": "Multiple calls to custom layer does not work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- TensorFlow version: 2.0.0-alpha0\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nWhen trying to use a custom *layer* into a new custom *Model*, the Model's call method doesn't work when I am initializing the custom layer in the Model's constructor but when instantiating this layer directly in the call method it works\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nfrom tensorflow.keras import Model, layers\r\n\r\nclass BottleneckLayer(layers.Layer):\r\n    def __init__(self, growthRate):\r\n        super().__init__()\r\n        self.conv1 = layers.Conv2D(4 * growthRate, kernel_size=1, strides=1, padding=\"same\")\r\n        self.conv2 = layers.Conv2D(growthRate, kernel_size=3, strides=1, padding=\"same\")\r\n        self.batchNorm = layers.BatchNormalization(momentum=0.99, epsilon=0.001)\r\n        self.relu = layers.Activation(\"relu\")\r\n\r\n    def call(self, x):\r\n        y = self.batchNorm(self.relu(self.conv1(x)))\r\n        y = self.batchNorm(self.relu(self.conv2(y)))\r\n        y = layers.concatenate([x, y])\r\n        return y\r\n\r\nclass DenseNet(Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.relu = layers.Activation(\"relu\")\r\n        growthRate = 12\r\n        self.conv1 = layers.Conv2D(2 * growthRate, kernel_size=7, strides=2, padding=\"same\")\r\n        self.maxpool = layers.MaxPooling2D((2, 2), strides=2) \r\n\r\n        self.bottleneck = BottleneckLayer(growthRate)\r\n\r\n    def call(self, x):\r\n        y = self.maxpool(self.relu(self.conv1(x)))\r\n        print(y.shape)\r\n\r\n        ## putting BottleneckLayer directly in call works\r\n        for _ in range(6):\r\n            y = BottleneckLayer(12)(y)\r\n            print(y.shape)\r\n\r\n        ## this approach does not work\r\n        for _ in range(6):\r\n            y = self.bottleneck(y)\r\n            print(y.shape)\r\n\r\n        return y\r\n```\r\n\r\nSpecifically with the first approach I have\r\n``` bash \r\n(16, 56, 56, 24)                                                                 \r\n(16, 56, 56, 36)                                                                 \r\n(16, 56, 56, 48)                                                                 \r\n(16, 56, 56, 60)                                                                 \r\n(16, 56, 56, 72)                                                                 \r\n(16, 56, 56, 84)                                                                 \r\n(16, 56, 56, 96)\r\n```\r\nHowever for the second, I have the following;\r\n``` bash\r\n(16, 56, 56, 24)                                                                 \r\n(16, 56, 56, 36)                                                                 \r\n2019-06-06 14:10:22.894332: W tensorflow/core/framework/op_kernel.cc:1431] OP_RE QUIRES\r\n failed at conv_ops.cc:461 : Invalid argument: input depth must be evenly  \r\ndivisible by filter depth: 36 vs 24\r\n```\r\nThe first call to self.bottleneck worked but not the second one suggesting that we need to instantiate a new custom Bottleneck layer and cannot reuse the self.bottleneck.  \r\n", "comments": ["@vmelan Can you check with `!pip install tensorflow-gpu==2.0.0rc0` and let us know whether it was resolved or not? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29492\">No</a>\n"]}, {"number": 29491, "title": " optimize method fold_batch_norms, speed up this method  ", "body": "init method\r\nthe code of removing will cost so much time\r\n\r\nuse a new piece of code to replace the init code\r\nit will speed up.", "comments": ["@shubham769 thank you , is it possible to add a test cases around new changes.", "> @shubham769 thank you , is it possible to add a test cases around new changes.\r\n\r\n@rthadur Not actually, but these changes will not affect previous functionality", "https://github.com/tensorflow/tensorflow/pull/29421\r\n\r\nare you kidding me?", "How dare you to **steal** my code?\r\nhttps://github.com/tensorflow/tensorflow/pull/29382/files\r\nHow stupid you are to **steal** my original code which I said there is something wrong.\r\n@google-admin ", "@rthadur \r\nYou can pay attention to the time you ask him to test case, and then after you ask him, he actually ran to my pr and asked me. I have never seen such a brazen man.\r\n\r\n![\u5fae\u4fe1\u56fe\u7247_20190617110025](https://user-images.githubusercontent.com/25046253/59575730-432b3780-90ef-11e9-84e9-f5851d4f6bf2.png)\r\n![\u5fae\u4fe1\u56fe\u7247_20190617110038](https://user-images.githubusercontent.com/25046253/59575733-458d9180-90ef-11e9-8cd1-36587604a820.png)\r\n"]}, {"number": 29490, "title": "TFLite GPU Delegates: Reshape fails to reshape input matrix with batch >1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nThe following test case fails.\r\n\r\n```\r\nTEST(ReshapeTest, Smoke) {\r\n  TensorRefFloat32 augend, addend, output;\r\n  augend.type = DataType::FLOAT32;\r\n  augend.ref = 0;\r\n  augend.shape = BHWC(1, 2, 6, 1);\r\n\r\n  output.type = DataType::FLOAT32;\r\n  output.ref = 2;\r\n  output.shape = BHWC(2, 3, 2, 1);\r\n\r\n  ReshapeAttributes attr;\r\n  attr.new_shape = BHWC(2, 3, 2, 1);\r\n\r\n  SingleOpModel model({ToString(OperationType::RESHAPE), std::move(attr)},\r\n                      {augend}, {output});\r\n  ASSERT_TRUE(model.PopulateTensor(0, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}));\r\n  ASSERT_TRUE(model.Invoke(*NewReshapeNodeShader()));\r\n  EXPECT_THAT(model.GetOutput(0),\r\n              Pointwise(FloatNear(1e-6), {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}));\r\n}\r\n\r\nValue of: model.Invoke(*NewReshapeNodeShader())\r\n  Actual: false\r\nExpected: true\r\n```\r\n\r\nIf the input batch is 2:\r\n```\r\nTEST(ReshapeTest, Smoke) {\r\n  TensorRefFloat32 augend, addend, output;\r\n  augend.type = DataType::FLOAT32;\r\n  augend.ref = 0;\r\n  augend.shape = BHWC(2, 3, 2, 1);\r\n\r\n  output.type = DataType::FLOAT32;\r\n  output.ref = 2;\r\n  output.shape = BHWC(2, 2, 3, 1);\r\n\r\n  ReshapeAttributes attr;\r\n  attr.new_shape = BHWC(2, 2, 3, 1);\r\n\r\n  SingleOpModel model({ToString(OperationType::RESHAPE), std::move(attr)},\r\n                      {augend}, {output});\r\n  ASSERT_TRUE(model.PopulateTensor(0, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}));\r\n  // ASSERT_TRUE(model.PopulateTensor(1, {2, 2, 3}));\r\n  ASSERT_TRUE(model.Invoke(*NewReshapeNodeShader()));\r\n  EXPECT_THAT(model.GetOutput(0),\r\n              Pointwise(FloatNear(1e-6), {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}));\r\n}\r\n\r\nValue of: model.GetOutput(0)\r\nExpected: contains 12 values, where each value and its corresponding value in { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 } are an almost-equal pair\r\n  Actual: { 1, 2, 3, 4, 5, 6, 0, 0, 0, 0, 0, 0 }, where the value pair (0, 7) at index #6 don't match, which is 7 from 0\r\n```\r\nHowever, the test case passes if the batch dimensions are identical.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe output sequence should match the input, even when the batch dimensions are greater than 1. Why do batch dimensions need to be 1?\r\n\r\n**Other info**\r\n\r\nI'm not sure if my take is correct. `gid.z` should take the value 1 to read the second batch in the second case, but `gid.z` is always 0. Perhaps it's a bug in the command queue?\r\n", "comments": ["@mun3 \r\n\r\nTFLite GPU treats the batch dimension specially, and currently doesn't allow operations along that dimension.  If you say batch size is `n`, it needs to be true for all input / output tensors.  In other words, you cannot specify a tensor shape `[2, 1, 1, 1]` and make it `[1, 1, 1, 2]` and vice versa.\r\n\r\nGiven the TFLite GPU's background of running things as efficient and as fast as possible, it didn't consider batch sizes, or rather, was assuming everything would be running on batch size 1.  We have plans to extend and allow such reshapes in the future, but it's not prioritized at the moment.", "@impjdi I've noticed these lines https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/gl/api.cc#L390-L392\r\n```c++\r\nif (!IsBatchMatchesForAllValues(model)) {\r\n    return InvalidArgumentError(\"Only identical batch dimension is supported\");\r\n}\r\n```\r\nwhich seem to suggest things should work as long as batch dimensions match. However, the following fails:\r\n```python\r\nTEST(ReshapeTest, MultiChan2) {\r\n  TensorRefFloat32 augend, addend, output;\r\n  \r\n  int b = 4;\r\n  int h = 4;\r\n  int w = 2;\r\n  int c = 8;\r\n\r\n  augend.type = DataType::FLOAT32;\r\n  augend.ref = 0;\r\n  augend.shape = BHWC(b, h, w, c);\r\n\r\n  output.type = DataType::FLOAT32;\r\n  output.ref = 2;\r\n  output.shape = BHWC(b, h, w, c);\r\n\r\n  ReshapeAttributes attr;\r\n  attr.new_shape = BHWC(b, h, w, c);\r\n\r\n  SingleOpModel model({ToString(OperationType::RESHAPE), std::move(attr)},\r\n                      {augend}, {output});\r\n  int n = 256;\r\n  std::vector<float> vec1(n, 1);\r\n  std::iota(vec1.begin(), vec1.end(), 1);\r\n\r\n  ASSERT_TRUE(model.PopulateTensor(0, vec1));\r\n  ASSERT_TRUE(model.Invoke(*NewReshapeNodeShader()));\r\n  EXPECT_THAT(model.GetOutput(0),\r\n              Pointwise(FloatNear(1e-6), vec1));\r\n}\r\n```\r\n```\r\nValue of: model.GetOutput(0)\r\nExpected: contains 256 values, where each value and its corresponding value in { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, ... } are an almost-equal pair\r\n  Actual: { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, ... }, where the value pair (0, 133) at index #132 don't match, which is 133 from 0\r\n```\r\n\r\nIf the assumption was batch size 1, would it be a better idea to change the `IsBatchMatchesForAllValues` check to something like `IsBatchOne` to make things more consistent?\r\n```c++\r\nbool IsBatchOne(const GraphFloat32& model) {\r\n  int32_t b = model.values()[0]->tensor.shape.b;\r\n  if (b != 1) return false;\r\n  for (auto value : model.values()) {\r\n    if (value->tensor.shape.b != 1) {\r\n      return false;\r\n    }\r\n  }\r\n  return true;\r\n}\r\n```", "@mun3 \r\n\r\nI *think* the right syntax there should be:\r\n\r\n> ReshapeAttributes attr;\r\n> attr.new_shape = BHWC(1, h, w, c);\r\n\r\nbecause the reshape op is not applicable to the batch dimension.  We will check the syntax and improve the error message.", "@mun3 Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29490\">No</a>\n"]}, {"number": 29489, "title": "tf.keras InvalidArgumentError: Incompatible shapes for batch size > 1", "body": "**System information**\r\n- Have I written custom code (as opposed to using example directory):  yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  macOS\r\n- TensorFlow backend (yes / no):  yes\r\n- TensorFlow version:  v1.12.1-1845-g27cfc61581 1.14.1-dev20190514\r\n- Keras version:  using tf.keras\r\n- Python version:  3.7.3\r\n- CUDA/cuDNN version:   -\r\n- GPU model and memory:  -\r\n\r\nWhen fitting a tf.keras LSTM encoder-decoder model (or any model with Timedistributed(Dense(1)) layer at the end)\r\n\r\n    model = Sequential()\r\n    model.add(LSTM(200, activation='relu', input_shape=(72,3)))\r\n    model.add(RepeatVector(24))\r\n    model.add(LSTM(200, activation='relu', return_sequences=True))\r\n    model.add(TimeDistributed(Dense(int(100), activation='relu')))\r\n    model.add(TimeDistributed(Dense(1)))\r\n    model.compile(loss='mse', optimizer='adam')\r\n\r\nwith a `batch_size>1` I see the following Error:\r\n\r\n    keras InvalidArgumentError: Incompatible shapes: [100,24,1] vs. [100,24]\r\n\r\nwhere in this case 100 is the `batch_size` and 24 is length of the output sequence.\r\n\r\nThe training data has the shape: `[samples, time_steps, n_features]`, so something like:\r\n\r\n    X_train:(13641, 72, 3) and y_train:(13641, 24, 1)\r\n\r\n\r\n\r\n", "comments": ["the error was due to a mal formated validation dataset, which was missing the final (batch_size,n_outputs,1) dimension", "I have the same problem.can you help me ?\r\nproblem:\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [50,3] vs. [3000,3]\r\n[[{{node training/Adam/gradients/loss/dense_1_loss/mul_grad/BroadcastGradientArgs}}]]\r\n#3000 is my total text number\r\n#batch_size =50\r\n\r\ncode:\r\n\r\nsentence_input = Input(shape=(1000,), dtype='int32')\r\nembedded_sequences = embedding_layer(sentence_input)\r\nl_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\r\nl_att = AttLayer(100)(l_lstm)\r\n\r\na = Lambda(concated,output_shape=(300,))(l_att)\r\n\r\npreds = Dense(3, activation='softmax')(a)\r\nmodel = Model(sentence_input, preds)\r\nmodel.summary()\r\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\r\nprint(\"model fitting - attention network\")\r\nh = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=50, epochs=5)\r\nplt.plot(h.history[\"loss\"], label=\"train_loss\")\r\nplt.plot(h.history[\"val_loss\"], label=\"val_loss\")\r\nplt.plot(h.history[\"acc\"], label=\"train_acc\")\r\nplt.plot(h.history[\"val_acc\"], label=\"val_acc\")\r\nplt.legend()\r\nplt.show()"]}, {"number": 29488, "title": "[2.0a0 AutoGraph] segmentation fault after refactoring into smaller functions", "body": "**System information**\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0alpha0\r\n- Python version: 3.6.5\r\n\r\n**Code to reproduce the issue**\r\n<pre>\r\n# -*- coding: utf-8 -*-\r\n# @Author  : Lin Lan (ryan.linlan@gmail.com)\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, inputs):\r\n        return self.dense(inputs)\r\n\r\n\r\nmodel = Model()\r\n\r\n\r\ndef forward(x):\r\n    batch_size = x.shape[0]\r\n    ys = tf.TensorArray(tf.float32, size=batch_size)\r\n    for i in tf.range(batch_size):\r\n        y = model(x[i][tf.newaxis, :])\r\n        ys = ys.write(i, y)\r\n    return ys.stack()\r\n\r\n\r\ndef train(x, forward_func):\r\n    with tf.GradientTape() as tape:\r\n        ys = forward_func(x)\r\n        loss = tf.reduce_mean(ys)\r\n    grads = tape.gradient(loss, model.trainable_weights)\r\n    return grads\r\n\r\n\r\ndef big_train(x, forward_func):\r\n    with tf.GradientTape() as tape:\r\n        batch_size = x.shape[0]\r\n        ys = tf.TensorArray(tf.float32, size=batch_size)\r\n        for i in tf.range(batch_size):\r\n            y = model(x[i][tf.newaxis, :])\r\n            ys = ys.write(i, y)\r\n        ys = ys.stack()\r\n        loss = tf.reduce_mean(ys)\r\n    grads = tape.gradient(loss, model.trainable_weights)\r\n    return grads\r\n\r\n\r\nx = np.random.rand(10, 100).astype(np.float32)\r\n\r\ntrain(x, forward)\r\nprint(\"pass\")\r\n\r\ntf.function(big_train)(x, tf.function(forward))\r\nprint(\"pass\")\r\n\r\ntf.function(train)(x, tf.function(forward))    # lead to segmentation fault (core dumpled)\r\nprint(\"pass\")\r\n</pre>\r\n\r\n**Other info / logs**\r\nPossibly related to #29393 . Also, due to that issue, we have to decorate some nested methods with `tf.function`.\r\n@alextp @mdanatg ", "comments": ["@martinwicke I ran the above code with `tf-nightly-gpu-2.0-preview 2.0.0.dev20190606`, and the segmentation fault disappeared. However, there are some other errors. Thus, I prefer to close this issue and open another."]}, {"number": 29487, "title": "TF 2.0 tf.distribute.MirroredStrategy without methods of experimental_run_v2 experimental_distribute_dataset and so on ", "body": "TF 2.0 tf.distribute.MirroredStrategy without method of experimental_run_v2 experimental_distribute_dataset and so on \r\n\r\n<img width=\"722\" alt=\"Screen Shot 2019-06-06 at 19 55 11\" src=\"https://user-images.githubusercontent.com/6704475/59031038-2e4fd800-8895-11e9-9efa-5f2cef8903c3.png\">\r\n\r\nI tried tensorflow==2.0.0-alpha0 and tensorflow-gpu==2.0.0-alpha0 on colab and my own computer.\r\n\r\n[tutorial](https://www.tensorflow.org/alpha/guide/distribute_strategy#using_tfdistributestrategy_with_keras) and documents refer to these methods \r\n", "comments": ["have to  install  tf-nightly-gpu-2.0-preview"]}, {"number": 29486, "title": "Crash in sess.run on CPU after restoring model from checkpoint.", "body": "I am trying to perform inference on CPU for tensorflow UNET implementation after restoring from checkpoint on Windows 10 64bit with miniconda (Python 3.6.8 :: Anaconda, Inc.). Here is a list of packages installed on used environment:\r\n\r\n```\r\nPackage              Version                 \r\n-------------------- ------------------------\r\nabsl-py              0.7.1                   \r\nasn1crypto           0.24.0                  \r\nastor                0.7.1                   \r\nbackports.weakref    1.0.post1               \r\nbleach               3.1.0                   \r\ncertifi              2019.3.9                \r\ncffi                 1.12.3                  \r\nchardet              3.0.4                   \r\ncloudpickle          1.0.0                   \r\nconda                4.6.14                  \r\ncryptography         2.7                     \r\ncycler               0.10.0                  \r\ncytoolz              0.9.0.1                 \r\ndask                 1.2.2                   \r\ndecorator            4.4.0                   \r\ngast                 0.2.2                   \r\ngrpcio               1.16.1                  \r\nh5py                 2.9.0                   \r\nhtml5lib             1.0.1                   \r\nidna                 2.8                     \r\nimage-slicer         0.2.0                   \r\nimageio              2.5.0                   \r\njoblib               0.13.2                  \r\nKeras                2.2.4                   \r\nKeras-Applications   1.0.7                   \r\nKeras-Preprocessing  1.0.9                   \r\nkiwisolver           1.1.0                   \r\nMako                 1.0.9                   \r\nMarkdown             3.1                     \r\nMarkupSafe           1.1.1                   \r\nmatplotlib           3.1.0                   \r\nmenuinst             1.4.16                  \r\nmkl-fft              1.0.12                  \r\nmkl-random           1.0.2                   \r\nmkl-service          2.0.2                   \r\nmock                 3.0.5                   \r\nnetworkx             2.3                     \r\nnumpy                1.16.4                  \r\nolefile              0.46                    \r\npbr                  5.1.3                   \r\nPillow               6.0.0                   \r\npip                  19.1.1                  \r\nprotobuf             3.7.1                   \r\npycosat              0.6.3                   \r\npycparser            2.19                    \r\npygpu                0.7.6                   \r\npyOpenSSL            19.0.0                  \r\npyparsing            2.4.0                   \r\npyreadline           2.1                     \r\nPySocks              1.7.0                   \r\npython-dateutil      2.8.0                   \r\npytz                 2019.1                  \r\nPyWavelets           1.0.3                   \r\npywin32              223                     \r\nPyYAML               5.1                     \r\nrequests             2.22.0                  \r\nruamel-yaml          0.15.46                 \r\nscikit-image         0.15.0                  \r\nscikit-learn         0.21.2                  \r\nscipy                1.2.1                   \r\nsetuptools           41.0.1                  \r\nsix                  1.12.0                  \r\ntensorboard          1.13.1                  \r\ntensorflow           1.13.1                  \r\ntensorflow-estimator 1.13.0                  \r\ntermcolor            1.1.0                   \r\ntf-unet              0.1.1                   \r\nTheano               1.0.3+2.g3e47d39ac.dirty\r\ntoolz                0.9.0                   \r\ntornado              6.0.2                   \r\nurllib3              1.24.2                  \r\nwebencodings         0.5.1                   \r\nWerkzeug             0.15.2                  \r\nwheel                0.33.4                  \r\nwin-inet-pton        1.1.0                   \r\nwincertstore         0.2                     \r\n```\r\n\r\nPROBLEM: if you run test file tf_unet_roofs_test.py, then it crashes silently on sess.run, next line of code is not executed. \r\nData to reproduce: https://yadi.sk/d/g-KU9fUw41t9_Q", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Its a bug. Tensorflow is not supposed to crash, or is it?", "@stiv-yakovenko Could you please let us know if you still need help on this ? We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. If it is resolved then please feel free to move this issue to close status ? Thanks!", "Sorry, I don't even remember how i was able to reproduce this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29486\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29486\">No</a>\n"]}, {"number": 29485, "title": "[XLA] Extend the argmin/argmax test to include all numeric types", "body": "Previously the argmin/argmax test was checking onto integer types as the input, rather than all types.  Only the output needs to be restricted to integer types.\r\n", "comments": ["Is `all_types` different from the new expression?", "yes - i think it contains things like string and complex\r\n", "@cheshire gentle ping", "Sorry I don't get it: what is the semantics of argmin over floats if it has to return an integer?", "argmin looks through the tensor and returns the index of the entry with the minimum value.  As it stands the test only puts integers into the input of the argmin.  This change tests the argmin works for inputs of floats as well as integers.\r\n\r\nin the code minmaxtypes is the output type - an integer because it is an index into the tensor.  dtype is the input type - which does not have to be constrained to an integer type. \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/argmin\r\n"]}, {"number": 29484, "title": "Update README for s390x Nightly artifacts.", "body": "Updated README for s390x Nightly artifacts.\r\nWe are working on release artifacts and will do PR for release artifacts soon.", "comments": ["@rthadur @gunan recreated PR with correct cla "]}, {"number": 29483, "title": "how can fix the problem \"Memory cannot be recycled\" in  my web service  build by tensorflow + flask ", "body": "**### This is my code:**\r\n\r\n```\r\nimport numpy as np\r\nfrom flask import Flask, request\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.platform import gfile\r\nfrom app.utils.log_util import LogUtils\r\nlog = LogUtils.get_stream_logger(__name__)\r\n\r\napp = Flask(__name__)\r\n\r\nwith gfile.FastGFile('../20190517-152605.pb', 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    \r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    tf.import_graph_def(graph_def, input_map=None, name='')\r\n    # print(tf.get_default_graph().as_graph_def().node)\r\n    images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\r\n    embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\r\n    phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\r\n    model = {\r\n        'images_placeholder': images_placeholder,\r\n        'embeddings': embeddings,\r\n        'phase_train_placeholder': phase_train_placeholder\r\n    }\r\n\r\nsess = tf.Session(graph=graph)\r\n\r\n@app.route('/', methods=['POST'])\r\ndef classify():\r\n    log.info('ss')\r\n    # data = request.files.get('data').read()\r\n    face = np.load(\"test.npy\")\r\n    images_placeholder = model['images_placeholder']\r\n    embeddings = model['embeddings']\r\n    phase_train_placeholder = model['phase_train_placeholder']\r\n    result = sess.run(embeddings, feed_dict={images_placeholder: face, phase_train_placeholder: False})\r\n    print(result)\r\n    return str(200)\r\napp.run(host='127.0.0.1',port=12480)\r\n```\r\nmemory grow situation :\r\nrequest times: |1 | 2 |4 ... |100 ... n\r\nmemory: |400+MB |600+MB | 700MB ... |2GB ... 2GB\r\n\r\n- OS Platform :  ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pipenv pypi\r\n- TensorFlow version (use command below):  1.13.1\r\n- Python version: 3.6.4\r\n- run at cpu\r\n\r\n\r\n", "comments": ["@johnsonyep Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@johnsonyep Could you provide more details about the issue and context?\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29482, "title": "Update README for s390x Nightly artifacts.", "body": "Updated README for s390x Nightly artifacts.\r\nWe are working on release artifacts and will do PR for release artifacts soon.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29482) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 29481, "title": "add_update in cross-replica mode is broken (BatchNormalization layer impossible to use)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-3374-g9eb67b17bf 2.0.0-dev20190605\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nI expect to do a forward pass with a model with a BachNormalization layer in training mode, when using the `tf.distribuite.MirroredStrategy` but I can't, because it reises the following exception:\r\n\r\n> RuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.\r\n\r\nWhy it is not expected?\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work.\r\nThe commit that introduced this behavior is: https://github.com/tensorflow/tensorflow/commit/316cd57883166e6a0b4c2d0eaacebddad7675b39#diff-8eb7e20502209f082d0cb15119a50413\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential(\r\n    [\r\n        tf.keras.layers.Dense(10),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Dense(1),\r\n    ]\r\n)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    out = model(tf.zeros((1, 10)), training=True)\r\n```", "comments": ["I was able to reproduce the issue on Colab with TensorFlow version 2.0.0-dev20190605.", "does this have been resolved yet?", "I met same question recently\uff0cdoes this have been resolved yet\uff1f", "Has this been resolved?  I can't use tf.distribute.MirroredStrategy with BatchNormalization in a training loop", "Hi @galeone, when using custom training loops with DistributionStrategy you have to use `experimental_run_v2`, please see: https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29481\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29481\">No</a>\n", "HI @omalleyt12 , I'm my tests it still continues to fail, even when using `experimental_run_v2`.\r\nIf the system has more then one GPU (and thus the distribution strategy can really distribute the computation) the same exception is raised even if I change the code in this way:\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n\r\n\r\n  model = tf.keras.Sequential([\r\n        tf.keras.layers.Dense(10),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Dense(1),\r\n    ])\r\n\r\n  def forward():\r\n    return model(tf.zeros((1, 10)), training=True)\r\n    \r\n  print(strategy.experimental_run_v2(forward, args=()))\r\n```", "I hit the same problem when I use batch normalization, too.\r\n\r\n```\r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/normalization.py\", line 659, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/normalization.py\", line 556, in _fused_batch_norm\r\n    self.add_update(mean_update)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1113, in add_update\r\n    '`add_update` was called in a cross-replica context. This is not '\r\nRuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue\r\n```\r\n\r\nThe problem happens while `Model.build()`-s so there is no way to try `experimental_run_v2` for me.\r\nUsing tf-nightly-gpu-2.0-preview from today.", "@galeone @vmarkovtsev this should be fixed in the latest nightly, could you please check and confirm?", "@vmarkovtsev if it's not fixed for your use case, could you provide a simple repro?", "I confirm that this issue is fixed now. Thank you @omalleyt12 !", "Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29481\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29481\">No</a>\n", "@omalleyt12 I met the same problem in 2.0.0rc. ", "@3fen can you share a simple reproduction that is failing?", "@omalleyt12  it is similar to the main thread.\r\n\r\n```python\r\nmodel = tf.keras.Sequential(\r\n    [\r\n        tf.keras.layers.Dense(10),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Dense(1),\r\n    ]\r\n)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    out = model(tf.zeros((1, 10)), training=True)\r\n    print(out)\r\n```\r\n\r\nOutupt:\r\n> RuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.\r\n", "@3fen , when using custom training loops with DistributionStrategy you have to use experimental_run_v2, please see: https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_custom_training_loops", "@omalleyt12 Got it, thanks for the confirmation. ", "I hit the same problem when I use batch normalization, too.\r\ni am using tensorflow 2.0 b1..\r\n![image](https://user-images.githubusercontent.com/5635674/64348747-afec0900-d030-11e9-9e45-6b3c4609ed4e.png)\r\n", "@DAEHEESHIN v2.0 b1 is ancient, we are discussing the current nightly here.", "I'm having the same issue as @vmarkovtsev using tf2 and tf-nightly when I use `tf.function` and `get_concrete_function`:\r\n\r\n`tf.function(train_model.__call__, autograph=False).get_concrete_function(\r\n                tf.TensorSpec([batch_size_per_replica, *flags_obj.image_dimension], tf.float32),\r\n                ...\r\n           )`", "@gdj0nes Could you share a larger code snippet that repros this?", "@omalleyt12 i'm working on making something I can share, just wanted to make sure there wasn't already a clear fix. It seems like it's an issue with setting the `training` parameter.", "@omalleyt12, here is an example snippet using tf-nightly:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nclass Model(tf.Module):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        with self.name_scope:\r\n            self.layers = [\r\n                layers.Conv2D(10, (3, 3)),\r\n                layers.BatchNormalization()\r\n            ]\r\n\r\n    def no_param_call(self, input):\r\n        x = input\r\n        for layer in self.layers:\r\n            x = layer(x)\r\n\r\n        return x\r\n\r\n\r\n    def param_call(self, input):\r\n        x = input\r\n        for layer in self.layers:\r\n            x = layer(x, training=True)\r\n\r\n        return x\r\n\r\n\r\ndef app():\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    image_dimension = []\r\n    with strategy.scope():\r\n        model = Model()\r\n        no_param_forward_fn = tf.function(model.no_param_call, autograph=False).get_concrete_function(\r\n                tf.TensorSpec([1, 64, 64, 3], tf.float32)\r\n        )\r\n        print('no param call succeeded')\r\n        param_forward_fn = tf.function(model.param_call, autograph=False).get_concrete_function(\r\n            tf.TensorSpec([1, 64, 64, 3], tf.float32)\r\n        )\r\n        print('param call succeeded')\r\n\r\n\r\nif __name__ == '__main__':\r\n    app()\r\n```\r\n\r\nOutput:\r\n\r\n```2019-10-28 13:20:34.214602: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-28 13:20:34.225201: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbc6e390880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-10-28 13:20:34.225217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nno param call succeeded\r\nTraceback (most recent call last):\r\n  File \"/Users/{{omitted}}/scratches/scratch_62.py\", line 48, in <module>\r\n    app()\r\n  File \"/Users/{{omitted}}/scratches/scratch_62.py\", line 42, in app\r\n    tf.TensorSpec([1, 64, 64, 3], tf.float32)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 902, in get_concrete_function\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2365, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2673, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2563, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 958, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/gareth/Library/Preferences/PyCharm2019.3/scratches/scratch_62.py\", line 27, in param_call\r\n    x = layer(x, training=True)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\", line 695, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\", line 592, in _fused_batch_norm\r\n    self.add_update(mean_update)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/{{omitted}}lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 1208, in add_update\r\n    '`add_update` was called in a cross-replica context. This is not '\r\nRuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.```", "I find this issue to be very tricky.\r\n\r\nThe core of the problem comes from creating the batchnorm layer with `training=True` under the strategy scope. If you look at the code, it checks whether `training` is `True` and indeed calls `add_update`. This comes out only when you try to subclass a Keras model.\r\n\r\nI have found a workaround though: wrap your subclassed model in another one:\r\n\r\n```python\r\nmy_model = Model(...)\r\ntf.keras.Model(inputs=[tf.keras.layers.Input(shape=...)], outputs=my_model.call(training=True))\r\n```\r\n\r\nThat wrapping gives an additional bonus of being able to `build()` and properly `summary()` the model.", "I guess this issue should be re-opened, I'm facing again the same issue", "Same issue for me!\r\n\r\nIt only occurs if I use `tf.keras.callbacks.ModelCheckpoint`, no problem otherwise...", "@omalleyt12 - are you looking into this?\r\n@rchao could this one of the issues you've fixed recently with checkpointing? ", "@guptapriya Thanks for reopening this.\r\nFor your information, this issue does not seems to occur for the `.h5` format, only the TF format is is affected, as pointed out in [my recent issue](https://github.com/tensorflow/tensorflow/issues/34127) (which is a duplicate of this one, sorry):\r\n\r\n> Note: no error if we use `.h5` instead of TF checkpoint format. However, just a few hours ago, @karmel and @jvishnuvardhan [recommended me to use TF format](https://github.com/tensorflow/tensorflow/issues/34016) (because `.h5` can cause issues, okay fine). So, which one should I use finally? None is working? I am lost...\r\n\r\n", "@netw0rkf10w @vmarkovtsev  @galeone What tf version are you using? This should be fixed in the nightly", "Recently we have submitted a few fixes related to using batch norm layer with tf.distribute strategies. If you could try with the nightly and let us know if it's fixed it'd be great.", "I confirm that this issue is fixed on tf-nightly v2.1.0-dev20200104.", "@galeone @vmarkovtsev  @netw0rkf10w  Issue seems to be fixed in latest TF version. Please confirm and close the issue if it is resolved for you. Thanks!", "Any updates regarding this issue? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29481\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29481\">No</a>\n", "In TF 2.1.0 the issue is not fixed... anybody help me ?\r\nIs this issue fixed in TF 2.2 ? ", "@taki0112 I don't understand. Why didn't you simply upgrade to TF 2.2 and see if it works for you?"]}, {"number": 29480, "title": "BatchNorm behavior", "body": "**Describe the feature and the current behavior/state.**\r\nCurrently the BatchNorm layer behavior is the following (if I have correctly understood):\r\nIf `training=True`:\r\n- The output is centered and scaled using the moving mean and moving variance are updated with the current minibatch mean and variance. The moving mean and variance are updated.\r\n\r\nIf `training=False`:\r\n- The output is centered and scaled using the moving mean and moving variance. The current batch statistics are not updated. \r\n\r\nI think this is a problem in some cases. For example in pix2pix (code: https://www.tensorflow.org/alpha/tutorials/generative/pix2pix) (paper: https://arxiv.org/abs/1611.07004) the evaluation is done using the flag `training=True` in order to use also the current batch statistics (apart for the dropout layer).\r\n\r\nI think that it would be useful to add an additional flag telling whether to use the current batch statistics without updating the moving mean and moving variance. In this case it will be possible to evaluate correctly models like pix2pix. \r\n\r\nIn other words: \r\nThe batch norm call can be changed into:\r\n`keras.layers.BatchNormalization()(input, training, update_statistics)`\r\n\r\nWith the current implementation is very difficult to evaluate models in which we need to use the flag `training=True`without updating the statistics.\r\n", "comments": []}]