[{"number": 18677, "title": "Tensorflow not respecting device placement for custom ops", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609, for the custom op\r\n- **CUDA/cuDNN version**: nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n- **GPU model and memory**: GTX 1080 Ti\r\n\r\n\r\n### Exact command to reproduce\r\n\r\nFolder for reproducing the issue:\r\n\r\nhttps://github.com/proteneer/khan/tree/tf_bug_repro/gpu_featurizer/debug.py\r\n\r\n0. Compile with instructions in README.build\r\n\r\n1. Run debug.py\r\n\r\nlog device placement shows that the op is placed on a gpu:\r\n\r\n2018-04-18 17:24:01.008389: I tensorflow/core/common_runtime/placer.cc:884] Featurize: (Featurize)/job:localhost/replica:0/task:0/device:GPU:0\r\n\r\nhowever, it actually runs on the CPU:\r\n\r\n``` bash\r\nstdout: RUNNING ON CPU!! (this is emitted by the actual CPU kernel in C++)\r\n```\r\n\r\n2. Removing the reshape line (line 55) in debug.py:\r\n\r\n``` python\r\nf0, f1, f2, f3 = tf.reshape(f0, (-1, 384)), tf.reshape(f1, (-1, 384)), tf.reshape(f2, (-1, 384)), tf.reshape(f3, (-1, 384))\r\n```\r\nSuccessfully triggers the custom op to be run on the GPU:\r\n\r\n``` bash\r\nstdout: RUNNING ON GPU.. (this is emitted by the actual GPU kernel in C++)\r\n```\r\n\r\n3. Removing the registration line in ani_op.cpp:\r\n\r\n``` cpp\r\nREGISTER_KERNEL_BUILDER(Name(\"Featurize\").HostMemory(\"acs\").Device(DEVICE_CPU), AniCombined<CPUDevice>);\r\n```\r\n\r\nRestores the intended behavior as in runs properly on the GPU with or without the reshape. But cripples the op in that it only works on GPUs.\r\n\r\n### Describe the problem\r\n\r\nThis should be a bug since we should be enforcing placement of the op onto the GPU. See steps to reproduce the bug. Oddly enough the custom Featurize op in addition to the Reshape op is shown being placed on the GPU:\r\n\r\n2018-04-18 17:24:01.008389: I tensorflow/core/common_runtime/placer.cc:884] Featurize: (Featurize)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape_3: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008404: I tensorflow/core/common_runtime/placer.cc:884] Reshape_3: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape_2: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008418: I tensorflow/core/common_runtime/placer.cc:884] Reshape_2: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008432: I tensorflow/core/common_runtime/placer.cc:884] Reshape_1: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008446: I tensorflow/core/common_runtime/placer.cc:884] Reshape: (Reshape)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape_3/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008462: I tensorflow/core/common_runtime/placer.cc:884] Reshape_3/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape_2/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008476: I tensorflow/core/common_runtime/placer.cc:884] Reshape_2/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape_1/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008490: I tensorflow/core/common_runtime/placer.cc:884] Reshape_1/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nReshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 17:24:01.008505: I tensorflow/core/common_runtime/placer.cc:884] Reshape/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n\r\n### Source code / logs\r\n\r\nAll the code required is inside here:\r\n\r\nhttps://github.com/proteneer/khan/tree/tf_bug_repro/gpu_featurizer", "comments": ["Please do not use constant input -- the runtime engine in this case is allowed to run your op on CPU and cache the result as an optimization. Use a placeholder as input, or set `config.graph_options.optimizer_options.opt_level = -1` to see if the issue still exists.", "@ppwwyyxx replacing constants with placeholders and passing in via feed-dict seems to have resolved the issue. If the tensorflow team confirms that this behavior is intended (albeit a little counter-intuitive), I will close.", "This is working as intended. Closing."]}, {"number": 18676, "title": "Bug: CPU/Eigen fp16 matmul kernel fails on AVX512 systems", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n('v1.8.0-rc0-561-g075fbb59d7', '1.8.0-rc0')\r\n- **Python version**: \r\nPython 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nbazel test --config=opt  -s //tensorflow/python/kernel_tests:batch_matmul_op_test\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nbazel test --config=opt  -s //tensorflow/python/kernel_tests:batch_matmul_op_test\r\nOn machines with AVX512, batch_matmul_op_test fails. Most likely culprit is Eigen's fp16 matmul running on CPU.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n======================================================================\r\nFAIL: testBatchMatmulOp_float16_False_True_True (__main__.BatchMatmulOpTest)\r\n----------------------------------------------------------------------\r\n\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0976562, atol=0.0976562\r\nMismatched value: a is different from b.\r\n(mismatch 25.8125%)\r\n x: array([[[ 7532.,  8860.,  6856., ...,  6372.,  8070.,  8264.],\r\n        [10400.,  9840.,  7188., ...,  8360.,  7640.,  9010.],\r\n        [ 8172.,  6590.,  3046., ...,  7400.,  6224.,  7656.],...\r\n y: array([[[ 7824.,  8156.,  6804., ...,  6372.,  8064.,  8264.],\r\n        [10140., 11500.,  8380., ...,  8360.,  7644.,  9010.],\r\n        [ 8080.,  7964.,  5756., ...,  7404.,  6224.,  7656.],...\r\n\r\n----------------------------------------------------------------------\r\nRan 4 tests in 0.681s\r\n\r\nFAILED (failures=1)\r\nnot close where =  (array([0, 0, 0, ..., 9, 9, 9]), array([ 0,  0,  0, ..., 63, 63, 63]), array([ 4,  8, 10, ..., 10, 12, 13]))\r\nnot close lhs =  [ 6570.  7150.  7024. ...  8530.  7310. 10290.]\r\nnot close rhs =  [ 5820.  5290.  9060. ... 10680.  6480.  9280.]\r\nnot close dif =  [ 748. 1864. 2032. ... 2152.  832. 1008.]\r\nnot close tol =  [ 568.5  516.5  884.5 ... 1043.   633.   906. ]\r\ndtype = float16, shape = (10, 64, 30)\r\n", "comments": ["@benoitsteiner is this expected behavior?", "I've tracked the problem down to an AVX512 specific function in Eigen/src/Core/arch/CUDA/PacketMathHalf.h and have submitted a PR to Eigen that should fix the issue:\r\n\r\nhttps://bitbucket.org/eigen/eigen/pull-requests/407/fix-the-packet16h-version-of-ptranspose/diff", "Awesome! Thanks @markdryan. ", "Fixed with update to newer Eigen.", "@jbobba Could you re-open this?  The Eigen patch ( https://bitbucket.org/eigen/eigen/pull-requests/407/fix-the-packet16h-version-of-ptranspose/diff ) that fixes the issue hasn't been merged into Eigen yet and so is not included in the recent tensorflow Eigen update.  We need to get the patch merged and then do another Eigen update before this issue can be closed.\r\n\r\n\r\n\r\n", "@markdryan are you sure? I dont see the test failure anymore, so I assumed that the fix went in. ", "@jbobba Yes.  I just pulled the latest master and retested.  batch_matmul_op_test is still failing for me.", "The patch has now been merged in Eigen.  The bug will be fixed by the next Eigen update.", "Nagging Assignee @benoitsteiner: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18675, "title": "Branch 193411332", "body": "", "comments": ["cc: @jlebar ", "What should we do here? @jlebar should we disable the test or can it be fixed easily?", "> What should we do here? @jlebar should we disable the test or can it be fixed easily?\r\n\r\nI am actively looking at it.  So far it's been half a day since I've been notified that there is a test failure here.\r\n\r\nMy current status is tracked in b/78290751.", "This is a BUILD file problem.  I have a fix and am currently running it through our presubmits.", "Thank you for investigating and fixing this. This PR is replaced by a newer merge."]}, {"number": 18674, "title": "Fix issue for float16 data type with reuse in CudnnLSTM", "body": "This fix tries to address the issue raised in #18669 where for float16 data type, the reuse in CudnnLSTM throws a ValueError.\r\n\r\nThis fix fixes the issue by passing the data type. This fix fixes #18669.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18673, "title": "gradient_override_map for tf.distributions.Bernoulli", "body": "I want to override gradient calculation for a series of ops that computes\r\n\r\n`v = tf.reduce_mean(tf.distributions.Bernoulli(prob=[x]*10).sample())`\r\n\r\nHere, shape of v is same as shape of x and I want dv/dx = input gradient at v\r\n\r\nNow I know one can register gradients for a single op but how about a series of ops. If I try to register gradient for individual ops in the series I get error with shapes of input and gradient at one of the ops in the series.\r\n\r\nUpdate 1:\r\nI have written custom code for this purpose\r\nOS Platform and Distribution I am using is Ubuntu 16.04 LTS\r\nTensorFlow installed from - pip \r\nTensorFlow version - 1.5.0\r\nBazel version - NA\r\nCUDA/cuDNN version - CuDNN9\r\nGPU model and memory - GForce GTX 1060TI\r\nExact command to reproduce - NA\r\n\r\nIs this possible in tensorflow?\r\nThanks in advance!!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Update 1:\r\nI have written custom code for this purpose\r\nOS Platform and Distribution I am using is Ubuntu 16.04 LTS\r\nTensorFlow installed from - pip\r\nTensorFlow version - 1.5.0\r\nBazel version - NA\r\nCUDA/cuDNN version - CuDNN9\r\nGPU model and memory - GForce GTX 1060TI\r\nExact command to reproduce - NA", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18672, "title": "Fix gen_git_version script not being able to find git binary.", "body": "This error is happening on our Window's release builds. Making sure\r\nwe add git binary to the PATH for Bazel.", "comments": []}, {"number": 18671, "title": "[feature request] should tf.name_scope be deprecated in favor tf.variable_scope", "body": "Hi everyone.\r\n### Describe the problem\r\nFrom my understanding, I believe that the main difference between `tf.name_scope` vs `tf.variable_scope` is that `tf.name_scope` only records ops while `tf.variable_scope` records both ops and variables in the graph.\r\n\r\nThen here's the question that I'm raising, why is it still necessary to have both. IMHO it seems like a redundancy and unnecessarily cluttering the style of the code when you write since you can achieve the same and more with `tf.variable_scope`.\r\n\r\nShould `tf.name_scope` be removed in a future release?\r\n\r\n------------------------\r\nTF version: 1.7\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "> you have not filled out the following field in the issue template.\r\n\r\nupdated them, though they are not related to the question", "The distinction between the two is a little more than what you suggested, see [this StackOverflow answer](https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow?answertab=votes#tab-top) for some detail.\r\n\r\nFurthermore, given that [TensorFlow uses semantic versioning](https://www.tensorflow.org/programmers_guide/version_compat), `tf.name_scope` cannot be removed in any of the 1.x releases.\r\n\r\nClosing this issue since it isn't going to be actionable for a while.", "@asimshankar The way that I see it is that `tf.variable_scope` is a superset while `tf.name_scope` is a subset of that superset. That was my understanding from reading the stack overflow site you referenced.", "@kirk86 In fact `VariableScope` and `NameScope` are exclusive. However, for convenience, `tf.variable_scope` will create an auxiliary `tf.name_scope` by default with `auxiliary_name_scope=True`.\r\n\r\nYou can turn off the behavior:\r\n```python\r\nwith tf.variable_scope(\"a\", auxiliary_name_scope=False):\r\n```", "@facaiy What does exclusive mean in this case. Sorry if the question seems dump. I'm not that much familiar with tf.", "@kirk86 - here's an example illustrating that the two are not identical (`name_scope` does not affect the names of variables fetched from `tf.get_variable` to facilitate easier sharing of variables, irrespective of the `name_scope` under which the operations are being created).\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default():\r\n  with tf.name_scope(\"my_scope\"):\r\n    print(tf.get_variable(\"v\", 1.).name)  # Will print v:0\r\n\r\nwith tf.Graph().as_default():\r\n  with tf.variable_scope(\"my_scope\"):\r\n    print(tf.get_variable(\"v\", 1.).name)  # Will print my_scope/v:0\r\n```\r\n\r\nFor example, the following graph shares the same variable value between two different computations. And `name_scope` is used so that the different computations are grouped together nicely by name (which means they also render nicely on graph visualizations like that of tensorboard):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.name_scope(\"first_computation\"):\r\n  v = tf.get_variable(\"v\", 1.)\r\n  c1 = tf.add(v, 1.)\r\n\r\nwith tf.name_scope(\"second_computation\"):\r\n  v = tf.get_variable(\"v\", 1., reuse=True)\r\n  c2 = tf.multiply(v, 10.)\r\n```\r\n\r\nThis will create a single variable named `v`, shared between different computations.\r\nMore information in https://www.tensorflow.org/programmers_guide/variables#sharing_variables\r\n\r\nHope that helps."]}, {"number": 18670, "title": "Update depreciation.py", "body": "writing if statement in one line", "comments": ["@martinwicke made the changes ", "@martinwicke it should work now ", "As far as I can tell, this only will print a warning if warn_once is true.", "Nagging Assignee @martinwicke: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 18669, "title": "Unable to reuse opaque_kernel variable in CudnnLSTM for FP16", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary using GPUs\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**:  Cuda 9.0/ CuDNN 7.0.5\r\n- **GPU model and memory**:  Volta(V100), 16 GB\r\n- **Exact command to reproduce**: Please see the code below\r\n\r\n### Describe the problem\r\nUnable to reuse opaque_kernel variables with FP16 data type. Works for FP32 data type. Here is the stack trace:\r\n\r\n File \"cudnn_lstm_example.py\", line 37, in <module>\r\n    outputs_2, _ = lstm(inputs_2)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 696, in __call__\r\n    self.build(input_shapes)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 358, in build\r\n    \"opaque_kernel\", initializer=opaque_params_t, validate_shape=False)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1297, in get_variable\r\n    constraint=constraint)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1093, in get_variable\r\n    constraint=constraint)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 431, in get_variable\r\n    return custom_getter(**custom_getter_kwargs)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 291, in _update_trainable_weights\r\n    variable = getter(*args, **kwargs)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 408, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 758, in _get_single_variable\r\n    found_type_str))\r\nValueError: Trying to share variable cudnn_rnn/cudnn_bi_lstm/opaque_kernel, but specified dtype float32 and found dtype float16_ref.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nnum_layers = 1\r\nnum_units = 40\r\nbatch_size = 60\r\ndir_count = 2\r\n\r\ninputDType= tf.float16 # Does not work\r\n#inputDType = tf.float32 # Works\r\n\r\ninputs_1 = tf.random_uniform([\r\n    num_layers * dir_count, batch_size, num_units], dtype=inputDType)\r\n\r\ninputs_2 = tf.random_uniform([\r\n    num_layers * dir_count, batch_size, num_units], dtype=inputDType)\r\n\r\nwith tf.variable_scope(\"cudnn_rnn\", reuse=False):\r\n    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\r\n        num_layers=num_layers,\r\n        num_units=num_units,\r\n        direction=\"bidirectional\",\r\n        dtype=inputDType,\r\n        name=\"cudnn_bi_lstm\")\r\n                \r\n    outputs_1, _ = lstm(inputs_1)\r\n\r\nwith tf.variable_scope(\"cudnn_rnn\", reuse=True):\r\n    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\r\n        num_layers=num_layers,\r\n        num_units=num_units,\r\n        direction=\"bidirectional\",\r\n        dtype=inputDType,\r\n        name=\"cudnn_bi_lstm\")\r\n\r\n    outputs_2, _ = lstm(inputs_2)\r\n\r\nloss1 = tf.reduce_sum(outputs_1)\r\nloss2 = tf.reduce_sum(outputs_2)\r\nloss= loss1+loss2\r\nvar = lstm.trainable_variables[0]\r\n\r\ngrad = tf.gradients(loss, var)[0]\r\nprint('grad.shape: %s' % grad.shape)\r\nprint('var.shape: %s' % var.shape)\r\n\r\nopt = tf.train.AdamOptimizer()\r\ntrain_op = opt.apply_gradients([(grad, lstm.trainable_variables[0])])\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(outputs_1))\r\n    print(sess.run(outputs_2))\r\n    sess.run(train_op)\r\n    print(sess.run(outputs_1))\r\n    print(sess.run(outputs_2))\r\n```\r\n", "comments": ["I think the following diff might be able to fix the issue:\r\n```diff\r\ndiff --git a/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py b/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\r\nindex 00d9544..d58198f 100644\r\n--- a/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\r\n+++ b/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\r\n@@ -358,7 +358,8 @@ class _CudnnRNN(base_layer.Layer):\r\n             \"CUDA/CuDNN generations.\")\r\n       # Initialize opaque params with a tensor.\r\n       self.kernel = vs.get_variable(\r\n-          \"opaque_kernel\", initializer=opaque_params_t, validate_shape=False)\r\n+          \"opaque_kernel\", dtype=self._plain_dtype,\r\n+          initializer=opaque_params_t, validate_shape=False)\r\n     # Create saveable in the outer scope of the cudnn subgraph, such that\r\n     # alternative subgraph with platform-independent rnn cells can load the\r\n     # checkpoints directly.\r\n```", "Added a PR #18674 for the fix.", "Thanks !  This seems to fix the issue"]}, {"number": 18668, "title": "Fix issue where git_tag_override would fail if \"-\" in tag name.", "body": "", "comments": []}, {"number": 18667, "title": "Update metric_loss.py", "body": "\"1.0\" and \"0.0\" is better to understand and also signifies it as float points when compared to \"1.\" and \"0.\"", "comments": ["The syntax 1. is very common. Let's not fix what's not broken."]}, {"number": 18666, "title": "Update train.py", "body": "This fixes several warnings in tf.contrib.gan by switching to addsummaries in tfgan_losses", "comments": ["@joel-shor is `addsummaries` really the preferred name? That seems to run counter to the style guide for naming.", "I don't understand this proposed change.\r\n\r\n1) There are no arguments called `addsummaries` anywhere in TFGAN, so I suspect this PR will cause tests to fail.\r\n\r\n2) @gautam1858  What warnings are you trying to avoid with this?", "@joel-shor this WARNING:tensorflow:update_ops in create_train_op does not contain all the  update_ops \r\n\r\nYou can find the warnings here \r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb", "Nagging Reviewer @joel-shor: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Your change will cause errors, and won't fix the problem. The issue should be fixed if you use the nightly build instead of the default one, and will be fixed by default when the next TensorFlow release is made."]}, {"number": 18665, "title": "Feature request: tf.layers.Layer.{trainable_variables,updates,...} should return values from all sublayers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.2\r\n- **CUDA/cuDNN version**: 7.1\r\n- **GPU model and memory**: Nvidia Titan Xp (12 GB)\r\n\r\n### Describe the problem\r\nThe properties trainable_variables, updates, and so on of tf.layers.Layer currently only returns the corresponding objects from the layer itself. This is different from Keras where it adds the corresponding objects from all child layers.\r\nPlease consider to make the behaviour of tf.layers.Layer the same as Keras. In it's current form, these properties are not really helpful.", "comments": ["Could you go into a bit more detail on your use-case? The expectation is that Layers which have sub-Layers will inherit from `tf.keras.Model`. Most built-in Layers don't have any sub-Layers to return variables/updates from.", "Ok, I was not aware that `tf.layers.Layer` is only supposed to be used for built-in layers. My understanding was that `tf.layers` sits beside Keras and could be used on it's own. And since there was no `Network` or `Model` in `tf.layers` I used `Layer`.\r\n\r\nBut if the expectation is that everyone uses `tf.keras.Model` that's ok.", "Yes, we're trying to keep the number of symbols down, sorry for the confusion. Using Layers from `tf.keras.layers` is good too (there shouldn't be much difference and they'll both work with `tf.keras.Model`, but we're slowly migrating toward the `tf.keras.layers` versions)."]}, {"number": 18664, "title": "Fix build failure in `//tensorflow/python/kernel_tests:init_ops_test`", "body": "With the most recent master the following test fails:\r\n```\r\nbazel test -s --config=opt --cache_test_results=no //tensorflow/python/kernel_tests:init_ops_test\r\n...\r\n...\r\n...\r\n    eye = linalg_ops.eye(n, dtype=self.dtype)\r\nNameError: global name 'linalg_ops' is not defined\r\n```\r\n\r\nThis fix fixes the test failure.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@annarev @martinwicke The current master build is failing with\r\n```\r\n//tensorflow/python/kernel_tests:init_ops_test\r\n```\r\n\r\nThe issue seems to be with overlapping merges from #17740. This PR should fix the build test failure I think.", "Thanks for the fix @yongtang!"]}, {"number": 18663, "title": "Fix incorrect format in community/documentation.md", "body": "This PR is to fix the incorrect format rendering in [community/documentation.md](https://www.tensorflow.org/community/documentation).\r\n\r\n- Fix the c++ code block format in [Ops defined in C++](https://www.tensorflow.org/community/documentation#ops_defined_in_c) section;\r\n![image](https://user-images.githubusercontent.com/1680977/38943439-019ef664-4364-11e8-9fef-cdc87237e3ef.png)\r\n\r\n- Fix the bold format of argument parameter according to [tf.image.decode_png](https://www.tensorflow.org/api_docs/python/tf/image/decode_png) in markdown style with \"**\".\r\n![image](https://user-images.githubusercontent.com/1680977/38943622-77af91ec-4364-11e8-8b4c-3677e0fec880.png)\r\n\r\n", "comments": ["@martinwicke Could u pls kindly help run the gate tests?", "The failure case is the timeout of `//tensorflow/core:common_runtime_ring_reducer_test`, which seems to be unrelated."]}, {"number": 18662, "title": "[r1.8] Fix tf.compat.as_str returns bytes issue in Python 3", "body": "**NOTE: This PR is against r1.8.** Please feel free to close the PR if this is not appropriate.\r\n\r\nThis PR is to carry the fix of #18601 `Fix tf.compat.as_str returns bytes issue in Python 3` to r1.8. \r\n\r\n@martinwicke @annarev I feel it might make sense to apply this patch to 1.8 release if time permits.\r\n\r\nNot sure about the overall release work flow though, so please feel free to close the PR if not appropriate.", "comments": []}, {"number": 18661, "title": "Document cache shuffle repeat order", "body": "https://github.com/tensorflow/tensorflow/issues/15343#comment-351817810\r\n\r\n/cc @saeta @jsimsa", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It was just referencing to another issue.", "Unassigning myself as I no longer work on this part of the project. CC @jsimsa (who might know if this has already been documented somewhere.)"]}, {"number": 18660, "title": "Bug: tf.keras.estimator.model_to_estimator() API giving error when Keras model contains Lambda layer.", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n`estimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)`\r\n\r\n### Describe the problem\r\n `tf.keras.estimator.model_to_estimator()` API is failing when Keras model contains `Lambda ` layer.\r\nThe error I am getting is `SystemError: unknown opcode`\r\nThe problem seems to be only there when I am using custom functions inside the Keras mode.\r\n\r\n### Source code / logs\r\nI implemented a VAE in Keras and was trying to convert it into an TF estimator model. The model works and trains using Keras. The Keras model has functions for gaussian sampling and VAE training loss. The code and  error trace back is given below.\r\n\r\n```\r\n#Encoder network, mapping inputs to our latent distribution parameters:\r\nx = Input(batch_shape=(batch_size, original_dim),name='encoder_input')\r\nencoded = Dense(intermediate_dim, activation='relu',name='encoder_dense_1')(x)\r\nz_mean = Dense(latent_dim,name='z_mean')(encoded)\r\nz_log_var = Dense(latent_dim,name='z_log_var')(encoded)\r\n\r\n# Sampling from Gaussian\r\ndef sampling(args):\r\n    \r\n    z_mean, z_log_var = args\r\n\r\n    epsilon = tf.random_normal(shape=(batch_size, latent_dim),\r\n                               mean=0., stddev=epsilon_std) \r\n    \r\n    return z_mean + tf.exp(z_log_var/2) * epsilon\r\nz = Lambda(sampling,name='z')([z_mean, z_log_var]) \r\n\r\n#Compute VAE loss\r\ndef vae_loss(x, x_decoded):\r\n    x_mse_loss = original_dim*tf.keras.losses.mean_squared_error(tf.layers.flatten(x), tf.layers.flatten(x_decoded))\r\n    beta = 4.0\r\n    kl_loss = - 0.5*beta* tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\r\n    return tf.reduce_mean(x_mse_loss + kl_loss)\r\n\r\n# Map these sampled latent points back to reconstructed inputs:\r\n#Decoder network layers\r\ndecoder_dense_1 = Dense(intermediate_dim, activation='relu',name='decoder_dense_1')\r\ndecoder_output = Dense(48, activation='relu',name='decoder_output')\r\ndecoded = decoder_dense_1(z)\r\nx_decoded = decoder_output(decoded)\r\n\r\n# end-to-end autoencoder\r\nvae = Model(x, x_decoded)\r\nvae.compile(optimizer='adam',loss=vae_loss) \r\nvae.summary()\r\n#Converting to tf estimator\r\nestimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)\r\n```\r\nOutput:\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\nencoder_input (InputLayer)      (50, 48)             0                                            \r\n__________________________________________________________________________________________________\r\nencoder_dense_1 (Dense)         (50, 24)             1176        encoder_input[0][0]              \r\n__________________________________________________________________________________________________\r\nz_mean (Dense)                  (50, 10)             250         encoder_dense_1[0][0]            \r\n__________________________________________________________________________________________________\r\nz_log_var (Dense)               (50, 10)             250         encoder_dense_1[0][0]            \r\n__________________________________________________________________________________________________\r\nz (Lambda)                      (50, 10)             0           z_mean[0][0]                     \r\n                                                                 z_log_var[0][0]                  \r\n__________________________________________________________________________________________________\r\ndecoder_dense_1 (Dense)         (50, 24)             264         z[0][0]                          \r\n__________________________________________________________________________________________________\r\ndecoder_output (Dense)          (50, 48)             1200        decoder_dense_1[0][0]            \r\n==================================================================================================\r\nTotal params: 3,140\r\nTrainable params: 3,140\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\nINFO:tensorflow:Using the Keras model provided.\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\SPLATH~1\\AppData\\Local\\Temp\\tmps7nagdhz\r\nINFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\SPLATH~1\\\\AppData\\\\Local\\\\Temp\\\\tmps7nagdhz', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000238136D9DA0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\n---------------------------------------------------------------------------\r\nSystemError                               Traceback (most recent call last)\r\n<ipython-input-10-77cc01c33881> in <module>()\r\n     13 vae.summary()\r\n     14 #Converting to tf estimator\r\n---> 15 estimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\estimator.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)\r\n    481                            estimator,\r\n    482                            custom_objects,\r\n--> 483                            keras_weights)\r\n    484   elif keras_model.built:\r\n    485     logging.warning('You are creating an Estimator from a Keras model '\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\estimator.py in _save_first_checkpoint(keras_model, estimator, custom_objects, keras_weights)\r\n    396       training_util.create_global_step()\r\n    397       model = _clone_and_build_model(model_fn_lib.ModeKeys.TRAIN, keras_model,\r\n--> 398                                      custom_objects)\r\n    399       if isinstance(model, models.Sequential):\r\n    400         model = model.model\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\estimator.py in _clone_and_build_model(mode, keras_model, custom_objects, features, labels)\r\n    270         model = models.clone_model(keras_model, input_tensors=input_tensors)\r\n    271     else:\r\n--> 272       model = models.clone_model(keras_model, input_tensors=input_tensors)\r\n    273   else:\r\n    274     model = keras_model\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\models.py in clone_model(model, input_tensors)\r\n    261     return _clone_sequential_model(model, input_tensors=input_tensors)\r\n    262   else:\r\n--> 263     return _clone_functional_model(model, input_tensors=input_tensors)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\models.py in _clone_functional_model(model, input_tensors)\r\n    166               kwargs['mask'] = computed_masks\r\n    167           output_tensors = generic_utils.to_list(layer(computed_tensors,\r\n--> 168                                                        **kwargs))\r\n    169           output_masks = generic_utils.to_list(\r\n    170               layer.compute_mask(computed_tensors, computed_masks))\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\base_layer.py in __call__(self, inputs, **kwargs)\r\n    237     \"\"\"\r\n    238     # Actually call the layer (optionally building it).\r\n--> 239     output = super(Layer, self).__call__(inputs, **kwargs)\r\n    240     if context.executing_eagerly():\r\n    241       return output\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    712 \r\n    713         if not in_deferred_mode:\r\n--> 714           outputs = self.call(inputs, *args, **kwargs)\r\n    715           if outputs is None:\r\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\core.py in call(self, inputs, mask)\r\n    640     if has_arg(self.function, 'mask'):\r\n    641       arguments['mask'] = mask\r\n--> 642     return self.function(inputs, **arguments)\r\n    643 \r\n    644   def compute_mask(self, inputs, mask=None):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\core.py in sampling(args)\r\n     16 def sampling(args):\r\n     17     #import tensorflow as tf\r\n---> 18     z_mean, z_log_var = args\r\n     19 \r\n     20     epsilon = tf.random_normal(shape=(batch_size, latent_dim),\r\n\r\nSystemError: unknown opcode\r\n```\r\n", "comments": ["@fchollet Can you take a look at this?", "I run the test on Mac (tf 1.8).  A similar exception is raised, which seems related with clone behavior of Lambda: the function copied jumps out the original code scope.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import Input, Model\r\nfrom tensorflow.python.keras.layers import Dense, Lambda\r\n\r\nx = Input(shape=(32,))\r\ny = Dense(10)(x)\r\n\r\ndef func(x):\r\n    return tf.exp(x)\r\n\r\ny = Lambda(func)(y)\r\n\r\nvae = Model(x, y)\r\nvae.compile(optimizer='adam', loss='categorical_crossentropy')\r\nvae.summary()\r\n\r\nestimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)\r\n```\r\n\r\n```python\r\n~/Downloads \u276f\u276f\u276f python test.py\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         (None, 32)                0\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                330\r\n_________________________________________________________________\r\nlambda_1 (Lambda)            (None, 10)                0\r\n=================================================================\r\nTotal params: 330\r\nTrainable params: 330\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/6n/7463w47j649757l3cyct9qp00000gn/T/tmptdyvm5v3\r\n2018-05-06 06:56:08.030004: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 19, in <module>\r\n    estimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 478, in model_to_estimator\r\n    keras_weights)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 395, in _save_first_checkpoint\r\n    custom_objects)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 272, in _clone_and_build_model\r\n    model = models.clone_model(keras_model, input_tensors=input_tensors)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/models.py\", line 263, in clone_model\r\n    return _clone_functional_model(model, input_tensors=input_tensors)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/models.py\", line 156, in _clone_functional_model\r\n    **kwargs))\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py\", line 314, in __call__\r\n    output = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 716, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/facai/Library/anaconda3/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/layers/core.py\", line 642, in call\r\n    return self.function(inputs, **arguments)\r\n  File \"test.py\", line 11, in func\r\n    return tf.exp(x)\r\nNameError: name 'tf' is not defined\r\n```", "I suggest the following:\r\n\r\n- Put the content of your `Lambda` inside a custom layer (say `MyLayer`)\r\n- Place the `MyLayer` definition in your file\r\n- Call `model_to_estimator` within the following scope:\r\n\r\n```python\r\nwith tf.keras.utils.CustomObjectScope({'MyLayer': MyLayer}):\r\n   estimator = model_to_estimator(model)\r\n```", "@sibyjackgrove Has @fchollet's advice resolved your issue?", "@angersson \r\nI tried making the custom layer like below:\r\n```\r\nclass MyLayer(Layer):\r\n    \r\n    def compute_output_shape(self, input_shape):\r\n        shape = list(input_shape)\r\n        assert len(shape) == 2  # only valid for 2D tensors\r\n        shape[-1] *= 2\r\n        return tuple(shape)\r\n\r\n    def call(self, x):\r\n        z_mean, z_log_var = x\r\n\r\n        epsilon = tf.random_normal(shape=(batch_size, latent_dim),mean=0., stddev=epsilon_std) \r\n    \r\n        return z_mean + tf.exp(z_log_var/2) * epsilon\r\n```\r\nThen used it within the model like below: \r\n`z = MyLayer()([z_mean, z_log_var]) `\r\nHowever it is throwing off an error.\r\nCould you tell me if I made the custom layer correctly? I was following example of antirectifier custom layer in the Keras repository.", "@fchollet Hasn't the `model_to_estimator` the `custom_objects` parameter already? \r\nGenerally It is better to officially document if lambda layer are not supported cause I think that it will be one of the first things that user will try after using standard layers.", "I am still having the same problem with TF 1.9-rc1 for my code. However, in case of @facaiy 's code, it works if you put an import statement in the lambda function declaration.\r\n\r\n```\r\ndef func(x):\r\n    import tensorflow as tf\r\n    return tf.exp(x)\r\n```\r\nCan anyone tell me why I am getting  \r\n\r\n> SystemError: unknown opcode\r\n\r\nfor my lamda function below while the above code doesn't get it.\r\n\r\n```\r\ndef sampling(args):\r\n    \r\n    z_mean, z_log_var = args\r\n\r\n    epsilon = tf.random_normal(shape=(batch_size, latent_dim),\r\n                               mean=0., stddev=epsilon_std) \r\n    \r\n    return z_mean + tf.exp(z_log_var/2) * epsilon\r\n```", "My workaround is to pass the `tf` module into the Lambda function. Below is @facaiy's modified test code (tf 1.8).\r\n\r\n    import tensorflow as tf\r\n    from tensorflow.python.keras import Input, Model\r\n    from tensorflow.python.keras.layers import Dense, Lambda\r\n\r\n    x = Input(shape=(32,))\r\n    y = Dense(10)(x)\r\n\r\n    def func(x, tf=tf):\r\n        return tf.exp(x)\r\n\r\n    y = Lambda(func)(y)\r\n\r\n    vae = Model(x, y)\r\n    vae.compile(optimizer='adam', loss='categorical_crossentropy')\r\n    vae.summary()\r\n\r\n    estimator_vae = tf.keras.estimator.model_to_estimator(keras_model=vae)\r\n", "Thanks, @azmikamis  I will try this out.", "@azmikamis I tried what you mentioned it still throws the same error, namely:\r\n`z_mean, z_log_var = args\r\nSystemError: unknown opcode`\r\n", "@sibyjackgrove Have you solved this problem? I am currently get into this problem too.", "Closing this issue now since the following code shared earlier in this thread now works in TF 2.4. [Gist here](https://colab.research.google.com/gist/nikitamaia/71dc9908be12c88b58fa4dfc4394897f/keras_m2e_lambda.ipynb)"]}, {"number": 18659, "title": "Add uint16 support for py_func", "body": "When using`py_func` in tf I noticed that uint16 support is not available (all other numeric types have already been supported):\r\n```\r\n$ python\r\n>>> import tensorflow as tf\r\n>>> def sum_func(x, y):\r\n...   return x + y\r\n...\r\n>>> x = tf.constant(1, dtype=tf.uint16)\r\n>>> y = tf.constant(2, dtype=tf.uint16)\r\n>>> z = tf.py_func(sum_func, [x, y], tf.uint16)\r\n>>> tf.Session().run(z)\r\n...\r\n...\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Unsupported numpy type 4\r\n\t [[Node: PyFunc = PyFunc[Tin=[DT_UINT16, DT_UINT16], Tout=[DT_UINT16], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Const, Const_1)]]\r\n...\r\n```\r\n\r\nThe reason is that there is no conversion between numpy uint16 and tf.uint16.\r\n\r\nThis fix adds the support so that py_func could process tf.uint16 data types.\r\n\r\nThis fix also adds test cases for different data types with py_func to increase the test coverage.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18658, "title": "TensorflowLite - Android", "body": "I am using TensorflowLite for Android. I tried setting up the java demo project as described in Tensorflow site (Android demo app) and succeeded.  \r\nI am getting an error while running the app in Android studio.  ERROR (Error:(152, 15) error: cannot find symbol method setNumThreads(int))\r\n\r\nIn file  **ImageClassifier.java**\r\n**public void setNumThreads(int num_threads) {\r\n    if (tflite != null)\r\n        tflite.setNumThreads(num_threads);\r\n\r\n  }**\r\n\r\nI understand that the method setNumThreads is not found in the Interpreter.  needed help to resolve this issue..\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Created android project in Android Studio 3.0\r\nNo custom code written\r\nCode: tensorflow/contrib/lite/java/demo - TensorflowLite demo \r\nOS Platform and Distribution- Windows 10,\r\nI have directly used the demo code from tensorflow/contrib/lite/java/demo. \r\nTensorFlow version: Tensorflow Version - 1.7\r\n", "I have the same issue.\r\nError:(152, 16) The method setNumThreads(int) is undefined for the type Interpreter\r\nError:Execution failed for task ':app:transformJackWithJackForDebug'.\r\n> com.android.build.api.transform.TransformException: com.android.builder.core.JackToolchain$ToolchainException: Jack compilation exception", "I am facing the same issue.\r\nIs it due to the depreciation of jackOptions?", "@tensorflowbutler the same issue,\r\ntf:1.7.0\r\nandroid studio:3.0\r\nos:macos 10.12.6\r\nno egpu and other\r\n used the github demo code from tensorflow/contrib/lite/java/demo.\r\ngpu/cudnn no\r\n\r\nI add the tflite into assets,then ./gradlew assembleDebug,terminal tip:\r\njust do this:\r\nDownload the quantized Mobilenet TensorFlow Lite model and unzip and copy mobilenet_quant_v1_224.tflite to the assets directory: tensorflow/contrib/lite/java/demo/app/src/main/assets/.\r\n\r\n> /Users/candrwow/Documents/GitHub/MachineLearning/tensorflow/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java:152:16: The method setNumThreads(int) is undefined for the type Interpreter\r\n/Users/candrwow/Documents/GitHub/MachineLearning/tensorflow/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java:177:21: Resource leak: 'inputStream' is never closed\r\n/Users/candrwow/Documents/GitHub/MachineLearning/tensorflow/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java:61:8: The import android.widget.Toast is never used\r\n\r\nI try ./gradlew clean and invalidate cache and restart,is also these error tip\r\n\r\n", "@candrwow i think you need to use https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/build_aar_for_release.sh  to build a newest version.\r\nBut it turns out to be slower when using float version, can someone  help me?", "@SANTHAKUMAR91 @Vivienmm @abhirajD @candrwow \r\nI solved this problem with a simple and crude method. I commented out \r\n\r\n**public void setNumThreads(int num_threads) {\r\nif (tflite != null)\r\ntflite.setNumThreads(num_threads);\r\n}**\r\n\r\nin ImageClassifier.java directly. And I also commented out\r\n\r\n**classifier.setNumThreads(newVal);**\r\n\r\nin Camera2BasicFragment.java.\r\n\r\nThen it works. I can build and run the demo.\r\nHowever, I encounter a new problem. When I open this demo on my mobile phone, it shows \"uninitialized classifier or invalid context\".\r\nI am really puzzled!  And does anyone know the function of the codes I commented out?", "I am facing the same issue", "@walkerwjt but i do it as your follows, it doesn't show what you said.", "@heng-yin , I tried it again with another mobile phone and I could build and run it again. But it still shows \"uninitialized classifier or invalid context\". It seems that the codes I commented out play some important role. I have not found any solution yet!\r\n", "Re compilation: make sure all you code and library are sync'ed to the same version.\r\n\r\nRe 'uninitialized classifier\": My guess is that the app is not finding the model. Could you make sure the paths are all correct?", "@walkerwjt after commented it , it does show \"uninitialized classifier or invalid context\",but after while,it worked.", "having the same problem ! no solution yet ?", "I too am having this problem", "I remove this method `setNumThreads ` and its refereced  \r\napp works fine , but I don't know the side effect of that.\r\n\r\nI think the reason for this problem is because the TensorFlow Lite has Updated ( include remove the method),  the sample not.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Built from the TF lite source, the demo works.", "@fast4break how SO?didn't you face the same question?", "having the same problem !", "Having the same problem from the TF source. Gradle version 4.4, Android Plugin Version 3.1.2, TF 1.8\r\n\r\nThe fix provided by @walkerwjt results in a unresponsive app that updates the viewfinder once every ~10 seconds on my Pixel 2XL running 8.1", "Same issue , \r\nThe method setNumThreads(int) is undefined for the type Interpreter\r\n> Message{kind=ERROR, text=The method setNumThreads(int) is undefined for the type Interpreter, sources=[H:\\Project\\demo\\app\\src\\main\\java\\com\\example\\android\\tflitecamerademo\\ImageClassifier.java:160:16]}\r\n \r\n![sqsq](https://user-images.githubusercontent.com/11449967/41087624-fa98a6ea-6a5e-11e8-9ef0-ed11ffff0bed.PNG)\r\n\r\n", "https://github.com/SANTHAKUMAR91 Yup i too have the same problem\r\n", "This is likely an incompatibility between the github code and the 0.1.7 AAR file. You can either comment out the setNumThreads call, or you can configure AndroidStudio to use TF Lite's 0.0.0-nightly AAR", "https://github.com/walkerwjt This answer worked for me!! Thanks!! But I don't know what the side effect it would create in future!! Have a happy tflite!!\r\nhttps://github.com/andrehentz thanks for your response!! :-)", "@Regilan  SO FAR so good for me ", "I found that I didn't have this problem if I rejected the prompt to upgrade Gradle to enable instant run when I first loaded the project.", "The gradle file has been updated to use the 0.0.0-nightly, as per-suggested by andrehentz, in this [commit](https://github.com/tensorflow/tensorflow/commit/b84506ec8961306100ee67bd06ed8d2b59f4b1c8). It's worked fine for me locally, whereas the 0.1.7 AAR ran into the issue you all ran into. Note that as the commit suggests, this solution is better, but not perfect (may need to pull new demo code sometimes).\r\n\r\nClosing this issue for now. Let us know if you continue to run into issues!", "@andreas-eberle  emm ,but 0.0.0 seems like still didn't work", "@andrewginns  how can i degrade gradle?thx\r\ngradle and sdk compatible really trouble me a lot ", "@zfk513: I think you wrongly mentioned me in your comment. ", "My Solution: compile 'org.tensorflow:tensorflow-lite:0.0.0-nightly'", "implementation this, this worked me \r\n**org.tensorflow:tensorflow-lite:2.3.0**"]}, {"number": 18657, "title": "Replace raw_input/input with six.moves.input", "body": "This fix is an enhancement to replace raw_input/input in python 2 and 3 with six.moves.input.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 18656, "title": "Tensorflow Yolo GPU Problem ", "body": "Hello Everyone, \r\n\r\nI am trying to use yolo but I am completely new at programming. I guess the error is about my GPU memory. \r\n\r\nI hope you can help. Thank you! \r\n\r\nCaner\r\n\r\n(base) C:\\Users\\derin\\Desktop\\Dark\\darkflow-master>python flow --model cfg/yolo.cfg --load bin/yolov2.weights --demo videofile.mp4 --gpu 1.0 --saveVideo\r\nC:\\Users\\derin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nWARNING:tensorflow:From C:\\Users\\derin\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\n\r\nC:\\Users\\derin\\Desktop\\Dark\\darkflow-master\\darkflow\\dark\\darknet.py:54: UserWarning: ./cfg/yolov2.cfg not found, use cfg/yolo.cfg instead\r\n  cfg_path, FLAGS.model))\r\nParsing cfg/yolo.cfg\r\nLoading bin/yolov2.weights ...\r\nSuccessfully identified 203934260 bytes\r\nFinished in 0.01562643051147461s\r\nModel has a coco model name, loading coco labels.\r\n\r\nBuilding net ...\r\nSource | Train? | Layer description                | Output size\r\n-------+--------+----------------------------------+---------------\r\n       |        | input                            | (?, 608, 608, 3)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)\r\n Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)\r\n Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\r\n Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\r\n Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\r\n Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\r\n Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\r\n Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\r\n Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\r\n Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\r\n Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\r\n Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\r\n Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)\r\n Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)\r\n Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)\r\n Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)\r\n Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\r\n Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)\r\n-------+--------+----------------------------------+---------------\r\nGPU mode with 1.0 usage\r\n2018-04-18 17:50:15.493755: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-04-18 17:50:15.737244: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties:\r\nname: Quadro P4000 major: 6 minor: 1 memoryClockRate(GHz): 1.2275\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.63GiB\r\n2018-04-18 17:50:15.742500: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-18 17:50:16.281176: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-18 17:50:16.283690: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0\r\n2018-04-18 17:50:16.285372: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N\r\n2018-04-18 17:50:16.287289: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8192 MB memory) -> physical GPU (device: 0, name: Quadro P4000, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-04-18 17:50:16.292054: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:936] failed to allocate 8.00G (8589934592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2018-04-18 17:50:16.294366: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:936] failed to allocate 7.20G (7730940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nFinished in 12.376373767852783s\r\n\r\nPress [ESC] to quit demo\r\n2018-04-18 17:50:22.102653: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:403] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2018-04-18 17:50:22.105704: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:407] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2018-04-18 17:50:22.108692: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:370] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2018-04-18 17:50:22.110948: F T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)", "comments": ["I can't replicate the problem without more detail, but you have 2 potential problems here:\r\n```\r\n\\cuda\\cuda_driver.cc:936] failed to allocate 7.20G (7730940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n```\r\nYour attempting to load too much into GPU memory (i.e. either the `yolov2.weights` or the `videofile.mp4`.  You have freeMemory: 6.63GiB, but are trying to load 7.2G.  \r\n\r\n```\r\ncould not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n```\r\nYou may want to check that you have a version of cuDNN installed that is compatible with your CUDA version.", "Hi, \r\n\r\nHave you tried to use less memory during training, constraining your GPU to work with less memory ?\r\nJust pass:\r\n\r\n```\r\n#Memory constrain\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\ncfg_ = tf.ConfigProto(gpu_options=gpu_options)\r\n\r\n#... later, pass this into your session, like this\r\ntf.Session(config=cfg_)\r\n```\r\n\r\nLet me know if it worked.\r\n", "Nagging Assignee @tatatodd: It has been 151 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ccelik13 Hi, have you tried the workaround suggested by @cyberwillis  ?  \r\nDoes this issue still exist ?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I'm getting this on a rtx 2080 super, cuda 10.1, driver 435, cudnn 7.6.5  trying to run https://github.com/theAIGuysCode/Object-Detection-API.\r\n Getting this error on the  'python load_weights.py' command\r\n\r\n'\r\nfailed to allocate 3.97G (4265122048 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n'\r\nThe GPU has 8gb? Any ideas\r\n"]}, {"number": 18655, "title": "Remove over-indentation", "body": "", "comments": []}, {"number": 18654, "title": " Cannot create interpreter: Model provided is schema version 0 not equal to supported version 3.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18653, "title": "Bug: expected_shape not work in tensorflow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac osx\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**:  3.5.0\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI try to use a variable without shape in tensorflow. My code is\r\n\r\n```\r\nsen_var_1 = tf.Variable(np.float32, trainable=False, validate_shape=False, expected_shape=[None, None, 300])\r\nsen_1 = tf.placeholder(shape=[None, None, 300], dtype=np.float32, name=\"q1\")\r\nsen_assign_1 = tf.assign(sen_var_1, sen_1, validate_shape=False)\r\n```\r\n\r\nI will run session with `sen_assign_1` when train begin, and each epoch I want to use `sen_var_1`. But it seems that `expected_shape` is not work in `sen_var_1`. So is there any way to do this?\r\n\r\n### Source code / logs\r\n`ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it does not appear to be a bug at cursory glance. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18652, "title": "Bug: tensorflow-gpu takes long time before beginning to compute", "body": "I noticed that tensorflow always takes about ~2min before it actually starts to compute. I've been trying to find out, why this happens, and nothing really worked so far. \r\n\r\n[Tensorflow site](https://www.tensorflow.org/install/install_windows) says, I should use CUDA\u00ae Toolkit 9.0 and cuDNN v7.0. I have CUDA 9.0, so I downloaded CuDNN 7.0.5 for CUDA 9.0 and pasted the files to *C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\*, overwriting the ones form cuDNN 7.1.2, which I tested earlier. To make sure, I pip-installed tensorflow-gpu into a fresh anaconda env. See install [here](https://pastebin.com/rjiV1s3b). The issue is still the same.\r\n\r\nCUDA works, since it prints the  *'Hello, TensorFlow!'*, when I use the official test example, but before that it takes like 2minutes every time! \r\n\r\nWhen I tested this with [another wheel](https://drive.google.com/drive/folders/1lVK_ABvVHzVYKs7X5SUhcZFBgKpC41Qw) ([which is linked in this tutorial](http://www.python36.com/install-tensorflow-gpu-windows/), I did not compile it myself.) on cuda 9.1/cudnn7.0.5, I had the same issues. A NVIDIA employee [on stackoverflow](https://stackoverflow.com/questions/49770217/why-does-cuda-initialisation-take-so-long-python-vscode-anaconda-tensorflow) suggested, I may be hitting a lengthy JIT compile step, because the GTX 1080 has compute capability of 6.1, which the wheel I used may not be compiled for. \r\n\r\nSo I tried to find wheels for tensorflow with compute capability 6.1 for windows, but [the only one I found](https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.5.0/py36/GPU/cuda91cudnn7avx2) and tested produced the same problem.\r\n\r\nAm I doing something wrong here, or do I just have to accept the 2min delay everytime I start my tensorflow/keras scripts?\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCode:\r\n```\r\nimport time\r\nstart_time = time.time()\r\nimport tensorflow as tf\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nprint(sess.run(c))\r\ntimer = time.time()\r\nprint(timer - start_time)\r\n```\r\nOutput:\r\n```\r\n(tf_clean) C:\\python_code\\test>C:/anaconda/envs/tf_clean/python.exe c:/python_code/test/tf_test.py\r\n2018-04-18 14:36:04.376661: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this\r\nTensorFlow binary was not compiled to use: AVX2\r\n2018-04-18 14:36:04.689661: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.60GiB\r\n2018-04-18 14:36:04.699485: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-18 14:38:12.227561: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-18 14:38:12.234504: I T:\\src\\github\\tens2018-04-18 14:38:12.237156: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N\r\n2018-04-18 14:38:12.240997: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6379 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n2018-04-18 14:38:12.548288: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:297] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 14:38:12.559262: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:884] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\nb: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 14:38:12.564847: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:884] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\na: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-04-18 14:38:12.570545: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:884] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n[[22. 28.]\r\n [49. 64.]]\r\n129.14624643325806\r\n```\r\n\r\n- **OS Platform and Distribution**:\r\nWindows 10 Education (Version 10.0.16299 Build 16299)\r\nIntel(R) Core(TM) i5-7500 CPU @ 3.40GHz, 3408 MHz, 4 Cores\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n\r\n- **TensorFlow version**:\r\ntensorflow-gpu 1.5.0, 1.7.0\r\n\r\n- **Python version**: \r\n3.5.5 & 3.6 (via anaconda, conda 4.5.1.)\r\n\r\n- **Bazel Version**:\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\nTested combinations: \r\n   CUDA 9.0 and CuDNN 7.1.2 (tested on tensorflow 1.5.0, 1.7.0 and 1.8.0-dev20180329)\r\n   CUDA 9.1 and CuDNN 7.0.5 (tested on tensorflow 1.5.0 and 1.7.0)\r\n\r\n- **GPU model and memory**:\r\nNVIDIA GeForce GTX 1080 (GP104-400) [Hewlett-Packard], 8192 MBytes of GDDR5X SDRAM [Micron]\r\n\r\n- **Exact command to reproduce**:\r\nSee: *Have I written custom code...*\r\n\r\n=================================================================\r\nEDIT:\r\n\r\nThreadstarter here, hello.\r\n> \r\n> \r\n> Could you try with the latest nightly?\r\n> https://files.pythonhosted.org/packages/67/c0/e68a4f0400340b54c887703baa8eee188042c3d65a0cf535dda71abffbc2/tf_nightly_gpu-1.13.0.dev20190205-cp37-cp37m-win_amd64.whl\r\n\r\n**This works!** I checked with that wheel, and then with `tf-nightly-gpu-2.0-preview` on PYPI, which also worked.\r\nI initially wanted to use the anaconda cudatoolkit and cudnn packages, but currently, cudnn is only available up to version 7.3.1 on anaconda-cloud. Tensorflow 2.0 however, is compiled with 7.4.1, so I had to do this the oldschool way, and download the setups from Nvidia.\r\nSoon, though...[soon](https://imgur.com/a/A2jZizt).\r\n\r\nFor everyone, here's what I did, as a guide:\r\n\r\n### How to install Tensorflow Nightly 2.0 GPU in Anaconda on Windows 10 x64\r\n\r\n\u2022 I installed these CUDA/CuDnn Versions:\r\n   \u2013 cuda_10.0.130_win10_network (Nvidia CUDA Download: https://developer.nvidia.com/cuda-toolkit)\r\n   \u2013 cuDNN v7.4.1 (Nov 8, 2018), for CUDA 10.0 (Nvidia CuDnn Download: https://developer.nvidia.com/cudnn)\r\n   \u2013 Don't forget to check, whether the Cuda setup has correctly written itself to the PATH system variable.\r\n   \u2013 Reboot.\r\n\u2022 Now make a new environment in Anaconda and activate it:\r\n   \u2013 `conda create --name tf2-nightly-gpu python=3.6`\r\n   \u2013 `activate tf2-nightly-gpu`\r\n\u2022 Now, with the new env still activated, install the latest Tensorflow 2.0 nightly GPU build from PYPI:\r\n   \u2013 `pip install tf-nightly-gpu-2.0-preview`\r\n\u2022 For machine learning in Jupyter notebook (or Jupyter Lab) , you need these as well:\r\n   \u2013 `conda install nb_conda matplotlib scipy Pillow pandas scikit-learn`\r\n\u2022 Check, if your GPU is recognized by Tensorflow. Open the Anaconda prompt, activate the new environment and type `python`, then press Enter. Now type:\r\n`import tensorflow as tf`\r\n`tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None) `\r\n\u2022 Output should be something like this:\r\n\r\n```\r\n(tf2-nightly-gpu) C:\\Users\\___>python\r\n>>> import tensorflow as tf\r\n>>> tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)\r\n2019-03-19 17:46:25.722209: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-03-19 17:46:25.729724: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-03-19 17:46:25.922934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1551] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.61GiB\r\n2019-03-19 17:46:25.938231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0\r\n2019-03-19 17:46:26.539185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1082] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-19 17:46:26.546009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1088]      0\r\n2019-03-19 17:46:26.550123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1101] 0:   N\r\n2019-03-19 17:46:26.554188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1222] Created TensorFlow device (/device:GPU:0 with 6360 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTrue\r\n```\r\n\r\n\u2022 Done.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "> Could you update them if they are relevant in your case, or leave them as N/A? \r\n\r\ndone", "@reedwm do you have any idea what could cause this?", "@gunan this sounds like an issue of JIT caching. What compute capabilities do the prebuilt wheels use? Are there plans to compile with CC 6.1?", "I think prebuilt wheels have every compute capability starting from 3.5 or 3.7.", "I have the same issue: a timeout of exactly 2 minutes before computation starts.\r\nIs it perhaps related to \"Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\" ?\r\nI'm using  \r\nhost: ubuntu 18.04\r\ncontainer: tensorflow/tensorflow:latest-gpu\r\n\r\n~~~\r\nroot@76611d5f5dd1:/notebooks# python /usr/local/bin/validate_installation.py \r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.flo\r\nat64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-06-18 10:15:12.462431: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-06-18 10:15:12.672108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-18 10:15:12.672988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.189\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 1.96GiB freeMemory: 1.93GiB\r\n2018-06-18 10:15:12.673024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-18 10:17:11.465729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-18 10:17:11.465769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-06-18 10:17:11.465778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-06-18 10:17:11.466050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1695 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:02:00.0, compute capability: 5.0)\r\nHello, TensorFlow!\r\n~~~\r\n\r\nthe script I used to install nvidia-docker after a fresh installation of ubuntu 18.04:\r\n~~~\r\n# Install packages to allow apt to use a repository over HTTPS:\r\nsudo apt-get install \\\r\n    apt-transport-https \\\r\n    ca-certificates \\\r\n    curl \\\r\n    software-properties-common\r\n\r\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\r\nsudo apt-key fingerprint 0EBFCD88\r\n\r\nsudo add-apt-repository \\\r\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\r\n   $(lsb_release -cs) \\\r\n   stable\"\r\n\r\nsudo apt-get update\r\n\r\n# docker-ce not yet ready -> docker.io\r\n#\r\napt -y install docker.io\r\n\r\n\r\necho blacklist nouveau >> /etc/modprobe.d/blacklist-nouveau.conf\r\necho options nouveau modeset=0 >> /etc/modprobe.d/blacklist-nouveau.conf\r\n\r\nsudo update-initramfs -u\r\n# reboot\r\n\r\n\r\nsudo apt-get install dkms build-essential make\r\n\r\nsudo dpkg --add-architecture i386\r\nsudo apt update\r\nsudo apt -y install libc6:i386\r\n\r\nsudo bash NVIDIA-Linux-x86_64-390.67.run --dkms --install-libglvnd\r\n\r\n# https://nvidia.github.io/nvidia-docker/\r\n\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \\\r\n  sudo apt-key add -\r\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\r\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\r\nsudo apt-get update\r\n\r\nsudo apt -y install nvidia-docker2\r\n\r\napt -y install nvidia-utils\r\n~~~", "[edit] \r\nI lied. It only took a long time on the first run. After that it starts up instantly, even after a reboot.\r\n\r\n@ludwigprager The AVX2 warning is irrelevant. I used to get the same thing on my cpu-only installation in the past.\r\n\r\nMy startup is about 2 seconds:\r\n\r\n```\r\nPython 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import time\r\n>>> start_time = time.time()\r\n>>> import tensorflow as tf\r\n>>> a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n>>> b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n>>> c = tf.matmul(a, b)\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2018-06-22 00:55:38.144614: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-06-22 00:55:38.545745: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.29GiB\r\n2018-06-22 00:55:38.550660: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-22 00:55:39.103765: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-22 00:55:39.107096: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:929]      0\r\n2018-06-22 00:55:39.109103: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:942] 0:   N\r\n2018-06-22 00:55:39.111204: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3025 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\r\n2018-06-22 00:55:39.266408: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:284] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\r\n\r\n>>> print(sess.run(c))\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-22 00:55:39.272426: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:886] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\nb: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-22 00:55:39.275348: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:886] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\na: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-06-22 00:55:39.278082: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:886] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n[[22. 28.]\r\n [49. 64.]]\r\n>>> timer = time.time()\r\n>>> print(timer - start_time)\r\n2.422179937362671\r\n```\r\n\r\n\r\n[old post]\r\n@reedwm I have the exact same issue on 1050ti. Takes forever to start session. Can we get an update on this, please?\r\n\r\n```\r\nPython 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant(\"hello\")\r\n>>> sess = tf.Session()\r\n2018-06-22 00:36:50.032759: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-06-22 00:36:50.531921: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.392\r\npciBusID: 0000:0a:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.29GiB\r\n2018-06-22 00:36:50.537715: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-22 00:38:43.427679: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-22 00:38:43.430876: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:929]      0\r\n2018-06-22 00:38:43.433374: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:942] 0:   N\r\n2018-06-22 00:38:43.435677: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3025 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)\r\n>>> print(sess.run(hello))\r\nb'hello'\r\n>>>\r\n```", "Has anything been discovered yet? I have the same problem with 'Adding visible gpu devices: 0' taking about 2-3 minutes, even after reboot and multiple runs. I'm using CUDA 9.0 and cuDNN 7.1.2\r\nSystem: Red Hat Linux\r\nGPU: GTX 750Ti\r\n\r\n", "A-ha, I think I may have an idea.\r\nIn our bazel builds, we have all the cuda compute capabilities built into the binaries we distribute.\r\nHowever, it is possible we are not doing that with cmake!\r\nI will take another look.", "Nice! \r\nHow do I get this on my machine now? Do I have to wait for the next release? I can't build.", "Yes, we will cherrypick this into 1.10 and you should be able to see the\nimprovement in 1.10\n\nOn Tue, Jul 24, 2018 at 11:49 AM philipp <notifications@github.com> wrote:\n\n> Nice!\n> How do I get this on my machine now? Do I have to wait for the next\n> release?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18652#issuecomment-407512287>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOV7iJ6vr_t710UuoWO5n33IYkFSTks5uJ2wrgaJpZM4TaBRC>\n> .\n>\n", "Hi all,\r\nI'm having the same problem... waiting time of about 2 minutes before running what I actually wants to run. The text below is what I get and what I see for two minutes:\r\n\r\nPython 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n`>>> import tensorflow as tf`\r\n`>>> tf.Session(config=tf.ConfigProto(log_device_placement=True))`\r\n`2018-09-05 09:54:50.130623: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA`\r\n`2018-09-05 09:54:50.374925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero`\r\n`2018-09-05 09:54:50.375571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce 940M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.96GiB freeMemory: 1.93GiB`\r\n`2018-09-05 09:54:50.375588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0`\r\n\r\nAfter the waiting time is finally over I get the rest of the execution:\r\n\r\n`2018-09-05 09:58:35.611421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:`\r\n`2018-09-05 09:58:35.611455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 `\r\n`2018-09-05 09:58:35.611462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N `\r\n`2018-09-05 09:58:35.611629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1687 MB memory) -> physical GPU (device: 0, name: GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0)`\r\n`Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0`\r\n`2018-09-05 09:58:35.623962: I tensorflow/core/common_runtime/direct_session.cc:288] Device mapping:`\r\n`/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0`\r\n\r\n`<tensorflow.python.client.session.Session object at 0x7f0556917668>`\r\n\r\nI'm using Ubuntu 18.04, Nvidia Driver 396.54, andrunning the script under an anaconda environment with Python 3.6.6, cuda 9.2 and tensorflow-gpu 1.10.0\r\n\r\nHow do I solve this?\r\nThanks,\r\nBoris", "@apolo74 Our official packages are not built to use cuda 9.2\r\nYou will need to reach out to the maintainers of the package you installed.\r\n\r\n", "@gunan I've been trying cuda 9.0 and cuda 9.1 with Tensorflow 1.8 and 1.9 but the results are the same... the first time I run `sess = tf.Session()` it stops around 3 minutes at `...Adding visible gpu devices: 0` and after that I can work normally. If I'm working within a Python shell and I close that session and WITHOUT exiting Python shell I open a new session `sess1 = tf.Session()` there is no problem, the GPU is added immediately.\r\nMy problem is that I don't work within a python shell but I run Python scripts and every time I run a new script I have to wait 3 minutes for it to add the GPU.", "We noticed that the builds started failing with https://github.com/tensorflow/tensorflow/issues/19198,\r\nTherefore, it turns out this issue is blocked right now until we can fix that problem.", "Still blocked by https://github.com/tensorflow/tensorflow/issues/19198, which is blocked by some internal issues we are having with eigen.\r\nWill update when we have a resolution.", "I experience exactly the same problem on GTX 1080Ti , Tensorflow 1.10.0 (both python and c++), CUDA 9.0, CUDNN 7.2.1 on Windows 10.", "I experience exactly the same problem on GTX 1080Ti , Tensorflow 1.11.0 , CUDA 9.0, CUDNN 7 on Windows 10.", "The same problem GeForce GTX 1050 Ti, Tensorflow 1.11.0, CUDA 9.0, CUDNN 7, Windows 10", "You will experience this on any graphics card that is GeForce GTX9xx or newer.\r\nAnd unfortunately, our status is we are still blocked upgrading eigen, due to some backwards incompatible changes that are happening in eigen.\r\n\r\nStill blocked by #19198", "Why does it happen on my windows install but not my linux? \r\n(Same hardware)\r\n\r\nEDIT: nevermind, because windows is built via cmake, and linux via bazel.\r\n\r\nIs it possible to set our specific compute capability while building from source? Would that solve the problem?", "The issue is because windows build uses MSVC, and other builds use clang or gcc.\r\nThe eigen bug surfaces with nvcc+msvc.\r\nYou can build from sources and set compute capability, but you will run into #19198.\r\n\r\n", "Have the same issue on GeForce GTX970", "Can confirm the results @gunan reports. Manually built to bypass 2min delay on start-up, ran into https://github.com/tensorflow/tensorflow/issues/19198\r\n\r\n@MacZel Just so you don't think your crazy, also seeing the issue on a few of the GTX970 series boards with Win 10", "Same issue here and same build as @rs0h .  1050ti, Win 10, CUDA 9.0, CUDNN 7\r\nMine seems to actually hang for a long time on \"Adding visible gpu devices: 0\"", "The JIT cache does seem to hang around for the entire Python process scope. \r\n\r\nFor those going in through Jupyter, on reset you hit the delay. But, subsequent part re-runs hit the existing JIT values. Just don't reset your notebook after init and it is pretty fast.", "Hi all, I'm having the same issue - did anybody find a workaround?\r\nIt takes a very long time on the first run but is very quick on subsequent runs. If I relaunch the container then it goes back to being very slow for one run. I get very similar output/behaviour if I run the same code using the Anaconda distribution of tensorflow.\r\n\r\nSystem information\r\n\r\n```\r\n    Code:\r\nimport time\r\nstart_time = time.time()\r\nimport tensorflow as tf\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nprint(sess.run(c))\r\ntimer = time.time()\r\nprint(timer - start_time)\r\n\r\nOutput:\r\n\r\n~$ docker run --runtime=nvidia -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-py3 bash\r\nroot@7ee18dae8673:/notebooks# python\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import time\r\n>>> start_time = time.time()\r\n>>> import tensorflow as tf\r\n>>> a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n>>> b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n>>> c = tf.matmul(a, b)\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2018-11-22 14:52:07.974293: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-11-22 14:52:08.052892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-22 14:52:08.053603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: Quadro M1000M major: 5 minor: 0 memoryClockRate(GHz): 1.0715\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.96GiB freeMemory: 1.70GiB\r\n2018-11-22 14:52:08.053621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-22 14:56:17.618342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-22 14:56:17.618385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2018-11-22 14:56:17.618395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2018-11-22 14:56:17.618726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1447 MB memory) -> physical GPU (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0\r\n2018-11-22 14:56:17.619285: I tensorflow/core/common_runtime/direct_session.cc:307] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0\r\n\r\n>>> print(sess.run(c))\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-11-22 14:56:17.620406: I tensorflow/core/common_runtime/placer.cc:927] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\na: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-11-22 14:56:17.620431: I tensorflow/core/common_runtime/placer.cc:927] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nb: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-11-22 14:56:17.620470: I tensorflow/core/common_runtime/placer.cc:927] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n[[22. 28.]\r\n [49. 64.]]\r\n>>> timer = time.time()\r\n>>> print(timer - start_time)\r\n250.8216381072998\r\n\r\n\r\n    OS Platform and Distribution:\r\ndocker on Ubuntu 16.04\r\n\r\n    TensorFlow installed from (source or binary):\r\n    binary / docker\r\n\r\n    TensorFlow version:\r\n    tensorflow-gpu 1.12\r\n\r\n    Python version:\r\n3.5\r\n\r\n    Bazel Version:\r\n    N/A\r\n\r\n    CUDA/cuDNN version:\r\n10.0\r\n\r\n    GPU model and memory:\r\n    NVIDIA Quadro M1000M with 2004MiB memory - 8GB system RAM\r\n\r\n    **Exact command to reproduce:**\r\n: docker run --runtime=nvidia -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-py3 bash\r\n: python\r\n: <script above>\r\n```", "(tf1) \u03bb python\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('hello, tensor!')\r\n>>> sess = tf.Session()\r\n2018-11-26 00:22:25.377848: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-11-26 00:22:25.875887: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.65GiB\r\n2018-11-26 00:22:25.906006: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n\r\n![fafafaf](https://user-images.githubusercontent.com/17380530/48986175-daa84900-f111-11e8-9e26-9d6bac4ca0dd.PNG)\r\nwhen cancelling the script when gets frozen , my gpu spikes a bit , dont know if its related...\r\n", "With the method described here it worked super fast ! \r\n[https://www.pugetsystems.com/labs/hpc/The-Best-Way-to-Install-TensorFlow-with-GPU-Support-on-Windows-10-Without-Installing-CUDA-1187/](url)\r\n\r\n> (tf-gpu) C:\\Users\\don> conda install -c aaronzs tensorflow-gpu\r\nNow, we can do the CUDA and cuDNN dependencies,\r\n\r\n(tf-gpu) C:\\Users\\don> conda install -c anaconda cudatoolkit\r\n(tf-gpu) C:\\Users\\don> conda install -c anaconda cudnn\r\n\r\n```\r\n`>>> sess = tf.Session()\r\n2018-11-26 00:30:08.575361: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-11-26 00:30:09.060813: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\r\nname: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.65GiB\r\n2018-11-26 00:30:09.091630: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-11-26 00:31:02.937801: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-26 00:31:02.959575: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\r\n2018-11-26 00:31:02.974227: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\r\n2018-11-26 00:31:02.993921: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1410 MB memory) -> physical GPU (device: 0, name: GeForce 840M, pci bus id: 0000:03:00.0, compute capability: 5.0)\r\n>>> print(sess.run(hello))\r\nb'hello ton '\r\n>>> print(sess.run(hello))\r\nb'hello ton '\r\n>>> quit()`\r\n``\r\n```\r\nit did it in a blink\r\n", "Hi all,\r\nI've been away for a time from this problem but I decided to give a new try... and I succeeded!!! :)\r\nIn short, it seems the best solution is to compile Tensorflow from sources.\r\nThe info of my system:\r\nUbuntu 18.04\r\nNvidia GeForce 940M\r\nDriver  version 396.54\r\nCuda 9.0.176\r\nCuDNN 7.2.1\r\nTensorflow 1.12.0\r\n\r\nFirst be sure that your path includes the right libraries:\r\n`LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH`\r\n\r\nInstall Java first:\r\n`sudo apt-get install openjdk-8-jdk`\r\n\r\nAnd assuming you already have python3-dev, pip3, numpy and wheel installed then install also the following packages:\r\n`sudo pip3 install six mock h5py enum34`\r\n\r\nThe main idea is download and compile Bazel... I went for Bazel 0.15.2\r\nhttps://github.com/bazelbuild/bazel/releases/download/0.15.2/bazel-0.15.2-dist.zip\r\n`unzip ~/path_to_downloaded/bazel-0.15.2-dist.zip -d bazel-0.15.2-dist\r\n`cd bazel-0.15.2-dist\r\n`./compile.sh`\r\n`sudo cp output/bazel /usr/local/bin`\r\n`bazel help`\r\n\r\nNext is to download and compile Tensorflow:\r\nhttps://github.com/tensorflow/tensorflow/archive/v1.12.0.tar.gz\r\n`tar xzvf ~/path_to_downloaded/tensorflow-1.12.0.tar.gz`\r\n`cd tensorflow-1.12.0`\r\n`./configure`\r\n\r\nBe careful about the right location of your CUDA, CuDNN and other libraries depending on your needs. After the configuration you'll start the building process:\r\n`build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nIf everything goes well you are now ready to create the PIP wheel package:\r\n`bazel-bin/tensorflow/tools/pip_package/build_pip_package wheel/tensorflow_pkg`\r\n\r\nAnd finally install the package:\r\n`sudo pip3 install ~/path_to_downloaded/tensorflow-1.12.0/wheel/tensorflow_pkg/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl`\r\n\r\nEnjoy!", "I have the exact same problem. It take around 5 minutes to add gpu. Is there a solution avaible on windows side?\r\nI have, win 8.1, CUDA 9.0, cuDNN 7.3.1, python 3.6.0 and with gtx 1060 6gb.", "It looks like compiling Tensorflow from the source on WIndows 10 with latest CUDA drivers solves the problem. In my case, I followed the step-by-step instructions described in this article: [https://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2](url). It seems to be working properly now.\r\nHope this helps someone!", "I have the exact same problem. It take around 5 minutes at:\r\nAdding visible gpu devices: 0\r\n\r\nMy environment is Win10, CUDA 9.0, cuDNN 7.4.15, python 3.6.0 and with MX150.\r\n\r\n", "i added below codes(refer to https://docs.google.com/presentation/d/1iO_bBL_5REuDQ7RJ2F35vH2BxAiGMocLC6t_N-6eXaE/edit#slide=id.g1df700e686_0_13), this phenomenon seems disappeared, i do not know the reason\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'", "We debug this, and we find 'err = cudaFree(nullptr)' takes long time in CreateDevices() method.\r\nhttps://devtalk.nvidia.com/default/topic/1032126/cuda-programming-and-performance/cudafree-takes-approx-99-5-of-total-time-/", "I have a similar problem with this hardware/software:\r\n\r\nGTXGeForce 1070\r\nCUDA 9.0\r\ncuDNN 7.0\r\nTensorFlow 1.12\r\n\r\nWhen I try to run the code from the python prompt it takes forever to start (like 5 minutes)\r\n\r\n![image](https://user-images.githubusercontent.com/45622297/49882971-0295ed00-fe32-11e8-9c91-5af6a60a8269.png)\r\n\r\nThe successive runs work just fine.\r\n\r\nWhile if I run the python.exe  with the script it's fine (like 5 seconds to run the session)\r\n\r\n![image](https://user-images.githubusercontent.com/45622297/49883349-d038bf80-fe32-11e8-84bf-e1f7413ab5b9.png)\r\n\r\n\r\n\r\n\r\n\r\n", "RTX 2070 \r\nCUDA 9.0\r\ncuDNN 7.0\r\ntensorflow-gpu 1.50\r\n\u9047\u5230\u4e86\u76f8\u540c\u7684\u95ee\u9898\uff0c\u5728CMD\u4f1a\u8bdd\u6846\u4e2d\u8fd0\u884c\u9700\u89811\u5206\u949f", "similar issue, adding device takes a few minutes.\r\nGPU  840M, python 3.6, CUDA 9.0, CUDNN 7.4.2, tensorflow 1.12.0", "which makes the following sentence, os.environ['TF_CPP_MIN_LOG_LEVEL']='2' I understand that it is like a jump.", "Any updates? Compiling tensorflow from source yield so many errors on my system, so I rely on the latest gpu container and updating the log level only has the effect of not displaying anything, the waiting time is the same, hangs after \"Adding visible gpu devices: 0\"\r\nAny other docker container that I can use that would solve the problem?", "@gunan Kindly asking for an update since #19198 is resolved. Is it official workaround to recompile tf? \r\nWe are having the problem with Tesla K80", "The same problem GeForce RTX 2080, Tensorflow 1.13.0 RC, CUDA 10.0, CUDNN 7, Windows 8.1.", "Let me take another look at this, if all the issues are resolved, hopefully with a single change we can include all necessary compute capabilities.", "I am having the same issue with long start time - though I don't know if this has hung -- or if it has started and is just taking forever.  I guess I will give it some time.  There is no load spike on the CPU or GPU.  Nothing seems to be happening - but the process hangs. \r\n\r\nThe same problem GeForce GTX1080TI, Tensorflow 1.12.0 RC, CUDA 10.0, CUDNN 7, Windows 10\r\n\r\n```\r\n2019-02-05 16:26:22.781196: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-02-05 16:26:22.803267: I tensorflow/stream_executor/platform/default/dso_loader.cc:161] successfully opened CUDA library nvcuda.dll locally\r\n2019-02-05 16:26:23.086000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.00GiB freeMemory: 9.11GiB\r\n2019-02-05 16:26:23.090067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n```", "Could you try with the latest nightly?\r\nhttps://files.pythonhosted.org/packages/67/c0/e68a4f0400340b54c887703baa8eee188042c3d65a0cf535dda71abffbc2/tf_nightly_gpu-1.13.0.dev20190205-cp37-cp37m-win_amd64.whl", "> Could you try with the latest nightly?\r\n> https://files.pythonhosted.org/packages/67/c0/e68a4f0400340b54c887703baa8eee188042c3d65a0cf535dda71abffbc2/tf_nightly_gpu-1.13.0.dev20190205-cp37-cp37m-win_amd64.whl\r\n\r\nAny release for Linux? Or even better, a docker build? I've tried the latest version of `tensorflow/tensorflow:nightly-devel-gpu-py3` but the problem remains...", "I got the same problem and it  took about 1 min to start", "Facing the same issue with Cuda 9.0, tensorflow 1.12.0, cuDNN 7.4, windows 10, Two Nvidia RTX 2080 Tis", "I have the same problem using tensorflow 1.13.1, CUDA 10.0, cuDNN 7.5, Windows 10, nVidia 960m.\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> tf.add(1, 2)\r\n2019-03-05 17:36:49.631611: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-03-05 17:36:49.923538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.34GiB\r\n2019-03-05 17:36:49.930602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-05 17:40:20.836194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-05 17:40:20.840822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0\r\n2019-03-05 17:40:20.843478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N\r\n2019-03-05 17:40:20.855747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3050 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n<tf.Tensor: id=2, shape=(), dtype=int32, numpy=3>\r\n```", "Pardon, how long should one expect the \"matmul\" code of the topicstarter to execute on a well-configured system? On my Win with tensorflow-gpu 1.13 it takes ~3.5 s which seems to be huge just to multiply 12 numbers.", "I have the same problem when using Google Colab. I installed keras_contrib library to train model", "Having the exact same problem with TF 1.13.1 built from sources that was working perfectly before, the only thing I changed was nvidia drivers from nvidia-415 to 418. Could it have something to do with this ?", "> Having the exact same problem with TF 1.13.1 built from sources that was working perfectly before, the only thing I changed was nvidia drivers from nvidia-415 to 418. Could it have something to do with this ?\r\n\r\nI just confirmed that this was exactly the problem,  it was not about the TF version, the problem persisted across all build versions including the latest nightly. I'm using arch, so the latest upgrade installed the linux kernel 5.0 and the latest nvidia drivers 418*.\r\n\r\n**What I did was downgrade both the drivers to nvidia 415.27-9 and the kernel/headers to linux 4.20.11** \r\n\r\nTF no longer hangs on tf.Session()\r\n\r\nA list of driver versions compatible with cuda can be found in the CUDA release notes:\r\nhttps://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html", "Threadstarter here, hello.\r\n> \r\n> \r\n> Could you try with the latest nightly?\r\n> https://files.pythonhosted.org/packages/67/c0/e68a4f0400340b54c887703baa8eee188042c3d65a0cf535dda71abffbc2/tf_nightly_gpu-1.13.0.dev20190205-cp37-cp37m-win_amd64.whl\r\n\r\n**This works!** I checked with that wheel, and then with `tf-nightly-gpu-2.0-preview` on PYPI, which also worked. No more JIT-Compiles.\r\nI initially wanted to use the anaconda cudatoolkit and cudnn packages, but currently, cudnn is only available up to version 7.3.1 on anaconda-cloud. Tensorflow 2.0 however, is compiled with 7.4.1, so I had to do this the oldschool way, and download the setups from Nvidia.\r\nSoon, though...[soon](https://imgur.com/a/A2jZizt).\r\n\r\nFor everyone, here's what I did, as a guide:\r\n\r\n### How to install Tensorflow Nightly 2.0 GPU in Anaconda on Windows 10 x64\r\n\r\n\u2022 I installed these CUDA/CuDnn Versions:\r\n   \u2013 cuda_10.0.130_win10_network (Nvidia CUDA Download: https://developer.nvidia.com/cuda-toolkit)\r\n   \u2013 cuDNN v7.4.1 (Nov 8, 2018), for CUDA 10.0 (Nvidia CuDnn Download: https://developer.nvidia.com/cudnn)\r\n   \u2013 Don't forget to check, whether the Cuda setup has correctly written itself to the PATH system variable.\r\n   \u2013 Reboot.\r\n\u2022 Now make a new environment in Anaconda and activate it:\r\n   \u2013 `conda create --name tf2-nightly-gpu python=3.6`\r\n   \u2013 `activate tf2-nightly-gpu`\r\n\u2022 Now, with the new env still activated, install the latest Tensorflow 2.0 nightly GPU build from PYPI:\r\n   \u2013 `pip install tf-nightly-gpu-2.0-preview`\r\n\u2022 For machine learning in Jupyter notebook (or Jupyter Lab) , you need these as well:\r\n   \u2013 `conda install nb_conda matplotlib scipy Pillow pandas scikit-learn`\r\n\u2022 Check, if your GPU is recognized by Tensorflow. Open the Anaconda prompt, activate the new environment and type `python`, then press Enter. Now type:\r\n`import tensorflow as tf`\r\n`tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None) `\r\n\u2022 Output should be something like this:\r\n\r\n```\r\n(tf2-nightly-gpu) C:\\Users\\___>python\r\n>>> import tensorflow as tf\r\n>>> tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)\r\n2019-03-19 17:46:25.722209: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-03-19 17:46:25.729724: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-03-19 17:46:25.922934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1551] Found device 0 with properties:\r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.61GiB\r\n2019-03-19 17:46:25.938231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0\r\n2019-03-19 17:46:26.539185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1082] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-19 17:46:26.546009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1088]      0\r\n2019-03-19 17:46:26.550123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1101] 0:   N\r\n2019-03-19 17:46:26.554188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1222] Created TensorFlow device (/device:GPU:0 with 6360 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nTrue\r\n```\r\n\r\n\u2022 Done.", "Reproducible with TF1.13, Nvidia Quadra1000M, Driver 419.67 Win10x64, CUDA10.0. Takes 6 minutes for starting compute.\r\n\r\n2019-03-31 07:24:03.216999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n--> (Stuck here for 6 minutes)\r\n2019-03-31 07:34:07.713831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:", "Same here with Dell Precision + Quadro M1200... using nvidia-docker and tensorflow/tensorflow:latest-gpu\r\nw some benchmark script:\r\n```\r\nimport sys\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom datetime import datetime\r\n\r\ndevice_name = sys.argv[1]  # Choose device from cmd line. Options: gpu or cpu\r\nshape = (int(sys.argv[2]), int(sys.argv[2]))\r\nif device_name == \"gpu\":\r\n    device_name = \"/gpu:0\"\r\nelse:\r\n    device_name = \"/cpu:0\"\r\n\r\nwith tf.device(device_name):\r\n    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\r\n    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\r\n    sum_operation = tf.reduce_sum(dot_operation)\r\n\r\nstartTime = datetime.now()\r\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\r\n        result = session.run(sum_operation)\r\n        print(result)\r\n\r\n# It can be hard to see the results on the terminal with lots of output -- add some newlines to improve readability.\r\nprint(\"\\n\" * 5)\r\nprint(\"Shape:\", shape, \"Device:\", device_name)\r\nprint(\"Time taken:\", str(datetime.now() - startTime))\r\n```\r\n\r\nFirst run:\r\n\r\n('Shape:', (10000, 10000), 'Device:', '/gpu:0')\r\n('Time taken:', '0:04:35.820064')\r\n\r\nafter:\r\n('Shape:', (10000, 10000), 'Device:', '/gpu:0')\r\n('Time taken:', '0:00:03.075177')\r\n\r\nas CPU instead of gpu its always the same:\r\n('Shape:', (10000, 10000), 'Device:', '/cpu:0')\r\n('Time taken:', '0:00:15.496725')\r\n\r\n\r\nFull Log:\r\n```\r\nroot@6ebfa19ce6e9:/data# python a.py gpu 10000\r\n2019-03-31 05:33:39.251397: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-31 05:33:39.298838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-03-31 05:33:39.299426: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x44fdca0 executing computations on platform CUDA. Devices:\r\n2019-03-31 05:33:39.299444: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Quadro M1200, Compute Capability 5.0\r\n2019-03-31 05:33:39.319279: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\r\n2019-03-31 05:33:39.320003: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4566410 executing computations on platform Host. Devices:\r\n2019-03-31 05:33:39.320038: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-31 05:33:39.320376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 1.94GiB\r\n2019-03-31 05:33:39.320405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-03-31 05:33:39.321193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-31 05:33:39.321218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-03-31 05:33:39.321230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-03-31 05:33:39.321451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1759 MB memory) -> physical GPU (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0\r\n2019-03-31 05:33:39.322533: I tensorflow/core/common_runtime/direct_session.cc:317] Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0\r\n\r\nrandom_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.323990: I tensorflow/core/common_runtime/placer.cc:1059] random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324021: I tensorflow/core/common_runtime/placer.cc:1059] random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324038: I tensorflow/core/common_runtime/placer.cc:1059] random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324055: I tensorflow/core/common_runtime/placer.cc:1059] random_uniform: (Add)/job:localhost/replica:0/task:0/device:GPU:0\r\ntranspose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324071: I tensorflow/core/common_runtime/placer.cc:1059] transpose: (Transpose)/job:localhost/replica:0/task:0/device:GPU:0\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324088: I tensorflow/core/common_runtime/placer.cc:1059] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\nSum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324104: I tensorflow/core/common_runtime/placer.cc:1059] Sum: (Sum)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324122: I tensorflow/core/common_runtime/placer.cc:1059] random_uniform/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324138: I tensorflow/core/common_runtime/placer.cc:1059] random_uniform/min: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nrandom_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324155: I tensorflow/core/common_runtime/placer.cc:1059] random_uniform/max: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\ntranspose/perm: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324172: I tensorflow/core/common_runtime/placer.cc:1059] transpose/perm: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nConst: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.324188: I tensorflow/core/common_runtime/placer.cc:1059] Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2019-03-31 05:33:39.331517: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n250030770000.0\r\n\r\n\r\n\r\n\r\n\r\n\r\n('Shape:', (10000, 10000), 'Device:', '/gpu:0')\r\n('Time taken:', '0:04:35.820064')\r\n```\r\n\r\nstarted by:\r\nnvidia-docker run tensorflow/tensorflow:latest-gpu\r\n\r\n\r\nthanks ahead", "> os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\n\r\nThis works for me as well.\r\n\r\n", "> I noticed that tensorflow always takes about ~2min before it actually starts to compute. I've been trying to find out, why this happens, and nothing really worked so far.\r\n> \r\n> [Tensorflow site](https://www.tensorflow.org/install/install_windows) says, I should use CUDA\u00ae Toolkit 9.0 and cuDNN v7.0. I have CUDA 9.0, so I downloaded CuDNN 7.0.5 for CUDA 9.0 and pasted the files to *C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0*, overwriting the ones form cuDNN 7.1.2, which I tested earlier. To make sure, I pip-installed tensorflow-gpu into a fresh anaconda env. See install [here](https://pastebin.com/rjiV1s3b). The issue is still the same.\r\n> \r\n> CUDA works, since it prints the _'Hello, TensorFlow!'_, when I use the official test example, but before that it takes like 2minutes every time!\r\n> \r\n> When I tested this with [another wheel](https://drive.google.com/drive/folders/1lVK_ABvVHzVYKs7X5SUhcZFBgKpC41Qw) ([which is linked in this tutorial](http://www.python36.com/install-tensorflow-gpu-windows/), I did not compile it myself.) on cuda 9.1/cudnn7.0.5, I had the same issues. A NVIDIA employee [on stackoverflow](https://stackoverflow.com/questions/49770217/why-does-cuda-initialisation-take-so-long-python-vscode-anaconda-tensorflow) suggested, I may be hitting a lengthy JIT compile step, because the GTX 1080 has compute capability of 6.1, which the wheel I used may not be compiled for.\r\n> \r\n> So I tried to find wheels for tensorflow with compute capability 6.1 for windows, but [the only one I found](https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.5.0/py36/GPU/cuda91cudnn7avx2) and tested produced the same problem.\r\n> \r\n> Am I doing something wrong here, or do I just have to accept the 2min delay everytime I start my tensorflow/keras scripts?\r\n> \r\n> ### System information\r\n> * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n>   Code:\r\n> \r\n> ```\r\n> import time\r\n> start_time = time.time()\r\n> import tensorflow as tf\r\n> a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n> b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n> c = tf.matmul(a, b)\r\n> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n> print(sess.run(c))\r\n> timer = time.time()\r\n> print(timer - start_time)\r\n> ```\r\n> \r\n> Output:\r\n> \r\n> ```\r\n> (tf_clean) C:\\python_code\\test>C:/anaconda/envs/tf_clean/python.exe c:/python_code/test/tf_test.py\r\n> 2018-04-18 14:36:04.376661: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this\r\n> TensorFlow binary was not compiled to use: AVX2\r\n> 2018-04-18 14:36:04.689661: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties:\r\n> name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\n> pciBusID: 0000:01:00.0\r\n> totalMemory: 8.00GiB freeMemory: 6.60GiB\r\n> 2018-04-18 14:36:04.699485: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n> 2018-04-18 14:38:12.227561: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2018-04-18 14:38:12.234504: I T:\\src\\github\\tens2018-04-18 14:38:12.237156: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N\r\n> 2018-04-18 14:38:12.240997: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6379 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n> Device mapping:\r\n> /job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n> 2018-04-18 14:38:12.548288: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\direct_session.cc:297] Device mapping:\r\n> /job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n> MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\n> 2018-04-18 14:38:12.559262: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:884] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\n> b: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n> 2018-04-18 14:38:12.564847: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:884] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n> a: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n> 2018-04-18 14:38:12.570545: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\placer.cc:884] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n> [[22. 28.]\r\n>  [49. 64.]]\r\n> 129.14624643325806\r\n> ```\r\n> \r\n> * **OS Platform and Distribution**:\r\n>   Windows 10 Education (Version 10.0.16299 Build 16299)\r\n>   Intel(R) Core(TM) i5-7500 CPU @ 3.40GHz, 3408 MHz, 4 Cores\r\n> * **TensorFlow installed from (source or binary)**:\r\n>   binary\r\n> * **TensorFlow version**:\r\n>   tensorflow-gpu 1.5.0, 1.7.0\r\n> * **Python version**:\r\n>   3.5.5 & 3.6 (via anaconda, conda 4.5.1.)\r\n> * **Bazel Version**:\r\n>   N/A\r\n> * **CUDA/cuDNN version**:\r\n>   Tested combinations:\r\n>   CUDA 9.0 and CuDNN 7.1.2 (tested on tensorflow 1.5.0, 1.7.0 and 1.8.0-dev20180329)\r\n>   CUDA 9.1 and CuDNN 7.0.5 (tested on tensorflow 1.5.0 and 1.7.0)\r\n> * **GPU model and memory**:\r\n>   NVIDIA GeForce GTX 1080 (GP104-400) [Hewlett-Packard], 8192 MBytes of GDDR5X SDRAM [Micron]\r\n> * **Exact command to reproduce**:\r\n>   See: _Have I written custom code..._\r\n> \r\n> =================================================================\r\n> EDIT:\r\n> \r\n> Threadstarter here, hello.\r\n> \r\n> > Could you try with the latest nightly?\r\n> > https://files.pythonhosted.org/packages/67/c0/e68a4f0400340b54c887703baa8eee188042c3d65a0cf535dda71abffbc2/tf_nightly_gpu-1.13.0.dev20190205-cp37-cp37m-win_amd64.whl\r\n> \r\n> **This works!** I checked with that wheel, and then with `tf-nightly-gpu-2.0-preview` on PYPI, which also worked.\r\n> I initially wanted to use the anaconda cudatoolkit and cudnn packages, but currently, cudnn is only available up to version 7.3.1 on anaconda-cloud. Tensorflow 2.0 however, is compiled with 7.4.1, so I had to do this the oldschool way, and download the setups from Nvidia.\r\n> Soon, though...[soon](https://imgur.com/a/A2jZizt).\r\n> \r\n> For everyone, here's what I did, as a guide:\r\n> \r\n> ### How to install Tensorflow Nightly 2.0 GPU in Anaconda on Windows 10 x64\r\n> \u2022 I installed these CUDA/CuDnn Versions:\r\n> \u2013 cuda_10.0.130_win10_network (Nvidia CUDA Download: https://developer.nvidia.com/cuda-toolkit)\r\n> \u2013 cuDNN v7.4.1 (Nov 8, 2018), for CUDA 10.0 (Nvidia CuDnn Download: https://developer.nvidia.com/cudnn)\r\n> \u2013 Don't forget to check, whether the Cuda setup has correctly written itself to the PATH system variable.\r\n> \u2013 Reboot.\r\n> \u2022 Now make a new environment in Anaconda and activate it:\r\n> \u2013 `conda create --name tf2-nightly-gpu python=3.6`\r\n> \u2013 `activate tf2-nightly-gpu`\r\n> \u2022 Now, with the new env still activated, install the latest Tensorflow 2.0 nightly GPU build from PYPI:\r\n> \u2013 `pip install tf-nightly-gpu-2.0-preview`\r\n> \u2022 For machine learning in Jupyter notebook (or Jupyter Lab) , you need these as well:\r\n> \u2013 `conda install nb_conda matplotlib scipy Pillow pandas scikit-learn`\r\n> \u2022 Check, if your GPU is recognized by Tensorflow. Open the Anaconda prompt, activate the new environment and type `python`, then press Enter. Now type:\r\n> `import tensorflow as tf`\r\n> `tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None) `\r\n> \u2022 Output should be something like this:\r\n> \r\n> ```\r\n> (tf2-nightly-gpu) C:\\Users\\___>python\r\n> >>> import tensorflow as tf\r\n> >>> tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)\r\n> 2019-03-19 17:46:25.722209: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n> 2019-03-19 17:46:25.729724: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n> 2019-03-19 17:46:25.922934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1551] Found device 0 with properties:\r\n> name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\n> pciBusID: 0000:01:00.0\r\n> totalMemory: 8.00GiB freeMemory: 6.61GiB\r\n> 2019-03-19 17:46:25.938231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1674] Adding visible gpu devices: 0\r\n> 2019-03-19 17:46:26.539185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1082] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-03-19 17:46:26.546009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1088]      0\r\n> 2019-03-19 17:46:26.550123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1101] 0:   N\r\n> 2019-03-19 17:46:26.554188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1222] Created TensorFlow device (/device:GPU:0 with 6360 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n> True\r\n> ```\r\n> \r\n> \u2022 Done.\r\n\r\nDoesn't work for me it still frozeen on \r\n\r\n>  I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0", "This should be fixed with 1.13.1", "it fixed but i have other error", "@gunan is there a workaround to fix this issue for tensorflow 1.12.2? \r\n(we are still on cuda 9 and 1.13.1 is compiled for cuda 10 -.-)", "Unfortunately, it will be complicated to port the change back to 1.12.2 as it uses an old eigen version which causes compilation issues when built for newer cuda compute capabilities.\r\n\r\nOnly option I see for your case is to build 1.13.1 from sources for cuda 9. You can follow our guide at www.tensorflow.org to build TF from sources.\r\n\r\nI will close this issue, as it should be fixed at head. Please feel free to open a new issue with further issues you see.", "Very embarrassed. I use nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04 as the base when building my docker image. I try many methods to avoid the problem (I try with Python3.5,3.6,3.7\uff0cinstall tensorflow-gpu-1.12.2 through pip or build tensorflow-1.13.0 by myself) but failed...... \r\n\r\nFinally, I install tensorflow-gpu-1.12.0 through pip and succeed.\r\n\r\nIt seems that the newest version incurs the error, may be helpful for you.", "Hey guys.\r\n\r\nI am super new to tensorflow and programming in python. Initializing my current programme takes forever aswell. \r\n\r\nIt seems like you guys handled the issue in 1.12.0. \r\n\r\nI am using PyCharm and installed Tensorflow through PyCharm, version 1.13.1. using python 3.6.\r\nIt seems like that the version 1.13.1 does not incur the error, at least for me. \r\nHonestly, I do not know what base I am using If even I am using a base.. :/\r\n\r\nDo you have any tips for me? Should I try to build tensorflow from source or something like that?", "I have the exact same problem. It take around 5 minutes at:\r\nAdding visible gpu devices: 0\r\n\r\nMy environment is Win10,  tensorflow-gpu-2.0-beta1\uff0c CUDA 10.0, cuDNN 7.6, python 3.6 and with GTX 850M\r\n\r\nWhen the problem will be fixed\uff1f", "Same problem here:\r\nUbuntu 16.04 / tensorflow-gpu-1.14 / CUDA 10.0 / cuDNN 7.4 / Python 3.7 / GTX 950M\r\n\r\nBut just like @steel3d, it happened ONLY on the very first run (stuck for around 3min here). After that, it becomes instant.", "Exactly the same problem with even _15_ minutes of waiting for Ubuntu 16.04 / tensorflow-gpu==2.0.0beta1 / CUDA 10.0 / cuDNN 7.5 / python 3.6 / 8 x Tesla P40. Btw for the same code this initialization time is much better with 1 GPU.", "in P100, the same problem with 10 minutes .  \r\n\r\n#0  0x00007f14855ed15c in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#1  0x00007f14855ed3d9 in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#2  0x00007f14858588ea in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#3  0x00007f148585f344 in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#4  0x00007f14855bba7e in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#5  0x00007f1485865190 in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#6  0x00007f1485865e39 in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#7  0x00007f14854a158d in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#8  0x00007f14854a60e5 in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#9  0x00007f1485b41f7d in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#10 0x00007f14854a83e4 in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#11 0x00007f14854aaf58 in ?? () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#12 0x00007f14854a293c in __cuda_CallJitEntryPoint () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#13 0x00007f1485515486 in nvPTXCompilerCompile () from /usr/lib64/nvidia/libnvidia-ptxjitcompiler.so.1\r\n#14 0x00007f163234d7df in fatBinaryCtl_Compile () from /usr/lib64/nvidia/libnvidia-fatbinaryloader.so.396.26\r\n#15 0x00007f165368a824 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#16 0x00007f165368b303 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#17 0x00007f16535d36dd in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#18 0x00007f16535d39f0 in ?? () from /usr/lib64/nvidia/libcuda.so.1\r\n#19 0x00007f1654b1360d in ?? () from /usr/local/cuda/lib64/libcudart.so.9.0\r\n#20 0x00007f1654b09dc0 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.0\r\n#21 0x00007f1654b18596 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.0\r\n#22 0x00007f1654b1bff1 in ?? () from /usr/local/cuda/lib64/libcudart.so.9.0\r\n#23 0x00007f1654b0e11e in ?? () from /usr/local/cuda/lib64/libcudart.so.9.0\r\n#24 0x00007f1654af993e in ?? () from /usr/local/cuda/lib64/libcudart.so.9.0\r\n#25 0x00007f1654b2e874 in cudaFree () from /usr/local/cuda/lib64/libcudart.so.9.0\r\n#26 0x00007f165d66d7d7 in tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) ()\r\n   from /usr/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#27 0x00007f165d6a0921 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::string const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) ()\r\n   from /usr/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#28 0x00007f1661877428 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#29 0x00007f165d6ee24f in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) () from /usr/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#30 0x00007f165fc15435 in TF_NewSession () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#31 0x00007f165f858635 in _wrap_TF_NewSession () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#32 0x00007f1725c672c0 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#33 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#34 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#35 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#36 0x00007f1725bf303d in function_call () from /lib64/libpython2.7.so.1.0\r\n#37 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#38 0x00007f1725bdd025 in instancemethod_call () from /lib64/libpython2.7.so.1.0\r\n#39 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#40 0x00007f1725c25057 in slot_tp_init () from /lib64/libpython2.7.so.1.0\r\n#41 0x00007f1725c23d6f in type_call () from /lib64/libpython2.7.so.1.0\r\n#42 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#43 0x00007f1725c62806 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#44 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#45 0x00007f1725bf303d in function_call () from /lib64/libpython2.7.so.1.0\r\n#46 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#47 0x00007f1725bdd025 in instancemethod_call () from /lib64/libpython2.7.so.1.0\r\n#48 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#49 0x00007f1725c25057 in slot_tp_init () from /lib64/libpython2.7.so.1.0\r\n#50 0x00007f1725c23d6f in type_call () from /lib64/libpython2.7.so.1.0\r\n#51 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#52 0x00007f1725c62806 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#53 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#54 0x00007f1725bf2f48 in function_call () from /lib64/libpython2.7.so.1.0\r\n#55 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#56 0x00007f1725bdd025 in instancemethod_call () from /lib64/libpython2.7.so.1.0\r\n#57 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#58 0x00007f1725c25057 in slot_tp_init () from /lib64/libpython2.7.so.1.0\r\n#59 0x00007f1725c23d6f in type_call () from /lib64/libpython2.7.so.1.0\r\n#60 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#61 0x00007f1725c5fec7 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0\r\n#62 0x00007f1725c5bf23 in builtin_map () from /lib64/libpython2.7.so.1.0\r\n#63 0x00007f1725c672c0 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#64 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#65 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#66 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#67 0x00007f1725be8888 in gen_send_ex.isra.0 () from /lib64/libpython2.7.so.1.0\r\n#68 0x00007f1725c62211 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#69 0x00007f1725be8888 in gen_send_ex.isra.0 () from /lib64/libpython2.7.so.1.0\r\n#70 0x00007f1725c62211 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#71 0x00007f1725be8888 in gen_send_ex.isra.0 () from /lib64/libpython2.7.so.1.0\r\n#72 0x00007f1725bf79a6 in listextend () from /lib64/libpython2.7.so.1.0\r\n#73 0x00007f1725bf7c30 in list_init () from /lib64/libpython2.7.so.1.0\r\n#74 0x00007f1725c23d6f in type_call () from /lib64/libpython2.7.so.1.0\r\n#75 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#76 0x00007f1725c62806 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#77 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#78 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#79 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#80 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#81 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#82 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#83 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#84 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#85 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#86 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#87 0x00007f1725bf303d in function_call () from /lib64/libpython2.7.so.1.0\r\n#88 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#89 0x00007f1725bdd025 in instancemethod_call () from /lib64/libpython2.7.so.1.0\r\n#90 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#91 0x00007f1725c25057 in slot_tp_init () from /lib64/libpython2.7.so.1.0\r\n#92 0x00007f1725c23d6f in type_call () from /lib64/libpython2.7.so.1.0\r\n#93 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#94 0x00007f1725c62806 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#95 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#96 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#97 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#98 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#99 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#100 0x00007f1725bf303d in function_call () from /lib64/libpython2.7.so.1.0\r\n#101 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#102 0x00007f1725c61ccd in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#103 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#104 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#105 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#106 0x00007f1725c69712 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\r\n#107 0x00007f1725c794fc in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0\r\n#108 0x00007f1725c79778 in load_source_module () from /lib64/libpython2.7.so.1.0\r\n#109 0x00007f1725c7a411 in import_submodule () from /lib64/libpython2.7.so.1.0\r\n#110 0x00007f1725c7a65d in load_next () from /lib64/libpython2.7.so.1.0\r\n#111 0x00007f1725c7b03e in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0\r\n#112 0x00007f1725c5e2ef in builtin___import__ () from /lib64/libpython2.7.so.1.0\r\n#113 0x00007f1725c672c0 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#114 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#115 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#116 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#117 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#118 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#119 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#120 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#121 0x00007f1725bf303d in function_call () from /lib64/libpython2.7.so.1.0\r\n#122 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#123 0x00007f1725bdd025 in instancemethod_call () from /lib64/libpython2.7.so.1.0\r\n#124 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#125 0x00007f1725c25057 in slot_tp_init () from /lib64/libpython2.7.so.1.0\r\n#126 0x00007f1725c23d6f in type_call () from /lib64/libpython2.7.so.1.0\r\n#127 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#128 0x00007f1725c62806 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#129 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#130 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#131 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#132 0x00007f1725c66c8d in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#133 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#134 0x00007f1725bf303d in function_call () from /lib64/libpython2.7.so.1.0\r\n#135 0x00007f1725bce033 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#136 0x00007f1725c61ccd in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#137 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#138 0x00007f1725c66b0c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#139 0x00007f1725c6960d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#140 0x00007f1725c69712 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0\r\n#141 0x00007f1725c82b4f in run_mod () from /lib64/libpython2.7.so.1.0\r\n#142 0x00007f1725c83d0e in PyRun_FileExFlags () from /lib64/libpython2.7.so.1.0\r\n#143 0x00007f1725c84f99 in PyRun_SimpleFileExFlags () from /lib64/libpython2.7.so.1.0\r\n#144 0x00007f1725c9614f in Py_Main () from /lib64/libpython2.7.so.1.0\r\n#145 0x00007f1724eb1445 in __libc_start_main () from /lib64/libc.so.6\r\n#146 0x000000000040071e in _start ()\r\n\r\n\r\n", "@chengdianxuezi OMG, unbelievable\uff01My GPU is GTX 850M , they told me that the problem is there because my gpu is not good enough. I can't image that the P100 still has the same problem. So many people have the problem, but the official or the community seem to have no idea how to fix it. \r\nSo If you can't stand it, just use Pytorch or MXnet!!! Mxnet is great. ", "It has something to do with ~/.nv/ComputeCache directory (compiling kernels?). Probably persisting it between container invocations can speed up things at least in second and further times.", "Hi @gunan, can we support compute capabilities 7.5 in the official build? It seems tensorflow are still using `TF_CUDA_COMPUTE_CAPABILITIES=3.5,3.7,5.2,6.0,6.1,7.0` as default. And we are facing slow start on NV 2080Ti.", "@chsigg For questions about 7.5", "I would have thought that CUBINs for 7.0 would also work for sm 7.5. But I guess the runtime prefers to JIT the PTX for 7.0 instead?\r\n\r\nThe list of CUDA kernel binaries that we include is already plenty long. Maybe it would be a good time to clean it up.\r\n\r\nHere is a short list of compute capabilities we currently support. Full list is at https://en.wikipedia.org/wiki/CUDA\r\n\r\nsm 3.5 (Kepler): Tesla K20/K40\r\nsm 3.7 (Kepler): Tesla K80\r\nsm 5.2 (Maxwell): Tesla M4, GeForce 9xx\r\nsm 6.0 (Pascal): Tesla P100\r\nsm 6.1 (Pascal): Titan X, Titan Xp, Tesla P4\r\nsm 7.0 (Volta): Titan V, Tesla V100\r\nsm 7.5 (Turing): Titan RTX, Tesla T4\r\n\r\nLooking at this list, I would probably go with 5.2, 6.0, 7.0, 7.5. I'm suspecting though that we added 6.1 for the same reason that you want to add 7.5: the CUBIN is compatible, but the runtime decides to JIT the PTX. This is in contrast to the following from https://docs.nvidia.com/cuda/volta-compatibility-guide/index.html:\r\n\r\n> If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU's native cubin format before launching it.\r\n\r\nIn theory, the long startup could also be from patching the CUBIN to work with a newer minor revision (even though the documentation says it's compatible, the SASS code usually does need some massaging). We could try whether CUDA_\u200bFORCE_\u200bPTX_\u200bJIT=1 makes the startup even slower, but probably it doesn't really matter what the actual cause is.\r\n\r\nWhat do you think?", "Actually, we probably have some more options playing with combinations of PTX and CUBINs. We can prevent the runtime to JIT from PTX 7.0 to SASS 7.5 (and PTX 6.0 to SASS 6.1) and force it to use CUBIN 7.0 (and 6.0) by stripping the corresponding PTX.\r\n\r\nCould you try nvprune'ing the sm 7.0 PTX to see if that improves your startup time? \r\n\r\nIf that works, maybe we should build the following:\r\nCUBIN for 5.2, 6.0, 7.0, 7.5\r\nPTX for 3.5 (keep basic support for sm 3.5 - 5.0 with expected slow startup time)\r\n", "Hi @chsigg, do you mean `nvprune  -arch sm_35 libtensorflow_framework.so ...`?", "A binary with just sm_35 is only going to work on sm_35 and sm_37 GPUs. \r\n\r\nFor the experiment whether forcing TF to use sm_70 CUBIN for your sm_75 GPU improves startup time, use `nvprune -arch sm_70 libtensorflow_framework.so`.\r\n\r\n", "Hi @chsigg , I can not nvprune the tf library: `nvprune fatal   : Input file 'libtensorflow_framework.so' not relocatable`.\r\n\r\nBTW, the slow start occurs when I restore the SavedModel through tensorflow c api (not tensorflow python at `sess = tf.Session(...)` as other people mentioned before). It takes 5 minutes to reload our model on the first start or after clearing `~/.nv`, but only 2 seconds on the subsequent runs.\r\n\r\n```\r\n2019-12-02 09:48:42.596399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10312 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:b5:00.0, compute capability: 7.5)\r\n2019-12-02 09:48:42.646051: I tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n2019-12-02 09:53:46.398005: I tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 304114966 microseconds.\r\n```\r\n", "If I compile tensorflow by `TF_CUDA_COMPUTE_CAPABILITIES=...,7.5`, the start time is also around 2s.\r\n\r\n```\r\n2019-12-02 10:22:00.665098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10312 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:b5:00.0, compute capability: 7.5)\r\n2019-12-02 10:22:00.727352: I tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n2019-12-02 10:22:02.507115: I tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 2362567 microseconds.\r\n```", "Sorry about that, I didn't know about that nvprune restriction. \r\n\r\nCould you please try to change [these](https://github.com/tensorflow/tensorflow/blob/456e027373d553ad10c15ceaeee1eb78041b79dc/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl#L216) lines to:\r\n\r\n`nvccopts += r'-gencode=arch=compute_%s,\\\"code=sm_%s\" ' % (capability, capability)`\r\n\r\nAnd try again with just `TF_CUDA_COMPUTE_CAPABILITIES=7.0` (i.e. without 7.5)?\r\n\r\nYou will need to use nvcc for the compiler for this to work (you can check that .bazelrc.user specifies `--config=cuda` and not `--config=cuda_clang`).\r\n\r\nThanks for you help.\r\n", "That works.\r\n\r\n```\r\n2019-12-03 03:48:46.492801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10312 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:b5:00.0, compute capability: 7.5)\r\n2019-12-03 03:48:46.538021: I tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n2019-12-03 03:48:48.441281: I tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 2268220 microseconds.\r\n```", "Thanks a lot Liwen for trying it out. I will update the list of compute capabilities we build for, and add a warning message when we need to JIT from PTX to make it more obvious what's going on.", "I'm having the same issue now.\r\nTF 1.15, Cuda 10.0, Cudnn 7, \r\nTF was custom compiled with AVX2, XLA, TRT, CC 3.5/3.7/7.0/7.5\r\n\r\nI tried to debug it with strace, and found that there's a futex that locks the execution thread:\r\n18:38:00.532805 futex(0x7f852818fa78, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff) = 0 <47.742524>\r\n\r\nAnother thread starts to work heavily with huge batch of mprotect:\r\n18:38:00.534906 mprotect(0x7f84f98ac000, 4096, PROT_READ|PROT_WRITE) = 0 <0.000030>\r\n\r\nDuring this process I see 2 messages in main thread:\r\n2019-12-03 18:38:16.642624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-03 18:38:26.258193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n\r\nThe futex is the released and returns the result.\r\n\r\nMy test run was like this:\r\n1) Run new aws p2 instance A from ami, trying to run the code - 1 minute delay\r\n2) Trying to run the code on A once more - no delay\r\n3) A is rebooted, trying to run the code - no delay\r\n4) Run new aws p2 instance B from ami, trying to run the code - 1 minute delay\r\n\r\nSeems like something is calculated once, then cached.\r\nThe cache is preserved upon system restart, but is missing on the first run.\r\nIt's also not part of my AMI.\r\n\r\nI also tried to rerun the test with official TF 1.15 wheel, but faced the same problem. \r\n\r\nMay somebody clarify several things?\r\n1) What is cached?\r\n2) Where?\r\n3) Is there any workaround to make this cache part of my ami?\r\n\r\nMy ~/.nv/ComputeCache is empty.\r\n\r\nMaybe it's related to https://github.com/keras-team/keras/issues/11126, not sure.\r\n\r\nThanks in advance!\r\n", "I think if the cache can be made persistent is a more nvidia question?\r\nTo make it a part of your AMI, you can rebuild TF with all the compute capabilities that your AMI will potentially need. AWS P2 uses k80 GPUs, which needs compute capability 3.7.\r\nYou may need other compute capabilities based on other GPUs available to you.\r\n\r\nI am closing this issue, as this is all known, and documented.\r\nTo sum up: If you see this error, that means your GPU has a Cuda compute capability TF binary you are using does not have packaged in.\r\nTo work through the problem, you will need to first check which compute capability your GPU needs here: https://developer.nvidia.com/cuda-gpus\r\nThen rebuild TF from sources with that compute capability enabled (which you select during configure)", "> Sorry about that, I didn't know about that nvprune restriction.\r\n> \r\n> Could you please try to change [these](https://github.com/tensorflow/tensorflow/blob/456e027373d553ad10c15ceaeee1eb78041b79dc/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl#L216) lines to:\r\n> \r\n> `nvccopts += r'-gencode=arch=compute_%s,\\\"code=sm_%s\" ' % (capability, capability)`\r\n> \r\n> And try again with just `TF_CUDA_COMPUTE_CAPABILITIES=7.0` (i.e. without 7.5)?\r\n> \r\n> You will need to use nvcc for the compiler for this to work (you can check that .bazelrc.user specifies `--config=cuda` and not `--config=cuda_clang`).\r\n> \r\n> Thanks for you help.\r\n\r\nHi @gunan @chsigg , is the change mentioned in this comment merged into tensorflow?", "**OS Platform and Distribution**:\r\n- OS: Ubuntu 18.04.4\r\n- Docker: 19.03.12, build 48a66213fe\r\n- CPU: Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz\r\n- GPU: GeForce RTX 2080 440.100 (compute capability: 7.5)\r\n- `nvidia-container-runtime` is installed too\r\n\r\n**Problem**:\r\nWhen using `tensorflow/serving:2.2.0-gpu`, running the image always takes around 5 minutes. However, older `tensorflow/serving` versions run instantly. I have tested `2.1.0-gpu` and `1.15.0-gpu`.\r\n\r\nWhile waiting for the container to run, one CPU core is active and some directories are created in the container's writable layer, most notably `/root/.nv/ComputeCache` (~ 250 MB).\r\n\r\n**What I have tried**:\r\n1. Persist all changes by committing the container as a new image.\r\n2. Persist changes by using Docker volumes.\r\n3. Build a new Docker image from source. Not sure if `--build-arg` is the right place to use `TF_CUDA_COMPUTE_CAPABILITIES`.\r\n\r\n    ```\r\n    docker build --pull -t $USER/tensorflow-serving-devel-gpu \\\r\n      --build-arg TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\" \\\r\n      -f tensorflow_serving/tools/docker/Dockerfile.devel-gpu .\r\n    ```\r\n\r\n    ```\r\n    docker build -t $USER/tensorflow-serving-gpu \\\r\n      --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel-gpu \\\r\n      -f tensorflow_serving/tools/docker/Dockerfile.gpu .\r\n    ```\r\n\r\nNo luck, each `docker run` still takes 5 minutes. Do you have any suggestions how to speed up running the container?", "You are probably seeing the driver JITing PTX code. See https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-compatibility for details.\r\n\r\nYou can use `nvidia-smi` to determine what GPUs you have, and you can use `cuobjdump` on `_pywrap_tensorflow_internal.so` to determine what GPUs you built for.", "@chsigg Should we print a warning if we detect we're running on a GPU we don't have SASS for?", "> You can use `nvidia-smi` to determine what GPUs you have, and you can use `cuobjdump` on `_pywrap_tensorflow_internal.so` to determine what GPUs you built for.\r\n\r\nThe output of `nvidia-smi` from running container `$USER/tensorflow-serving-gpu`:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2080    Off  | 00000000:01:00.0  On |                  N/A |\r\n| 27%   39C    P8    13W / 215W |    364MiB /  7981MiB |      4%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nThe output of `cuobjdump -lelf` from running container `$USER/tensorflow-serving-devel-gpu`:\r\n\r\n```\r\nELF file    1: _pywrap_tensorflow_internal.1.sm_35.cubin\r\nELF file    2: _pywrap_tensorflow_internal.2.sm_37.cubin\r\nELF file    3: _pywrap_tensorflow_internal.3.sm_52.cubin\r\nELF file    4: _pywrap_tensorflow_internal.4.sm_60.cubin\r\nELF file    5: _pywrap_tensorflow_internal.5.sm_61.cubin\r\nELF file    6: _pywrap_tensorflow_internal.6.sm_70.cubin\r\nELF file    7: _pywrap_tensorflow_internal.7.sm_35.cubin\r\nELF file    8: _pywrap_tensorflow_internal.8.sm_37.cubin\r\nELF file    9: _pywrap_tensorflow_internal.9.sm_52.cubin\r\nELF file   10: _pywrap_tensorflow_internal.10.sm_60.cubin\r\nELF file   11: _pywrap_tensorflow_internal.11.sm_61.cubin\r\nELF file   12: _pywrap_tensorflow_internal.12.sm_70.cubin\r\n...\r\nELF file 1441: _pywrap_tensorflow_internal.1441.sm_35.cubin\r\nELF file 1442: _pywrap_tensorflow_internal.1442.sm_37.cubin\r\nELF file 1443: _pywrap_tensorflow_internal.1443.sm_52.cubin\r\nELF file 1444: _pywrap_tensorflow_internal.1444.sm_60.cubin\r\nELF file 1445: _pywrap_tensorflow_internal.1445.sm_61.cubin\r\nELF file 1446: _pywrap_tensorflow_internal.1446.sm_70.cubin\r\n```\r\n\r\nIt seems that the devel container was not built with compute capability 7.5. Can you advise where should the `TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\"` argument go? I have used it as a `--build-arg` which apparently has no effect:\r\n\r\n```\r\ndocker build --pull -t $USER/tensorflow-serving-devel-gpu \\\r\n  --build-arg TF_CUDA_COMPUTE_CAPABILITIES=\"7.5\" \\\r\n  -f tensorflow_serving/tools/docker/Dockerfile.devel-gpu .\r\n```\r\n\r\n@sanjoy I agree that a warning would be a good idea. I was used to run `tensorflow/serving:2.1.0` and immediately be able to make gRPC calls against it. With `2.2.0`, the client was returning `StatusCode.UNAVAILABLE` and I spent a fair amount of time blaming the connection (my `docker-compose.yml` file, to be specific). I completely overlooked the fact that the message `Running gRPC ModelServer at 0.0.0.0:8500 ...` was not printed yet.", "See https://www.tensorflow.org/install/source#gpu_support_3 for instructions how to build TF inside a docker. Specify `7.5` when asked for the compute capabilities during `./configure`.\r\n\r\nI think we have tried before to issue a warning when JITing from PTX but couldn't find a clean way to do so. With the build metadata available this should be possible now, but maybe not until after the kernels have been JITed.", "add this line to your code:\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\n\r\nthis worked for me.", "@SujanSharma07 worked for what? What is that accomplishing?", "> @SujanSharma07 worked for what? What is that accomplishing?\r\n\r\nAFTER THAT, my tensorflow model starts quickly ", "@SujanSharma07 can you be more specific? You did nothing else, didn't recompile, you just added os.environ['TF_CPP_MIN_LOG_LEVEL']='2'? \r\nNote that in my case, my first run was very slow, but subsequent runs were fast. Are you sure you'd get slow startup over and over without your change? \r\nIf it works, what's the justification? Where did you get the idea?", "my code stop at the following point:\r\n```\r\n2020-08-10 22:16:25.588882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-08-10 22:16:26.719130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-10 22:16:26.719170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 1 2 3\r\n2020-08-10 22:16:26.719178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N Y N N\r\n2020-08-10 22:16:26.719181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1: Y N N N\r\n2020-08-10 22:16:26.719185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2: N N N Y\r\n2020-08-10 22:16:26.719189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3: N N Y N\r\n2020-08-10 22:16:26.719882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2020-08-10 22:16:26.720283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10409 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2020-08-10 22:16:26.720567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10409 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2020-08-10 22:16:26.720884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10409 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n```\r\nit stucks for over 20 minutes, could anybody help me solve this or know the reason of this?", "export CUDA_CACHE_MAXSIZE=2147483648 \r\nexport CUDA_CACHE_DISABLE=0", "@RayerXie what version of TF are you using?  `tf-nightly` should not need to block for JIT compilation on compute capability 6.1.", "> my code stop at the following point:\r\n> \r\n> ```\r\n> 2020-08-10 22:16:25.588882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\r\n> 2020-08-10 22:16:26.719130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-08-10 22:16:26.719170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 1 2 3\r\n> 2020-08-10 22:16:26.719178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N Y N N\r\n> 2020-08-10 22:16:26.719181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1: Y N N N\r\n> 2020-08-10 22:16:26.719185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2: N N N Y\r\n> 2020-08-10 22:16:26.719189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3: N N Y N\r\n> 2020-08-10 22:16:26.719882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n> 2020-08-10 22:16:26.720283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10409 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n> 2020-08-10 22:16:26.720567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10409 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n> 2020-08-10 22:16:26.720884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10409 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n> ```\r\n> \r\n> it stucks for over 20 minutes, could anybody help me solve this or know the reason of this?\r\n\r\nThe same issue. Did you have any effective solution? @RayerXie", "Hi everyone, we found a solution that worked for us.\r\nAdd the following to your code:\r\n```\r\nimport tensorflow as tf\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu, True)\r\ntf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n```\r\n\r\nWe found the solution on this other issue solves this issue as well:\r\nhttps://github.com/keras-team/keras/issues/10634#issuecomment-608265288", "@steel3d Any updates/ workarounds you managed to find? Running into a similar situation as you, with a Tesla T4, Ubuntu 18.04.5 LTS, on AWS.\r\nBuilt tensorflow from scratch for `sm 7.5`, which helped, but it still takes more time on the first run compared to the successive ones. If it's any help, the process in question that the time difference is for involves loading some saved model protobufs as well.", "After upgrading to 2.5.0 I experience this on one of my machines.\r\n\r\nAll machines on Ubuntu 16.04, which use GeForce 1080 GTX cards, the trainings starts immediately.\r\n\r\nHowever, on Ubuntu 18.04 which uses two Titan RTX cards I have to wait a few minutes after the last line:\r\n\r\n```\r\n...\r\n2021-06-18 10:54:01.545228: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-18 10:54:01.547936: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-06-18 10:54:01.548037: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-06-18 10:54:01.548949: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-06-18 10:54:01.549216: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-06-18 10:54:01.550131: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\r\n2021-06-18 10:54:01.550900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-06-18 10:54:01.551052: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-06-18 10:54:01.554267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\r\n```\r\n\r\nI did not experience this with 2.4.1\r\n\r\n### Versions\r\n\r\n- Python: 3.9.5\r\n- Tensorflow: 2.5.0\r\n- CUDA: 11.2\r\n- NVIDIA driver: 465.19.01", "My solution is, build from source, and set the same  compute capabilities as the target GPU devices when you exec ./configure. For me, I set 6.0 for p100 and 7.5 for t4,\r\n# capabilities >= 3.5 [Default is: 6.1,6.1,6.1,6.1]: 6.0,7.5\r\nThen use the .so(for c++ infer) or .whl(for py), and you will not stuck at \"Adding visible gpu devices: 0\".", "> My solution is, build from source, and set the same compute capabilities as the target GPU devices when you exec ./configure. For me, I set 6.0 for p100 and 7.5 for t4,\r\n> # capabilities >= 3.5 [Default is: 6.1,6.1,6.1,6.1]: 6.0,7.5\r\n> \r\n> Then use the .so(for c++ infer) or .whl(for py), and you will not stuck at \"Adding visible gpu devices: 0\".\r\n\r\nI recompiled tensorflow 1.14  with 7.5 CUDA compute camabilities for tesla T4, it still can't not work..\r\nBTW,  I use C++ api for inference.", "> @steel3d Any updates/ workarounds you managed to find? Running into a similar situation as you, with a Tesla T4, Ubuntu 18.04.5 LTS, on AWS.\r\n> Built tensorflow from scratch for `sm 7.5`, which helped, but it still takes more time on the first run compared to the successive ones. If it's any help, the process in question that the time difference is for involves loading some saved model protobufs as well.\r\n\r\nwhat's sm means?", "> @SujanSharma07 can you be more specific? You did nothing else, didn't recompile, you just added os.environ['TF_CPP_MIN_LOG_LEVEL']='2'?\r\n> Note that in my case, my first run was very slow, but subsequent runs were fast. Are you sure you'd get slow startup over and over without your change?\r\n> If it works, what's the justification? Where did you get the idea?\r\n\r\nDid you solve it? I got the same situation as yours.\r\nThe first computation is very slow, and the subsequent process work fine.", "> \u6211\u7684\u4ee3\u7801\u505c\u5728\u4ee5\u4e0b\u70b9\uff1a\r\n> \r\n> ```\r\n> 2020-08-10 22:16:25.588882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\r\n> 2020-08-10 22:16:26.719130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020-08-10 22:16:26.719170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 1 2 3\r\n> 2020-08-10 22:16:26.719178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N Y N N\r\n> 2020-08-10 22:16:26.719181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1: Y N N N\r\n> 2020-08-10 22:16:26.719185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2: N N N Y\r\n> 2020-08-10 22:16:26.719189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3: N N Y N\r\n> 2020-08-10 22:16:26.719882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10409 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n> 2020-08-10 22:16:26.720283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10409 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n> 2020-08-10 22:16:26.720567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10409 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n> 2020-08-10 22:16:26.720884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10409 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\n> ```\r\n> \r\n> \u5b83\u5361\u4f4f\u4e86 20 \u591a\u5206\u949f\uff0c\u6709\u4eba\u53ef\u4ee5\u5e2e\u6211\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u6216\u77e5\u9053\u539f\u56e0\u5417\uff1f\r\n\r\nSo is mine.Have you solved it?", "> After upgrading to 2.5.0 I experience this on one of my machines.\r\n> \r\n> All machines on Ubuntu 16.04, which use GeForce 1080 GTX cards, the trainings starts immediately.\r\n> \r\n> However, on Ubuntu 18.04 which uses two Titan RTX cards I have to wait a few minutes after the last line:\r\n> \r\n> ```\r\n> ...\r\n> 2021-06-18 10:54:01.545228: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n> 2021-06-18 10:54:01.547936: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n> 2021-06-18 10:54:01.548037: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n> 2021-06-18 10:54:01.548949: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n> 2021-06-18 10:54:01.549216: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n> 2021-06-18 10:54:01.550131: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\r\n> 2021-06-18 10:54:01.550900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n> 2021-06-18 10:54:01.551052: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n> 2021-06-18 10:54:01.554267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\r\n> ```\r\n> \r\n> I did not experience this with 2.4.1\r\n> \r\n> ### Versions\r\n> * Python: 3.9.5\r\n> * Tensorflow: 2.5.0\r\n> * CUDA: 11.2\r\n> * NVIDIA driver: 465.19.01\r\n\r\nHi @stefan-falk Were you able to solve this problem? thanks", "Same problem.\r\n\r\nPython: 3.7\r\nTensorflow: 2.0\r\nCUDA: 11.4\r\nNVIDIA driver: 470.103.01 ", "@AnishKumarNayak I'm now on 2.6.0 - the issue is gone but I don't know if it was the Tensorflow version or something else. "]}, {"number": 18651, "title": "tf.keras: Warn user when they mix up `sparse_categorical_crossentropy` and `categorical_crossentropy`", "body": "Sometimes we convert one-hot labels to integer labels, and forget to change the loss function `categorical_crossentropy` to `sparse_categorical_crossentropy`.\r\n\r\nBecause `categorical_crossentropy` doesn't check the shape of `target` and `output`, tf.keras works well except a totally wrong result.\r\n\r\nThe PR only checks the last dimension of static shape for efficient, perhaps we'd better to check whole shape with help of `tf.contrib.framework.with_same_shape`?", "comments": ["@fchollet Would you like to take a look? Thank you.", "@fchollet Yes, `training_utils.check_loss_and_target_compatibility()` contains the same checks. Thank you for pointing out this.\r\nBy the way, I cannot figure out where the function has been used in `tf.keras`, so does we expect users do the check by themselves explicitly?"]}, {"number": 18650, "title": "Add shape check to TextLineDataset op", "body": "This PR is to add shape check to TextLineDataset op to validate `filenames` is a scalar or a vector.\r\n\r\nThe inputs of TFRecordDataset have the requirements for shapes.\r\nHowever, the check was not done in the shape function. This fix adds shape checks whenever possible.", "comments": []}, {"number": 18649, "title": "Feature Request: Backprop through the transformation matrix in tf.contrib.image.transform and tf.contrib.image.rotate", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary, cpu\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7\r\n- **Have I written custom code**: No\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: GTX 730M but not in use\r\n- **Exact command to reproduce**: N/A\r\n\r\nHello Guys,\r\n\r\ntoday I met a problem while using tf.contrib.image.rotate and tf.contrib.image.translate in TensorFlow. I want to train a Convolutional Network to estimate the rotation and translation between two 2D Laserscan images as Input (estimate some kind of odometry).\r\n`network = NeuralTransformEstimator()`\r\n`xy, alpha = tf.split(network.logits, [2, 1], 1)`\r\n`img1, img2 = tf.split(network.inputs, [1, 1], 3)`\r\n`alphas = tf.squeeze(alpha, axis=1)`\r\n`rotated = tf.contrib.image.rotate(img1, alphas, interpolation='NEAREST')`\r\n`translated = tf.contrib.image.translate(rotated, xy, interpolation='NEAREST')`\r\n`loss_function = tf.reduce_mean(tf.square(img2-translated))`\r\n\r\nThe Network outputs three values(x,y, alpha) and the input is two images. When I use BackProp I get this error message:\r\n\r\n> ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables\r\n\r\nSo I created a more lightweight approach to check if I did something wrong elsewhere but it seems that translate and rotate do not provide gradients.\r\nIs there a chance that this feature will be added?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes, I need this feature for my master thesis. It would be great if this issue could be resolved.\n\nVon meinem iPhone gesendet\n\n> Am 04.05.2018 um 20:43 schrieb Alfred Sorten Wolf <notifications@github.com>:\n> \n> It has been 15 days with no activity and the awaiting response label was assigned. Is this still an issue?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "@ringw you've done a lot of work on tf.contrib.image. Is this feature request something you find appealing?", "The gradient is implemented for [the image pixels but not the transformation](https://github.com/tensorflow/tensorflow/blob/daecc72653e81d3c4592ad4ab246275a2cbd2712/tensorflow/contrib/image/python/ops/image_ops.py#L366). I haven't really developed or even directly used TF gradients yet, but I can take a look when I get a chance. This might be significantly more complex than the original image gradient, not sure yet.", "Nagging Assignee @ringw: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ringw: It has been 36 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ringw: It has been 51 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ringw: It has been 66 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ringw: It has been 81 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ringw: It has been 96 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delay. Closing because I think this would require another C++ kernel. The gradient for bilinear interpolation can't be calculated using the existing projective transformation, because the interpolation would also depend on the gradient of the transformation matrix. I don't think we can support a second C++ kernel for this particular case.\r\n\r\nI think this use case is something like [optical flow](https://en.wikipedia.org/wiki/Optical_flow) between two frames. However, I'm not sure if gradient descent on the transformation matrix will work as well as some of the methods mentioned in the article. It also might be out of scope for TensorFlow, where the purpose of image processing is mostly for data augmentation for a classifier, and this kind of computer vision is better handled by OpenCV. ", "Hi, is there a way that the feature for computing gradients through the translation in `tf.contrib.image.translate` could be added? It does not involve any interpolation like in `tf.contrib.image.transform` or `tf.contrib.image.rotate`."]}, {"number": 18648, "title": "eager scatter_nd forward works with incorrect code", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\ncode, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nOSX 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: \r\n3.6.1\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nsee code below\r\n\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\ntfe.enable_eager_execution()\r\n\r\nx_var = tfe.Variable(tf.random_uniform([10]))\r\ny_var = tfe.Variable(tf.random_uniform([10]))\r\n\r\nwith tfe.GradientTape() as tape:\r\n    dot = tf.scatter_nd(indices=[0],  # this should be indices=[[0]]\r\n                        updates=[tf.einsum('i,i->', x_var, y_var)],\r\n                        shape=[1])\r\nprint(dot)\r\ngradient = tape.gradient(dot, [x_var, y_var])\r\nprint(gradient)\r\n```\r\n\r\nError traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/m/workspace/bug/experimental/bug.py\", line 31, in <module>\r\n    gradient = tape.gradient(dot, [x_var, y_var])\r\n  File \"/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 764, in gradient\r\n    output_gradients=output_gradients)\r\n  File \"/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 65, in imperative_grad\r\n    tape._tape, vspace, target, sources, output_gradients, status)  # pylint: disable=protected-access\r\n  File \"/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 141, in grad_fn\r\n    op_inputs, op_outputs, orig_outputs)\r\n  File \"/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 109, in _magic_gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py\", line 39, in _PackGrad\r\n    return array_ops.unstack(grad, num=op.get_attr(\"N\"), axis=op.get_attr(\"axis\"))\r\n  File \"/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1084, in unstack\r\n    return gen_array_ops.unpack(value, num=num, axis=axis, name=name)\r\n  File \"/Users/m/workspace/bug/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8741, in unpack\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: axis = 0 not in [0, 0) [Op:Unpack] name: unstack\r\n```\r\n\r\n### Describe the problem\r\n\r\nThis is erroneous code which runs half-way. It will still calculate the forward pass, but fail on the backward pass. This does not happen with static graph tensorflow, where you get a correct error that tensor shapes are not matching.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Checked tf1.8, it's still there."]}]