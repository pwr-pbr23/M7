[{"number": 8603, "title": "fix windows gpu build", "body": "copy cuda_runtime_api.h into build dir to fix gpu build on windows", "comments": ["Can one of the admins verify this patch?", "jenkins, test this please.\r\n\r\n@guschmue , do you need me to also run a windows gpu presubmit?", "For this PR? Can not hurt of course but I had our Jenkins build it so I'm sure its good.."]}, {"number": 8602, "title": "Multiple simultaneous distributed-TF runs on single machine", "body": "I am running multiple distributed-TF sessions simultaneously on a single machine. I am setting up the configuration as described below. However, I am not getting the expected parallelism speedup.\r\n\r\nI am running 4 different runs simultaneously, each of them with following config:\r\n```\r\n# Run - 1\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:5000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:5001\",\r\n\t\t\"localhost:5002\",\r\n\t\t\"localhost:5003\"],\r\n\t})\r\n\r\n# Run - 2\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:6000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:6001\",\r\n\t\t\"localhost:6002\",\r\n\t\t\"localhost:6003\"],\r\n\t})\r\n\r\n# Run - 3\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:7000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:7001\",\r\n\t\t\"localhost:7002\",\r\n\t\t\"localhost:7003\"],\r\n\t})\r\n\r\n# Run - 4\r\ntf.train.ClusterSpec({\r\n\t\"ps\": [\"localhost:8000\"]\r\n\t\"worker\": [\r\n\t\t\"localhost:8001\",\r\n\t\t\"localhost:8002\",\r\n\t\t\"localhost:8003\"],\r\n\t})\r\n```\r\n\r\nThe command that I use to setup each server looks **exactly** like this for each worker and parameter server respectively:\r\n```\r\n# worker: index varies from 0 to 2\r\nserver = tf.train.Server(clusterSpec, job_name=\"worker\", task_index=0,\r\n                                 config=tf.ConfigProto(intra_op_parallelism_threads=1,\r\n                                 inter_op_parallelism_threads=2))\r\n\r\n### ps\r\nserver = tf.train.Server(clusterSpec, job_name=\"ps\", task_index=0,\r\n                                 config=tf.ConfigProto(device_filters=[\"/job:ps\"]))\r\n```\r\n\r\nWhile setting up the network variables, the device is setup **exactly** like this for all runs:\r\n```\r\nps_device = \"/job:ps\"\r\nworker_device = \"/job:worker/task:0/cpu:0\"\r\nworker_device = \"/job:worker/task:1/cpu:0\"\r\nworker_device = \"/job:worker/task:2/cpu:0\"\r\n```\r\n\r\nThere are `80` cores and enough memory. Each job has `3 workers and 1 ps`. In some fixed amount of time, I get following performance:\r\n```\r\n1 run on machine: x iterations\r\n4 runs on machine: approx. x/2 iterations\r\n6 runs on machine: approx. x/3 iterations\r\n```\r\n\r\nThe number of cores are enough such that I would expect 4 simultaneous runs to go as fast as single run, but it is no where close. Is there some mistake in my usage of the distributed-tf api ? I suspect there might be issue in the way I am using cpu device for worker: `cpu:0`. But as I read from docs, it seems to mention that `cpu:0` would do automatic scheduling. Does this hold even if I am running 4 different runs of distributed-tf on single machine? Should I be setting up `cpu:1 , cpu:2, cpu:3, cpu:4` for workers of 4 different runs ? Any help would be greatly appreciated. \r\n\r\nThanks in advance !!", "comments": ["Hi @pathak22 ,\r\n\r\nWhat is your end goal?  If you are training a single model on a single machine there is usually no need to run distributed, which creates more to keep track of.  If I am reading your config correctly you are running 12 workers + 4 ps_servers simultaneously on one server.  If I made a random guess, you are getting contention for resources.  TF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other and the OS at that point owns the scheduling.  \r\n\r\nSome questions that might help get someone give you a better answer\r\n- Which model are you training?\r\n- What CPU are you using?  I did not research but to get 80 cores seem like a possibly but not necessarily exotic setup.  \r\n- What was the CPU utilization when running a single setup, then 2, then 3 and then 4?  No need to report it in this issue if you do not want but I would check that.  \r\n- What performance do you get with nothing distributed and what is the CPU utilization?  \r\n\r\nIf you are trying to get the most out of your single server.  I would start with nothing distributed and assuming the code is fully optimized and I thought the server was not fully utilized I would try a distributed setup.  I have not personally setup this situation and others might have more or better advice  \r\n\r\nSample code to run would also be useful.  I am not guaranteeing someone will try it.  Your details are good but there are a lot of variables and unknowns.  \r\n", "Thanks @tfboyd for your comment. Sorry for the confusion, but no I am not trying to run single model. I am trying to run _4 completely different programs on same machine_. These four programs have no relation to each other. Each program has 3 workers and 1 parameter server of its own, all 3 workers update their unique parameter server asynchronously using hogwild SGD. This is what I meant when I say `Run` in my main comment above, sorry if it was not clear. To concretize the model, I am training 4 A3C models on 4 different tasks. Each A3C has 3 workers.\r\n\r\nActually, I think you have pin-pointed my question: \r\n> TF will handle scheduling for CPU:0 but multiple tensorflow processes are not going to be aware of each other\r\n\r\nI have two questions:\r\n1) Does different worker processes in distributed-TF handle parallelization if I just say `CPU:0` in all of them ? I would believe they should, given the documentation, because TF knows about different worker processes via the cluster spec.\r\n2) If answer to 1) is yes, then what happens when I run multiple independent programs (or \"runs\") in which each program is \"a distributed-TF session with multiple worker processes\" ? Should I still use `CPU:0` in all of them ? \r\n\r\nI would believe TF won't handle the second case, because each distributed-TF run does not know about other programs being running. So, is there a way to define affinity for different processes for different cpu cores ? For example:\r\n```\r\nps_device = \"/job:ps\"\r\nworker_device = \"/job:worker/task:0/cpu:2\"\r\nworker_device = \"/job:worker/task:1/cpu:4\"\r\nworker_device = \"/job:worker/task:2/cpu:6\"\r\n```\r\nCan I do something like above, i.e., tag different cpu devices ?\r\n\r\nMy question is more of a usage question and let me know if this is not the right place to ask. But tensor-flow docs talk only about `cpu:0` and do not talk about assigning affinity for each cpu core e.g. `cpu:1, cpu:2` .. similar to what TF has for gpu. Are these supported ? If yes, what is the right usage. \r\n\r\nI found one answer by @mrry has some answer here regarding this: http://stackoverflow.com/a/37864489 , but don't know the usage in context of distributed-TF.", "I would not do this distributed. Unless I am missing something, I would run 4 different Tensorflow instances targeted at CPU:0, one for each model you are training.  If that performance looks poor you could go back and compare running multiple workers with a ps_server on a single server vs 1 instance not distributed.   If you think the performance is not expected, then you can compare (if you wanted) 1 instance vs. a distributed setup.  I would be interested in the results but I suspect for a normal 2 socket XEON it would not be better to run distributed on a single server.  ", "I might be off here, but maybe you should look into using a cluster manager like Mesos, Kubernetes, etc. Even though you're only running on a single node, I think these should still help you schedule and run your separate models on different CPUs. Or I believe you can manually achieve something similar with multiple docker containers, but using a framework is probably easier.", "I don't think this is a TF bug. This is an issue with user setup, so should be asked in Stackoverflow. I will close this issue. I also want to clarify a few things.\r\n\r\nTF will only see `CPU:0`. CPU:x where x is not 0 will give you errors in your code.\r\nI do not understand what you mean by handling parallelization. Operating system manages parallel processes running on CPU. If you want one process to be reserved to run on a specific core, you will need\r\n\r\nIt does not make sense to run distributed TF on a single machine, and expect good performance out of it. On the same machine, a single process training will always be faster, because distributed TF always has the RPC communication overheads, even if we do not consider any other overhead of distributed training.\r\nIn the end, on the same machine distributed TF and single process TF has access to the same \"pool\" of resources, and distributed TF needs to do more things than just training, so it is slower."]}, {"number": 8601, "title": "Import the Ops Research tools in TensorFlow as well as their dependencies", "body": "", "comments": ["Looks like windows build flaked.\r\njenkins, test this please.", "I will approve this now to unblock work.\r\nBut we should explore how we can dedupe proto libraries @jart for help on that\r\n@vrv about gflags addition. It is a dependency of a dependency, but I think we would like to avoid using it, right?", "Jenkins, test this please.", "If needs be, we can always ask the ops research guys to use another library to manage their flags."]}, {"number": 8600, "title": "update_version to 1.1.0", "body": "Making the update_version script for 1.1 release.", "comments": ["Jenkins, test this please.", "We need to build 1.1.0-rc0 first.\r\nCould you rerun update_version script?"]}, {"number": 8599, "title": "fix documentation of tf.switch outputs", "body": "", "comments": ["Can one of the admins verify this patch?", "Note the order in switch is reverse to that of `tf.cond`,\r\n\r\nYou have\r\n`tf.cond(cond, condition_true, condition_false)`\r\n\r\nbut\r\n`condition_false, condition_true = tf.switch(cond, data)`", "Jenkins, test this please."]}, {"number": 8598, "title": "docfix, reverse switch outputs", "body": "", "comments": ["Can one of the admins verify this patch?", "just learned that github edit feature creates new branch on official repo, closing this to make branch on my own cloned version and make PR from there"]}, {"number": 8597, "title": "fix description of tf.switch outputs", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8596, "title": "tf.nn.conv3d ignoring first spacial dimension", "body": "Im using tensorflow 1.0.0 (python 3.5) on OSX(10.11.6 El Capitan), but the same problem occurred with tf version 1.0.1. also got this error when I tried to run the module with the google ml-engine.\r\n\r\n# **Relevant code**\r\n\r\n\r\n\r\n````\r\ndef conv3d(inputs,weights, biases,layer_name,act=tf.nn.relu,padding='VALID'):\r\n    with tf.name_scope(layer_name):\r\n        with tf.name_scope('weights'):\r\n            tf.add_to_collection(tf.GraphKeys.WEIGHTS, weights)\r\n            variable_summaries(weights)\r\n        with tf.name_scope('biases'):\r\n            variable_summaries(biases)\r\n        with tf.name_scope('convWx_plus_b'):\r\n            preactivate = tf.nn.conv3d(inputs,weights,strides=[1,1,1,1,1],padding=padding) + biases\r\n            #preactivate = tf.nn.convolution(inputs,weights,padding,data_format='NDHWC')\r\n            tf.summary.histogram('preactivate', preactivate)\r\n            activation = act(preactivate) \r\n            tf.summary.histogram('activation', activation)\r\n    return activation\r\n````        \r\n\r\n```\r\ndef weight_variable(shape,dtype=np.float32,partition_info=None):\r\n    shape[shape==None] = 1\r\n    n = np.prod(shape)\r\n    w = (np.random.randn(n) * np.sqrt(2./n)).astype(np.float32)\r\n    return tf.Variable(w.reshape(shape),trainable=True)\r\n```\r\n\r\n```\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape=shape)\r\n    return tf.Variable(initial,trainable=True)\r\n\r\n```\r\n\r\n\r\n```\r\ndef maxpool(inputs,padding='VALID'):\r\n    return tf.nn.max_pool3d(inputs,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding=padding)\r\n```\r\n\r\n```\r\ndef ll_model(inputs):\r\n    \r\n    input_layer = tf.reshape(inputs,[-1,65,65,65,2])\r\n    \r\n    W_conv1 = weight_variable([3,3,3,2,16])\r\n    b_conv1 = bias_variable([16])\r\n    conv1 = conv3d(input_layer,W_conv1,b_conv1,'conv1')\r\n\r\n    print(conv1.get_shape().as_list())\r\n\r\n    pad = tf.pad(conv1,[[0,0],[1,0],[1,0],[1,0],[0,0]],mode='CONSTANT')\r\n    print(pad.get_shape().as_list())\r\n    maxpool1 = maxpool(pad)\r\n    print(maxpool1.get_shape().as_list())\r\n\r\n    W_conv2 = weight_variable([3,3,3,16,24])\r\n    b_conv2 = bias_variable([24])\r\n    conv2 = conv3d(maxpool1,W_conv2, b_conv2,'conv2',padding=\"SAME\")\r\n    print(conv2.get_shape().as_list())\r\n    W_conv3 = weight_variable([3,3,3,24,28])\r\n    b_conv3 = bias_variable([28])\r\n    conv3 = conv3d(conv2,W_conv3,b_conv3,'conv3',padding=\"SAME\")\r\n    print(conv3.get_shape().as_list())\r\n    W_conv4 = weight_variable([4,4,4,28,34])\r\n    b_conv4 = bias_variable([34])\r\n    conv4 = conv3d(conv3,W_conv4,b_conv4,'conv4')\r\n    print(conv4.get_shape().as_list())\r\n    W_conv5 = weight_variable([5,5,5,34,42])\r\n    b_conv5 = bias_variable([42])\r\n    conv5 = conv3d(conv4,W_conv5,b_conv5,'conv5')\r\n    print(conv5.get_shape().as_list())\r\n    W_conv6 = weight_variable([5,5,5,42,50])\r\n    b_conv6 = bias_variable([50])\r\n    conv6 = conv3d(conv5,W_conv6,b_conv6,'conv6')\r\n    print(conv6.get_shape().as_list())\r\n    W_conv7 = weight_variable([5,5,5,50,50])\r\n    b_conv7 = bias_variable([50])\r\n    conv7 = conv3d(conv6,W_conv7,b_conv7,'conv7')\r\n    print(conv7.get_shape().as_list())\r\n    W_conv8 = weight_variable([1,1,1,50,2])\r\n    b_conv8 = bias_variable([2])\r\n    conv8 = conv3d(conv7,W_conv8, b_conv8,'output',act=depth_softmax)\r\n    \r\n    return conv8\r\n```\r\n\r\n\r\nthe error happens when compiling the model. conv8 is supposed to have the shape [None,17,17,17,2].\r\n\r\n# The Error:\r\n\r\n```\r\npython task.py                                                                       \r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 ins\r\ntructions, but these are available on your machine and could speed up CPU computations.                     \r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 ins\r\ntructions, but these are available on your machine and could speed up CPU computations.                     \r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instru\r\nctions, but these are available on your machine and could speed up CPU computations.                        \r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instr\r\nuctions, but these are available on your machine and could speed up CPU computations.                       \r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instru\r\nctions, but these are available on your machine and could speed up CPU computations.                        \r\n[None, 65, 63, 63, 16]                                                                                      \r\n[None, 66, 64, 64, 16]                                                                                      \r\n[None, 33, 32, 32, 16]                                                                                      \r\n[None, 33, 32, 32, 24]                                                                                      \r\n[None, 33, 32, 32, 28]                                                                                      \r\n[None, 33, 29, 29, 34]                                                                                      \r\n[None, 33, 25, 25, 42]                                                                                      \r\n[None, 33, 21, 21, 50]                                                                                      \r\n[None, 33, 17, 17, 50]                                                                                      \r\nTraceback (most recent call last):                                                                          \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\ncommon_shapes.py\", line 670, in _call_cpp_shape_fn_impl                                                     \r\n    status)                                                                                                 \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/contextlib.py\", line 66, in __exit__      \r\n    next(self.gen)                                                                                          \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\nerrors_impl.py\", line 469, in raise_exception_on_not_ok_status                                              \r\n    pywrap_tensorflow.TF_GetCode(status))                                                                   \r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 17 and 33 fo\r\nr 'SquaredDifference' (op: 'SquaredDifference') with input shapes: [?,17,17,17,2], [?,33,17,17,2].          \r\n                                                                                                            \r\nDuring handling of the above exception, another exception occurred:                                         \r\n                                                                                                            \r\nTraceback (most recent call last):                                                                          \r\n  File \"task.py\", line 33, in <module>                                                                      \r\n    loss = mean_square_error(y,ll_model(x))                                                                 \r\n  File \"/Users/vhasfclanga/tflow_trainer/trainer/functions.py\", line 80, in mean_square_error               \r\n    return tf.reduce_sum(tf.squared_difference(a,b)) / N                                                    \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gen_ma\r\nth_ops.py\", line 2754, in squared_difference                                                                \r\n    result = _op_def_lib.apply_op(\"SquaredDifference\", x=x, y=y, name=name)                                 \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\nop_def_library.py\", line 763, in apply_op                                                                   \r\n    op_def=op_def)                                                                                          \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\nops.py\", line 2397, in create_op                                                                            \r\n    set_shapes_for_outputs(ret)                                                                             \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\nops.py\", line 1757, in set_shapes_for_outputs                                                               \r\n    shapes = shape_func(op)                                                                                 \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\nops.py\", line 1707, in call_with_requiring                                                                  \r\n    return call_cpp_shape_fn(op, require_shape_fn=True)                                                     \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\ncommon_shapes.py\", line 610, in call_cpp_shape_fn                                                           \r\n    debug_python_shape_fn, require_shape_fn)                                                                \r\n  File \"/Users/vhasfclanga/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/\r\ncommon_shapes.py\", line 675, in _call_cpp_shape_fn_impl                                                     \r\n    raise ValueError(err.message)                                                                           \r\nValueError: Dimensions must be equal, but are 17 and 33 for 'SquaredDifference' (op: 'SquaredDifference') wi\r\nth input shapes: [?,17,17,17,2], [?,33,17,17,2].\r\n```\r\n\r\nAs you can see, these are the shapes of the tensors resulting from each convolutional layer.\r\n\r\n[None, 65, 63, 63, 16]                                                                                      \r\n[None, 66, 64, 64, 16]                                                                                      \r\n[None, 33, 32, 32, 16]                                                                                      \r\n[None, 33, 32, 32, 24]                                                                                      \r\n[None, 33, 32, 32, 28]                                                                                      \r\n[None, 33, 29, 29, 34]                                                                                      \r\n[None, 33, 25, 25, 42]                                                                                      \r\n[None, 33, 21, 21, 50]                                                                                      \r\n[None, 33, 17, 17, 50]     \r\n\r\n\r\nNotice the first spacial dimension is only effected by pooling and padding layers, and is totally ignored by convolutional layers. It's strange to me because everything should be symmetric across spatial dimensions.\r\n\r\nI've tried using tf.nn.convolution as seen in my conv3d wrapper, that yielded the same result. I've tried switching up the padding, that also didn't work. I tried using the higher level functions in tf.layers to construct the model, that also didn't work. The fact that none of these methods worked makes me think this must be a programming error on my part, but the error is coming from a simple propagation of tensor shapes, starting with placeholders\r\n\r\n```\r\nwith tf.name_scope('inputs'):\r\n    x = tf.placeholder(tf.float32,shape=[None,65,65,65,2],name='features')\r\n    tf.summary.histogram('feature-hist', x)\r\nwith tf.name_scope('ground_truth'):\r\n    y = tf.placeholder(tf.float32,shape=[None,17,17,17,2],name='targets')\r\n    tf.summary.histogram('target-hist', y)\r\n\r\n```\r\nSo I'm not sure where I could have possibly gone wrong.\r\n\r\nAlso, this exact model structure resulted in the correct output shape when used with the Estimator + ModelFnOps API.\r\n\r\nthe error can be reproduced by using one of the built in loss functions and running\r\n\r\n`tf.some_loss_function(y,ll_model(x))`\r\n\r\nthe function I was using was \r\n```\r\ndef mean_square_error(a,b):\r\n    shape = a.get_shape().as_list()\r\n    shape[shape==None] = 1\r\n    N = np.prod(shape)\r\n    return tf.reduce_sum(tf.squared_difference(a,b)) / N\r\n\r\n```\r\nDoes anyone know if this is a bug or programming error on my part?\r\n\r\nThanks in advance,\r\nAlex", "comments": ["Hi @alexanderalang, can you compile the code you have above into a single script that reproduces the error? I tried pasting your snippets into a script but am getting undefined methods (e.g. variable_summaries, depth_softmax). It would also be extremely useful if you could try to simplify the code while still producing the error. Thanks.", "here is an error producing script, I tried to simplify it a bit\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef conv3d(inputs,weights, biases,layer_name,act=tf.nn.relu,padding='VALID'):\r\n    preactivate = tf.nn.conv3d(inputs,weights,strides=[1,1,1,1,1],padding=padding) + biases\r\n    activation = act(preactivate) \r\n    return activation        \r\n\r\ndef maxpool(inputs,padding='VALID'):\r\n    return tf.nn.max_pool3d(inputs,ksize=[1,2,2,2,1],strides=[1,2,2,2,1],padding=padding)\r\n\r\ndef weight_variable(shape,dtype=np.float32,partition_info=None):\r\n    shape[shape==None] = 1\r\n    n = np.prod(shape)\r\n    w = (np.random.randn(n) * np.sqrt(2./n)).astype(np.float32)\r\n    return tf.Variable(w.reshape(shape),trainable=True)\r\n\r\n## initializes biases\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape=shape)\r\n    return tf.Variable(initial,trainable=True)\r\n\r\ndef mean_square_error(a,b):\r\n    shape = a.get_shape().as_list()\r\n    shape[shape==None] = 1\r\n    N = np.prod(shape)\r\n    return tf.reduce_sum(tf.squared_difference(a,b)) / N\r\n\r\n\r\n###  low level api ################################ \r\ndef ll_model(inputs):\r\n    \r\n    input_layer = tf.reshape(inputs,[-1,65,65,65,2])\r\n    \r\n    W_conv1 = weight_variable([3,3,3,2,16])\r\n    b_conv1 = bias_variable([16])\r\n    conv1 = conv3d(input_layer,W_conv1,b_conv1,'conv1')\r\n\r\n    print(conv1.get_shape().as_list())\r\n\r\n    pad = tf.pad(conv1,[[0,0],[1,0],[1,0],[1,0],[0,0]],mode='CONSTANT')\r\n    print(pad.get_shape().as_list())\r\n    \r\n    maxpool1 = maxpool(pad)\r\n    print(maxpool1.get_shape().as_list())\r\n\r\n    W_conv2 = weight_variable([3,3,3,16,24])\r\n    b_conv2 = bias_variable([24])\r\n    conv2 = conv3d(maxpool1,W_conv2, b_conv2,'conv2',padding=\"SAME\")\r\n    print(conv2.get_shape().as_list())\r\n    \r\n    W_conv3 = weight_variable([3,3,3,24,28])\r\n    b_conv3 = bias_variable([28])\r\n    conv3 = conv3d(conv2,W_conv3,b_conv3,'conv3',padding=\"SAME\")\r\n    print(conv3.get_shape().as_list())\r\n    \r\n    W_conv4 = weight_variable([4,4,4,28,34])\r\n    b_conv4 = bias_variable([34])\r\n    conv4 = conv3d(conv3,W_conv4,b_conv4,'conv4')\r\n    print(conv4.get_shape().as_list())\r\n    \r\n    W_conv5 = weight_variable([5,5,5,34,42])\r\n    b_conv5 = bias_variable([42])\r\n    conv5 = conv3d(conv4,W_conv5,b_conv5,'conv5')\r\n    print(conv5.get_shape().as_list())\r\n    \r\n    W_conv6 = weight_variable([5,5,5,42,50])\r\n    b_conv6 = bias_variable([50])\r\n    conv6 = conv3d(conv5,W_conv6,b_conv6,'conv6')\r\n    print(conv6.get_shape().as_list())\r\n    \r\n    W_conv7 = weight_variable([5,5,5,50,50])\r\n    b_conv7 = bias_variable([50])\r\n    conv7 = conv3d(conv6,W_conv7,b_conv7,'conv7')\r\n    print(conv7.get_shape().as_list())\r\n    \r\n    W_conv8 = weight_variable([1,1,1,50,2])\r\n    b_conv8 = bias_variable([2])\r\n    conv8 = conv3d(conv7,W_conv8, b_conv8,'output')\r\n    \r\n    return conv8\r\n\r\n\r\nsess = tf.Session()    \r\n## placeholders\r\nx = tf.placeholder(tf.float32,shape=[None,65,65,65,2],name='features')\r\n\r\n\r\ny = tf.placeholder(tf.float32,shape=[None,17,17,17,2],name='targets')\r\n\r\nloss = mean_square_error(y,ll_model(x))\r\n```", "Thanks, this script is very handy! It does appear to be a bug, but I'm not familiar enough with conv3d to diagnose. I've reached out to some people who I think should be able to help (I don't know their github handles).", "@alexanderalang: shape[shape==None] = 1 causes the first dimension of your kernels to be set to '1', which explains why you don't see a reduction in the 'z' dimension of the conv3d output.", "@mjanusz welp, I played myself. \r\nThat line isn't even supposed to be there. I wonder why a normal python list, when indexed by a boolean returns the first element of the list instead of raising an error..\r\n\r\nanyways thanks for finding my silly error. case closed\r\n\r\n"]}, {"number": 8595, "title": "Update get_started.md", "body": "Typo in the tutorial file  get_started.md with initialize_all_variables() replacing global_variables_initializer()", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I have signed the CLA Agreement", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 8594, "title": "tf.case doesn't return a tensor with the given name", "body": "With TF 1.0.1.\r\n```python\r\n>> print(tf.case([(tf.constant(True, dtype=tf.bool), lambda: tf.constant(1))], lambda: tf.constant(2), name='aa'))\r\nTensor(\"aa/If_1/Merge:0\", shape=(), dtype=int32)\r\n```\r\nI expect to see \"aa:0\".\r\n\r\nSimilar to #6741 ", "comments": ["I'll work on getting a patch in to fix this.", "@skye Did this fall through the cracks?", "Hi, sorry for leaving this hanging for so long. After some internal discussion we decided that to keep the current behavior, where \"aa\" refers to the enclosing name scope rather than the final output op. This is to be consistent with other functions that produce multiple ops, and since it makes TensorBoard output more useful.", "I think using \"aa\" for the output op doesn't conflict with using an internal name scope to organize tensorboard graph (e.g. the scope can be named \"aa_internal\").\r\n\r\nUsing the name for output op is actually consistent with most other `tf.xxx` functions. At least from the documentation of all `tf.xxx` functions, we cannot tell whether \"name\" will be used for the op or the scope."]}, {"number": 8593, "title": "dtype of methods in tensorflow.contrib.distributions does not depend on self.dtype", "body": "In master (f4a0c2c) (and versions such as r1.0), the docstring in `log_prob()` claims to return a tensor which has the same dtype as the distributions' dtype. This is not true for the built-in distributions. For example:\r\n```python\r\n>>> from tensorflow.contrib import distributions as ds\r\n>>>>\r\n>>> x = ds.Bernoulli(p=0.5)\r\n>>> x.dtype\r\ntf.int32\r\n>>> x.log_prob(1)\r\n<tf.Tensor 'Bernoulli_1/log_prob/Neg:0' shape=() dtype=float32>\r\n```\r\n\r\nThis inconsistency also occurs for many methods beyond `log_prob()`.", "comments": ["@ebrevdo @jvdillon @langmore is this just a problem in documentation?", "Perhaps it is a documentation problem.  the dtype of a distribution is the\ndtype of the *samples it emits *(the dtype of dist.sample(...)).  log_prob\nis always going to depend on the dtypes of the logits / probabilities /\nwhatever you passed to the initializer of the distribution.\n\nOn Wed, Mar 22, 2017 at 9:48 AM, Skye Wanderman-Milne <\nnotifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> @jvdillon\n> <https://github.com/jvdillon> @langmore <https://github.com/langmore> is\n> this just a problem in documentation?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8593#issuecomment-288463155>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim23wxvlioFkvA5i2R2-qv7mtfVhzks5roVDZgaJpZM4MkPhT>\n> .\n>\n", "Oops, got it. [retracted comment]", "I don't know of any distributions where sample().dtype != dist.dtype.\n\nMost other methods should not be expected to match the dtype, because they\ninclude computations on the probability and the domain of the\ndistribution.  All statistics, log_prob, etc, require computations that mix\nthe domain values (of type dist.dtype) of the probabilities (usually one of\ntf.float{16,32,64}).\n\nOn Wed, Mar 22, 2017 at 11:04 AM, Dustin Tran <notifications@github.com>\nwrote:\n\n> In Bernoulli, sample() casts the output to self.dtype before returning\n> it. So it has the same dtype as the distribution's.\n>\n> >>> from tensorflow.contrib import distributions as ds>>>>>>> x = ds.Bernoulli(p=0.5)>>> x.dtype\n> tf.int32>>> x.log_prob(1)<tf.Tensor 'Bernoulli_1/log_prob/Neg:0' shape=() dtype=float32>>>> x.sample()<tf.Tensor 'Bernoulli/sample/Reshape:0' shape=() dtype=int32>\n>\n> Not sure about the others. Are there cases where the dtype returned by\n> sample() is different from the distribution's dtype?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8593#issuecomment-288487875>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzUCNuafEWMpdXFDleMtiORz9Bdmks5roWLIgaJpZM4MkPhT>\n> .\n>\n"]}, {"number": 8592, "title": "Windows Bazel Build: Cut dependency on broken targets", "body": "TensorFlow build with Bazel 0.4.5 is currently broken:\r\nhttp://ci.bazel.io/job/TensorFlow/BAZEL_VERSION=latest,PLATFORM_NAME=windows-x86_64/735/console\r\n\r\nIn e8aefd216930667ff0b278d24cf411b216a9c100, a dependency:\r\n```\r\n//tensorflow/tools/pip_package:build_pip_package\r\n//tensorflow/tools/pip_package:simple_console_for_windows\r\n//tensorflow/python/debug:debug_pip\r\n//tensorflow/python/debug:debug_examples\r\n//tensorflow/python/debug:debug_mnist\r\n//tensorflow/examples/tutorials/mnist:input_data\r\n//tensorflow/contrib/learn/python/learn/datasets:datasets\r\n//tensorflow/contrib/framework:framework_py\r\n//tensorflow/contrib/framework:python/ops/_variable_ops.so\r\n```\r\nwas introduced.\r\n\r\nCut this dependency by excluding :debug_examples on Windows\r\n\r\n@gunan ", "comments": ["@caisq Since we build windows distributables using cmake, this wont have any effect on the release. But it may fix bazel build on windows.", "@tensorflow-jenkins test this please"]}, {"number": 8591, "title": "TensorBoard fails grouping summaries by names", "body": "Hi. Using Tensorboard, I found that it fails grouping scalar summaries by its name.\r\n\r\nAs I know, summaries are grouped by their name scopes, which are delimited by slash('/').\r\nHowever, for some cases, the group to be bound together would be separated.\r\n\r\nHere's a reproducible code that causes the same error, and the resulting tensorboard page.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nif not os.path.exists('./temp'):\r\n    os.mkdir('./temp')\r\nsummary_writer = tf.summary.FileWriter('./temp', None)\r\n\r\nsummary = tf.Summary()\r\nsummary.value.add(tag='train/accuracy', simple_value=1.0)\r\nsummary.value.add(tag='train/cross_entropy', simple_value=1.0)\r\nsummary.value.add(tag='train/learning_rate', simple_value=1.0)\r\nsummary.value.add(tag='train_image/foo', simple_value=1.0)\r\n\r\nfor i in range(10):\r\n    summary_writer.add_summary(summary, i)\r\nsummary_writer.flush()\r\n```\r\n\r\n![image](https://cloud.githubusercontent.com/assets/13655756/24163367/b4673d48-0ead-11e7-8478-b0733581b90e.png)\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\nInstalled version of CUDA and cuDNN: No cuda related, I guess.\r\nTensorFlow version: Only confirmed in the nightly build, Linux-GPU version. But I think it will occur in other versions as well.\r\n\r\nThank you for your hard work. :^)", "comments": ["It seems that tag names are sorted ignoring non-alphabet characters (such as `_` and `/`) first and they are grouped according to the sort order :-)", "@dandelionmane I'm assigning to you for now, please reassign if necessary.", "This is fixed by 1968b8b3f9316a8327176e25211c8cc2b01459bc."]}, {"number": 8590, "title": "Comment referring to the wrong variable", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 8589, "title": "contrib.crf + XLA doesn't work", "body": "The combination of tf.contrib.crf + XLA seems not working (sometimes hang forever).\r\n\r\nFollowing code is for reproducing errors.\r\nThe code is from standard example - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf\r\nThe only difference is XLA part. \r\n\r\nWhen XLA is enabled, loss is not reducing.\r\nIn more large network(real NLP applications), it hangs forever after graph building. \r\n\r\nMy settings is \r\n- tensorflow 1.0.1\r\n- python 3.6 (anaconda 4.3.0 )\r\n- cuda 7.5 \r\n \r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Data settings.\r\nnum_examples = 10\r\nnum_words = 20\r\nnum_features = 100\r\nnum_tags = 5\r\n\r\n# Random features.\r\nx = np.random.rand(num_examples, num_words, num_features).astype(np.float32)\r\n\r\n# Random tag indices representing the gold sequence.\r\ny = np.random.randint(num_tags, size=[num_examples, num_words]).astype(np.int32)\r\n\r\n# All sequences in this example have the same length, but they can be variable in a real model.\r\nsequence_lengths = np.full(num_examples, num_words - 1, dtype=np.int32)\r\n\r\n\r\n# to enable XLA\r\ntf_config = tf.ConfigProto()\r\ntf_config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\n\r\n# Train and evaluate the model.\r\nwith tf.Graph().as_default():\r\n  with tf.Session(config=tf_config) as session:\r\n    # Add the data to the TensorFlow graph.\r\n    x_t = tf.constant(x)\r\n    y_t = tf.constant(y)\r\n    sequence_lengths_t = tf.constant(sequence_lengths)\r\n\r\n    # Compute unary scores from a linear layer.\r\n    weights = tf.get_variable(\"weights\", [num_features, num_tags])\r\n    matricized_x_t = tf.reshape(x_t, [-1, num_features])\r\n    matricized_unary_scores = tf.matmul(matricized_x_t, weights)\r\n    unary_scores = tf.reshape(matricized_unary_scores,\r\n                              [num_examples, num_words, num_tags])\r\n\r\n    # Compute the log-likelihood of the gold sequences and keep the transition\r\n    # params for inference at test time.\r\n    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\r\n        unary_scores, y_t, sequence_lengths_t)\r\n\r\n    # Add a training op to tune the parameters.\r\n    loss = tf.reduce_mean(-log_likelihood)\r\n    \r\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\r\n\r\n\r\n    # Train for a fixed number of iterations.\r\n    session.run(tf.global_variables_initializer())\r\n    for i in range(1000):\r\n      tf_unary_scores, tf_transition_params, _ = session.run(\r\n          [unary_scores, transition_params, train_op])\r\n      if i % 100 == 0:\r\n        correct_labels = 0\r\n        total_labels = 0\r\n        for tf_unary_scores_, y_, sequence_length_ in zip(tf_unary_scores, y,\r\n                                                          sequence_lengths):\r\n          # Remove padding from the scores and tag sequence.\r\n          tf_unary_scores_ = tf_unary_scores_[:sequence_length_]\r\n          y_ = y_[:sequence_length_]\r\n\r\n          # Compute the highest scoring sequence.\r\n          viterbi_sequence, _ = tf.contrib.crf.viterbi_decode(\r\n              tf_unary_scores_, tf_transition_params)\r\n\r\n          # Evaluate word-level accuracy.\r\n          correct_labels += np.sum(np.equal(viterbi_sequence, y_))\r\n          total_labels += sequence_length_\r\n        accuracy = 100.0 * correct_labels / float(total_labels)\r\n        print(\"Accuracy: %.2f%%\" % accuracy)\r\n```\r\n\r\nit generates following outputs\r\n```sh\r\n2017-03-22 00:40:01.110489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:06:00.0\r\nTotal memory: 11.25GiB\r\nFree memory: 11.16GiB\r\n2017-03-22 00:40:01.110631: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2b8c6d0\r\n2017-03-22 00:40:01.337723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:07:00.0\r\nTotal memory: 11.25GiB\r\nFree memory: 11.16GiB\r\n2017-03-22 00:40:01.337815: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2b90150\r\n2017-03-22 00:40:01.544460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 2 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:84:00.0\r\nTotal memory: 11.25GiB\r\nFree memory: 11.16GiB\r\n2017-03-22 00:40:01.544569: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x2b93e00\r\n2017-03-22 00:40:01.750146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 3 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:85:00.0\r\nTotal memory: 11.25GiB\r\nFree memory: 11.16GiB\r\n2017-03-22 00:40:01.750302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 2\r\n2017-03-22 00:40:01.750319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 3\r\n2017-03-22 00:40:01.750346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 2\r\n2017-03-22 00:40:01.750358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 3\r\n2017-03-22 00:40:01.750369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 0\r\n2017-03-22 00:40:01.750380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 1\r\n2017-03-22 00:40:01.750479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 0\r\n2017-03-22 00:40:01.750519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 1\r\n2017-03-22 00:40:01.750597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3\r\n2017-03-22 00:40:01.750608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y N N\r\n2017-03-22 00:40:01.750614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y N N\r\n2017-03-22 00:40:01.750619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   N N Y Y\r\n2017-03-22 00:40:01.750625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   N N Y Y\r\n2017-03-22 00:40:01.750643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)\r\n2017-03-22 00:40:01.750650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0)\r\n2017-03-22 00:40:01.750656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:84:00.0)\r\n2017-03-22 00:40:01.750662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:85:00.0)\r\n2017-03-22 00:40:03.347959: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 4 visible devices\r\n2017-03-22 00:40:03.347994: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 48 visible devices\r\n2017-03-22 00:40:03.358665: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4b47600 executing computations on platform Host. Devices:\r\n2017-03-22 00:40:03.358682: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-03-22 00:40:03.359012: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 4 visible devices\r\n2017-03-22 00:40:03.359026: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 48 visible devices\r\n2017-03-22 00:40:03.367439: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x4b6fcb0 executing computations on platform CUDA. Devices:\r\n2017-03-22 00:40:03.367459: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\r\n2017-03-22 00:40:03.367468: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7\r\n2017-03-22 00:40:03.367475: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (2): Tesla K80, Compute Capability 3.7\r\n2017-03-22 00:40:03.367482: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (3): Tesla K80, Compute Capability 3.7\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\nAccuracy: 15.26%\r\n```\r\n\r\n\r\n\r\n", "comments": ["As a side note, https://github.com/tensorflow/tensorflow/issues/7751#issuecomment-281770324 indicated that \"[tf.contrib.crf] appears to be an unmaintained contrib utility\".", "@tatatodd, do you want to investigate this to find corner cases in XLA? Otherwise, we do not support tf.contrib models as @Franck-Dernoncourt points out.\r\n", "Thanks for the pointer @aselle, and thanks for the clear bug report @hugman!\r\n\r\nIndeed we can't dedicate much time to supporting tf.contrib code, especially not with XLA at the moment.\r\n\r\nHowever I **am** interested in looking into this issue, since it may uncover weird bugs.  @hugman, can you provide a log of the same code **with XLA disabled**, to let me know what normal convergence looks like for you?\r\n\r\nI can't promise when I'll have time to take a look, but it'll be useful for a rainy day, just not today.  :)", "`tf.contrib.crf` was written and then abandoned by the author.  Will be removed shortly."]}, {"number": 8588, "title": "Got an unexpected keyword argument 'fix_global_step_increment_bug'", "body": "I get this error while trying to run the [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py).\r\n\r\n```\r\nTypeError: __init__() got an unexpected keyword argument 'fix_global_step_increment_bug'\r\n```\r\n\r\nIt appears to be coming from - [ea963dd048749df525d2a1f19c31d7abcdc3268e](https://github.com/tensorflow/tensorflow/commit/ea963dd048749df525d2a1f19c31d7abcdc3268e).\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: MacOS 10.11.5\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n1. A link to the pip package you installed: official python package tensorflow.\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI just ran this code - [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py)\r\n\r\n```\r\n ~$ python wide_n_deep_tutorial.py --model_type=wide_n_deep\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"wide_n_deep_tutorial.py\", line 234, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/sdua/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"wide_n_deep_tutorial.py\", line 197, in main\r\n    FLAGS.train_data, FLAGS.test_data)\r\n  File \"wide_n_deep_tutorial.py\", line 185, in train_and_eval\r\n    m = build_estimator(model_dir, model_type)\r\n  File \"wide_n_deep_tutorial.py\", line 132, in build_estimator\r\n    fix_global_step_increment_bug=True)\r\nTypeError: __init__() got an unexpected keyword argument 'fix_global_step_increment_bug'\r\n```", "comments": ["Same problem. I'm using python3, windows OS. version 1.0.1\r\nAfter that, I checked the help doc, I found that fix_global_step_increment_bug is not a init arg for class DNNLinearCombinedClassifier\r\n\r\n __init__(self, model_dir=None, n_classes=2, weight_column_name=None, linear_feature_columns=None, linear_optimizer=None, _joint_linear_weights=False, dnn_feature_columns=None, dnn_optimizer=None, dnn_hidden_units=None, dnn_activation_fn=<function relu at 0x00000000057B1840>, dnn_dropout=None, gradient_clip_norm=None, enable_centered_bias=False, config=None, feature_engineering_fn=None, embedding_lr_multipliers=None, input_layer_min_slice_size=None)\r\n\r\nSo, I think this a bug for wide_n_deep_tutorial.py cause there is no such init arg.\r\nWhat I did is just delete this arg, but after do that I got other exceptions. \r\nHope it will work for you, if just delete this arg option.\r\n\r\nAnd wait for developers' response.", "@lcytzk indeed, that's the problem.\r\n\r\nI realised that it's a new feature that they added, however not yet released. So, we have to use an older version of the tutorial until the time new version with that change is released.", "This tutorial has been updated since the 1.0.1 release, and it looks like you're trying to run the updated version from master using 1.0.1. Either upgrade to nightly or try https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/examples/learn/wide_n_deep_tutorial.py.\r\n\r\nI'm gonna close this issue for now, but please reopen if you continue to experience problems."]}, {"number": 8587, "title": "Android Example Doesn't Build: Could not get unknown property 'assembleRelease'", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttp://stackoverflow.com/questions/39602587/could-not-get-unknown-property-assemblerelease-for-project/39720618\r\nhttp://stackoverflow.com/questions/39590549/after-update-to-android-studio-2-2-gradle-plugin-2-2-0-could-not-get-unknown\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.10\r\nInstalled version of CUDA and cuDNN: \r\nToolkit 8.0.1 cuDNN 5.1\r\n\r\nIf installed from binary pip package, provide:\r\n\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp36-cp36m-linux_x86_64.whl\r\n1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThe default android example code. Specifically `build.gradle`\r\nWhen trying to run the code on an emulator, for example, there is a config error:\r\n`Could not get unknown property 'assembleRelease for root project 'android' of type org.gradle.api Project` (Get this twice)\r\nLinks to:\r\n\r\n```\r\nafterEvaluate {\r\n    assembleDebug.dependsOn copyNativeLibs\r\n    assembleRelease.dependsOn copyNativeLibs\r\n}\r\n```\r\nfrom build.gradle lines 143-146\r\n\r\n### What other attempted solutions have you tried?\r\nIt looks like this is a conflict with gradle of some sorts. My knowledge is minimal with gradle/adroid studio. Some online threads say it should be included in a task but when putting \r\n`task afterEvaluate{ ...}` there is a similar error and this doesn't work either.\r\n\r\nThis post from a googler implies that your code is written as it should be though.\r\nhttps://code.google.com/p/android/issues/detail?id=219732#32\r\n\r\nAt my wits end with this. For reference, I am able to build with bazel from the terminal. This seems to be a gradle/AS induced issue.\r\n### Logs or other output that would be helpful\r\n```\r\nInformation:Gradle tasks [:assembleDebug]\r\n\r\nError:(145, 1) A problem occurred configuring root project 'android'.\r\n> Failed to notify project evaluation listener.\r\n   > Could not get unknown property 'assembleRelease' for root project 'android' of type org.gradle.api.Project.\r\n   > Could not get unknown property 'assembleRelease' for root project 'android' of type org.gradle.api.Project.\r\n```", "comments": ["What version of Gradle are you using? This should work with 2.3.0", "@andrewharp Gradle Version 3.3 but Android Plug-In version 2.3.0 (which I assume is what you meant). This is a fresh version of AS on a fresh OS with zero tinkering so not sure why I can't get it to work.\r\n\r\nFor reference I was just following the tutorial. i.e. Open project, Run (on emulator in this case). I did make one change to force sensor orientation to 90 as it runs at 3 deg on my pixel C but that's it. \r\n\r\nI will try again without that change and start from scratch to see if it works again.", "Same issue.\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* Where:\r\nBuild file '/Users/tensorflow/tensorflow/examples/android/build.gradle' line: 144\r\n\r\n* What went wrong:\r\nA problem occurred configuring root project 'android'.\r\n> Failed to notify project evaluation listener.\r\n   > Could not get unknown property 'assembleRelease' for root project 'android' of type org.gradle.api.Project.\r\n   > Could not get unknown property 'assembleRelease' for root project 'android' of type org.gradle.api.Project.\r\n\r\n", "@ggfan I think this may be related to 360f449d95cf487fd35dbcbc548a6b65fa7ae64f", "@shyzh @jubjamie Did this ever build before? I'm not able to reproduce with a fresh pull.\r\n\r\nMaybe try `rm -rf gradleBuild/ gradlew gradlew.bat gradle` to make sure you don't have incompatible version of gradle cached somewhere then try building again?", "@andrewharp Not in the office atm but I've never built it before. Brand new OS, build etc. So shouldn't be a cache issue. Never even used gradle before. And I've been doing fresh clones from git in separate directories each time so doubt that will fix. Can try it tomorrow though.", "@andrewharp Sorry for double posting but gradle cache doesn't seem to be the issue.\r\nEDIT: Strangely, I had the problem again once. Then the next time it built correctly (android_debug.apk). I will try and reproduce this but I really have no idea why sometimes it builds and sometimes it doesn't. Will report back.\r\nUPDATE: It appears that, from a fresh install it IS possible to build the APK. If you edit WORKSPACE/build.gradle and load it up and build>Build APK you should get android_debug.apk (or similar) which can be installed via adb just fine.\r\nHOWEVER, if you try and RUN the project, for example via an emulator, then you get the above errors. I have not tried your solution in #3444 but I will next week. Still looks like there is some sort of bug here. In the meantime, would you like me to make a PR for the android readme to assist others or are there some more complete docs coming out along side the inclusion of make for building on windows? Thanks ", "I have had the same issue and this seems to help fix it.\r\n\r\n        tasks.whenTaskAdded { task ->\r\n            if (task.name == 'assembleDebug') {\r\n                task.dependsOn 'extractModels'\r\n            }\r\n            if (task.name == 'assembleRelease') {\r\n                task.dependsOn 'extractModels'\r\n            }\r\n        }", "I'm having the same error as @jubjamie .  @sam-trt adding that block to my ```build.gradle``` file didn't seem to change anything, by adding it like so:\r\n\r\n![screen shot 2017-03-26 at 3 53 32 pm](https://cloud.githubusercontent.com/assets/805682/24334665/7962aef2-123c-11e7-94ae-65dadd018753.png)\r\n", "@nick-jonas sorry you need to change it to look like this\r\n\r\n\r\nassemble.dependsOn copyNativeLibs\r\n\r\nafterEvaluate {\r\n    tasks.whenTaskAdded { task ->\r\n        if (task.name == 'assembleDebug') {\r\n            task.dependsOn 'copyNativeLibs'\r\n        }\r\n        if (task.name == 'assembleRelease') {\r\n            task.dependsOn 'copyNativeLibs'\r\n        }\r\n    }\r\n}\r\n\r\n// Download default models; if you wish to use your own models then\r\n// place them in the \"assets\" directory and comment out this line.\r\napply from: \"download-models.gradle\"", "@sam-trt I now get the following error with ```download-models.gradle```:\r\n\r\n<img width=\"696\" alt=\"screen shot 2017-03-27 at 10 56 55 am\" src=\"https://cloud.githubusercontent.com/assets/805682/24362958/5ec10fbc-12dc-11e7-9910-7da030f9b764.png\">\r\n", "@nick-jonas you'll need to make an equivalent change in download-models.gradle too (altering it to take I to account the different targets).", "I think @sam-trt solution works and matches some similar issues I spotted in my searches online. I'm getting an INSTALL_FAILED_NO_MATCHING_ABIS error now which I believe is to do with CPU architecture etc. and unrelated to this issue. @sam-trt and @andrewharp would you like me to pop this in as a PR?", "@jubjamie I haven't seen the INSTALL_FAILED_NO_MATCHING_ABIS error before -- can you describe where you're seeing that and what the fix was?\r\n\r\nI've made a change internally to apply @sam-trt's fix to download-model.gradle and build.gradle; should be pushed within a day or two.", "Here is build procedure I used on Ubuntu:\r\n\r\n1. Using Android Studio 2.3.0,  import the project ( file --> new --> import (Select Eclipse or Gradle Project to import) --> example/android/build.gradle\r\n2) Let Android Studio to create wrapper for the project ( click OK)\r\n3) in build.gradle, change \"def buildWithMake = true\"  ( this should not matter too much: bazel build is the same )\r\n4) make sure this one works:  def makeNdkRoot = System.getenv('NDK_ROOT'); otherwise I use android studio's ndk version [ inside local.properties file, copy NDK.DIR setting to it ]\r\n5) Build --> Build APK.\r\n\r\nthere are some error messages coming from make build, but it could be ignored and APK is built on my system.", "Ok, I see it is IDE \"run\" button causing the trouble: need to remove assembleRelease.dependsOn when building for debug ( vice versa ).  So just comment that line would be fine, a patch pretty soon to fix it.  ", "@ggfan Building the APK isn't an issue, it was running through the IDE as you pointed out. Building with Bazel is fine and not causing the problem so no need for step 3 (however I guess it's personal preference? Or is one method preferred?)\r\n\r\n@andrewharp the INSTALL_FAILED_NO_MATCHING_ABIS error is caused when your app has native libraries but none for the cpu architecture that you are installing on. I see that in build.gradle there is a place to change the CPU architecture so I will try building with that changed. However, by the looks of it, it will overwrite the previous APK and libraries?\r\nIt's not a TF issue but more of a Android development thing. Nonetheless I would suggest it was mentioned in the docs a bit clearer for Android Studio. I'm happy to make those edits.", "@jubjamie Right, if building with Gradle you need to manually select the right ABI. If running in an emulator, for example, typically you'll want to select x86 or x86_64.\r\n\r\nIt's possible to use bazel to build for multiple ABIs at a time with the --fat_apk_cpu flag. [android_full.sh](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/builds/android_full.sh) (which is how we build the nightly APK) passes `--fat_apk_cpu=armeabi-v7a,arm64-v8a,x86,x86_64`\r\n\r\nContributions welcome to replicate the multi-ABI functionality in build.gradle. It would probably require invoking bazel multiple times, once for each ABI being built, as I believe the --fat_apk_cpu only works when building android_binary targets.", "@andrewharp yeah I did notice the flag in the gradle file. I haven't had a big play around with building for multiple targets as I wanted to get in Android Studio to help me understand the code etc.\r\nI am a newbie to a lot of this so I might not jump at the multi-ABI in gradle task but if I come across needing to do it in the next couple of weeks then I can add that to my android pull request.\r\nI noticed in the 1.1 release notes that cmake building is becoming available soon so will this impact cmake building too?\r\n\r\nEDIT: Wait, does that mean the APK contains all the different builds for different architectures then? So gradle will build them all into one APK at once instead of changing the flag in gradle and building separate ones? New to this and curious in case I try to implement multi-ABI functionality.", "@jubjamie The bazel-built APK from android_full.sh will work on all 4 of those architectures.\r\n\r\nIf you want to simulate this with the Gradle+bazel build, you can manually change the `cpuType` variable in build.gradle to each in turn and hit build. The built native libraries will accumulate in the `jniLibs` directory, and Gradle will package them in the resulting APK. You can see what libraries actually get packaged with `unzip -v tensorflow/examples/android/gradleBuild/outputs/apk/android-debug.apk`\r\n\r\nHowever be aware that making any changes to the native code will require a full repeat of the entire process, as otherwise your native libraries will become out of sync with each other. This is why it would be good to have build.gradle itself automate the process of building for all desired ABIs in one go (it would call bazel multiple times internally).\r\n\r\nedit: We're not yet able to get everything fully building with cmake -- the current cmake Android build uses the Makefile internally, so for now we just recommend the 100% Makefile build over the cmake+Makefile build as it has less moving parts. Eventually we want to have a 100% cmake build as well.", "@andrewharp so if I build the APK 4 times after changing the cpu type, all of them will get packaged because the binaries (or whatever) accumulate in the Libs folder etc? Because if so then looping gradle automatically might not be too tricky for me to have a stab at.\r\n\r\nRe: cmake. Ok. I was hoping I could get it working on Windows with make. I feel like I hit some errors before. Might give it another go now i'm more clued up. Thanks", "@jubjamie \r\nYes, that's exactly how it would work if you want to take a crack at it :)\r\n\r\nUnfortunately the make build doesn't work on Windows out of the box. [This](https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600) is the current easiest way I know of to build Android TensorFlow on Windows. It should be approximately the same amount of effort to use this approach for either make or bazel builds. Hopefully we get cmake working in the near future so we can avoid the extra steps (contributions welcome!)", "@andrewharp I am definitely not knowledgeable enough  to help with cmake stuff but I might give the gradle bits a go if I find some time in the next week or so. When/If I have a go I will add it my currently open Pull Request (or make a new one) and we can continue the conversation there.\r\n\r\nThanks for your help with this particular issue and android in general. I'm still struggling to understand how most of it works so I appreciate having this communication. Source code is hard when you didn't write it :(\r\n\r\nThanks!"]}, {"number": 8586, "title": "TensorFlow upgrade to 1.0.1 ", "body": "I upgraded my server from 1.0.0 (ubuntu):\r\n\r\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\r\nBut I get the following discrepancy:\r\n\r\n(tensorflow)$ pip list | grep tensorflow\r\ntensorflow (1.0.0)\r\n(tensorflow)$ python -c 'import tensorflow as tf; print(tf.__version__)'\r\n1.0.1", "comments": ["What env manager are you using?\r\nDid you uninstall 1.0.0 before upgrading to 1.0.1? If it's causing concern I would try uninstalling and reinstalling. It shouldn't do any harm.", "I am using virtualenv\r\nNo didn\u2019t. But it should upgrade smoothly without uninstall and then install.\r\n\r\n- Best Regards,\r\nNeha Dave\r\n\r\n\r\nFrom: jubjamie <notifications@github.com>\r\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\r\nDate: Tuesday, 21 March 2017 at 9:13 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: \"Dave, Neha\" <Neha.Dave@fmr.com>, Author <author@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] TensorFlow upgrade to 1.0.1 (#8586)\r\n\r\n\r\nWhat env manager are you using?\r\nDid you uninstall 1.0.0 before upgrading to 1.0.1? If it's causing concern I would try uninstalling and reinstalling. It shouldn't do any harm.\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/8586#issuecomment-288120122>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AYkejUoW5cSzh3qUK2--iakSbRL7pU-xks5rn_AagaJpZM4Mj7pb>.\r\n", "The upgrade process is completely managed by PIP package manager.\r\nPIP is just a tool we use to deliver tensorflow.\r\nUnfortunately, it at times has bugs. Looks like you ran into one of these.\r\nIt looks like TF actually upgraded, but pip was not able to record the metadata correctly.\r\n\r\nIf you are not using virtualenv, you may need to use \"sudo\" to when running pip. If you are in a virtualenv, make sure you did not accidentally run pip command with sudo previously.\r\nAlso, most pip issues are resolved when you rerun the command.\r\n\r\nFinally, as this looks like a PIP issue, there is not much we can do to help with this.\r\nYou can try reporting an issue to pypi, through http://pypi.org"]}, {"number": 8585, "title": "Consistent dtypes for discrete tensorflow.contrib.distributions?", "body": "In master (https://github.com/tensorflow/tensorflow/commit/f4a0c2c0f1bbff6e9b4d5d4a0796e7645a974321), the following is true:\r\n+ Bernoulli, Categorical, and OneHotCategorical accept dtype as an argument, defaulting to `tf.int32`.\r\n+ Binomial and Geometric have dtype given by its `probs` parameter, which is returned by the `get_logits_and_probs` utility function. It errors unless `probs` is float, meaning they must have dtype float.\r\n+ Deterministic has dtype given by its `loc` parameter, which can have either dtype float or int.\r\n+ Poisson has dtype given by its `rate` parameter, which can have either dtype float or int.\r\n\r\nIt seems like the default dtype for discrete distributions with fixed and bounded support is int. And the dtype for discrete distributions whose support depends on a parameter, or whose support is unbounded, varies between always being float or being either float or int.", "comments": ["@ebrevdo @jvdillon @langmore ", "How high priority is this bug for you, Dustin?\n\nOn Wed, Mar 22, 2017 at 10:48 AM, Skye Wanderman-Milne <\nnotifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> @jvdillon\n> <https://github.com/jvdillon> @langmore <https://github.com/langmore>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8585#issuecomment-288482508>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzjYcXe1LotZTjfWxYSch0EFIngWks5roV7qgaJpZM4Mj4R9>\n> .\n>\n", "Not a high priority. Am curious if there's a convention. This helps determine what dtype to use for the random variables we have in Edward: PointMass (here, it is Deterministic), Empirical, and DirichletProcess. https://github.com/blei-lab/edward/tree/master/edward/models\r\n\r\nThis also relates to the support specs on discrete vs continuous.", "Thanks for raising this issue--this bug should now be resolved. "]}, {"number": 8583, "title": "Bernoulli's probability mass function produces output beyond its support", "body": "Bernoulli's (log) probability mass function is valid for values between 0 and 1, even though its support is only {0, 1}. This is a result of [its implementation](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/distributions/python/ops/bernoulli.py#L150) relying on `tf.sigmoid_cross_entropy_with_logits(logits, targets)`, where `targets` does not have to be {0, 1}.\r\n\r\nIs this intended behavior?\r\n\r\n```python\r\nfrom tensorflow.contrib import distributions as ds\r\n\r\nx = ds.Bernoulli(p=0.5)\r\ny = x.log_prob(0.5)\r\n\r\nsess = tf.Session()\r\nsess.run(y)\r\n## -0.69314718\r\n```\r\n\r\n```python\r\nprint(tf.__version__)\r\n## 1.0.1\r\n```", "comments": ["@ebrevdo @jvdillon @langmore ", "This is indeed a bug.  Solution is:\n\n1. if the value can be checked statically, we should ensure its either 0 or\n1.\n2. if not, and validate_args is true, we need to add a dynamic assertion.\n\nOn Wed, Mar 22, 2017 at 10:48 AM, Skye Wanderman-Milne <\nnotifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> @jvdillon\n> <https://github.com/jvdillon> @langmore <https://github.com/langmore>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8583#issuecomment-288482616>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimy0TiPeQQhOcm-2tbAKAaIPcFQDmks5roV79gaJpZM4Mj1Yr>\n> .\n>\n"]}, {"number": 8582, "title": "Greater than fifteen minutes to run a C++ example code using Tensorflow libraries using bazel", "body": "Why does it take greater than fifteen minutes to run a C++ example code using the Tensorflow libraries using bazel?\r\n\r\nhttps://www.tensorflow.org/api_guides/cc/guide\r\n\r\n", "comments": ["Because of Wirths law", "You are probably seeing delays because you actually have to compile from sources when using bazel.\r\n"]}, {"number": 8581, "title": "tf.read_file doesn't support non ASCII characters in filename", "body": "Windows 2003 server R2, if input file names contain non ASCII characters (such as Chinese), exception will raise.\r\n\r\n     [[Node: ReadFile_17 = ReadFile[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](unstack_5:2)]]\r\n\r\nCaused by op 'ReadFile_17', defined at:\r\n  File \"train.py\", line 660, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train.py\", line 657, in main\r\n    train()\r\n  File \"train.py\", line 430, in train\r\n    num_preprocess_threads=num_preprocess_threads)\r\n  File \"input.py\", line 216, in distorted_inputs\r\n    num_readers = FLAGS.num_readers)\r\n  File \"input.py\", line 256, in batch_inputs\r\n    image_buffer = tf.read_file(filename)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 203, in read_file\r\n    result = _op_def_lib.apply_op(\"ReadFile\", filename=filename, name=name)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2334, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): Can not get size for: 01_\u745c_057_14734860\r\n91829.jpg : \\u03f5\\u0373\\udcd5\\u04b2\\udcbb\\udcb5\\udcbd\\u05b8\\udcb6\\udca8\\udcb5\\udcc4\\udcce\\u013c\\udcfe\\udca1\\udca3\r\n\r\n         [[Node: ReadFile_17 = ReadFile[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](unstack_5:2)]]\r\n", "comments": ["Right. We need to call the right function on windows. It would be nice if you could submit a PR.", "This is a bug in TensorFlow. It looks like we're using the ANSI versions of Windows filesystem functions (e.g. in [windows_file_system.cc:444](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/windows_file_system.cc#L444) we should use GetFileAttributesExW instead of GetFileAttributesExA, see [Microsoft's docs](https://msdn.microsoft.com/en-us/library/windows/desktop/aa364946(v=vs.85).aspx)).\r\n\r\nI'm working on a fix, but unfortunately I don't have a good setup for testing Windows code so it might take a while before it's committed. For now, I suggest using ASCII filenames if possible.", "Oh, didn't see @drpngx's comment :) If @wangxianliang or someone else wants to spin up a patch that would be great.", "Maybe the bug is not related to the ANSI version of GetFileAttributesExA.\r\nI created a new project, and used the code snippet of GetFileSize. The function can get the file attributes correctly whenever the input file name contains non ASCII characters or not.\r\n", "I wonder if we're getting the string properly utf8 encoded from python.\nWorth checking.\n\nOn Mar 22, 2017 11:16 PM, \"Xianliang Wang\" <notifications@github.com> wrote:\n\n> Maybe the bug is not related to the ANSI version of GetFileAttributesExA.\n> I created a new project, and used the code snippet of GetFileSize. The\n> function can get the file attributes correctly whenever the input file name\n> contains non ASCII characters or not.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8581#issuecomment-288627061>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbVWqE5ksJtJWG67QcfAGdI5MsBZgks5rog47gaJpZM4MjzGo>\n> .\n>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@drpngx @girving @skye \r\n\r\nThis bug can be fixed by changing **TranslateName** in **windows_file_system.h** from\r\n```c++\r\nstring TranslateName(const string& name) const override {\r\n    return name;\r\n  }\r\n```\r\nto\r\n```c++\r\nstd::wstring ToUtf16(std::string str)\r\n{\r\n\tstd::wstring ret;\r\n\tint len = MultiByteToWideChar(CP_UTF8, 0, str.c_str(), str.length(), NULL, 0);\r\n\tif (len > 0)\r\n\t{\r\n\t\tret.resize(len);\r\n\t\tMultiByteToWideChar(CP_UTF8, 0, str.c_str(), str.length(), &ret[0], len);\r\n\t}\r\n\treturn ret;\r\n}\r\nstd::string ToMBCS(std::wstring str)\r\n{\r\n\tstd::string ret;\r\n\tint len = WideCharToMultiByte(CP_ACP, 0, str.c_str(), str.length(), NULL, 0, NULL, NULL);\r\n\tif (len > 0)\r\n\t{\r\n\t\tret.resize(len);\r\n\t\tWideCharToMultiByte(CP_ACP, 0, str.c_str(), str.length(), &ret[0], len, NULL, NULL);\r\n\t}\r\n\treturn ret;\r\n}\r\nstring TranslateName(const string& name) const override {\r\n    return ToMBCS(ToUtf16(name));\r\n  }\r\n```\r\nThe input fname is 'UTF-8' format, however, windows can't open utf8 encoded filename in c++.", "another fix could change some of the python codes in **'tensorflow\\python\\lib\\io\\file_io.py'**,\r\nfrom\r\n```python\r\ndef _preread_check(self):\r\n    if not self._read_buf:\r\n      if not self._read_check_passed:\r\n        raise errors.PermissionDeniedError(None, None,\r\n                                           \"File isn't open for reading\")\r\n      with errors.raise_exception_on_not_ok_status() as status:\r\n        self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\r\n            compat.as_bytes(self.__name), 1024 * 512, status)\r\n```\r\nto\r\n```python\r\ndef _preread_check(self):\r\n    if not self._read_buf:\r\n      if not self._read_check_passed:\r\n        raise errors.PermissionDeniedError(None, None,\r\n                                           \"File isn't open for reading\")\r\n      with errors.raise_exception_on_not_ok_status() as status:\r\n        encoding = 'mbcs' if platform.system() == 'Windows' else 'utf-8'\r\n        self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\r\n            compat.as_bytes(self.__name, encoding), 1024 * 512, status)\r\n```\r\n@drpngx @skye @girving "]}, {"number": 8580, "title": "modify annotations of data_format of average_pooling2d in file python/layers/pooling.py ", "body": "Modified wrong annotation of data_format in average_pooling2d in file python/layers/pooling.py\r\n\r\nfrom (batch, height, channels, width) to (batch, height, width, channels) when 'channels_last'", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!\n\nOn Tue, Mar 21, 2017 at 8:02 PM, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8580#issuecomment-288057605>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQxIANTW99YFpjPm26-2TeTbC1f8TJ6rks5rn7xvgaJpZM4MjssR>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 8579, "title": "OutOfMemoryError: Java heap space on Tensorflow tests execution", "body": "Executing Tensorflow test suite using command: \r\n\r\n    bazel test //tensorflow/...\r\n\r\nthrows Out of memory issue: \r\n\r\n    INFO: Found 1886 targets and 1155 test targets...\r\n    INFO: Elapsed time: 291.775s, Critical Path: 0.91s\r\n    java.lang.OutOfMemoryError: Java heap space\r\n            at com.google.devtools.build.skyframe.SkyKey.create(SkyKey.java:57)\r\n            at com.google.devtools.build.lib.skyframe.ArtifactSkyKey.key(ArtifactSkyKey.java:43)\r\n            at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.toKeys(ActionExecutionFunction.java:576)\r\n            at com.google.devtools.build.lib.skyframe.ActionExecutionFunction.compute(ActionExecutionFunction.java:158)\r\n            at com.google.devtools.build.skyframe.ParallelEvaluator$Evaluate.run(ParallelEvaluator.java:370)\r\n            at com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:501)\r\n            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n            at java.lang.Thread.run(Thread.java:745)\r\n    Java heap space\r\n    \r\n    bazel ran out of memory and crashed.\r\n\r\n\r\nI have tried below options still issue persists. \r\n\r\n> export _JAVA_OPTIONS=\"-Xms1024m -Xmx1024m\"\r\n> \r\n> export JVM_ARGS=\"-Xmx1024m -XX:MaxPermSize=256m\"\r\n> \r\n> export JVM_ARGS=\"-XX:PermSize=64M -XX:MaxPermSize=256m\"\r\n\r\nAlso, changed the file from bazel code: scripts/bootstrap/compile.sh\r\n\r\n`\" run \"${JAVAC}\" -J-Xms1g -J-Xmx1g -classpath \"${classpath}\" -sourcepath \"${sourcepath}\"`\"\r\n\r\n\r\nMachine configurations: Ubuntu distribution, openjdk8, RAM 16G\r\nTensorFlow : master\r\n\r\n", "comments": ["Below command worked from me: \r\n\r\n`bazel  --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\"  test //tensorflow/...`", "Thank you for sharing the solution to your problem!"]}, {"number": 8578, "title": "Update computecpp.tpl", "body": "Computecpp can't generate code for AVX function __builtin_ia32_cmpps256. Build error is attached.\r\n\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/857872/error.txt)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!\n\nOn Tue, Mar 21, 2017 at 7:05 AM, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8578#issuecomment-288045452>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGrbvlY383YUVBas-58tlgwRfv_8Jaemks5rn68HgaJpZM4Mjo0Z>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 8577, "title": "Adding mechanism for XLA device plugins to be loaded into the python unit tests", "body": "By copying and configuring 2 files, the unit test framework will include a plugin when running.\r\n\r\n1) tensorflow/compiler/tests/plugin_config.tpl -> tensorflow/compiler/tests/plugin_config.py\r\n\r\nThis file should contain the registered name of the device, the types that it supports, and the name of a python module that loads the plugin.\r\n\r\n2) tensorflow/compiler/tests/plugin/BUILD.tpl -> tensorflow/compiler/tests/plugin/BUILD\r\n\r\nThis should contain a rule called 'deps' that contains attributes for ensuring that the plugin and its associated files are included in the runfiles of the unit tests. A 'data' attribute is probably most appropriate.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8576, "title": "Tensorflow Install issue on windows 10", "body": "I tried to install TensorFlow on my PC with Windows 10 OS, and I've installed with Conda Command\r\n\r\nC:> conda create -n tensorflow \r\nBelow is the error .. please let me know how can I install .. Thanks.\r\n\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\conda\\core\\index.py\", line 391, in fetch_repodata\r\n        with open(cache_path, 'w') as fo:\r\n    **PermissionError: [Errno 13] Permission denied: 'C:\\\\ProgramData\\\\Anaconda3\\\\pkgs\\\\cache\\\\2116b818.json'**\r\n", "comments": ["Is that the error from conda or from pip? Where is your conda environment set located? In your user folder or Program Files? Tried running cmd as an administrator?\r\nA reinstall of conda might solve this. \r\nThis doesn't appear to be a tensorflow bug so I would move your question to Stack Overflow or to Conda forums.", "This is a windows issue on your setup.\r\nYou need an terminal with administrator rights to write to that folder.\r\n\r\nClosing as this is not a TF issue."]}, {"number": 8575, "title": "error when attempting to run image_retraining", "body": "### Environment info\r\nOperating System: Ubuntu Linux 16.04 LTS\r\n\r\nInstalled from source\r\n1. Tensorflow version: 1.0.1\r\n2. The output of `bazel version`: \r\n```\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n```\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nLink to the tutorial: https://www.tensorflow.org/tutorials/image_retraining\r\n```\r\nbazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 1052, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 768, in main\r\n    maybe_download_and_extract()\r\n  File \"/home/sria91/Development/tensorflow-1.0.1/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 305, in maybe_download_and_extract\r\n    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\r\n  File \"/usr/lib/python3.5/tarfile.py\", line 1996, in extractall\r\n    numeric_owner=numeric_owner)\r\n  File \"/usr/lib/python3.5/tarfile.py\", line 2038, in extract\r\n    numeric_owner=numeric_owner)\r\n  File \"/usr/lib/python3.5/tarfile.py\", line 2108, in _extract_member\r\n    self.makefile(tarinfo, targetpath)\r\n  File \"/usr/lib/python3.5/tarfile.py\", line 2156, in makefile\r\n    copyfileobj(source, target, tarinfo.size, ReadError)\r\n  File \"/usr/lib/python3.5/tarfile.py\", line 241, in copyfileobj\r\n    buf = src.read(BUFSIZE)\r\n  File \"/usr/lib/python3.5/gzip.py\", line 274, in read\r\n    return self._buffer.read(size)\r\n  File \"/usr/lib/python3.5/_compression.py\", line 68, in readinto\r\n    data = self.read(len(byte_view))\r\n  File \"/usr/lib/python3.5/gzip.py\", line 480, in read\r\n    raise EOFError(\"Compressed file ended before the \"\r\nEOFError: Compressed file ended before the end-of-stream marker was reached\r\n```", "comments": ["I think the error `EOFError: Compressed file ended before the end-of-stream marker was reached` means the model file you've downloaded `http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz` is incomplete. Try to download the model file again.", "@derekhh Many thanks, that helped.", "I came across the same error.I abruptly stopped when inception was being downloaded. @sria91  where have you saved the downloaded model? ", "@Srigowri By default, the model gets downloaded to `/tmp/imagenet`. Delete that folder and run `image_retraining` again to initiate redownload.\r\n", "Thanks \ud83d\udc4d  @sria91 "]}, {"number": 8574, "title": "Problems with saver and restore: Failed to find any matching files", "body": "This is the code:\r\n\r\n```\r\n\r\nsave_path = model_path+\"%s.ckpt\"%model_name\r\nif flag == \"initial_train\":\r\n    print(\"Training new model\")\r\n    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\r\n        train_writer =  tf.summary.FileWriter('/tmp/log/train_tanh2_reg_100_%s'%beta_reg, sess.graph) # activate TensorBoard\r\n        val_writer =  tf.summary.FileWriter('/tmp/log/val_reg_tanh2_100_%s'%beta_reg, sess.graph) \r\n        sess.run(tf.global_variables_initializer())\r\n        print(\"The model will be saved in \",save_path)\r\n        training_loop(num_epochs)\r\n        flag = None\r\nelse:\r\n    print(\"Training pretrained model\")\r\n    # Restore from previous model\r\n    print(\"Restore from: \" + save_path)\r\n    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\r\n        train_writer =  tf.summary.FileWriter('/tmp/log/train_tanh2_reg_100_%s'%beta_reg, sess.graph) # activate TensorBoard\r\n        val_writer =  tf.summary.FileWriter('/tmp/log/val_reg_tanh2_100_%s'%beta_reg, sess.graph) \r\n        saver.restore(sess,save_path)\r\n        print(\"Model loaded\")\r\n        training_loop(nump_epochs)\r\ntrain_writer.close()\r\nval_writer.close()\r\n```\r\nand these are the ckpt files that have been created:\r\n\r\ncheckpoint\r\nprezi.ckpt-0.data-00000-of-00001\r\nprezi.ckpt-0.index\r\nprezi.ckpt-0.meta\r\nprezi.ckpt-1.data-00000-of-00001\r\nprezi.ckpt-1.index\r\nprezi.ckpt-1.meta\r\nprezi.ckpt-2.data-00000-of-00001\r\nprezi.ckpt-2.index\r\nprezi.ckpt-2.meta\r\nprezi.ckpt-3.data-00000-of-00001\r\nprezi.ckpt-3.index\r\nprezi.ckpt-3.meta\r\n\r\nBut still I get this error:\r\n\r\n**Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/carol/tensorflowLF/clasification/prezi.ckpt**\r\n\r\nThanks for your help in advance :)", "comments": ["Which version of TensorFlow are you running?", "Same problem here - using tf 1.0", "I am having similar issue.\r\nI am saving checkpoint in the same session and trying to read / restore it later in the same session. I get this error \r\nNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt\r\n\t [[Node: save/RestoreV2_24 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_24/tensor_names, save/RestoreV2_24/shape_and_slices)]]\r\n\r\nI know for sure the model is saved. For some reason, most of the checkpoint files are removed.\r\n\r\nsession creation\r\nwith tf.Graph().as_default(): \r\n    # global step\r\n    batch_start_index = tf.placeholder(tf.int32, shape=(1), name=\"BatchStartIndex\")\r\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n    print(\"Global Step\", global_step)\r\n\r\nSaver\r\n    saver = tf.train.Saver()\r\n    print(\"RNN Saver: \", saver)\r\n    sess = tf.Session()\r\n    print(\"RNN Session: \", sess)\r\n    sw = tf.summary.FileWriter(base_checkpoint_path, sess.graph)\r\n    print(\"Summary Writer: \", sw)\r\n    sess.run(init)\r\n    print(\"sesson init run complete\")\r\n\r\nSaving session\r\n            file_name = \"modelrnnopt_\" + str(step) + \".ckpt\"\r\n            checkpoint_file = os.path.join(base_checkpoint_path, file_name)\r\n            saver.save(sess, checkpoint_file, global_step=step)\r\n            checkpoint_files[checkpoint_file] = 0    \r\n-------------------->>>> I see this file getting saved\r\nRestoring session\r\n        print(\"Running checkpoint: \", cp)\r\n        test_feed_dict = build_rnn_feed(HYPER_BATCH_SIZE, std_test_data, std_test_label,128)\r\n        saver.restore(sess, cp) ------------------------------------->>>>>>> This is where it fails\r\n#### cp => D:\\temp\\rnnopt\\modelrnnopt_2300.ckpt\r\n        test_prediction = sess.run(accuracy, feed_dict = test_feed_dict)\r\n\r\nWhy are the checkpoints removed?\r\nbase_checkpoint_path = \"D:\\\\temp\\\\rnnopt\"\r\n", "Actually, I fixed it and I don't remember what it was - but I can tell you the error message was way off. I think I had to re-initialize the variables first or something. ", "you must keep the model paramerer to be unchanged including optimizer\r\neg. --optimizer ADAGRAD to --optimizer ADAM ", "I was stucked in the same , got a fix in different thread.\r\nYou have to use full relative path for the model \r\nEg : \"model_08\" has to be replaced by \"./model_08\" or full relative path.\r\n\r\nTry replacing and let us know, if it works !"]}, {"number": 8573, "title": "bazel-bin command not found", "body": "I am having issues while building pip package.I am building tensorflow with cpu configurations Steps i followed\r\n\r\nbazel release= 0.4.5, ubuntu -version = 14.04 LTS\r\n1. bazel clean\r\n2. ./configure \r\n3. bazel build --config op\r\n```\r\nkush@kush-Lenovo-B40-80:~/machine_learning/deeplearning/tensorflow$ bazel build --config=op\r\nWarning: ignoring LD_PRELOAD in environment.\r\nWARNING: Config values are not defined in any .rc file: op\r\nINFO: Found 0 targets...\r\nINFO: Elapsed time: 2.365s, Critical Path: 0.03s\r\n\r\n```\r\n4. Now when i try to run \r\n`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n```\r\nkush@kush-Lenovo-B40-80:~/machine_learning/deeplearning/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nbash: bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory\r\n```\r\n\r\nBut then i tried sudo it gives \r\n`sudo: bazel-bin/tensorflow/tools/pip_package/build_pip_package: command not found\r\n`\r\n\r\nbazel is installed in my system. i have checked it. \r\n\r\nI also tried giving full path in bazel-bin\r\n\r\n`sudo: /usr/bin/bazel-bin/tensorflow/tools/pip_package/build_pip_package: command not found\r\n`\r\n\r\ni have bazel release = 0.4.5\r\nCan anyone help me on this?\r\n", "comments": ["Can you do those steps and let us know where it goes wrong? These are copied from the basic serving tutorial it's really easy to skip one and fall into trouble\r\n\r\n`rm -fr ~/.bazel ~/.bazelrc`\r\n\r\nDownload a fresh binary from:\r\n[https://github.com/bazelbuild/bazel/releases/download/0.4.5/bazel-0.4.5-installer-linux-x86_64.sh](url)\r\n\r\n`cd ~/Downloads\r\nchmod +x bazel-0.4.2-installer-linux-x86_64.sh\r\n./bazel-0.4.2-installer-linux-x86_64.sh --user`\r\n\r\n`export PATH=\"$PATH:$HOME/bin\"`\r\n\r\n`sudo apt-get update && sudo apt-get install -y \\\r\n        build-essential \\\r\n        curl \\\r\n        libcurl3-dev \\\r\n        git \\\r\n        libfreetype6-dev \\\r\n        libpng12-dev \\\r\n        libzmq3-dev \\\r\n        pkg-config \\\r\n        python-dev \\\r\n        python-numpy \\\r\n        python-pip \\\r\n        software-properties-common \\\r\n        swig \\\r\n        zip \\\r\n        zlib1g-dev`\r\n\r\n`sudo pip install grpcio`\r\n\r\n`git clone --recurse-submodules https://github.com/tensorflow/serving\r\ncd serving\r\ncd tensorflow\r\n./configure\r\ncd ..`\r\n\r\nMake sure you are executing `./configure` in the tensorflow folder that is inside the serving folder", "@ammarasmro  i got same error. PLease see the output of each and every step\r\n\r\nI was not able to download bazel with --user tag as it gives error \r\n1. ./configure\r\n\r\n`kush@kush-Lenovo-B40-80:~/Desktop/serving/tensorflow$ ./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] n\r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\nWarning: ignoring LD_PRELOAD in environment.\r\n......................................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\nWarning: ignoring LD_PRELOAD in environment.\r\n................\r\nINFO: All external dependencies fetched successfully.\r\n`\r\n2. `\r\nkush@kush-Lenovo-B40-80:~/Desktop/serving/tensorflow$ bazel build --config=opt\r\nWarning: ignoring LD_PRELOAD in environment.\r\nINFO: Found 0 targets...\r\nINFO: Elapsed time: 2.939s, Critical Path: 0.03s\r\nkush@kush-Lenovo-B40-80:~/Desktop/serving/tensorflow$ ls\r\n`\r\n\r\n3.\r\n`kush@kush-Lenovo-B40-80:~/Desktop/serving/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nbash: bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory\r\n`\r\n4. \r\nkush@kush-Lenovo-B40-80:~/Desktop/serving/tensorflow$ sudo !!\r\nsudo bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n[sudo] password for kush: \r\nsudo: bazel-bin/tensorflow/tools/pip_package/build_pip_package: command not found\r\n", "The bazel command you are writing does not really do anything.\r\nPlease follow the guide here to build pip package from sources:\r\nhttps://www.tensorflow.org/install/install_sources\r\n\r\nAnd please use stackoverflow with help on how to use bazel.", "Following these instructions exactly, yields the same problem i.e. there is no file by the name of build_pip_package", "You may have not installed bazel correctly. I had the same issue and then i understood that i forgot the step 4 of bazel 0.18.1 installation which is (in terminal):\r\n\r\nexport PATH=\"$PATH:$HOME/bin\"\r\n"]}]