[{"number": 30819, "title": "Update release notes for TensorFlow 1.14.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 1.14.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 30818, "title": "tf.py_function in tf.data.Dataset pipeline doesn't work with TPUEstimator", "body": "Environment is Google Colab with TPU runtime.\r\n\r\n**Describe the current behavior**\r\nI have an input pipeline that contains tf.py_function as one of its processing steps when training a model with TPUEstimator. When I run the code, I get the following error:\r\n\r\n```\r\nNo registered 'EagerPyFunc' OpKernel for CPU devices compatible with node {{node EagerPyFunc}}\r\n\t.  Registered:  <no registered kernels>\r\n\r\n\t [[EagerPyFunc]]\r\n\t [[input_pipeline_task0/MakeIterator]]\r\n```\r\n\r\nAccording to the documentation (https://www.tensorflow.org/guide/using_tpu) \"The input pipeline generated by your input_fn is run on CPU.\" Running the same input code with a standard Estimator on CPU/GPU works just fine. Manually placing the Dataset + all processing steps on the CPU with `tf.device('/cpu:0')` also fails with the same error when training with TPUEstimator.\r\n\r\n**Describe the expected behavior**\r\nI should be able to run Python code on the CPU as part of my input pipeline when training on TPUs.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/15KuVkukIWdN753ffNjY6jVzT1gyhsBRQ\r\n\r\nThe notebook linked above is nearly identical to https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpuestimator.ipynb. I've just added the following line to the Training Data code cell:\r\n\r\n```idx = tf.py_function(lambda x: x, [idx], tf.int32)```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nIf you are unclear what to include see the issue template displayed in the [Github](https://github.com/tensorflow/tensorflow/issues/new/choose) new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "I shared a Google Colab notebook that reproduces the issue... so operating system, architecture, source vs. binary, etc. questions in the new issue template are not relevant. I'm using the default TF version which, at the time of writing, is 1.14.0.\r\n\r\nAre you unable to reproduce the issue after running the Colab notebook?", "Unfortunately this will be complicated with TPUEstimator since:\r\n\r\n- This requires a Python interpreter.\r\n- We run the input pipeline on the TPU VM, which doesn't have an interpreter.\r\n- The Python VM isn't included in the TensorFlow graph.", "Is the eventual goal to run the input pipeline on CPU with TPUEstimator or are the docs incorrect? I need to decide if I should re-think my use of TPUs / structure of my input pipeline or if I should wait for the TPUEstimator implementation to catch up to the docs.", "The input pipeline does run on CPU, but on the CPU of the TPU VM. Essentially you're connecting to a remote host to run your model. If you can express your input pipeline in terms of tf.data but without the py_func, then it should work.", "Gotcha, thanks for the input.\r\n\r\nDo you anticipate having a Python interpreter running in the TPU VM at some point in the future? The input pipeline often needs much more flexibility than the model itself and it's difficult (if not impossible) at times to express a pipeline in terms of _pure_ TF ops.\r\n\r\nIn my case, I've hacked up our input pipeline to avoid tf.py_function (yay!) but the result is pretty gnarly.", "Yeah, in the new TPUStrategy API, we do already support this, and additionally it offers flexibility for the user to do step-at-a-time training and feed in whatever Python numpy arrays they want. It's a bit new, so we'll start documenting it soon.", "@jhseu Does the distribute strategy support using tf.py_func outside the input pipeline. For instance, is it possible to apply python logic(using pure python, numpy,opencv etc)  to the output tensor of some layer in a model while training?", "@jhseu with the TPUStrategy API I am still getting the same error as described in the initial issue. I am running on a Google Cloud v3-8, TF version 2.1.0-dev20200102, following [these instructions](https://github.com/tensorflow/models/tree/master/official/transformer#using-tpus). It works fine as is, but if I add the following two lines after [line 86](https://github.com/tensorflow/models/blob/master/official/transformer/v2/data_pipeline.py#L86) to use py_functions: \r\n\r\n```\r\ninputs = tf.py_function(lambda x: x, [inputs], tf.int64)\r\ntargets = tf.py_function(lambda x: x, [targets], tf.int64\r\n```\r\n\r\nthen I get this error: \r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'EagerPyFunc' used by {{node EagerPyFunc}} with these attrs: [Tout=[DT_INT64], is_async=false, Tin=[DT_INT64], _xla_inferred_shapes=[<unknown>], token=\"pyfunc_0\"]\r\nRegistered devices: [CPU, TPU, TPU_SYSTEM, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n         [[EagerPyFunc]]\r\n```\r\n\r\nI will try to write my pipeline in terms of tf.data, but if you had any updates on using tf.py_function in this context, that would be great. ", "I have the same problem as above, any updates?", "@samkleeman1 Sorry, guys I was away for a long while. Why are you using tensorflow estimators, still? Best is to use TF 2.0", "My experience is a bit dated but you cannot use tf.py_func, The TPU is a standalone unit and any interaction with python needs to be done carefully. As far as I can tell, you need to implement using tensorflow functions, all operations that fit onto the TPU. You can make a host call but If I were you, I would try and minimize host calls and rather use the generic code that Tensorflow provides as a starting point. I have used estimators too, but I think everyone is moving on to TF 2.0", "> Yeah, in the new TPUStrategy API, we do already support this, and additionally it offers flexibility for the user to do step-at-a-time training and feed in whatever Python numpy arrays they want. It's a bit new, so we'll start documenting it soon.\r\n\r\n@jhseu Is there any updates? I tried to use tf.py_function inside dataset.map and still gets an error on TPU.", "Have you tried with tf 2.3.0 nightly? ", "tf 2.3.0 does not work either.", "Any update on this? Perhaps it was fixed in tf 2.3.1? ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30818\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30818\">No</a>\n", "Is there any way out to this? I am trying to train EfficientDet-D1 on colab TPUs and running into this."]}, {"number": 30817, "title": "Add default eager session", "body": "Cherry pick of #28781", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only \"I consent.\" in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30817) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 30816, "title": "tensorflow latency 2-3x worse with multiple input tensors", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12, 1.11, 1.13\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a (but 4.8)\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have trained a model and then i 2 versions of frozen graph .\r\n1) version 1 : single placeholder which takes in tfexample and rune the ParseExample operator and does inference\r\n2)Version 2 : instead of single placeholder , i have about ~300 placeholders . one placholder per feature (i.e since the features are already parsed , i do not create tfexample , but rather feed in individual tensors i.e a feed dictionalry where for each feature column i feed in the respective tensor)\r\n\r\nThe number of ops in both graphs are approximately same (multiple placeholders being slightly less ) ~5-6k ops/nodes\r\n\r\nLatency of version 2 is 2-3x higher then version 1 \r\n\r\n**Describe the expected behavior**\r\n\r\nExpected behavior is latency when i parse the feature before hand and feed in per feature tensor , there is lesser work for tensorflow to do , so it should be faster or at worse the same ? why do we observe 2-3x higher latency ?\r\n\r\nis this due to the way feed dictionary gets copied from python to c++ ?\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@patelprateek ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30815, "title": "Feedback on Tensorflow 2.0 beta - documentation issues?", "body": "Hi,\r\n\r\nPlease accept the following as feedback on my experience of Tensorflow 2.0 (beta):\r\n1. Read the [migration guide](https://www.tensorflow.org/beta/guide/migration_guide) and figured I would give it a go.\r\n2. Installed without issues on my Mac Conda environment.  This was to be the highlight of my successes.\r\n3. The [documentation site](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf) isn't searchable by version and given the frequency and amount of change here it would be nice to find a way to locate where various things have moved to.\r\n4.  By way of example, PhasedLSTMCell, I knew the contrib module is gone in 2.0 but its still in the GitHub 2.0 branch which is misleading.  Further misleading is the comments [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/__init__.py): \r\n> Created in contrib, eventual plans to move to core.\r\n\r\nNo indication of where one might currently find it.\r\n5. Figured it might be in in [Addons](https://github.com/tensorflow/addons) but no luck there.\r\n6. I wanted to use tf.keras for the first time.  Maybe its my Pip/Conda environment but no matter what I did I could not get it to import unless I did import tensorflow._python._keras.  Did I miss this in the docs because I swear I didn't read it anywhere.  \r\nAfter some more poking around I decided I had enough exposure to 2.0 and went back to 1.14 - it did give me some minutes of excitement.\r\n\r\nHowever, I would love 2.0 to be speedily and widely adopted.  The API looks a lot cleaner (from what I read of it, I didn't get to use any ultimately) and I think some improvements around how the documentation is accessible would help uptake.  I'm willing to contribute to help this along if I can.\r\n", "comments": ["@emrul Just few questions and pointers on docs.\r\n\r\nItem (3) : What do you mean by `3. The documentation site isn't searchable by version`?. On the page you listed, you can select any TF version (For Ex. TF1.12) from the dropdown of `API`.  You could see detailed release notes from [here](https://github.com/tensorflow/tensorflow/releases).\r\n\r\nItem (4): Regarding `contrib module is gone in 2.0`, contrib is removed from TF2.0 but it is still part of TF1.x. The link you provided points to `master` not  `2.0`. PhasedLSTMCell you can see it [here](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1915-L2062).\r\n\r\nPlease feel free to contribute through PRs to update documentation and code. Thanks!\r\n", "@emrul \r\nIs this still an issue", "I am closing this issue as it is a stale issue. Thanks!\r\n"]}, {"number": 30814, "title": "Precision change of tf.variable while doing some ops", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed : via conda\r\n- TensorFlow version (use command below): 1.14.0-rc1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0.130/7.6 (driver 410.78)\r\n- GPU model and memory: Tesla V100 16GB\r\n\r\n**Describe the current behavior**\r\nThe output difference is non zero\r\n**Describe the expected behavior**\r\nIt should be zero\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.reset_default_graph()\r\nwith tf.device('/cpu:0'):    \r\n    inp = tf.Variable(np.random.normal(size=(10,5)), dtype=tf.float64)\r\n\"\"\"\r\nwith tf.device('/gpu:0'):    \r\n    inp = tf.Variable(np.random.normal(size=(10,5)), dtype=tf.float64)       \r\n\"\"\"\r\nvar_test = tf.Variable(np.zeros((3,5), dtype=np.float64), dtype=tf.float64)\r\nplh = tf.placeholder(shape=(3,), dtype=tf.int32)\r\n   \r\nop = var_test.assign(tf.gather(inp, plh))\r\nwith tf.control_dependencies([op]):\r\n    out_var = var_test + tf.constant(np.float64(0.07))\r\n    out_var = out_var - tf.constant(np.float64(0.07))\r\n    update_op = tf.scatter_update(inp, plh, out_var)\r\n\r\nidx = np.arange(10)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    out_ip_before = sess.run(inp)\r\n    print(out_ip_before)\r\n    print('sum of all params (before):', np.sum(out_ip_before))\r\n    for i in range(10):\r\n        idx_batch = np.random.permutation(idx)[:3]\r\n        out = sess.run(update_op, feed_dict={plh:idx_batch})\r\n        \r\n    out_ip_after = sess.run(inp)\r\n    print('sum of all params (after):', np.sum(out_ip_after))\r\n    print(out_ip_after)\r\n    diff_after_before = out_ip_before - out_ip_after\r\n    print('before-after:', np.sum(diff_after_before))\r\n    print(diff_after_before)\r\n```\r\n**Other info / logs**\r\nCould have come up with a better issue title :)", "comments": ["I am able to reproduce the issue on Colab with Tenosrflow 1.14.0. Please find the [gist here](https://colab.research.google.com/drive/1f3m2HkQMO2MczoLjBchlFYyU791rIBP8). Thanks! ", "I don't understand. In @gadagashwini's reproduction the precision difference is 1e-16, which seems to be in line of what you'd expect from double precision addition / subtraction at this difference in order of magnitude.\r\n\r\nIs this something you don't see in numpy but see in tf?", "My bad.. I had not verified the numpy equivalent. I was expecting it to be much lower in magnitude (like 1e-30 or so for float 32 but it is of the order of 1e-7 for float 32 in both tf and np). \r\n\r\nI felt this would be an issue, depending on variable initialization, if we are constantly moving variables between cpu and gpu (every batch) , especially in cases when the tf.Variable doesn't fit on GPU memory.\r\n\r\nYou may close this issue as the behavior is similar to that of numpy.\r\n\r\n", "@sumitpai Closing the issue since it is answered. Thanks!"]}, {"number": 30813, "title": "variable scope is changed by force when reopened (with regard to use Adam)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 1080 ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope('scope'):\r\n    a = tf.get_variable('a', [4,3,2], tf.float32)\r\n    b = tf.get_variable('b', [4,3,2], tf.float32)\r\n    c = tf.reduce_sum(a + b)\r\n   \r\n    d = tf.train.AdamOptimizer()\r\n    #g = d.compute_gradients(c, tf.trainable_variables())\r\n    #h = d.apply_gradients(g)\r\n\r\nwith tf.variable_scope('scope'):\r\n    g = d.compute_gradients(c, tf.trainable_variables())\r\n    h = d.apply_gradients(g)\r\n\r\nprint tf.global_variable()\r\n```\r\n\r\nthe scope of beta1_power and beta2_power is not 'scope' but 'scope_1'.\r\nInstead of reopening the variable_scope, if I run those two commented lines, the scope of beta1_power and beta2_power will be 'scope' not 'scope_1'\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@yanghoonkim ,\r\nI tried executing the same code and faced the following error `AttributeError: 'module' object has no attribute 'global_variable'`.let us know if the same issue is faced by you.Thanks!", "Oh, It was my fault.The last line of the code should be `print tf.global_variables()`", "@yanghoonkim ,\r\nCan you please elaborate  beta1_power ,beta2_power and scope_1.Thanks!", "Please compare the two code blocks below and their printed result.\r\nThe only difference is that I reopened the tf.variable_scope('myscope') and defined 'compute_gradient', and 'apply_gradient' in the reopened scope.\r\n\r\n**The first code**\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope('myscope'):\r\n    a = tf.get_variable('a', [4,3,2], tf.float32)\r\n    b = tf.get_variable('b', [4,3,2], tf.float32)\r\n    c = tf.reduce_sum(a + b)\r\n   \r\n    d = tf.train.AdamOptimizer()\r\n    g = d.compute_gradients(c, tf.trainable_variables())\r\n    h = d.apply_gradients(g)\r\n\r\nprint tf.global_variables()\r\n```\r\n**Result:**\r\n[<tf.Variable 'myscope/a:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/b:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable '**myscope**/beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable '**myscope**/beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'myscope/myscope/a/Adam:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/myscope/a/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/myscope/b/Adam:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/myscope/b/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref>]\r\n\r\n**The second code**\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope('myscope'):\r\n    a = tf.get_variable('a', [4,3,2], tf.float32)\r\n    b = tf.get_variable('b', [4,3,2], tf.float32)\r\n    c = tf.reduce_sum(a + b)\r\n   \r\n    d = tf.train.AdamOptimizer()\r\n\r\nwith tf.variable_scope('myscope'):\r\n    g = d.compute_gradients(c, tf.trainable_variables())\r\n    h = d.apply_gradients(g)\r\n\r\nprint tf.global_variables()\r\n```\r\n**Result:**\r\n[<tf.Variable 'myscope/a:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/b:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable '**myscope_1**/beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable '**myscope_1**/beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'myscope/myscope/a/Adam:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/myscope/a/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/myscope/b/Adam:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/myscope/b/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref>]\r\n\r\nAnd I don't know why the basic scope of beta1_power and beta2_power is changed from 'myscope' to 'myscope_1'", "I was able to replicate the issue with TF version-1.12.Thanks!", "Apologies for the delay in response. \r\nYou may try creating optimizer outside of the variable scope;\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.variable_scope('myscope'):#, reuse=False):\r\n    a = tf.get_variable('a', [4,3,2], tf.float32)\r\n    b = tf.get_variable('b', [4,3,2], tf.float32)\r\n    c = tf.reduce_sum(a + b)\r\n    \r\n#Create optimizer outside of variable scope\r\nd = tf.train.AdamOptimizer()\r\ng = d.compute_gradients(c, tf.trainable_variables())\r\nh = d.apply_gradients(g)\r\n\r\nprint(tf.global_variables())\r\n```\r\nOutput:\r\n```python\r\n[<tf.Variable 'myscope/a:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/b:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'myscope/a/Adam:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/a/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/b/Adam:0' shape=(4, 3, 2) dtype=float32_ref>, <tf.Variable 'myscope/b/Adam_1:0' shape=(4, 3, 2) dtype=float32_ref>]\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30813\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30813\">No</a>\n", "I just want to know why this kind of situation happens and is it solved in the updated version?"]}, {"number": 30812, "title": "Updated speech_commands example to work with TensorFlow 2.0+", "body": "I had trouble running the speech_commands example to work, so after some research I updated the script to work with the latest version of TensorFlow using this tool: https://www.tensorflow.org/beta/guide/upgrade as well as modifying the location of audio_ops. I thought I would open a pull request so others don't have the same issues as I was having and are able to run the example out of the box instead of having to debug version issues. This is my first time contributing.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30812) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30812) for more info**.\n\n<!-- ok -->", "Thanks for the approval @petewarden - I noticed that some of the checks were not successful. Is there anything I need to do?", "@sungkhum Could you please check build failures and resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "@gbaned I tried messing around with it a little, but still not exactly sure what is causing the conflict. On my Mac everything works fine.\r\nIt looks like some of the imports aren't working in other systems:\r\n`from tensorflow.python.ops import audio_ops as contrib_audio\r\n11\r\nImportError: cannot import name 'audio_ops'`\r\nDoes anyone know what should be imported?\r\n\r\nAlso this: https://github.com/tensorflow/tensorflow/issues/13031 might be the reason.", "@sungkhum please do \"git fetch\", \"git merge\", resolve conflicts, and \"git push\" , please follow this instructions [here](https://help.github.com/en/articles/resolving-a-merge-conflict-using-the-command-line) ", "@rthadur I'm still not sure how this can be resolved:\r\n`from tensorflow.python.ops import audio_ops as contrib_audio 11 ImportError: cannot import name 'audio_ops'` Do you know the import name that should be used?", "Thanks for all the fixes @sungkhum!\r\n\r\n> from tensorflow.python.ops import audio_ops as contrib_audio 11 ImportError: cannot import name 'audio_ops'\r\n\r\n`gen_audio_ops` seems to exist. But I think this can use `tf.audio`.\r\n\r\nI'll try to merge this and see if I can fix this in the merge commit.\r\n", "@sungkhum can you please resolve conflicts ?", "> @sungkhum can you please resolve conflicts ?\r\n\r\n@rthadur If you look at my comment to you previously you'll see I asked a question because I wasn't sure how to resolve the conflict (unless you are like a bot or something...). But it looks like @MarkDaoust is going to try and resolve the conflict in the merge commit. Let me know if there is anything else I can do.", "I've resolved the conflicts.\r\n\r\nIt looks like an auto-formatter caused the conflicts. \r\n\r\nIt might be good to merge now.", "Thanks @MarkDaoust !"]}, {"number": 30811, "title": "Saving of BatchNormalization layer fails", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB\r\n\r\n**Describe the current behavior**\r\nWhen I try to save a BatchNormalization layer as in the example code it fails with the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_bn.py\", line 31, in <module>\r\n    tf.saved_model.save(infer, saved_model_dir, signature_dict)\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 840, in save\r\n    meta_graph_def, saveable_view, signatures)\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 536, in _fill_meta_graph_def\r\n    object_map, resource_map, asset_info = saveable_view.map_resources()\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 270, in map_resources\r\n    \"supported.\").format(concrete_function.name, capture))\r\nValueError: Attempted to save a function b'__inference_batch_normalization_layer_call_and_return_conditional_losses_414' which references a symbolic Tensor Tensor(\"batch_normalization_trainable:0\", dtype=bool) that is not a simple constant. This is not supported.\r\n```\r\n\r\n**Describe the expected behavior**\r\nSaving succeeds without error.\r\n\r\n**Code to reproduce the issue**\r\nThe following testcase can be used to reproduce the issue:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass Outer(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.bn = tf.keras.layers.BatchNormalization()\r\n\r\n    def call(self, x, train_bn=False):\r\n        return self.bn(x, training=train_bn)\r\n\r\nclass Infer(tf.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Decorate the inference function with tf.function\r\n        self.infer_ = tf.function(self.infer, input_signature=[\r\n             tf.TensorSpec([1, 64, 64, 8], tf.float32, 'prev_img')])\r\n\r\n        self.outer = Outer()\r\n\r\n    def infer(self, input):\r\n        return self.outer(input, train_bn=False)\r\n\r\n# Create model\r\ninfer = Infer()\r\n\r\n# Save the trained model\r\nsignature_dict = {'infer': infer.infer_}\r\nsaved_model_dir = '/tmp/saved_model'\r\ntf.saved_model.save(infer, saved_model_dir, signature_dict)\r\n```", "comments": ["@olesalscheider I tried executing the code on Colab with Tensorflow 1.14.0. But I did not get any error. Please take a look at gist of [Colab](https://colab.research.google.com/drive/1A4jaDTzJ4M_b8g86FnIaAv_7Dl55JCrK). Thanks!", "Oh, I forgot to mention: The code above used to work with older versions of Tensorflow (I think including 1.14.0). This is a regression in the current master branch (with tf2 API).", "I am able to reproduce the issue with Tensorflow 2.0.0.beta1. Please take a look at [gist here](https://colab.research.google.com/drive/1YgQAH_apOpqzuY_bdXiF8sCYCPvNbRQs). Thanks!", "I am getting the exact same error. Anybody have any way to fix this? I am using the nightly previews.", "I believe this should be fixed in TF 2.0 RC 0. Closing this, but if you are still having issues please reopen this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30811\">No</a>\n"]}, {"number": 30810, "title": "AttributeError: module 'tensorflow' has no attribute 'init_scope'", "body": "**System information**\r\n\r\n**Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\n -No\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\n-Windows\r\n**Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**\r\n-No\r\n**TensorFlow installed from (source or binary):**\r\n-Pip\r\n**TensorFlow version (use command below):** \r\n-Tensorflow-gpu 1.9.0\r\n**Python version:**\r\n-Python 3.6.6\r\n**CUDA/cuDNN version:**\r\n-CUDA 10.0.0\r\n-cuDNN 7.6.1\r\n**GPU model and memory:**\r\n-NVIDIA GeForce GTX 1070 Ti  (8gb dedicated)\r\n\r\n**Describe the current behavior**\r\n - Try to train a model based on  faster_rcnn_nass_coco.config. Get an  - error AttributeError: module 'tensorflow' has no attribute 'init_scope'.\r\nSame code works if I use ssd_mobilenet_v1_pets.config\r\n\r\n**Code to reproduce the issue**\r\n`python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_nass_coco.config`\r\n\r\n\r\n**Other info / logs**\r\n_Traceback (most recent call last):\r\n  File \"train.py\", line 184, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\Vitalie\\Anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Users\\Vitalie\\Anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 250, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"train.py\", line 180, in main\r\n    graph_hook_fn=graph_rewriter_fn)\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\object_detection\\legacy\\trainer.py\", line 291, in train\r\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\slim\\deployment\\model_deploy.py\", line 193, in create_clones\r\n    outputs = model_fn(*args, **kwargs)\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\object_detection\\legacy\\trainer.py\", line 204, in _create_losses\r\n    prediction_dict = detection_model.predict(images, true_image_shapes)\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 821, in predict\r\n    prediction_dict = self._predict_first_stage(preprocessed_inputs)\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 872, in _predict_first_stage\r\n    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1250, in _extract_rpn_feature_maps\r\n    feature_map_shape[2])]))\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\object_detection\\core\\anchor_generator.py\", line 103, in generate\r\n    anchors_list = self._generate(feature_map_shape_list, **params)\r\n  File \"C:\\Users\\Vitalie\\Downloads\\models-master\\research\\object_detection\\anchor_generators\\grid_anchor_generator.py\", line 111, in _generate\r\n    with tf.init_scope():\r\nAttributeError: module 'tensorflow' has no attribute 'init_scope'_", "comments": ["how did solve?", "@VitalieStirbu ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!", "@anush-o,\r\nWhat code snippet? train.py is the file from /models/research/object_detection/legacy folder. I didn't make any changes.  In faster_rcnn_nass_coco.config I changed only NUM_CLASSES to 1 and the path to my data.\r\nI tried to follow the steps from this <a href=\"https://www.youtube.com/watch?v=K_mFnvzyLvc&list=PLQVvvaa0QuDcNK5GeCQnxYnSSaar2tpku&index=3\">video</a>\r\n", "I have the same Error\r\n\r\n**System information**\r\n\r\n**Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\n-No\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\n-Windows 10\r\n**Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**\r\n-No\r\n**TensorFlow installed from (source or binary):**\r\n-Pip\r\n**TensorFlow version (use command below):**\r\n-Tensorflow 1.5.0\r\n**Python version:**\r\n-Python 3.6.8\r\n**CUDA/cuDNN version:**\r\n-No\r\n**GPU model and memory:**\r\n-No\r\n\r\nDescribe the current behavior\r\n\r\nTry to train a model based on faster_rcnn_nass_coco.config. Get an - error AttributeError: module 'tensorflow' has no attribute 'init_scope'.\r\nSame code works if I use ssd_mobilenet_v1_pets.config\r\nCode to reproduce the issue\r\npython train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_nass_coco.config\r\n\r\nOther info / logs\r\nWARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\r\nWARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\aliff\\AppData\\Local\\Temp\\tmp1_qeig4v\r\nWARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x000001FAB28D5158>) includes params argument, but params are not passed to Estimator.\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nTraceback (most recent call last):\r\n  File \"model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\aliff\\Desktop\\ObjectIdentifier\\env\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"model_main.py\", line 105, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"C:\\Users\\aliff\\Desktop\\ObjectIdentifier\\env\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 432, in train_and_evaluate\r\n    executor.run_local()\r\n  File \"C:\\Users\\aliff\\Desktop\\ObjectIdentifier\\env\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\", line 611, in run_local\r\n    hooks=train_hooks)\r\n  File \"C:\\Users\\aliff\\Desktop\\ObjectIdentifier\\env\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 314, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\aliff\\Desktop\\ObjectIdentifier\\env\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 743, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"C:\\Users\\aliff\\Desktop\\ObjectIdentifier\\env\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 725, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\Users\\aliff\\Desktop\\models-master\\models-master\\research\\object_detection\\model_lib.py\", line 301, in model_fn\r\n    features[fields.InputDataFields.true_image_shape])\r\n  File \"C:\\Users\\aliff\\Desktop\\models-master\\models-master\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 822, in predict\r\n    prediction_dict = self._predict_first_stage(preprocessed_inputs)\r\n  File \"C:\\Users\\aliff\\Desktop\\models-master\\models-master\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 873, in _predict_first_stage\r\n    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)\r\n  File \"C:\\Users\\aliff\\Desktop\\models-master\\models-master\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1252, in _extract_rpn_feature_maps\r\n    feature_map_shape[2])]))\r\n  File \"C:\\Users\\aliff\\Desktop\\models-master\\models-master\\research\\object_detection\\core\\anchor_generator.py\", line 108, in generate\r\n    anchors_list = self._generate(feature_map_shape_list, **params)\r\n  File \"C:\\Users\\aliff\\Desktop\\models-master\\models-master\\research\\object_detection\\anchor_generators\\grid_anchor_generator.py\", line 111, in _generate\r\n    with tf.init_scope():\r\nAttributeError: module 'tensorflow' has no attribute 'init_scope'", "I'm having the same error, \r\n\r\n(tensorflow_gpu) C:\\TensorFlow\\workspace\\training_demo>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config\r\nWARNING:tensorflow:From C:\\Users\\THIS PC\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py:125: main (from __main__) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse object_detection/model_main.py.\r\nWARNING:tensorflow:From C:\\TensorFlow\\models\\research\\object_detection\\legacy\\trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 184, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\THIS PC\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"C:\\Users\\THIS PC\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 250, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"train.py\", line 180, in main\r\n    graph_hook_fn=graph_rewriter_fn)\r\n  File \"C:\\TensorFlow\\models\\research\\object_detection\\legacy\\trainer.py\", line 291, in train\r\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\r\n  File \"C:\\TensorFlow\\models\\research\\slim\\deployment\\model_deploy.py\", line 193, in create_clones\r\n    outputs = model_fn(*args, **kwargs)\r\n  File \"C:\\TensorFlow\\models\\research\\object_detection\\legacy\\trainer.py\", line 204, in _create_losses\r\n    prediction_dict = detection_model.predict(images, true_image_shapes)\r\n  File \"C:\\TensorFlow\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 822, in predict\r\n    prediction_dict = self._predict_first_stage(preprocessed_inputs)\r\n  File \"C:\\TensorFlow\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 873, in _predict_first_stage\r\n    image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)\r\n  File \"C:\\TensorFlow\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1252, in _extract_rpn_feature_maps\r\n    feature_map_shape[2])]))\r\n  File \"C:\\TensorFlow\\models\\research\\object_detection\\core\\anchor_generator.py\", line 108, in generate\r\n    anchors_list = self._generate(feature_map_shape_list, **params)\r\n  File \"C:\\TensorFlow\\models\\research\\object_detection\\anchor_generators\\grid_anchor_generator.py\", line 111, in _generate\r\n    with tf.init_scope():\r\nAttributeError: module 'tensorflow' has no attribute 'init_scope'\r\n\r\nhelp me ASAP, ", "@VitalieStirbu This is more related to `TF models` repository. Please post it in https://github.com/tensorflow/models/issues. Thanks!", "@aidenmj ,  @alfdnl  this issue was resolved, please check this <a href=\"https://github.com/tensorflow/hub/issues/324#issuecomment-513847409\">link</a>", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30810\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30810\">No</a>\n", "> how did solve?\r\nI solved delete the lines 174 till 178 of ../object_detection/utils/variables_helper.py\r\n\r\n\r\n```\r\ndef get_global_variables_safely():\r\n  \"\"\"If not executing eagerly, returns tf.global_variables().\r\n\r\n  Raises a ValueError if eager execution is enabled,\r\n  because the variables are not tracked when executing eagerly.\r\n\r\n  If executing eagerly, use a Keras model's .variables property instead.\r\n\r\n  Returns:\r\n    The result of tf.global_variables()\r\n  \"\"\"\r\n  # with tf.init_scope():\r\n  #   if tf.executing_eagerly():\r\n  #     raise ValueError(\"Global variables collection is not tracked when \"\r\n  #                      \"executing eagerly. Use a Keras model's `.variables` \"\r\n  #                      \"attribute instead.\")\r\n  return tf.global_variables()\r\n\r\n```\r\n", "I solved this problem in models/research/object_detection/export_inference_graph.py with tf1.8. I add these codes in the head of object_detection/anchor_genetators/grid_anchor_generator.py:\r\n\r\n```\r\nfrom tensorflow.python.util.tf_export import tf_export\r\nfrom tensorflow.python.util import tf_contextlib\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.eager import tape\r\n\r\n@tf_export(\"init_scope\")\r\n@tf_contextlib.contextmanager\r\ndef init_scope():\r\nif context.executing_eagerly():\r\n    with tape.stop_recording():\r\n        yield\r\nelse:\r\n    default_graph = tf.get_default_graph()\r\n    scope = default_graph.get_name_scope()\r\n    if scope and scope[-1] != \"/\"\r\n        scope = scope + \"/\"\r\n    inner_device_stack = default_graph._device_function_stack\r\n    outer_context = None\r\n    for stack_entry in reversed(context.context().context_switches.stack):\r\n        if not stack_entry.is_building_function:\r\n            outer_context = stack_entry.enter_context_fn\r\n            break\r\n\r\n    if outer_context is None:\r\n        outer_context = tf.Graph().as_default\r\n    if outer_context is None:\r\n        raise RuntimeError(\"All graphs are building functions, and no eager context was previously active\")\r\n    outer_graph = None\r\n    outer_device_stack = None\r\n    try:\r\n        with outer_context(), tf.name_scope(scope), tf.control_dependencies(None), tape.stop_recording():\r\n            if not context.executing_eagerly():\r\n                outer_graph = tf.get_default_graph()\r\n                outer_device_stack = outer_graph._device_function_stack\r\n                outer_graph._device_function_stack = inner_device_stack\r\n            yield\r\n    finally:\r\n        if outer_graph is not None:\r\n            outer_graph._device_function_stack = outer_device_stack\r\n```\r\nThen, instead tf.init_scope() with init_scope()\r\n\r\nI can't copy code from computer of company, so, i press every character by key board. Maybe it will have some mistakes in format.\r\nI have verified that these modified codes is right in  ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03 pretrained model.\r\nIf you have find some questions of this method, please contract me by my github.\r\n", "at lease give correct indent code", "\r\n(object_dection) C:\\Users\\pabhi\\OneDrive\\Desktop\\object\\workspace\\training_demo>python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/pipeline.config --trained_checkpoint_prefix training/model.ckpt-936 --output_directory inference_graph\r\nC:\\Users\\pabhi\\Anaconda3\\envs\\object_dection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\pabhi\\Anaconda3\\envs\\object_dection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\pabhi\\Anaconda3\\envs\\object_dection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\pabhi\\Anaconda3\\envs\\object_dection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\pabhi\\Anaconda3\\envs\\object_dection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\pabhi\\Anaconda3\\envs\\object_dection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nTraceback (most recent call last):\r\n  File \"export_inference_graph.py\", line 162, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\pabhi\\Anaconda3\\envs\\object_dection\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"export_inference_graph.py\", line 158, in main\r\n    write_inference_graph=FLAGS.write_inference_graph)\r\n  File \"C:\\Users\\pabhi\\OneDrive\\Desktop\\object\\models\\research\\object_detection\\exporter.py\", line 510, in export_inference_graph\r\n    write_inference_graph=write_inference_graph)\r\n  File \"C:\\Users\\pabhi\\OneDrive\\Desktop\\object\\models\\research\\object_detection\\exporter.py\", line 413, in _export_inference_graph\r\n    graph_hook_fn=graph_hook_fn)\r\n  File \"C:\\Users\\pabhi\\OneDrive\\Desktop\\object\\models\\research\\object_detection\\exporter.py\", line 380, in build_detection_graph\r\n    output_collection_name=output_collection_name)\r\n  File \"C:\\Users\\pabhi\\OneDrive\\Desktop\\object\\models\\research\\object_detection\\exporter.py\", line 356, in _get_outputs_from_inputs\r\n    output_tensors, true_image_shapes)\r\n  File \"C:\\Users\\pabhi\\OneDrive\\Desktop\\object\\models\\research\\object_detection\\meta_architectures\\ssd_meta_arch.py\", line 783, in postprocess\r\n    with tf.init_scope():\r\nAttributeError: module 'tensorflow' has no attribute 'init_scope'", "can any one solve this\r\n", "what is your goal TensorFlow object detection API or image classification?\n\n>\n", "@ChenSi521 Can you please provide step-by-step explaination?\r\n", "> @ChenSi521 Can you please provide step-by-step explaination?\r\n\r\ndid u solve it ?", "Yes but not by the way you mentioned, I just installed Tensorflow 1.12 and it solved.\r\n"]}, {"number": 30809, "title": "Add keras.layers.Layer.add_callback", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): `1.13.1`\r\n- Are you willing to contribute it (Yes/No): maybe, but I won't have time in the next week or two\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, custom Keras layers have `add_loss` and `add_metric`. It would be nice if layers could also contribute their own callback functions as well. \r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would add a method `add_callback` to the `keras.layers.Layer` class. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nThe main use case that I see this for is layers that have some warm start or epoch-dependent parameters. By adding them directly inside the layer, it keeps layers completely composable. \r\n\r\nCurrently, we need to do something like this:\r\n\r\n```python\r\ninput = x\r\nmy_layer = MyLayer(...)\r\nx = my_layer(x)\r\n...\r\nmodel = Model(input, x)\r\nmodel.compile(...)\r\nmodel.fit(..., callbacks=[WarmStartCallback(my_layer.alpha)])\r\n```\r\n\r\nBut it would be much cleaner to be able to do this:\r\n\r\n```python\r\nclass MyLayer(Layer):\r\n    def __init__(self, warm_start=True):\r\n        self.alpha = tf.Variable(...)\r\n        if warm_start:\r\n            self.add_callback(WarmStartCallback(self.alpha))\r\n\r\n\r\ninput = x\r\nx = MyLayer(...)(x)\r\n...\r\nmodel = Model(input, x)\r\nmodel.compile(...)\r\nmodel.fit(...)\r\n```\r\n\r\n**Any Other info.**\r\n\r\nOff hand, here's a sketch of major changes that I see being needed:\r\n\r\n```python\r\n# tf.keras.engine.base_layer\r\nclass Layer:\r\n    def __init__(self, ...):\r\n        ...\r\n        self._metrics = []\r\n\r\n        # +\r\n        self._callbacks = []\r\n        ...\r\n\r\n    @property\r\n    def callbacks(self):\r\n        return self._callbacks + self._gather_children_attribute('callbacks')\r\n\r\n    def add_callback(self, callback):\r\n        # maybe some checks, conversions, etc\r\n        ...\r\n        self._callbacks.append(callback)\r\n\r\n# tf.keras.engine.training\r\nclass Model:\r\n    def fit(..., callbacks=None, ...):\r\n        ...\r\n        # +\r\n        layer_callbacks = self._gather_children_attribute('callbacks') \r\n        callbacks = layer_callbacks + callbacks if callbacks else layer_callbacks\r\n\r\n        func = self._select_training_loop(x)\r\n        ...\r\n\r\n```", "comments": ["@beasteers -- thanks for the request. At this time, we are having trouble coming up with a way to do this that doesn't introduce a lot of complexity into the code, so we will close this unless we come up with a better approach."]}, {"number": 30808, "title": "Serialization of keras object fails if called with different input sizes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB\r\n\r\n**Describe the current behavior**\r\nWhen I try to save a function of tf.Module as saved_model that calls another function with different input shapes, it fails with the following error:\r\n\r\n```\r\nW0717 17:37:44.384423 139641189812032 save.py:129] Skipping full serialization of object <__main__.Outer object at 0x7f009a2e94e0>, because an error occurred while tracing layer functions. Error message: in converted code:                                                                                               \r\n                                                                                                                                                                                                                                                                                                                             \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:539 call_and_return_conditional_losses  *                                                                                                                                                                      \r\n        return layer_call(inputs), layer.get_losses_for(inputs)                                                              \r\n    test_signature.py:32 call  *\r\n        return self.inner(x, dummy=dummy), self.inner(x_small, dummy=dummy)                                                         \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:708 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py:48 wrapped_call\r\n        outputs, losses = call_fn(inputs)                                                                                           \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:506 __call__\r\n        self.call_collection.add_trace(*args, **kwargs)                                                                                \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:467 add_trace\r\n        fn.original_get_concrete_function(*args, **kwargs)                                                                         \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py:515 original_get_concrete_function\r\n        return super(LayerCall, self).get_concrete_function(*args, **kwargs)                                                            \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:692 get_concrete_function\r\n        concrete = self._stateful_fn.get_concrete_function(*args, **kwargs)                                                        \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py:1750 get_concrete_function\r\n        (str(args), str(self.input_signature)))                                                                                      \r\n                            \r\n    ValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'down/Identity:0' shape=(None, 32, 32, 8) dtype=float32>,)), input_signature ((TensorSpec(shape=(None, 64, 64, 8), dtype=tf.float32, name='input_1'),))\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe model can be saved successfully.\r\n\r\n**Code to reproduce the issue**\r\nThe following testcase allows to reproduce the issue:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass Inner(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.conv1 = tf.keras.layers.Conv2D(8,\r\n            (3, 3),\r\n            kernel_initializer=tf.keras.initializers.he_normal(),\r\n            padding='same',\r\n            name='conv1')\r\n\r\n    def call(self, x, dummy=False):\r\n        x = self.conv1(x)\r\n        return x\r\n\r\nclass Outer(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.down = tf.keras.layers.Conv2D(8,\r\n            (3, 3),\r\n            strides=(2, 2),\r\n            kernel_initializer=tf.keras.initializers.he_normal(),\r\n            padding='same',\r\n            name='down')\r\n\r\n        self.inner = Inner()\r\n\r\n    def call(self, x, dummy=False):\r\n        x_small = self.down(x)\r\n        return self.inner(x, dummy=dummy), self.inner(x_small, dummy=dummy)\r\n\r\nclass Infer(tf.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Decorate the inference function with tf.function\r\n        self.infer_ = tf.function(self.infer, input_signature=[\r\n             tf.TensorSpec([1, 64, 64, 8], tf.float32, 'prev_img')])\r\n\r\n        self.outer = Outer()\r\n\r\n    def infer(self, input):\r\n        return self.outer(input, False)\r\n\r\n# Create model\r\ninfer = Infer()\r\n\r\n# Save the trained model\r\nsignature_dict = {'infer': infer.infer_}\r\nsaved_model_dir = '/tmp/saved_model'\r\ntf.saved_model.save(infer, saved_model_dir, signature_dict)\r\n```", "comments": ["@olesalscheider I tried reproducing the issue on Colab with Tensorflow version 1.14.0. I didn't receive any error. Please have a look at [Colab](https://colab.research.google.com/drive/18DPNelOKIdJiXq2q8-oNHiTadqcuwRY-) link. Let us know is this expected behavior.Thanks!  ", "This is a regression in the current master branch (with tf2 API). The code above used to work with older Tensorflow versions including 1.14.0.\r\n\r\nI had to request access to the Colab link (because my Google account has a different email address?). I will look at it once I have access.", "I could reproduce the issue with tensorflow 2.0.0.beta1. Please find the [gist here](https://colab.research.google.com/drive/14_RzRKLpMb1OBQuk5LEOxAI4INA1yUwE). Thanks!", "@olesalscheider I think the `ValueError` is correct as there is a mismatch in the shape. If you use strides=(1,1), then shapes are same and there is not error. Please throw little more details on why you think this as a bug? Thanks!", "You mean there is a mismatch in the input to Inner.call()? Yes, that is true. But this just contains a fully convolutional layer which can operate on inputs of different sizes.\r\n\r\nAlso, using the model works fine: Executing for example `infer.infer_(tf.random.uniform([1, 64, 64, 8]))` instead of saving the model runs without error. If the shape mismatch was a problem it should not run. I expect that I can save a model if I can run it?", "Thanks for submitting this report, I've submitted a fix, which I'll request to be cherrypicked into 2.0.", "I think the issue was resolved. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/65552c805709d58ec94c99b2143e285a/tf_30808_savedmodel.ipynb). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30808\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30808\">No</a>\n"]}, {"number": 30807, "title": "Memory Leak in `tf.estimator.experimental.InMemoryEvaluatorHook`", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Red Hat Enterprise Linux 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.8\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Tesla K80 11441MiB\r\n- **Exact command to reproduce**: python min.py\r\n\r\n### Describe the problem\r\nThere seems to be some kind of memory leak in `tf.estimator.experimental.InMemoryEvaluatorHook`\r\n\r\nMy example is a little bit messy - because I had to copy over the `InMemoryEvaluatorHook` from a future version of TensorFlow and modify it very slightly (because I'm not able to upgrade) So technically, this might be a bug with TF1.14.0 which is where I copied the source code from.  \r\n\r\nEssentially, if I feed a large constant tensor (a 3.6GB embedding in my case) in the scaffold of the training model, CPU memory usage seems to balloon, expanding every time an evaluation is run, eventually exceeding 64GB.  I checked memory usage simply by using `htop` and looking for my python instance.\r\n\r\nAre there are any alternatives to what I'm doing here that would result in reasonable memory usage?\r\n\r\n### Source code / logs\r\n```python\r\n# min.py\r\nimport tensorflow as tf\r\nfrom hooks import InMemoryEvaluatorHook\r\nimport numpy as np\r\n\r\ndef train_input_fn():\r\n  dataset = tf.data.Dataset.range(100)\r\n  # Make sequence data\r\n  dataset = dataset.map(lambda x: {'x': [x]*10})\r\n  dataset = dataset.repeat(100)\r\n  return dataset\r\n\r\ndef eval_input_fn():\r\n  dataset = tf.data.Dataset.range(100)\r\n  # Make sequence data\r\n  dataset = dataset.map(lambda x: {'x': [x]*10})\r\n  return dataset\r\n\r\ndef model_fn(features, labels, mode, params):\r\n  seq = features['x']\r\n  with tf.device('/cpu:0'):\r\n    arr = np.random.rand(3000000, 300)\r\n    var = tf.get_variable('big', arr.shape, trainable=False)\r\n    emb = tf.nn.embedding_lookup(var, seq)\r\n  logits = tf.layers.dense(emb, 2)\r\n  # Don't care about loss but have to provide something\r\n  loss = tf.reduce_mean(logits)\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    def init_fn(scaffold, sess):\r\n      sess.run(var.initializer, {var.initial_value: arr})\r\n    trainable_vars = tf.trainable_variables()\r\n    saver = tf.train.Saver(var_list=trainable_vars)\r\n    scaffold = tf.train.Scaffold(init_fn=init_fn, saver=saver)\r\n    global_step = tf.train.get_or_create_global_step()\r\n    optimizer = tf.train.GradientDescentOptimizer(0.1)\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n    output_spec = tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      loss=loss,\r\n      scaffold=scaffold,\r\n      train_op=train_op)\r\n  elif mode == tf.estimator.ModeKeys.EVAL:\r\n    output_spec = tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      loss=loss)\r\n  return output_spec\r\n\r\nestimator = tf.estimator.Estimator(model_fn=model_fn)\r\nevaluator = InMemoryEvaluatorHook(\r\n    estimator,\r\n    eval_input_fn,\r\n    every_n_iter=300)\r\nestimator.train(train_input_fn, hooks=[evaluator])\r\n```\r\n```python\r\n# hooks.py\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import control_flow_ops\r\nfrom tensorflow.python.ops import state_ops\r\nfrom tensorflow.python.training import training\r\n\r\n\r\nclass InMemoryEvaluatorHook(training.SessionRunHook):\r\n  \"\"\"Hook to run evaluation in training without a checkpoint.\r\n  Example:\r\n  python\r\n  def train_input_fn():\r\n    ...\r\n    return train_dataset\r\n  def eval_input_fn():\r\n    ...\r\n    return eval_dataset\r\n  estimator = tf.estimator.DNNClassifier(...)\r\n  evaluator = tf.estimator.experimental.InMemoryEvaluatorHook(\r\n      estimator, eval_input_fn)\r\n  estimator.train(train_input_fn, hooks=[evaluator])\r\n  \r\n  Current limitations of this approach are:\r\n  * It doesn't support multi-node distributed mode.\r\n  * It doesn't support saveable objects other than variables (such as boosted\r\n    tree support)\r\n  * It doesn't support custom saver logic (such as ExponentialMovingAverage\r\n    support)\r\n\r\n  COPIED FROM TENSORFLOW REPO (FUTURE VERSION 1.14.0), PERMALINK BELOW\r\n  https://github.com/tensorflow/estimator/blob/faa1058f2664b952c246e9097e898fab8042b0ce/tensorflow_estimator/python/estimator/hooks/hooks.py#L66\r\n  \"\"\"\r\n\r\n  def __init__(self,\r\n               estimator,\r\n               input_fn,\r\n               steps=None,\r\n               hooks=None,\r\n               name=None,\r\n               every_n_iter=100):\r\n    \"\"\"Initializes a `InMemoryEvaluatorHook`.\r\n    Args:\r\n      estimator: A `tf.estimator.Estimator` instance to call evaluate.\r\n      input_fn:  Equivalent to the `input_fn` arg to `estimator.evaluate`. A\r\n        function that constructs the input data for evaluation.\r\n        See [Createing input functions](\r\n        https://tensorflow.org/guide/premade_estimators#create_input_functions)\r\n        for more information. The function should construct and return one of\r\n        the following:\r\n          * A 'tf.data.Dataset' object: Outputs of `Dataset` object must be a\r\n            tuple (features, labels) with same constraints as below.\r\n          * A tuple (features, labels): Where `features` is a `Tensor` or a\r\n            dictionary of string feature name to `Tensor` and `labels` is a\r\n            `Tensor` or a dictionary of string label name to `Tensor`. Both\r\n            `features` and `labels` are consumed by `model_fn`. They should\r\n            satisfy the expectation of `model_fn` from inputs.\r\n      steps: Equivalent to the `steps` arg to `estimator.evaluate`.  Number of\r\n        steps for which to evaluate model. If `None`, evaluates until `input_fn`\r\n        raises an end-of-input exception.\r\n      hooks: Equivalent to the `hooks` arg to `estimator.evaluate`. List of\r\n        `SessionRunHook` subclass instances. Used for callbacks inside the\r\n        evaluation call.\r\n      name:  Equivalent to the `name` arg to `estimator.evaluate`. Name of the\r\n        evaluation if user needs to run multiple evaluations on different data\r\n        sets, such as on training data vs test data. Metrics for different\r\n        evaluations are saved in separate folders, and appear separately in\r\n        tensorboard.\r\n      every_n_iter: `int`, runs the evaluator once every N training iteration.\r\n    Raises:\r\n      ValueError: if `every_n_iter` is non-positive or it's not a single machine\r\n        training\r\n    \"\"\"\r\n    if every_n_iter is None or every_n_iter <= 0:\r\n      raise ValueError('invalid every_n_iter=%s.' % every_n_iter)\r\n    if (estimator.config.num_ps_replicas > 0 or\r\n        estimator.config.num_worker_replicas > 1):\r\n      raise ValueError(\r\n          'InMemoryEvaluator supports only single machine (aka Local) setting.')\r\n    self._estimator = estimator\r\n    self._input_fn = input_fn\r\n    self._steps = steps\r\n    self._name = name\r\n    self._every_n_iter = every_n_iter\r\n    self._eval_dir = os.path.join(self._estimator.model_dir, 'eval'\r\n                                  if not name else 'eval_' + name)\r\n\r\n    self._graph = None\r\n    self._hooks = _check_hooks_type(hooks)\r\n    self._hooks.extend(self._estimator._convert_eval_steps_to_hooks(steps))\r\n    self._timer = training.SecondOrStepTimer(every_steps=every_n_iter)\r\n\r\n  def begin(self):\r\n    \"\"\"Build eval graph and restoring op.\"\"\"\r\n    self._timer.reset()\r\n    self._iter_count = 0\r\n    self._graph = ops.Graph()\r\n    with self._graph.as_default():\r\n      (self._scaffold, self._update_op, self._eval_dict,\r\n       self._all_hooks) = self._estimator._evaluate_build_graph(\r\n           self._input_fn, self._hooks, checkpoint_path=None)\r\n\r\n      if self._scaffold.saver is not None:\r\n        raise ValueError('InMemoryEvaluator does not support custom saver')\r\n      if self._scaffold.init_fn is not None:\r\n        raise ValueError('InMemoryEvaluator does not support custom init_fn')\r\n\r\n      self._var_name_to_eval_var = {\r\n          v.name: v for v in ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)\r\n      }\r\n      self._var_name_to_placeholder = {\r\n          v.name: array_ops.placeholder(v.dtype)\r\n          for v in ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)\r\n      }\r\n\r\n  def after_create_session(self, session, coord):  # pylint: disable=unused-argument\r\n    \"\"\"Does first run which shows the eval metrics before training.\"\"\"\r\n    if ops.get_collection(ops.GraphKeys.SAVEABLE_OBJECTS):\r\n      raise ValueError(\r\n          'InMemoryEvaluator does not support saveables other than global '\r\n          'variables.')\r\n    self._var_name_to_train_var = {\r\n        v.name: v for v in ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)\r\n    }\r\n    var_names_to_transfer = set(self._var_name_to_placeholder.keys()) & set(\r\n        self._var_name_to_train_var.keys())\r\n    # Filter training var names that do not exist in evaluation\r\n    self._var_name_to_train_var = {\r\n        v_name: self._var_name_to_train_var[v_name]\r\n        for v_name in var_names_to_transfer\r\n    }\r\n    # Filter eval var names that do not exist in training\r\n    self._var_name_to_eval_var = {\r\n        v_name: self._var_name_to_eval_var[v_name]\r\n        for v_name in var_names_to_transfer\r\n    }\r\n\r\n    with self._graph.as_default():\r\n      self._var_feed_op = control_flow_ops.group([\r\n          state_ops.assign(self._var_name_to_eval_var[v_name],\r\n                           self._var_name_to_placeholder[v_name])\r\n          for v_name in var_names_to_transfer\r\n      ])\r\n\r\n    self._evaluate(session)\r\n\r\n  def _evaluate(self, train_session):\r\n    var_name_to_value = train_session.run(self._var_name_to_train_var)\r\n    placeholder_to_value = {\r\n        self._var_name_to_placeholder[v_name]: var_name_to_value[v_name]\r\n        for v_name in var_name_to_value\r\n    }\r\n\r\n    def feed_variables(scaffold, session):\r\n      del scaffold\r\n      session.run(self._var_feed_op, feed_dict=placeholder_to_value)\r\n\r\n    scaffold = training.Scaffold(\r\n        init_fn=feed_variables, copy_from_scaffold=self._scaffold)\r\n\r\n    with self._graph.as_default():\r\n      self._estimator._evaluate_run(\r\n          checkpoint_path=None,\r\n          scaffold=scaffold,\r\n          update_op=self._update_op,\r\n          eval_dict=self._eval_dict,\r\n          all_hooks=self._all_hooks,\r\n          output_dir=self._eval_dir)\r\n\r\n    self._timer.update_last_triggered_step(self._iter_count)\r\n\r\n  def after_run(self, run_context, run_values):  # pylint: disable=unused-argument\r\n    \"\"\"Runs evaluator.\"\"\"\r\n    self._iter_count += 1\r\n    if self._timer.should_trigger_for_step(self._iter_count):\r\n      self._evaluate(run_context.session)\r\n\r\n  def end(self, session):  # pylint: disable=unused-argument\r\n    \"\"\"Runs evaluator for final model.\"\"\"\r\n    self._evaluate(session)\r\n\r\n\r\ndef _check_hooks_type(hooks):\r\n  \"\"\"Returns hooks if all are `SessionRunHook`, raises TypeError otherwise.\"\"\"\r\n  hooks = list(hooks or [])\r\n  for h in hooks:\r\n    if not isinstance(h, training.SessionRunHook):\r\n      raise TypeError('Hooks must be a SessionRunHook, given: {}'.format(h))\r\n  return hooks\r\n```", "comments": ["We do not currently have the time to look into this bug. Marking as open for contributions if anyone wants to help debug and resolve.", "@corynezin We see that you are using old version of tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30807\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30807\">No</a>\n"]}, {"number": 30806, "title": "Inference on Larger Data Size on Edge ", "body": "**System information**\r\n- Have I written custom code (Custom Code using resources and my own design):\r\n- OS Platform and Distribution (Linux Ubuntu 16.04):\r\n- Mobile device (trying inference on google coral dev board)\r\n- TensorFlow installed from (pip3 install tensorflow==2.0.0-beta1):\r\n- TensorFlow version (2.0.0-beta1):\r\n- Python version: 3.6.9\r\n- Bazel version (not used):\r\n- GCC/Compiler version (not used):\r\n- CUDA/cuDNN version: not used\r\n- GPU model and memory: not used \r\n- Run On CPU \r\n\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                8\r\nOn-line CPU(s) list:   0-7\r\nThread(s) per core:    2\r\nCore(s) per socket:    4\r\nSocket(s):             1\r\nNUMA node(s):          1\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 94\r\nModel name:            Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz\r\nStepping:              3\r\nCPU MHz:               800.210\r\nCPU max MHz:           3500.0000\r\nCPU min MHz:           800.0000\r\nBogoMIPS:              5183.86\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              6144K\r\n\r\n32 GB RAM\r\n\r\n```\r\n# The full neural network code!\r\n###############################\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.layers import UpSampling1D\r\n\r\n#Reference:https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb\r\ndef UpSamplingCustom2D(scale=(2, 2)):\r\n  if isinstance(scale, int):\r\n    scale = (scale, scale)\r\n\r\n  def upsampling(x):\r\n    shape = x.shape\r\n    print(\"Upsampling : \", shape)\r\n    x = keras.layers.Concatenate(-2)([x] * scale[0])\r\n    x = keras.layers.Reshape([shape[1] * scale[0], shape[2], shape[3]])(x)\r\n    x = keras.layers.Concatenate(-1)([x] * scale[1])\r\n    x = keras.layers.Reshape([shape[1] * scale[0], shape[2] * scale[1], shape[3]])(x)\r\n    return x\r\n\r\n  return upsampling\r\n\r\n\r\nimage_size = 28\r\noutput_image_size = image_size * 2\r\n\r\ntrain_images = np.random.rand(10,image_size,image_size,3)\r\ntrain_images_labels = np.random.rand(10,output_image_size,output_image_size,3)\r\n\r\n\r\ntest_images = np.random.rand(10,image_size,image_size,3)\r\ntest_images_labels = np.random.rand(10,output_image_size,output_image_size,3)\r\n\r\ninputs = keras.layers.Input(shape=(image_size, image_size, 3))\r\nx = keras.layers.Dense(image_size, activation='relu')(inputs)\r\nx = keras.layers.Dense(image_size, activation='relu')(x)\r\nx = UpSamplingCustom2D()(x)\r\ndecoded = keras.layers.Dense(3, activation='relu')(x)\r\nmodel = keras.models.Model(inputs, decoded)\r\n\r\n#Compile the model.\r\nmodel.compile(\r\n  optimizer='adam',\r\n  loss='binary_crossentropy',\r\n)\r\n\r\n#Train the model.\r\nmodel.fit(\r\n  train_images,\r\n  train_images_labels,\r\n  epochs=1,\r\n  batch_size=32,\r\n)\r\n\r\n#Evaluate the model.\r\nmodel.evaluate(\r\n  test_images,\r\n  test_images_labels\r\n)\r\n\r\n\r\n\r\n#Save the model to disk.\r\nmodel.save_weights('model.h5')\r\n\r\n#Load the model from disk later using:\r\nmodel.load_weights('model.h5')\r\n\r\n#Predict on the first 5 test images.\r\npredictions = model.predict(test_images)\r\n\r\nprint(predictions[0].shape)\r\n\r\n\r\ndef representative_dataset_gen():\r\n  for i in range(5):\r\n    yield [train_images[i: i + 1].astype(np.float32)]\r\n\r\nkeras_file = \"upsampling2d.h5\"\r\ntf.keras.models.save_model(model, keras_file)\r\n\r\n#Convert to TensorFlow Lite model.\r\nif (tf.__version__ == '1.14.0'):\r\n  converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nif (tf.__version__ == '2.0.0-beta1'):\r\n  converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\ntflite_fname = \"upsampling2d_\" + str(tf.__version__) + \".tflite\"\r\nopen(tflite_fname, \"wb\").write(tflite_model)\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=tflite_fname)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_detail = interpreter.get_input_details()[0]\r\noutput_detail = interpreter.get_output_details()[0]\r\n\r\nprint(\"Input Details : \", input_detail)\r\nprint(\"Output Details : \", output_detail)\r\n\r\ndef quantize(real_value):\r\n  std, mean = input_detail['quantization']\r\n  return (real_value / std + mean).astype(np.uint8)\r\n\r\n\r\nsample_input = quantize(test_images[0]).reshape(input_detail['shape'])\r\nprint(\"Sample Input Shape : \", sample_input.shape)\r\nprint(\"Inference Sample Input Shape : \", sample_input.shape)\r\n\r\ninterpreter.set_tensor(input_detail['index'], sample_input)\r\ninterpreter.invoke()\r\n\r\n#original_image = test_images[0].reshape((28, 28))\r\npred_original_model = model.predict(test_images[:1])\r\npred_quantized_model = interpreter.get_tensor(output_detail['index'])\r\n\r\nprint(\"Prediction : \",pred_quantized_model.shape)\r\n```\r\n\r\nBecause of quantization issue for UpSampling2D, I used a snippet from \r\n`https://colab.research.google.com/gist/ohtaman/c1cf119c463fd94b0da50feea320ba1e/edgetpu-with-keras.ipynb`\r\n\r\nMy issue is when I use image sizes from 28 till 256 (I just used powers of 2 and used 28 just to check), it worked fine for inferencing. I tested inferencing on the CPU itself using the interpreter package. But 512 or anything above freezes the machine and gives the following error. \r\n\r\n`2019-07-17 08:33:18.567609: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 5368709120 exceeds 10% of system memory.\r\n2019-07-17 08:33:20.626046: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 5368709120 exceeds 10% of system memory.\r\n2019-07-17 08:33:26.517300: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 10737418240 exceeds 10% of system memory.\r\n2019-07-17 08:33:27.836867: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 21474836480 exceeds 10% of system memory.\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n`\r\nIs there a limit for the array size that can be used at inferencing after making the input array flatten. Or am I doing something wrong here? \r\n\r\n\r\n", "comments": ["@ravikyram @liyunlu0618 \r\n\r\nWere you able to follow this issue? For now, I use a workaround and it adds unnecessary complication to our system. I just need to clarify this before we design the production model. \r\nIs this a known issue or something else is wrong in my end?", "@vibhatha Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "Yes it is. I checked with TF2.0rc versions. Couldn\u2019t check with the stable release. I will also check that. Is it possible for you to test this as well?", "@vibhatha \r\nCould you please let us know if this issue still persist.", "This is a 7-month-old issue. At that time, I solved this by feeding smaller images and stitching this up. \r\nI haven't looked into this after that time. Sorry, I cannot confirm it now. I no longer work on this project. \r\n", "@vibhatha\r\nplease confirm if we can move this issue to closed status", "Yes, please close the issue.", "Closing the issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30806\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30806\">No</a>\n"]}, {"number": 30805, "title": "Make the output format the same as printf above line.", "body": "", "comments": []}, {"number": 30804, "title": "tf.keras.layers.Conv2D fails because it captures tensor from inner function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-6461-gc6352706d6 1.14.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0 / 7.5\r\n- GPU model and memory: Nvidia Geforce GTX 1080 Ti, 11 GB\r\n\r\n**Describe the current behavior**\r\nSince commit `546308e322a6b95542ba9f3cbb14136128aaad1e` a `tf.keras.layers.Conv2D` in my training code fails with the following error:\r\n\r\n```\r\nTraceback (most recent call last):                                                                                                                                        \r\n  File \"./train.py\", line 351, in <module>                                                                                                   \r\n    total_loss = train_step()                                                                                                                    \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 417, in __call__                   \r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)                                                                                \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 360, in _initialize                  \r\n    *args, **kwds))                                                                                                                                  \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1709, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)                                                                            \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2013, in _maybe_define_function             \r\n    graph_function = self._create_graph_function(args, kwargs)                                                                                     \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1899, in _create_graph_function            \r\n    capture_by_value=self._capture_by_value),                                                                                                    \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 795, in func_graph_from_py_func      \r\n    func_outputs = python_func(*func_args, **func_kwargs)                                                                                             \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 310, in wrapped_fn                      \r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)                                                                                               \r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 785, in wrapper                       \r\n    raise e.ag_error_metadata.to_exception(type(e))                                                                                                   \r\nValueError: in converted code:                                                                                                                        \r\n                                                                                                                                                      \r\n    ./train.py:328 train_step  *                                                                                                                      \r\n        per_replica_losses = distribution_strategy.extended.call_for_each_replica(joint_train_step_fn, args=())                                  \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1776 call_for_each_replica                                                                                                                                                        \r\n        return self._call_for_each_replica(fn, args, kwargs)                                                                                                        \r\n    /home/salscheider/deeplearning/nnad_playground/model/Resnet.py:173 call  *                                                                   \r\n        x = self.module_3a(x, train_batch_norm=train_batch_norm)                                                                                   \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:713 __call__                               \r\n        outputs = call_fn(inputs, *args, **kwargs)                                                                                                 \r\n    /home/salscheider/deeplearning/nnad_playground/model/Resnet.py:97 call  *                                                                      \r\n        x = self.conv2(x)                                                                                                                        \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:713 __call__                                \r\n        outputs = call_fn(inputs, *args, **kwargs)                                                                                                  \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/convolutional.py:198 call                          \r\n        outputs = self._convolution_op(inputs, self.kernel)                                                                                                                                                                                                                                 \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:1078 __call__                                            \r\n        return self.conv_op(inp, filter)                                                                         \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:634 __call__                 \r\n        return self.call(inp, filter)                                                                                                                                                                                                                        \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py:610 _with_space_to_batch_call                         \r\n        block_shape=self.dilation_rate)                                                                                                         \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:3084 required_space_to_batch_paddings              \r\n        pad_start = base_paddings[:, 0]                                                                                                                                                                                                                      \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:694 _slice_helper                                                                                                                                              \r\n        name=name)                                                                                                                                                                                                                                           \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:860 strided_slice                                                                                                                                              \r\n        shrink_axis_mask=shrink_axis_mask)                                                                                                                                                                                                                   \r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:10397 strided_slice                                                                                                                                        \r\n        shrink_axis_mask=shrink_axis_mask, name=name)\r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:793 _apply_op_helper\r\n        op_def=op_def)\r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:528 create_op\r\n        inp = self.capture(inp)\r\n    /home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:589 capture\r\n        % (tensor, tensor.graph, self))\r\n\r\n    ValueError: Trying to capture a tensor from an inner function. This can be caused by accessing a tensor defined inside a loop or conditional body, or a subfunction, from a calling function, without going through the proper return value mechanism. Consider using TensorFlow mechanisms such as TensorArrays to retur\r\nn tensors from inner functions or loop / conditional bodies. Tensor: Tensor(\"conv2/stack:0\", shape=(2, 2), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0); tensor graph: FuncGraph(name=build_graph, id=140610256740760); this graph: FuncGraph(name=train_step, id=140614894336264)\r\n```\r\n\r\nThis error only occurs with `dilation_rate` set to 2. With `dilation_rate == 1` the training script runs.\r\nIt also works if I revert `546308e322a6b95542ba9f3cbb14136128aaad1e`.\r\n\r\n**Describe the expected behavior**\r\nThe training runs with `dilation_rate == 2`.\r\n\r\n**Code to reproduce the issue**\r\nSo far I could not come up with a small testcase to reproduce the issue. I will continue to try, but until then I only have the backtrace and the problematic commit.", "comments": ["@olesalscheider ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!", "You can use this as a testcase:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef learning_rate_fn():\r\n    return tf.constant(1e-4)\r\n\r\nopt = tf.keras.optimizers.Adam(learning_rate_fn, epsilon=1e-3, amsgrad=True)\r\n\r\ndef get_data():\r\n    return np.random.random([1, 512, 1024, 3]).astype(np.float32)\r\n\r\nbackbone = tf.keras.layers.Conv2D(16, (3, 3),\r\n            dilation_rate=(2, 2),\r\n            kernel_initializer=tf.keras.initializers.he_normal())\r\n\r\n@tf.function\r\ndef train_step():\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        img = tf.py_function(get_data, [], tf.float32)\r\n        img.set_shape([1, None, None, 3])\r\n        feature_map = backbone(img)\r\n        img2 = tf.random.uniform([1, 1024, 1024, 3])\r\n        feature_map2 = backbone(img)\r\n        losses = tf.reduce_sum(feature_map) + tf.reduce_sum(feature_map2)\r\n    vs = backbone.trainable_variables\r\n    gs = tape.gradient(losses, vs)\r\n    opt.apply_gradients(zip(gs, vs))\r\n\r\ntrain_step()\r\n```", "@olesalscheider ,\r\nWhen i executed the given code with dilation_rate == 2, i got the error `KeyError: 'conv2d_4/dilation_rate`. Can you confirm if the same error is faced. Thanks!", "No, I get this error with the testcase:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./train.py\", line 31, in <module>\r\n    train_step()\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 429, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1684, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 645, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 755, in _call_flat\r\n    outputs = self._inference_function.call(ctx, args)\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 469, in call\r\n    ctx=ctx)\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 76, in quick_execute\r\n    raise e\r\n  File \"/home/salscheider/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: conv2d/dilation_rate:0\r\n```\r\n\r\nWhich version of tensorflow do you use? I used the current master branch.", "I was able to reproduce the issue with `!pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190724`. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/9fbcb2e8c3b7bb622aeb61455dfff6cb/untitled324.ipynb). Thanks!\r\n\r\nHere is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/a2d5481c9d0f4393de6ad872824aa974/tf_30804.ipynb) with `tf-nightly` and the error is different `KeyError: 'conv2d/dilation_rate'`. Thanks!", "@olesalscheider this issue should be fixed in tf-nightly. Will close it for now. Feel free to re-open it if the issue appears again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30804\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30804\">No</a>\n"]}, {"number": 30803, "title": "Padding causing 64x increase in memory size on TPU", "body": "I'm trying to train a variation of a u-net on a TPU. However, I can't seem to get it to fit into memory. I'm using tf.keras with tensorflow 1.13.2. Here's a portion of the error logs proving a list of the ops that are the greatest memory hogs. CrossReplicaSum appears to be especially bad, but so are some multiplication ops. I don't quite know how to go about debugging this as the model won't even compile so I don't know what layers these ops actually correspond to.\r\n\r\n```\r\nRuntimeError: Compilation failed: Compilation failure: Ran out of memory in memory space hbm. Used 27.90G of 16.00G hbm. Exceeded hbm capacity by 11.90G.\r\n\r\nTotal hbm usage >= 27.90G:\r\n    reserved        528.00M\r\n    program          27.38G\r\n    arguments       unknown size\r\n\r\nOutput size unknown.\r\n\r\nProgram hbm requirement 27.38G:\r\n    reserved          12.0K\r\n    scoped             1.0K\r\n    HLO temp         27.38G (5.6% utilization, 0.0% fragmentation (1.14M))\r\n\r\n  Largest program allocations in hbm:\r\n\r\n  1. Size: 8.00G\r\n     Operator: op_type=\"CrossReplicaSum\" op_name=\"tpu_140280287273760/CrossReplicaSum\"\r\n     Shape: f32[256,512,128,2]{3,2,1,0}\r\n     Unpadded size: 128.00M\r\n     Extra memory due to padding: 7.88G (64.0x expansion)\r\n     XLA label: %cross-replica-sum = f32[256,512,128,2]{3,2,1,0} cross-replica-sum(f32[256,512,128,2]{3,2,1,0} %bitcast.1), replica_groups={{0,1,2,3,4,5,6,7}}, barrier=\"custom:0\", to_apply=%sum.902, metadata=\r\n{op_type=\"CrossReplicaSum\" op_name=\"tpu_140280287273760/CrossRep...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  2. Size: 8.00G\r\n     Operator: op_type=\"Mul\" op_name=\"tpu_140280287273760/mul_1\"\r\n     Shape: f32[8,32,512,128,2]{4,3,2,1,0}\r\n     Unpadded size: 128.00M\r\n     Extra memory due to padding: 7.88G (64.0x expansion)\r\n     XLA label: %fusion.4 = (f32[8,32,512,128,2]{4,3,2,1,0}, f32[8,32,512,128,2]{4,3,2,1,0}) fusion(f32[8]{0} %fusion.1265, f32[32,512,128,2]{3,2,1,0} %reshape.319, f32[32,512,128,2]{3,2,1,0} %copy.5), kind=k\r\nLoop, calls=%fused_computation.4, metadata={op_type=\"Mul\" op_nam...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  3. Size: 8.00G\r\n     Operator: op_type=\"Mul\" op_name=\"tpu_140280287273760/mul_1\"\r\n     Shape: f32[8,32,512,128,2]{4,3,2,1,0}\r\n     Unpadded size: 128.00M\r\n     Extra memory due to padding: 7.88G (64.0x expansion)\r\n     XLA label: %fusion.4 = (f32[8,32,512,128,2]{4,3,2,1,0}, f32[8,32,512,128,2]{4,3,2,1,0}) fusion(f32[8]{0} %fusion.1265, f32[32,512,128,2]{3,2,1,0} %reshape.319, f32[32,512,128,2]{3,2,1,0} %copy.5), kind=k\r\nLoop, calls=%fused_computation.4, metadata={op_type=\"Mul\" op_nam...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n    4. Size: 1.00G\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_1/Conv2D\"\r\n     Shape: f32[32,512,128,32]{3,0,2,1}\r\n     Unpadded size: 256.00M\r\n     Extra memory due to padding: 768.00M (4.0x expansion)\r\n     XLA label: %fusion.12 = f32[32,512,128,32]{3,0,2,1} fusion(f32[32]{0} %get-tuple-element.1107, f32[32,512,128,32]{3,0,2,1} %fusion.13, bf16[4,4,32,32]{3,2,1,0} %reshape.3), kind=kOutput, calls=%fused_com\r\nputation.12, metadata={op_type=\"Conv2D\" op_name=\"tpu_14028028727...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  5. Size: 1.00G\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d/Conv2D\"\r\n     Shape: f32[32,512,128,32]{3,0,2,1}\r\n     Unpadded size: 256.00M\r\n     Extra memory due to padding: 768.00M (4.0x expansion)\r\n     XLA label: %fusion.13 = f32[32,512,128,32]{3,0,2,1} fusion(f32[32]{0} %get-tuple-element.1105, bf16[32,512,128,2]{0,3,2,1} %copy.4, f32[4,4,2,32]{3,2,1,0} %get-tuple-element.1106), kind=kOutput, calls=%f\r\nused_computation.13, metadata={op_type=\"Conv2D\" op_name=\"tpu_140...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  6. Size: 256.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_4/Conv2D\"\r\n     Shape: f32[32,256,64,64]{3,0,2,1}\r\n     Unpadded size: 128.00M\r\n     Extra memory due to padding: 128.00M (2.0x expansion)\r\n     XLA label: %fusion.32 = f32[32,256,64,64]{3,0,2,1} fusion(f32[64]{0} %get-tuple-element.1165, bf16[4,4,32,64]{3,2,1,0} %reshape.6, f32[32,256,64,32]{3,0,2,1} %get-tuple-element.964), kind=kOutput, calls=\r\n%fused_computation.32, metadata={op_type=\"Conv2D\" op_name=\"tpu_1...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  7. Size: 256.00M\r\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"tpu_140280287273760/conv2d_transpose_4/conv2d_transpose\"\r\n     Shape: f32[32,256,64,32]{3,0,2,1}\r\n     Unpadded size: 64.00M\r\n     Extra memory due to padding: 192.00M (4.0x expansion)\r\n     XLA label: %fusion.23 = f32[32,256,64,32]{3,0,2,1} fusion(f32[4,4,32,128]{3,2,1,0} %get-tuple-element.1186, f32[32]{0} %get-tuple-element.1185, bf16[32,128,32,64]{3,0,2,1} %get-tuple-element.965, f32[32,\r\n128,32,64]{3,0,2,1} %fusion.48), kind=kOutput, calls=%fused_comp...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  8. Size: 256.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_34/Conv2D\"\r\n     Shape: f32[32,256,64,32]{3,0,2,1}\r\n     Unpadded size: 64.00M\r\n     Extra memory due to padding: 192.00M (4.0x expansion)\r\n     XLA label: %fusion.22 = f32[32,256,64,32]{3,0,2,1} fusion(f32[32]{0} %get-tuple-element.1161, f32[32,256,64,32]{3,0,2,1} %fusion.23, bf16[4,4,32,32]{3,2,1,0} %reshape.12), kind=kOutput, calls=%fused_comp\r\nutation.22, metadata={op_type=\"Conv2D\" op_name=\"tpu_140280287273...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n      9. Size: 128.00M\r\n     Operator: op_type=\"LeakyRelu\" op_name=\"tpu_140280287273760/leaky_re_lu/LeakyRelu\"\r\n     Shape: bf16[32,256,64,32]{3,0,2,1}\r\n     Unpadded size: 32.00M\r\n     Extra memory due to padding: 96.00M (4.0x expansion)\r\n     XLA label: %fusion.38 = (bf16[32,256,64,32]{3,0,2,1}, f32[32,256,64,32]{3,0,2,1}) fusion(f32[32]{0} %get-tuple-element.1151, f32[32,512,128,32]{3,0,2,1} %fusion.14, bf16[4,4,32,32]{3,2,1,0} %reshape.5),\r\nkind=kOutput, calls=%fused_computation.38, metadata={op_type=\"Le...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  10. Size: 64.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_32/Conv2D\"\r\n     Shape: f32[32,128,32,64]{3,0,2,1}\r\n     Unpadded size: 32.00M\r\n     Extra memory due to padding: 32.00M (2.0x expansion)\r\n     XLA label: %fusion.46 = f32[32,128,32,64]{3,0,2,1} fusion(f32[64]{0} %get-tuple-element.1157, f32[32,128,32,64]{3,0,2,1} %fusion.47, bf16[4,4,64,64]{3,2,1,0} %reshape.10), kind=kOutput, calls=%fused_comp\r\nutation.46, metadata={op_type=\"Conv2D\" op_name=\"tpu_140280287273...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  11. Size: 64.00M\r\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"tpu_140280287273760/conv2d_transpose_3/conv2d_transpose\"\r\n     Shape: f32[32,128,32,64]{3,0,2,1}\r\n     Unpadded size: 32.00M\r\n     Extra memory due to padding: 32.00M (2.0x expansion)\r\n     XLA label: %fusion.47 = f32[32,128,32,64]{3,0,2,1} fusion(f32[4,4,64,256]{3,2,1,0} %get-tuple-element.1184, f32[64]{0} %get-tuple-element.1183, bf16[32,64,16,128]{3,0,2,1} %get-tuple-element.967, f32[32,\r\n64,16,128]{3,0,2,1} %fusion.99), kind=kOutput, calls=%fused_comp...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  12. Size: 64.00M\r\n     Operator: op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"\r\n     Shape: bf16[32,512,128,2]{0,3,2,1}\r\n     Unpadded size: 8.00M\r\n     Extra memory due to padding: 56.00M (8.0x expansion)\r\n     XLA label: %copy.4 = bf16[32,512,128,2]{0,3,2,1} copy(bf16[32,512,128,2]{3,2,1,0} %reshape.318), metadata={op_type=\"InfeedDequeueTuple\" op_name=\"infeed-train\"}\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  13. Size: 64.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_8/Conv2D\"\r\n     Shape: f32[32,128,32,128]{3,0,2,1}\r\n     Unpadded size: 64.00M\r\n     XLA label: %fusion.56 = f32[32,128,32,128]{3,0,2,1} fusion(f32[128]{0} %get-tuple-element.1173, f32[4,4,64,128]{3,2,1,0} %copy.77, f32[32,128,32,64]{3,0,2,1} %get-tuple-element.966), kind=kOutput, calls=\r\n%fused_computation.56, metadata={op_type=\"Conv2D\" op_name=\"tpu_1...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n\r\n     14. Size: 32.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_12/Conv2D\"\r\n     Shape: f32[32,64,16,256]{3,0,2,1}\r\n     Unpadded size: 32.00M\r\n     XLA label: %fusion.78 = f32[32,64,16,256]{3,0,2,1} fusion(f32[256]{0} %get-tuple-element.1113, f32[4,4,128,256]{3,2,1,0} %get-tuple-element.1114, f32[32,64,16,128]{3,0,2,1} %get-tuple-element.968), kind=\r\nkOutput, calls=%fused_computation.78, metadata={op_type=\"Conv2D\"...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  15. Size: 32.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_13/Conv2D\"\r\n     Shape: f32[32,64,16,256]{3,0,2,1}\r\n     Unpadded size: 32.00M\r\n     XLA label: %fusion.77 = f32[32,64,16,256]{3,0,2,1} fusion(f32[256]{0} %get-tuple-element.1115, f32[32,64,16,256]{3,0,2,1} %fusion.78, f32[4,4,256,256]{3,2,1,0} %get-tuple-element.1116), kind=kOutput, cal\r\nls=%fused_computation.77, metadata={op_type=\"Conv2D\" op_name=\"tp...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  16. Size: 32.00M\r\n     Operator: op_type=\"LeakyRelu\" op_name=\"tpu_140280287273760/leaky_re_lu_1/LeakyRelu\"\r\n     Shape: bf16[32,128,32,64]{3,0,2,1}\r\n     Unpadded size: 16.00M\r\n     Extra memory due to padding: 16.00M (2.0x expansion)\r\n     XLA label: %fusion.89 = (bf16[32,128,32,64]{3,0,2,1}, f32[32,128,32,64]{3,0,2,1}) fusion(f32[64]{0} %get-tuple-element.1171, f32[32,256,64,64]{3,0,2,1} %fusion.33, bf16[4,4,64,64]{3,2,1,0} %reshape.9), k\r\nind=kOutput, calls=%fused_computation.87, metadata={op_type=\"Lea...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  17. Size: 16.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_16/Conv2D\"\r\n     Shape: f32[32,32,8,512]{3,0,2,1}\r\n     Unpadded size: 16.00M\r\n     XLA label: %fusion.107 = f32[32,32,8,512]{3,0,2,1} fusion(f32[512]{0} %get-tuple-element.1121, f32[4,4,256,512]{3,2,1,0} %get-tuple-element.1122, f32[32,32,8,256]{3,0,2,1} %get-tuple-element.970), kind=k\r\nOutput, calls=%fused_computation.105, metadata={op_type=\"Conv2D\"...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  18. Size: 16.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_17/Conv2D\"\r\n     Shape: f32[32,32,8,512]{3,0,2,1}\r\n     Unpadded size: 16.00M\r\n     XLA label: %fusion.106 = f32[32,32,8,512]{3,0,2,1} fusion(f32[512]{0} %get-tuple-element.1123, f32[32,32,8,512]{3,0,2,1} %fusion.107, f32[4,4,512,512]{3,2,1,0} %get-tuple-element.1124), kind=kOutput, cal\r\nls=%fused_computation.104, metadata={op_type=\"Conv2D\" op_name=\"t...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  19. Size: 16.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_30/Conv2D\"\r\n     Shape: f32[32,64,16,128]{3,0,2,1}\r\n     Unpadded size: 16.00M\r\n     XLA label: %fusion.97 = f32[32,64,16,128]{3,0,2,1} fusion(f32[128]{0} %get-tuple-element.1153, f32[32,64,16,128]{3,0,2,1} %fusion.98, f32[4,4,128,128]{3,2,1,0} %get-tuple-element.1154), kind=kOutput, cal\r\nls=%fused_computation.95, metadata={op_type=\"Conv2D\" op_name=\"tp...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n    19. Size: 16.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_30/Conv2D\"\r\n     Shape: f32[32,64,16,128]{3,0,2,1}\r\n     Unpadded size: 16.00M\r\n     XLA label: %fusion.97 = f32[32,64,16,128]{3,0,2,1} fusion(f32[128]{0} %get-tuple-element.1153, f32[32,64,16,128]{3,0,2,1} %fusion.98, f32[4,4,128,128]{3,2,1,0} %get-tuple-element.1154), kind=kOutput, cal\r\nls=%fused_computation.95, metadata={op_type=\"Conv2D\" op_name=\"tp...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  20. Size: 16.00M\r\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"tpu_140280287273760/conv2d_transpose_2/conv2d_transpose\"\r\n     Shape: f32[32,64,16,128]{3,0,2,1}\r\n     Unpadded size: 16.00M\r\n     XLA label: %fusion.98 = f32[32,64,16,128]{3,0,2,1} fusion(f32[4,4,128,512]{3,2,1,0} %get-tuple-element.1182, f32[128]{0} %get-tuple-element.1181, bf16[32,32,8,256]{3,0,2,1} %get-tuple-element.969, f32[32\r\n,32,8,256]{3,0,2,1} %fusion.179, f32[32,64,16,128]{3,0,2,1} %rng...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  21. Size: 16.00M\r\n     Operator: op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_5/dropout/random_uniform/RandomUniform\"\r\n     Shape: f32[32,64,16,128]{3,0,2,1}\r\n     Unpadded size: 16.00M\r\n     XLA label: %rng.5 = f32[32,64,16,128]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_5/dropout/rando\r\nm_uniform/RandomUniform\"}\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  22. Size: 8.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_21/Conv2D\"\r\n     Shape: f32[32,16,4,1024]{3,0,2,1}\r\n     Unpadded size: 8.00M\r\n     XLA label: %fusion.186 = f32[32,16,4,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1133, f32[32,16,4,1024]{3,0,2,1} %fusion.187, f32[4,4,1024,1024]{3,2,1,0} %get-tuple-element.1134), kind=kOutput\r\n, calls=%fused_computation.180, metadata={op_type=\"Conv2D\" op_na...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  23. Size: 8.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_28/Conv2D\"\r\n     Shape: f32[32,32,8,256]{3,0,2,1}\r\n     Unpadded size: 8.00M\r\n     XLA label: %fusion.177 = f32[32,32,8,256]{3,0,2,1} fusion(f32[256]{0} %get-tuple-element.1147, f32[32,32,8,256]{3,0,2,1} %fusion.178, f32[4,4,256,256]{3,2,1,0} %get-tuple-element.1148), kind=kOutput, cal\r\nls=%fused_computation.171, metadata={op_type=\"Conv2D\" op_name=\"t...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  24. Size: 8.00M\r\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"tpu_140280287273760/conv2d_transpose_1/conv2d_transpose\"\r\n     Shape: f32[32,32,8,256]{3,0,2,1}\r\n     Unpadded size: 8.00M\r\n     XLA label: %fusion.178 = f32[32,32,8,256]{3,0,2,1} fusion(f32[4,4,256,1024]{3,2,1,0} %get-tuple-element.1180, f32[256]{0} %get-tuple-element.1179, bf16[32,16,4,512]{3,0,2,1} %get-tuple-element.971, f32[3\r\n2,16,4,512]{3,0,2,1} %fusion.229, f32[32,32,8,256]{3,0,2,1} %rng...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  25. Size: 8.00M\r\n     Operator: op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_4/dropout/random_uniform/RandomUniform\"\r\n     Shape: f32[32,32,8,256]{3,0,2,1}\r\n     Unpadded size: 8.00M\r\n     XLA label: %rng.4 = f32[32,32,8,256]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_4/dropout/random\r\n_uniform/RandomUniform\"}\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  26. Size: 8.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_20/Conv2D\"\r\n     Shape: f32[32,16,4,1024]{3,0,2,1}\r\n     Unpadded size: 8.00M\r\n     XLA label: %fusion.187 = f32[32,16,4,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1131, f32[4,4,512,1024]{3,2,1,0} %get-tuple-element.1132, f32[32,16,4,512]{3,0,2,1} %get-tuple-element.972), kin\r\nd=kOutput, calls=%fused_computation.181, metadata={op_type=\"Conv...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  27. Size: 8.00M\r\n     Operator: op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout/dropout/random_uniform/RandomUniform\"\r\n     Shape: f32[32,32,8,256]{3,0,2,1}\r\n     Unpadded size: 8.00M\r\n     XLA label: %rng = f32[32,32,8,256]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout/dropout/random_uni\r\nform/RandomUniform\"}\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  28. Size: 8.00M\r\n     Operator: op_type=\"LeakyRelu\" op_name=\"tpu_140280287273760/leaky_re_lu_2/LeakyRelu\"\r\n     Shape: bf16[32,64,16,128]{3,0,2,1}\r\n     Unpadded size: 8.00M\r\n     XLA label: %fusion.219 = (bf16[32,64,16,128]{3,0,2,1}, f32[32,64,16,128]{3,0,2,1}) fusion(f32[128]{0} %get-tuple-element.1111, f32[32,128,32,128]{3,0,2,1} %fusion.57, f32[4,4,128,128]{3,2,1,0} %get-tuple\r\n-element.1112), kind=kOutput, calls=%fused_computation.209, meta...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  29. Size: 4.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_26/Conv2D\"\r\n     Shape: f32[32,16,4,512]{3,0,2,1}\r\n     Unpadded size: 4.00M\r\n     XLA label: %fusion.227 = f32[32,16,4,512]{3,0,2,1} fusion(f32[512]{0} %get-tuple-element.1143, f32[32,16,4,512]{3,0,2,1} %fusion.228, f32[4,4,512,512]{3,2,1,0} %get-tuple-element.1144), kind=kOutput, cal\r\nls=%fused_computation.217, metadata={op_type=\"Conv2D\" op_name=\"t...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  30. Size: 4.00M\r\n     Operator: op_type=\"Conv2DBackpropInput\" op_name=\"tpu_140280287273760/conv2d_transpose/conv2d_transpose\"\r\n     Shape: f32[32,16,4,512]{3,0,2,1}\r\n     Unpadded size: 4.00M\r\n     XLA label: %fusion.228 = f32[32,16,4,512]{3,0,2,1} fusion(f32[32,8,2,1024]{3,0,2,1} %fusion.339, f32[4,4,512,1024]{3,2,1,0} %get-tuple-element.1178, f32[512]{0} %get-tuple-element.1177, f32[32,16,4,512]{\r\n3,0,2,1} %rng.3), kind=kOutput, calls=%fused_computation.218, me...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n\r\n  31. Size: 4.00M\r\n     Operator: op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_3/dropout/random_uniform/RandomUniform\"\r\n     Shape: f32[32,16,4,512]{3,0,2,1}\r\n     Unpadded size: 4.00M\r\n     XLA label: %rng.3 = f32[32,16,4,512]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_3/dropout/random\r\n_uniform/RandomUniform\"}\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  32. Size: 4.00M\r\n     Operator: op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_1/dropout/random_uniform/RandomUniform\"\r\n     Shape: f32[32,16,4,512]{3,0,2,1}\r\n     Unpadded size: 4.00M\r\n     XLA label: %rng.1 = f32[32,16,4,512]{3,0,2,1} rng(f32[] %constant.2, f32[] %constant.36), distribution=rng_uniform, metadata={op_type=\"RandomUniform\" op_name=\"tpu_140280287273760/dropout_1/dropout/random\r\n_uniform/RandomUniform\"}\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  33. Size: 4.00M\r\n     Operator: op_type=\"LeakyRelu\" op_name=\"tpu_140280287273760/leaky_re_lu_3/LeakyRelu\"\r\n     Shape: bf16[32,32,8,256]{3,0,2,1}\r\n     Unpadded size: 4.00M\r\n     XLA label: %fusion.301 = (bf16[32,32,8,256]{3,0,2,1}, f32[32,32,8,256]{3,0,2,1}) fusion(f32[32,32,8,256]{3,0,2,1} %rng, f32[256]{0} %get-tuple-element.1119, f32[32,64,16,256]{3,0,2,1} %fusion.79, f32[4,4\r\n,256,256]{3,2,1,0} %get-tuple-element.1120), kind=kOutput, calls...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  34. Size: 2.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_24/Conv2D\"\r\n     Shape: f32[32,8,2,1024]{3,0,2,1}\r\n     Unpadded size: 2.00M\r\n     XLA label: %fusion.338 = f32[32,8,2,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1139, f32[4,4,1024,1024]{3,2,1,0} %get-tuple-element.1140, f32[32,8,2,1024]{3,0,2,1} %fusion.421), kind=kOutput,\r\ncalls=%fused_computation.326, metadata={op_type=\"Conv2D\" op_name...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n\r\n  35. Size: 2.00M\r\n     Operator: op_type=\"Conv2D\" op_name=\"tpu_140280287273760/conv2d_23/Conv2D\"\r\n     Shape: f32[32,8,2,1024]{3,0,2,1}\r\n     Unpadded size: 2.00M\r\n     XLA label: %fusion.421 = f32[32,8,2,1024]{3,0,2,1} fusion(f32[1024]{0} %get-tuple-element.1137, f32[32,8,2,1024]{3,0,2,1} %rng.2, f32[32,16,4,1024]{3,0,2,1} %fusion.188, f32[4,4,1024,1024]{3,2,1,0} %get-\r\ntuple-element.1138), kind=kOutput, calls=%fused_computation.407,...\r\n     Allocation type: HLO temp\r\n     ==========================\r\n```", "comments": ["@lminer \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github ](https://github.com/tensorflow/tensorflow/issues/new/choose)new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I have a similar error when running my model. I created a basic Colab notebook that reproduces this error without actually loading data.\r\n[https://colab.research.google.com/drive/1GPR8c1Buy6bpw1PQmiqYBUjdJfjI8_Mh?usp=sharing\r\n](https://colab.research.google.com/drive/1GPR8c1Buy6bpw1PQmiqYBUjdJfjI8_Mh?usp=sharing)\r\nThis is running on the TPU Hardware Accelerator with a Standard Runtime. Everything else is standard for Colab and should be reproducible by running the notebook."]}, {"number": 30802, "title": "Using tf.function while enumerating a dataset causes an infinite loop", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nUsing `tf.function` when enumerating a dataset will cause an infinite loop.\r\n\r\n**Describe the expected behavior**\r\nUsing `tf.function` when enumerating a dataset should not change the looping behavior.\r\n\r\n**Code to reproduce the issue**\r\nThe code snippet below will hang after the last function call. I'm not printing anything because calling `tf.print` results in a syntax error on colab and I know that these snippets are being run on colab by you. When printing the variable `i`, it's clear that the loop just never stops, i.e. `i` increase indefinitely.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nds = tf.data.Dataset.from_tensor_slices([1,2,3,4])\r\n\r\ndef test_loop_without_enumerate_without_decorator(ds):\r\n    for val in ds:\r\n        pass\r\n\r\n@tf.function\r\ndef test_loop_without_enumerate_with_decorator(ds):\r\n    for val in ds:\r\n        pass\r\n\r\ndef test_loop_with_enumerate_without_decorator(ds):\r\n    for i, val in enumerate(ds):\r\n        pass\r\n\r\n@tf.function\r\ndef test_loop_with_enumerate_with_decorator(ds):\r\n    for i, val in enumerate(ds):\r\n        pass\r\n\r\nprint(\"Without tf.function and without enumerate\")\r\ntest_loop_without_enumerate_without_decorator(ds)\r\n\r\nprint(\"Without tf.function and with enumerate\")\r\ntest_loop_with_enumerate_without_decorator(ds)\r\n\r\nprint(\"With tf.function and without enumerate\")\r\ntest_loop_without_enumerate_with_decorator(ds)\r\n\r\nprint(\"With tf.function and with enumerate\")\r\ntest_loop_with_enumerate_with_decorator(ds)\r\n```\r\n\r\n**Other info / logs**\r\nOutput of the above snippet:\r\n```\r\nWithout tf.function and without enumerate\r\nWithout tf.function and with enumerate\r\nWith tf.function and without enumerate\r\nWith tf.function and with enumerate\r\n**HANGS HERE**\r\n```\r\n", "comments": ["I was able to reproduce the issue on Colab with Tensorflow 2.0.0.beta1. Please have a look at gist of [Colab](https://colab.research.google.com/drive/1UXGUopt4bVq-NUslCJVxw_BHeOlq0Rkz) link. Thanks ", "AutoGraph does not currently override `enumerate`, so it has the wrong behavior in graph mode. It would be a nice feature to add, and it shouldn't be too hard to do it.\r\n\r\nIn the mean time, to enumerate over datasets, please use `ds.enumerate()`. It works both inside and outside `tf.function`:\r\n\r\n```\r\ndef test_loop_with_ds_enumerate_without_decorator(ds):\r\n    for i, val in ds.enumerate():\r\n        tf.print(i, val)\r\n\r\n@tf.function\r\ndef test_loop_with_ds_enumerate_with_decorator(ds):\r\n    for i, val in ds.enumerate():\r\n        tf.print(i, val)\r\n\r\nprint(\"Without tf.function and ds.enumerate\")\r\ntest_loop_with_ds_enumerate_without_decorator(ds)\r\n\r\nprint(\"With tf.function and ds.enumerate\")\r\ntest_loop_with_ds_enumerate_with_decorator(ds)\r\n```\r\n```\r\nWithout tf.function and ds.enumerate\r\n0 1\r\n1 2\r\n2 3\r\n3 4\r\nWith tf.function and ds.enumerate\r\n0 1\r\n1 2\r\n2 3\r\n3 4\r\n```", "@mdanatg thank you, that worked for me. I'm gonna let this issue stay open, as the underlying problem hasn't been fixed yet, but feel free to close if you need to.", "@mdanatg I'm interested in working on this! I'm confused on how to get started. I've read the documentation on AutoGraph & tf.function decorator as well as [the code](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/def_function.py#L725-L1027) and I can't spot which code is responsible for overriding such action. Pardon my inexperience.", "@ilhamfp that's great, happy to help you get started!\r\n\r\nThe dynamic dispatch can indeed make things a bit confusing.\r\n\r\nThe place to add this is in the Python builtin overloads file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/operators/py_builtins.py\r\n\r\nThe file contains a number of overloads, check out `len` for instance, that you can use as a model. Then, once you have the overload of `enumerate`, you just need to add it to the lists at the bottom of the file, `SUPPORTED_BUILTINS` and `BUILTIN_FUNCTIONS_MAP` (there's a bit of duplication there). That should be it - the overload for function calls (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/impl/api.py#L390, placed in the `api` module for various reasons) should pick it up at Python runtime.", "> AutoGraph does not currently override `enumerate`, so it has the wrong behavior in graph mode.\r\n\r\nHi @mdanatg . Just out of curiosity, what do you mean by \"wrong behavior in graph mode\"? With `tensorflow==2.0alpha0`, I often use `enumerate` to iterate over a list of layers within a call function as follows:\r\n```\r\nIn [2]: import tensorflow as tf\r\n\r\nIn [3]: tf.__version__\r\nOut[3]: '2.0.0-alpha0'\r\n\r\nIn [4]: from tensorflow.keras.layers import Dense\r\n\r\nIn [5]: layers = [Dense(10), Dense(20), Dense(30)]\r\n\r\nIn [6]: @tf.function\r\n   ...: def call(x):\r\n   ...:     for i, layer in enumerate(layers):\r\n   ...:         x = layer(x)\r\n   ...:     return x\r\n\r\nIn [7]: y = call(tf.random.uniform((10, 20)))\r\n```\r\nIt would be grateful if you could explain a bit more.", "@llan-ml I was referring to calling `enumerate` with a `tf.data.Dataset` argument. Called with Python lists it was working fine. See the original post - before #31038, calling `test_loop_with_enumerate_with_decorator` in the original example caused an infinite loop, which was incorrect. At any rate, `enumerate` should now be fully supported for datasets.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30802\">No</a>\n", "@mdanatg\r\n\r\n> AutoGraph does not currently override enumerate, so it has the wrong behavior in graph mode. It would be a nice feature to add, and it shouldn't be too hard to do it.\r\n\r\nCould you guys add this to [the documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for tf.Dataset? Seems like a quick addition that would prove super valuable. Thanks!", "@tgsmith61591 This issue should be resolved - enumerate should now work correctly with datasets in tf.function, with TF >= 1.5. Have you been still experiencing issues?"]}, {"number": 30801, "title": "fit_generator with use_multiprocessing=True causes memory usage grow (leakage) on TF 1.13+", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9 (GCP AI Platform Jobs and notebooks), Ubuntu 18.04 (custom) \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): preinstalled via official GCP deep learning VM image, also via pip and conda\r\n- TensorFlow version (use command below): 1.13 and 1.14\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: NVIDIA Tesla K80 and GFX 1080\r\n\r\n**Describe the current behavior**\r\nWe use `fit_generator` with a custom build on `tensorflow.keras.utils.Sequence`, but we also have seen similar behavior with `tensorflow.keras.preprocessing.image.ImageDataGenerator`. Namely, when we have the default `use_multiprocessing=False`, the training process is slow but the memory usage saturates at some point and everything works fine. When we use `use_multiprocessing=True`, despite the number of workers or `max_queue_size` the memory keeps growing linearly up to the point when the process gets killed by the system. See the enclosed memory graph from GCP AI Platform Job monitoring: \r\n![ai_job_mem](https://user-images.githubusercontent.com/1014323/61370638-2bcc9f00-a893-11e9-829e-7ed1ee16039d.png)\r\nThe same code worked fine on TF 1.12, i.e. the memory saturated at some point, but broke from 1.13, and we see the same effect with 1.14. \r\n**Describe the expected behavior**\r\nThe memory shouldn't grow indefinitely as it did in TF 1.12. See the memory timeseries for TF 1.12 below:\r\n![ai_job_mem_ok](https://user-images.githubusercontent.com/1014323/61373323-ca5bfe80-a899-11e9-857d-47c85f985a78.png)\r\n\r\n**Code to reproduce the issue**\r\nOur Data Generator looks as follows:\r\n```python\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n    def __init__(self, affect_net_dir, data_file, batch_size, aug=True):\r\n        self.affect_net_dir = affect_net_dir\r\n        with FileIO(os.path.join(self.affect_net_dir, data_file), 'r') as fr:\r\n            self.data = pd.read_csv(fr)\r\n        self.batch_size = batch_size\r\n        self.aug = aug\r\n        self.indexes = np.arange(self.data.shape[0])\r\n        np.random.shuffle(self.indexes)\r\n\r\n    @property\r\n    def steps(self):\r\n        return int(np.floor(self.data.shape[0] / self.batch_size))\r\n\r\n    def __getitem__(self, index):\r\n        batch_indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\r\n\r\n        x = np.zeros(shape=(self.batch_size, 224, 224, 3))\r\n        y = np.zeros(shape=(self.batch_size, 11))\r\n\r\n        for i, b_index in enumerate(batch_indexes):\r\n            file_path = self.data.iloc[b_index]['subDirectory_filePath']\r\n            with FileIO(os.path.join(self.affect_net_dir, file_path), 'rb') as fr:\r\n                face_img = Image.open(BytesIO(fr.read()))\r\n            face_x = self.data.iloc[b_index]['face_x']\r\n            face_y = self.data.iloc[b_index]['face_y']\r\n            face_width = self.data.iloc[b_index]['face_width']\r\n            face_height = self.data.iloc[b_index]['face_height']\r\n            expression = self.data.iloc[b_index]['expression']\r\n            # crop face from image\r\n            face_img = face_img.crop([face_x, face_y, face_x + face_width, face_y + face_height])\r\n            face_img = face_img.resize(size=(224, 224), resample=Image.LANCZOS)\r\n            face_img = tf.keras.preprocessing.image.img_to_array(face_img)\r\n\r\n            # face augmentation if needed\r\n            if self.aug:\r\n                if np.random.rand() >= 0.5:\r\n                    face_img = face_aug.random_brightness(face_img)\r\n                if np.random.rand() >= 0.5:\r\n                    face_img = face_aug.random_hue(face_img)\r\n                if np.random.rand() >= 0.5:\r\n                    face_img = face_aug.random_saturation(face_img)\r\n                if np.random.rand() >= 0.5:\r\n                    face_img = face_aug.random_contrast(face_img)\r\n                if np.random.rand() >= 0.5:\r\n                    face_img = face_aug.horizontal_mirror(face_img)\r\n\r\n            x[i] = face_img\r\n            y[i] = tf.keras.utils.to_categorical(expression, num_classes=11)\r\n\r\n        return tf.keras.applications.resnet50.preprocess_input(x), y\r\n\r\n    def __len__(self):\r\n        return int(np.floor(self.data.shape[0] / self.batch_size))\r\n```\r\nBut it is quite problem specific and we use it with a large data set O(100k) samples. \r\n\r\nYou can reproduce a similar memory growing behaviour with the Flower example https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c04_exercise_flowers_with_data_augmentation_solution.ipynb. When I run it on GCP AI Platform notebook with TF 1.14 and NVIDIA Tesla K80, with multiprocessing on, i.e. (train the model section):\r\n```python\r\nepochs = 80\r\n\r\nhistory = model.fit_generator(\r\n    train_data_gen,\r\n    steps_per_epoch=int(np.ceil(train_data_gen.n / float(batch_size))),\r\n    epochs=epochs,\r\n    validation_data=val_data_gen,\r\n    validation_steps=int(np.ceil(val_data_gen.n / float(batch_size))),\r\n    use_multiprocessing=True,\r\n    workers=3,\r\n)\r\n```\r\nI get the below memory plot in Stackdriver:\r\n![ai_job_mem_sample](https://user-images.githubusercontent.com/1014323/61373689-bf559e00-a89a-11e9-95e5-397ce8c1113f.png)\r\nIt is not as profound but the same trace remains quite flat when train with `use_multiprocessing=False`. I guess in our case the number of iteration within one epoch, combined with the linear memory growth kills the machine.\r\n\r\n", "comments": ["I'm having this same issue, and wanted to confirm that from my experience it's still happening in Tensorflow 2.2.0.  I'm running a method similar to [NIMA](https://arxiv.org/abs/1709.05424).  I'm using `tf.keras`' `ImageDataGenerator` to pre-process the images, and then a second generator wrapping that one to perform a random crop of the image in order to get a patch that can be trained on.  It's not possible to keep the GPU saturated as the method bottlenecks on the CPU-based pre-processing, so multiprocessing is essential.  Further, according to [this bug](https://github.com/keras-team/keras-preprocessing/issues/145), the Keras team seems to believe that the warnings about bad interactions in multiprocessing can be safely ignored and that `ImageDataGenerator` is thread-safe.  I hope that what follows will help diagnose and fix the issue:\r\n\r\nI've noticed that the system never crashes with a single thread, but on multiple threads fails increasingly quickly.  I profiled the memory with `guppy` after each epoch, as such:\r\n\r\n```\r\nfrom guppy import hpy\r\n\r\n# Profile memory, if desired.\r\nclass MemoryProfilerCallback(keras.callbacks.Callback):\r\n  def __init__(self, session):\r\n    self.session = session\r\n\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    print(self.session.heap())\r\n\r\nif args.memory_profile:\r\n  profile_session = hpy()\r\n  cp_callbacks.append(MemoryProfilerCallback(profile_session))\r\n\r\nmodel.fit(..., callbacks=cp_callbacks, ...)\r\n```\r\n\r\nThe memory leak consists of a lot of `np.ndarray` objects corresponding to the image patches, and the garbage collector won't pick them up. I did try a custom `GarbageCollectorCallback` that simply calls `gc.collect()` which did not alleviate the issue.  This means that something, somewhere, is still holding onto a reference to these `np.ndarray` objects.\r\n\r\nThe raw number of `np.ndarray` objects at the end of each epoch gives us a hint as to what's going on:\r\n```\r\nEpoch\t\t1-CPU\t\t3-CPU\t\tRatio\r\n1\t\t247\t\t759\t\t3.07\r\n2\t\t255\t\t1308\t\t5.13\r\n3\t\t257\t\t1785\t\t6.94\r\n4\t\t259\t\t2260\t\t8.72\r\n5\t\t\t\t2759\r\n6\t\t\t\t3262\r\n7\t\t\t\t3763\r\n```\r\n\r\nIt makes sense that the number of arrays might be triple initially for 3 CPUs, and if it stayed this way then memory wouldn't be an issue for me.  However, it appears that after each epoch, 2 of the 3 worker threads never cleaned up their `ndarray`s, and they were left sitting around!  Then, epoch after epoch, the memory growth increases by the requirement of (num_threads - 1) threads, eventually causing an out-of-memory error.\r\n\r\nThis is all to say that if I were debugging the issue, I'd be looking at where the worker threads are being shut down and their memory freed, and in particular for an error where an operation is being applied to one thread where it should be applied to all.\r\n\r\n**Edit:  This issue seems to be resolved for me in Tensorflow 2.3.0**", "https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237", "@jsnowacki,\r\nSorry for the delayed response. Can you please go through [this comment](https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237) and let us know if it helps? Thanks!", "@rmothukuru well, it's almost 2 years since I posted this, so in my case I cannot even confirm it anymore, as I'm not on the project and we somehow used different approach which works (not workaround really just bigger machine as I recall). So as for me, if TF 2.3.0 solves it, the issue can be closed.", "@jsnowacki,\r\nThank you for the confirmation.  "]}, {"number": 30800, "title": "tf.function doesn't see methods of class", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 2.0beta\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0 \r\n\r\n**Describe the current behavior**\r\n\r\nI am working with GAN. I have `Trainer` class which has the following methods:  `pass_one_step_gan`, `__pass_critic` and `__pass_generator`. It looks like around that:\r\n```python\r\n@tf.function\r\ndef pass_one_step_gan(self, inputs, target_imgs, it, stage, training):\r\n        with tf.GradientTape() as g_tape, tf.GradientTape() as c_tape:\r\n            new_imgs = self.model(inputs, training=training)\r\n\r\n            if training:\r\n                c_loss, c_loss_with_gp = self.__pass_critic(target_imgs, new_imgs)\r\n                g_loss = self.__pass_generator(new_imgs, it)\r\n\r\n```\r\n\r\nI got the following error:\r\n```\r\n.../trainer.py:254 __pass_one_step_gan  *\r\n    c_loss, c_loss_with_gp = self.__pass_critic(\r\n    .../tensorflow/python/autograph/impl/api.py:329 converted_call\r\n            f = getattr(owner, f)\r\n    AttributeError: 'Trainer' object has no attribute '__pass_critic'\r\n```\r\n\r\nI tried to wrap `__pass_critic` in `tf.function`, but it didn't help.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work, but doesn't.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee above.\r\n\r\n**Other info / logs**\r\n\r\nI don't know why but it seems that python doesn't see my method and, thus, tf.function can't track it. ", "comments": ["@Oktai15 \r\nIn order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30799, "title": "Unable to find implementation of Select TensorFlow operators", "body": "I am trying to use Select tensorflow operators ( https://www.tensorflow.org/lite/guide/ops_select ). I have successfully created AAR file. But android studio is unable to find implementation of \r\n**implementation 'org.tensorflow:tensorflow-lite-with-select-tf-ops:0.1.100'**\r\n\r\n> ERROR: Failed to resolve: org.tensorflow:tensorflow-lite-with-select-tf-ops:0.1.100\r\n\r\n\r\nas described in (https://www.tensorflow.org/lite/guide/ops_select). \r\nKindly tell me how should I implement select-tf-ops tflite version ?\r\nThanks\r\n", "comments": ["@Jnasic, try to replace all `tf.where` with `where` from this [issue](https://github.com/tensorflow/tensorflow/issues/30222).", "But I am trying to implement this functionality ( https://www.tensorflow.org/lite/guide/ops_select ) which is different than select operator. Actually I shifted to tensorflow lite select operators because I was unable to find custom implementation of ExpandDims which tflite does not support now. If you can help me with this it would be great though.", "@Jnasic I use `tf.expand_dims` and everything is okay, my model is converted to tflite sucessfully. \r\nExample:\r\n`ps = tf.expand_dims(tf.expand_dims(ps, 1), 1)` ", "Ohh great!\r\nBut how can I include that in TOCO command. Actually I am using DeepLabv3 + coco trained (http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz) but I am unable to convert it to tflite because of ExpandDims. I used following command for conversion.\r\n\r\n`toco --graph_def_file=/home/abdullah/models-master/research/frozen_casted.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=deeplabv3_mnv2_pascal_trainval.tflite --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,1024,1024,3 --default_ranges_min=0 --default_ranges_max=255 --allow_custom_ops --architecture=mobilenet_v2 --std_dev_values=128 --mean_values=1\r\n` \r\nIt is almost 15 days I am working to convert it.\r\nHow did you converted  the model ?", "Rather than using mvn to install your .aar, you can also add a \"libs\" folder in your Android Studio project, put your `tensorflow-lite-with-select-tf-ops.aar` file there, and modify your gradle project with:\r\n\r\n```\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n        flatDir {\r\n            dirs 'libs'\r\n        }\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile(name:'tensorflow-lite-with-select-tf-ops', ext:'aar')\r\n}\r\n```", "@Jnasic Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ? check https://github.com/tensorflow/tensorflow/issues/30799#issuecomment-513356671 Thanks!", "Closing due to the no response. Please file a new issue if this issue persists on the recent TF version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30799\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30799\">No</a>\n"]}, {"number": 30798, "title": "slim", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@liudefu ,\r\nPlease provide the information asked in the template.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30797, "title": ".predict with numpy arrays fails if predictions are time major instead of batch major", "body": "In TF 1.14, .predict fails if predictions are time major i.e. their dimensions are [NB_TIME_STEPS, BATCH_SIZE, N] and not batch major as usual i.e. [BATCH_SIZE, NB_TIME_STEPS, N].\r\nThe error is : \r\n```\r\n  File \"......\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 169, in aggregate\r\n    self.results[i][batch_start:batch_end] = batch_out\r\nValueError: could not broadcast input array from shape (NB_TIME_STEPS,BATCH_SIZE,N) into shape (BATCH_SIZE,BATCH_SIZE,N)\r\n```\r\nThe cause of the bug is that when TF pre-allocates the destination ndarray in self.results, it always assume batch major dimensions (hardcode).\r\nCode in `tensorflow/python/keras/engine/training_utils.py` :\r\n```\r\nclass OutputsAggregator(Aggregator):\r\n  def create(self, batch_outs):\r\n```\r\n```\r\n        elif isinstance(batch_out, np.ndarray):\r\n          # If the output is a ndarray, append an output array pre-allocated\r\n          # to the expected shape of the output.\r\n          shape = (self.num_samples_or_steps,) + batch_out.shape[1:]  <-- HERE IS THE BUG\r\n          self.results.append(np.zeros(shape, dtype=batch_out.dtype))\r\n```\r\nI guess the problematic code is `batch_out.shape[1:]`.\r\nThe `[1:]` gets rid of batch size dimension but this is wrong in case of time major ndarray where it removes time dimensions instead. That is why `shape` is (BATCH_SIZE,BATCH_SIZE,N).", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the [Github](https://github.com/tensorflow/tensorflow/issues/new/choose) new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Actually it may not be a bug because TF is somewhat forced to assume first dimension of predictions is batch dimension.\r\nI solved my issue by switching to batch major for my predictions (removing the `x = tf.transpose(x, perm=[1,0,2])` from my graph).\r\nI needed time major dimensions because `tf.nn.ctc_greedy_decoder` expects time major logits. \r\nThe workaround I found is to move the transpose operation outside of the graph and to do it in the metric just before the call to tf.nn.ctc_greedy_decoder :\r\n```\r\ndef edit_distance_metric(labels, logits):\r\n    sparse_labels = dense_to_sparse_tensor(labels)\r\n    nb_timesteps_ctc = tf.shape(logits)[1]\r\n    batch_size = tf.shape(logits)[0]\r\n    #logits are batch major\r\n    logits = tf.transpose(logits, perm=[1, 0, 2])  # the fix is that I moved this line outside the graph to put it here instead\r\n    #logits are now time major\r\n    decoded, log_prob = tf.nn.ctc_greedy_decoder(inputs=logits,  \r\n                                                 sequence_length=tf.fill([batch_size], nb_timesteps_ctc),\r\n                                                 merge_repeated=True)\r\n```"]}, {"number": 30796, "title": "Removed the deprecated API from the Files", "body": "", "comments": []}, {"number": 30795, "title": "New Updated curl to 7.65.1", "body": "Updated curl to latest version 7.65.1", "comments": ["@shubham769 Could you please check failed build errors? Thanks!"]}, {"number": 30794, "title": "ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "import tflearn\r\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\r\nfrom tflearn.layers.core import input_data, dropout, fully_connected\r\nfrom tflearn.layers.estimator import regression\r\n\r\nconvnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\r\n\r\nconvnet = conv_2d(convnet, 32, 2, activation='relu')\r\nconvnet = max_pool_2d(convnet, 2)\r\n\r\nconvnet = conv_2d(convnet, 64, 2, activation='relu')\r\nconvnet = max_pool_2d(convnet, 2)\r\n\r\nconvnet = fully_connected(convnet, 1024, activation='relu')\r\nconvnet = dropout(convnet, 0.8)\r\n\r\nconvnet = fully_connected(convnet, 2, activation='softmax')\r\nconvnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\r\n\r\nmodel = tflearn.DNN(convnet, tensorboard_dir='log')\r\n\r\n\r\nI am facing below issue \r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-6-8507f364adee> in <module>\r\n----> 1 import tflearn\r\n      2 from tflearn.layers.conv import conv_2d, max_pool_2d\r\n      3 from tflearn.layers.core import input_data, dropout, fully_connected\r\n      4 from tflearn.layers.estimator import regression\r\n      5 \r\n\r\n~\\Anaconda3\\Lib\\site-packages\\tflearn\\__init__.py in <module>\r\n      2 \r\n      3 # Config\r\n----> 4 from . import config\r\n      5 from .config import is_training, get_training_mode, init_graph\r\n      6 \r\n\r\n~\\Anaconda3\\Lib\\site-packages\\tflearn\\config.py in <module>\r\n      3 import tensorflow as tf\r\n      4 \r\n----> 5 from .variables import variable\r\n      6 \r\n      7 # -------------------\r\n\r\n~\\Anaconda3\\Lib\\site-packages\\tflearn\\variables.py in <module>\r\n      5 import tflearn\r\n      6 \r\n----> 7 from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\n      8 from tensorflow.python.framework import ops\r\n      9 from tensorflow.python.ops import variable_scope\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib'", "comments": ["Which version of tensorflow are you using?\r\n\r\n`tensorflow.contrib` is being removed in version 2.0, you therefore need version <= 1.14 to operate tflearn (by the way, this is a TFlearn issue, not a tensorflow one).\r\n\r\nIn your case, I would consider moving to tensorflow (instead of tflearn) and using the tf.keras API, which provides the higher-level API tflearn aimed at offering in times when tf.keras was not out yet.", "Thank you for the reply. I am using tensorflow 2.0. I will uninstall and reinstall tensor flow 1.14", "Thank you. It works \ud83d\ude0a", "You are welcome! Don't forget to close the issue :)", "\"I will uninstall and reinstall tensor flow 1.14\"..... how to do this in jupyter notebook ? ( anaconda 3) Thanks..", "> \"I will uninstall and reinstall tensor flow 1.14\"..... how to do this in jupyter notebook ?\r\n\r\nIn the command line (_not_ the notebook), `conda install tensorflow=1.14` (or `tensorflow-gpu=1.14` if you want GPU support ; or `pip install` rather than `conda install` depending on what you are used to do).", "It worked ! Thanks ! \r\n\r\nOne problem left though is that, by using tflearn package, we will miss new features in tensorflow 2.0 and later versions. Would tflearn be upgraded in the future ?", "I cannot speak for `tflearn` developers, but I think the all idea of making TF 2.0 revolve around the built-in keras implementation (which is also available in the latest 1.x versions) is to make it easier to use tensorflow without requiring an external high-level wrapper. In other words, I do not know whether `tflearn` will allow to run TF 2 as backend, but I think you can probably envision making the move to using the tensorflow built-in high level API (thus dropping tflearn). But, you know, that is up to you and depending on context too :)", "Does anybody knows why this pop ups? \r\n\r\nERROR: Could not find a version that satisfies the requirement 1.14 (from versions: none)\r\n\r\nI was trying to do de uninstall install earlier version of tensorflow but it seems it's deprecated right now. \r\nI hope somebody see this. ", "@sickWill Could you please share the line you executed that triggered this error?\r\n\r\n`pip install tensorflow==1.14` or `pip install tensorflow-gpu==1.14` should work (or you could target the recently-released 1.15 version ; `pip install tensorflow==1.15` which works both with and without GPU usage)", "> @sickWill Could you please share the line you executed that triggered this error?\r\n> \r\n> `pip install tensorflow==1.14` or `pip install tensorflow-gpu==1.14` should work (or you could target the recently-released 1.15 version ; `pip install tensorflow==1.15` which works both with and without GPU usage)\r\n\r\nThank you, this worked. Here is what I had to do since I had permission issues:\r\n`pip3 install --user tensorflow==1.14`", "Thanks! It worked.", "@pandrey-fr Thank you, that worked pretty well.\r\n@cluzier And thank you too for bothering to come with an answer. ", "> Which version of tensorflow are you using?\r\n> \r\n> `tensorflow.contrib` is being removed in version 2.0, you therefore need version <= 1.14 to operate tflearn (by the way, this is a TFlearn issue, not a tensorflow one).\r\n> \r\n> In your case, I would consider moving to tensorflow (instead of tflearn) and using the tf.keras API, which provides the higher-level API tflearn aimed at offering in times when tf.keras was not out yet.\r\n\r\nCan you share a example how to convert from tflearn to tensorflow in the code\r\n", "> > \"I will uninstall and reinstall tensor flow 1.14\"..... how to do this in jupyter notebook ?\r\n> \r\n> In the command line (_not_ the notebook), `conda install tensorflow=1.14` (or `**tensorflow-gpu==1.14`** if you want GPU support ; or `pip install` rather than `conda install` depending on what you are used to do).\r\n\r\n", "ModuleNotFoundError: No module named 'tensorflow.contrib' \r\nanyone please help to solve this\r\n", "When I do \"sudo -H pip2 install tensorflow==1.15\" I get\r\nERROR: Package 'mock' requires a different Python: 2.7.18 not in '>=3.6'\r\n\r\nWhen I do \"sudo -H pip3 install tensorflow==1.15\" I get\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0).\r\n\r\nThe fact that  tensorflow.contrib was removed from python3 tensorflow is causing quite pain. Any help?", "> When I do \"sudo -H pip2 install tensorflow==1.15\" I get\r\nERROR: Package 'mock' requires a different Python: 2.7.18 not in '>=3.6'\r\n\r\nThis looks like an issue with the 'mock' dependency not being compatible with python 2 (which, as a reminder, has been officially deprecated as of January 1st, 2020). You may try to `python2 -m pip install -h mock`, possibly specifying a compatible version of the dependency to install, before resuming tensorflow installation.\r\n\r\n> When I do \"sudo -H pip3 install tensorflow==1.15\" I get\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0).\r\n\r\nThis should be working, but PyPi seems to have trouble parsing the version number. Could you try with `python3 -m pip install -h tensorflow==1.15.3`?\r\n\r\n> The fact that tensorflow.contrib was removed from python3 tensorflow is causing quite pain. \r\n\r\n`tensorflow.contrib` was not removed from python3, but from TF 2.0, as part of a major API clean-up, which has been documented for a year. I understand that this is causing some troubles with code maintenance, but most features can actually be found back either in tensorflow, tensorflow_addons or other third-party packages.", "Thanks for the response.\r\n\r\n* Tensorflow v1.15.x on Python2 requires mock, but now mock requires python 3.7 or above.\r\n* Tensorflow v.1.15.x on Python3 is no available anymore, only 2.2.x.\r\n* I cannot find a pointers list of old 1.15.x tensorflow.contrib packages (https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib#modules) to the new 2.2.x tensorflow versions' distribution (https://www.tensorflow.org/api_docs/python/tf#modules_2). Check slim for instance, which is a deprecated module that cannot be converted automatically.\r\n\r\nI have been able to solve it finally, doing:\r\ntrizen -S python37 --noconfirm\r\nsudo -H python3.7 -m pip install tensorflow==1.15.3\r\nsudo -H python3.7 -m pip install tf_slim\r\nsed -i \"s/from tensorflow.contrib import slim/import tf_slim as slim/g\" file.py\r\nsudo -H python3.7 -m pip install opencv-python\r\n", "Just yesterday TF OD API is now officially compatible with TF 2.X, FINALLY!!!\r\nhttps://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html?m=1", "from tensorflow.contrib.layers.python import layers as tf layers \r\n\r\n ModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nDo I need to change tensorflow2.41 to 1.14? Do I need to delete tensorflow2.4?", "Installed tf version 1.14, but still facing the same issue:\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n", "Can someone help me how to install tensorflow 1.14? \r\nI try with the command \r\npip install tensorflow == 1.14\r\nand it displays :\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0)\r\nERROR: No matching distribution found for tensorflow==1.14\r\n\r\nSo what do I do?", "> \r\nBecause your python version is too high, if you want to install tensorflow 1.14, you should use python 3.5 - 3.7"]}, {"number": 30793, "title": "2019-07-17 12:43:20.046568: F tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:180] An array, resize/ResizeNearestNeighbor, still does not have a known data type after all graph transformations have run.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):0.24\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n- CUDA/cuDNN version:10.0/7.6\r\n- GPU model and memory:Gtx 1060, 16 GB ram\r\n\r\n\r\n\r\n@suharshs Hello I am trying to convert mobilenet model to tflite with following commands.\r\n\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco toco --input_file=/home/abdullah/models-master/research/frozen_casted.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=let_see.tflite --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,1024,1024,3 --default_ranges_min=0 --default_ranges_max=255 --architecture=mobilenet_v2 --std_dev_values=128 --mean_values=1`\r\n\r\nBut I am getting the following error:\r\n\r\n`2019-07-17 12:43:19.954318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: ResizeNearestNeighbor\r\n2019-07-17 12:43:19.963358: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 813 operators, 1238 arrays (0 quantized)\r\n2019-07-17 12:43:19.988357: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 801 operators, 1216 arrays (0 quantized)\r\n2019-07-17 12:43:20.017312: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 801 operators, 1216 arrays (0 quantized)\r\n2019-07-17 12:43:20.036581: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 132 operators, 328 arrays (0 quantized)\r\n2019-07-17 12:43:20.039009: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 132 operators, 328 arrays (0 quantized)\r\n2019-07-17 12:43:20.041679: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 132 operators, 328 arrays (0 quantized)\r\n2019-07-17 12:43:20.044543: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 132 operators, 328 arrays (0 quantized)\r\n2019-07-17 12:43:20.046568: F tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:180] An array, resize/ResizeNearestNeighbor, still does not have a known data type after all graph transformations have run.\r\nAborted (core dumped)\r\n`\r\nI am using **tensorflow1.14 from source**. Kindly tell me why this issue is occurring ?", "comments": ["Is this a quantized model? If so, can you provide a link?", "I have the same problem\uff0ccan you show your solution?", "Can you upload your graph def for us to reproduce? Also is the TF model a quantization-aware trained model?", "@mabdullahrafique,\r\n\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions. we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30793\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30793\">No</a>\n"]}, {"number": 30792, "title": "Fix tf-mlir-translate GraphDef <-> MLIR round-trip bug in node name", "body": "I have found that the `tf-mlir-translate` tool built under `tensorflow/compiler/mlir/tensorflow` cannot round-trip (GraphDef -> MLIR -> GraphDef) as expected.\r\n\r\n- How to reproduce.\r\n\r\n```\r\n$ cd tensorflow/compiler/mlir/tensorflow\r\n$ curl -o simple.pbtxt https://gist.githubusercontent.com/nagachika/22c7d37555a1a9ed295dc3d915ebac35/raw/cb378e5745d59c5c6b34cfd58c24800af0081c0a/simple.pbtxt\r\n$ bazel run tf-mlir-translate -- --graphdef-to-mlir ${PWD}/simple.pbtxt -o ${PWD}/simple.mlir\r\n$ bazel run tf-mlir-translate -- --mlir-to-graphdef ${PWD}/simple.mlir -o ${PWD}/simple_round_tripped.pbtxt\r\n$ diff simple.pbtxt simple_round_tripped.pbtxt\r\n2c2\r\n<   name: \"Const\"\r\n---\r\n>   name: \"_tf.Const\"\r\n25c25\r\n<   name: \"Const_1\"\r\n---\r\n>   name: \"_tf.Const1\"\r\n48c48\r\n<   name: \"add\"\r\n---\r\n>   name: \"_tf.Add\"\r\n50,51c50,51\r\n<   input: \"Const\"\r\n<   input: \"Const_1\"\r\n---\r\n>   input: \"_tf.Const\"\r\n>   input: \"_tf.Const1\"\r\n65c65\r\n< }\r\n\\ No newline at end of file\r\n---\r\n> }\r\n```\r\n\r\nAs shown above, --mlir-to-graphdef wrongly use Function name as corresponding Node name  in GraphDef.\r\n\r\nI have took a short investigation and found that the issue was in `GetName()` in `tensorflow/compiler/mlir/tensorflow/translate/export_graphdef.cc`.\r\n", "comments": ["@jpienaar Thank you for your comment. I added TODO comment just before the fixed line. Is that OK?\r\nJust out of curiosity, is the node name shouldn't came from Function's attributes? I wonder how the tensorflow graph node name could be translated though MLIR.", "> @jpienaar Thank you for your comment. I added TODO comment just before the fixed line. Is that OK?\r\n> Just out of curiosity, is the node name shouldn't came from Function's attributes? I wonder how the tensorflow graph node name could be translated though MLIR.\r\n\r\nIt was updated recently to use the location instead.", "> It was updated recently to use the location instead.\r\n\r\nThat makes sense. Thank you for your explanation!", "Hi,\r\n\r\nThe original issue in the description (GraphDef <-> MLIR round trip) seems be fixed at https://github.com/tensorflow/tensorflow/commit/db6d5a4289c28a82fdec368e9a239d5757f542cd. \r\n\r\nCan I close this PR?", "@nagachika Please feel free to close if it is fixed. Thank you."]}, {"number": 30791, "title": "Use XLA to Tacotron2 is slower than without XLA", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:V9.0.176 / 7\r\n- GPU model and memory: P40\r\n\r\n**Describe the current behavior**\r\nBefore using XLA, the [Tacotron 2](https://github.com/Rayhane-mamah/Tacotron-2) runs about 0.5s, however after using XLA, it increases to about 0.7s which is much slower.\r\n```python\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1              \r\nself.session = tf.Session(config=config) \r\n```\r\nI use XLA like above. Any thoughts would be appreciated. Thanks.", "comments": ["Hi, sorry to hear about the performance problems.\r\n\r\nThe checklist that comes to mind immediately is:\r\n- When testing, make sure to take the graph and run it multiple times then take the lowest number. XLA is a jit, so sometimes people compare the number on the first run, and that includes the compile time. Averaging multiple runs will also have the same problem for small times like this also.\r\n- It's also possible that we compile many times if this model has varying shape sizes. If you try setting the environment variable `TF_CPP_VMODULE=xla_compilation_cache=2` and are not seeing messages saying there are cache hits, then it's still a compile time issue due to varying shapes.\r\n- A variant of the compile problem is that we modify the graph to run XLA, which can hurt performance when XLA cannot provide benefits when it compiles the modified graph. This would be the case if you set the environment variable `TF_XLA_FLAGS=--tf_xla_always_defer_compilation=true` and get the 0.7 number still. This indicates that the graph rewriting slows down the model, and that XLA is not providing benefits on this modified graph.\r\n\r\nIf these compile related issues are not the problem, then it sounds like we're actually producing slow code in some way. Let's start with checking these points then go from there?", "Thanks for your detailed answer. @tpopp \r\n- I skipped the first run and averaging 1000 runs after the first run. Is averaging multiple runs excluding the first run OK?\r\n- I tried setting `TF_CPP_VMODULE=xla_compilation_cache=2`, nothing was printed. I used the Python API as the code is written in Python, does this environment variable work with Python API?\r\n- I tried setting `TF_XLA_FLAGS=--tf_xla_always_defer_compilation=true`, the averaging time of 1000 runs was about the same.\r\n- One more situation is that I tried to run the [tutorial demo](https://www.tensorflow.org/xla/jit#tutorial), run with XLA still lags behind run without XLA.\r\nThis is run without XLA.\r\n![image](https://user-images.githubusercontent.com/29224933/61631025-cd9d2300-acbb-11e9-972d-583e9e3affe3.png)\r\nThis is run with XLA.\r\n![image](https://user-images.githubusercontent.com/29224933/61630884-7ac36b80-acbb-11e9-9c0f-fcb546bcf267.png)\r\n", "Averaging the 1000 runs is fine as long as we aren't recompiling based on different steps. And the tutorial demo not being faster is probably reasonable. It's a small piece of code which gives us less opportunity to optimize.\r\n\r\nI also realize that I wasn't fully clear on my directions, so I want to confirm that both `TF_CPP_VMODULE=xla_compilation_cache=2` and `TF_XLA_FLAGS=--tf_xla_always_defer_compilation=true` should only have an effect (printing and possibly changing timing respectively) when using XLA, and \"always_defer_compilation\" will cause the TF_CPP_VMODULE to not have an effect, so they should not be used together.\r\n\r\nRegarding the VMODULE issue, can you see if running the command below in a shell creates additional logs with the line \"xla_compilation_cache\" in them?\r\n\r\n```\r\nTF_CPP_VMODULE=xla_compilation_cache=2 TF_XLA_FLAGS='--tf_xla_auto_jit=1 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=1' python -c \"import tensorflow as tf; sess = tf.Session(); print(tf.reduce_sum(tf.random.normal([1000, 1000]) + 1).eval(session=sess))\"\r\n```\r\n\r\nI would recommend the next 2 steps:\r\nRunning the model with the environment variables will dump many outputs to /tmp/tacotron/. If you're able to share the output in file \"/tmp/tacotron//after_phase_30_PartiallyDeclusterPass[_*].pbtxt\" we can try to see where the problem might be:\r\nTF_DUMP_GRAPH_PREFIX=/tmp/tacotron/ TF_CPP_VMODULE=optimization_registry=2\r\n\r\nThe best thing would be if you can obtain a trace like you did with the tutorial, but that might be a lot of work. The most likely outcome of this research will be that this model has pieces of Tensorflow functionality that we can't support in XLA, so we're causing some additional overhead through control flow, then providing no speedups.", "Thanks again for your detailed analysis.\r\n\r\nI'm doing inference instead of training, and the input is the same for 1000 runs, but as there is dropout during inference, the length of output may be different. I don't know if this could cause recompiling.\r\n\r\nI use `TF_CPP_VMODULE=xla_compilation_cache=2` and `TF_XLA_FLAGS=--tf_xla_always_defer_compilation=true` separately.\r\n\r\nI tried running the command below, but found no prints with \"xla_compilation_cache\" in them.\r\n```\r\n$ TF_CPP_VMODULE=xla_compilation_cache=2 TF_XLA_FLAGS='--tf_xla_auto_jit=1 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=1' python -c \"import tensorflow as tf; sess = tf.Session(); print(tf.reduce_sum(tf.random.normal([1000, 1000]) + 1).eval(session=sess))\"\r\n2019-07-24 11:39:14.339840: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-24 11:39:14.775164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 22.38GiB freeMemory: 22.21GiB\r\n2019-07-24 11:39:14.775238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-07-24 11:39:15.137972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-24 11:39:15.138047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-07-24 11:39:15.138063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-07-24 11:39:15.138596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21544 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2019-07-24 11:39:15.490174: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f0b6c002210 executing computations on platform CUDA. Devices:\r\n2019-07-24 11:39:15.490259: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): Tesla P40, Compute Capability 6.1\r\n2019-07-24 11:39:15.633148: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:402] *** WARNING *** You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\n1001091.1\r\n```\r\n\r\nI can't find anything under /tmp/tacotron/ after running the Tacotron model with environment variables `TF_DUMP_GRAPH_PREFIX=/tmp/tacotron/ TF_CPP_VMODULE=optimization_registry=2`, so I recorded the timeline [with](https://drive.google.com/open?id=1J7yyX480hc0JrVzSRTfiQ-sxPQ5u47Mn) and [without](https://drive.google.com/open?id=10ciVqnzM6DQZtS_fRE1x4lH4XGpnCwbY) XLA. Maybe you can find something useful in timeline, thanks.", "Just a warning, I am without my work computer until next Wednesday or Tuesday. I will look at this at that time, but I won't get a chance to before.\r\n\r\nThat's strange that the TF_CPP_VMODULE is not working for you, but we will try to work around it.", "No problems, thanks!", "So in this case, it does not seem like XLA is a problem. To be specific, there are two parts to XLA used by Tensorflow. There is a \"tf2xla bridge\" that changes the graph to be compatible with XLA, then XLA itself. The latter is not clearly a problem based on your timelines. The former (the bridge) does seem to be an issue though. We would hope for the timeline to be primarily a single XlaCompile and then a very long XlaRun operation, and almost nothing else.\r\n\r\nI figured out the TF_CPP_VMODULE issue. I thought this feature was around longer, but it is not in tf 1.12. If you can upgrade tensorflow to tf 1.14 is the simplest solution to make the commands work out. I will post in a couple hours the commands to run for tf 1.12. There will be less debug information but hopefully still enough.", "To get additional information with your version of Tensorflow, you can add:\r\nTF_CPP_MIN_VLOG_LEVEL=2 TF_XLA_FLAGS=\"--tf_dump_graph_prefix=/tmp/tacotron/ --tf_xla_clustering_debug\". You might want to redirect output to a file when running this because much other unneeded information will be printed that will flood your screen.  The files logged out with this can be analyzed to figure out why we aren't running XLA on a larger amount of the graph.", "Thanks again for your patience.\r\nI ran the command with the environment variables above and recorded outputs to [the file](https://drive.google.com/open?id=17ncyy2Xl0jRJBxdMPR8oojqdV0OT6P8V). It's a very big file. Maybe you can help figure out what happened. :)", "Any thoughts? @tpopp ", "@tpopp is out of office this week so he won't be able to reply before next week.\r\n\r\nLet me know if this is urgent and I can try to take a look.", "Thanks, @sanjoy, next week is fine.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30791\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30791\">No</a>\n"]}, {"number": 30790, "title": "how to debug the tensorflow source code", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n does anyone know how to debug the tensorflow source code with the test module;\r\n I have tried to use bazel build -c dbg while_loop_test, but it seems that it will compile all the project, and it will take much time. anyone knows how to debug only a small module.\r\n Thanks!!!\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}]