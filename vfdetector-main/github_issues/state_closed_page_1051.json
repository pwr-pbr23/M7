[{"number": 21761, "title": "[XLA] code generation for ARM NEON produce very slow code with tensorflow 1.9.0 and 1.10.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Cortex A7\r\n- **TensorFlow installed from (source or binary)**: from source\r\n- **TensorFlow version (use command below)**: v1.9.0 and superior\r\n- **Python version**: 3.4\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 4.9.4\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\ntfcompile --graph=network.pb --config=network.config.pbtxt  --cpp_class=Network --out_header=network.h --out_function_object=network.o --xla_enable_fast_math=true --target_triple=armv7a-none-android --target_features=+neon --entry_point=run_network\r\n\r\narm-linux-androideabi-g++ -shared -o test.so network.o\r\n\r\n(details of network.pb and network.config.pbtxt are not relevant, The only thing I can say is that it contains 2D Convolution and Dense layer)\r\n\r\n### Describe the problem\r\nI use XLA AOT (tfcompile) to compile network down to shared library with tfcompile and it was working great until v1.9.0.\r\nStarting from v1.9.0, I see huge speed penalty (more than 15x slowdown) when executing the network on ARM v7 + NEON. After some investigation, **I think it is pretty clear that v1.9.0 and v1.10.0 do not use specialized neon runtime for matmul and conv** (even when +neon is specified in target_features) as shown in the following objdumps:\r\n\r\n- ** With tensorflow v1.8.0 and previous versions\r\nobjdump -CT test.so               \r\n\r\ntest.so:     file format elf32-little\r\n\r\nDYNAMIC SYMBOL TABLE:\r\n00000000      DF *UND*\t00000000  LIBC        __cxa_finalize\r\n00000000      DF *UND*\t00000000  LIBC        __cxa_atexit\r\n00000000      D  *UND*\t00000000              **__xla_cpu_runtime_EigenConvF32**\r\n00000000      D  *UND*\t00000000              **__xla_cpu_runtime_EigenMatMulF32**\r\n000003c0 g    DF .text\t0000abc0  Base        run_network\r\n00672004 g    D  *ABS*\t00000000  Base        __bss_start\r\n00672004 g    D  *ABS*\t00000000  Base        _end\r\n00672004 g    D  *ABS*\t00000000  Base        _edata\r\n\r\n- ** With tensorflow v1.9.0 and superior:\r\n\r\nobjdump -CT test.so \r\n\r\ntest.so:     file format elf32-little\r\n\r\nDYNAMIC SYMBOL TABLE:\r\n00000000      DF *UND*\t00000000  LIBC        __cxa_finalize\r\n00000000      DF *UND*\t00000000  LIBC        __cxa_atexit\r\n00000320 g    DF .text\t0000cb40  Base        run_network\r\n00674004 g    D  *ABS*\t00000000  Base        __bss_start\r\n00674004 g    D  *ABS*\t00000000  Base        _end\r\n00674004 g    D  *ABS*\t00000000  Base        _edata\r\n\r\nI tried to follow the code generation process but got lost in LLVM code generation.\r\nCould you have a look at the problem, please? Any help will be helpful.\r\n\r\n### Source code / logs\r\nRunning code just call run_network function in the shared library with the correct parameters.", "comments": ["Sorry for the regression.\r\n\r\nCan you try changing `DotOpEmitter::EmitLlvmIrDotIfProfitable` to return `false` unconditionally and see if the regression disappears?  If yes the fix should be relatively simple.", "@sanjoy Thanks for taking some time to investigate. I really appreciate.\r\n\r\nI made the suggested change but XLA is still very slow (same as before).\r\nDetails:\r\nI have the same objdump as before:\r\nobjdump -CT test.so\r\n\r\ntest.so: file format elf32-little\r\n\r\nDYNAMIC SYMBOL TABLE:\r\n00000000 DF UND\t00000000 LIBC __cxa_finalize\r\n00000000 DF UND\t00000000 LIBC __cxa_atexit\r\n00000320 g DF .text\t0000cb40 Base run_network\r\n00674004 g D ABS\t00000000 Base __bss_start\r\n00674004 g D ABS\t00000000 Base _end\r\n00674004 g D ABS\t00000000 Base _edata\r\n\r\nFor reference I made a test that compare TF, XLA and TFLite on the same network and I have the following results on ARM with tensorflow 1.10:\r\n- TF: 201 ms\r\n- TFLite: 273 ms\r\n- XLA: 4446 ms\r\nNeural networks outputs for each inference engine are the same so correctness is not an issue and it indicates the same operations are performed for each tests.\r\nSystem monitoring indicates no other computation are performed in the background.\r\n\r\nI am happy to help if you have other tests I can perform.", "Can you please attach `network.pb`  and `network.config.pbtxt` so that I can try reproducing this?", "I am sorry but I can't provide that kind of information.\r\nIt is not restricted to this particular architecture, we observe the same regression for all our networks.", "Can you extract out a small reproducer that you _can_ share?", "I noticed the same issue , i`m trying to deploy Inception model on arm target , i managed to deployed for linux and Xeon processor but when i move to arm aarch64 it took almost 10X running time slower. ", "Hi,\r\n\r\nI'm not actively working on this, but I'd be happy to review pull requests if someone (outside Google or within Google) wants to drive performance on aarch64.", "So as far as I can tell, in the ideal situation this is supposed to call into a platform specific runtime for the target platform to make use of highly optimized ops such as gemm, gemv, fft, etc. Where would it get that runtime when it's cross-compiling from Intel? Is there a high level doc I could read for how this is supposed to work?", "I am closing this issue. Please open a new ticket if the issue persists with latest version of TFlow. Thanks!"]}, {"number": 21760, "title": "OverflowError: timeout value is too large", "body": "Preliminaries\uff1awin10, tensorflow-gpu1.8. I got bugs like this.\r\nTraceback (most recent call last):\r\n  File \"D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py\", line 450, in <module>\r\n    ig.show(table)\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\img.py\", line 13, in show\r\n    return imtable.show(*args, **kwargs)\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\imtable.py\", line 72, in show_table\r\n    html_rows = html_from_rows(table, output_dir)\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\imtable.py\", line 413, in html_from_rows\r\n    html_rows.append(\"<td>\" + \"<td>\".join(html_from_cell(x, output_dir) for x in row))\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\imtable.py\", line 413, in <genexpr>\r\n    html_rows.append(\"<td>\" + \"<td>\".join(html_from_cell(x, output_dir) for x in row))\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\imtable.py\", line 308, in html_from_cell\r\n    return x.make_html(output_dir)\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\imtable.py\", line 587, in make_html\r\n    make_video(fname, self.ims, self.fps, sound = self.sound)\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\imtable.py\", line 498, in make_video\r\n    [(i, x, in_dir, tmp_ext) for i, x in enumerate(ims)])\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\aolib\\util.py\", line 2725, in parmap\r\n    ret = pool.map_async(f, xs).get(10000000)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\multiprocessing\\pool.py\", line 638, in get\r\n    self.wait(timeout)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\multiprocessing\\pool.py\", line 635, in wait\r\n    self._event.wait(timeout)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\threading.py\", line 551, in wait\r\n    signaled = self._cond.wait(timeout)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\threading.py\", line 299, in wait\r\n    gotit = waiter.acquire(True, timeout)\r\nOverflowError: timeout value is too large\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n\r\nIn particular, a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve) will be helpful.\r\n\r\nThat said, just looking at the stacktrace, it seems you're using the Python multiprocessing module, which may not be friendly to the TensorFlow runtime (see https://github.com/tensorflow/tensorflow/issues/8220#issuecomment-302826884 as well).\r\n"]}, {"number": 21759, "title": "tf.placeholder changes random number sequence generated by a seed", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\nDebian Buster (Debian 10)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**\r\nN/A\r\n- **TensorFlow installed from (source or binary):**\r\nN/A\r\n- **TensorFlow version (use command below):**\r\n1.8.0\r\n- **Python version:**\r\n2.7.14\r\n- **Bazel version (if compiling from source):**\r\nN/A\r\n- **GCC/Compiler version (if compiling from source):**\r\n7.3.0\r\n- **CUDA/cuDNN version:**\r\nN/A\r\n- **GPU model and memory:**\r\nN/A\r\n- **Exact command to reproduce:**\r\nN/A\r\n\r\nHere's the code to test. Inserting a placeholder changes the sequence of `a` and `b`.\r\n``` python\r\ntf.set_random_seed(1234)\r\n#c = tf.placeholder(dtype=tf.float32, shape=[10,10])\r\na = tf.random_uniform([1])\r\nb = tf.random_normal([1])\r\n\r\nprint(\"Session 1\")\r\nwith tf.Session() as sess1:\r\n  print(sess1.run(a))  # generates 'A1'\r\n  print(sess1.run(a))  # generates 'A2'\r\n  print(sess1.run(b))  # generates 'B1'\r\n  print(sess1.run(b))  # generates 'B2'\r\n\r\nprint(\"Session 2\")\r\nwith tf.Session() as sess2:\r\n  print(sess2.run(a))  # generates 'A1'\r\n  print(sess2.run(a))  # generates 'A2'\r\n  print(sess2.run(b))  # generates 'B1'\r\n  print(sess2.run(b))  # generates 'B2'\r\n```\r\nFew Observations - \r\n1. I tried adding the line `c = tf.placeholder(dtype=tf.float32, shape=[10,10])` before and after `tf.set_random_seed(1234)`. The issue persists.\r\n2. Insertion of placeholder does not affect the outcome when op level seed is set.\r\n\r\nDoes the placeholder affect the sequence in which random numbers are generated by graph level seed?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "The issue template is updated now.", "duplication of #14675.", "Closing issue as its a duplicate."]}, {"number": 21758, "title": "Libtensorflow bazel build link error LNK2019 on debug mode on Windows ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     Yes, made change of bazel command in\r\n     tensorflow\\tensorflow\\tools\\ci_build\\windows\\libtensorflow_cpu.sh\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Windows 10 64bit \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n      N/A\r\n- **TensorFlow installed from (source or binary)**:\r\n     source\r\n- **TensorFlow version (use command below)**:\r\n     Tensorflow version 1.10\r\n- **Python version**:\r\n     Python 3.5\r\n- **Bazel version (if compiling from source)**:\r\n     Bazel 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n     VS2015(Microsoft Visual Studio 14.0\\VC)\r\n- **CUDA/cuDNN version**:\r\n     N/A\r\n- **GPU model and memory**:\r\n     N/A\r\n- **Exact command to reproduce**:\r\n     Code path: tensorflow\\tensorflow\\tools\\ci_build\\windows\\libtensorflow_cpu.sh\r\n     Bazel command(my code change): \r\n     bazel --output_user_root=${TMPDIR} build --distinct_host_configuration=false -c **dbg** -- \r\n     copt=/arch:AVX -s --strip=never --verbose_failures tensorflow:libtensorflow.so \r\n     tensorflow/tools/lib_package:clicenses_generate \r\n### Describe the problem\r\nI want to **build libtensorflow on debug mode with bazel on Windows**, and change the corresponding  bazel command from '-c opt' to '-c dbg', but when build process 'action Linking tensorflow:libtensorflow.so', encountered the following link error:\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \r\n\r\n### Source code / logs\r\nHere comes the specific error log:\r\n[libtensorflow_dbg_error_log.txt](https://github.com/tensorflow/tensorflow/files/2305961/libtensorflow_dbg_error_log.txt)\r\n\r\nERROR: D:/tensorflow/libtensorflow/tensorflow/tensorflow/BUILD:520:1: Linking of rule '//tensorflow:libtensorflow.so' failed (Exit 1120): link.exe failed: error executing command\r\n  cd C:/tmp/k6dssnhw/execroot/org_tensorflow\r\nC:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib /MACHINE:X64 /DEBUG:FULL @bazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow.so-2.params /DEBUG:FULL /INCREMENTAL:NO\r\ndepth_space_ops.lo.lib(spacetodepth_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,int,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\ndepth_space_ops.lo.lib(depthtospace_op.obj) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,int,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@H$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nbazel-out/x64_windows-dbg/bin/tensorflow/libtensorflow.so : fatal error LNK1120: 32 unresolved externals\r\nINFO: Elapsed time: 1609.057s, Critical Path: 741.15s", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nMobile device", "I have updated the issue following the template.", "Similar to issue #17647.\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I have not investigated this issue further, thanks!"]}, {"number": 21757, "title": " Fix bug: divide by zero in embedding_lookup_sparse", "body": "Fix #14851", "comments": ["@drpngx Since `tf.div_no_nan` had been merged into master in #21621, I think we can fix the NaN issue easily. Could you take a look? Thanks.", "Let's wait until the GPU version is checked in.\r\n\r\n@tatatodd do you see a perf issue here?", "Sure, many thanks.", "Hi, any update ?", "So, we should definitely change the doc. There is also `safe_embedding_lookup_sparse` in contrib, which obviously has a different implementation. Can you explain what the difference is?", "You're right. \r\n\r\nThe API of ` tf.nn.embedding_lookup_sparse` requires:\r\n\r\n> This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids\r\n\r\nLet's close it.", "@drpngx  Could you close #14851 as well ?", "@facaiy Came across this problem in version 1.3, 1.4. 1.5 again. Seems your changes have been overridden by someone else?\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/ops/embedding_ops.py\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/ops/embedding_ops.py\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/embedding_ops.py\r\nAll the implementation in these versions is:\r\n```\r\nelif combiner == \"mean\":\r\n        embeddings = math_ops.segment_sum(embeddings, segment_ids)\r\n        weight_sum = math_ops.segment_sum(weights, segment_ids)\r\n        embeddings = math_ops.div(embeddings, weight_sum, name=name)\r\n```"]}, {"number": 21756, "title": "input and output of @tf.custom_gradient", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: TensorFlow 1.10\r\n- **Python version**: Python 3.6.5 by Anaconda\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0/ cuDNN 7.1\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1080Ti 11G\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\nI am confusing about the input and output of [tf.custom_gradient](https://www.tensorflow.org/api_docs/python/tf/custom_gradient).\r\n\r\n#### Input\r\n\r\nIn [doc](https://www.tensorflow.org/api_docs/python/tf/custom_gradient), it says:\r\n\r\n `x` is a `Tensor` or sequence of `Tensor` inputs to the function. But with multiple inputs, instead of taking a sequence of `Tensor`s, function `f` takes `N` positional arguments. I think this is a mistake in documentation. A sequence of `Tensor`s can't be passed to `f` which can be reproduced by code below:\r\n```python3\r\ndef self_define_op_multiple_inputs():\r\n    @tf.custom_gradient\r\n    def loss_func(input_):\r\n        x = input_[0]\r\n        label = input_[2]\r\n\r\n        def grad(dy):\r\n            return [dy, dy]\r\n\r\n        return x - label, grad\r\n\r\n    x = tf.range(10, dtype=tf.float32)\r\n    y = tf.range(10, dtype=tf.int32)\r\n\r\n    loss = loss_func([x, y])\r\n\r\n\r\nif __name__ == '__main__':\r\n    self_define_op_multiple_inputs()\r\n```\r\nIt will try to convert `[x, y]` to a single `Tensor` and raises a error:\r\n```\r\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 280, in <module>\r\n    self_define_op_multiple_inputs()\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 276, in self_define_op_multiple_inputs\r\n    loss = loss_func([x, y])\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 111, in decorated\r\n    return _graph_mode_decorator(f, *args, **kwargs)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in _graph_mode_decorator\r\n    args = [ops.convert_to_tensor(x) for x in args]\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py\", line 124, in <listcomp>\r\n    args = [ops.convert_to_tensor(x) for x in args]\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 998, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1094, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 961, in _autopacking_conversion_function\r\n    return _autopacking_helper(v, inferred_dtype, name or \"packed\")\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 903, in _autopacking_helper\r\n    elem))\r\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'range_1:0' shape=(10,) dtype=int32>)\r\n```\r\nWhile change to positional arguments can fix the bug:\r\n\r\n```python3\r\n@tf.custom_gradient\r\n    def loss_func(x, label):\r\n\r\n        def grad(dy):\r\n            return [dy, dy]\r\n```\r\n\r\n\r\nRelated discussion can be found at [https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs](https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs).\r\n\r\n\r\n#### Output\r\nThis is the problem about the output of `grad_fn`.\r\nIn doc, `grad_vars` is a `list<Tensor>`  with the derivatives of `Tensor`s in `y` with respect to the variables, and signature is `g(*grad_ys, variables=None)`. \r\n1. Is `variables` is original `variables` or the gradient of `variables` like `grad_ys`?\r\n2. Return `grad_vars` as a `list<Tensor>` will raise an error:\r\n```python3\r\n\r\ndef self_define_op_multiple_inputs():\r\n    @tf.custom_gradient\r\n    def loss_func(x):\r\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\r\n                            initializer=tf.constant_initializer([10]), use_resource=True)\r\n\r\n        def grad(dy, variables=None):\r\n            return dy, [variables]  # just for testing\r\n\r\n        return tf.multiply(x, w), grad\r\n\r\n    x = tf.constant([5], dtype=tf.float32, shape=(1,))\r\n\r\n    loss = loss_func(x)\r\n    dl = tf.gradients(loss, x)\r\n\r\n    with tf.Session(config=config) as sess:\r\n        derivative = sess.run(dl)\r\n        print(derivative)\r\n\r\n\r\nif __name__ == '__main__':\r\n    self_define_op_multiple_inputs()\r\n\r\n```\r\n\r\nIt seems like it handles `grad_vars` as a `Tensor`:\r\n\r\n```\r\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 259, in <module>\r\n    self_define_op_multiple_inputs()\r\n  File \"/home/hyh/projects/benchmark/test.py\", line 251, in self_define_op_multiple_inputs\r\n    dl = tf.gradients(loss, x)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 795, in _GradientsHelper\r\n    _LogOpGradients(op, out_grads, in_grads)\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in _LogOpGradients\r\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\r\n  File \"/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 945, in <listcomp>\r\n    \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\r\nAttributeError: 'list' object has no attribute 'name'\r\n```\r\nChange `grad_vars` to `Tensor` doesn't work either:\r\n```python3\r\n\r\ndef self_define_op_multiple_inputs():\r\n    @tf.custom_gradient\r\n    def loss_func(x):\r\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\r\n                            initializer=tf.constant_initializer([10]), use_resource=True)\r\n\r\n        def grad(dy, variables=None):\r\n            return dy, variables  # just for testing\r\n\r\n        return tf.multiply(x, w), grad\r\n\r\n    x = tf.constant([5], dtype=tf.float32, shape=(1,))\r\n\r\n    loss = loss_func(x)\r\n    dl = tf.gradients(loss, x)\r\n\r\n    with tf.Session(config=config) as sess:\r\n        derivative = sess.run(dl)\r\n        print(derivative)\r\n\r\n\r\nif __name__ == '__main__':\r\n    self_define_op_multiple_inputs()\r\n```\r\n", "comments": ["@andydavis1 would you PTAL, or reassign to someone who knows the custom gradients code?", "The documentation could be improved here. Saying \"`x` can be a list of Tensors\" is confusing. What we really wanted to convey was that `f` can be a function with multiple arguments, not just a single `Tensor`.\r\n\r\nCC @alextp  \r\n\r\n@DSRYhh - do you have suggestions for better phrasing?", "I'm preparing a PR which removes \"list\" from the documentation, fixing the issue you saw there.\r\n\r\nIn your last example the correct way to do this is\r\n\r\n    @tf.custom_gradient\r\n    def loss_func(x):\r\n        w = tf.get_variable(\"margin_inner_product_layer/W\", shape=(1,), dtype=tf.float32,\r\n                            initializer=tf.constant_initializer([10]), use_resource=True)\r\n\r\n        def grad(dy, variables=None):\r\n            return dy, [dy for v in variables]\r\n\r\n        return tf.multiply(x, w), grad\r\n\r\n\r\nas in, the second return value when variables is not None should be a list with one element per variable in variables. I'll clarify the documentation there too.", "@alextp To be more clear, for the second parameter (and the second return value), `grad_fn` accepts *original* variables (not the *gradient* of variables) and return the  *gradient* of variables, is that correct?\r\n\r\nIf that's correct, why not `grad_fn` accepts *gradient* of variables instead of *original* variables ( in order to be consistent with `grad_ys` (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually.", "grad_ys is the \"downstream\" gradient of the outputs of your function; since\nthe variables are not outputs there is no gradient already computed wrt\nthem. If you want you can call tf.gradients or use the tf.GradientTape\nyourself to compute the gradient wrt the variables to then modify it, but\nwe don't force you to do that since it would waste computation in eager\nexecution.\n\nOn Tue, Sep 11, 2018 at 3:56 AM Huang Yuheng <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> To be more clear, for the second\n> parameter (and the second return value), grad_fn accepts *original*\n> variables (not the *gradient* of variables) and return the *gradient* of\n> variables, is that correct?\n>\n> If that's correct, why not grad_fn accepts *gradient* of variables\n> instead of *original* variables ( in order to be consistent with grad_ys\n> (the first parameter))? In that case, we can use the derivates of variables\n> by automatic differentiation instead of writing the derivates of variables\n> manually.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21756#issuecomment-420231669>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxetzCj348nk4KLUjm3FAJNNXJqR3ks5uZ5b1gaJpZM4WFeuE>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 21755, "title": "Getting Shifted values of Close Price in Tensorflow training, validation and Testing. Python", "body": "I am trying to experiment with the Tensorflow. I am supplying the sequence of 5 out of which 4 are input and the 5th value is the output from the close price.\r\nThe graphs are plotted according to the respective values of the output as the original and the predicted for the training, validation and Testing.\r\n\r\nBut the graph shows shifted output. See the graphs:\r\n![file](https://user-images.githubusercontent.com/13446197/44395357-44371b00-a557-11e8-8e22-94c4484fcbac.jpg)\r\n\r\nI do not understand what is the problem as I am trying to get the prediction one step ahead of the given input.\r\n\r\nHere is the training code that I am trying to run:\r\n\r\n```\r\nindex_in_epoch = 0;\r\nperm_array  = np.arange(x_train.shape[0])\r\nnp.random.shuffle(perm_array)\r\n\r\n# function to get the next batch\r\ndef get_next_batch(batch_size):\r\n    global index_in_epoch, x_train, perm_array   \r\n    start = index_in_epoch\r\n    index_in_epoch += batch_size\r\n\r\n    if index_in_epoch > x_train.shape[0]:\r\n        np.random.shuffle(perm_array) # shuffle permutation array\r\n        start = 0 # start next epoch\r\n        index_in_epoch = batch_size\r\n\r\n    end = index_in_epoch\r\n    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\r\n\r\n# parameters\r\nn_steps = seq_len-1 \r\nn_inputs = x_train.shape[2]#4\r\nn_neurons = 500\r\nn_outputs = y_train.shape[1]#4\r\nn_layers = 2\r\nlearning_rate = 0.0001\r\nbatch_size =10\r\nn_epochs = 1000#200 \r\ntrain_set_size = x_train.shape[0]\r\ntest_set_size = x_test.shape[0]\r\n\r\ntf.reset_default_graph()\r\n\r\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\r\ny = tf.placeholder(tf.float32, [None, n_outputs])\r\n\r\nlayers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons, \r\n                                 activation=tf.nn.leaky_relu, use_peepholes = True)\r\n         for layer in range(n_layers)]\r\n\r\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\r\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\r\n\r\nstacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \r\nstacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\r\noutputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\r\noutputs = outputs[:,n_steps-1,:] # keep only last output of sequence\r\n\r\nloss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \r\ntraining_op = optimizer.minimize(loss)\r\n\r\nsaver = tf.train.Saver()\r\ndisplay = 40\r\nwith tf.Session() as sess: \r\n    sess.run(tf.global_variables_initializer())\r\n    plt.ion()\r\n    fig = plt.figure()\r\n    fig.set_size_inches(30,10)\r\n    ax1 = fig.add_subplot(231)\r\n    line1, = ax1.plot(y_train[:display],color='blue', label='train original',marker=\".\")\r\n    line2, = ax1.plot(y_train[:display],color='red', label='train Prediction',marker=\".\")\r\n    ax2 = fig.add_subplot(232)\r\n    line3, = ax2.plot(y_valid[:display],color='blue', label='valid original',marker=\".\")\r\n    line4, = ax2.plot(y_valid[:display],color='red', label='valid Prediction',marker=\".\")\r\n    ax3 = fig.add_subplot(233)\r\n    line5, = ax3.plot(y_test[:display],color='blue', label='valid original',marker=\".\")\r\n    line6, = ax3.plot(y_test[:display],color='red', label='valid Prediction',marker=\".\")\r\n    ax4 = fig.add_subplot(234)\r\n    candlestick2_ohlc(ax4,df_train_candle.o.values[:display],df_train_candle.h.values[:display],df_train_candle.l.values[:display],df_train_candle.c.values[:display],width=0.6)\r\n    ax5 = fig.add_subplot(235)\r\n    candlestick2_ohlc(ax5,df_valid_candle.o.values[:display],df_valid_candle.h.values[:display],df_valid_candle.l.values[:display],df_valid_candle.c.values[:display],width=0.6)\r\n    ax6 = fig.add_subplot(236)\r\n    candlestick2_ohlc(ax6,df_test_candle.o.values[:display],df_test_candle.h.values[:display],df_test_candle.l.values[:display],df_test_candle.c.values[:display],width=0.6)\r\n    ax1.set_title('Training')\r\n    ax2.set_title('Validation')\r\n    ax3.set_title('Testing')\r\n    ax1.set_xlabel('time')\r\n    ax2.set_xlabel('time')\r\n    ax3.set_xlabel('time')\r\n    ax1.set_ylabel(\"Train Reversal\")\r\n    ax2.set_ylabel(\"Valid Reversal\")\r\n    ax3.set_ylabel(\"Test Reversal\")\r\n\r\n    ax4.set_title('Training Candles')\r\n    ax4.set_xlabel('time')\r\n    ax4.set_ylabel(\"Train OHLC\")\r\n    ax5.set_title('Validation Candles')\r\n    ax5.set_xlabel('time')\r\n    ax4.set_ylabel(\"Valid OHLC\")\r\n    ax6.set_title('Testing Candles')\r\n    ax6.set_xlabel('time')\r\n    ax4.set_ylabel(\"Test OHLC\")\r\n\r\n\r\n    plt.show()\r\n    #if(tf.train.checkpoint_exists(tf.train.latest_checkpoint(\"modelsOHLC\"))):\r\n     #   saver.restore(sess, tf.train.latest_checkpoint(\"modelsOHLC\"))\r\n      #  print(tf.train.latest_checkpoint(\"modelsOHLC\") + \"Session Loaded for Testing\")\r\n    for iteration in range(int(n_epochs*train_set_size/batch_size)):\r\n        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \r\n        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \r\n        if iteration % int(1*train_set_size/batch_size) == 0:\r\n            mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) \r\n            mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) \r\n            mse_test = loss.eval(feed_dict={X: x_test, y: y_test})\r\n            y_train_pred = sess.run(outputs, feed_dict={X: x_train})\r\n            y_valid_pred = sess.run(outputs, feed_dict={X: x_valid})\r\n            y_test_pred = sess.run(outputs, feed_dict={X: x_test})\r\n            line2.set_ydata(y_train_pred[:display])\r\n            line4.set_ydata(y_valid_pred[:display])\r\n            line6.set_ydata(y_test_pred[:display])\r\n            ax1.set_title(\"Training Loss: \"+str(mse_train))\r\n            ax2.set_title(\"Validation Loss: \"+str(mse_valid))\r\n            ax3.set_title(\"Testing Loss: \"+str(mse_test))\r\n            plt.pause(0.01)\r\n            print('%.2f epochs: MSE train/valid/test = %.10f/%.10f/%.10f'%(\r\n                iteration*batch_size/train_set_size, mse_train, mse_valid,mse_test))\r\n            save_path = saver.save(sess, \"modelsOHLC\\\\model\"+str(iteration)+\".ckpt\")\r\n```\r\n\r\nThe input data for the above is here\r\n\r\n[Input file](https://gist.github.com/JafferWilson/c17a4d70b4aaf839b76bde13f7e32141)\r\n\r\nYou can access the Jupyter version of the code from here: [Jupyter Version of the Code\r\n](https://gist.github.com/JafferWilson/2f7a2374e7b5ea4f92c2edda8b9f7691)\r\nPlease guys let me know what I have missed due to which the graph is getting shifted. I am in an ambiguous situation, please let me know what I can do to correct it.   \r\n\r\nI have opened a question on Stackoverflow too, here is the [link for it](https://stackoverflow.com/questions/51945064/getting-shifted-values-of-close-price-in-tensorflow-training-validation-and-tes).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Here's what to be added. Please check the details.\r\nHave I written custom code --- N/A\r\nOS Platform and Distribution ---- Windows 10 \r\nTensorFlow installed from --- pip installer\r\nTensorFlow version ---- 1.9.0\r\nBazel version --- N/A\r\nCUDA/cuDNN version ---- 9.0\r\nGPU model and memory ----- I do not have the GPU mode right now. The system memory is around 32 GB and the GP memory is 12 GB. I am using Titan GTX -Titan Xp.\r\nExact command to reproduce --- This I have already provided, please check it in the question. Even you will get the full code with dataset that I tried.\r\nMobile device --- N/A\r\n\r\nLet me know if anything else needed please.", "I think it's more of a stackoverflow question, since it's not really a bug or a feature request AFAICT. So I will close this one.\r\n\r\nFor best results, I would encourage you to ask a more targeted question -- regarding a specific problem that fits in two or three lines of code.", "@drpngx Thank you for your reply sir. But there isn't anyone answering to me on Stackoverflow. Hence, I thought this is the issue with Tensorflow itself. Please let me know if there is anything you can do to help me. I tried everything and this is my last attempt to resolution.  Please help me.", "Please try to find a small test case that exhibits the problem. It's hard to analyze a longer program.", "@drpngx Thank you sir for your suggestion. I got your point. I understood that tensorflow is no good tool for my analysis. It is just for show and has no real worth.\r\nI was fascinated from the videos and seminars related to it. Now I am knowing the reality. As it is giving shifted result and no one has any views on that, instead just telling me what I should try. If that is not working on the larger data and working on smaller data then what it is worthy for? I am not willing to try it for smaller dataset as it is not giving good result for that.\r\nTrust me sir, my guess is that the tool have some issues and am not able to correct it or understand it at this moment.", "@JafferWilson Can you rephrase in a smaller case what the problem is? I don't understand what the shifting problem is.", "@drpngx Sure, I can. The graph in images you see:-- The blue color is the real data graph. The red one is predicted. You see the red one is the exact match of the blue graph. But the red graph is getting shifted one step ahead of the real one which is not correct. The output of the predicted must match the real values then only we can say that the prediction is perfect.\r\nIf they are not matching or getting plotted one step ahead of what is required, then there are some issue. I am not able to understand why tensorflow is predicting the values one extra step where as the real values are equal to what the predicted values are.\r\nCan you please have a look at the issue? I have tried everything I could but am not able to understand the issue. My guess is that its an issue of the tensorflow itself.\r\nWell, I guess no one is able to give solution to me. So I better stop thinking about it.\r\n", "Can you try to have `x[t] = 0.01 * xrange(100)`, and train/test on the same data? What happens then?", "@drpngx Sir, where exactly you want me to edit this part in my code? Please let me know I will try it out.", "@JafferWilson hi. I have same problem and my predictions had one lag. Could you solve the problem?\r\n", "@hastirad I don't know. that's why I ask a query here. If I had solution for this I would have already gone with Tensorflow. Currently, I am using a hybrid system with Tensorflow and other frameworks. But none is useful. Need to create my own framework.\r\n"]}, {"number": 21754, "title": "How to compile tensorflow's C++ library on Ubuntu with GPU support?", "body": "Many answers to show compile tensorflow's C++ library ,but it only support cpu , I want to know the exact method to compile tensorflow's C++ library on Ubuntu with GPU support.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@TanLingxiao Please take a look at https://www.tensorflow.org/install/install_sources. It has complete instructions on how to build TensorFlow with GPU support.", "having trouble installing tensorflow in linux for c++\r\n"]}, {"number": 21753, "title": "Add more doc for the dict returned by an estimator's evaluate() method", "body": "There is currently no information about the difference between the `loss` and the `average_loss`.  It's unclear what the difference is (see #21557).  The other metrics are arguably self-explanatory, but still worth mentioning in the doc.\r\n\r\nThis PR:\r\n* adds more info about the returned dict in the [API documentation](https://www.tensorflow.org/api_docs/python/tf/estimator/BaselineClassifier#evaluate),\r\n* and adds more info in the [TensorFlow guide](https://www.tensorflow.org/guide/premade_estimators#evaluate_the_trained_model).", "comments": ["Thanks for merging! :)"]}, {"number": 21752, "title": "tflite-nnapi sometimes returns error result on same image.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu14.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Xiaomi 8, OnePlus 5T, Oppo.\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7.1\r\n\r\n### Describe the problem\r\n\r\nI use tflite in android phone and it return right result. But when setting `tflite.useNNAPI(true)`, the result sometimes is error, while sometimes is right on the same images. See the source code below for details.\r\n\r\n### Source code / logs\r\n\r\n    private boolean NNAPI = true; # when set to `false` everything is ok, but when set to `true`, sometimes returns error result on the same images.\r\n\r\n    private String getResult(final Bitmap bitmap) {\r\n        tflite.setUseNNAPI(NNAPI );\r\n\r\n        startTime = System.currentTimeMillis();\r\n\r\n        int width = bitmap.getWidth();\r\n        int height = bitmap.getHeight();\r\n        float scale = (float)IMG_HEIGHT / height;\r\n        int nWidth = (int) Math.floor(width * scale);\r\n        int nHeight = (int) Math.floor(height * scale);\r\n        int IMG_WIDTH =  (int) Math.ceil((float)nWidth/BLOCK)*BLOCK;\r\n\r\n        Bitmap nBitmap = Bitmap.createScaledBitmap(bitmap, nWidth, nHeight, true);\r\n\r\n        int intValues[] = new int[nWidth * nHeight];\r\n        nBitmap.getPixels(intValues, 0, nWidth, 0, 0, nWidth, nHeight);\r\n\r\n        ByteBuffer imgData = ByteBuffer.allocateDirect(IMG_WIDTH*IMG_HEIGHT);\r\n        imgData.order(ByteOrder.nativeOrder());\r\n\r\n        for (int i = 0; i < IMG_HEIGHT; ++i) {\r\n            for (int k = 0; k < IMG_WIDTH; ++k) {\r\n                if(i >= nHeight || k >= nWidth) {\r\n                    imgData.put((byte)0);\r\n                    continue;\r\n                }\r\n\r\n                int val = intValues[i*nWidth+k];\r\n                int b = val & 0xFF;\r\n                int g = (val >> 8) & 0xFF;\r\n                int r = (val >> 16) & 0xFF;\r\n\r\n                float gray = (float) (r * 0.30 + g * 0.59 + b * 0.11);\r\n                imgData.put((byte) gray);\r\n            }\r\n        }\r\n\r\n        endTime1 = System.currentTimeMillis();\r\n\r\n        int MAX_LENGTH = IMG_WIDTH / BLOCK;\r\n        int dims[] = { 1, IMG_HEIGHT, IMG_WIDTH, 1 };\r\n        byte outputs[][] = new byte[MAX_LENGTH][KIND];\r\n\r\n        tflite.resizeInput(0, dims);\r\n        tflite.run(imgData, outputs);\r\n\r\n        endTime2 = System.currentTimeMillis();\r\n\r\n        int indices[] = new int[MAX_LENGTH];\r\n        for(int i=0; i < MAX_LENGTH; ++i) {\r\n            float prob = Byte.MIN_VALUE;\r\n            int index = KIND-1;\r\n            for (int k = 0; k < KIND; ++k) {\r\n                if (outputs[i][k] > prob) {\r\n                    index = k;\r\n                    prob = outputs[i][k];\r\n                }\r\n            }\r\n            indices[i] = index;\r\n        }\r\n\r\n        int blank = KIND - 1;\r\n        StringBuilder sb = new StringBuilder();\r\n        for(int i=0; i < MAX_LENGTH; ++i) {\r\n            if(indices[i] == blank) { continue; }\r\n\r\n            if(i==0 || indices[i] != indices[i-1]) {\r\n                sb.append(CHARS[indices[i]]);\r\n            }\r\n        }\r\n\r\n        endTime3 = System.currentTimeMillis();\r\n        return sb.toString();\r\n    }\r\n", "comments": ["@suharshs Could you please take a look ?", "@aselle: Are you the right person to look at this?", "What is the error that occurs sometimes?", "@suharshs @aselle \r\n\r\nActually I use tflite to perform cnn+ctc in android, and the greedy decoder is written in java while the cnn part is done by tflite. When `tflite.setUseNNAPI(false)` the features return by tflite (that is the line: `tflite.run(imgData, outputs);`) can be decoded to the right prediction. However, when `tflite.setUseNNAPI(true)`, in the line `tflite.run(imgData, outputs);` the values in the `outputs` sometimes changes, and the decoded result is error.\r\n\r\nI wonder why the values return by line `tflite.run(imgData, outputs);` which is done by tflite with NNAPI will sometimes return different results with the same input ?\r\n\r\nThanks.", "> However, when tflite.setUseNNAPI(true), in the line tflite.run(imgData, outputs); the values in the outputs sometimes changes, and the decoded result is error.\r\n\r\nWhich Android device (and Android version) manifests this behavior? All of them?  Can you share any additional details about the model that you're using, or whether you'd be willing to share the model with us for debugging?\r\n\r\nAnd can you be more specific about the reproducibility? When you say it sometimes changes, do mean during arbitrary inference calls? Does it always happen at some point during the app execution? Do you see any errors in the logcat when this happens?", "@jdduke \r\n\r\nI use XiaoMi 8, which is Android 8.1.\r\n\r\nIn fact, the model I use is CNN+CTC on the OCR task. I run the CNN part on tflite, and write CTC decoder code in java.\r\n\r\nBut we need the input images to have variable widths, so each time before feeding, I call `tflite.resize`.\r\n\r\nIf setting `tflite.setUseNNAPI(false)`, the result is always right.\r\n\r\nHowever, **when setting `tflite.setUseNNAPI(true)`, if the width of the first image is larger than the second image, then the result of the second image is always wrong**. Then I set `tflite.setUseNNAPI(false)` and run the same (the second) image and set `tflite.setUseNNAPI(true)` and run again, the result is right.\r\n\r\n\r\n", "Thanks for the details, @jiarenyf.\r\n\r\nUnfortunately, this is an existing (and undocumented) limitation of current NNAPI functionality. Once enabled, it does not properly handle resizes. This should be reported as an error (we'll fix that directly), though longer term we do plan on allowing dynamic resizes.\r\n\r\nThe best way to work around this at the moment is just as you mentioned; when you have to resize an input tensor, first disable NNAPI, then perform the resize, then re-enable NNAPI. This operation is non-trivial, however, so it should be done as infrequently as possible.", "@jdduke Thank you."]}, {"number": 21751, "title": "Why does tf.feature_column.input_layer return my feature values in rearranged order", "body": "I currently am testing some ideas with `LinearClassifier` with `TensorFlow`'s New API. Have created some features using just `tf.feature_column.numeric_column` and feed them into a `tf.feature_column.input_layer` layer as following:\r\n\r\n    layer_test = tf.feature_column.input_layer(input_fn('data.csv')[0], feature_columns_raw)\r\n\r\nAnd then simply check if my input features are corrently fed via:\r\n\r\n    with tf.Session() as sess:\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n        layer_test_val = sess.run(layer_test)\r\n        print(layer_test_val)\r\n\r\nBut I found if very strange and counterintuitive that column order in `layer_test_val` were completely changed out of my control. It's no big deal if the `LinearClassifier` can eventually get the decent prediction accuracy despite the rearranged features' order. However, if what I need is the learned weights of those features, how can I make sure that the output of \r\n`weights = lr_classifier.get_variable_value('logits/kernel')` is in the exact order of the features as they are specified.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We sort the feature columns in order to ensure repeatibility and consistency. You can get the variable names from lr_classifier.get_variable_names(). The names of the weights have the feature names in them and you can use them to retrieve the values.", "@jingwb222 Hi, does @rohan100jain 's suggestion solve your question?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "If your code depends on feature ordering and you change the feature name it will just break. Why cant you simply ensure that the feature columns is a list/tuple and mention in the documentation that the order is the same as in the provided list? Also the current documentation does not mention the 'magic' ordering at all.", "mark. tf.feature_column.input_layer value is not in param order."]}, {"number": 21750, "title": "SwappingPass technology problem in grappler", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nMobile device : N/A\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below):1.8.0\r\nPython version: 3.5\r\nBazel version (if compiling from source):0.10.0\r\nGCC/Compiler version (if compiling from source): c++11\r\nCUDA/cuDNN version: 9/7\r\nGPU model and memory: gtx 1080ti, 11G\r\nExact command to reproduce: N/A\r\nDescribe the problem:\r\n\r\nDescribe the problem:\r\n1.\r\nif I use this config \" gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\", why the prop.memory_size() is not following the \"per_process_gpu_memory_fraction=0.5\" size ? \r\n\r\nIn IdentifySwappingCandidates (memory_optimizer.cc:530) \r\n if (mem_usage.used_memory <= prop.memory_size()) {\r\n      continue;\r\n    }\r\n int64 required_savings = mem_usage.used_memory - prop.memory_size()\r\n\r\nThe size is determined by the user is also let the tensorflow know to calculate the demand.\r\n\r\n2.\r\nAlthough the tensorflow has a swap function, I found it only for kernel tensors not feature tensors.\r\nIf I want to add a new swap data function for feature tensors, can it be added in memory_optimizer.cc file?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Can you provide a repro script for 1? Can you provide links to the specific functions and files you're talking about for 2?\r\n", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21749, "title": "TypeError: convolution() got multiple values for argument 'weights_regularizer'", "body": "I got error like this, how do I fix it, please help me\r\nTraceback (most recent call last):\r\n  File \"D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py\", line 398, in <module>\r\n    ret = run(arg.vid_file, t, arg.clip_dur, pr, gpus[0], mask = arg.mask, arg = arg, net = net)\r\n  File \"D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py\", line 294, in run\r\n    net.init()\r\n  File \"D:/Workspace/PythonProjects/studyProjects/multisensory/src/sep_video.py\", line 42, in init\r\n    pr, reuse = False, train = False)\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\sourcesep.py\", line 953, in make_net\r\n    vid_net_full = shift_net.make_net(ims, sfs, pr, None, reuse, train)\r\n  File \"D:\\Workspace\\PythonProjects\\studyProjects\\multisensory\\src\\shift_net.py\", line 419, in make_net\r\n    sf_net = slim.conv2d(sf_net,num_outputs= 64, kernel_size= [65, 1], scope = 'sf/conv1_1', stride = [4, 1], padding='SAME', reuse = reuse) \r\n  File \"C:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 1154, in convolution2d\r\n    conv_dims=2)\r\n  File \"C:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\nTypeError: convolution() got multiple values for argument 'weights_regularizer'", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue since the issues template was not filled out."]}, {"number": 36555, "title": "Transpose op only supports 1D-4D input arrays", "body": "When I run tflite object detection on android demo, I got this problem:\r\n\r\njava.lang.RuntimeException: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/contrib/lite/kernels/transpose.cc Transpose op only supports 1D-4D input arrays.Node number 148 (TRANSPOSE) failed to prepare.\r\n\r\nThanks ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nWhat is the top-level directory of the model you are using\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "  TF_LITE_ENSURE_MSG(context, dims <= 4,\r\n                     \"Transpose op only supports 1D-4D input arrays.\");\r\n", "Any update?", "TFLite doesn\u2019t support 5D tensors", "Yes. It is better to re-address this feature request for TF-Lite team in tensorflow repo.", "The feature is on the way. I'll update when it get merged.", "met this problem with my tflite model too", "if anybody has this problem whiile running tflite in pc, this problem has been solve with commit 97b63ed8769d09a70a6617b067e63a12c7f8e071, install newest tf-nightly should fix this.", "The issue should be fixed at the latest master now.", "Is this already available in prebuilt nightlys for Android? \r\nCould I use nightlys in the ```build.gradle```-file:\r\n\r\n```implementation 'org.tensorflow:tensorflow-lite:+' ```\r\n", "Seems ```implementation  'org.tensorflow:tensorflow-lite:0.0.0-nightly'/```did the trick :)"]}, {"number": 21747, "title": "device_properties_pb2 file was not found under the tensorflow.core.protobuf directory", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Nagging Assignee @rohan100jain: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21746, "title": "Adding op doc fixes", "body": "Doc Fix:\r\n* Errant `\\` in example code\r\n* Case sensitive anker link", "comments": ["Code has moved repository location. Closing this PR and re-creating here:\r\nhttps://github.com/tensorflow/docs/pull/28\r\n"]}, {"number": 21745, "title": "Distributed tensorflow worker hangs at TF_CloseSession() when using MonitoredTrainingSession", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  RHEL7 and also Mac OS X 10.13.6\r\n- **TensorFlow installed from (source or binary)**: binary (pip install)\r\n- **TensorFlow version (use command below)**: 1.9.0+\r\n- **Python version**: 2.7 (RHEL7), 3.6 (Mac)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nWhen using `MonitoredTrainingSession` in TensorFlow version 1.9 or higher, I'm seeing the following deadlock/hang (as reported by the `hanging-threads` pip package) when the context manager exits.  Note: I do not see this hang for versions 1.8 or earlier.  Also, note that this does not occur if using the older `tf.train.Supervisor` API.\r\n```\r\n----------     Thread 140682110711616 hangs       ----------\r\n\tFile \"trainer.py\", line 102, in <module>\r\n\t\ttf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n\t\t_sys.exit(main(argv))\r\n\tFile \"trainer.py\", line 67, in main\r\n\t\tprint(\"step: {}\".format(step))\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 689, in __exit__\r\n\t\tself._close_internal(exception_type)\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 726, in _close_internal\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1121, in close\r\n\t\t_WrappedSession.close(self)\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 690, in close\r\n\t\ttf_session.TF_CloseSession(self._session)\r\n```\r\n\r\nThe [code](https://gist.github.com/leewyang/7bf6d0df0328fbc4e2e97275d741a044) that generates this is based on the [Distributed Tensorflow](https://www.tensorflow.org/deploy/distributed) documentation (with a trivial/dummy model).  I start one PS node and two worker nodes on a single box as follows:\r\n```\r\nrm -rf /tmp/train_logs; \\\r\npython trainer.py \\\r\n     --ps_hosts=localhost:2222 \\\r\n     --worker_hosts=localhost:2223,localhost:2224 \\\r\n     --job_name=ps --task_index=0\r\n\r\npython trainer.py \\\r\n     --ps_hosts=localhost:2222 \\\r\n     --worker_hosts=localhost:2223,localhost:2224 \\\r\n     --job_name=worker --task_index=0\r\n\r\npython trainer.py \\\r\n     --ps_hosts=localhost:2222 \\\r\n     --worker_hosts=localhost:2223,localhost:2224 \\\r\n     --job_name=worker --task_index=1\r\n```\r\n\r\nI've been able to reproduce this quite consistently on:\r\n- Mac 10.13.6, Python 3.6, TensorFlow 1.10\r\n- RHEL7, Python2.7, TensorFlow 1.9\r\n\r\nAnd the symptom goes away when switching to 1.8 or earlier.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I've updated the original description w/ N/As.", "Any news for this defect, I got similar issue for distributed training.", "@yuefengz : Mind taking a look?\r\n\r\n(CC @mrry FYI)", "Any update \r\nI also met the problem.", "I also met this problem. Looks like grpc server inside the chief worker did't response to the close request.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6\r\nTensorFlow installed from (source or binary): binary (pip install)\r\nTensorFlow version (use command below): 1.12.0\r\nPython version: 2.7.5\r\n\r\nBacktrace of chief worker:\r\n```\r\n(gdb) bt\r\n#0  0x00007ffff6e15ec9 in syscall () from /lib64/libc.so.6\r\n#1  0x00007fff98241d24 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff982414f1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff9823ea34 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff9823ef55 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff93cecaa3 in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fff93ced6f8 in tensorflow::LocalMaster::CloseSession(tensorflow::CallOptions*, tensorflow::CloseSessionRequest const*, tensorflow::CloseSessionResponse*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fff93b7dbcc in tensorflow::GrpcSession::Close() ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fff93b6499a in tensorflow::SessionRef::Close() ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fff93d48acb in TF_CloseSession ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007fff93af4fd6 in _wrap_TF_CloseSession ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007ffff7af3cf0 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#12 0x00007ffff7af603d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#13 0x00007ffff7af353c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#14 0x00007ffff7af603d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#15 0x00007ffff7af353c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#16 0x00007ffff7af603d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#17 0x00007ffff7a7f978 in function_call () from /lib64/libpython2.7.so.1.0\r\n#18 0x00007ffff7a5aa63 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#19 0x00007ffff7a69a55 in instancemethod_call () from /lib64/libpython2.7.so.1.0\r\n#20 0x00007ffff7a5aa63 in PyObject_Call () from /lib64/libpython2.7.so.1.0\r\n#21 0x00007ffff7aef236 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#22 0x00007ffff7af603d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#23 0x00007ffff7af353c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n#24 0x00007ffff7af603d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0\r\n#25 0x00007ffff7af353c in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0\r\n(gdb) thread 1442\r\n[Switching to thread 1442 (Thread 0x7ffcb8ff9700 (LWP 2691597))]\r\n#0  0x00007ffff6e15ec9 in syscall () from /lib64/libc.so.6\r\n(gdb) bt\r\n#0  0x00007ffff6e15ec9 in syscall () from /lib64/libc.so.6\r\n#1  0x00007fff98241d24 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff982414f1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff9823ea34 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff9823ef55 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff93d10a1b in tensorflow::MasterSession::DeleteWorkerSessions() ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fff93d10e32 in tensorflow::MasterSession::Close() ()\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fff93cefbad in std::_Function_handler<void (), tensorflow::Master::CloseSession(tensorflow::CloseSessionRequest const*, tensorflow::CloseSessionResponse*, std::function<void (tensorflow::Status const&)>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n---Type <return> to continue, or q <return> to quit---\r\n   from /home/abclab/zhichyu/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007fff90f1b070 in ?? () from /lib64/libstdc++.so.6\r\n#9  0x00007ffff77fae25 in start_thread () from /lib64/libpthread.so.0\r\n#10 0x00007ffff6e1bbad in clone () from /lib64/libc.so.6\r\n(gdb) \r\n\r\n```\r\n\r\n", "same issue for me\r\n\r\ndistributed training on tf 1.10 , same code works on tf 1.7\r\n\r\nmaster is hang on following traceback, but worker already closed\r\n\r\n```\r\nStack for thread 140497678165760\r\n  File \"mnist.py\", line 235, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"mnist.py\", line 232, in main\r\n    sess.run(train_op)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 695, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 732, in _close_internal\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 980, in close\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1127, in close\r\n    _WrappedSession.close(self)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 980, in close\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 980, in close\r\n    self._sess.close()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 676, in close\r\n    tf_session.TF_CloseSession(self._session)\r\n\r\n```", "same issue for me too. For workaround, we set timeout for tf_session.TF_CloseSession(self._session).\r\n\r\nIn this function:\r\n  master_impl_->CloseSession(request, response, [&n, &ret](const Status& s) {\r\n    ret.Update(s);\r\n    n.Notify();\r\n  });\r\n.\r\n But ret.Update(s) and n.Notify() are not called. \r\n\r\nTherefore, TF_RETURN_IF_ERROR(WaitForNotification(call_options, default_timeout_in_ms_, &n))  hang forever.", "[Update: On closer inspection, the issue I encountered seems slightly different from the one reported in this issue, but I'll leave my post below in case it's helpful to someone.]\r\n\r\nI had a similar issue doing asynchronous training with MNIST where once the first worker finishes, the rest of the workers would hang. I managed to solve this by disabling communication between workers (each worker only needs to talk to the ps): https://github.com/linkedin/TonY/pull/120/files\r\n\r\n```\r\nconfig_proto = tf.ConfigProto(device_filters = ['/job:ps', '/job:worker/task:%d' % task_index])\r\n...\r\nwith tf.train.MonitoredTrainingSession(master=server.target,\r\n                                       is_chief=(task_index == 0),\r\n                                       checkpoint_dir=FLAGS.working_dir,\r\n                                       hooks=hooks,\r\n                                       config=config_proto) as sess:\r\n...\r\n```", "@erwa @yuefengz \r\nThe function Master::CloseSession in master.cc, the following code is blocked at `session->Close()`\r\n\t  SchedClosure([session, done]() { \r\n\t\t  Status s = session->Close(); \r\n\t\t  session->Unref();\r\n\t\t  done(s);\r\n\t  });\r\n.\r\n\r\nThen, in master_session.cc, MasterSession::Close() calls DeleteWorkerSessions().\r\nIn master_session.cc, MasterSession::DeleteWorkerSessions() deletes all workers using DeleteWorkerSessionAsync(). If a worker has been gone away, it will blocked at done.Wait().\r\n\r\nFor @erwa 's advise, in master_session.cc, MasterSession::DeleteWorkerSessions() only deletes the current worker. Because filtered_worker_list_ is {\"/job:ps/replica:0/task:0\", \"/job:worker/replica:0/task:0\"}.\r\n\r\nBut we do not known how to filter the workers have gone away.\r\n", "We set timeout for closing worker session.", "I have a similar problem when two workers didn't close at the same time.\r\n\r\nIn the following code, I deliberately delayed one session from closing, which could cause the program failing to terminate:\r\n\r\n```python\r\nimport os\r\nimport subprocess\r\nimport sys\r\nimport time\r\n\r\n_CLUSTER_SPEC = {\r\n    'ps': ['localhost:2222'],\r\n    'worker': ['localhost:2223', 'localhost:2224']\r\n}\r\n\r\n\r\ndef main_task(job_name, task_index):\r\n    import tensorflow as tf\r\n\r\n    server = tf.train.Server(server_or_cluster_def=_CLUSTER_SPEC, job_name=job_name, task_index=task_index)\r\n\r\n    if job_name == 'ps':\r\n        server.join()\r\n    elif job_name == 'worker':\r\n        with tf.device(tf.train.replica_device_setter(cluster=_CLUSTER_SPEC)):\r\n            train_op = tf.assign_add(tf.train.get_or_create_global_step(), 1)\r\n\r\n        print('[Worker {}] Creating session'.format(task_index), flush=True)\r\n\r\n        with tf.train.MonitoredTrainingSession(master=server.target,\r\n                                               is_chief=task_index == 0,\r\n                                               hooks=[tf.train.StopAtStepHook(last_step=10000)]) as session:\r\n            print('[Worker {}] Session created'.format(task_index), flush=True)\r\n\r\n            while not session.should_stop():\r\n                global_step = session.run(train_op)\r\n\r\n                if global_step % 1000 == 0:\r\n                    print('[Worker {}] Step = {}'.format(task_index, global_step), flush=True)\r\n\r\n            print('[Worker {}] Closing session'.format(task_index), flush=True)\r\n\r\n            # Delay the worker deliberately.\r\n\r\n            if task_index == 1:\r\n                time.sleep(1)\r\n\r\n        print('[Worker {}] Session closed'.format(task_index), flush=True)\r\n\r\n\r\ndef _new_task(process_list, job_name, task_index):\r\n    env = os.environ.copy()\r\n\r\n    env['GRPC_VERBOSITY'] = 'DEBUG'\r\n\r\n    process = subprocess.Popen(args=[sys.executable, __file__, job_name, str(task_index)], env=env)\r\n\r\n    process_list.append(process)\r\n\r\n    return process\r\n\r\n\r\ndef main_entry_point():\r\n    created_processes = []\r\n\r\n    try:\r\n        processes = {job_name: [_new_task(process_list=created_processes, job_name=job_name, task_index=task_index)\r\n                                for task_index in range(len(task_targets))]\r\n                     for job_name, task_targets in _CLUSTER_SPEC.items()}\r\n\r\n        for i, process in enumerate(processes['worker']):\r\n            process.wait()\r\n    finally:\r\n        for process in reversed(created_processes):\r\n            process.terminate()\r\n            process.wait()\r\n\r\n\r\ndef main(args):\r\n    if args:\r\n        job_name, task_index_str = args\r\n\r\n        main_task(job_name=job_name, task_index=int(task_index_str))\r\n    else:\r\n        main_entry_point()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main(sys.argv[1:])\r\n```\r\n\r\nI also have set `GRPC_VERBOSITY` to `DEBUG`, and I see worker 1 still tries to contact worker 0 even it is closed.", "@erwa thank you, that works fine", "I have a similar problem , when I use TPUEstimator.\r\nIn running \"TPUEstimator.train\",  I stop TPU instance, and it happens.\r\n", "Met the similar issue where the same code works fine for tf 1.6 and not for tf 1.12. Isn't this issue being solved ?", "Hi @leewyang !\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Could you confirm whether issue is replicating after [migrating](https://www.tensorflow.org/guide/migrate/migrate_tf2) to 2.6 or not ? Thanks!", "@mohantym yeah, this is really old now, and the TF2.x branch behaves differently, so feel free to close.", "Ok @leewyang ! Thanks for the confirmation.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 21744, "title": "Loading contrib ops in Windows via C API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.10.0.zip\r\n- **TensorFlow version (use command below)**:\r\n1.10.0\r\n- **Python version**:\r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nMSVC 14.15\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\ndescribed below\r\n\r\n### Describe the problem\r\nI am trying to load and run a graph with contrib ops (specifically lstm ops) trained in python via the C API. On Linux and Mac, I am able to do this using `TF_LoadLibrary` and the `_lstm_ops.so` from the python wheel. However, doing the same on Windows yields an error status `<path to _lstm_ops.dll> not found`, and I am sure I entered the global path correctly (and thus am confused why the error seems to be with finding the library itself, not with registering the ops).  Is there something I am missing? Any tips/suggestions someone can provide?\r\n\r\nFurther, is there a better/more supported way of doing this in general?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler apologies! i have updated my comment with the missing fields.", "@robieta are there any update you can provide on this issue? thanks!", "Is there any update regarding this issue? Thanks!", "And I have found [this stack overflow question](https://stackoverflow.com/questions/50475320/executing-frozen-tensorflow-graph-that-uses-tensorflow-contrib-resampler-using-c) that states that you cannot do this on Windows since there is no separate .dll ", "Hi @adamAlnatsheh ! Contrib has removed from 2.x versions . Please refer [addons](https://www.tensorflow.org/addons) and migration [document](https://www.tensorflow.org/guide/migrate) to upgrade your codebase.Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 21743, "title": "Add caisq as code owner for tfdbg-related folders", "body": "", "comments": ["Thanks @caisq. I included this change in the batch update. Closing this."]}, {"number": 21742, "title": "An exception has occurred, use %tb to see the full traceback.", "body": "\r\n\r\n### System information\r\n- I copied/pasted the classify_image.py\r\n- window 10 64 bits\r\n- anaconda 64 bits / anaconda navigator 1.8.7 / spyder 3.3.1\r\n- TensorFlow version 1.10 installed form anaconda/python prompt\r\n- **Python version**: 3.5 spyder (installed 3.7)\r\n\r\nHello guys,\r\n\r\nI opened classify_image.py in spyder from the master of tensorflow (cloned on my PC before), When I executed the script I have this  message:\r\n![classify](https://user-images.githubusercontent.com/17961650/44364942-9a10b200-a4c8-11e8-9edc-04fd48fb0d60.jpg)\r\n\r\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\r\nrunfile('E:/developpements/tensorflow_projects/models-master/tutorials/image/imagenet/classify_image.py', wdir='E:/developpements/tensorflow_projects/models-master/tutorials/image/imagenet')\r\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.89632)\r\nindri, indris, Indri indri, Indri brevicaudatus (score = 0.00766)\r\nlesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00266)\r\ncustard apple (score = 0.00138)\r\nearthstar (score = 0.00104)\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit\r\n\r\nE:\\developpements\\anaconda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\r\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\r\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\r\n\r\n\r\n(tensorflow) C:\\Users\\david>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n\r\nthanks,\r\n\r\nDavid.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "When I executed the script I have this message:\r\n![p4](https://user-images.githubusercontent.com/47982518/53351309-e56fa700-38d5-11e9-9a2b-c342e0b162cc.PNG)\r\n\r\n\r\n\r\nmaster\\p1.PNG\r\n               [C:\\Users\\Tahira Shehzadi\\Desktop\\Master\\semester1\\MLCS\\Image-Super-Resolution-master\\p1.PNG ...]\r\nmain.py: error: the following arguments are required: C:\\Users\\Tahira Shehzadi\\Desktop\\Master\\semester1\\MLCS\\Image-Super-Resolution-master\\p1.PNG\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 2\r\n\r\nC:\\Users\\Tahira Shehzadi\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\r\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\r\n\r\n\r\n\r\n", "can any one help me  how  to remove  this error An exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 2\r\n\r\n", "\"parser = argparse.ArgumentParser(description='Code for Changing the contrast and brightness of an image! tutorial.')\r\nparser.add_argument('--input', help='Path to input image.', default='id1.jpg')\r\nargs = parser.parse_args()\"    remove this part and replace this code (image = cv.imread(cv.samples.findFile(args.input))) to image = cv.imread(\"your_image_path\")\r\n", "@72ohit this does not solve the error. Did you figure out why the error is occurring due to parse_args()?", "> can any one help me how to remove this error An exception has occurred, use %tb to see the full traceback.\r\n> \r\n> SystemExit: 2\r\n\r\npython my_programme.py --arg1=5 --arg2=7", "can anyone help me ?\r\nmy error:\r\nthe following arguments are required: -i/--images/example_test.png\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 2", "the function takes -i and image_name as argument. So to run the app you need to type on the console: \r\npython color_detection.py -i colorpic.jpg", "use \r\napp.run(debug=True, use_reloader=False)\r\n\r\n", "> ### System information\r\n> * I copied/pasted the classify_image.py\r\n> * window 10 64 bits\r\n> * anaconda 64 bits / anaconda navigator 1.8.7 / spyder 3.3.1\r\n> * TensorFlow version 1.10 installed form anaconda/python prompt\r\n> * **Python version**: 3.5 spyder (installed 3.7)\r\n> \r\n> Hello guys,\r\n> \r\n> I opened classify_image.py in spyder from the master of tensorflow (cloned on my PC before), When I executed the script I have this message:\r\n> ![classify](https://user-images.githubusercontent.com/17961650/44364942-9a10b200-a4c8-11e8-9edc-04fd48fb0d60.jpg)\r\n> \r\n> \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\r\n> runfile('E:/developpements/tensorflow_projects/models-master/tutorials/image/imagenet/classify_image.py', wdir='E:/developpements/tensorflow_projects/models-master/tutorials/image/imagenet')\r\n> giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.89632)\r\n> indri, indris, Indri indri, Indri brevicaudatus (score = 0.00766)\r\n> lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00266)\r\n> custard apple (score = 0.00138)\r\n> earthstar (score = 0.00104)\r\n> An exception has occurred, use %tb to see the full traceback.\r\n> \r\n> SystemExit\r\n> \r\n> E:\\developpements\\anaconda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\r\n> warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\r\n> \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\r\n> \r\n> (tensorflow) C:\\Users\\david>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n> b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n> \r\n> thanks,\r\n> \r\n> David.\r\n\r\nHi \r\nI am also facing the same issue. Can you please share the solution. \r\n\r\nRegards,\r\nKrishna ", "> use\r\n> app.run(debug=True, use_reloader=False)\r\n\r\nIt worked for me, tnx a lot.", "I have the same problem: `usage: get_frozen_graph.py [-h] --checkpoint CHECKPOINT\r\n                           [--out_graph_name OUT_GRAPH_NAME]\r\nget_frozen_graph.py: error: the following arguments are required: --checkpoint\r\nAn exception has occurred, use %tb to see the full traceback. SystemExit: 2` \r\ncould you please tell me how can I use `app.run(debug=True, use_reloader=False)`  in my code", "> > use\r\n> > app.run(debug=True, use_reloader=False)\r\n> \r\n> It worked for me, tnx a lot.\r\n\r\nit showing error like app no define . i tried other things but not solve error ..plzz suggest .", "I have the same problem: usage: untitled1.py [-h] video\r\nuntitled1.py: error: the following arguments are required: video\r\nAn exception has occurred, use %tb to see the full traceback.\r\ncould you please tell me how can I use app.run(debug=True, use_reloader=False) in my code", "use\r\n app.run_server(debug=True, use_reloader=False)"]}, {"number": 21741, "title": "CUDA 10 release", "body": "Is Cuda toolkit 10 compatible with the latest version of Tensorflow?\r\nAre there any benchmarks?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Have I written custom code?\r\nI have not. Have yet to buy the GPU.\r\n\r\nOS Platform and Distribution?\r\nLinux(Ubuntu 16.04) \r\n\r\nTensorFlow installed from?\r\nNA\r\n\r\nTensorFlow version\r\nNA\r\n\r\nBazel version\r\nNA\r\n\r\nCUDA/cuDNN version?\r\nThis is actually the question. Will Tensorflow support CUDA 10, and how soon will it happen.\r\n\r\nGPU model and memory\r\nGEFORCE RTX 2080 Ti\r\n\r\nExact command to reproduce\r\nNA\r\n\r\nMobile device\r\nNA", "Stupid bot just answer the question", "Looks like official tensorflow package is not compatible with cuda 10, but there's a dockerized version named `tensorflow-gpu 1.10.0+nv`(should be a custom build from Nvidia) works.\r\nThe version is available in `docker pull nvcr.io/nvidia/tensorflow:18.09-py3`, it's rather a workaround to use tensorflow under cuda 10, still need official support...\r\n", "@GeorgeKaspar  TensorFlow supports CUDA 9.0. For using TF with cuda 10, you have to build it from sources yourself. Here is the documentation for that:\r\nhttps://www.tensorflow.org/install/source_windows", "By when can we expect TF to support cuda 10?", "I have the same problem ,when can TF support cuda 10?\r\n", "Is there a timeline when we can expect official Support of TF on CUDA 10 ?", "I am also interested to know when we can expect Cuda 10 support.", "At least next release, since v1.12 has been cut and it doesn\u2019t support CUDA 10 in the prebuilt binaries.\r\n\r\nBeing said that, you could still compile TF by yourself. I have done that with CUDA 10 and I have not noticed any problems.", "OS Platform and Distribution?\r\nFedora 28\r\n\r\nTensorFlow installed from?\r\nCompiled, with downgraded gcc 7.3, because fedora 28 comes with gcc8\r\n\r\nTime to compile it?\r\nreal    122m13.771s                                                             \r\nuser    0m0.705s                                                                \r\nsys     0m1.311s  \r\n\r\nTensorFlow version\r\n1.11\r\n\r\nTensorRT version:\r\nTensorRT-5.0.0.10.Red-Hat.x86_64-gnu.cuda-10.0.cudnn7.3.tar.gz\r\n\r\nNCCL version:\r\nnccl_2.3.5-2+cuda10.0_x86_64.txz\r\n\r\nBazel version\r\nBuild label: 0.17.2- (@non-git)\r\n\r\nGPU model and memory\r\nNvidia 1070, GPU Memory: 8GB", "I believe you can compile tensorflow with cuda 10 support. Nvidia also provides the compiled tensroflow with cuda 10  in their docker cloud. ", "TF 2.0 will have CUDA 10 support and TensorRT. https://www.tensorflow.org/community/roadmap", "Hey,\r\nlook at my link. I created an project. Download the file! It works fine!\r\n\r\nhttps://github.com/Lxnus/tensorflow_r1.12_cuda_10", "I found a page: \r\n[https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-ubuntu/](url)\r\nIt shows how to  build Tensorflow using bazel.", "TF 1.13.0-rc0 has released and it contains pre-built binaries to support cuda 10. Give it a try. Thanks!"]}, {"number": 21740, "title": "How to fix ValueError: Cannot feed value of shape (0, 0) for Tensor 'Placeholder_85:0', which has shape '(?, 10)'", "body": "ValueError: Cannot feed value of shape (0, 0) for Tensor 'Placeholder_85:0', which has shape '(?, 10)'", "comments": ["I used parent_dir = os.getcwd(), it works", "Nagging Assignee @jart: It has been 105 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue since its resolved. Thanks!"]}, {"number": 21739, "title": "Add installation of keras_applications in the documentation of intall from sources", "body": "This fix tries to address the issue raised in #21734 where\r\nthe installation of keras_applications was not mentioned in the\r\ndocumentation of \"intall from sources\".\r\n\r\nThis fix add the steps of keras_applications install.\r\nThis fix fixes #21734.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["The `keras_application` module is available as `keras-application` pip package. Which is also automatically installed when install using `pip install -U keras`\r\nThe pypi project page of keras-application `https://pypi.org/project/Keras-Applications/`", "Thanks @sanjibnarzary. The `pip install keras_applications` matches the current build process in tf:\r\nhttps://github.com/tensorflow/tensorflow/blob/92a94f30523f02dd69e3d055a4c258367a37fdde/tensorflow/tools/docker/Dockerfile#L32\r\n\r\nThe `keras` itself is not the dependency of tensorflow I believe.", "@fchollet FYI -- I think you mentioned you have a fix for this in mind that wouldn't require keras_applications to be present during build?", "Nagging Assignee @akshaym: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @akshaym: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I checked the documentation website and think the issue has been resolved. So this PR is not relevant any more. I will close this PR but do appreciate the help \ud83d\udc4d .", "I checked the documentation website and think the issue has been resolved. So this PR is not relevant any more. I will close this PR but do appreciate the help \ud83d\udc4d ."]}, {"number": 21738, "title": "Model learns with keras but doesn't learn with tf.keras", "body": "The below code would just work fine on keras,\r\n\r\n````python\r\nimport tensorflow as tf\r\n\r\nfrom keras.layers import Flatten,Dense,Dropout\r\nfrom keras.models import Sequential\r\n\r\n#from tensorflow.python.keras.layers import Flatten,Dense,Dropout\r\n#from tensorflow.python.keras.models import Sequential\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nimport keras\r\nprint(f'tf version = {tf.__version__} ')\r\n\r\nmodel = Sequential([\r\n  Flatten(input_shape=(28,28)),\r\n  Dense(512, kernel_initializer='uniform', activation=tf.nn.relu),\r\n  Dropout(0.2),\r\n  Dense(10, kernel_initializer='uniform', activation=tf.nn.softmax)\r\n])\r\n\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=1)\r\nmodel.evaluate(x_test, y_test)\r\n````\r\n\r\n\r\n**KERAS**\r\n````Using TensorFlow backend.\r\ntf version = 1.10.0 \r\nEpoch 1/1\r\n2018-08-20 20:57:06.754898: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n\r\n   32/60000 [..............................] - ETA: 14:00 - loss: 2.3347 - acc: 0.1250\r\n  224/60000 [..............................] - ETA: 2:14 - loss: 2.1174 - acc: 0.3839 \r\n  416/60000 [..............................] - ETA: 1:19 - loss: 1.8965 - acc: 0.4712\r\n  576/60000 [..............................] - ETA: 1:03 - loss: 1.7533 - acc: 0.5365\r\n  ....\r\n .....\r\n59744/60000 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9308\r\n59936/60000 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9310\r\n60000/60000 [==============================] - 22s 360us/step - loss: 0.2363 - **acc: 0.9310**\r\n````\r\nbut if you the import layers from tf.keras instead of the keras(i.e comment lines 3 & 4 and uncomment lines 6 & 7), the weights won't be updated and model doesn't learn at all.\r\n\r\n**tf.keras**\r\n````\r\nUsing TensorFlow backend.\r\ntf version = 1.10.0 \r\nEpoch 1/1\r\n2018-08-20 21:08:05.112916: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n   32/60000 [..............................] - ETA: 10:26 - loss: 14.1033 - acc: 0.1250\r\n  224/60000 [..............................] - ETA: 1:42 - loss: 14.0464 - acc: 0.1205 \r\n  448/60000 [..............................] - ETA: 58s - loss: 14.3412 - acc: 0.1027 \r\n....\r\n....\r\n59744/60000 [============================>.] - ETA: 0s - loss: 13.2745 - acc: 0.1744\r\n59904/60000 [============================>.] - ETA: 0s - loss: 13.2730 - acc: 0.1745\r\n60000/60000 [==============================] - 19s 323us/step - loss: 13.2708 - **acc: 0.1746**\r\n````\r\n\r\nThe problem is with **kernel_initalizer**, if you give a unsupported initalizer instead of giving an error tf.keras just fails somewhere. However keras works just fine. \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution - Macbook Pro , MacOS High Sierra - 10.13.5\r\nTensorFlow installed from - Conda install\r\nTensorFlow version - 1.10.0\r\nBazel version - NA\r\nCUDA/cuDNN version - NA\r\nGPU model and memory  - NA\r\nExact command to reproduce - Given Above\r\nMobile device NA", "Hi @kmadhugit, it looks like the default values for some tf.keras initializers are different than those in external Keras, causing this problem. I have a fix in progress, in the meantime you can get the desired behavior with:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.keras.layers import Flatten,Dense,Dropout\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.initializers import RandomUniform\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nimport keras\r\nprint(f'tf version = {tf.__version__} ')\r\n\r\nmodel = Sequential([\r\n  Flatten(input_shape=(28,28)),\r\n  Dense(512, kernel_initializer=RandomUniform(minval=-0.05, maxval=0.05), activation=tf.nn.relu),\r\n  Dropout(0.2),\r\n  Dense(10, kernel_initializer=RandomUniform(minval=-0.05, maxval=0.05), activation=tf.nn.softmax)\r\n])\r\n\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=1)\r\nmodel.evaluate(x_test, y_test)\r\n```\r\n\r\nI'll update this thread when the fix is available\r\n", "The default initializers seems to be working fine, if I comment  kernel_initializer it would work fine as below,\r\n\r\n````python\r\nimport tensorflow as tf\r\n\r\n#from keras.layers import Flatten,Dense,Dropout\r\n#from keras.models import Sequential\r\n\r\nfrom tensorflow.python.keras.layers import Flatten,Dense,Dropout\r\nfrom tensorflow.python.keras.models import Sequential\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nimport keras\r\nprint(f'tf version = {tf.__version__} ')\r\n\r\nmodel = Sequential([\r\n  Flatten(input_shape=(28,28)),\r\n  Dense(512,\r\n#       kernel_initializer='uniform',\r\n        activation=tf.nn.relu),\r\n  Dropout(0.2),\r\n  Dense(10,\r\n#       kernel_initializer='uniform',\r\n        activation=tf.nn.softmax)\r\n])\r\n\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=1)\r\nmodel.evaluate(x_test, y_test)\r\n````\r\nit fails only when I give a wrong initializer. \r\n", "@kmadhugit I'm not quite following--are you saying that an unknown value for kernel_initializer is handled fine by keras, but not for tf.keras? ", "Just pushed the fix for this yesterday, it should be available in the nightly build shortly. The 'uniform' initializer is known, the issue was that the default values for `minval` and `maxval` for this initializer were set too large for the model to converge. I sync'd up their defaults with those in external Keras", "I thought TF doesn't understand the 'uniform' initializer but @omalleyt12 just mentioned that its a problem with minval and maxval.\r\n\r\n", "@kmadhugit the fix should be available in the nightly build now, could you try ```pip install tf-nightly``` (or ```pip install tf-nightly-gpu``` for GPU support) and let me know if the model is now training in tf.keras for you?", "Hi @omalleyt12, Thank you. The model starts learning now, I think you have capped min and max values for 'uniform' initializer to (-0.05, 0.05) from (0,1). Can you give me the link to the PR, I like to have a look at the code changes.", "Great, sure here's the PR:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/de05ebe296d83607ca0ab1803bd8eed6afa7f74f\r\n\r\nAffected initializers were RandomUniform, RandomNormal, and TruncatedNormal"]}, {"number": 21736, "title": "Is it EEG a tool for Performance tracing open-sourced ?", "body": "As describe in paper 9.2 section [TensorFlow:\r\nLarge-Scale Machine Learning on Heterogeneous Distributed Systems](http://download.tensorflow.org/paper/whitepaper2015.pdf):\r\n\r\n> We also have an internal tool called EEG (not included in the initial open source release in November, 2015) that we use to collect and visualize very fine-grained information about the exact ordering and performance...\r\n\r\nI want to know that Does EEG open source? \r\nEEG includes 3 tools:\r\n\r\n1. Linux ftrace\r\n2. Internal Google tracing tools\r\n3. The CUDA Profiling Tools Interface\r\n\r\nAnd what's the method of \"internal Google tracing tools\"?\r\n\r\nHave I written custom code:  N/A\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: docker\r\nTensorFlow version: 1.8.0\r\nBazel version: N/A\r\nCUDA/cuDNN version 9.1, 7.0\r\nGPU model and memory P100, 16GB\r\nExact command to reproduce N/A\r\nMobile device N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler Yes, I have update these field.", "Nagging Assignee @drpngx: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@prb12 care to comment on this?", "Note: This isn't a bug or issue, so should not have been filed on GitHub issues. \r\n\r\nNo, EEG is not open source.  It makes extensive use of some google-specific libraries for thread tracing, as well as an internal wrapper around Linux kernel tracing that would be difficult to open-source.\r\n\r\nThe trace-visualization server wouldn't be too hard to port, but somebody would have to provide all of the equivalent kernel-level context-switch traces and application-level threadpool tracing."]}, {"number": 21735, "title": "sparse_categorical_crossentropy of keras is not available", "body": "https://github.com/tensorflow/tensorflow/blob/05f8ea8e9522a3027d4f3f7a54d716bfafed427a/tensorflow/python/keras/metrics.py#L584-L591\r\n\r\nI don't know why `sparse_categorical_crossentropy` is NOT decorated by `tf_export` like `binary_accuracy`, `sparse_top_k_categorical_accuracy` and so on.\r\nAs a result , we have to explicitly import `sparse_categorical_crossentropy`\r\n i.e. `from tensorflow.keras.metrics import sparse_categorical_crossentropy`\r\n\r\n*additional information*:\r\nHave I written custom code: No\r\nOS Platform and Distribution:  Ubuntu 16.04.5 LTS\r\nTensorFlow installed from: pypi\r\nTensorFlow version: tensorflow-gpu 1.10.0\r\nBazel version: 0.15.0\r\nCUDA/cuDNN version: CUDA 9.0, cuDNN v7.2.1 \r\nGPU model and memory: nvidia GTX 1080TI, 11GB GDDR5\r\n Exact command to reproduce: `from tensorflow import keras; metric_func =keras.metrics.sparse_categorical_crossentropy`\r\nMobile device: N/A\r\nThanks ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler @tatianashp I have complemented some information. Thanks", "Seems like a mistake. Would you like to create a Pull Request to fix it?", "@facaiy  Thanks I will pull request soon", "Nagging Assignee @tatianashp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21734, "title": "Latest tensorflow build from source requires keras but no where in the docs it is mentioned", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: latest\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.16\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.2/7.2\r\n- **GPU model and memory**: NVidia Tesla V100/16 GB\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package with default configuration\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nLatest tensorflow build from source requires keras but it is not mentioned in the documentation.\r\n\r\n![screenshot from 2018-08-20 19-06-00](https://user-images.githubusercontent.com/1001052/44344133-86a41d80-a4ad-11e8-980a-13430f2a3fce.png)\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Added a PR #21739 to update the docs for keras_applications install.", "Nagging Assignee @fchollet: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It looks like the issue has been fixed by the latest documentation. Will close this issue but feel free to re-open if there are still issues."]}, {"number": 21733, "title": "Update SparseTensor docstring", "body": "`SparseTensor` doesn't return anything in `__init__`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLA signed.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21732, "title": "Request for functions to handle duplicates in SparseTensors", "body": "`tensorflow` currently handles duplicates in `SparseTensor` by keeping the value for the last index of a repeated coordinate. However, some operations for this would come in quite handy, namely non-max or non-min suppression (keeping the maximum or minimum values for repeated coordinates), sum (summing the values for repeated coordinates) or mean (averaging the values for repeated coordinates). \r\n\r\nThere is currently a solution for this in https://stackoverflow.com/questions/38233821/merge-duplicate-indices-in-a-sparse-tensor. It works great, but to be honest it feels a bit clunky to use.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 21731, "title": "Issue with tensorflow runtime. Failed to load DLL.", "body": "\r\nThe issue \r\nI have installed tensorflow 1.10.0 on my Win 7 machine and trying to do an \r\nimport tensorflow as tf \r\ngives an \"Dll failed to load\" error. \r\n\r\n\r\n### Source code / logs\r\n Code\r\nimport tensorflow as tf\r\n\r\nError on the console\r\nrunfile('C:/Users/Teluser/Desktop/Megha/FujitsuCode/test_tensorflow.py', wdir='C:/Users/Teluser/Desktop/Megha/FujitsuCode')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-88270b61ce99>\", line 1, in <module>\r\n    runfile('C:/Users/Teluser/Desktop/Megha/FujitsuCode/test_tensorflow.py', wdir='C:/Users/Teluser/Desktop/Megha/FujitsuCode')\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/Teluser/Desktop/Megha/FujitsuCode/test_tensorflow.py\", line 8, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nI found many such issues but no exact resolution. Please can anyone help me here to understand what I am missing ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I saw this exact stack trace when updating from TF 1.8 to 1.10 on Windows 10. Installing 2015 Visual C++ Redistributable fixed the import for me. I think the error was happening due to missing vcomp140.dll, but I'm not 100% sure because the output of Dependency Walker on Windows 10 is cluttered with a lot of errors about missing DLLs that have actually just been moved, and I didn't bother to check exactly what DLL was causing the error after it was fixed. However, I'm not sure how using anaconda might change the environment compared to using pip. If installing that doesn't work, you could also try using Dependency Walker and see what dependencies it finds are missing.", "I have downloaded TF 1.9 and that seemed to fix the issue. Probably problem with TF1.10.\r\n\r\nThanks ", "@andrewgconnors I took your advice and installed the 2015 Visual C++ Redistributable.  Now I can import tensorflow without error dll. Then I checked the `windows\\system32\\` and found the `msvcp140.dll` was there (before there was not)... Thx!"]}]