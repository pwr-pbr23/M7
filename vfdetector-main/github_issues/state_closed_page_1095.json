[{"number": 20417, "title": "Update kafka to v0.11.4", "body": "\r\nThis fix updates kafka from v0.11.1 to v0.11.4\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20416, "title": "How to uninstall tensorflow or reset pip installs, to do a fresh install?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi @Curious-Nikhil. You didn't provide any details here except for your vague issue title. I'm assuming you are trying to uninstall everything from your `pip` environment. You can find this answer easily on StackOverflow at [\"What is the easiest way to remove all packages installed by pip?\"](https://stackoverflow.com/questions/11248073/what-is-the-easiest-way-to-remove-all-packages-installed-by-pip)\r\n\r\nPlease read and fill out the issue template next time. These types of questions should be directed to [StackOverflow](https://stackoverflow.com/) and not here since this is not an issue with TensorFlow. ", "Sry, my bad!\r\nthis is the new post - #20439 \r\nhttps://github.com/tensorflow/tensorflow/issues/20439 \r\n", "switch to this - #20439"]}, {"number": 20415, "title": "nnapi error: unable to open library libneuralnetworks.so Model provided has model identifier 'Plac', should be 'TFL3'  2018-06-29 14:52:30.475235+0200 tflite_simple_example[3723:177396] Failed to mmap model ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Mac OS Sierra)**: Mac OS Sierra\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: Run it as an ios application\r\n\r\n\r\n\r\n### Describe the problem\r\nI try to run the demo application for ios with tensorflow lite. With your model it works, but when I put my model it can not load it.\r\n\r\n### Source code / logs\r\n```\r\n2018-06-29 14:52:20.618818+0200 tflite_simple_example[3723:177396] [MC] Lazy loading NSBundle MobileCoreServices.framework\r\n2018-06-29 14:52:20.620767+0200 tflite_simple_example[3723:177396] [MC] Loaded MobileCoreServices.framework\r\n2018-06-29 14:52:20.883003+0200 tflite_simple_example[3723:177396] [MC] System group container for systemgroup.com.apple.configurationprofiles path is /Users/macbook13/Library/Developer/CoreSimulator/Devices/25BA8841-5579-4FBB-ABE4-CEAEF108661B/data/Containers/Shared/SystemGroup/systemgroup.com.apple.configurationprofiles\r\n**nnapi error: unable to open library libneuralnetworks.so**\r\n**Model provided has model identifier 'Plac', should be 'TFL3'**\r\n\r\n**2018-06-29 14:52:30.475235+0200 tflite_simple_example[3723:177396] Failed to mmap model my_model_tflite_graph.**\r\n```\r\nPlease anyone who can tell me how to fix this.", "comments": ["@aselle @lmoroney @mikeknapp Please review my issue, I really do not know what to do...", "Hi, was there any solution for that issue?"]}, {"number": 20414, "title": "cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE ... Failed to create session", "body": "Hello everyone, when I execute this code :\r\n\r\n```\r\nimport tensorflow as tf\r\nhello = tf.constant('hi,tensorflow')\r\nsess = tf.Session()\r\n```\r\n\r\nI get each time the same error :\r\n\r\n```\r\n2018-06-29 15:14:01.587077: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow bhis actinary was not compiled to use: AVX2 FMA\r\n2018-06-29 15:14:01.694678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-29 15:14:01.695507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: Quadro P400 major: 6 minor: 1 memoryClockRate(GHz): 1.2525\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 1.95GiB freeMemory: 1.92GiB\r\n2018-06-29 15:14:01.715820: E tensorflow/core/common_runtime/direct_session.cc:154] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\nTraceback (most recent call last):\r\n  File \"/home/mounir/PycharmProjects/ssd_keras-master/ssd512_inference.py\", line 6, in <module>\r\n    sess = tf.Session()\r\n  File \"/home/mounir/anaconda3/envs/gpukeras/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1560, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/mounir/anaconda3/envs/gpukeras/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 633, in __init__\r\n    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n\r\n```\r\nI have two GPUs in my computer, the P400 is the most powerful one.\r\nI tried to troubleshoot it by adding to .bashrc export CUDA_VISIBLE_DEVICES with alternatively these values =0, =1, =0,1, but it outputs always the same error\r\n\r\n\r\n**Have I written custom code :** \r\n```\r\nimport tensorflow as tf\r\nhello = tf.constant('hi,tensorflow')\r\nsess = tf.Session()\r\n```\r\n**OS Platform and Distribution** :  Linux, Ubuntu 18.04 \r\n**TensorFlow installed from** : PyPI\r\n**TensorFlow version** 1.8.0\r\n**Bazel version** Seems not installed\r\n**CUDA/cuDNN version** Cudatoolkit installed within the virtual env is 9.0 and Cuda installed within the system is 9.1\r\n**GPU model and memory** P400 Quadro, 2 GB\r\n**Exact command to reproduce** \r\n```\r\nimport tensorflow as tf\r\nhello = tf.constant('hi,tensorflow')\r\nsess = tf.Session()\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I'm not sure what you mean by \"Cudatoolkit installed within the virtual env is 9.0\". I do not think it's possible to install CUDA in a virtualenv. You should only have one CUDA version, and currently the TensorFlow nightlies only work with Cuda 9.0. Try replacing the system CUDA with 9.0 if possible, or use docker.", "I'll be back at my office on Thursday, I'll retry it at that moment !", "Nagging Assignee @zheng-xq: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @zheng-xq: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hello, I abandoned the double GPU setup. I kept only the Quadro P400, and I am intending to reinstall everything from scratch. This problem was probably, essentially, due to a bad connection between Cuda and the GPU version of Tensorflow. I think that it is suitable to close the issue as I may have understood what the source of the problem was."]}, {"number": 20413, "title": "Syntactic Sugar: Utilizing dict comprehension", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "hi @Horstage, do you mind fixing the pylint error in the sanity build?", "Thank you @Horstage!", "Nagging Assignee @yifeif: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Horstage could you pull rebase and push again? Thanks."]}, {"number": 20412, "title": "BindToDevice() binds graph to specified (gpu) device which forces all its operations to be prcessed on that device.", "body": "Export to golang as well.\r\n\r\nIf you want to implement strict processing of graph on specified device (for example one GPU among multiple processors), one must bind graph or separate operations to that device. Only C++ API somewhat support that, this patch makes it easier to use and creates helper functions for C and golang.\r\n\r\nN.B. no matter which settings you use, currently (master of jun 27 and 1.6.0 release) all golang inference happens on CPU. Before 1.6 one could play with GPUConfig.VisibleDeeviceList, but it crashes currently (there are some reasons for that), and anyway always binds to CPU. So, the side effect of this patch is that one can not only tune GPU execution but turn if on again.", "comments": ["crash issue: https://github.com/tensorflow/tensorflow/issues/19083\r\nresolution for c++: https://github.com/tensorflow/tensorflow/issues/18861", "Thanks for the PR @bioothod , but I'd like to understand more of your use case first.\r\n\r\nFirstly, could you describe the issue you were running into with using the GPU? By default, if linked with the GPU-enabled version of the TensorFlow C library, the Go program should automatically select a GPU for kernels where this is appropriate. If that is not happening, it seems like something we should fix.\r\n\r\nSecondly, the graph construction API as it stands intentionally does not allow nodes to be mutated after they are added to the graph, as this may lead to confusing behavior (e.g., if two `Session`s are created on the same graph and the graph is modified between the two). The `TF_BindToDevice` function added here is a mutation interface, so I'm a little weary of it. You'll notice that the workaround suggested operated on the `GraphDef` protocol buffer (adding the device name to nodes there) instead of operating on the in-memory representation of the graph.\r\n\r\nLooking forward to your response.", "> Firstly, could you describe the issue you were running into with using the GPU? By default, if linked with the GPU-enabled version of the TensorFlow C library, the Go program should automatically select a GPU for kernels where this is appropriate. If that is not happening, it seems like something we should fix.\r\n\r\nIt should, but it does not. I have a pretty simple golang application, which uses default session options, and while I can list all gpu devices on the board, session is never being bound to any of them. It changed probably around 1.6, but I can not say for sure. Literally the same code binds to random gpus on 1.4.1 and it does not on 1.6.0 or recent master. Previously I used `nvidia-smi` for example to monitor GPU load and saw (and now do not) load on one or another card. If I use custom session option with LogDevicePlacement=true, then I see that session is always placed on CPU on 1.6.0.\r\n\r\nBut it is only part of the problem, I also want to run particular session on particular GPU device among several available. Previously I could tune VisibleDeviceList in gpu part of session options, and things worked great. But after 1.6.0 you decided that multiple virtual mapping into the same physical device might confuse some operations (do not know how is it ever possible, but still), so golang code panics now if you ever touch VisibleDeviceList with something different than CUDA_VISIBLE_DEVICES (or empty string).\r\n\r\nHence this patch - now I can create multiple graphs each of them bind to own gpu device and select them when creating new session according to my policies. GraphDef solution is quite heavy for many sessions, and anyway c++ code has ability to access graph nodes, and `TF_BindToDevice` is just a nice wrapper to allow that same functionality. As long as there is a way to bind graph to device (that's what new golang function `ImportWithDevice()` does), its ok with me to hide details, but there is no easy way to iterate over graph nodes in go bindings, so there is a wrapper function.", "Thanks for the note. Let's separate the two issues.\r\n\r\nThe GPU should work fine in 1.6+. I just tried with 1.9 and it does seem to work.\r\nSee https://gist.github.com/asimshankar/ef367e4897e248466c42c2dc629814e0\r\n\r\nParticularly the lines in the output:\r\n```\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\nIf the same is not happening in the setup you're running it, it's worth investigating. So this PR shouldn't be required to enable use of the GPU.\r\n\r\nThat said, I appreciate that running the same model on multiple devices isn't as smooth as it should be and that experience can be improved. However, I don't think that this approach is the best one. As I mentioned earlier, I'm weary of mutations to the graph since that can lead to confusing behavior with multiple sessions. Furthermore, the implementation here is forcing every node to run on GPU - which will be problematic if the graph has nodes that only have CPU kernels (unless the user set `soft_device_placement=True`). Thus, I don't think this works out as an appropriate general purpose solution (though it may work out just fine for specific models).\r\n\r\nIn both cases (whether the approach in this PR, or via having the process that writes out the graph to import) we're creating multiple sessions, each with their own copy of the graph.\r\n\r\nOne somewhat ugly workaround is to have the program that creates the graph create a single saved model, with one tag per GPU. Then the Go program can create one session per GPU by providing the right tag to `LoadSavedModel`. Though, yeah, I admit this is a bit cumbersome. Another possibly cleaner option would be to have `ImportGraphDef` take a device specification and use that (similar to how `tf.import_graph_def` in Python respects the device stack). Though, that will be a bit more work.\r\n\r\nThanks for your understanding.\r\n\r\n", "That's not quite what I'm working with, problem with GPU placement happens when you load new graph from protobuf and run in in go.\r\n\r\nI've made a simple example repo to highlight the problem: https://github.com/bioothod/golang_gpu_example\r\n\r\nIf you clone it into `$GOPATH/src` and run following commands, then you will find the problem:\r\n```\r\n$ cd golang_gpu_example\r\n$ python3 ./make_graph.py --output test.pb \r\n$ go build\r\n$ ./gpu ./test.pb \r\n2018-07-12 15:40:27.892564: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2018-07-12 15:40:28.025248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 10.91GiB freeMemory: 9.21GiB\r\n2018-07-12 15:40:28.025276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-07-12 15:40:28.212742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-12 15:40:28.212771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \r\n2018-07-12 15:40:28.212778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \r\n2018-07-12 15:40:28.212941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8900 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\r\n2018-07-12 15:40:28.294047: I tensorflow/core/common_runtime/direct_session.cc:288] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1\r\n\r\noutput/op: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-12 15:40:28.294321: I tensorflow/core/common_runtime/placer.cc:886] output/op: (MatMul)/job:localhost/replica:0/task:0/device:CPU:0\r\ninput/ph0: (Placeholder): /job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-12 15:40:28.294335: I tensorflow/core/common_runtime/placer.cc:886] input/ph0: (Placeholder)/job:localhost/replica:0/task:0/device:CPU:0\r\ninput/ph1: (Placeholder): /job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-12 15:40:28.294341: I tensorflow/core/common_runtime/placer.cc:886] input/ph1: (Placeholder)/job:localhost/replica:0/task:0/device:CPU:0\r\nop: [[3 4 5] [6 8 10] [9 12 15]]\r\n```\r\n\r\nThis happens not all the time though, with this particular graph it is always on CPU, but `op = tf.reduce_sum(ph0 * ph1, axis=1, name='output/op')` runs on GPU. With my rather large graphs it is always on CPU.\r\n\r\nForcing graph to run on GPU with CPU-only kernels should not be a problem - it is not a real force, but only a hint, and it would be rather great if TF emits some kind of warning in this case. Yet it is MUCH better than running on CPU when all the kernels do have GPU implementations.\r\n", "Are there any question you might have concerning this issue? Does my code highlight the problem in your environment?", "@bioothod - sorry, I missed your last update. Will take a look at your example soon. ", "@asimshankar, still no progress on this?", "@bioothod - sorry for delay. I'm traveling right now, but will definitely respond by Tuesday. \r\n\r\nThat said, one quick observation in your example - it seems the graph is operating on `int32` tensors - for which there are few GPU kernels. Out of curiosity, does your application require `int32`, or was that just for this demo program? I suspect behavior will be quite different for other types like float or int64", "@bioothod : Took a look at your example and had some comments/observations.\r\n\r\nAs mentioned above, the story will be much different if you're using types other than `int32` (for example, with `float32`). Support for 32-bit integer operations on GPU is unfortunately a bit iffy in TensorFlow at the moment and the log message you're observing might be misleading as in both cases the operation is actually executing on CPU. The [`MatMul` kernel does not have a GPU implementation for `int32`](https://github.com/tensorflow/tensorflow/blob/a8e78e2e617b6ca10f4878fe99fdf43ddedfa7c6/tensorflow/core/kernels/matmul_op.cc#L622) and thus the device annotation is ignored (since you set `AllowSoftPlacement: true`). The `Sum` kernel (from `tf.reduce_sum`) requires the input and output for the `int32` type to be in host memory (not device memory) and [also just executes on CPU](https://github.com/tensorflow/tensorflow/blob/1c1dad105a57bb13711492a8ba5ab9d10c91b5df/tensorflow/core/kernels/reduction_ops_sum.cc#L58) (the log messaging is misleading there because of the hack mentioned in the comment of the kernel registration). \r\n\r\nLong story short, the `BindToDevice` call in your example has no real effect (the unfortunately misleading log message aside).\r\n\r\nFurthermore, going back to my original reservation to adding this mutation to the graph - we've consciously avoided C APIs to mutate existing nodes in the graph as it can be hard to determine whether or not the mutations apply correctly. For example, consider the following:\r\n\r\n```c\r\nTF_Graph* graph = MakeMyGraph();\r\nTF_BindToDevice(graph, \"/cpu:0\");\r\nTF_Session* session = TF_NewSession(graph, ...);\r\nTF_SessionRun(session, ...);\r\nTF_BindToDevice(graph, \"/gpu:0\");\r\nTF_SessionRun(session, ...);\r\n```\r\n\r\nIn this snippet, the second call to `TF_BindToDevice` has no effect, and all operations will execute on CPU. I'm hesitant to add a function which will have such an esoteric and hard to explain contract.\r\n\r\nAlternatives would be the following:\r\n\r\n1. Do this binding as an option to `TF_ImportGraphDef`, so it applies only to nodes being imported.\r\n2. Rewrite the `GraphDef` proto before importing (see [similar snippet for Java](https://stackoverflow.com/questions/47799972/tensorflow-java-multi-gpu-inference/47915987#47915987))\r\n3. As suggested in the StackOverflow question above, somehow add a notion of virtual devices so that each `Session` can map the device in the graph to a different device at runtime.\r\n\r\nSound reasonable?", "Thank you for response @asimshankar !\r\n\r\nI have to disagree - using `TF_BindToDevice` allows to bind whole graph to specified device and it will be executed there. I can not argue about int32 operations, but I do see that my large graphs are not executed on the GPU at all currently without this patch. So, basically, currently TF c/c++/go bindings are broken, all graphs are always executed on CPU.\r\n\r\nWith this patch nodes (all or only float32 for example, I can not say for sure) are executed on GPU. This is being confirmed not only by device placement log, which you say is misleading (and that's a bug too imho), but also HW monitoring tools (like `nvidia-smi`).\r\n\r\nIt could be interesting to check float32 operations (I'm currently away from the servers and can not run similar float32 ops test), but what's the point? We have a graph which is supposed to be executed on GPU by documentation, and it is not. Hence the patch.\r\n\r\nAs you said, second call for `TF_BindToDevice` will not replace graph if session has been created already, but it should not be a problem, `TF_BindToDevice` is not a strict order but a suggestion, a hint for execution, which can be clearly stated in the documentation with the example you have made.\r\n\r\nSession config already had this virtual-physical device mapping, and TF developers decided that it must not be used at all - at least go bindings crash if visible device list does not match per-process device list (CUDA_VISIBLE_DEVICES for instance) or is not empty. Logic that multiple virtual devices will point to the same physical remains, although I personally never saw such operations that require changing physical device properties.\r\n\r\n`TF_BindToDevice` is actually just a helper, if you are strongly against adding it (with appropriate documentation), I think updating `TF_GraphImportGraphDef` will work too, although a bit more clumsy. \r\n\r\nSo, let the solution be to extend `TF_GraphImportGraphDef`?", "@bioothod : You say that \"So, basically, currently TF c/c++/go bindings are broken, all graphs are always executed on CPU.\", but this is not true :). One can certainly execute graphs on GPUs from Go, as demonstrated in [the gist I previously linked to](https://gist.github.com/asimshankar/ef367e4897e248466c42c2dc629814e0), and when I change the placeholders to be `tf.float32` instead of `tf.int32` in your example (https://github.com/bioothod/golang_gpu_example).\r\n\r\nRegarding `TF_BindToDevice` - yes we _could_ just add it with documentation trying to explain that it may or may not work. But that seems like a bad API as there is no way for potential callers to know whether or not the call will have an effect. For example, if I write some utility library which invokes `TF_BindToDevice` - whether or not it has an effect depends on the environment in which the _caller_ of my library function invoked my library function. This will make it very hard to reason about. \r\n\r\nI'm not opposed to adding to `TF_GraphToImportGraphDef` yet (haven't thought it through). But, the fact that you aren't able to use the GPU without this explicit binding (as you said \"We have a graph which is supposed to be executed on GPU by documentation, and it is not.\") seems like it is some other issue which we should get to the bottom of.\r\n\r\nCan you provide an example of such a graph?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Sorry for long delays, I will send updated patch for review soon, some other things have distracted me from this task, but I haven't given up on it.", "I've updated patch to hide all binding implementation within C import function and do not allow to (re)bind graph in runtime (i.e. after it has been imported).\r\n\r\nTF 1.10 without this patch *does* allow execution of the graph on GPU by default (1.6-1.8 did not), but it is only the first device among multiple, so, if there are many devices one has no way to bind execution of the particular graph on particular device. It can be somehow mitigated by running multiple processes each one having access to only particular cuda device, but it is cumbersome and is not enough in some cases.\r\n\r\nMy patch allows to bind particular graph to particular GPU during its import, which allows to implement load balancing policies and generally load every GPUs without hacks.", "(FYI @skye )", "I've extended options and added bind device there, also propagated this into graph constructor options, where update nodes to be imported", "Hi @asimshankar @skye any chance you can look at the updated patch?", "Ping, still no luck?", "I've made all requested changes, please take a look", "@asimshankar sorry, what was the comment I haven't answered, things have intermixed so much...\r\n\r\nAs of unit test, those tests you've mentioned check that graphs loaded from protobuf string do contain variables described in protobuf, I would rather call it protobuf unit tests.\r\nBut my patch does not really change on-disk protobuf layout, it is runtime feature that I allow to set.\r\n\r\nIs there a way to kind of *run* simple graph and check that it has been executed on the requested device?", "Hi @bioothod, for your unit test, you can just check the device of the imported nodes, without actually running them. To access the device of a node, use [Node.requested_device](https://github.com/tensorflow/tensorflow/blob/903a6399aab19b549fefd0ead836af644f3d00f8/tensorflow/core/graph/graph.h#L102). To access the imported Nodes, use [return_nodes](https://github.com/tensorflow/tensorflow/blob/903a6399aab19b549fefd0ead836af644f3d00f8/tensorflow/core/graph/graph_constructor.h#L129).\r\n\r\nI believe Asim is referring to his comment from Aug 16.", "Yup, the one from August 16 (sorry seems I messed up [the link](https://github.com/tensorflow/tensorflow/pull/20412#issuecomment-413449551) last time).\r\n\r\n@bioothod : It's not strictly a runtime change, if you import a graph with these options set and then export it back out to a `GraphDef`, the exported `GraphDef` will have the device annotations, right?", "@asimshankar @skye graphs I use did not run on GPU between tf 1.6 and 1.8, but they did on 1.4 and they do now on 1.10-1.11. So, previously it was not only matter of load balancing, but also kind of a workaround for some bug. Currently it is only a way to implement load balancing.\r\n\r\nI can provide serialized protobuf graph if you are interested, but this is a quite large convolutional network (frozen protobuf is about 100MB in size) and includes quite a bunch of different operations built on a similar to resnet arch. I think I have a smaller network which I can try to run on older TF now, if you are interested, and show you the graph, but right now current TF does run on GPU randomly selecting one from the set for each session run.", "I've added unit test which checks whether execution device is indeed propagated into node via node_def", "I've updated test, it now includes both assigned and graph_def-stored devices test in a single unit test, please review and, if everything is okay, merge", "Updated patch, but it looks like this reset the merge :)\r\n\r\nSorry. And thank you!", "I intend to temporarily disable this to resolve #23257 \r\nAnd then re-enable after a release that includes this PR in the C library has been made.\r\n\r\n(That way, \"go get\" works by default and some code tweaks are needed to pin devices from Go, instead of the other way around where \"go get\" fails by default and \"git checkout\" it needed to make it work)", "Yeah, I read that issue, does not really know how to solve this kind of a problem, maybe only by manually splitting C and other parts and only merging golang/python/whatever after C part has been released", "Once the dust settles on `go dep` / `go mod`, perhaps we can utilize that for versioning. \r\n\r\nThis hasn't happened often enough to be troublesome, so I'm okay with some manual work for now. ", "@asimshankar Default execution device is still disabled in golang API, should it be uncommented now as 1.13.0-rc0 is out?", "@bioothod : May be best to wait till 1.13.0 final is out instead of relying on the RC?", "Fair enough, let's wait for 1.13 release to roll out", "@asimshankar hi, is it time to merge go changes which were commented in https://github.com/tensorflow/tensorflow/commit/6f09a093d1e5e6947204db1b5fc1d85032e6a78d\r\n\r\nalthough documentation for C library build still references 1.12", "Yeah, I think so. @jhseu @alextp would be happy to help that through if you could send a PR (I'm no longer actively working on TensorFlow). Thanks!", "Yes, happy to review and help merge a PR", "This is it: https://github.com/tensorflow/tensorflow/pull/27891\r\nI've made it against r.13, but it should be applicable to master too."]}, {"number": 20411, "title": "Boosted trees classifier fails with segmentation fault", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCustom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\npip install\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\ncuda 9.0, cudnn 7\r\n- **GPU model and memory**:\r\nNVidia V100, aws p3 instance\r\n- **Exact command to reproduce**:\r\npython tf_trees.txt\r\n[tf_trees.txt](https://github.com/tensorflow/tensorflow/files/2149208/tf_trees.txt)\r\n\r\n\r\n### Describe the problem\r\nI try to train a boosted trees model and it seems to work. When I try to predict on new data, it crashes.\r\n\r\nLogs\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-06-29 11:27:47.891292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-29 11:27:47.891344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-29 11:27:47.891359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0\r\n2018-06-29 11:27:47.891373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N\r\n2018-06-29 11:27:47.891523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\r\nINFO:tensorflow:Restoring parameters from /data/model.ckpt-110\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nSegmentation fault (core dumped)\r\n\r\n", "comments": ["Hi, any chance to have some information on it?", "Nagging Assignee @zheng-xq: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@OlegKremnyov if you use your boosted trees model to make predictions on the existing data, does it also crash? ", "Closing this for now, feel free to open a new issue if encountering any problems."]}, {"number": 20410, "title": "fix a minor comment issue in commit 034236aa27ae5e40a6fe619d43c94fb01\u2026", "body": "\u2026f529601", "comments": ["guys, anybody any idea? ", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied."]}, {"number": 20409, "title": "fix todo in session_test with list device", "body": "", "comments": ["Nagging Assignee @yifeif: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like https://github.com/tensorflow/tensorflow/commit/109ae67a7e99e3dcb4d93cc22df5b3912f4558c9 already did a similar fix. I will close this PR. Thanks for the change @u2takey! "]}, {"number": 20408, "title": "LoadLibrary crashed at LoadLibraryExW when load custom ops dll on Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n\r\n- **Python version**: \r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nVS 2017\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\n- **GPU model and memory**:\r\n\r\n- **Exact command to reproduce**:\r\n\r\nwrite op \r\n```c++\r\n#if defined(_MSC_VER) && !defined(COMPILER_MSVC)\r\n#define COMPILER_MSVC // Set MSVC visibility of exported symbols in the shared library.\r\n#ifndef NOMINMAX\r\n# define NOMINMAX\r\n#endif\r\n#endif\r\n\r\n#include \"tensorflow/core/framework/types.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n.Input(\"to_zero: int32\")\r\n.Output(\"zeroed: int32\")\r\n.SetShapeFn([](shape_inference::InferenceContext* c) {\r\n    c->set_output(0, c->input(0));\r\n    return Status::OK();\r\n});\r\n\r\nclass ZeroOutOp : public OpKernel {\r\npublic:\r\n    explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n    void Compute(OpKernelContext* context) override {\r\n        // Grab the input tensor\r\n        const Tensor& input_tensor = context->input(0);\r\n        auto input = input_tensor.flat<int32>();\r\n\r\n        // Create an output tensor\r\n        Tensor* output_tensor = NULL;\r\n        OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n            &output_tensor));\r\n        auto output_flat = output_tensor->flat<int32>();\r\n\r\n        // Set all but the first element of the output tensor to 0.\r\n        const int N = input.size();\r\n        for (int i = 1; i < N; i++) {\r\n            output_flat(i) = 0;\r\n        }\r\n\r\n        // Preserve the first input value if possible.\r\n        if (N > 0) output_flat(0) = input(0);\r\n    }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\nload ops in C++\r\n```\r\nEnv* env = Env::Default();\r\nenv->LoadLibrary(library_filename, &library.handle);\r\n```\r\nLoadLibraryExW crashed when load the op.dll\r\n\r\n### Describe the problem\r\nLoadLibraryExW crashed when load the op.dll\r\nThe above code works fine under Linux, but crashed under windows 7 compiled by VS 2017\r\n\r\n### Source code / logs\r\n", "comments": ["Now I add the custom op code into my C++ code and find that crashed.\r\n![image](https://user-images.githubusercontent.com/619331/42131213-940c28d2-7d2f-11e8-83e6-53047a626f0b.png)\r\nSeems `~OpDef()` have problem", "https://github.com/tensorflow/tensorflow/issues/10685", "I have solved the problem by changing the link order of libraries.\r\nLink tf_protos_cc.lib after tensorflow.lib with /FORCE:MULTIPLE solved this crash."]}, {"number": 20406, "title": "bugfix(issue 20043): Parameter save_relative_paths of tf.train.Saver do not work non-local filesystem.", "body": "Fix the bug on issue 20043:  parameter save_relative_paths of tf.train.Saver do not work non-local filesystem.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "i am sure that i have signed a CLA with my gihub account name  \"jony0917\" as \"GitHub Account Name\" in the CLA form,  and i am sure that the email address  from \"git config --global user.email\" has been added to my github account, so do i miss something?", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Reviewer @rohan100jain: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 164 days with no activity and the `awaiting review` label has been applied.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 20405, "title": " only supports 'NHWC' format under the GPU mode ", "body": "I use tensorflow 1.8 version.\r\nwe konw the gpu should support both 'NHWC' and 'NCHW' but i get bellow errors under GPU mode:\r\n\r\nConv2DCustomBackpropInputOp only supports NHWC.\r\n\t [[Node: model_1/inference/conv2d_transpose/conv2d_transpose = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 16], **use_cudnn_on_gpu=true**, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model_1/inference/conv2d_transpose/stack, model/inference/conv2d_transpose/kernel/read, model_1/inference/ExpandDims_1)]]", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "CC @zheng-xq for comment, but I don't believe anyone is actively working on this at the moment, so marking as \"Contributions Welcome\".", "No this probably is a user error, because the op was placed on CPU:  `_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"`.\r\nIf the GPU is configured properly and the op is placed correctly on GPUs by users, the op `Conv2DCustomBackpropInputOp` will not appear in the graph at all.", "@ppwwyyxx I think i have a right config about gpu:\r\n        num_gpus = 1, #Determines the number of gpus in use\r\n\t#Memory allocation on the memory\r\n\tconfig = tf.ConfigProto()\r\n\tconfig.gpu_options.allow_growth = True\r\n\r\n\t#Train\r\n\twith tf.Session(config=config) as sess:\r\n\t\ttry:\r\n\t\t\tsummary_writer = tf.summary.FileWriter(log_dir, sess.graph)\r\n\t\t\tsess.run(tf.global_variables_initializer())", "This is a stale issue. Looks like in the recent TF versions, other data formats also supported. For example, check [this page](https://www.tensorflow.org/api_docs/python/tf/nn/depth_to_space).\r\n\r\n> # tf.nn.depth_to_space\r\n> The data_format attr specifies the layout of the input and output tensors with the following options: \"NHWC\": [ batch, height, width, channels ] \"NCHW\": [ batch, channels, height, width ] \"NCHW_VECT_C\": qint8 [ batch, channels / 4, height, width, 4 ]\r\n> \r\n\r\nClosing this issue as this was resolved in recent TF versions. Feel free to reopen if I am mistaken. Thanks!"]}, {"number": 20404, "title": "Where the data is converted into tensor", "body": "Like this:\r\n\r\n```\r\nta = tf.convert_to_tensor(np.array([1,2,3]))\r\nwith tf.Session() as sess:\r\n         result= sess.run(ta)\r\n```\r\n\r\nI want to know where the list is converted to tensor. \r\nIs in GPU, or in CPU and then being feeded to GPU ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20403, "title": "[tflite][operator: cast] How to quantize MobileNetV2 for deeplabV3+?", "body": "**System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**:1.9.0\r\n- **Python version**:2.7.12\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:cuda-9.0/7.0\r\n- **GPU model and memory**:GeForce GTX 1080/8105MiB\r\n- **Phone**:xiaomi5 (Snapdragon 820)\r\n- **Exact command to reproduce**:\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\r\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\r\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\r\n--inference_type=QUANTIZED_UINT8\r\n--input_shape=1,513,513,3\r\n--input_array=sub_7\r\n--output_array=logits/semantic/BiasAdd\r\n\r\n**Describe the problem**\r\nI have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.\r\nFrom the following issue, I saw that the operations were not supported for the option of quantization.\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md\r\nCheckpoint name: mobilenetv2_coco_voc_trainaug\r\n\r\nWho can explain and support to resolve the issue?\r\n\r\n**Source code / logs**\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\r\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\r\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\r\n--inference_type=QUANTIZED_UINT8\r\n--input_shape=1,513,513,3\r\n--input_array=sub_7\r\n--output_array=logits/semantic/BiasAdd\r\n--default_ranges_min=0  \r\n--default_ranges_max=6\r\n\r\nUnimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n\r\n\r\n**Source code / logs**\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\r\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\r\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\r\n--inference_type=QUANTIZED_UINT8\r\n--input_shape=1,513,513,3\r\n--input_array=sub_7\r\n--output_array=logits/semantic/BiasAdd\r\n\r\ntensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.\r\n\r\ntensorflow/contrib/lite/toco/tflite/export.cc:367] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Stack.", "comments": ["@suharshs - is there a unified listing somewhere of the quantized mobilenet models that are available with TFLite?", "@karmel is related to https://github.com/tensorflow/tensorflow/issues/17468?", "I suspect similar to #20272 but with mobilenet v2 instead of v1", "@suharshs\r\n\r\nWhere can I find whether the operation of cast is implemented?\r\n\r\nAnd do you have a plan to support the operation of Stack with fake quantization?", "I am also wondering about the timeline for Stack.", "Nagging Assignees @suharshs, @liyunlu0618: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi all, apologies for the delay,\r\n\r\nStack has been implemented in this change: https://github.com/tensorflow/tensorflow/commit/df7344f1933d932f03f472402068ff1883f0c011#diff-0a7763f2b5c90990e015f610fa5f49df\r\n\r\n", "@suharshs How about cast? ", "Hi\uff0c @suharshs. Thanks for the implement of stack. But I have encounter a problem when use stack [here](https://github.com/tensorflow/tensorflow/issues/23855). Do you have any idea to solve it?", "i join @myth01 's request for the support on cast...\r\ncannot convert a deeplabv3+ model without it..", "HI all,\r\n\r\ntry to add` --change_concat_input_ranges=false` when using toco to quantize your model.\r\n"]}, {"number": 20402, "title": "Feature request: non-stateful scatter_nd_min and scatter_nd_max", "body": "Currently tf.scatter_min and tf.scatter_max, unlike tf.scatter_nd, accept a variable ref input and are stateful op. It would be great if there are a non-stateful version of tf.scatter_min and tf.scatter_max that can back-propagate gradients to their inputs.\r\n\r\n@facaiy @drpngx", "comments": ["Are you sure it's stateful?\r\n\r\nThe [op registration](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/state_ops.cc#L186) doesn't look like it.", "@drpngx \r\nIt's input is ref: Ref(T), which must be a reference to a variable node. So gradients are not back propagate to its input.", "I see. That sounds like a useful addition then.", "Should be a small extension to the code in tf.tensor_scatter_add", "I would like to take this issue. I couldn't find source code for tf.tensor_scatter_add. I read the source code for scatter_add in tensorflow/python/ops/state_ops.py, but couldn't figure out which changes to make. Can you suggest me a starting point.", "@pratjosh9 The TensorScatter kernel is implemented in https://github.com/tensorflow/tensorflow/blob/ac3faa26869258b6149f154618feb08dccd810ff/tensorflow/core/kernels/scatter_nd_op.cc#L140 . In https://github.com/tensorflow/tensorflow/blob/ac3faa26869258b6149f154618feb08dccd810ff/tensorflow/core/kernels/scatter_nd_op.cc#L387 you see that we only register assign, add, and sub, so that's the place to add the registration for max / min.", " @alextp Thanks for the quick response. I've implemented the above changes for CPU as follow. Is this the correct way of implementing the registration for max and min? I will then start extending these changes to the GPU section defined in the aforementioned file.\r\n\r\n```C\r\n#define REGISTER_SCATTER_ND_MIN_MAX(type, dev)                            \\\r\n  REGISTER_SCATTER_ND_UPDATE_KERNEL(type, dev, \"ScatterNdMin\",            \\\r\n                                    scatter_nd_op::UpdateOp::MIN);        \\\r\n  REGISTER_SCATTER_ND_UPDATE_KERNEL(type, dev, \"ScatterNdNonAliasingMin\", \\\r\n                                    scatter_nd_op::UpdateOp::MIN);        \\\r\n  REGISTER_SCATTER_ND_UPDATE_KERNEL(type, dev, \"ScatterNdMax\",            \\\r\n                                    scatter_nd_op::UpdateOp::MAX);        \\\r\n  REGISTER_SCATTER_ND_UPDATE_KERNEL(type, dev, \"ScatterNdNonAliasingMax\", \\\r\n                                    scatter_nd_op::UpdateOp::MAX);        \\\r\n  \r\n  REGISTER_RESOURCE_SCATTER_ND_UPDATE_KERNEL(                             \\\r\n      type, dev, \"ResourceScatterNdMin\", scatter_nd_op::UpdateOp::MIN);   \\\r\n  REGISTER_RESOURCE_SCATTER_ND_UPDATE_KERNEL(                             \\\r\n      type, dev, \"ResourceScatterNdMax\", scatter_nd_op::UpdateOp::MAX);\r\n\r\n// Registers CPU kernels.\r\n#define REGISTER_SCATTER_ND_MIN_MAX_CPU(type) \\\r\n  REGISTER_SCATTER_ND_MIN_MAX(type, CPU);\r\n\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_MIN_MAX_CPU);\r\n\r\n#define REGISTER_SCATTER_ND_TENSOR_MIN_TYPE_INDEX_TYPE(type, index_type,    \\\r\n                                                          dev)              \\\r\n  REGISTER_KERNEL_BUILDER(Name(\"TensorScatterMin\")                          \\\r\n                              .Device(DEVICE_##dev)                         \\\r\n                              .TypeConstraint<type>(\"T\")                    \\\r\n                              .TypeConstraint<index_type>(\"Tindices\"),      \\\r\n                          TensorScatterOp<dev##Device, type, index_type,    \\\r\n                                          scatter_nd_op::UpdateOp::MIN>)\r\n\r\n#define REGISTER_SCATTER_ND_TENSOR_MAX_TYPE_INDEX_TYPE(type, index_type,    \\\r\n                                                          dev)              \\\r\n  REGISTER_KERNEL_BUILDER(Name(\"TensorScatterMax\")                          \\\r\n                              .Device(DEVICE_##dev)                         \\\r\n                              .TypeConstraint<type>(\"T\")                    \\\r\n                              .TypeConstraint<index_type>(\"Tindices\"),      \\\r\n                          TensorScatterOp<dev##Device, type, index_type,    \\\r\n                                          scatter_nd_op::UpdateOp::MAX>)\r\n\r\n#define REGISTER_SCATTER_ND_TENSOR_MIN_CPU(type)                    \\\r\n  REGISTER_SCATTER_ND_TENSOR_MIN_TYPE_INDEX_TYPE(type, int32, CPU); \\\r\n  REGISTER_SCATTER_ND_TENSOR_MIN_TYPE_INDEX_TYPE(type, int64, CPU);\r\n\r\n#define REGISTER_SCATTER_ND_TENSOR_MAX_CPU(type)                    \\\r\n  REGISTER_SCATTER_ND_TENSOR_MAX_TYPE_INDEX_TYPE(type, int32, CPU); \\\r\n  REGISTER_SCATTER_ND_TENSOR_MAX_TYPE_INDEX_TYPE(type, int64, CPU);\r\n\r\n\r\n#define REGISTER_SCATTER_ND_TENSOR_CPU(type)   \\\r\n  REGISTER_SCATTER_ND_TENSOR_UPDATE_CPU(type); \\\r\n  REGISTER_SCATTER_ND_TENSOR_ADD_CPU(type);    \\\r\n  REGISTER_SCATTER_ND_TENSOR_SUB_CPU(type);    \\\r\n  REGISTER_SCATTER_ND_TENSOR_MIN_CPU(type);    \\\r\n  REGISTER_SCATTER_ND_TENSOR_MAX_CPU(type);    \r\n\r\n// Register TensorScatterUpdate/Add/Sub/Min/Max for all number types.\r\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ND_TENSOR_CPU);\r\n```", "Yes, I think so.\r\n\r\nThanks!", "I've implemented the corresponding changes in the required file and created a PR  #26923. Kindly suggest if any further changes are required."]}, {"number": 20401, "title": "Combine two models in tensorflow", "body": "I have checkpoints of two different models in tensorflow.\r\n\r\nHow to combine them both and save them as one model in one checkpoint file and the resulting computation graph looks like the earlier ones were merged?\r\n\r\nWhat I intend to do is -\r\n\r\n    Load model 1 weights and configuration\r\n    Load model 2 weights and configuration\r\n\r\n    combine model1 and model2 to get model3\r\n\r\n    save model3\r\n\r\nAlso the models 1 and 2 have identical graphs (they are CNNs and have same variable names for tensors)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20400, "title": "Update nasm from 2.12.02 to 2.13.03", "body": "\r\nThis fix updates nasm from 2.12.02 (2016-07-06)\r\nto the latest version of 2.13.03 (2018-02-07)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["/cc @gunan @yifeif to take a look", "Thanks @yifeif. The PR has been updated with included files list ordered. The `XLA` test is failing but that might be unrelated I think? "]}, {"number": 20399, "title": "Feature request: A Layer object wrapping multiple Layer objects", "body": "`tf.contrib.seq2seq.BasicDecoder` module allows provision for a single output layer, to modify the output of the RNN before proceeding to further sample.\r\n\r\nIn my particular application, I require application of multiple layers.\r\nThis is not possible as far as I have checked.\r\n\r\nA `MultiLayer` class could be implemented, similar to the `tf.contrib.rnn.MultiRNNCell`.\r\nThis class could wrap multiple `Layer` objects and apply them on `call`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code - N/A\r\nOS Platform and Distribution - N/A\r\nTensorFlow installed from - N/A\r\nTensorFlow version - 1.8\r\nBazel version - N/A\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A", "We will consider it, thanks.\r\n", "@varshiths,\r\nSorry for the delayed response. Can you please let us know if adding [StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells) as an [Argument, Output_Layer](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BasicDecoder#args) in the [BasicDecoder](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BasicDecoder) fulfills your request? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20398, "title": "Why dense layer cannot be speed up in tf.contrib.trt ?", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nWhen using tf.contrib.trt.create_inference_graph, I meet the following error, it seems that dense layer is not support because the input tensor is not rank 4 ? Why has this request on dense layer ?\r\n\r\n> subgraph conversion error for subgraph_index:9 due to: \"Unimplemented: Require 4 dimensional input. Got 2 dense/MatMul\" SKIPPING\r\n\r\n### Source code / logs\r\n\r\nNeed not to code, clear above ...\r\n\r\n", "comments": ["@samikama would you please take a look?", "TRT 3 doesn't support Matmul.", "Nagging Assignee @samikama: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20397, "title": "tf.image.decode_png bug", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- official docker image , Linux Ubuntu 16.04\r\n- tf1.8.0  v1.8.0-0-g93bc2e2072 1.8.0\r\n- python3.5.2\r\n- Cuda compilation tools, release 9.0, V9.0.176\r\n- GPU:1080Ti \r\n\r\n### Describe the problem\r\ntf.image.decode_png can not decode a gray png which has size 60064\r\n\r\n### Source code / logs\r\npp = 'SRAD2018_TRAIN_004/RAD_296582494212544/RAD_296582494212544_004.png'\r\nim = io.imread(pp)\r\nprint('haha:  ',  im.shape)\r\ncontent = tf.read_file(pp)\r\nimg = tf.image.decode_png(content, channels=1)\r\nwith tf.Session() as sess:\r\n     ig = sess.run(img)\r\n     print(ig.shape)\r\npicture can be downloaded here https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.451d5147iiIyoD&raceId=231662\r\nit is a competition hosted by aliyun,  the competition name is \"IEEE ICDM 2018 \u5168\u7403\u6c14\u8c61AI\u6311\u6218\u8d5b\"\r\n\r\npicture name is 'SRAD2018_TRAIN_004/RAD_296582494212544/RAD_296582494212544_004.png'\r\n\r\nI met this problem several times. maybe there are several pictures can repeat this bug. i just give an example above. at everytime the bug occurs with image having size 65564.\r\n\r\nonly images having size 65504 can not be decoded, and skimage.io can read the picture correctly.\r\n### Logs\r\nhaha:  (501, 501)\r\n2018-06-29 02:57:24.012213: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-06-29 02:57:24.128019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-29 02:57:24.128635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 222.81MiB\r\n2018-06-29 02:57:24.328695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-29 02:57:24.329699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 10.91GiB freeMemory: 9.93GiB\r\n2018-06-29 02:57:24.527217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-29 02:57:24.528368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2018-06-29 02:57:24.539737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2\r\n2018-06-29 02:57:25.708418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-29 02:57:25.708454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 \r\n2018-06-29 02:57:25.708465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y \r\n2018-06-29 02:57:25.708475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y \r\n2018-06-29 02:57:25.708486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N \r\n2018-06-29 02:57:25.709161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 166 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-06-29 02:57:25.710945: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 166.81M (174915584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2018-06-29 02:57:25.715583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9604 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2018-06-29 02:57:25.827449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10397 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid PNG data, size 65564\r\n\t [[Node: DecodePng = DecodePng[channels=1, dtype=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"model.py\", line 452, in <module>\r\n    ig = sess.run(img)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid PNG data, size 65564\r\n\t [[Node: DecodePng = DecodePng[channels=1, dtype=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n\r\nCaused by op 'DecodePng', defined at:\r\n  File \"model.py\", line 450, in <module>\r\n    img = tf.image.decode_png(content, channels=1)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_image_ops.py\", line 1062, in decode_png\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Invalid PNG data, size 65564\r\n\t [[Node: DecodePng = DecodePng[channels=1, dtype=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n\r\n", "comments": ["@whyguu The download is pretty large and requires signup before download. Can you post the sample png file to GitHub directly if possible?", "https://github.com/whyguu/HelloWorld/blob/master/t4_RAD_296582494212544_004.png\r\n\r\nthe above  is an example @yongtang ", "I am wondering if it is because the png you provided does not contain the `IEND` section?\r\n```\r\nubuntu@ubuntu:~/workspace$ pngcheck -vt t4_RAD_296582494212544_004.png\r\nFile: t4_RAD_296582494212544_004.png (65564 bytes)\r\n  chunk IHDR at offset 0x0000c, length 13\r\n    501 x 501 image, 8-bit palette, non-interlaced\r\n  chunk sRGB at offset 0x00025, length 1\r\n    rendering intent = perceptual\r\n  chunk gAMA at offset 0x00032, length 4: 0.45455\r\n  chunk PLTE at offset 0x00042, length 768: 256 palette entries\r\n  chunk pHYs at offset 0x0034e, length 9: 3779x3779 pixels/meter (96 dpi)\r\n  chunk IDAT at offset 0x00363, length 64665\r\n    zlib: deflated, 32K window, fast compression\r\n  chunk IDAT at offset 0x10008, length 65524:  EOF while reading data\r\nERRORS DETECTED in t4_RAD_296582494212544_004.png\r\nubuntu@ubuntu:~/workspace$ \r\n```", "pngcheck -vt RAD_206482464212530_030.png \r\nFile: RAD_206482464212530_030.png (41751 bytes)\r\n  chunk IHDR at offset 0x0000c, length 13\r\n    501 x 501 image, 8-bit palette, non-interlaced\r\n  chunk sRGB at offset 0x00025, length 1\r\n    rendering intent = perceptual\r\n  chunk gAMA at offset 0x00032, length 4: 0.45455\r\n  chunk PLTE at offset 0x00042, length 768: 256 palette entries\r\n  chunk pHYs at offset 0x0034e, length 9: 3779x3779 pixels/meter (96 dpi)\r\n  chunk IDAT at offset 0x00363, length 40864\r\n    zlib: deflated, 32K window, fast compression\r\n  chunk IEND at offset 0x0a30f, length 0\r\nNo errors detected in RAD_206482464212530_030.png (7 chunks, 83.4% compression).\r\n\r\nmaybe that is the problem. I try \"pngcheck\" on another good image file . Above is the output, and IEND exists. But I should say that skimage.io can read the bad one correctly. so maybe there are some modifications can be done in tensorflow\u3002Anyway I delete these bad images making the program run. \r\nThank you for your answers under this topic.", "Thanks I will close this for now."]}, {"number": 20396, "title": "Feature Request: .string_handle() support for iterator based on Dataset to which .prefetch_to_device() has been applied", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: AWS p3.2xlarge instance\r\n- **Exact command to reproduce**:\r\n\r\nRequest:\r\n\r\nAs stated in the title, it seems that a Dataset object upon which .prefect_to_device has been applied and an iterator defined from results in an iterator for which .string_handle() gives an error. In my case, my dataset and iterator are defined as\r\n```\r\ntraining_dataset_shuffle_batch = tf.data.Dataset.from_tensor_slices(training_data)\r\ntraining_dataset_shuffle_batch = training_dataset_shuffle_batch.apply(tf.contrib.data.shuffle_and_repeat(buffer_size = dataset_size))\r\ntraining_dataset_shuffle_batch = training_dataset_shuffle_batch.batch(minibatch_size)\r\ntraining_dataset_shuffle_batch = training_dataset_shuffle_batch.apply(tf.contrib.data.prefetch_to_device(gpu_names[0]))\r\ntraining_shuffle_batch_iterator = training_dataset_shuffle_batch.make_initializable_iterator()\r\n```\r\nand I'd like to use this with a feedable iterator defined as\r\n```\r\nhandle = tf.placeholder(tf.string, shape=[])\r\niterator = tf.data.Iterator.from_string_handle(handle, training_dataset_shuffle_batch.output_types, training_dataset_shuffle_batch.output_shapes) \r\nnext_input_data_element = iterator.get_next()\r\n```\r\nHowever, \r\n`training_shuffle_batch_handle = sess.run(training_shuffle_batch_iterator.string_handle())`\r\nresults in an \r\n`AttributeError: '_PrefetchToDeviceIterator' object has no attribute 'string_handle'`\r\n\r\nThis unfortunately prevents us from using feedable iterators and simultaneous benefit from prefetching data to the GPU.\r\n\r\nThank you.", "comments": ["Rohan is already working on this, so assigning it to him :).", "Awesome, thank you guys!", "The preferred new way of doing prefetching is \r\n\r\nds.apply(prefetching_ops.copy_to_device(\"/gpu:0\")).prefetch(1) [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/prefetching_ops.py#L355]\r\n\r\nThis should give you a regular PrefetchDataset object that you can create a regular iterator out of. \r\n\r\nLet me know if that helps. ", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20395, "title": "Update eager notebooks with buttons, licenses, and change filenames", "body": "", "comments": ["Thanks. Updated, PTAL", "This was my attempt to remove that file from this PR.\r\nI'm updating that file in this branch: https://github.com/lamberta/tensorflow/blob/r1.10-tutorials/tensorflow/docs_src/tutorials/_index.yaml\r\nWhich depends on this PR.", "Reverted my attempt to make it easier that did not make it easier. File names should be in sync now."]}, {"number": 20394, "title": "Update nasm from 2.12.02 to 2.13.03", "body": "\r\nThis fix updates nasm from 2.12.02 (2016-07-06)\r\nto the latest version of 2.13.03 (2018-02-07)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Close for now as there are some build issues."]}, {"number": 20393, "title": "[Intel MKL] Adding support to checkout both branches and tags", "body": "Hi @gunan. This PR will allow us to generate and publish MKL whls and containers from both branches and tags. Thanks for your help! ", "comments": []}, {"number": 20392, "title": "Feature Request: n-dimensinal extract_glimpse ", "body": "Currently there exists a method [`tf.image.extract_glimpse`](https://www.tensorflow.org/versions/master/api_docs/python/tf/image/extract_glimpse), but this is specific for 2D images (and `tf.extract_image_patches` as well).\r\n\r\nSince I work mostly with 3D data it would be nice to have a similar function which produces *glimpses* for 3D data. One could generalise this even more by computing glimpses for n-dimensinal input.\r\n\r\nI am happy to look into this, although I am not quite sure where the actual source code is located for the `tf.image.extract_glimpse` method.\r\nThere is a class `ExtractGlimpseOp ` in [tensorflow/tensorflow/core/kernels/attention_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/attention_ops.cc), but the name `attention_ops` is a bit confusing.\r\nAnd of course the op registration in [tensorflow/tensorflow/core/ops/image_ops.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L539).\r\n\r\nWhat is a good way to start?\r\n\r\n\r\n#### System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): N/A\r\n- Python version: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Exact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Updated the system information, but these are non-relevant.", "The [Adding a New Op Guide](https://www.tensorflow.org/extend/adding_an_op) seems to cover everything needed to extend tf, I will look into this further.", "Nagging Assignee @jart: It has been 140 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20391, "title": "Feature Request: tf.data.Dataset.truncated_batch()", "body": "The `Dataset` api currently provides a `padded_batch` transformation, but for dense sequential data (audio, video) it might be better to drop a few frames to make the batched examples even. Can we have a new function to `Dataset` like this?\r\n```python\r\ntruncated_batch(\r\n    batch_size,\r\n    truncating_shapes\r\n)\r\n```\r\n\r\nThe semantic would be very similar to `padded_batch` but in reverse. The `truncating_shapes` argument determines the resulting shape for each dimension of each component in an output element:\r\n* If the dimension is a constant (e.g. `tf.Dimension(37)`), the component will be truncated to that length in that dimension.\r\n* If the dimension is unknown (e.g. `tf.Dimension(None)`), the component will be truncated to the minimum length of all elements in that dimension.\r\n\r\nThoughts? @mrry ", "comments": ["Thoughts: We already have too many different methods for batching that don't play together super-well.  @jsimsa is working on a new transformation that will subsume `Dataset.batch()` and `Dataset.padded_batch()` by allowing the caller to specify how the batch elements are reduced into one or more tensors. It should be possible to implement this proposed transformation in Jiri's framework, and we'll keep it in mind when providing synatactic sugar for common reducers.", "Cool, that'll be great. Is there a tracking issue for that work?", "Hello @rongou, the initial implementation of the API alluded to by @mrry has been merged to master (https://github.com/tensorflow/tensorflow/commit/a6471888cc9dfe9c18d121149bc0516a3f423fbb). It contains implementations for batching and padded batching of both dense and sparse tensors which can serve as an example of how to write custom batching transformation (such as your \"truncated\" batch)."]}, {"number": 20390, "title": "Fix install location for Eigen headers in pip build.", "body": "", "comments": ["Cherrypicking recent pip fix from master branch to 1.9. Should fix some custom ops build issues."]}, {"number": 20389, "title": "Branch 202534388", "body": "", "comments": ["Seeing this error in the MacOS build...\r\n\r\nLooks like you've seen this before Skye? Do you know how to fix it?\r\n\r\n```\r\nIn file included from tensorflow/core/graph/tensor_id.cc:16:                         \r\nIn file included from ./tensorflow/core/graph/tensor_id.h:19:                        \r\nIn file included from /Applications/Xcode_8.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/string:439:\r\nIn file included from /Applications/Xcode_8.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/algorithm:627:\r\n/Applications/Xcode_8.1.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/utility:280:38: error: no type named 'type' in 'std::__1::enable_if<false, void>'; 'enable_if' cannot be used to disable this declaration \r\n                 ,typename enable_if<is_convertible<const _U1&, _T1>::value &&  \r\n                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \r\n./tensorflow/core/graph/tensor_id.h:66:15: note: in instantiation of member function 'std::__1::pair<std::__1::basic_string<char>, int>::pair' requested here\r\n  using Base::pair;                                                                  \r\n              ^                                                                      \r\ntensorflow/core/graph/tensor_id.cc:34:10: note: while substituting deduced template arguments into function template 'SafeTensorId' [with _U1 = tensorflow::StringPiece, _U2 = int]\r\n  return ParseTensorName(StringPiece(name.data(), name.size()));                     \r\n         ^          \r\n```\r\n", "Not yet, there's something wrong with the MacOS toolchain I think... it seems to be flaky, so you can try rerunning."]}, {"number": 20388, "title": "Alternative fix for tf.contrib.ffmpeg.decode_video error", "body": "An alternative fix to #20366 that might be slightly simpler. Basically only parse the `width x height` information from the `ffmpeg` output, and rely on the `rawvideo` size to calculate the number of frames. This might be a little more robust to `ffmpeg` output changes.\r\n\r\nAlso added the `-vsync 0` flag so that `ffmpeg` doesn't duplicate or drop frames.\r\n\r\nFixes #20348.\r\n\r\n@yongtang ", "comments": ["ping @yongtang @yifeif ", "@fredbertsch let me know if you can take a look at this PR. Thanks!", "Is it going to be merged soon or superceded by tensorflow-io?", "@wookayin The `tf.contrib.ffmpeg` directory will be removed in the future tensorflow 2.0\r\n\r\nIt is preferable to use tensorflow-io for video input now forward looking. Also, tensorflow-io does not use command line to spawn a process. It calls ffmpeg api directly so it should be much more reliable and performs better: https://github.com/tensorflow/io\r\n", "@yongtang Thanks for your comment. I see, and I like the direction of migrating video ops to `tensorflow-io`, but it is pretty hard to make it work with TF 1.13.0 because of many API changes. So I was wondering if it is still worthwhile to include this changes in TF 1.x series until `tensorflow_io.video` gets easly usable.\r\n\r\nUPD: By building bleeding-edge `tensorflow-io` from source, and managed to make it work. So we can maybe wait for tensorflow-io things get more stablized. Eventually including this changes can be incorporated to the latest TF releases (then we can simply use tensorflow-io), maybe we don't need it anymore...?", "Thanks @wookayin for trying out `tensorflow-io`.\r\n\r\n\r\nThe tensorflow-io is actually compatible with tf 1.13 and our nightly build: \r\n```\r\npip install tensorflow-io-nightly\r\n```\r\nrequires `tensorflow==1.13.0rc2`.\r\n\r\nThe reason the released version of tensorflow-io still matches 1.12, is that we could not release a version that is based on release candidate (rc2). We are just waiting for 1.13 release and then a version of tensorflow-io will be released immediately.\r\n\r\nAlso, `tensorflow-io` has a 2.0 preview version as well:\r\n\r\n```\r\npip install tensorflow-io-2.0-preview\r\n```\r\nwhich is based on TF 2.0 (tf-nightly-2.0-preview). You could also give it a try if you are more interested in TF 2.0.\r\n\r\nWe have a google discussion group:\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!forum/io\r\n\r\nYou may consider joining this group to get more information if you are interested.", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 20387, "title": "Tensorflow C++ not releasing GPU resources after closing the session", "body": "### System information\r\n- **Have I written custom code:**: No\r\n- **OS Platform and Distribution: **: Windows 10\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version (use command below)**: tensorflow r1.7\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: MSVC v140\r\n- **CUDA/cuDNN version**:  CUDA 9.0 and cuDNN v7.0.5\r\n- **GPU model and memory**:  NVIDIA GeForce GTX 1050 Ti 4095 MB\r\n- **Exact command to reproduce**: See code below\r\n\r\n### Describe the problem\r\nCompiled tensorflow C++ with GPU support from source (branch r1.7) on Windows 10. Upon creating a new session 3GBs of memory are allocated on the GPU. Closing the session does not seem to result in the memory being released from the GPU as confirmed by the nvidia-smi command. Resources are only released when the C++ program exits.\r\n\r\nIt would be desirable to have the option of releasing all the resources that tensorflow allocated since in the application I am developing the GPU computation is needed only very infrequently and if tensorflow keeps the GPU memory it is not available for rendering etc.\r\n\r\n### Source code / logs\r\n```\r\nint main()\r\n{\r\n    SessionOptions options;\r\n    Session* session;\r\n    tensorflow::Status status = NewSession(SessionOptions(), &session); // returns ok\r\n\r\n    status = session->Close(); // returns ok\r\n    delete session;\r\n\r\n    // GPU memory is still occupied at this point\r\n    std::string s;\r\n    std::cin >> s;\r\n\r\n    return 0;\r\n}\r\n// GPU memory is released when process exits\r\n```\r\n\r\n", "comments": ["Related to\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/19731\r\n- https://github.com/tensorflow/tensorflow/issues/19571\r\n- https://github.com/tensorflow/tensorflow/issues/15880\r\n- https://github.com/tensorflow/tensorflow/issues/1578\r\n", "@cy89 do you know how GPU memory is supposed to be released, or can you redirect as necessary? Thanks.", "From perusing the related issues that @sjperkins listed, it seems to me like this issue remains open in a number of places, and that @zheng-xq seems to have the most definitive opinions. I'll redirect to him.", "One suggestion comes to mind: Change the ProcessState (base and GPU) instance method to return a shared_ptr that destroys ProcessState when the reference count drops to zero.", "I too am having this issue. I wish to embed tensorflow within a c++ application that has other modules that wish to use the GPU so it is essential that I am able to free the GPU memory when Tensorflow has finished with it. Is there any progress on this??", "Same here. We could not find code sample to free the GPU memory when Tensorflow has finished processing. This can be done either as part of config itself or a code snippet at the end of the program.", "Adding one more related  issue #17048\r\n\r\nThe mechanism in eager execution where memory can be allocated or deallocated on the fly may be potentially helpful, or add built-in function to check and free cache in cuda after session finishes.", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue as there are a couple of workarounds depending on implementation specifics. ", "The mitigations don't really work.  In particular, the `numba` mitigation leaves Tensorflow in a broken state.  We need a proper fix, see https://github.com/tensorflow/tensorflow/issues/15880 "]}]